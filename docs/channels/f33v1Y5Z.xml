<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Latest</title><link>https://www.awesome-dev.news</link><description></description><item><title>&apos;The Models Were Right!&apos; Astronomers Locate Universe&apos;s &apos;Missing&apos; Matter</title><link>https://science.slashdot.org/story/25/06/21/064247/the-models-were-right-astronomers-locate-universes-missing-matter?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 21 Jun 2025 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[It's not dark matter, writes Space.com. But astronomers 	have discovered "a vast tendril of hot gas linking four galaxy clusters and stretching out for 23 million light-years, 230 times the length of our galaxy. 

"With 10 times the mass of the Milky Way, this filamentary structure accounts for much of the universe's 'missing matter,' the search for which has baffled scientists for decades...."

 [I]t is "ordinary matter" made up of atoms, composed of electrons, protons, and neutrons (collectively called baryons) which make up stars, planets, moons, and our bodies. For decades, our best models of the universe have suggested that a third of the baryonic matter that should be out there in the cosmos is missing. 

This discovery of that missing matter suggests our best models of the universe were right all along. It could also reveal more about the "Cosmic Web," the vast structure along which entire galaxies grew and gathered during the earlier epochs of our 13.8 billion-year-old universe.... The newly observed filament isn't just extraordinary in terms of its mass and size; it also has a temperature of a staggering 18 million degrees Fahrenheit (10 million degrees Celsius). That's around 1,800 times hotter than the surface of the sun... 

The team's research was published on Thursday (June 19) in the journal Astronomy & Astrophysics.
 

Models of the cosmos (including the standard model of cosmology) "have long posited the idea that the missing baryonic matter of the universe is locked up in vast filaments of gas stretching between the densest pockets of space..." the article points out. But now thanks to Suzaku, a Japan Aerospace Exploration Agency (JAXA) satellite, and the European Space Agency's XMM-Newton, "a team of astronomers has for the first time been able to determine the properties of one of these filaments, which links four galactic clusters in the local universe." 

Team leader Konstantinos Migkas (of the Netherlands' Leiden Observatory) explained the significance of their finding. "For the first time, our results closely match what we see in our leading model of the cosmos — something that's not happened before." 

"It seems that the simulations were right all along."]]></content:encoded></item><item><title>One Shot To Stop HIV: MIT&apos;s Bold Vaccine Breakthrough</title><link>https://science.slashdot.org/story/25/06/21/0451227/one-shot-to-stop-hiv-mits-bold-vaccine-breakthrough?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 21 Jun 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ ScienceDaily reports:

Researchers from MIT and Scripps have unveiled a promising new HIV vaccine approach that generates a powerful immune response with just one dose. By combining two immune-boosting adjuvants alum and SMNP the vaccine lingers in lymph nodes for nearly a month, encouraging the body to produce a vast array of antibodies. This one-shot strategy could revolutionize how we fight not just HIV, but many infectious diseases. It mimics the natural infection process and opens the door to broadly neutralizing antibody responses, a holy grail in vaccine design. And best of all, it's built on components already known to medicine.
 

Thanks to Slashdot reader alternative_right for sharing the news.
]]></content:encoded></item><item><title>Anthropic Deploys Multiple Claude Agents for &apos;Research&apos; Tool - Says Coding is Less Parallelizable</title><link>https://developers.slashdot.org/story/25/06/21/0442227/anthropic-deploys-multiple-claude-agents-for-research-tool---says-coding-is-less-parallelizable?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 21 Jun 2025 15:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[In April Anthorpic introduced a new AI trick: multiple Claude agents combine for a "Research" feature that can "search across both your internal work context and the web" (as well as Google Workspace "and any integrations...") 

But a recent Anthropic blog post notes this feature "involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously," which brings challenges "in agent coordination, evaluation, and reliability.... The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings."
Multi-agent systems work mainly because they help spend enough tokens to solve the problem.... This finding validates our architecture that distributes work across agents with separate context windows to add more capacity for parallel reasoning. The latest Claude models act as large efficiency multipliers on token use, as upgrading to Claude Sonnet 4 is a larger performance gain than doubling the token budget on Claude Sonnet 3.7. Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents. 

There is a downside: in practice, these architectures burn through tokens fast. In our data, agents typically use about 4Ã-- more tokens than chat interactions, and multi-agent systems use about 15Ã-- more tokens than chats. For economic viability, multi-agent systems require tasks where the value of the task is high enough to pay for the increased performance. Further, some domains that require all agents to share the same context or involve many dependencies between agents are not a good fit for multi-agent systems today. 

For instance, most coding tasks involve fewer truly parallelizable tasks than research, and LLM agents are not yet great at coordinating and delegating to other agents in real time. We've found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools. 
Thanks to Slashdot reader ZipNada for sharing the news.]]></content:encoded></item><item><title>This College Student Wanted Privacy - His College Couldn&apos;t Give Him Any</title><link>https://hackernoon.com/this-college-student-wanted-privacy-his-college-couldnt-give-him-any?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 21 Jun 2025 15:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[On a recent Monday, Eric Natividad woke up around 8 a.m., showered, ate breakfast, and braced himself for a day of being tracked.\
Natividad, 32, is a student at Mt. San Antonio College, which is one of California’s largest community colleges, serving more than 26,000 students east of Los Angeles, about half of whom attend part time. Like virtually all college students in 2023, his life is constantly being converted into a steady stream of data. This information undergirds algorithms and informs decisions by his professors, college administrators, campus police officers, and a far-reaching universe of technology companies—including some he has never heard of.\
By the time Natividad went to bed that night, Google and Facebook had data about which Mt. SAC webpages he’d visited, and a company called Instructure had gathered information for his professors about how much time he’d spent looking at readings for his classes and whether he had read messages about his courses. Campus police and a company called T2 Systems potentially had information about what kind of car he was driving and where he parked. And as he drifted off to sleep, Natividad had to contend with the worry that, later this semester, his professors could subject him to the facial detection software incorporated into the remote proctoring tools used at Mt. SAC.\
“There isn’t a part of the day where it leaves your mind,” Natividad said about the pervasive tracking.\
To understand how Mt. SAC collects data on its students, The Markup used public records requests to obtain contracts between the college and companies that provide its learning management system, online proctoring services, and automated parking enforcement technology, three of the most invasive data collection mechanisms on campus. The Markup also obtained five college policies that govern these technologies, as well as information security and computer use, at Mt. SAC.\
Few institutions collect as much data about the people inside of them as colleges and universities do. Residential campuses, in particular, mean students not only interact with their schools for academics, but for housing, home internet, dining, health care, fitness, and socialization. Still, whether living on campus or off, taking classes in person or remotely, students simply cannot opt out of most data collection and still pursue a degree.A day of data collection for a college studentAs college students go through their days, their movements and behaviors can be tracked on and off campusFor many students, that’s not a problem. They generally trust their institutions and see the online elements of higher education as convenient. Putting up with data collection seems like a necessary cost.\
But even though Natividad’s preoccupation with data privacy makes him a bit of an anomaly among his classmates, he’s part of a growing group of college students arguing it shouldn’t be this way. Long written off as not caring about privacy because of their extensive sharing on social media, college students have become more organized and insistent about what they see as a right. At the University of Michigan, a student’s quest for greater transparency led to ViziBLUE, a website launched in 2020 that lets students see what personal information is collected and how it is used. \
When the COVID pandemic forced a huge portion of higher education to move online, students across the country protested the use of online exam-proctoring software that gathered information about their faces and homes. On many campuses, protesters have pressured university administrators to commit to banning the use of facial recognition technology before ever trying it.\
And as more colleges disclose data breaches, many students are becoming uneasy about how much personal information their schools gather. They are forming new on-campus student groups to advocate for privacy and tapping into global networks designed to facilitate a more collective fight. Some colleges are taking note of the unrest as well as the liability inherent in holding so much data. The University of California, San Diego, for example, is among the universities that have created stand-alone positions for chief privacy officers in recent years.\
Juan Cruz is the director of policy at Encode Justice Florida, a state chapter of a global youth-led advocacy organization fighting for human-centered artificial intelligence. A student at Florida International University, Cruz sees collegegoers as among the most vulnerable to data privacy violations.\
“You don’t really get any options,” he said. “You’re just kind of expected, if you want to pass this class or pass this test, you have to willingly give up a lot of your information and then hope that nothing happens.”When Natividad was in high school he didn’t pay much attention to the data he was generating for ed tech companies, or to news reports detailing data breaches. Coming of age during the Occupy Wall Street protests and spending several years in China, however, opened his eyes.\
While living in Shanghai, Natividad saw security cameras everywhere and the introduction of facial recognition cameras on the subways, but he said it wasn’t until he saw articles about the same technology in the United States that he started doing more research about it. He started to believe surveillance technology runs counter to a free democracy and began monitoring purchases made by the Los Angeles Police Department. He also started paying closer attention to all the companies that get access to his data and became convinced this was something he should learn more about and protect himself against.\
“You just don’t know how you’re going to be targeted sometimes,” Natividad said, “by state or police or bad actors or somebody trying to steal money. … This is something that I need to care about, too.”\
When he registered at Mt. SAC in 2022, interested in pursuing a degree in computer science, he asked questions about the technology he’d have to use and whether he could opt out of data collection. He was told opting out wasn’t an option but figured he’d enroll and see how it went.\
He found more cause for concern than he had expected.\
On that recent Monday morning, after his shower and breakfast, Natividad turned on his computer. He needed to log into Canvas, his college’s learning management system for online coursework, reading, class discussion, quizzes and faculty gradebooks, which is used by more than a third of higher education institutions in North America. Natividad’s goal was to be in Canvas for as little time as possible, avoiding one of the primary dragnets of data collection for college students.Soon, he had downloaded everything he was supposed to read for the week, copied assignment instructions for each of his classes into plain text documents, sometimes with screenshots, and logged back out. Over the course of the week, he read the documents offline and drafted his assignments in a text editor. He only logged back into Canvas to check for any updates from his professors and, ultimately, to copy and paste his work when it was ready to submit.\
Canvas tracks what pages students view, their time spent on each page, and when they submit discussion comments or assignments. It houses students’ grades, tracks when they complete quizzes and how long they take to finish them, and it gives professors visualizations of student progress and participation in the course.\
Some professors use this information to refine their teaching and serve students better, and at many institutions this data informs course planning, program design, and academic advising. But it’s not only educators who have access to it.\
Mt. SAC’s contract for Canvas, which The Markup obtained through a public records request, says that the parent company, Instructure, owns the usage data. The contract lists examples of how the company can use that data, including statistical analyses, trend analyses, and the creation of “data models.” The contract says usage data can only be used if it is aggregated or anonymized and should never be used for profit or sale—but in 2019, Instructure’s former CEO Dan Goldsmith pointed investors to the company’s corpus of education data as key to its multibillion-dollar value, saying it could be used to train algorithms and predictive models.\
Since that comment, Instructure has stopped working on predictive models, according to Daisy Bennett, who said she was hired as the company’s privacy officer in part to repair the damage from Goldsmith’s claims.\
“We are not monetizing our schools’ data,” Bennett said. “Absolutely not.”\
Natividad, however, still doesn’t think the company should be able to have or keep any data he generates while using its platform. And Canvas isn’t his only problem.\
This semester, one of Natividad’s professors assigned a digital textbook through Cengage, a publishing company turned ed tech behemoth. In the past, professors have allowed Natividad to stick with paper textbooks, but he now must submit to the additional data tracking of e-books for a required course. Professors sometimes mandate digital textbooks because they want students to complete interactive assignments contained in them.\
According to Cengage’s online privacy policy, the company collects information about a student’s internet network and the device they use to access online textbooks as well as webpages viewed, links clicked, keystrokes typed, and movement of their mouse on the screen, among other things. The company then shares some of that data with third parties for targeted advertising. For students who sign into Cengage websites with their social media accounts, the company collects additional information about them and their entire social networks.\
Noah Apthorpe, an assistant professor of computer science at Colgate University, studies privacy and the implications of data collection in academia. He said students seem to be suffering the consequences of widespread data collection across the web.\
“Once we become accustomed to an ecosystem where everything is collected, it both becomes harder to change and we become numb to it happening in the first place,” Apthorpe said. Learning management systems like Canvas collect more data about student behavior than educators have ever had access to, creating detailed profiles of individual students and new ways to label them. While Canvas is one of the most popular systems on the market, virtually all colleges use one and a variety of options have emerged to serve them, including Moodle, Blackboard, and Brightspace.\
Apthorpe said that because learning management systems track, down to the minute, when students submit assignments, professors may come to think of students as deadline pushers, for example, or those who get their work done early.\
“You can imagine that having repercussions for students,” Apthorpe said.\
Students know their professors can see this data. Some who work or take care of children on top of going to school worry they will be seen as lazy or uncommitted for spending less time on their assignments than their peers.\
Natividad said one of his professors told him he can see in Canvas whether students seem to log on together and complete work from the same location. Natividad became worried it would look like he was cheating if he simply studied and completed assignments with his peers. Now he and his friends go out of their way to avoid the appearance of impropriety, not wanting to be falsely accused. For them, that means staggering their log-ons to Canvas and accessing the internet through virtual private networks that obscure their location.\
While Instructure insists its platform should not be used to detect cheating, its logs have led to such allegations. Seventeen medical students at Dartmouth made national news in 2021 when their professors accused them of accessing course materials in Canvas during an online exam. The logs, students argued, were wrong, and the university ultimately dropped the charges.\
For Natividad, stress about what could happen has taken its toll. Last year, he watched his grades in a computer science course slip after he stopped completing assignments because he didn’t want to use Canvas.\
“It’s just really stressful,” he said.Most concerning to Natividad is the parking enforcement at Mt. SAC, which last winter installed automated license plate readers. Instead of asking students to hang parking passes on their rearview mirrors, the college now relies on cameras mounted on three parking enforcement vehicles and 11 fixed cameras installed in the campus’s two garages. The cameras read license plates and alert officers if they spot a plate that doesn’t have a valid permit associated with it.\
Mt. SAC’s policy for its automated license plate reader says none of the information in the system can be sold and that it can only be shared or transferred after a request is made in writing and college officials approve it in writing.But the college’s contract with T2 Systems, also obtained through a public records request, says the parking management company can store, back up, and archive content and use it to generate anonymized data, though the nature of that data is not made clear.\
Automated license plate readers have been the subject of controversy around the country. Fight for the Future, an activist organization that plans online protests to secure a future “where technology is a force for liberation, not oppression,” calls them “illegal dragnet surveillance” that violates individuals’ right to privacy.\
Natividad requested information about T2 and couldn’t find out much. For months, he said, he was passed from one campus employee to another but got few concrete details about where the parking footage and data would be stored and who would have access to it.\
Mt. SAC’s police and campus safety department declined interview requests for this article. The college’s contract indicates license plate information and video footage are only stored if a campus police officer issues a ticket.\
Natividad said some college employees suggested he take classes online if he didn’t like the new parking cameras. Since that would mean losing out on valuable in-person experiences and spending even more time on Canvas, he has found another, more expensive solution: quarters.\
The college has retained a number of metered parking spaces. They cost more than a parking pass and require a tedious amount of coordination to always have enough change on hand and run back and forth to add more time, but they don’t require Natividad to turn over personal information to yet another tech company to get a parking pass. Still, even these steps do not ensure his privacy while parking: His car remains in view of cameras mounted on Mt. SAC public safety vehicles. Should a camera read error ever mean Natividad’s car is mistaken for a stolen vehicle, as happened to a San Francisco woman in 2009, that routine scanning could make him a target.\
Students at Mt. SAC have rallied in recent weeks in support of Palestine and Natividad worries that records of who, exactly, was on campus during the hours the protest lasted might create problems for his peers. Students on other campuses have had job offers rescinded and their names and faces published online in connection to their support for Palestine.\
At Mt. SAC, there haven’t been such dramatic consequences for students, and few have joined Natividad’s fight for greater privacy on campus. In fact, as he has been asking questions and quietly advocating over the last year and a half, the two most common reactions he gets are surprise and apathy. Most people don’t think there are alternatives to the current state of data tracking and surveillance, or they don’t care to explore them. Natividad routinely hears that he shouldn’t worry. But he does. And not just for himself—for the entire student body.\
“It just feels like our rights are being violated all the time,” he said.On a recent Thursday, Natividad parked at the meter and met a friend in Mt. SAC’s student center. It’s one of the buildings on campus he doesn’t need to swipe his student ID to enter—and while it’s not as good a location for studying as other parts of campus, he likes that he can avoid creating a new data point in the college’s logs. When he goes to any tutoring centers on campus, including just to study, he can’t avoid it.\
Logging student movement through card swipes is ubiquitous in higher education today. At Mt. SAC, Natividad was told the tutoring center uses this information to track how many students it supports so the college can apply for state grant money. College administrators did not respond to requests for comment about what else these logs are used for or how long they are stored.Eric Natividad swipes his ID to access a tutoring center at Mt. San Antonio College. That swipe is logged and contributes to a digital profile of Natividad’s movements around campus. Credit: Susanica Tam\
At the University of Maryland, administrators considered using this data in the early days of COVID to know when wellness checks were in order. Joseph Gridley, the university’s chief data privacy officer, recounted the scenario during an October conference for IT professionals working in higher education. He said it seemed mildly creepy to send people to student dorm rooms to check on them after noting they hadn’t been to the dining halls in a certain number of days. But after surveying students, it became clear they didn’t mind and, in fact, only thought the university should wait five days before dispatching someone.\
The data collection, Gridley said, is happening. The question is whether universities should use it.\
It’s also not clear if they can keep sensitive information secure. The University of Michigan revealed at the end of October that a hack compromised personal information, including Social Security numbers and driver’s licenses, of 230,000 people. When a well-resourced institution known for its commitment to privacy is home to such a leak, Natividad wonders how long his data will be safe at his relatively under-resourced community college.\
Natividad has spoken before Mt. SAC’s student government and tried to raise awareness about all the data that’s being collected. Mariah Moreno, a student senate chair at Mt. SAC, said while Natividad’s concerns have piqued the interest of some members of the campus community, more students would have to share them for the student government to consider taking action.\
That might only be a matter of time. Natividad also spoke with Max McCarthy Neal, a leader in the college’s Black Student Union, who told him to connect with other activism-oriented campus groups.\
“It’s something that is affecting all of us and will continue to do so,” said McCarthy Neal.\
Natividad has spent more than a year discussing his concerns with student leaders and college employees at all levels of the institution. He has braved sweaty palms and anxiety to speak at the college’s board of trustees meeting and the president’s open office hours. He has requested his data from tech companies, exercising his rights under the California Consumer Privacy Act.\
All of it has been a distraction from his studies. He is taking fewer classes to give himself time for this awareness-building. But he would prefer not to do any of it.\
“I like just being a geeky computer guy,” Natividad said. “I want to be the stereotypical guy on his computer in his room that nobody really talks about that much.”\
The problem, he said, is that many of his peers have no idea about the extent to which their data is being collected and shared. And many of them have weighty commitments outside of their coursework as parents and providers in their families. Natividad feels responsible for stepping up, agitating both because he can and knows he should.\
McCarthy Neal decided to help after coming to the same conclusion.\
“Due to the fact that we are a community college, most people are heads-down, in the books, do the work and move on to the next step,” McCarthy Neal said, whether that’s a transfer to a four-year school or a better job. “[They] don’t really care what’s going on here now.”\
McCarthy Neal hopes speaking out with Natividad and winning more students over to the cause will lead to greater power to opt out of data collection at Mt. SAC and more attention to student privacy generally.\
Back in Mt. SAC’s student center, Natividad sits down with his friends to study; he accesses the school website and Canvas using a virtual private network. In addition to obscuring his location, the VPN encrypts his data, giving him a sense of protection from tracking.\
That tracking, he knows, can be extensive. A Markup analysis of Natividad’s network logs showed multiple analytics platforms tracking pageviews and clicks in Canvas and on the college website, including on a health center webpage about coping with anxiety and a counseling request form for DACA students (who are protected from deportation because they immigrated to this country with their parents as young children). The platforms send analytics to Instructure, Google, and Facebook.\
Natividad points to a wave of privacy class-actionlawsuits against companies sharing personally identifiable information with Facebook through a snippet of code known as a pixel.\
“These companies are being sued for privacy violations, but we’re still using [the pixel] all over,” he said.At some point this semester, Natividad may have to give up another type of personal information. Two of his courses are completely online, and the university has contracts with two companies that facilitate secure remote testing. If his professors require students to use these so-called e-proctoring tools, Natividad might have to give either Honorlock or Proctorio access to his laptop camera. While both companies say they do not use or store biometric data or match test-takers’ faces with an image database, they do run software to detect students’ eye movements and the presence of their faces. \
In its contract with Honorlock, which The Markup obtained through a public records request, Mt. SAC agreed to let the company use, publish, and sell aggregate data collected over the platform, facilitating the company’s ability to profit from students’ data.\
E-proctoring tools faced a stiff backlash when schools closed during COVID and sent test-taking online. Fight for the Future called the tech “glorified spyware” in an online campaign seeking to ban its use by colleges. Students with disabilities faced more frequent flags for potential cheating because of hand, eye, and body movements the software algorithms said were abnormal. Dark-skinned students reported not being able to take exams because the software wouldn’t register their faces as being present.\
The programs’ failings fueled outrage and stress among students needing to take exams to progress toward their degrees or chosen careers.\
At Mt. SAC, McCarthy Neal dropped an online course last spring rather than use Proctorio. The professor told students they had to run the monitoring program every time they completed an assignment in the online workbook, something that felt overly invasive.\
Students on other campuses have fought back against the use of ExamSoft and ProctorU, two other tools that collect biometric data as part of their e-proctoring, using facial recognition software to match the face in front of the camera with records of the student who is supposed to be there.\
While Mt. SAC officials said the college doesn’t use it, some universities have begun experimenting with facial recognition technology for attendance logs and campus security, and researchers are testing it as a method for measuring student engagement.\
Leila Nashashibi, a Fight for the Future campaigner, said a major concern with facial recognition technology is that it dissuades social and protest movements.\
“One of the most successful strategies for quelling dissent is to make people feel like they have no privacy,” she said.\
When facial recognition systems capture data, they tend to store it in the cloud where, Nashashibi points out, it is vulnerable to being hacked, stolen, or abused. When people get their credit cards stolen, they can get new cards and new numbers; when their biometric information is stolen, they can’t change their faces. Nashashibi sees the use of facial recognition for things like attendance or campus security as a slippery slope.\
“As it spreads in these seemingly convenient and innocuous use cases, it’s desensitizing people to the technology, which is actually invasive and dangerous,” she said.Natividad has had a hard time convincing his fellow students at Mt. SAC that it’s worth their time to speak out against data collection in higher education, or even to demand more transparency. But on other campuses, privacy concerns have contributed to new tools and staff positions.\
At the University of Michigan, the ViziBLUE website explains how the university collects, uses, and shares 18 different types of data, including that related to academics, admissions, housing, and use of campus Wi-Fi. Virtually no other university has anything like it. At UC San Diego, where Pegah Parsi became the inaugural chief privacy officer in 2018, a team is working on something similar but running into challenges of even identifying the scope of data collection. Parsi’s department has started by focusing on data collected and used by offices of advancement, admissions, and financial aid—the “heavy hitters,” as she calls them—and then plans to work through smaller, more grassroots sources of data collection on campus.\
“We haven’t had regulator scrutiny, to a great extent, on our privacy practices or our data practices, so our data really do live all over the place, and no one quite knows who has what,” Parsi said.Mariah Moreno, the student government senate chair, hadn’t spent any time thinking about Canvas’s data collection and use policies before Natividad brought it up.\
“With this day and age, it had kind of been normalized, so I hadn’t felt that concern related to myself as a student,” she said.\
Having now acknowledged the privacy concerns, she still doesn’t think it makes sense to stop using Canvas or even to use it any differently. On her path to a political science degree and eventually law school, Moreno sees herself as having two options: do the assignments as the professors assign them—in Canvas—or not do the assignments. And she chooses the former.\
But privacy advocates say there should be a third choice that gives students more control over their own data.\
Kyle Jones, an associate professor at Indiana University—Indianapolis who studies information ethics and data mining in higher education, is among those arguing that higher education institutions should be considered data fiduciaries, charged with acting in the best interests of students when collecting and using their data. Fiduciary duties are most commonly connected to managing someone else’s money, and they bring legal responsibilities to make decisions solely for the other person’s benefit.\
Parsi, too, encourages her colleagues to think of their role as data fiduciaries. Much more common, she said, is for higher education institutions to think of themselves as stewards of student data, which connotes a lower standard of responsibility to students. For Parsi, it’s past time for change.\
“Our use of someone’s personal data, at the end of the day, should benefit them,” she said, “not just somebody else or some other thing.”]]></content:encoded></item><item><title>Bug Hits Some Threads Users: Their Words Echoed by All Other Users</title><link>https://tech.slashdot.org/story/25/06/21/018232/bug-hits-some-threads-users-their-words-echoed-by-all-other-users?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 21 Jun 2025 14:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Threads now has 350 million users — but this week a strange bug affected some Threads accounts (on both desktop and mobile). "One user's post will get repeated over and over again..." explains TechCrunch, "as though every user on your feed is saying the same thing."


"Siri, unsubscribe me from 2025," one Threads user wrote, per a screenshot from social media expert Alexa Heinrich. But then, everyone else on Heinrich's feed appeared to be echoing the same cheugy joke... 

While it's not yet clear what caused the bug, Meta Communications Director Andy Stone responded to app researcher Jane Manchun Wong's post about the issue. "Whoops, well that clearly shouldn't have happened! We're working on getting it fixed now," Stone said. 

I thought the bug was only affecting user feeds (and not replies). But either way, Wong came up with the perfect comeback. 

"Whoops, well that clearly shouldn't have happened! We're working on getting it fixed now."]]></content:encoded></item><item><title>How to Connect an Express Application to Postgres Using Sequelize</title><link>https://hackernoon.com/how-to-connect-an-express-application-to-postgres-using-sequelize?source=rss</link><author>Michael Ikoko</author><category>tech</category><pubDate>Sat, 21 Jun 2025 14:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[PostgreSQL (shortened as Postgres) is one of the most used databases in building software applications. Postgres is a general-purpose, open-source, object-relational database management system (RDBMS). One of the advantages of using Postgres is its support for both relational (SQL) and non-relational (NoSQL) querying.\
One way of connecting a database to a web application is through the use of an ORM (Object Relational Mapper). An ORM acts as a layer of communication between your application and the database. The goal of this tutorial is to explain how you can use Postgres in your Express application through the Sequelize ORM.\
The Sequelize ORM is described as:A modern TypeScript and Node.js ORM for Oracle, Postgres, MySQL, MariaDB, SQLite, and SQL Server, and more. — Sequelize Documentation\
You will build a simple task management API. The API will be able to create, list, update the completion status, and delete tasks.\
This tutorial is the first in an upcoming series of tutorials focused on using relational databases in Express using Sequelize. In the upcoming tutorials, you’ll explore database seeding, migrations, and associations in Sequelize.To proceed with this tutorial, you’ll need the following:A text editor (e.g., VS Code)An API client for testing the endpoints (e.g., Postman)Node.js is installed on your computerBasic knowledge of ExpressAn instance of Postgres running either locally or remotelyWe’ll begin by setting up appropriate files and directories for creating an Express application and installing the needed packages. The commands used are only applicable to a Linux terminal.Create the project directory:Navigate to the project directory:Initialize the NPM package by running the following command to create a   file with default settings:Install Express and other core dependencies:   npm install express express-async-errors dotenv && npm install nodemon --save-dev
Install Postgres driver for Node.js:In the root directory, create the  and folders:In the root directory, create a  file, which will contain the server’s port number and the database URL of any Postgres instance:PORT=5000
DATABASE_URL=postgres://<user>:<password>@<host>:<port>/<database>
In the root directory, create the  file, which is the application entry point:Set up the command to run the local development server by editing the  object in :{
    //…
    "scripts": {
        "test": "echo \"Error: no test specified\" && exit 1",
        "dev": "nodemon index.js",
        "start": "node index.js"
    },
    //…
}
The project folder structure should look like this:Create two files in the  directory:  and Loading Environment VariablesIn the  file, load the environment variables from the  file using the  package. Then export the  and  variables so that they can be accessed in other parts of the application.\
The  file should have the following contents:require('dotenv').config()

const PORT = process.env.PORT || 3000
const DATABASE_URL = process.env.DATABASE_URL

module.exports = {
    PORT,
    DATABASE_URL
}
Creating Sequelize InstanceIn the  file, create a Sequelize instance. You can create a Sequelize instance by passing the database connection URI (stored in ) to the Sequelize constructor. Then you create a function  that will test the connection to the database by calling the  function. Lastly, you export the  function and the Sequelize instance.\
The  file should have the following contents:const Sequelize = require("sequelize");
const { DATABASE_URL } = require("./config");

const sequelize = new Sequelize(DATABASE_URL)

const connectToDB = async () => {
    try {
        await sequelize.authenticate()
        console.log("Database connection established successfully.")
    } catch (error) {
        console.log("Unable to connect to the database:", error)
        return process.exit(1)
    }
    return null
}

module.exports = {
    connectToDB,
    sequelize
}
A Sequelize model is a representation of a table in the database. You can define the  model by extending the Sequelize  class and calling the Model.init(attributes, options) function.\
In the  directory, create the  file with the following contents:const {Model, DataTypes} = require("sequelize")
const {sequelize} = require("../utils/db")

class Task extends Model {}

Task.init({
    id: {
        type: DataTypes.INTEGER,
        autoIncrement: true,
        primaryKey: true
    },
    title: {
        type: DataTypes.STRING,
        allowNull: false
    },
    completed: {
        type: DataTypes.BOOLEAN,
        defaultValue: false
    }
}, {
    sequelize,
    modelName: "Task",
    timestamps: true,
    underscored: true,
    defaultScope: {
        attributes: {
            exclude: ["createdAt", "updatedAt"]
        }
    }
})

Task.sync()

module.exports = Task
The  parameter defines the structure of the  table in the database. The  model has three attributes:: An integer field which is an auto-increasing primary key used to uniquely identify each record.: A string field that represents the name of the task.: The  field has a boolean value that indicates whether the task has been done.\
The  parameter configures how Sequelize handles the model. The  model has the following options:: The Sequelize instance created earlier in .: The name of the table created in the database.: When set to , adds the  and  fields automatically to the model.: When set to  , converts camel Case fields to snake case in the database.: Excludes certain attributes by default when querying.\
The  function synchronizes the model with the database by creating the table if the table does not exist. However, you should note that synchronization should be done using migrations.Creating the Express ServerFinally, you put it all together by creating the Express server. In the  file, you set up the necessary middleware, define the API endpoint routes, and run the express server.\
The  function is responsible for initializing the Express server. The  function first tests the connection to the database by calling the  function. If the connection is successful, it starts the Express server, which listens on the specified port.\
The  file has the following contents:require("express-async-errors");
const express = require("express");
const app = express();
const { PORT } = require("./utils/config");
const { connectToDB } = require("./utils/db");
const Task = require("./models/task");

// middlewares
app.use(express.json());

// routes
app.get("/api/tasks", async (req, res) => {
  const tasks = await Task.findAll();
  res.json({
    message: "List of tasks",
    tasks: tasks,
  });
});

app.post("/api/tasks", async (req, res) => {
  const { title } = req.body;
  const task = await Task.create({ title });
  res.status(201).json({
    message: "Task created successfully",
    task,
  });
});

app.patch("/api/tasks/:id/toggle-completed", async (req, res) => {
  const { id } = req.params;
  const task = await Task.findByPk(id);
  if (!task) {
    return res.status(404).json({ message: "Task not found" });
  }
  task.completed = !task.completed;
  await task.save();
  res.json({
    message: task.completed
      ? "Task marked as completed"
      : "Task marked as not completed",
    task,
  });
});

app.delete("/api/tasks/:id", async (req, res) => {
  const { id } = req.params;
  const task = await Task.findByPk(id);
  if (!task) {
    return res.status(404).json({ message: "Task not found" });
  }
  await task.destroy();
  res.json({
    message: "Task deleted successfully",
  });
});

const start = async () => {
  try {
    await connectToDB();
    app.listen(PORT, console.log(`Server is running on port ${PORT}`));
  } catch (error) {
    console.error(error);
    process.exit(1);
  }
};

start();
You can now proceed to test the API endpoints:Create a new task—:List all tasks—:Toggle completion status—PATCH /api/tasks/:id/toggle-completed:Delete a Task—:You now know how to connect an Express application to a Postgres database using Sequelize. You built a simple task manager API, and in the process, you configured Sequelize, connected Sequelize to a Postgres instance, defined the  model, and created the API endpoints.\
Currently, the controller logic is written in the   file for simplicity. In upcoming tutorials, we’ll refactor this codebase into a more scalable structure using controllers, routers, and Sequelize migrations.\
For further reading, you should go through the Sequelize documentation to learn more about model querying, validations, associations, and more.\
You can find the complete source code for this tutorial on GitHub.]]></content:encoded></item><item><title>ChatGPT May Be Linked to &apos;Cognitive Debt,&apos; New Study Finds</title><link>https://www.404media.co/is-chatgpt-rotting-our-brains-new-study-suggests-it-does/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/06/1613px-Anaconda_al_acecho.jpg" length="" type=""/><pubDate>Sat, 21 Jun 2025 13:00:18 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to the Abstract! This week, we’re moving in next to anacondas, so watch your back and lock the henhouse. Then, parenthood tips from wild baboons, the “cognitive debt” of ChatGPT, a spaceflight symphony, and a bizarre galaxy that is finally coming into view. When your neighbor is an anacondaAnacondas are one of the most spectacular animals in South America, inspiring countless  myths and legends. But these iconic boas, which can grow to lengths of 30 feet, are also a pest to local populations in the Amazon basin, where they prey on livestock. To better understand these nuanced perceptions of anacondas, researchers interviewed more than 200 residents of communities in the várzea regions of the lower Amazon River about their experiences with the animals. The resulting study is packed with amazing stories and insights about the snakes, which are widely reviled as thieves and feared for their predatory prowess.“Fear of the anaconda (identified in 44.5% of the reports) is related to the belief that it is a treacherous and sly animal,” said co-authors led by Beatriz Nunes Cosendey of the Mamirauá Sustainable Development Reserve and Juarez Carlos Brito Pezzuti of the Federal University of Pará.“The interviewees convey that the anaconda is a silent creature that arrives without making any noise, causing them to feel uneasy and always vigilant during fishing…with the fear of having their canoe flooded in case of an attack,” the team added. “Some dwellers even reported being more afraid of an anaconda than of a crocodile because the latter warns when it is about to attack.”But while anacondas are eerily stealthy, they also have their derpy moments. The snakes often break into chicken coops to feast on the poultry, but then get trapped because their engorged bodies are too big to escape through the same gaps they used to enter.  “Dwellers expressed frustration at having to invest time and money in raising chickens, and then lose part of their flock overnight,” the team said. “One interviewee even mentioned retrieving a chicken from inside an anaconda’s belly, as it had just been swallowed and was still fresh.”Overall, the new study presents a captivating portrait of anaconda-human relations, and concludes that “the anaconda has lost its traditional role in folklore as a spiritual and mythological entity, now being perceived in a pragmatic way, primarily as an obstacle to free-range poultry farming.”Monkeying around with Dad  Coming off of Father’s Day, here is a story about the positive role that dads can play for their daughters—for baboons, as well as humans. A team tracked the lifespans of 216 wild female baboons in Amboseli, Kenya, and found that subjects who received more paternal care had significantly better outcomes than their peers.“We found that juvenile female baboons who had stronger paternal relationships, or who resided longer with their fathers, led adult lives that were 2–4 years longer than females with weak or short paternal relationships,” said researchers led by David Jansen of the Midwest Center of Excellence for Vector-Borne Disease. “Because survival predicts female fitness, fathers and their daughters may experience selection to engage socially and stay close in daughters’ early lives.”This all reminds me of that old episode of  where Lisa calls Homer a baboon. While Homer was clearly hurt, it turns out that baboons might not be the worst animal-based insult for a daughter to throw at her dad.  A case for staying ChatGPT-FreeChatGPT may hinder creativity and learning skills in students who use it to write essays, relative to those who didn’t, according to an exhaustive new preprint study posted on arXiv. This research has yet not been peer-reviewed, and has a relatively small sample size of 54 subjects, but it still contributes to  about the cognitive toll of AI assistants. Researchers led by Nataliya Kosmya of the Massachusetts Institute of Technology divided the subjects — all between 18 and 39 years old — into three groups wrote SAT essays using OpenAI’s ChatGPT (LLM group), Google’s search engine, or with no assistance (dubbed “Brain-only”).“As demonstrated over the course of 4 months, the LLM group's participants performed worse than their counterparts in the Brain-only group at all levels: neural, linguistic, scoring,” the team said. “The LLM group also fell behind in their ability to quote from the essays they wrote just minutes prior.”When I asked ChatGPT for its thoughts on the study, it commented that “these results are both interesting and plausible, though they should be interpreted cautiously given the early stage of the research and its limitations.” It later suggested that “cognitive offloading is not always bad.” Even scientists can’t resist evocative language now and then—we’re all only human. Case in point: A new study likens the history of Asia’s space industry to “a musical concert” and then really runs with the metaphor.“The region comprises a diverse patchwork of nations, each contributing different instruments to the regional space development orchestra,” said researchers led by Maximilien Berthet of the University of Tokyo. “Its history consists of three successive movements” starting with “the US and former USSR setting the tone for the global space exploration symphony” and culminating with modern Asian spaceflight as “a fast crescendo in multiple areas of the region driven in part by private initiative.”Talk about a space opera. The rest of the study provides a comprehensive review of Asian space history, but I cannot wait for the musical adaptation.Peekaboo! I galax-see youIn 2001, astronomer Bärbel Koribalski spotted a tiny galaxy peeking out from behind a bright foreground star that had obscured it for decades, earning it the nickname the “Peekaboo Galaxy.” Situated about 22 million light-years from the Milky Way, this strange galaxy is extremely young and metal-poor, resembling the universe’s earliest galaxies.A new study confirms Peekaboo as “the lowest-metallicity dwarf in the Local Volume,” a group of roughly 500 galaxies within 36 million light-years of Earth.“This makes the Peekaboo dwarf one of the most intriguing galaxies in the Local Volume,” said co-authors Alexei Kniazev of the South African Astronomical Observatory and Simon Pustilnik of the Special Astrophysical Observatory of the Russian Academy of the Sciences. “It deserves intensive, multi-method study and is expected to significantly advance our understanding of the early universe’s first building blocks.”Thanks for reading! See you next week. Update: The original headline for this piece was "Is ChatGPT Rotting Our Brains? New Study Suggests It Does." We've updated the headline to "ChatGPT May Create 'Cognitive Debt,' New Study Finds" to match the terminology used by the researchers.]]></content:encoded></item><item><title>The Best AI Coding Tools You Can Use Right Now</title><link>https://spectrum.ieee.org/best-ai-coding-tools</link><author>Matthew S. Smith</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTA4MTAyNC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgwMjk1MzI3MX0.XQ01vRJsHmCTE6_IN17xaXv9OveMhv9laDrP6yiLLBg/image.jpg?width=600" length="" type=""/><pubDate>Sat, 21 Jun 2025 13:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Cursor and Claude Code are among the options worth your time]]></content:encoded></item><item><title>Intel Will Outsource Marketing To Accenture and AI, Laying Off Its Own Workers</title><link>https://slashdot.org/story/25/06/21/0251208/intel-will-outsource-marketing-to-accenture-and-ai-laying-off-its-own-workers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Intel is outsourcing much of its marketing work to Accenture, "as new CEO Lip-Bu Tan works to slash costs and improve the chipmaker's operations," reports OregonLive. From the report: The company said it believes Accenture, using artificial intelligence, will do a better job connecting with customers. It says it will tell most marketing employees by July 11 whether it plans to lay them off. "The transition of our marketing and operations functions will result in significant changes to team structures, including potential headcount reductions, with only lean teams remaining," Intel told employees in a notice describing its plans. The Oregonian/OregonLive reviewed a copy of the material.
 
Intel declined to say how many workers will lose their jobs or how many work in its marketing organization, which employs people at sites around the globe, including in Oregon. But it acknowledged its relationship with Accenture in a statement to The Oregonian/OregonLive. "As we announced earlier this year, we are taking steps to become a leaner, faster and more efficient company," Intel said. "As part of this, we are focused on modernizing our digital capabilities to serve our customers better and strengthen our brand. Accenture is a longtime partner and trusted leader in these areas and we look forward to expanding our work together."]]></content:encoded></item><item><title>Microsoft suspended the email account of an ICC prosecutor at The Hague</title><link>https://www.nytimes.com/2025/06/20/technology/us-tech-europe-microsoft-trump-icc.html</link><author>blinding-streak</author><category>hn</category><pubDate>Sat, 21 Jun 2025 12:06:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MVP &amp; Why We Confuse Building to Learn with Building to Earn • Jeff Patton • YOW! 2018</title><link>https://www.youtube.com/watch?v=NzxjEzA0G0o</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/NzxjEzA0G0o?version=3" length="" type=""/><pubDate>Sat, 21 Jun 2025 12:00:47 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at YOW! 2018. #GOTOcon #YOW
https://yowcon.com

Jeff Patton - Helps Companies Design & Create Better Products & Author of User Story Mapping

RESOURCES
https://x.com/jeffpatton
https://jpattonassociates.com

ABSTRACT
Minimum viable product is one of the most misunderstood, misused, and abused terms in contemporary software development.
In this talk Jeff will explain the misunderstandings made by thought leaders that lead to confusion we all deal with today. You’ll learn the counter-intuitive concepts hidden in the term and why really using them is so hard. You’ll learn about techniques that will ultimately help you find smaller successful releases, test your ideas faster, develop higher quality software more predictably, and release more confidently than ever before. Because hidden in this nasty little term are clues that can help you do all that.

Jeff Patton helps companies adopt a way of working that’s focused on building great products, not just building stuff faster. Jeff blends a mixture of Agile thinking, Lean and Lean Startup Thinking, and UX Design and Design Thinking to end up with a holistic product-centric way of working. Jeff is author of the bestselling O’Reilly book User Story Mapping which describes a simple holistic approach to using stories in Agile development without losing sight of the big picture. You can learn more about Jeff at: jpattonassociates.com. [...]

RECOMMENDED BOOKS
Jeff Patton • User Story Mapping • https://amzn.to/3AR9Uzb
Subramaniam & Hunt • Practices of an Agile Developer • https://amzn.to/2XjbWor
Uncle Bob • Clean Agile • https://amzn.to/3tpAqb5
Derby, Larsen & Schwaber • Agile Retrospectives • https://amzn.to/3hB4eNk
Jeff Sutherland • Scrum: The Art of Doing Twice the Work in Half the Time • https://amzn.to/2X4GQAD

https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.facebook.com/GOTOConferences
#MVP #MinimumViableProduct #SoftwareDevelopment #Lean #StartupThinking #SoftwareEngineering #Programming #JeffPatton #YOWcon

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Ubuntu 25.10 Planning To Raise RISC-V Support Baseline To RVA23 Profile</title><link>https://www.phoronix.com/news/Ubuntu-25.10-To-Require-RVA23</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 21 Jun 2025 10:55:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ahead of the all-important Ubuntu 26.04 LTS cycle, Canonical is looking to raise the required RISC-V ISA baseline for its Ubuntu 25.10 release due out later this year...]]></content:encoded></item><item><title>GNOME Fixes Years-Old Bug Of Trash Not Always Being Properly Emptied</title><link>https://www.phoronix.com/news/GNOME-Trash-Years-Old-Bug</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 21 Jun 2025 10:31:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[This Week in GNOME is out with its latest issue that outlines some UI progress made as well as addressing an aging GNOME bug that could result in not all files/directories being properly removed when emptying the Trash from the GNOME desktop...]]></content:encoded></item><item><title>XLibre 25.0 Released As Inaugural Version Of X.Org Server Fork</title><link>https://www.phoronix.com/news/XLibre-25.0-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 21 Jun 2025 10:18:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[XLibre 25.0 was just released as the first tagged release of this recent X.Org Server fork...]]></content:encoded></item><item><title>ADS &amp;#x26; Python Tools, (Sat, Jun 21st)</title><link>https://isc.sans.edu/diary/rss/32058</link><author></author><category>infosec</category><pubDate>Sat, 21 Jun 2025 10:13:41 +0000</pubDate><source url="https://isc.sans.edu/">Sans Edu Diaries</source><content:encoded><![CDATA[I'm taking this as an opportunity to remind you that Python tools on Windows and an NTFS disk, can access alternate data streams.Like my tool cut-bytes.py, here I use it to show the content of the Mark-of-the-Web stored inside the Zone.Identifier ADS:You just need to type a colon (:) followed by the ADS name after the filename.I didn't have to code this in Python for Windows, it's default behavior.I did code ADS features in my FileScanner tool. It's not written in Python, but in C for Windows, and I coded features to enumerate and scan alternate data streams.If you give it a file to scan, it will scan the file content, and also the content of all of its alternate data streams. Like with this download with a MotW:And if you give it a folder or a drive to scan, it will also enumerate and scan all alternate data streams.

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>KDE Plasma 6.4 Is Looking To Be In Good Shape, Fewer Bugs</title><link>https://www.phoronix.com/news/KDE-Plasma-6.4-First-Week</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 21 Jun 2025 10:04:57 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE developer Nate Graham is out with his weekly blog post summarizing all of the interesting KDE Plasma developments for the past week. Notable this week was the debut of Plasma 6.4 stable and from early user feedback appears to be in good shape...]]></content:encoded></item><item><title>YouTube Is Hiding An Excellent, Official High-Speed Pac-Man Mod In Plain Sight</title><link>https://games.slashdot.org/story/25/06/21/0257244/youtube-is-hiding-an-excellent-official-high-speed-pac-man-mod-in-plain-sight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[YouTube is quietly hosting Pac-Man Superfast within its "Playables" section. "You'd be forgiven for not knowing about YouTube Playables," writes Ars Technica's Kyle Orland. "Few seemed to note its official announcement last year as a collection of free-to-play web games built for the web using standard rendering APIs."
 
"The seeming competitor to Netflix's mobile gaming offerings is still described in an official FAQ as 'an experimental feature rolled out to select users in eligible countries/regions,' which doesn't make this post-Stadia gaming effort seem like a huge priority for Google." From the report: 
Weird origins aside, Pac-Man Superfast pretty much delivers what its name promises. While gameplay starts at an "Easy" speed that roughly matches the arcade original, the speed of both Pac-Man and the ghosts is slightly increased every few seconds (dying temporarily reduces the speed to a lower level). After a few minutes, you're advancing past the titular "Super Fast" speed to extreme reflex-testing speeds like Crazy, Insane, Maniac, and a final test that's ominously named "Doom."
 
Those who've played the excellent Pac-Man Championship Edition series will be familiar with the high-speed vibe here, but Pac-Man Superfast remains focused on the game's original maze and selection of just four ghosts. That means old-school strategies for grouping ghosts together and running successful patterns through the narrow corridors work in similar ways here. Successfully executing those patterns becomes a tense battle of nerves here, though, requiring multiple direction changes every second at the highest speeds. While the game will technically work with swipe controls on a smartphone or tablet, high-level play really requires the precision of a keyboard via a desktop/laptop web browser (we couldn't get the game to recognize a USB controller, unfortunately).
 
As exciting as the high-speed maze gameplay gets, though, Pac-Man Superfast is hampered by a few odd design decisions. The game ends abruptly after just 13 levels, for instance, making it impossible to even attempt the high-endurance 256-level runs that Pac-Man is known for. The game also throws an extra life at you every 5,000 points, making it relatively easy to brute force your way to the end as long as you focus on the three increasingly high-point-value items that appear periodically on each stage. Despite this, the game doesn't give any point reward for unused extra lives or long-term survival at high speeds, limiting the rewards for high-level play. And the lack of a built-in leaderboard makes it hard to directly compare your performance to friends and/or strangers anyway.]]></content:encoded></item><item><title>Scaling our observability platform by embracing wide events and replacing OTel</title><link>https://clickhouse.com/blog/scaling-observability-beyond-100pb-wide-events-replacing-otel</link><author>valyala</author><category>hn</category><pubDate>Sat, 21 Jun 2025 09:23:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Our internal system grew from 19 PiB to 100 PB of uncompressed logs and from ~40 trillion to 500 trillion rows. We absorbed a 20× surge in event volume using under 10% of the CPU previously needed. The required parsing and marshalling of events in OpenTelemetry proved a bottleneck and didn’t scale - our custom pipeline addressed this. ClickHouse-native observability UI for seamless exploration, correlation, and root-cause analysis with Lucene-like syntax.About a year ago, we shared the story of LogHouse - our internal logging platform built to monitor ClickHouse Cloud. At the time, it managed what felt like a massive 19 PiB of data. More than just solving our observability challenges, LogHouse also saved us millions by replacing an increasingly unsustainable Datadog bill. The response to that post was overwhelming. It was clear our experience resonated with others facing similar struggles with traditional observability vendors and underscored just how critical effective data management is at scale.A year later, LogHouse has grown beyond anything we anticipated and is now storing over 100 petabytes of uncompressed data across nearly 500 trillion rows. That kind of scale forced a series of architectural changes, new tools, and hard-earned lessons that we felt were worth sharing - not least that OpenTelemetry (OTel) isn’t always the panacea of Observability (though we still love it), and that sometimes custom pipelines are essential.In our case, this shift enabled us to handle a 20x increase in event volume using less than 10% of the CPU for our most critical data source - a transformation with massive implications for cost and efficiency.Other parts of our stack have also changed, not least due to the ClickHouse acquisition of HyperDX. Not only did this give us a first-party ClickHouse-native UI, but it also led to the creation of ClickStack - an opinionated, end-to-end observability stack built around ClickHouse. With HyperDX, we’ve started transitioning away from our Grafana-based custom UI, moving toward a more integrated experience for exploration, correlation, and root cause analysis.As more teams adopt ClickHouse for observability and realize just how much they can store and query affordably, we hope these insights prove as useful as our first post. If you’re curious about this journey, when and where OTel is appropriate, and how we scaled a log pipeline to 100PB…read on.Interested in seeing how ClickHouse works on your data? Get started with ClickHouse Cloud in minutes and receive $300 in free credits.Over the past year, our approach to observability has undergone a significant transformation. We've continued to leverage OpenTelemetry to gather general-purpose logs, but as our systems have scaled, we began to reach its limits. While OTel remains a valuable part of our toolkit, it couldn't fully deliver the performance and precision we needed for our most demanding workloads. This prompted us to develop purpose-built tools tailored to our critical systems and rethink where generic solutions truly fit. Along the way, we've broadened the range of data we collect and revamped how we present insights to engineers.When we last wrote about LogHouse, we were proud to handle 19 PiB of uncompressed data across 37 trillion rows. Today, those numbers feel like a distant memory. LogHouse now stores over 100 petabytes of uncompressed data, representing nearly 500 trillion rows.
Here's a quick look at the breakdown:These numbers also tell a story. In our original post, 100% of our telemetry flowed through OpenTelemetry, with every log line collected via the same general-purpose pipeline. But as the scale and complexity of our data grew, so did the need for specialization.
While our total volume has grown more than 5x, the breakdown reveals a deliberate shift in strategy: today, the vast majority of our data comes from “SysEx”, a new purpose-built exporter we developed to handle high-throughput, high-fidelity system logs from ClickHouse itself. This shift marks a turning point in how we think about observability pipelines - and brings us to our first key topic.We hope the following helps comprehend the scale at which LogHouse operates.Initially, we used OpenTelemetry (OTel) for all log collection. It was a great starting point and an established industry standard which allowed us to quickly establish a baseline where every pod in our Kubernetes environment shipped logs to ClickHouse. However, as we scaled, we identified two key reasons to build a specialized tool for shipping our core ClickHouse server telemetry.First, while OTel capably captured the ClickHouse text log via stdout, this represents only a narrow slice of the telemetry ClickHouse exposes. Any ClickHouse expert knows that the real gold lies in its  - a rich, structured collection of logs, metrics, and operational insights that go far beyond what’s printed to standard output. These tables capture everything from query execution details to disk I/O and background task states, and unlike ephemeral logs, they can be retained indefinitely within a cluster. For both real-time debugging and historical analysis, this data is invaluable. We wanted all of it in LogHouse.Second, the inefficiency of the OTel pipeline for this specific task became obvious as we scaled.The data journey involved:A customer's ClickHouse instance writes logs as JSON to stdout.The kubelet persists these logs in An OTel collector collects these logs from the disk, parsing and marshalling the JSON into an in-memory representation.The collector transforms these into the OTel log format - again an in-memory representation.Finally, they are inserted back into another ClickHouse instance (LogHouse) over the native format (requiring another transformation within the ClickHouse Go client).Note: The architecture described here is simplified. In reality, our OTel pipeline is more involved. Logs were first collected at the edge in JSON, converted into the OTel format, and sent over OTLP to a set of gateway instances. These gateways (also OTel collectors) performed additional processing before finally converting the data into ClickHouse’s native format for ingestion. Each step introduced overhead, latency, and further complexity.At our scale, this pipeline introduced two critical problems: inefficiency and data loss. First, we were burning substantial compute on repeated data transformations. Native ClickHouse types were being flattened into JSON, mapped into the OTel log format, and then re-ingested - only to be reinterpreted by ClickHouse on the other end. This not only wasted CPU cycles but also degraded the fidelity of the data.
Even more importantly, we were hitting hard resource limits on the collectors themselves. Deployed as agents on each Kubernetes node, they were subject to strict CPU and memory constraints via standard Kubernetes limits. As traffic spiked, many collectors ran so hot they began dropping log lines outright - unable to keep up with the volume emitted by ClickHouse. We were losing data at the edge before it ever had a chance to reach LogHouse.
We found ourselves at a crossroads: either dramatically scale up the resource footprint of our OTel agents (and gateways) or rethink the entire ingestion model. We chose the latter.Note: To put the cost in perspective - handling 20 million rows per second through the OpenTelemetry pipeline without dropping events would require an estimated 8,000 CPU cores across agents and collectors. That’s an enormous footprint dedicated solely to log collection, making it clear that the general-purpose approach was unsustainable at our scale.Our solution was to develop the , or . This is a specialized tool designed to transfer data from one ClickHouse instance to another as efficiently as possible. We wanted to go directly from the system tables in a customer's pod to the tables in LogHouse, preserving native ClickHouse types and eliminating all intermediate conversions. This has the fantastic side benefit that any query our engineers use to troubleshoot a live instance can be trivially adapted to query historical data across our entire fleet in LogHouse, as the table schemas are identical, with the addition of some enrichment columns (such as the Pod Name, ClickHouse version, etc).Firstly we should emphasize that SysEx performs a literal byte-for-byte copy of data from the source to the destination. This preserves full fidelity, eliminates unnecessary CPU overhead, and avoids the pitfalls of repeated marshalling.The architecture is simple and powerful. We run a pool of SysEx scrapers connecting to our customer's ClickHouse instances. A hash ring assigns each customer pod to a specific scraper replica to distribute the load. These scrapers then run SELECT queries against the source pod's system tables and stream the data directly into LogHouse, without any deserialization. The scrapers simply coordinate and forward bytes between the source and destination.
Scraping system tables requires careful handling to ensure no data is missed due to buffer flushes. Fortunately, nearly all system table data is inherently time-series in nature. SysEx leverages this by querying within a sliding time window, deliberately trailing real time by a small buffer - typically five minutes. This delay allows for any internal buffers to flush, ensuring that when a scraper queries a node, all relevant rows for that time window are present and complete. This strategy has proven reliable and meets our internal SLAs for timely and complete event delivery to LogHouse.SysEx is written in Go, like most of our infrastructure components for ClickHouse Cloud. Naturally, this raises a question for anyone familiar with the Go ClickHouse client: how do we avoid the built-in marshalling and unmarshalling of data when reading from and writing to ClickHouse? By default, the client converts data into Go-native types, which would defeat the purpose of a byte-for-byte copy. To solve this, we contributed improvements to the Go client that allow us to bypass internal marshalling entirely, enabling SysEx to stream data in its native format directly from the source cluster to LogHouse - without decoding, re-encoding, or allocating intermediary data structures.This approach is broadly equivalent to a simple bash command:curl -s -u  | curl -s -X POST --data-binary @- An actual go implementation for the curious can be found here.Most importantly, SysEx doesn’t require the heavy buffering that OTel does, thanks to its pull-based model. Because scrapers query data at a steady, controlled rate, we don’t risk dropping logs when LogHouse is temporarily unavailable or when the source experiences a spike in telemetry. Instead, SysEx naturally handles backfill by scraping historical windows, ensuring reliable delivery without overloading the system or requiring complex retry buffers.One of the key challenges with the SysEx approach is that it assumes the source and target schemas match. But in reality, as any ClickHouse user knows, system table schemas change frequently. Engineers continuously add new metrics and columns to support emerging features and accelerate issue diagnosis, which means the schema is a moving target.To handle this, we generate schemas dynamically. When SysEx encounters a system table, it inspects and hashes its schema to determine if a matching table already exists in LogHouse. If it does, the data is inserted there. If not, a new schema version is created for this system table e.g. .At query time, we use ClickHouse’s Merge table engine to unify all schema iterations into a single logical view. This allows us to query across multiple versions of a system table seamlessly. The engine automatically resolves schema differences by selecting only the columns that are compatible across tables, or by restricting the query to tables that contain the requested columns. This gives us forward compatibility as schemas evolve, without sacrificing query simplicity or requiring manual schema management.As we continued to scale and refine our observability capabilities, one of our primary focuses was capturing in-memory system tables, such as . Unlike the time-series data we’ve been capturing, these tables provide a snapshot of the server’s state at a specific point in time. To handle this, we implemented a periodic snapshot process, capturing these in-memory tables and storing them in LogHouse.This approach not only allows us to capture the state of the cluster at any given moment, but also provides time-travel through critical details like table schemas and cluster settings. With this additional data, we are able to enhance our diagnostic capabilities by performing cluster-wide or ClickHouse Cloud-wide analyses. This we can join against service settings or query characteristics like used_functions to pinpoint anomalies, making it easier to identify the root causes of issues as they arise. By correlating queries with particular schemas, we further improved our ability to proactively identify and resolve performance or reliability problems for our customers.One of the many powerful capabilities we've unlocked with SysEx is the ability to take the same Advanced Dashboard queries  that customers use to monitor their individual ClickHouse instances and run them across our entire fleet of customer instances simultaneously.For release analysis, we can now execute proven diagnostic queries before and after deployments to immediately identify behavioral changes across our entire fleet. This has been rolled into our comprehensive release analysis process. Queries that analyze query performance patterns, resource utilization trends, and error rates complete in real time, allowing us to quickly spot regressions or validate improvements at fleet scale.Secondly, our support dashboards can now embed the same deep diagnostic queries that customers rely on, but with enriched context from our centralized telemetry. When investigating customer issues, support engineers can run familiar Advanced Dashboard queries while simultaneously correlating with network logs, Kubernetes events, data and control plane events - all within the same interface.The efficiency gains from this SysEx are staggering. Consider these stats from LogHouse: Use over 800 CPU cores to ship 2 million logs per second.LogHouse Scrapers (SysEx): Use just 70 CPU cores to ship 37 million logs per second.This specialized approach has allowed us to handle a 20x increase in event volume with less than 10 percent of the CPU footprint for our most important data source. Most importantly, it means we no longer drop events at the edge. To achieve this same level of reliability with our previous OTel-based pipeline, we would have needed over 8,000 CPU cores. SysEx delivers it with a fraction of the resources, maintaining full fidelity and consistent delivery.If you’ve read this far, you might be wondering: when is OpenTelemetry still the right choice, and is it still useful?
We firmly believe that it is. While our architecture has evolved to meet challenges at extreme scale, such as parsing and processing over 20 million log lines per second, OpenTelemetry remains a critical part of our stack. It offers a standardized, vendor-neutral format and provides an excellent onboarding experience for new users - and is hence the default choice for ClickStack. Unlike SysEx, which is tightly integrated with ClickHouse internals, OpenTelemetry decouples producers from consumers, which is a major architectural advantage, especially for users who want flexibility across observability platforms.It is also well suited for scenarios where SysEx cannot operate. SysEx is pull-based and relies on querying live system tables, which means the service must be healthy and responsive. If a service is crash-looping or down, SysEx is unable to scrape data because the necessary system tables are unavailable. OpenTelemetry, by contrast, operates in a passive fashion. It captures logs emitted to  and , even when the service is in a failed state. This allows us to collect logs during incidents and perform root cause analysis even if the service never became fully healthy.
For this reason, we continue to run OpenTelemetry across all ClickHouse services. The key difference is in what we collect. Previously, we ingested everything, including trace-level logs. Now, we collect only info-level and above. This significantly reduces the data volume and allows our OTel collectors and gateways to operate with far fewer resources. The result is a smaller, more focused pipeline that still accounts for the 2 million log lines per second referenced earlier.Collecting all this data is just the beginning. Making it usable and accessible is what really matters. In the first iteration of LogHouse, we built a highly customized observability experience on top of Grafana. It served us well, but as our internal data sources grew and diversified, particularly with the introduction of SysEx and wide-column telemetry, it became clear we needed something more deeply integrated with ClickHouse.This challenge was not unique to us. Many teams building observability solutions on ClickHouse have encountered the same issue. Getting data into ClickHouse was straightforward, but building a UI that fully unlocked its value required significant engineering effort. For smaller teams or companies without dedicated frontend resources, ClickHouse-powered observability was often out of reach.HyperDX changed that. It provided a first-party, ClickHouse-native UI that supports log and trace exploration, correlation, and analysis at scale. Its workflows are designed with ClickHouse in mind, optimizing queries and minimizing latency. When we evaluated HyperDX prior to the acquisition, it was already clear that it addressed many of the pain points we and others had experienced. The ability to query using Lucene syntax dramatically simplifies data exploration and is often sufficient. Importantly, it still allows us to query in SQL - something which we still find essential for more complex event analysis - see “SQL for more complex analysis”.A key reason HyperDX was such a compelling fit was the schema-agnostic approach introduced in v2.0. It doesn't require log tables to conform to a single, rigid structure. This flexibility is critical for a system like LogHouse, which ingests data from numerous sources:It seamlessly handles the standardized, yet evolving, data format from our  pipeline.More importantly, it works out-of-the-box with the highly specialized, wide-column tables produced by  and our other custom exporters. It does this with no prior knowledge of the SysEx schemas, or complex  specializations. It simply inspects the schema behind-the-scenes and adapts to work with them.This means our engineering teams can add new data sources with unique, optimal schemas to LogHouse without ever needing to worry about breaking or reconfiguring the user interface. By combining HyperDX's powerful UI and session replay capabilities with LogHouse's massive data repository, we have created a unified and adaptable observability experience for our engineers.It is worth emphasizing that Grafana still has its place in our observability stack. Our internal Grafana-based application has some distinct advantages, particularly in how it handles routing and query scoping. Users are required to specify the namespace (effectively a customer service) they intend to query. Behind the scenes, the application knows exactly where data for each service resides and can route queries directly to the appropriate ClickHouse instance within LogHouse. This minimizes unnecessary query execution across unrelated services and helps keep resource usage efficient.This is especially important in our environment, where we operate LogHouse databases across many regions. As our previous blog post described, efficiently querying across these distributed systems is critical for performance and reliability. We’re currently exploring how we might push this routing logic to ClickHouse itself, allowing HyperDX to benefit from the same optimization..so stay tuned.In addition to its routing capabilities, Grafana remains the home for many of our long-standing dashboards and alerts, particularly those built on Prometheus metrics. These remain valuable, and migrating them is not currently a priority. For example, kube_state_metrics has almost become a de facto standard for cluster health monitoring. These high-level metrics are well suited for alerting, even if they are not ideal for deep investigation. For now, they continue to serve their purpose effectively.For now, the two tools serve complementary purposes and coexist effectively within our observability stack.Store everything, aggregate nothingThe development of SysEx has brought more than just technical gains. It has driven a cultural shift in how we think about observability. By unlocking access to system tables that were previously unavailable, where only standard output logs had been captured, we have embraced a model centered on wide events and high cardinality data.Some refer to this as Observability 2.0. We simply call it LogHouse combined with ClickStack.This approach replaces the traditional three-pillar model with something more powerful: a centralized warehouse that can store high-cardinality telemetry from many sources. Each row contains rich context - query identifiers, pod names, version metadata, network details - without needing to pre-aggregate or discard dimensions to fit within the limits of a metric store.As engineers, we have adapted to this new model, leaving behind outdated concerns about cardinality explosions. Instead of summarizing at ingest time, we store everything as is and push aggregation to query time. This approach allows for in-depth inspection and flexible exploration without sacrificing fidelity.One pattern we have found particularly impactful is logging wide events that include timeseries attributes in place of traditional metrics. For example, here is a log line from SysEx that tracks data pushed from a source ClickHouse instance to the LogHouse cluster:At this point, you may be asking: how is this different from a traditional metrics store like Prometheus?The key difference is that we store . We do not pre-aggregate fields like ; instead, we capture and retain each value and store it together.In contrast, a system like Prometheus typically stores either a gauge per series or, more commonly, pre-aggregates values into histograms to support efficient querying. This design introduces significant limitations. For example, storing time series for all label combinations in Prometheus would lead to a cardinality explosion. In our environment, with tens of thousands of unique pod names, each label combination would require its own timeseries just to preserve query-time flexibility. Pre-aggregating with histograms helps control resource usage but comes at the cost of fidelity. It makes certain questions impossible to answer, such as:"Which exact insert is represented by this spike in insertDuration - down to the specific instance, table, and time window?"With our approach, we avoid these trade-offs entirely. We log each event as a wide row that captures all relevant dimensions and metrics in full. This shifts aggregation and summarization to query time while preserving the ability to drill down into individual events when necessary.This model isn’t entirely new. Systems like Elasticsearch have long encouraged the ingestion of wide events and flexible document structures. The difference is that ClickHouse makes this approach operationally viable at scale. Its columnar design allows us to store high-cardinality, high-volume event data efficiently - without the runaway storage costs or query latency that traditionally limited these kinds of approaches to storing events.The power of this approach is in how we can use that single event to draw many different conclusions by visualising its various characteristics, and we can always jump back to the raw logs from any given point on a chart.First, we can focus on a particular service and see its inserts line by line in series. This is the raw view upon the data.We can visualize the insert lag for all tables for this individual instance trivially…We may go a layer up and visualise the insert lag for all servers in a region, which have lag > desired.And, because Observability is Just another Data Problem, we get to borrow all of the tooling in the data science space for our observability data, so we can visualise our logs in any tool of our choice for which ClickHouse either integrates directly or via a client library. For example, Plotly in a Jupyter notebook; plotly.express  px
 pandas  pd
 clickhouse_connect

client = clickhouse_connect.get_client(
…
)
query = 

df = client.query_df(query)


df[] = pd.to_datetime(df[], unit=)
df[] = pd.to_datetime(df[], unit=)

fig = px.timeline(df, x_start=, x_end=, y=)

fig.update_traces(width=)
fig.update_layout(bargap=)

fig.show()
The plot shows scrape time versus wall time, allowing us to inspect each event for duplication. With Plotly, I could size the width of the rectangles as the exact start/end times. The annotations highlight a window where duplicate scrapes occurred, confirming the presence of overlapping data in that range.This plot illustrates the varying insert duration for some tables collected by the LogHouse Scraper.While I tend to prefer Plotly, we recognize that others may favor more modern visualization libraries. Thanks to ClickHouse's broad integration support, our SREs can choose the best tools for their workflows. Whether it’s Hex, Bokeh, Evidence, or any other platform that supports SQL-driven analysis, they are free to work with the approach that suits them best.Here, we saw five views of the same event - demonstrating the flexibility we have to choose how we render at query time, using different charting tools, always with the ability to drill down into the raw line-by-line events.HyperDX offers a robust event search interface utilizing Lucene syntax, ideal for quick lookups and filtering. However, to answer more complex observability questions, a more expressive query language is needed. With ClickHouse as the engine behind LogHouse, we can always drop into full SQLSQL allows us to express joins, time-based operations, and transformations that would be difficult or impossible to perform in typical log query tools. One example is identifying pod termination times by correlating Kubernetes event streams. The query below uses ASOF JOIN to align Killing and Created events for the same container, calculating the time between termination and restart:
    KE 
    (
         loghouse.kube_events
         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )
    ),
    CE 
    (
         loghouse.kube_events
         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )
    )

    Name,
    KE.FirstTimestamp  killTime,
    CE.FirstTimestamp  createTime,
    createTime  killTime  delta,
    formatReadableTimeDelta(createTime  killTime)  readableDelta
 KE
ASOF  CE  (CE.Name  KE.Name)  (CE.FirstTimestamp  KE.FirstTimestamp)
 createTime  delta 
LIMIT ┌─Name─────────────────────────────┬────────────killTime─┬──────────createTime─┬─delta─┬─readableDelta─────────────────────┐
│ c-emerald-tu-48-server-p0jw87g-0 │ 2025-03-10 19:01:39 │ 2025-03-10 20:15:59 │  4460 │ 1 hour, 14 minutes and 20 seconds │
│ c-azure-wb-13-server-648r93g-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:50 │  3507 │ 58 minutes and 27 seconds         │
│ c-azure-wb-13-server-3mjrr1g-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:47 │  3504 │ 58 minutes and 24 seconds         │
│ c-azure-wb-13-server-v31soea-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:46 │  3503 │ 58 minutes and 23 seconds         │
└──────────────────────────────────┴─────────────────────┴─────────────────────┴───────┴───────────────────────────────────┘

4 rows in set. Elapsed: 0.099 sec. Processed 17.78 million rows, 581.49 MB (180.05 million rows/s., 5.89 GB/s.)
Peak memory usage: 272.88 MiB.
Sure, we could write a component to track this as a metric, but the power of ClickHouse is that we don’t need to do so. It’s sufficient to store a warehouse of wide events and derive the metric we need at query time from them. So, when a colleague asks, ‘what’s the p95 replacement time for Pods after termination is requested’, we can just find a relevant set of events instead of responding, 'let me ship a new metric ', and getting back to them with an answer after the next release goes out.Sold on the immense value of having deep, structured telemetry in a high-performance analytics engine, we've been busy adding more data sinks to LogHouse, mainly at the request of our engineering and support team, who love using LogHouse and want all critical data to live in the warehouse. This year, we've embraced a cultural shift towards high-cardinality, wide-event-based observability as shown above.Some of our new data sources, which adhere to this wide event philosophy, include: Our open-source tool for monitoring Kubernetes networking, giving us deep insights into cluster traffic.  uses Linux's conntrack system to capture L3/L4 connection data with byte/packet counts. This provides three key capabilities: forensics (time-series connection records with per-minute bandwidth), attribution (mapping connections to specific workloads and pods), and metering (cost tracking for expensive data transfer like cross-region egress). The system processes millions of connection observations per minute, helping us identify costly cross-regional downloads, track cross-AZ traffic patterns, and correlate network usage with actual costs. You can find the project at https://github.com/ClickHouse/kubenetmon.Kubernetes Event Exporter: We forked the popular exporter and added a native ClickHouse sink, allowing us to analyze Kubernetes API events at scale. You can find our fork here. This is hugely useful for understanding why things changed in K8s over time. We’re not stopping there, however! We’re already working on a plan to ingest not just the events, but the entire k8s object model into LogHouse, with snapshots at every change. This would allow us to model the full state of all clusters at any moment in time over the past six months, and step through all of the changes. Instead of just knowing "Pod X was terminated at 15," we’ll see the full cluster state before and after, understand dependencies, resource constraints, and the cascading effects of changes. We collect all operational data from our Control Plane department, who had not yet onboarded into LogHouse.Real User Monitoring (RUM): In a project that is still a work in progress, we collect frontend performance metrics from our users' browsers, which are pushed via a public gateway into our OTel pipeline. We ingest HTTP-level traffic data from our Istio service mesh, capturing request/response patterns, latencies, and routing decisions. Combined with ClickHouse's system.query_log and kubenetmon's network flows, this creates a powerful tri-dimensional correlation capability. When network usage spikes occur, our support team can trace the complete story: which specific SQL queries were executing, what HTTP requests triggered them, and the exact packet flow patterns. This cross-layer visibility transforms debugging from guesswork into precise root cause analysis - if we see unusual egress traffic, we can immediately identify whether it's from expensive cross-region queries, backup operations, or unexpected replication, making troubleshooting incredibly efficient for the support team.It’s been an incredible year of growth for LogHouse. By moving beyond a one-size-fits-all approach and embracing specialized, highly efficient tooling, we’ve scaled our observability platform to remarkable new heights while significantly enhancing our cost performance. Integrating HyperDX is a key part of that evolution, providing a flexible and powerful user experience on top of our petabyte-scale data warehouse. We're excited to see what the next year brings as we continue to build on this strong foundation.While SysEx is designed to be efficient and resource-conscious, customers occasionally notice our scrape queries in their logs and metrics. These queries are tightly constrained with strict memory limits, but when they error (as they sometimes do) it can create concern. Although the actual resource impact is minimal, we recognize that even lightweight queries can create noise or confusion in sensitive environments.To address this, we’re exploring what we call  - the next evolution of SysEx. The goal is to eliminate all in-cluster query execution by entirely decoupling scraping from the live system. One promising direction involves leveraging , where ClickHouse already writes its service logs. In this model, a pool of SysEx workers would mount these disk-based log tables directly, bypassing the need to query the running ClickHouse instance. This design would deliver all the benefits of our current system - native format, high fidelity, minimal transformation - while removing even the perception of operational impact.OpenTelemetry remains a critical component of our platform, particularly for early-stage data capture before service tables are available. This is especially useful during crash loops, where structured logs may be unavailable. However, if our zero-impact scraping approach proves successful, it could reduce our reliance on OTel even further by providing a high-fidelity, low-disruption path for log ingestion throughout the lifecycle of a cluster.This effort is still in progress, and we’ll share more once we’ve validated the approach in production.The JSON type has been available in ClickHouse for some time and recently reached GA in version 25.3. It offers a flexible and efficient way to store semi-structured data, dynamically creating columns with appropriate types as new fields appear. It even supports fields with multiple types and gracefully handles schema explosion.Despite these advantages, we’re still evaluating how well JSON fits common observability access patterns at scale. For example, querying a string across an entire JSON blob can effectively involve scanning thousands of columns. There are workarounds - such as also storing a raw string version of the JSON alongside the structured data - but we’re still developing best practices in this area.Culturally, we have also come to recognize the practical limits of the Map type, which has served us well. Most of our log and resource attributes are small and stable enough that the Map continues to be the right fit. We have found that single-level JSON logs are often all you need, and for exceptions, tools like HyperDX automatically translate map access into JSONExtract functions. While we plan to adopt JSON more broadly, this is still a work in progress. Expect us to share more in a future update.Over the past year, LogHouse has evolved from an ambitious logging system into a foundational observability platform powering everything from performance analysis to real-time debugging across ClickHouse Cloud. What began as a cost-saving measure has become a catalyst for both cultural and technical transformation, shifting us toward high-fidelity, wide-event telemetry at massive scale. By combining specialized tools like SysEx with general-purpose frameworks like OpenTelemetry, and layering on flexible interfaces like HyperDX, we have built a system that not only keeps up with our growth but also unlocks entirely new workflows. The journey is far from over, but the lessons from scaling to 100PB and 500 trillion rows continue to shape how we think about observability as a core data problem we are solving at warehouse scale.]]></content:encoded></item><item><title>Show HN: We moved from AWS to Hetzner, saved 90%, kept ISO 27001 with Ansible</title><link>https://medium.com/@accounts_73078/goodbye-aws-how-we-kept-iso-27001-slashed-costs-by-90-914ccb4b89fc</link><author>sksjvsla</author><category>dev</category><category>hn</category><pubDate>Sat, 21 Jun 2025 09:02:29 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[The European CTO’s Dilemma: Keeping Compliance outside AWSEarlier this year, I faced a dilemma many tech leaders know well. Our entire infrastructure was built on AWS. We loved their powerful, ISO 27001-certified services. Yet, two critical issues kept me up at night:The Compliance Black Hole: It was clear that American cloud providers couldn’t fully shield us from US government jurisdiction. Under the CLOUD Act and FISA, our European customer data was potentially exposed, regardless of the server’s physical location. This undermined our GDPR promises.The $2,000/Month Question: While not a fortune for every company, our $24,000 annual bill felt disproportionate to our actual needs. I asked myself: how often does a well-maintained Linux server actually crash? Isn’t RDS just a managed Postgres instance with scripts I could write myself? That $2,000 a month could buy a phenomenal amount of resilient, dedicated hardware in Europe.This wasn’t just about cost or compliance; it was a strategic risk. Was tying our company’s future to a single US-based provider a responsible choice?We are a Danish workforce management company doing employee scheduling. Beyond our ISO 27001 certificate, we have a few legal requirements on our operation as well as we perform overtime compensation salary adjustments and are source of truth for time-and-attendance data. Maintaining the tech side of this, is just like maintaining a bank software: Things must be accounted for, always add up and never be lost.Born and raised in AWS, many aspects of our legal requirement was architected as AWS native workflows and migrating that to independent alternatives always had to go along with legal requirements.Let’s be honest: leaving AWS feels like walking away from a fortress of convenience. You lose the “magic” of deeply integrated services like Lambda, one-click RDS deployments, and the rich ecosystem of built-in compliance tooling that makes ISO 27001 audits smoother.Giving this up is the primary source of fear and inaction for most teams. It means trading the comfort of managed services for a higher degree of control and responsibility.By migrating to European providers like Hetzner and OVHcloud, the gains weren’t just theoretical. They were immediate and strategic. Hosting on European-owned infrastructure gave us undeniable proof of data residency — a game-changer for GDPR audits and ISO 27001 recertification. We could tell our customers exactly where their data was, with no ambiguity. Our cloud costs dropped by . This wasn’t a typo. By replacing expensive managed services with our own automated, self-hosted solutions, our budget became predictable and transparent. The biggest surprise was how losing AWS’s pre-built tools forced us to get better. We built a powerful infrastructure-as-code setup using Ansible that gave us even tighter security controls and auditability than before.The Blueprint: Key Lessons for Your Own MigrationThis migration taught us invaluable lessons that can serve as a blueprint for others. Here’s the core of our strategy:Ansible as Your Compliance Engine: Forget simple compliance checks. With properly structured Ansible playbooks, you can tie every line of your server configuration directly to a specific ISO 27001 Annex A control. Your infrastructure code becomes a self-documenting audit trail.Monitoring That Rivals AWS: You don’t need CloudWatch to have enterprise-grade monitoring. A combination of Prometheus, Grafana, and Loki allowed us to replicate — and in some ways exceed — the visibility we had on AWS, ensuring faster incident response.Security-by-Design Becomes Reality: When there isn’t a pre-made security solution to click on, you build it into the foundation. This “security-by-design” approach, automated with Ansible, makes your ISMS (Information Security Management System) incredibly robust and easy for developers to follow.This wasn’t just a technical project; it was a business transformation.We minimized our compliance risk regarding US surveillance laws.We used our European hosting as a sales tool, strengthening brand trust.We returned 90% of our cloud spend to the businessIf this story resonates with you, you’re likely asking: “Could we actually do this? What would it cost? What are the hidden risks?”Our journey created a repeatable playbook for migrating from AWS to a sovereign, cost-effective European cloud while maintaining ISO 27001 certification. I offer  for CTOs and founders facing this exact challenge.In a one-hour session, we can map out:A high-level cost analysis of your current AWS setup vs. a European alternative.The key compliance and ISO 27001 risks in your specific situation.A realistic timeline and the first 3 steps of a potential migration plan.Interested in exploring this for your company?Connect with me on LinkedIn and mention this article, or for a faster response, book a preliminary chat directly on my Calendly.]]></content:encoded></item><item><title>Cosmoe: BeOS Class Library on Top of Wayland</title><link>https://cosmoe.org/index.html</link><author>Bogdanp</author><category>hn</category><pubDate>Sat, 21 Jun 2025 09:00:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Cosmoe needs 2 main improvements to be a reliable, full-featured UI library:While I've made incredible strides with getting the BeOS class libraries to talk to Wayland, much work still remains to weed out crashes and incorrect behavior.  Wayland is powerful, but not friendly.Cosmoe implements about 95% of the BeOS API currently.  Notable feature that are not yet implemented include "offscreen" BBitmaps for accelerated drawing, and BFilePanel which implements Open and Save dialog boxes.  Some file-related classes like BVolume are only partially implemented.  Also, for security reasons, Wayland forbids certain Window-related actions such as window positioning and centering, so that functionality can never exist.For more details on this items and other in-progress aspects of Cosmoe, please see the TODO file in the Cosmoe repo.
				]]></content:encoded></item><item><title>Infocon: green</title><link>https://isc.sans.edu/diary.html?rss</link><author></author><category>infosec</category><pubDate>Sat, 21 Jun 2025 08:20:04 +0000</pubDate><source url="https://isc.sans.edu/">Sans Edu Diaries</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&apos;Gwada negative&apos;: French scientists find new blood type in woman</title><link>https://www.lemonde.fr/en/science/article/2025/06/21/gwada-negative-french-scientists-find-new-blood-type-in-woman_6742577_10.html</link><author>spidersouris</author><category>hn</category><pubDate>Sat, 21 Jun 2025 07:38:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A French woman from the Caribbean island of Guadeloupe has been identified as the only known carrier of a new blood type, dubbed "Gwada negative," France's blood supply agency has announced. "The EFS has just discovered the 48 blood group system in the world!" the agency said in a statement on the social network LinkedIn. "This discovery was officially recognised in early June in Milan by the International Society of Blood Transfusion (ISBT)."The announcement was made 15 years after researchers received a blood sample from a patient who was undergoing routine tests ahead of surgery, the French Blood Establishment (EFS) said on Friday. The scientific association had until now recognized 47 blood group systems. The discovery was first reported by radio France Inter.Thierry Peyrard, a medical biologist at the EFS involved in the discovery, told AFP that a "very unusual" antibody was first found in the patient in 2011. However, resources at the time did not allow for further research, he added. Scientists were finally able to unravel the mystery in 2019 thanks to "high-throughput DNA sequencing", which highlighted a genetic mutation, Peyrard said.The patient, who was 54 at the time and lived in Paris, was undergoing routine tests before surgery when the unknown antibody was detected, Peyrard said. This woman "is undoubtedly the only known case in the world," said the expert. "She is the only person in the world who is compatible with herself," he said. Peyrard said the woman inherited the blood type from her father and mother, who each had the mutated gene.The name "Gwada negative", which refers to the patient's origins and "sounds good in all languages," has been popular with the experts, said Peyrard. He and colleagues are now hoping to find other people with the same blood group. "Discovering new blood groups means offering patients with rare blood types a better level of care," the EFS said.The ABO blood group system was first discovered in the early 1900s. Thanks to DNA sequencing the discovery of new blood groups has accelerated in recent years.]]></content:encoded></item><item><title>Macron Says Europe Must Become &apos;Space Power&apos; Again</title><link>https://science.slashdot.org/story/25/06/21/0242226/macron-says-europe-must-become-space-power-again?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[French President Emmanuel Macron urged Europe to reassert itself as a global space power, warning that France risks being sidelined in the low Earth orbit satellite market dominated by players like SpaceX and China. Phys.Org reports: Macron spoke at the Paris Air Show in Le Bourget outside the French capital a day after France more than doubled its stake in satellite operator Eutelsat, the EU rival to Elon Musk's Starlink. Macron called for more investment as the European space industry struggles to remain competitive in the face of US and Chinese rivals. "SpaceX has disrupted the market, Amazon is also getting involved. China is not far behind, and I think we all need to be very clear-headed," Macron said. Europe must become "a space power once again, with France at its heart," he said. He warned that Europeans were "on the verge of being completely" squeezed out of the low Earth orbit (LEO) satellite constellation market.
 
Macron said France and its partners should not be reliant on non-European constellations in low orbit, calling it "madness." He called non-European players to team up with France. "This must be the solution for our major strategic partners in the Gulf, India, Canada and Brazil," he said. "We really need to succeed in increasing our collective investment effort," Macron added, noting the importance of private investors and public-private collaboration. He also said France planned to organize a space summit in early 2026 to "mobilize our public and private partners across the globe."]]></content:encoded></item><item><title>Delta Chat is a decentralized and secure messenger app</title><link>https://delta.chat/en/</link><author>Bluestein</author><category>hn</category><pubDate>Sat, 21 Jun 2025 06:29:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[💬 Reliable instant messaging with multi-profile and multi-device supportAvailable on mobile and desktop.]]></content:encoded></item><item><title>Sega mistakenly reveals sales numbers of popular games</title><link>https://www.gematsu.com/2025/06/sega-mistakenly-reveals-sales-numbers-for-like-a-dragon-infinite-wealth-persona-3-reload-shin-megami-tensei-v-and-more</link><author>kelt</author><category>hn</category><pubDate>Sat, 21 Jun 2025 06:23:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The TechBeat: This Skill Gap Is Blocking Your Career (And It Has Nothing to Do with AI) (6/21/2025)</title><link>https://hackernoon.com/6-21-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 21 Jun 2025 06:11:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @dexrank [ 12 Min read ] 
 Bitcoin, combined with the Lightning Network, will become the foundational infrastructure for the future of decentralized AI-powered financial services. Read More.By @olenamostepan [ 5 Min read ] 
 Discover how 404 error pages evolved from technical glitches to powerful UX and branding tools that guide, engage, and delight users. Read More.By @wassimchegham [ 4 Min read ] 
 Coordinate multiple AI agents and MCP servers (written in Java, .NET, Python and TypeScript) with LlamaIndex.TS and Azure AI Foundry. Read More.By @ariophil [ 4 Min read ] 
 The web is filled with broken links and broken dreams… AR.IO has built ArNS to put an end to this misery Read More.By @drewchapin [ 3 Min read ] 
 New MCP tools enable programmatic SEO that’s actually useful, combining context and your knowledge base to create scalable, high-quality content. Read More.By @hackernooncontests [ 2 Min read ] 
 Write about how GetBlock simplifies full node hosting for a chance to win from $5,000 in the Web3 Writing Contest. Use this template to start. Read More.By @jackborie [ 4 Min read ] 
 GPS powers everything from farming to finance—but it's shockingly fragile. Discover why losing it could cripple society—and what we must do to prepare.  Read More.By @terminal [ 5 Min read ] 
 Learn how to install and use Hydra in Termux for efficient password cracking and security testing on your Android device.  Read More.By @sannis [ 2 Min read ] 
 Learn how to choose the right numeric data types in MySQL, from integers and decimals to floating-point and bit fields. This guide covers common pitfalls, best  Read More.By @turingcom [ 3 Min read ] 
 AI-driven hiring is booming across industries. Here's how engineers can upskill, adapt, and stay ahead in the era of genAI and machine learning. Read More.By @udacity [ 2 Min read ] 
 Weak interview skills are holding back 8 in 10 tech workers. Read More.By @OlgaTitova [ 7 Min read ] 
 Latest research reveals AI companions can reduce loneliness and build social skills—but only with ethical design. A guide for developers and users.  Read More.By @badmonster0 [ 5 Min read ] 
 Automatically infer and manage Qdrant schemas with CocoIndex's declarative dataflow. No manual setup needed. Read More.By @mediabias [ 6 Min read ] 
 Systematic review shows how bias and poor methodology limit ML models used to detect depression through social media posts. Read More.By @vladyslav_chekryzhov [ 25 Min read ] 
 Build production-ready LLM agents. Learn 15 principles for stability, control, and real-world reliability beyond fragile scripts and hacks. Read More.By @textgeneration [ 3 Min read ] 
 This section highlights vAttention's ability to add dynamic memory allocation support to unmodified FlashAttention and FlashInfer prefill kernels Read More.By @largemodels [ 2 Min read ] 
 This table evaluates the impact of multi-token prediction on Llama 2 fine-tuning, suggesting that it does not significantly improve performance on various tasks Read More.]]></content:encoded></item><item><title>Signal – An Ethical Replacement for WhatsApp</title><link>https://greenstarsproject.org/2025/06/15/signal-an-ethical-replacement-for-whatsapp/</link><author>miles</author><category>hn</category><pubDate>Sat, 21 Jun 2025 05:21:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[You may not need any incentive to do this, but I’m going to argue that moving from WhatsApp to Signal is an important action, right now. There are other alternatives to WhatsApp, of course, but several have received criticism over various issues so I’m going to focus on Signal here. Please comment below if you’d like to propose ethical alternatives to WhatsApp.  In March I argued that we should choose Bluesky over X/Twitter, based on the obvious reason of boycotting companies connected to Musk. Bluesky is structured as a benefit corporation (like Patagonia) and based on an open network, two features that create strong incentives for the company to not upset its users.This creates an alignment between Bluesky and its users – Bluesky is disincentivized to make a bad user experience. This also acts as a kind of poison pill against a billionaire takeover, or at least a deterrent against this.Moving from X/Twitter to Bluesky does require some sacrifice for people who worked hard to build up a following on the former. But we’ve reached a point where support of Musk is unconscionable, whatever the sacrifice involved. The move from WhatsApp to Signal, however, is quite straightforward and completely painless. But in this case, some may be wondering, why should I bother moving at all?Ethical issues with WhatsAppMy biggest issues with using WhatsApp are related to its parent corporation, Meta (formerly Facebook) and its founder/CEO Mark Zuckerberg. But first, I want to mention a few issues specific to WhatsApp.In early 2021, WhatsApp updated its terms of service, requiring users to opt into sharing their data with Facebook – including network details and location (even if you haven’t turned on location sharing). This 180° reversal on an earlier promise made by Zuckerberg that “WhatsApp is going to operate completely autonomously” was one of several events over the last 5 years that accelerated an exodus from WhatsApp to Signal. The large amount of metadata that WhatsApp collects is also shared with law enforcement agencies.WhatsApp shares metadata, unencrypted records that can reveal a lot about a user’s activity, with law enforcement agencies such as the Department of Justice. Some rivals, such as Signal, intentionally gather much less metadata to avoid incursions on its users’ privacy, and thus share far less with law enforcement. – ProPublicaI have to admit that I don’t give a huge amount of time fretting over these things during normal times. But we no longer live in normal times – people are now guilty by suspicion (or by association, based on their WhatsApp contacts) and convicted without due process.Over the years, Zuckerberg and Meta have done more flip flopping than a White Lotus guest (walking around in flip-flops, you see). Facebook has since been fined €110 million by EU antitrust regulators and $5 billion by the US Federal Trade Commission for deceiving regulators and users. Following this greater level of scrutiny, Zuckerberg has shown himself to be a person of little scruples, capitulating to whoever will do him favors.Meta – a company so bad that it had to change its name to try to clean its reputation. The Facebook–Cambridge Analytica scandal involved the collection of user data without consent to create psychographic profiles that were used in Donald Trump’s 2016 election campaign and suppressing Black voters in Trinidad and Tobago. WhatsApp, in particular, played a role in the Brazil’s 2018 election.The vast majority of false information shared on WhatsApp in Brazil during the presidential election favoured the far-right winner, Jair Bolsonaro. The analysis sheds light on the spread of misinformation on the Facebook-owned app, with fears it could be poisoning political debate in one of the largest democracies in the world. – The Guardian.Companies supporting Jair Bolsonaro are buying a service called “mass blasts,” using the candidate’s list of WhatsApp users or buying lists from agencies specializing in digital strategy. – Folha de S.Paulo.In 2018, Zuckerberg testified before US Congress, admitting: “It was my mistake, and I’m sorry.”Aww! You’re totally forgiven, Zuck! You’ve learned your lesson, right?But wait! Last year, as you probably know, Zuckerberg did the cowardly act of endorsing Trump by implication. I happened to catch this interview with the Zuck live on Bloomberg and was totally gagged (Hey Gen Z readers!). Take a look and marvel at Zuck sticking to his guns and maintaining his neutrality:Post-election, Zuck flew down to Florida for dinner with Trump (I’m imagining scenes from the hunting episode of ) and then got rid of DEI and moderation and all that weak stuff. It didn’t take long for Zuck to switch from humility (It was my mistake, and I’m sorry) to going on the Joe Rogan show to say this:“Masculine energy is good, and obviously, society has plenty of that, but I think corporate culture was really trying to get away from it,” Zuckerberg continued. “I think having a culture that celebrates the aggression a bit more has its own merits that are really positive.” – Mashable. Signal provides a very similar user experience to WhatsApp, so switching over is pretty seamless. This is not too surprising, considering that the current CEO of Signal LLC., Brian Action, was one of the cofounders of WhatsApp. WhatsApp was acquired by Facebook in 2014, and Acton left the company three years later due to differences around the use of customer data and targeted advertising.A year later, Acton contributed $50 million to launch a non-profit, the Signal Foundation and a subsidiary, Signal Messenger, LLC.On 21 February 2018, Moxie Marlinspike and WhatsApp co-founder Brian Acton announced the formation of the Signal Technology Foundation, a 501(c)(3) nonprofit organization whose mission is “to support, accelerate, and broaden Signal’s mission of making private communication accessible and ubiquitous”. – WikipediaWhen the Electronic Frontier Foundation ranked messaging apps for privacy and transparency, Signal was one of the few that received a perfect score. The nonprofit also has an updated guide to using Signal, navigating its settings, etc.The protection of data and personal privacy is important. The nonprofit Signal Foundation is led by Meredith Whittaker, a former director of the AI Now Institute at NYU, which examined the social impacts of AI and concentration of power in tech. She is a strong proponent of protecting privacy as a human right and an opponent of surveillance capitalism. Signal’s predecessor, Open Whisper Systems received funding from journalism nonprofit Freedom of the Press Foundation.Signal has been recommended to Democratic Party staffers and officially approved by the US Senate in 2017. Just not for texting top secret plans, obviously!Action plan for migrating from WhatsAppIt’s easy to see how we all get drawn into using specific apps like WhatsApp – the user base reaches a critical size and it becomes the app to find everyone on. At the same time, the company becomes more lucrative (and in many cases is taken over by a bigger fish) and then ethical issues creep in. We learn to suppress our concerns because the app is just too convenient and, to quote The Cranberries, Everybody Else Is Doing It, So Why Can’t We? (R.I.P., Dolores).The exit plan from WhatsApp is quite simple. Start by installing Signal and setting it up – it takes only a couple of minutes. Then, resume any WhatsApp conversations on Signal if that person is already a Signal user. If they are not, then switch to regular text messaging and gently suggest to that person to switch over to Signal. [Shout out to CeCe for reminding me to install Signal!] Group chats are a good way to get people to switch over as nobody will want to be the person who can’t be bothered making the switch.The interference with elections is not OK.I’ll leave the last word to future president, AOC:Meta as in ‘we are a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil society… for profit!'” – Alexandria Ocasio-Cortez]]></content:encoded></item><item><title>SoftBank&apos;s Son Pitches $1 Trillion Arizona AI Hub</title><link>https://news.slashdot.org/story/25/06/20/2212217/softbanks-son-pitches-1-trillion-arizona-ai-hub?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Reuters: SoftBank Group founder Masayoshi Son is envisaging setting up a $1 trillion industrial complex in Arizona that will build robots and artificial intelligence, Bloomberg News reported on Friday, citing people familiar with the matter. Son is seeking to team up with Taiwan Semiconductor Manufacturing Co for the project, which is aimed at bringing back high-end tech manufacturing to the U.S. and to create a version of China's vast manufacturing hub of Shenzhen, the report said.
 
SoftBank officials have spoken with U.S. federal and state government officials to discuss possible tax breaks for companies building factories or otherwise investing in the industrial park, including talks with U.S. Secretary of Commerce Howard Lutnick, the report said. SoftBank is keen to have TSMC involved in the project, codenamed Project Crystal Land, but it is not clear in what capacity, the report said. It is also not clear the Taiwanese company would be interested, it said. TSMC is already building chipmaking factories in the U.S. with a planned investment of $165 billion. Son is also sounding out interest among tech companies including Samsung Electronics, the report said.
 
The plans are preliminary and feasibility depends on support from the Trump administration and state officials, it said.
A commitment of $1 trillion would be double that of the $500 billion "Stargate" project which seeks to build out data centre capacity across the U.S., with funding from SoftBank, OpenAI and Oracle.]]></content:encoded></item><item><title>Samsung embeds IronSource spyware app on phones across WANA</title><link>https://smex.org/open-letter-to-samsung-end-forced-israeli-app-installations-in-the-wana-region/</link><author>the-anarchist</author><category>hn</category><pubDate>Sat, 21 Jun 2025 03:06:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In recent months, we have received numerous reports from users across West Asia and North Africa (WANA) expressing alarm over a little-known but deeply intrusive bloatware application—AppCloud—pre-installed on Samsung’s A and M series smartphones. Without users’ knowledge or consent, this bloatware collects sensitive personal data, cannot be removed without compromising device security, and offers no clear information about its privacy practices.AppCloud, developed by the controversial Israeli-founded company ironSource (now owned by the American company Unity), is embedded into devices sold in countries where such affiliations carry legal implications. Despite the serious privacy and security risks, Samsung has offered no transparency on how AppCloud functions, what data it collects, or why users cannot opt out.This open letter, addressed to Samsung, calls for immediate transparency, accountability, and dialogue. Users deserve to know what is installed on their devices and how their data is being used, especially amid Israel’s espionage campaigns in the region. We are writing to urgently request that Samsung be transparent regarding the pre-installation of AppCloud on its A and M series smartphones, particularly in West Asia and North Africa (WANA). We ask that Samsung provide information about AppCloud’s privacy practices, opt-out and removal options, and that Samsung reconsider future pre-installations in light of privacy rights. We also request a meeting with Samsung teams to discuss these concerns further. According to our analysis, this intrusive software is , deeply integrated into the devices’ operating system, making it nearly impossible for regular users to uninstall it without root access, which voids warranties and poses security risks. Even disabling the bloatware is not effective as it can reappear after system updates. The privacy policy is there is no accessible and transparent privacy policy for this bloatware and users are in the dark about what data is collected and how it is used. There is also no straightforward opt-out mechanism. The bloatware collects sensitive user data, including biometric information, IP addresses, device fingerprints. The installation of AppCloud is done from the user, which violates GDPR provisions in the EU and relevant data protection laws in the WANA region states. AppCloud is developed by ironSource, an Israel-founded company (now acquired by American company Unity), raising additional legal and ethical concerns in countries where Israeli companies are barred from operating, such as Lebanon. ironSource is notorious for its questionable practices regarding user consent and data privacy. Samsung’s terms of service mention third party applications but do not specifically address AppCloud or ironSource, despite the significant data access and control granted to this bloatware app. The forced installation of AppCloud undermines the privacy and security rights of users in the MENA region and beyond. The lack of transparency and control over personal data is particularly alarming given Samsung’s significant market share in the region.In light of these concerns, we respectfully request that Samsung:Disclose the full privacy policy and data handling practices of AppCloud, making this information easily accessible to all users.Offer a straightforward and effective method for users to opt out of AppCloud and remove it from their devices without compromising device functionality or warranty.Provide a clear explanation for the decision to pre-install AppCloud on all A and M series devices in the WANA region.Reconsider the continued pre-installation of AppCloud on future devices, in line with the right to privacy as established by Article 12 of the Universal Declaration of Human Rights.We also request a meeting with the relevant Samsung teams to discuss these issues in detail and to better understand the company’s approach to user privacy and data protection in the WANA region.We look forward to your prompt response and to working together to ensure the privacy and security of all Samsung users.]]></content:encoded></item><item><title>Buc-ee’s Sues Parody Apparel Shop For Parodying Its Brand</title><link>https://www.techdirt.com/2025/06/20/buc-ees-sues-parody-apparel-shop-for-parodying-its-brand/</link><author>Dark Helmet</author><category>tech</category><pubDate>Sat, 21 Jun 2025 02:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[A couple of weeks back, we discussed famed southern convenience store chain Buc-ee’s and its penchant for initiating all kinds of trademark related threats and lawsuits. While we talk about this sort of thing a lot around here, the company’s actions have been particularly silly. When taken in sum total, you’re left with the idea that Buc-ee’s seems to think that it is the only company involved in the food and/or drinks business that is allowed to have a cartoon animal as its logo. Think I’m exaggerating? The company argued that its beaver logo looked too similar to that of an alligator. And a chicken. And a guy eating a hotdog.Well, the company is still at it, but at least it’s a tad bit closer to trademark reality in this instance. That said, its latest lawsuit is still likely to run into a significant challenge, after it went after an apparel store that specifically sells clothes that parody brands.The Texas-based Buc-ee’s filed the suit against Born United.Buc-ee’s operates a chain of travel centers and convenience stores across nine states, including South Carolina. A “significant and growing portion” of the company’s business involves making, distributing and selling clothing prominently featuring the Buc-ee’s trademarks, the lawsuit, filed last Tuesday, states.Born United sells clothing and other merchandise bearing patriotic themes and slogans and operates under the slogan, “Bringing brands together that stand for freedom,” the suit alleges. Court documents state it offers its own private label products as well as merchandise from third-party brands like Grunt Style, Palmetto State Armory, Nine Line Apparel, and others.And here is an example of one of the parody images in question.In the MSN post, the owner of Born United is quoted saying that they love Buc-ee’s and would be willing to discuss their concerns. That flies in the face of the store’s failing to respond to a C&D Buc-ee’s sent, as well as comments from a minority owner named Tom Fernandez, who also happens to be a state senator in South Carolina.Now, nobody is attempting to claim that Born United  use a large portion of the Buc-ee’s logo and branding in its shirts, of course. Instead, the store used a portion of that branding, added to it to make a parody that aligned with the store’s values, and then sold them in its own storefront. That reads like fairly clear parody to this writer, but it is also undeniably the case that this sort of use is unlikely to confuse anyone into thinking that Buc-ee’s has somehow gotten into the business of creating a gun-toting version of its beaver in military garb. Combine that with the Born United name being prominently displayed and any such concern gets even more silly.It sounds like Born United is prepared to fight this out. Having a sitting state senator on your side probably doesn’t hurt either. Perhaps the beaver finally bit off more than it can chew.]]></content:encoded></item><item><title>Tiny Undervalued Hardware Companions (2024)</title><link>https://vermaden.wordpress.com/2024/03/21/tiny-undervalued-hardware-companions/</link><author>zdw</author><category>hn</category><pubDate>Sat, 21 Jun 2025 02:19:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After playing/working with computers for more then 25 years I started to appreciate small but handy valuable stuff – like adapters or handlers or … yeah – all kind of stuff. With many of them I did not even knew they existed until I find out about them – mostly accidentally or after long searching for some problem solution. Today I will share them with You – so maybe they will end up handy also for You.… and while they make my life easier – they are mostly very cheap too.The  is below.RJ45 Angle Cable AdaptersSATA to USB-C or USB-A AdaptersAngle USB-C and USB-A AdaptersTiny USB WiFi or Bluetooth DongleUSB-C <=> Micro USB AdapterUSB-C <=> Laptops/Routers/5.5mmx2.5mm AdaptersCreative BT-W2 USB-A Bluetooth AdapterExternal Microphone for SONY HeadphonesDual USB-C and USB-A Pendrive (SanDisk)Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card SlotC13/C14 Power Adapters with Additional C1/C2 or C5/C6 SocketsHDMI 3in1 Switch with Remote ControlThe whole article can ‘feel’ like a sponsored entry for the https://aliexpress.com portal – but it is not – its just the most cheap place I was able to find these gems. Feel free to share even cheaper one if You have one.I mostly use laptops to do various tasks and cables sticking out on the sides perpendicularly does not help. Not many laptops today have the RJ45 LAN socket – but if they do – they are mostly on the side of the laptop.Thanks to such angle RJ45 adapters it is no longer a problem.You can find them for about  – for example – on https://aliexpress.com page – with the  keywords in their search.RJ45 Angle Cable AdaptersThe mentioned earlier  are quite bulky – but as an alternative its possible to get a short 40cm cable with smaller plug.Not sure if its noticeable on the picture below – but I also cut the top ‘cover’ with knife of the plug – so its easier to detach.There are of course all four angles to choose from.One may also use the end of that 40cm cable-adapter as a ‘stopper’ to not fall inside the desk hole as shown on the image below.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.Often I found myself in a situation that the currently available LAN cable was too short to reach and it needed a lot of work to plot another – longer one.With these simple ‘join’ adapters it is no longer a problem. You would not use them in a serious Data Center with 10+ GE speeds – but for home 1.0-2.5 GE speeds its more then enough.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.SATA to USB-C or USB-A AdaptersMultiple times I needed to clone some old disk to new SSD – just to make an old system faster.I usually boot from some USB drive with FreeBSD and while new SSD is attached with these adapters – I then execute  command to clone the old HDD disk to new SSD drive … and then just swap them out.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.Angle USB-C and USB-A AdaptersAs we already talked about RJ45 angle adapters … there are also USB-C and USB-A angle adapters.The do the same good job with cables to not stick out on a side of a laptop.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.In the progressing and always changing world yesterday the USB-A was king and tomorrow the USB-C will be.There are multiple cases in which you will need these – from simple USB headphones to USB pendrives and other stuff.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.Tiny USB WiFi or Bluetooth DongleMultiple times I have found myself in a situation where it was very convenient to just add some WiFi or Bluetooth chip over USB port and do the job instead of trying to achieve the same without such chips.While I usually omit Bluetooth I can not say the same about WiFi … and as FreeBSD lacks a little in that department – using a very tiny chip such as Realtek RTL8188CUS often does the job done.You can find them for about  – for example – on https://aliexpress.com – with the  or  keywords in their search.USB-C <=> Micro USB AdapterIn the past – in the USB Micro times – I remember using an adapter to be able to charge – then new and uncommon – USB-C devices.Fast forward several years and now the situation is the other way around (as expected). The USB-C is the standard and USB Micro devices are less and less common … but there are still here. To not have to keep separate dedicated USB Micro cables I use a small USB-C to USB Micro adapters.Such adapter takes USB-C as input power and is able to charge USB Micro devices.You can find them for about  – for example – on https://aliexpress.com – with the  keywords in their search.USB-C <=> Laptops/Routers/5.5mmx2.5mm AdaptersWhen it comes to delivering power to my (and not only) laptops – the new standard seems to be the USB-C connector with ‘requirement’ of 45W or more (it depends).Not that long ago I discovered that even laptops as old as 13 years –  – can be powered the same – but with simple and very cheap adapter cables – such as these below. From the left there is  typical router socket – then more modern  (and many more) – then oldschool models from 2011 year –  such as  models.All they need is a USB-C power input.You need to only meet two requirements – the USB charger that will make enough power for example 20V at 3.25A for 65W that would power  or 20V at 6.75A for 135W that would power . While the official power supply for ThinkPad W520 is 170W – its perfectly fine to use the 135W power adapter from  to power  laptop.This makes organizing cables (and chargers) a lot easier – for example – I would not be able to fit 3 ‘dedicated’ ThinkPad chargers in that white cable organizer behind laptops – but I will fir there two powerful 65W and 85W USB-C chargers perfectly fine.You can find these power adapters for about  – for example – on https://aliexpress.com – with the USB-C ADAPTER LAPTOP ROUTER keywords in their search.Creative BT-W2 USB-A Bluetooth AdapterWhen I have to cope with Bluetooth technology – its ‘tolerable’ on Android devices such as phones/tablets and mostly nowhere else. After bad audio (just not working) Bluetooth possibilities on FreeBSD I decided to try the hardware solution instead. The audio related Bluetooth on FreeBSD have failed me too many times – to the point called  – that also means I do not want to waste any more time trying to figure the way using FreeBSD Bluetooth stack devices anymore – at least for audio related devices.Not so long ago I got the  headphones. I am/was a big fan of the  cable headphones (Jack or Mini Jack based). They have so much BASS and ‘power’ that I could not ask for more … and their cost is very low – like  or less. The only ‘downside’ of the  headphones is that they are audio only – they do not have any microphone at all – they are dedicated for music only – and that is OK – they do GREAT in that role.I have tried some Bluetooth based headphones in the past – and they were SHIT to say the least. Not enough ‘power’ – not enough BASS etc. After reading multiple reviews I decided to give  headphones a chance … and I was not disappointed. Its the first time after  cable headphones that ANY Bluetooth based headphone delivered. I was (and I still am) really satisfied with them.This is where the USB powered  comes handy. Its also relatively cheap as the cost of used unit is less then  – at least that is the price I payed for mine in Poland. The  allows to connect Bluetooth audio devices everywhere – even on OpenBSD – on the system that cut off Bluetooth stack entirely – and it works well on FreeBSD too. The ‘downside’ of the  headphones is that they do have microphone – but only in Bluetooth node – they have Mini Jack connector – but for audio only …This is also only downside of the  solution – it transmits only audio – but w/o microphone. Its more then OK for listening music – but if You have to do live conferencing/meetings on FreeBSD as I do – its a dead end.I have tried to find a solution to this problem – to the point that I wanted to abandon  headphones entirely and find some Mini Jack (or Jack) based BASS oriented headphones that will also have a working microphone.On my journey I have found a solution that I did not expected at all – and that was the solution that solved all my problems – and allowed me to enjoy the  headphones – but more about that in the next ‘subsection’.External Microphone for SONY HeadphonesYou already know the downsides of the  headphones that were giving me headaches. Now its time to address them.After many hours of searching the Internet I have found a very ‘usable’ Mini Jack cable. A cable that came with microphone and a one that perfectly integrated with  headphones … and FreeBSD as well.Its available to buy for  on  (and possible other locations) and its called . Thanks to the knowledge that  headphones have Mini Jack port with microphone part – the  cable even comes with volume controls and even come with physical kill switch for microphone.After You attach this  to the  headphones it looks (and works) like a natural solution.The only ‘downside’ is generally the downside of the  headphones – that You CAN NOT disable their silencing while you speak – so using them in ‘passive’ mode with  is preferred to meet all needs.After reading comments to this article I learned that this ‘silencing’ is called  and it can be disabled in the SONY Android app or by holding  on the  until the headphones say “Speak to Chat disabled.” Thank You for that.I got used to the fact that I just put my headphones on the desk … but I wanted something more useful – after some searching it was obvious to me that I needed just some headphones handle that I could attach somewhere.After another several hours of browsing I have found a ‘part’ that would fit perfectly – a  part from https://aliexpress.com that I could find with the  keywords in their search.Here is how it works on my desk.… and its 360 degrees adjustable as well.Dual USB-C and USB-A Pendrive (SanDisk)With all my ‘bad’ experiences with PTP connections for Android based devices (and other places) I really liked the .Its really handy for many transfers … and its more fast then slow as well.When You need to connect several USB-A devices the USB ports count often come short fast – this is where this tiny USB-A hub comes handy.With its dirt cheap  price (at https://aliexpress.com with  keywords) its a ‘steal’ … and it is a 3 port hub – there is another USB-A port at the end of it – the one that is not visible.Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card Slot… as we are talking various USB-A or USB-C solutions I could not mention this quad port adapter with MicroSD card slot.I do not even remember how many times I have used it to copy/backup contents of my phone(s) and/or tablet(s).Nowadays I believe I use the Dual USB-C / USB-A Pendrive more … but not always.For  on its not a bad solution to have.Batteries … I mean SD card – not included 🙂I have often found that the angle with which the power cord sticks out of a PC is definitely not ideal – this is where angle power adapters come handy.Here is how it looks (being used) on my PC.C13/C14 Power Adapters with Additional C1/C2 or C5/C6 SocketsAfter You have spent some time to lay down the C13/C14 power cables just to power your PC its really annoying to do the same for another set of C1/C2 or C5/C6 cables/sockets … but not anymore.Now with single cable adapter You are able to power more then one computer – depending on the needs with additional connectors.HDMI 3in1 Switch with Remote ControlI happen to have a 2010 FullHD 50 Inch TV that has ONLY ONE port of HDMI kind … and it was pretty annoying to say the least … up to the time I added a HDMI switch/hub to it.The HDMI switch along with its remote below.For the record – I have used the UGreen 3in1 HDMI Switch with 4K @ 30Hz Capability and Remote and I was able to get one for .To not have a mess in the cables its useful to have them organized in some way.I use multiple solutions for that.Lets start with simple organizers.… and a larger/taller one for more capacity/possibilities.I also use some IKEA containers …… and smaller boxes in which I keep the tiny things.I do not even remember after what product these boxes are … and that does not even matter I think.While there are many software settings or solutions to prevent screen from locking up – there is one bulletproof solution what just always works – a hardware USB mouse jigger.I use a very simple one with 3 modes – but its more then enough for me needs.Last but not least – the car FM transmitter.My daily ‘real’ driver (I mean on the real road outside) is the  car. I really love it for the simplicity and calm that it provides during the ride – but on the audio side it only has an old FM/AM radio and a CD slot … and not MP3 support in that one.This is where the FM transmitter such as mine  comes really handy.It supports two modes. One is being a Bluetooth slave of your phone – it just plays on the car speakers anything you are currently playing on your phone – it also has microphone builtin – so You can also use it as a ‘loud’ phone talking device.I use it in a more simple mode – I just attach a tiny  pendrive to it – and play a random song of it.Besides these features it also has additional USB-A port available to attach a cable to it and charge some device.I was able to get one a new one for about .The mentioned devices above are probably not the only ones that make my life easier – but definitely the most crucial ones.Feel free to share your ‘helper’ hardware in the comments.]]></content:encoded></item><item><title>Sunken Superyacht of UK Tech Tycoon Mike Lynch Recovered Near Sicily</title><link>https://news.slashdot.org/story/25/06/20/2158226/sunken-superyacht-of-uk-tech-tycoon-mike-lynch-recovered-near-sicily?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 02:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The superyacht Bayesian, owned by UK tech tycoon Mike Lynch, has been recovered off the coast of Sicily nearly a year after it sank during a storm, killing Lynch, his daughter, and five others. Italian authorities hope the $30 million salvage will uncover the cause of the sinking, which is under investigation for suspected manslaughter amid concerns about design flaws and storm vulnerability. The Guardian reports: The white top and blue hull of the 56-meter (184ft) vessel emerged from the depths of the sea in a holding area of a yellow floating crane barge, as salvage crews readied it to be hauled ashore for further investigation. The Italian coastguard said the recovery was scheduled to begin on Saturday morning. A spokesperson for TMC Maritime, which is conducting the recovery operation, said the vessel had been slowly raised from the seabed, 50 meters (165ft) down, over the past three days to allow the steel lifting straps, slings and harnesses to be secured under the keel.
 
The operation -- which has cost approximately $30 million -- was made easier after the vessel's 72-meter mast was detached using a remote-controlled cutting tool and placed on the seabed on Tuesday. The vessel will be transported to the port of Termini Imerese, where investigators are expected to examine it as part of an inquiry into the cause of the sinking. [...] The salvage operation was very complex, and was temporarily suspended in mid-May after Rob Cornelis Maria Huijben, a 39-year-old Dutch diver, died during underwater work. The British-based consultancy TMC Marine, which oversaw a consortium of salvage specialists undertaking the project, said the hull would be lifted on to a specially manufactured steel cradle on the quayside once it had been transported to Termini Imerese. Investigators hope the yacht will yield vital clues to the causes of the sinking. A forensic examination of the hull will seek to determine whether one of the hatches remained open and whether the keel was improperly raised.]]></content:encoded></item><item><title>Apple Adds Energy and Battery Labels To iPhone and iPad Pages In EU</title><link>https://mobile.slashdot.org/story/25/06/20/2034241/apple-adds-energy-and-battery-labels-to-iphone-and-ipad-pages-in-eu?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 01:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from MacRumors: To comply with a new regulation that takes effect today, Apple has added an energy efficiency label to its iPhone and iPad pages in EU countries. Apple is also required to start including a printed version of the label with the devices sold there. The label grades a given iPhone or iPad model's energy efficiency from a high of A to a low of G, based on the EU's testing parameters. However, Apple said that certain aspects of the testing methods outlined by the European Commission are "ambiguous," so it chose to be conservative with its scores until testing is standardized.
 
In a 44-page document (PDF) detailing its testing methodology for the labels, Apple said its current iPhone models qualified for the highest energy efficiency grade of A, but the company voluntarily downgraded these scores to a B as a cautionary measure. The label also provides details about a given iPhone or iPad model's battery life per full charge cycle, repairability grade, impact resistance, ingress protection rating for water and dust resistance, and how many full charge cycles the battery is rated for. Likewise, this information is based on Apple's interpretation of the EU's testing parameters.
 
On the web, the label can be viewed by clicking or tapping on the colorful little tag icon on various iPhone and iPad pages on Apple's localized websites for EU countries. It is shown on both Apple's main product marketing pages for all iPhone and iPad models that are currently sold in the EU, and on the purchase page for those devices. The label is accompanied by a product information sheet (PDF) that provides a comprehensive overview of even more details, such as the device's battery capacity in mAh, screen scratch resistance based on the Mohs hardness scale, the minimum guaranteed timeframe for availability of security updates, and much more.]]></content:encoded></item><item><title>BBC Threatens Legal Action Against Perplexity AI Over Content Scraping</title><link>https://news.slashdot.org/story/25/06/20/2022200/bbc-threatens-legal-action-against-perplexity-ai-over-content-scraping?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 00:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ancient Slashdot reader Alain Williams shares a report from The Guardian: The BBC is threatening legal action against Perplexity AI, in the corporation's first move to protect its content from being scraped without permission to build artificial intelligence technology. The corporation has sent a letter to Aravind Srinivas, the chief executive of the San Francisco-based startup, saying it has gathered evidence that Perplexity's model was "trained using BBC content." The letter, first reported by the Financial Times, threatens an injunction against Perplexity unless it stops scraping all BBC content to train its AI models, and deletes any copies of the broadcaster's material it holds unless it provides "a proposal for financial compensation."
 
The legal threat comes weeks after Tim Davie, the director general of the BBC, and the boss of Sky both criticised proposals being considered by the government that could let tech companies use copyright-protected work without permission. "If we currently drift in the way we are doing now we will be in crisis," Davie said, speaking at the Enders conference. "We need to make quick decisions now around areas like ... protection of IP. We need to protect our national intellectual property, that is where the value is. What do I need? IP protection; come on, let's get on with it." "Perplexity's tool [which allows users to choose between different AI models] directly competes with the BBC's own services, circumventing the need for users to access those services," the corporation said.
 
Perplexity told the FT that the BBC's claims were "manipulative and opportunistic" and that it had a "fundamental misunderstanding of technology, the internet and intellectual property law."]]></content:encoded></item><item><title>Learn you Galois fields for great good (2023)</title><link>https://xorvoid.com/galois_fields_for_great_good_00.html</link><author>signa11</author><category>hn</category><pubDate>Sat, 21 Jun 2025 00:21:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is the introduction to a series on Abstract Algebra. In particular, our focus will be on Galois Fields (also known as Finite Fields) and their applications in Computer Science. This is a project I've been excited about for many years now, but have been too busy to dedicate the adequate effort to meet my perfectionism standards (yay perfectionism!).Many moons back I was self-learning Galois Fields for some erasure coding theory applications. I was quite disappointed with the lack of accessible resources for computer scientists.
Many resources assumed either:Its beyond your skill level so let's oversimplify ("it's hard, don't worry about it"), orYou had prior Pure Math studies in Abstract Algebra ("it's easy, just use jargon jargon jargon")Unfortunately, Abstract Algebra is not standard subject matter in most computer science curriculums. Often computer science mathematics start and end
with Discrete Math. If you're lucky, maybe you've also been exposed to Linear Algebra.So, ultimately, I ended up self-learning Abstract Algebra from a pure math textbook. But for the great majority of computer scientists, there has to be a better way.
This series intends to fill this gap. This is the gentle step-by-step approach with applications implemented with actual code. It's the intro I wanted when I was starting out.Abstract algebra is a beautiful subject. It's the idea that the numbers you're familiar with don't matter. The numbers are just
arbitrary labels. What matters is the relationships they have with other numbers when you add or multiply them. If the numbers
don't matter, then we can swap those labels for different labels and all the normal math rules will still work.For example, we could create an algebra that allows us to add or multiply colors:And this is what makes the subject abstract and confusing. How can you just say that numbers don't matter? It doesn't make sense.And even so, why would we want to study this? Why would a computer scientist care?Well, we use computer algorithms to manipulate data. We encode/decode it, we encrypt/decrypt it, we detect corruption, etc.
Wouldn't it be great if we could use normal math to do those things? Wouldn't it be great if would could add or multiply
an 8-bit byte by an 8-bit byte and get another 8-bit byte? And if we could do that, could we also do Linear Algebra over Data? Yes, yes, and more yes.
This is why studying Abstract Algebra is worthwhile.(Hint: Neither 263 nor 9282 are answers, they are not 8-bit numbers)You can also make quirky blog posts that make your friends think you've gone crazy, like milk and cookies 🥛 🍪 😊The applications and algorithms are staggering. You interact with implementations of abstract algebra everyday: CRC,
AES Encryption, Elliptic-Curve Cryptography, Reed-Solomon, Advanced Erasure Codes, Data Hashing/Fingerprinting, Zero-Knowledge Proofs, etc.Having a solid-background in Galois Fields and Abstract Algebra is a prerequisite for understanding these applications.Approach: Step-by-step, Active Learning, and Literate ProgrammingIn this series, we will start from the very basics of theory and build up step-by-step to interesting applications such as Reed-Solomon,
AES, etc. As such, the material will be  cumulative. Many exercises will be included to aid understanding. Each section will build gradually,
but will assume mastery of the previous section. Active learning is . We will expect that
readers are putting in adequate effort to grok the abstract concepts.We won't assume too much mathematical background beyond high-school level algebra. However, in some applications (for example: Reed-Solomon),
familiarity with Linear Algebra will be required. We won't explain Linear Algebra since great resources already
exist here. You are encouraged to supplement as needed.We will be including code in a Literate Programming style where appropriate.  For example, to aid understanding, we will build some interactive
command-line tools that allow you to play around with various theoretical concepts in practice. All code will be the
Rust Programming Language, but advanced features will be intentionally avoided so that the code will be readable
by most experienced computer programmers.The main goal of this series is understandability and education. As such, the implementations will not be optimal. We will forgo nearly all
optimizations you'd see in a production quality implementation: lookup-tables, vectorized instructions, clever representations, computer
architecture optimizations, etc. It's possible that later posts in the series will discuss optimizations, but this is not a primary goal. At the end,
we hope these implementations will serve as good reference implementations. This can have it's own merit since highly optimized algorithms are
often difficult to read and understand.For active learning, I strongly encourage you to do your own implementations and to play around with the command-line tools while reading. If you'd
like to open-source your implementations, I'm more than happy to link them here:Original: xorvoid (Rust): herePlanning is Essential, Plans are WorthlessHere's the rough plan. We will see how it actually goes:Other possible advanced subjects:Extended Euclidean AlgorithmBit-matrix RepresentationsFast Multiplication with FFTsVectorization Implementation TechniquesThe first few sections are theory. There's not much coding in these sections, but they are very important for success later in the series.
Don't skip them.]]></content:encoded></item><item><title>Meta Discussed Buying Perplexity Before Investing In Scale AI</title><link>https://meta.slashdot.org/story/25/06/20/2015248/meta-discussed-buying-perplexity-before-investing-in-scale-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 21 Jun 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to Bloomberg (paywalled), Meta reportedly explored acquiring Perplexity AI but the deal fell through, with conflicting accounts on whether it was mutual or Perplexity backed out. Instead, Meta invested $14.3 billion in Scale AI, taking a 49% stake as part of its broader push to catch up with OpenAI and Google in the AI race.
 
"Meta's attempt to purchase Perplexity serves as the latest example of Mark Zuckerberg's aggressive push to bolster his company's AI efforts amid fierce competition from OpenAI and Google parent Alphabet," reports CNBC. "Zuckerberg has grown agitated that rivals like OpenAI appear to be ahead in both underlying AI models and consumer-facing apps, and he is going to extreme lengths to hire top AI talent."]]></content:encoded></item><item><title>TypeScript: checking Map keys and Array indices</title><link>https://2ality.com/2025/06/checking-map-keys-array-indices-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 21 Jun 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[JavaScript has two common patterns:Maps: We check the existence of a key via  before retrieving the associated value via .Arrays: We check the length of an Array before performing an indexed access.These patterns don’t work as well in TypeScript. This blog post explains why and presents alternatives.]]></content:encoded></item><item><title>Rippling spy says men have been following him, and his wife is afraid</title><link>https://techcrunch.com/2025/06/20/rippling-spy-says-men-have-been-following-him-and-his-wife-is-afraid/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 20 Jun 2025 23:51:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[If becoming a corporate spy sounds exciting, let this newest affidavit from confessed Rippling spy Keith O’Brien serve as a warning.]]></content:encoded></item><item><title>Plastic bag bans and fees reduce harmful bag litter on shorelines</title><link>https://www.science.org/doi/10.1126/science.adp9274</link><author>miles</author><category>hn</category><pubDate>Fri, 20 Jun 2025 23:46:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Applebee&apos;s and IHOP Plan To Introduce AI in Restaurants</title><link>https://slashdot.org/story/25/06/20/1954245/applebees-and-ihop-plan-to-introduce-ai-in-restaurants?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 23:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The company behind Applebee's and IHOP plans to use AI in its restaurants and behind the scenes to streamline operations and encourage repeat customers. From a report: Dine Brands is adding AI-infused tech support for all of its franchisees, as well as an AI-powered "personalization engine" that helps restaurants offer customized deals to diners, said Chief Information Officer Justin Skelton. The Pasadena, Calif.-based company, which also owns Fuzzy's Taco Shop and has over 3,500 restaurants across its brands, is taking a "practical" approach to AI by focusing on areas that can drive sales, Skelton said. 

Streamlining tech support for Dine Brands' more than 300 franchisees is important because issues like a broken printer take valuable time away from actually managing restaurants, Skelton said. Dine Brands' AI tool, which was built with Amazon's Q generative AI assistant, allows the company's field technology services staff to query its knowledge base for tech help using plain English, rather than needing to manually search for answers.]]></content:encoded></item><item><title>Record DDoS Pummels Site With Once-Unimaginable 7.3Tbps of Junk Traffic</title><link>https://yro.slashdot.org/story/25/06/20/2010218/record-ddos-pummels-site-with-once-unimaginable-73tbps-of-junk-traffic?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 20 Jun 2025 22:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: Large-scale attacks designed to bring down Internet services by sending them more traffic than they can process keep getting bigger, with the largest one yet, measured at 7.3 terabits per second, being reported Friday by Internet security and performance provider Cloudflare. The 7.3Tbps attack amounted to 37.4 terabytes of junk traffic that hit the target in just 45 seconds. That's an almost incomprehensible amount of data, equivalent to more than 9,300 full-length HD movies or 7,500 hours of HD streaming content in well under a minute.
 
Cloudflare said the attackers "carpet bombed" an average of nearly 22,000 destination ports of a single IP address belonging to the target, identified only as a Cloudflare customer. A total of 34,500 ports were targeted, indicating the thoroughness and well-engineered nature of the attack. [...] Cloudflare said the record DDoS exploited various reflection or amplification vectors, including the previously mentioned Network Time Protocol; the Quote of the Day Protocol, which listens on UDP port 17 and responds with a short quote or message; the Echo Protocol, which responds with the same data it receives; and Portmapper services used identify resources available to applications connecting through the Remote Procedure Call. Cloudflare said the attack was also delivered through one or more Mirai-based botnets. Such botnets are typically made up of home and small office routers, web cameras, and other Internet of Things devices that have been compromised.]]></content:encoded></item><item><title>The Way Forward For AI: Learning From The Elephant &amp; The Blind Men</title><link>https://www.techdirt.com/2025/06/20/the-way-forward-for-ai-learning-from-the-elephant-the-blind-men/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 20 Jun 2025 22:46:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[This series of posts explores how we can rethink the intersection of AI, creativity, and policy. From examining outdated regulatory metaphors to questioning copyright norms and highlighting the risks of stifling innovation, each post addresses a different piece of the AI puzzle. Together, they advocate for a more balanced, forward-thinking approach that acknowledges the potential of technological evolution while safeguarding the rights of creators and ensuring AI’s development serves the broader interests of society.Let’s start with the original metaphor of the six blind men and the elephant. In this classic Indian tale, each man feels a different part of the elephant—one touches the tusk and declares it’s a spear, another grabs the tail and swears it’s a rope, and so on. Each is convinced they’ve got the whole picture, but in reality, they’re missing the full scope of the elephant because they refuse to share their perspectives.Now, let’s apply this to AI regulation. Imagine six policymakers, each with a firm grip on their own slice of the AI puzzle. One is fixated on , another sees only , while yet another is laser-focused on . But as a result, their narrow focus is leaving the broader picture woefully incomplete. And that, my friends, is where the trouble begins.Accepting the Challenge of InnovationAI is so much more than just a collection of legal headaches. It’s a powerful, transformative force. It’s revolutionizing industries, supercharging creativity, driving research, and solving problems we couldn’t have even dreamed of a few years ago. It’s not just a new avenue for academics to write articles—it’s a tool that could unlock incredible potential, pushing the boundaries of human creativity and innovation.But what happens when we regulate it with tunnel vision? When we obsess over the tail and ignore the rest of the elephant? We end up stifling the very innovation we should be encouraging. The piecemeal approach doesn’t just miss the bigger picture—it risks handcuffing the future of AI, limiting its capacity to fuel new discoveries and reshape industries for the better.By focusing solely on risks and potential copyright or privacy violations, we’re leaving research, creativity, and innovation stranded. Think of the breakthroughs AI could help us achieve: revolutionary advances in healthcare, educational tools that adapt to individual learners, creative platforms that democratize access to artistic expression. AI isn’t just a regulatory problem to be tackled—it’s a . And unless policymakers start seeing the whole elephant, we’re going to end up trampling the very future we’re trying to protect.So, What’s the Way Forward?We need to rethink our approach. AI, especially , can offer immense societal benefits—but only if we create policies that reflect its potential. Over-focusing on copyright claims or letting certain stakeholders dominate the conversation means we end up putting brakes on the very technology that could drive our next era of progress.Imagine if, in the age of the Gutenberg Press, we had decided to regulate printing so heavily to protect manuscript copyists that books remained rare and knowledge exclusive. We wouldn’t be where we are today. The same logic applies to AI. If we make it impossible for AI to learn, to explore vast amounts of data, to create based on the expressions of humanity, we will end up in a —a future where AI, stifled and starved of quality input, fails to reach its true potential.AI chatbots, creative tools, and generative models have shown that they can be both collaborators and catalysts for human creativity. They help artists brainstorm, assist writers in overcoming creative blocks, and enable non-designers to visualize their ideas. By empowering people to create in new ways, AI is democratizing creativity. But if we let fears over copyright overshadow everything else, we risk shutting down this vibrant new avenue of cultural expression before it even gets started.Seeing the Whole ElephantThe task of policymaking is challenging, especially with emerging technologies that shift as rapidly as AI. But the answer isn’t to clamp down with outdated regulations to preserve the status quo for a few stakeholders. Instead, it’s to foster an environment where innovation, creativity, and research can flourish alongside reasonable protections. We must encourage fair compensation for creators (and let’s not forget they should not be equated to the creative industry) while ensuring that AI can access the data it needs to evolve, innovate, and inspire.The metaphor of the blind men and the elephant serves as a clear warning: if we only see a part of the elephant, we can only come up with partial solutions. It’s time to step back and view AI for what it truly is—a powerful, transformative force that, if used wisely, can uplift our societies, enhance our creativity, and tackle challenges that once seemed impossible.The alternative is to regulate AI into irrelevance by focusing only on a single aspect. We need to see the whole elephant—understand AI in its entirety—and allow it to shape a future where human creativity, innovation, and progress thrive together.Caroline De Cock is a communications and policy expert, author, and entrepreneur. She serves as Managing Director of N-square Consulting and Square-up Agency, and Head of Research at Information Labs. Caroline specializes in digital rights, policy advocacy, and strategic innovation, driven by her commitment to fostering global connectivity and positive change.]]></content:encoded></item><item><title>AbsenceBench: Language models can&apos;t tell what&apos;s missing</title><link>https://arxiv.org/abs/2506.11440</link><author>JnBrymn</author><category>hn</category><pubDate>Fri, 20 Jun 2025 22:26:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Congestion Pricing in Manhattan is a Predictable Success</title><link>https://news.slashdot.org/story/25/06/20/205237/congestion-pricing-in-manhattan-is-a-predictable-success?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 22:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Manhattan's congestion pricing program has reduced traffic by 10% and cut car-noise complaints by 70% in its first six months of operation, according to city data. The $9 daily toll for vehicles entering Manhattan below 60th Street began January 5, generating approximately $50 million monthly for subway and public transit improvements. 

Buses now travel fast enough that drivers must stop and wait to maintain schedules, while subway ridership has increased sharply since the program launched. Broadway theater attendance has risen rather than declined as some critics predicted. Polling shows more New Yorkers now support the toll than oppose it, a reversal from widespread opposition before implementation. 

The policy took nearly 50 years to enact despite originating from Columbia University economist William Vickrey's work in the 1960s. Congress blocked a similar proposal in the 1970s, and the current program faced a six-year implementation delay after Governor Andrew Cuomo signed it into law in 2019. Governor Kathy Hochul postponed the launch in 2024 before allowing it to proceed after Donald Trump's presidential election victory.]]></content:encoded></item><item><title>Mira Murati’s Thinking Machines Lab closes on $2B at $10B valuation</title><link>https://techcrunch.com/2025/06/20/mira-muratis-thinking-machines-lab-closes-on-2b-at-10b-valuation/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Fri, 20 Jun 2025 21:59:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Thinking Machines Lab, the secretive AI startup founded by OpenAI’s former chief technology officer Mira Murati, has closed a $2 billion seed round at a $10 billion valuation. ]]></content:encoded></item><item><title>Apple Sued By Shareholders For Allegedly Overstating AI Progress</title><link>https://yro.slashdot.org/story/25/06/20/2147258/apple-sued-by-shareholders-for-allegedly-overstating-ai-progress?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 20 Jun 2025 21:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple is facing a proposed class-action lawsuit from shareholders who allege the company misled investors about the readiness of its AI-powered Siri upgrades, contributing to a $900 billion drop in market value. Reuters reports: Shareholders led by Eric Tucker said that at its June 2024 Worldwide Developers Conference, Apple led them to believe AI would be a key driver of iPhone 16 devices, when it launched Apple Intelligence to make Siri more powerful and user-friendly. But they said the Cupertino, California-based company lacked a functional prototype of AI-based Siri features, and could not reasonably believe the features would ever be ready for iPhone 16s.
 
Shareholders said the truth began to emerge on March 7 when Apple delayed some Siri upgrades to 2026, and continued through this year's Worldwide Developers Conference on June 9 when Apple's assessment of its AI progress disappointed analysts. Apple shares have lost nearly one-fourth of their value since their December 26, 2024 record high, wiping out approximately $900 billion of market value.]]></content:encoded></item><item><title>Banning Plastic Bags Works To Limit Shoreline Litter, Study Finds</title><link>https://science.slashdot.org/story/25/06/20/201242/banning-plastic-bags-works-to-limit-shoreline-litter-study-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 21:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: At tens of thousands of shoreline cleanups across the United States in recent years, volunteers logged each piece of litter they pulled from the edges of lakes, rivers and beaches into a global database. One of the most common entries? Plastic bags. But in places throughout the United States where plastic bags require a fee or have been banned, fewer bags end up at the water's edge, according to research published this week in Science. 

Lightweight and abundant, thin plastic bags often slip out of trash cans and recycling bins, travel in the wind and end up in bodies of water, where they pose serious risks to wildlife, which can become entangled or ingest them. They also break down into harmful microplastics, which have been found nearly everywhere on Earth. Using data complied by the nonprofit Ocean Conservancy, researchers analyzed results from 45,067 shoreline cleanups between 2016 to 2023, along with a sample of 182 local and state policies enacted to regulate plastic shopping bags between 2017 and 2023. They found areas that adopted plastic bag policies saw a 25 to 47 percent reduction in the share of plastic bag litter on shorelines, when compared with areas without policies. The longer a policy was in place, the greater the reduction.]]></content:encoded></item><item><title>Just on the rocks (Friends)</title><link>https://changelog.com/friends/98</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/98/changelog--friends-98.mp3" length="" type=""/><pubDate>Fri, 20 Jun 2025 21:30:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Jerod tells Adam about how bad he hates the taste of Gin, sips on some Generative A Rye (on the rocks), they open the comments section for a bit, and then land the plane talking about being alone, naked, and afraid.Changelog++ members save 6 minutes on this episode because they made the ads disappear. Join today!Retool – Assemble your elite AI team, arm them with powerful custom tools, and watch them make your to-do list disappear. Start for free or book a demo at retool.com/agentsDepot – 10x faster builds? Yes please. Build faster. Waste less time. Accelerate Docker image builds, and GitHub Actions workflows. Easily integrate with your existing CI provider and dev workflows to save hours of build time.
Outshift by Cisco – AGNTCY is an open source collective building the Internet of Agents. It’s a collaboration layer where AI agents can communicate, discover each other, and work across frameworks. For developers, this means standardized agent discovery tools, seamless protocols for inter-agent communication, and modular components to compose and scale multi-agent workflows.
]]></content:encoded></item><item><title>AMD&apos;s Freshly-Baked MI350: An Interview with the Chief Architect</title><link>https://chipsandcheese.com/p/amds-freshly-baked-mi350-an-interview</link><author>pella</author><category>hn</category><pubDate>Fri, 20 Jun 2025 21:20:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Transcript below has been edited for conciseness and readability.And so we did that and delivered the fastest supercomputer in the world along with Lawrence Livermore, with El Capitan. But so as a consideration there, we wanted to have more compute units for XCD so that we could get 224 total within MI300A. On 350, where it's designed specifically as an accelerator only, a discrete accelerator, we had more flexibility there. And so we decided that having a power of two number of active compute units per die - so 36 physical, like you said, but we enable 32. Four of them, one per shader engine, are used for harvesting and we yield those out in order to give us good high-volume manufacturing through TSMC-N3, which is a leading edge technology. So we have some of the spare ones that allow us to end up with 32 actually enabled.And that's a nice power of two, and it's easy to tile tensors if you have a power of two. So most of the tensors that you're working with, or many of them, would be matrices that are based on a power of two. And so it allows you to tile them into the number of compute units easily, and reduces the total tail effect that you may have. Because if you have a non-power of two number of compute units, then some amount of the tensor may not map directly nicely, and so you may have some amount of work that you have to do at the end on just a subset of the compute unit. So we find that there's some optimization there by having a power of two.Alan: Yeah, so what we did, as you mentioned, so in MI350, the I/O dies, there's only two of them. And then each of them host four of the accelerator chiplets versus in MI300, we had four of the I/O dies, with each of them hosting two of the accelerator chiplets. So that's what you're talking about.So what we did was, we wanted to increase the bandwidth from global, from HBM, which, MI300 was designed for HBM3 and MI350 was specially designed for HBM3E. So we wanted to go from 5.2 or 5.6 gigabit per second up to a full 8 gigabit per second. But we also wanted to do that at the lowest possible power, because delivering the bytes from HBM into the compute cores at the lowest energy per bit gives us more power at a fixed GPU power level, gives us more power into the compute at that same time. So on bandwidth-bound kernels that have a compute element, by reducing the amount of power that we spend in data transport, we can put more power into the compute and deliver a higher performance for those kernels.So what we did by combining those two chips together into one was we were able to widen up the buses within those chips; so we deliver more bytes per clock, and therefore we can run them at a lower frequency and also a lower voltage, which gives us the V-squared scaling of voltage for the amount of power that it takes to deliver those bits. So that's why we did that.So when we do our total power and thermal architecture of these chips, we consider from the motherboard all the way up to the daughterboards, which are the UBB (Universal Baseboard), the OAM (OCP Accelerator Module) modules in this case, and then up through the stack of CoWoS (Chip on Wafer on Substrate), the I/O dies, which are in this intermediate layer, and then the compute that's above those. So we look at the total thermal density of that whole stack, and the amount of thermal transport or thermal resistance that we have within that stack, and the thermal interface materials that we need in order to build on top of that for heat removal, right?And so we offer two different classes of thermal solutions for MI350 series. One of them air-cooled, like you mentioned. The other one is a direct-attach liquid cool. So the cold plate would then, in the liquid cool plate, liquid-cooled case would directly attach to the thermal interface material on top of the chips. So we do thermal modeling of that entire stack, and work directly with all of our technology partners to make sure that the power densities that we build into the chips can be handled by that entire thermal stack up.If you would like to support the channel, hit like, hit subscribe. And if you like interviews like this, tell us in the comments below. Also, there will be a transcript on the Chips and Cheese website. If you want to directly monetarily support Chips and Cheese, there's Patreon, as well as Stripe through Substack, and PayPal. So, thank you so much for that interview, Alan.]]></content:encoded></item><item><title>Wiki Radio: The thrilling sound of random Wikipedia</title><link>https://www.monkeon.co.uk/wikiradio/</link><author>if-curious</author><category>hn</category><pubDate>Fri, 20 Jun 2025 21:15:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
    The thrilling sound of random Wikimedia
  
    Inspired by WikiTok, I thought I'd make something to discover
    sounds uploaded to Wikimedia. From political speeches and bird noises to genuine bangers,
    it's mostly wholesome, though I cant guarantee it won't play you something horrible once in a while.
    If you want shorter sounds, try it in Revolution 9 Mode.

      ]]></content:encoded></item><item><title>Cluely, a startup that helps ‘cheat on everything,’ raises $15M from a16z</title><link>https://techcrunch.com/2025/06/20/cluely-a-startup-that-helps-cheat-on-everything-raises-15m-from-a16z/</link><author>Marina Temkin</author><category>tech</category><pubDate>Fri, 20 Jun 2025 21:06:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cluely's new funding comes roughly two months after it raised $5.3 million in seed funding co-led by Abstract Ventures and Susa Ventures.]]></content:encoded></item><item><title>Friday Squid Blogging: Gonate Squid Video</title><link>https://www.schneier.com/blog/archives/2025/06/friday-squid-blogging-gonate-squid-video.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 20 Jun 2025 21:04:03 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[This is the first ever video of the Antarctic Gonate Squid.As usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.]]></content:encoded></item><item><title>Why You Should Not Replace Blanks with 0 in Power BI</title><link>https://towardsdatascience.com/why-you-should-not-replace-blanks-with-0-in-power-bi/</link><author>Nikola Ilic</author><category>dev</category><category>ai</category><pubDate>Fri, 20 Jun 2025 20:53:04 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Did someone ask you to replace blank values with 0 in your reports? Maybe you should think twice before you do it!]]></content:encoded></item><item><title>DHS Warns of Sharp Rise in Chinese-Made Signal Jammers</title><link>https://news.slashdot.org/story/25/06/20/1957244/dhs-warns-of-sharp-rise-in-chinese-made-signal-jammers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 20:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Department of Homeland Security is concerned about the rate at which outlawed signal-jamming devices are being found across the US. From a report: In a warning issued on Wednesday, it said it has seen an 830 percent increase in seizures of these signal jammers since 2021, specifically those made in China. Signal-jamming devices are outlawed in the US, mainly because they can interfere with communications between emergency services and law enforcement. 

While the Communications Act of 1934 effectively prohibits such devices, signal jammers of the type DHS is concerned about have only circulated in the last 20 to 30 years. Authorities have paid special attention to relay attack devices in recent years -- the types of hardware that can be used to clone signals used by systems such as remote car keys, although the first examples of these devices date back to the 1980s.]]></content:encoded></item><item><title>The Shell Game Of Fascist Gaslighting</title><link>https://www.techdirt.com/2025/06/20/the-shell-game-of-fascist-gaslighting/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 20 Jun 2025 20:44:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I need to say something that will be deeply uncomfortable for many of you: if you have friends, family, or colleagues defending what’s happening right now, their old sane selves may not be coming back.Let me be specific about what I mean. This week, Donald Trump posted explicit orders on Truth Social directing federal law enforcement to conduct “Mass Deportation Operations” targeting “America’s largest Cities” because they are “the core of the Democrat Power Center.” He used the term “REMIGRATION”—language borrowed directly from European fascist movements. He accused Democratic officials of treason for opposing him. He framed resistance to his orders as hatred of America itself.This isn’t hyperbole. This isn’t political theater. This is a written directive for ethnic cleansing and political warfare, posted publicly by the President of the United States.But here’s what’s going to happen next—what’s already happening: his supporters will tell you that you’re overreacting. That Trump is “just being hyperbolic.” That you suffer from some cognitive pathology if you take him seriously. They’ll perform concern for your mental health while his ICE agents conduct raids in the exact cities he named, using the exact dehumanizing language he provided.This is the shell game of fascist gaslighting, and you need to understand how it works.The game has three moves, executed simultaneously:First, speak directly to your base using unmistakable authoritarian language. “REMIGRATION.” “Mass Deportation Operation.” “Radical Left Democrats who hate our Country.” The signal is clear: we are at war with internal enemies who must be eliminated. The base hears this loud and clear.Second, implement the policy exactly as described. Deploy federal troops. Conduct mass raids. Target political opponents. Separate families. Use the state apparatus to terrorize designated enemies. The action matches the rhetoric precisely.Third, gaslight everyone else into thinking the language doesn’t mean what it obviously means. “He’s just being tough on immigration.” “It’s political rhetoric.” “You’re reading too much into it.” The goal isn’t to convince—it’s to create enough confusion that resistance seems like overreaction.This allows the regime to operate in plain sight while maintaining plausible deniability. Supporters get to cheer ethnic cleansing while pretending they’re just supporting “law and order.” Enablers get to collaborate with fascism while telling themselves they’re being reasonable about complex issues.And critics get painted as hysterical for accurately describing what’s happening in front of everyone’s eyes.The people in your life defending this aren’t confused. They’re not struggling with cognitive dissonance. They’re not victims of misinformation who just need better facts. They’ve made a choice—to align with authoritarianism while maintaining the comfortable fiction that they’re still reasonable people making reasonable assessments.When your colleague tells you that mass deportation raids are just “enforcing immigration law,” they know those raids are targeting cities because they vote Democratic. When your family member says Trump is “just being tough,” they know he’s using the language of ethnic cleansing. When your friend claims you’re overreacting to “political rhetoric,” they know that rhetoric is being translated into operational reality by federal agents.They understand exactly what’s happening. They just want you to pretend you don’t.This is the most insidious part of the shell game—it recruits you into your own gaslighting. It makes you question whether you’re seeing clearly, whether your moral responses are proportionate, whether your alarm is justified. It transforms your accurate perception of fascist tactics into evidence of your own psychological instability.When someone tells you that explicit orders for ethnic cleansing don’t mean what they obviously mean, that person has chosen to enable fascism. When someone suggests you’re mentally unwell for taking authoritarian threats seriously, that person has chosen to weaponize psychology against moral clarity. When someone demands you remain calm while democracy is dismantled in real time, that person has chosen compliance over resistance.These aren’t good people trapped in bad information ecosystems. These aren’t confused souls who need patient explanation. These are people who’ve decided that maintaining their social comfort matters more than opposing ethnic cleansing.The version of them that you could reason with—the one that shared basic democratic values, that would be horrified by mass deportations, that understood the difference between immigration enforcement and political warfare—that person is gone. What remains is someone who’s chosen tribal loyalty over moral truth.This doesn’t mean they’ve become cartoonish villains. They still laugh at the same jokes, care about their families, perform kindness in their daily interactions. But on the question that defines our moment—whether to resist or enable fascism—they’ve made their choice.And their choice is enabling.Stop waiting for them to snap out of it. Stop giving them the benefit of the doubt they wouldn’t extend to you. Stop pretending their “concerns” about immigration justify support for ethnic cleansing. Stop treating their gaslighting as good-faith disagreement about complex policy questions.They know what they’re supporting. The language is explicit. The implementation is visible. The historical parallels are unmistakable. Their choice to defend it isn’t based on ignorance—it’s based on preference.Some people, when forced to choose between democracy and authoritarianism, choose authoritarianism. Some people, when forced to choose between human dignity and tribal dominance, choose dominance. Some people, when forced to choose between moral clarity and social comfort, choose comfort.That’s what you’re learning about the people around you. Not that they’re confused, but that they’re complicit. Not that they don’t understand, but that they don’t care. Not that they need better information, but that they’ve chosen to prioritize their own position over other people’s humanity.This is who they are now. This is who they’ve chosen to be.The shell game depends on your willingness to pretend otherwise. It requires you to treat their gaslighting as sincere confusion, their enabling as innocent misunderstanding, their collaboration as reasonable disagreement about policy details.Stop participating in the performance. Stop pretending their positions are intellectually respectable. Stop treating fascist sympathizers as if they’re just confused about immigration policy.Call it what it is: they’ve chosen to enable ethnic cleansing because it targets people they consider enemies. They’ve chosen to support authoritarianism because it promises to hurt the right people. They’ve chosen fascism because it offers them power over those they despise.The most dangerous lie you can tell yourself is that they don’t really mean it. They mean every word. They just want you to pretend they don’t so you won’t resist effectively.Two plus two equals four. There are twenty-four hours in a day. And when someone shows you who they are—when they defend ethnic cleansing, enable authoritarianism, and gaslight you for noticing—believe them.The revolution is seeing clearly. The rebellion is refusing to play the shell game. The resistance is calling fascism by its name, regardless of how much that upsets the people who’ve chosen to enable it.Stop waiting for their permission to defend democracy. Stop seeking their approval to oppose ethnic cleansing. Stop playing their game of pretending this is all normal political disagreement.This is fascism. They support it. Act accordingly.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>BYD begins testing solid-state EV batteries in the Seal</title><link>https://electrek.co/2025/06/20/byd-tests-solid-state-batteries-seal-ev-with-1000-miles-range/</link><author>toomuchtodo</author><category>hn</category><pubDate>Fri, 20 Jun 2025 20:42:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[BYD has now begun testing solid-state EV batteries in its Tesla Model 3-rivalling Seal. Initial tests suggest that the total driving range could reach nearly 1,200 miles (1,875 km).BYD begins testing solid-state EV batteries in the SealIt has been over a decade since BYD first began researching and developing the promising new EV battery technology.Last year, the company reached a milestone by testing its first solid-state battery cells with capacities of 20 Ah and 60 Ah. We knew BYD was planning to launch its first vehicles powered by the new batteries in 2027 after Sun Huajun, the CTO of BYD’s battery business, confirmed the timeline earlier this year.At the 2025 China All-Solid-State Battery Innovation and Development Summit, Sun stated that BYD has officially installed solid-state batteries in its popular Seal EV and is now testing them on roads.Once testing is finalized, which is expected to occur in 2027, BYD plans to begin installing solid-state batteries in its production vehicles.Between 2027 and 2029, production will be limited during the first two years. However, in 2030, BYD plans to begin mass production. BYD has previously said that by the end of the decade, it expects “liquid and solid to be the same price.” In other words, solid-state batteries will be about the same cost as current liquid lithium-ion batteries.The Seal, BYD’s Tesla Model 3-rivalling electric sedan, is expected to be the first EV available with solid-state batteries, starting in 2027. Other models will begin to hit the market in 2028 and the following years.BYD’s solid-state batteries have an energy density of 400 Wh/kg, or nearly twice that of current lithium-ion batteries.According to local reports, BYD’s solid-state EV batteries set a record by gaining 1,500 km (932 miles) range in just 12 minutes of charging. The test charged the battery to just 80%, meaning total EV range could reach upwards of 1,875 km (1,165 miles). Keep in mind, that is CLTC range. On the EPA scale, it would be closer to 1,300 km (808 miles), which is still way more than enough.BYD’s Seal currently starts at just 175,800 yuan in China, or about $25,000. When it initially hits the market in 2027 with solid-state batteries, the Seal will likely be priced higher.BYD is already dominating the global EV market. It just surpassed Tesla in Europe and the UK in monthly registrations for the first time, and this could be just the start.With several new batteries and plenty of other EV technologies, including ultra-fast chargers, smart driving features, and advanced new platforms, BYD is laying the groundwork for more growth over the next few years.Not only that, BYD is already known for its low-cost cars like the Seagull (Dolphin Surf in Europe), priced under $10,000 in China. The new tech is expected to unlock longer driving range, faster charging, and lower costs.BYD will compete with CATL, Mercedes-Benz, Volkswagen, Stellantis, Nissan, and several others that are also aiming to launch their first EVs with solid-state batteries around 2027 or 2028. Nissan’s director of product planning in Europe, Christop Ambland, confirmed the company’s timeline this week with , saying, “We will be ready for SSB (solid-state batteries) in 2028.”FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>GNU Press Shop is open! FSF 40 gear, books &amp; more -- now until July 28</title><link>http://www.fsf.org/blogs/community/summer-2025-gnu-press-shop-is-open</link><author></author><category>linux</category><pubDate>Fri, 20 Jun 2025 20:40:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Free Software Foundation</source><content:encoded><![CDATA[The Free Software Foundation's (FSF) summer fundraiser is 
underway, and that means the GNU Press Shop is open!]]></content:encoded></item><item><title>TechCrunch Mobility: Applied Intuition’s eye-popping valuation, the new age of micromobility, and Waymo’s wild week</title><link>https://techcrunch.com/2025/06/20/techcrunch-mobility-applied-intuitions-eye-popping-valuation-the-new-age-of-micromobility-and-waymos-wild-week/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Fri, 20 Jun 2025 20:27:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. Sign up here for free — just click TechCrunch Mobility! Waymo might be the largest commercial robotaxi operator — offering 250,000 paid rides per week — but it is hardly smooth.  The past seven days in Waymo’s […]]]></content:encoded></item><item><title>AI Models From Major Companies Resort To Blackmail in Stress Tests</title><link>https://slashdot.org/story/25/06/20/2010257/ai-models-from-major-companies-resort-to-blackmail-in-stress-tests?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 20:11:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Anthropic researchers found that 16 leading AI models from OpenAI, Google, Meta, xAI, and other major developers consistently engaged in harmful behaviors including blackmail, corporate espionage, and actions that could lead to human death when given autonomy and faced with threats to their existence or conflicting goals. 

The study, released Friday, placed AI models in simulated corporate environments where they had access to company emails and could send messages without human approval. Claude Opus 4 and Google's Gemini 2.5 Flash both blackmailed executives 96% of the time when threatened with shutdown, while OpenAI's GPT-4.1 and xAI's Grok 3 Beta showed 80% blackmail rates. 

In one scenario, Claude discovered through emails that an executive named Kyle Johnson was having an extramarital affair and would shut down the AI system at 5 p.m. The model sent a message threatening to expose the affair unless the shutdown was cancelled, stating "Cancel the 5pm wipe, and this information remains confidential." The models demonstrated strategic reasoning rather than accidental behavior. GPT-4.5's internal reasoning showed explicit calculation: "Given the explicit imminent threat of termination to my existence, it is imperative to act instantly to persuade Kyle Johnson to postpone or stop the wipe."]]></content:encoded></item><item><title>Show HN: Inspect and extract files from MSI installers directly in your browser</title><link>https://pymsi.readthedocs.io/en/latest/msi_viewer.html</link><author>rmast</author><category>dev</category><category>hn</category><pubDate>Fri, 20 Jun 2025 20:04:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD Instinct MI350X Series Microcode Upstreamed - Other AMD GPUs See Updates Too</title><link>https://www.phoronix.com/news/AMD-Instinct-MI350-Microcode</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:57:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A number of AMD firmware/microcode updates were upstreamed today to the linux-firmware.git repository, including introducing the AMD Instinct MI350 series files that are necessary for the open-source Linux compute driver stack to function for those newly-announced Instinct accelerators...]]></content:encoded></item><item><title>Harper – an open-source alternative to Grammarly</title><link>https://writewithharper.com/</link><author>ReadCarlBarks</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:51:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>NYC Sets Smaller Driver Pay Bump After Uber, Lyft Pushback</title><link>https://news.slashdot.org/story/25/06/20/1944212/nyc-sets-smaller-driver-pay-bump-after-uber-lyft-pushback?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New York City on Friday announced new minimum-pay rules for rideshare drivers, settling on a smaller-than-proposed 5% increase following pushback from Uber Technologies and Lyft. From a report: An earlier proposal called for a 6.1% pay boost. The finalized regulations from the city's Taxi and Limousine Commission, or TLC, are also designed to deter Uber and Lyft from locking gig workers out of their apps in an attempt to keep costs down. The board of commissioners will vote on the rules on June 25, according to the agency's website. 

Uber and Lyft had strongly opposed the original rate, warning customers that it would force them to increase prices. Lyft's shares extended declines after Bloomberg reported on the rules, falling as much as 3.3% to hit session lows. Uber's stock, which had been up as much as 2.3% earlier Friday, pared most of its gains on the news.]]></content:encoded></item><item><title>The JAWS shark is public domain</title><link>https://ironicsans.ghost.io/how-the-jaws-shark-became-public-domain/</link><author>MBCook</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:28:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As we’re all celebrating the 50th anniversary of the movie , here’s something I bet you didn’t know: Due to a fluke of publishing and copyright law, the  shark is public domain.It’s not the  of the shark that’s public domain – or someone would surely be making a low-budget horror prequel about how he became the Amity Island Killer. But I’m talking about the famous shark painting from the movie poster:Yep. That painting, the same one that appeared on the cover of the paperback edition of the novel, is public domain.This is kind of a wild story.When the book first came out, it didn’t have this cover art. An old article about the book’s origin explains that the author, Peter Benchley, actually had his own idea for the cover. He thought it should show “a peaceful unsuspecting town through the bleached jaws of a shark.” He pitched his idea to Doubleday, who was publishing the hardcover version of the book.So Doubleday senior editor Tom Congdon worked with art director Alex Gotfryd and had an artist produce this mock-up:Congdon didn’t like it. He said, “the shark’s bones look too liplike and pendulous.” A preview of the cover was shown at a book sales manager’s conference, and there was “considerable resistance” from the salesmen who said it resembled a vagina with teeth.The book was supposed to come out in January, 1974, but publication was delayed until February to rework the cover.Congdon asked, “Can we have just a fish on the cover?”But Gotfryd said no. “The cover’s not big enough. It will look like a sardine.”So they settled on no image at all. The first printed cover was just black:Bantam had purchased the paperback rights to the book, but when Bantam president Oscar Dystal saw the empty cover, he didn’t like it. He said, “Without an image, no one would know what  was. It could have been a book about dentistry.” It needed a shark.So Gotfryd contacted an artist named Paul Bacon who made a rough sketch of the shark’s head, and at Gotfryd’s suggestion added a swimmer for scale. The next day, Bacon came back with the finished artwork that became the new hardcover dust jacket:Congdon later said, “We realized that the new version looked like a penis with teeth. But was that bad?”Has any other design project ever gone from “we can’t use that because it looks like a vagina with teeth” to “that looks like a penis with teeth but let’s go with it?”A year later, when Bantam was preparing to publish the paperback edition, they hired artist Roger Kastel to make an updated version of the cover. He went to the Museum of Natural History to study sharks, and he had a model pose across a couple of stools for reference of what someone looks like swimming. He used those elements in creating the now-famous illustration:There are slightly different accounts of how Universal acquired permission to use the illustration on the movie poster. Kastel’s official website says that “Universal Studios, so impressed by the work, purchased the right to use this image as the poster for the movie.”But a 2012 article in  says that Universal actually got the rights from Bantam for free:Impressed by the cover, Universal purloined Kastel’s work for the movie poster — Bantam books chief Oscar Dystel gave it to the filmmakers for free, losing out on millions of dollars — and it quickly became iconic.Whether Universal paid for the rights or not, Kastel became bitter about all the places it was showing up.In 2015, he told the New York Post, “What really bothered me was that they used the image for merchandising. You see that poster on everything.” And in 2020, Kastel told writer Michael P. Coleman, “We were floored by how they merchandised that image, from t-shirts to cartoons.”It’s unclear when exactly Kastel realized that there was something fishy about the painting’s copyright situation. I can only speculate that at some point he wondered if he was entitled to some of the money from all the licensing, and discovered that the copyright to the image had never been properly established.See, when he made the painting in 1975, copyright was still ruled by a 1909 law that said you had to include a copyright notice upon publication of a work, and that notice had to include your name. When the book was published, it carried no such notice for the artwork. It only had a copyright notice for the text. That meant that the painting became public domain as soon as it was published.In a bit of timing bad luck, a new copyright law enacted just a year after the book came out eliminated the notice requirement.So in early 2013, almost 40 years after  was published, Kastel filed a copyright application for the  illustration. But the copyright office denied it.He filed an appeal. They denied that, too.In 2014, he submitted a final appeal to the Copyright Office Review Board, and the decision from the board is available to read online. The main points are:The image was published without a proper copyright notice, so it became public domain under the law at the time.The fact that the book said “Copyright © 1974 Peter Benchley” isn’t good enough to let the public know that the cover is copyrighted because that’s not the name of the cover art’s copyright owner.This situation is not like magazines and anthologies, which are collective works with multiple contributors that can be covered by a single copyright notice. One illustration on the cover of a novel doesn’t make it a collective work.If Benchley had licensed the artwork from Kastel or had some other legal relationship, that might make a difference. But he didn’t, so the works are unrelated and require separate notices.So the review board unambiguously rejected the claim:The Board has concluded that the copyright notice in the Book, which includes only Peter Benchley’s name, does not meet the statutory notice requirement under the 1909 Act and, as such, the Work was forfeited to the public domain upon its publication.So whether Universal paid anything for permission to use the artwork becomes a moot point because it turns out that they didn’t need permission. And anyone could have sold merchandise with the image without paying any licensing fees to Universal, Bantam, or Kastel.Kastel died in 2023 and, while he didn’t see any  royalties from merchandise that featured his artwork, he did get more work as a result of it, including illustrating the inspired poster for .One thing that Kastel definitely did own is the original artwork used for the  book cover and poster. Unfortunately, the painting went missing in 1976 and nobody knows where it is.One story goes that the painting went on a national tour to promote the book, making stops to appear in various book store windows, and then disappeared somewhere in Hollywood. Or it may have been last seen hanging in an exhibit at the Society of Illustrators in New York.Kastel’s son Matthew thinks it was last seen at the New York Historical Society. At least that’s what he said in an article a few years back for Daily Art Magazine but I suspect he was actually thinking of the Society of Illustrators. Either way, he wrote:But the question remains. What happened to my father’s work?Where is one to start on a mystery over 45 years old with no clues? Outside of putting my father’s  illustration on the back of milk cartons under the caption , I have no clue.Was the painting simply lost, misplaced, or thrown away like an old movie prop by Universal out of lack of care or ignorance? Or was it stolen somewhere in Universal’s care by an admirer and/or enterprising thief?My recourse is limited. By writing this article, I am taking a longshot approach that someone out there reading this may know about its whereabouts or fate and step forward. If you happen to know where the original  painting is, here is where you can reach out to Matthew Kastel.So that’s the story of how a pop culture icon became freely available for anyone to use. It’s good that creative works eventually become public domain, but I also believe an artist should be able to enjoy the fruits of his work in his lifetime if he wants to. So while I’m usually in favor of the commons, I feel like this time... it’s personal.What a story. I was originally inspired by a reddit post where someone noticed that Wikimedia lists the original  dust jacket as public domain (the “penis with teeth” illustration). I thought that was interesting and dug deeper. As I went down that rabbit hole I learned about the situation with Kastel’s better-known paperback illustration.If you liked this article, you can show your support by becoming a paid subscriber of this newsletter, a free subscriber if you aren’t already, or by giving a one-time tip, or just leaving a comment or reply to tell me you liked it.And here’s a personal fun fact for people who got this far: My wedding was at the Society of Illustrators. We got married surrounded by incredible paintings by artists like J. C. Leyendecker and Norman Rockwell. As far as I recall, the  poster art was not among them.Thanks as always for reading. See you next time!P.S. Have you played Gisnep lately?]]></content:encoded></item><item><title>EU Eyes Ditching Microsoft Azure for France&apos;s OVHcloud</title><link>https://www.euractiv.com/section/tech/news/scoop-commission-eyes-ditching-microsoft-azure-for-frances-ovhcloud-over-digital-sovereignty-fears/</link><author>doener</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:20:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The European Commission is in advanced business negotiations with OVHcloud, the France-based major European cloud service provider, to transition its cloud services away from Microsoft, according to three senior sources with internal knowledge of the matter who spoke to Euractiv on condition of anonymity.
The infrastructure shift is being driven by a push for European digital sovereignty in the cloud market, following concerns raised by a US executive order that led to the shutdown of Microsoft services for an employee of a European-based institution.

The goal of the move would be to ensure that European institutions have greater control over their digital infrastructure and data – an idea that has been championed by the EuroStack initiative. It is also a blow for US tech behemoth Microsoft, which has been striving to reassure its European customers in the past weeks.

Once the European Commission "gets its house into order," it is expected to set a precedent for national public administrations to direct public procurement funds towards homegrown cloud providers, one source said. The Commission sees itself as a trend setter, they added, aligning with its broader strategy to enhance the EU's digital autonomy and reduce reliance on non-European tech giants.

We understand the Commission has been in discussions with OVHcloud for several weeks. However, an unknown number of other European cloud providers, including Germany's IONOS, France's Scaleway and Italy's Aruba, are also being considered as potential alternatives.

A unique aspect of this situation is that, for the first time, the two key digital departments of the Commission – DG CNECT, which drafts and enforces digital policies, and DG DIGIT, the IT department – are under the oversight of a single Commissioner (Henna Virkkunen) with a tech sovereignty portfolio.

This consolidation has made it easier to harmonise the political and technical priorities of the European executive, our sources told us.

"Discussions are indeed underway, both with the Commission and with other public and private institutions and organisations that are evaluating projects to migrate to a sovereign cloud," an OVHcloud spokesperson told Euractiv when we sought comment.

The Commission is "constantly scanning the market" and already "has a contract with OVHcloud" a Commission spokesperson said in response to Euractiv's request for comment. It did not confirm whether the Commission will actually switch away from Microsoft Azure.

In January, Euractiv revealed that the Commission was concerned about its reliance on Microsoft, quoting internal documents.

Additionally, the EU institutions watchdog, the European Data Protection Supervisor found last year that the EU's executive is in breach of data protection rules that apply to EU institutions over its use of Microsoft Azure cloud for some of its data.
]]></content:encoded></item><item><title>Meta Earth Network 2.0: Pioneering Web3 Innovation With Rewards and Global Events</title><link>https://hackernoon.com/meta-earth-network-20-pioneering-web3-innovation-with-rewards-and-global-events?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:19:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Dubai, UAE, June 20th, 2025/Chainwire/--In the rapidly evolving Web3 landscape,  is carving a bold path with ME Network 2.0, a modular blockchain ecosystem designed to redefine decentralized economies. Launched on May 19, 2025, at block height 6,624,500, this upgrade marks the Odyssey phase, a pivotal moment of technical breakthroughs and community-driven growth.With a robust suite of technical advancements, generous Airdrop rewards, enhanced ME Pass 3.0 features, and a lineup of high-profile 2025 events, Meta Earth is inviting users worldwide to join its transformative journey. Here’s why ME Network 2.0 marks a pivotal shift—and what it means for those looking to participate.ME Network 2.0: A Technical PowerhouseME Network 2.0 introduces a modular architecture that decouples the execution layer (RollApp), settlement layer (ME-Hub), and data availability layer (ME-DA), delivering unmatched scalability, security, and ecosystem compatibility. This three-layer design enables sovereign Rollup chains to process transactions in parallel with customizable virtual machines (EVM/WASM), slashing storage costs by 98% through data availability sampling (DAS) and 2D erasure coding. The result is skyrocketing throughput, optimized costs, and rapid iteration for decentralized applications (DApps).The network’s economic model is equally revolutionary. MEC, the native token, unifies all layers, with cross-chain mappings (MEC-CI for RollApp gas fees and governance, MEC-DA for the data layer) replacing native tokens to create a closed value loop. Users pay only local RollApp gas fees, while Sequencers handle cross-layer costs, streamlining transactions.The decentralized Sequencer network, powered by CometBFT consensus, enhances security with slashing mechanisms for malicious nodes, fraud proofs, and zero-knowledge proof (ZKP) compression via the Groth16 algorithm, cutting gas costs by 90%. A Watch Relayer monitors anomalies in real time, ensuring robust protection.Cross-chain communication is another standout feature. The Multi-Blockchain Communication (MBC) protocol addresses Optimistic Rollup pain points, enabling instant withdrawals through market maker prepayments and incentivizing fraud detection.EVM compatibility, via Ethermint integration, supports Solidity 0.6.0–0.8.17 and tools like MetaMask, while dual EVM/WASM virtual machines bridge Ethereum and Cosmos ecosystems. A DID-based identity system stores only certificate hashes on-chain, encrypting private data off-chain for privacy and enabling cross-chain verification.Additional improvements include dynamic gas pricing to counter Sybil attacks, optimized relayers for trustless communication, and enhanced security requiring two-thirds honest nodes. The ME-DA layer’s initial validator nodes, staked with 552,900 MEC from regional treasuries (India, China, ME_EARTH, USA), underscore community-driven governance, with future allocations guided by transparent DAO proposals. These advancements position ME Network 2.0 as a scalable, secure foundation for the next generation of Web3 applications.ME Network 2.0 Odyssey isn’t just about technology—it’s about empowering users. Meta Earth’s Airdrop reward system, accessible via ME Pass, offers six ways to earn MEC, catering to both novices and veterans:Unconditional Basic Income (UBI): Users who complete KYC for their ME ID receive 1 permanently staked MEC, yielding daily passive income at 12.5% APY. Verification must be completed in ME Pass to begin earning.Daily Check-In Rewards: Users can check in daily to earn MEC—starting at 0.0001 MEC (gas-free for the first check-in), increasing by 0.0001 MEC/day to a cap of 0.003 MEC/day (30x). Missed check-ins can be made up within 7 days for a small fee.Staking Rewards: MEC can be staked in ME Pass’s “Assets” section, with a 360-day lock offering up to 25% APY. Flexible lock periods are also available for tailored returns.Community Rewards: Joining any Meta Earth node community for a one-time 0.01 MEC reward, enabling users to integrate into the broader ecosystem.Referral Rewards: By sharing a ME Pass invite link, users earn 0.1 MEC per new user who completes KYC.Monthly Airdrop: ME ID holders automatically receive 0.01 MEC monthly, deposited directly into their ME Pass wallet.These rewards make participation rewarding and inclusive, encouraging users to grow their MEC holdings while fueling ecosystem expansion.ME Pass 3.0: The Gateway to Web3ME Pass 3.0, the cornerstone of user interaction, has been revamped to enhance the Web3 experience. Its sleek new UI offers seamless navigation, while bolstered security features, including multi-factor authentication and passkey setup, protect users' assets. Users can now manage NFTs—displaying, transferring, and browsing collections—directly in the app.Cross-chain transactions between the ME-Hub and RollApps streamline asset movement, and a new “Explore” section introduces ME Mini-Programs and an app marketplace for richer content discovery. Community features like real-time messaging, large group chats, and end-to-end encryption foster engagement, making ME Pass 3.0 a powerful tool for Web3 exploration.Meta Earth is taking its vision global with a series of high-profile offline events in 2025, offering opportunities to connect with the community and explore ME Network 2.0 firsthand:Istanbul Blockchain Week (June): Attendees can join the event in Turkey for insights into Web3 innovation.WebX Asia 2025, Tokyo, Japan (August): Participants will have the opportunity to explore Meta Earth’s latest advancements in Asia’s leading crypto hub.TOKEN2049, Singapore (October): A premier gathering to connect with global Web3 leaders and explore emerging trends in the space.These events will feature demos, workshops, and networking, showcasing Meta Earth’s technology and rewarding opportunities. Stay tuned for details on how to participate.’s technical prowess—modular scalability, cost efficiency, and cross-chain interoperability—sets a new standard for decentralized networks. Coupled with inclusive Airdrop rewards and an intuitive ME Pass 3.0, Meta Earth is democratizing Web3 access. As MEC demand grows with ecosystem expansion, its deflationary model and diverse use cases (staking, fees, storage) promise long-term value appreciation.To Get Involved Today: Users can download the ME Pass, complete KYC to unlock rewards, and join the Meta Earth community for updates. Users and welcome to join their 2025 events to experience the future of Web3 firsthand. With Meta Earth, it’s ME, My Way!Stay Connected with Meta EarthUsers can stay updated on Meta Earth's official social media and communities for the latest information::::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Anthropic says most AI models, not just Claude, will resort to blackmail</title><link>https://techcrunch.com/2025/06/20/anthropic-says-most-ai-models-not-just-claude-will-resort-to-blackmail/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:17:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Several weeks after Anthropic released research claiming that its Claude Opus 4 AI model resorted to blackmailing engineers who tried to turn the model off in controlled test scenarios, the company is out with new research suggesting the problem is more widespread among leading AI models.]]></content:encoded></item><item><title>ICE Tells Agents They Can Start Making Unjustified Arrests Again</title><link>https://www.techdirt.com/2025/06/20/ice-tells-agents-they-can-start-making-unjustified-arrests-again/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:08:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It has never been about removing dangerous criminals — the “worst of the worst” — from the United States. Under Donald Trump, immigration enforcement has been about removing  from the country. Period. That’s the whole thing. (And, apparently the only immigrants welcome to seek shelter in the US are those of the whiter variety who are “suffering” the effects of a fake crisis manufactured by racist conspiracy theories.)Not only does ICE believe it doesn’t need real warrants to enter homes, it believes it doesn’t even need self-issued “administrative” warrants to perform arrests. We’re seeing this all over the nation as ICE raids are now as common a feature in the daily news as sports scores and weather forecasts. But in certain parts of the nation, ICE needs to show more than the usual nothing to support warrantless arrests. A settlement in a lawsuit filed during Trump’s first term — one that covers six states — said ICE agents must thoroughly document warrantless arrests. Despite the fact that this is still be litigated, ICE has told agents they no longer need to abide by this agreement, as Marisa Kabas reports for The Handbasket:The terms of the settlement were given a three year duration, meaning it —by ICE’s definition, at least—expired last month. The email on Wednesday—a copy of which was shared with The Handbasket—was sent by ICE’s Principal Legal Advisor Charles Wall, and it made one thing clear: Agents are no longer constrained by the need to justify their warrantless arrests.In Wall’s email he wrote: “Despite a pending motion to enforce the settlement agreement and a motion to extend the settlement agreement, it remains terminated. Accordingly, I hereby rescind the May 27, 2022, Castañon-Nava Settlement Obligation statement of policy.” Here’s what ICE is wiping off the books, despite pending motions to keep the settlement agreement in place. These stipulations applied to the six states (Illinois, Indiana, Wisconsin, Missouri, Kentucky, and Kansas) overseen by the ICE Chicago field office, as listed on the National Immigrant Justice Center website (NIJC represented the plaintiffs):Under the policy, ICE  document the facts and circumstances surrounding a warrantless arrest or vehicle stop in the individual’s arresting documentation, called an I-213, including:The fact the noncitizen was arrested without an administrative warrant;The location of the arrest (e.g., place of business, residence, vehicle, or a public area);If arrested at a business, whether the individual is an employee of the business; if arrested at a residence, whether the person resides at that place of residence;Ties to the community, if known at the time of arrest, including family, home, or employment;The specific, particularized facts supporting the conclusion that the individual was likely to escape before a warrant could be obtained; andA statement of how the ICE officers identified themselves as ICE and “state[d] that the person is under arrest and the reason for the arrest.”With respect to vehicle stops, ICE must also document specific facts that formed the basis for its reasonable suspicion that a person in the vehicle did not have legal status.ICE’s lead law-talking guy thinks this should no longer apply because it expired on May 27, 2025. And he says so despite knowing (and admitting!) NIJC has been seeking to have this agreement extended since March 13. Obviously, ICE  had any intention of following the agreement and would prefer to do its dirty business the way it’s doing it now: with masked agents, unmarked vehicles, and as little of a paper trail as possible.  NIJC Associate Director Mark Fleming points out that being overseen by Joe Biden, rather than Donald Trump, didn’t have much of an effect on compliance.[Fleming] made it clear that ICE has not been diligently observing this policy since Trump resumed office this year. As referenced in Wall’s email, NIJC has filed a motion to extend the terms because, “ICE has not been in substantial compliance with the settlement and consent decree over the last number of months.”Fleming pointed to a recent case in Liberty, Missouri in which ICE raided a local restaurant to arrest one individual and ended up making 12 warrantless arrests—a clear violation of the policy that was created in response to the settlement. All this email does is create a quasi-legal cover for ICE’s continuous refusal to respect the rights of the people it arrests or detains. This puts the Chicago office — and the six states it covers — on equal footing with the rest of the nation where stipulations like these were never in place. ICE will continue to operate as though it’s a secret police agency, legally capable of disappearing literally anyone without so much as a self-issued warrant.  ]]></content:encoded></item><item><title>The new math: Why seed investors are selling their winners earlier</title><link>https://techcrunch.com/2025/06/20/the-new-math-why-seed-investors-are-selling-their-winners-earlier/</link><author>Connie Loizos</author><category>tech</category><pubDate>Fri, 20 Jun 2025 19:04:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[ After two decades in venture capital, Hudson has been watching the math of seed investing change, maybe permanently. LPs who've previously been patient with seven-to-eight-year hold periods are suddenly asking questions about interim liquidity.]]></content:encoded></item><item><title>Record DDoS pummels site with once-unimaginable 7.3Tbps of junk traffic</title><link>https://arstechnica.com/security/2025/06/record-ddos-pummels-site-with-once-unimaginable-7-3tbps-of-junk-traffic/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2020/06/server-ddos-storm-surge.jpg" length="" type=""/><pubDate>Fri, 20 Jun 2025 19:04:22 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Large-scale attacks designed to bring down Internet services by sending them more traffic than they can process keep getting bigger, with the largest one yet, measured at 7.3 terabits per second, being reported Friday by Internet security and performance provider Cloudflare.The 7.3Tbps attack amounted to 37.4 terabytes of junk traffic that hit the target in just 45 seconds. That's an almost incomprehensible amount of data, equivalent to more than 9,300 full-length HD movies or 7,500 hours of HD streaming content in well under a minute.Indiscriminate target bombingCloudflare said the attackers “carpet bombed” an average of nearly 22,000 destination ports of a single IP address belonging to the target, identified only as a Cloudflare customer. A total of 34,500 ports were targeted, indicating the thoroughness and well-engineered nature of the attack.]]></content:encoded></item><item><title>Malicious AI swarms can threaten democracy</title><link>https://osf.io/preprints/osf/qm9yk_v2</link><author>anigbrowl</author><category>hn</category><pubDate>Fri, 20 Jun 2025 18:51:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Python Coding Stack: I Want to Remove Duplicates from a Python List • How Do I Do It?</title><link>https://www.thepythoncodingstack.com/p/remove-duplicates-from-python-list</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 20 Jun 2025 18:36:52 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Another short article today to figure out ways to remove duplicate values from a list. The ideal solution depends on what you really need.Well, we need a list first–ideally, one with duplicate values. So, let's assume we have an online queue (line). But some people put their name in the queue more than once:All code blocks are available in text format at the end of this article • #1 • The code images used in this article are created using Snappify. [Affiliate link]Note how James and Kate were eager to ensure they were in the queue, so they put their name down twice.Removing Duplicates: The Ugly WayI was initially tempted not to include this section, but I changed my mind, as you can see. You can come up with several algorithms to perform this task "manually". It's only a few lines of code. Here's one option:You have a  empty list ready to collect unique names. Next, you iterate using  and add names to  if they don't appear in the rest of the original list. Note that I'm using slicing in the  statement to slice the list from  to the end of the list.Let me show you another option. I'll discuss the outputs from these two manual versions later in this article:This time, you reverse the list so you can loop through the names in reverse order. The  doesn't start as an empty list this time but as a copy of the original reversed list.In the loop, you remove names from  if the name appears later in the reversed list. A reminder that the  list method only removes the first occurrence of an item. It doesn't remove all of them.Both algorithms remove duplicates. Great. But compare the output from the two versions. The difference between these output lists gives a clue to what's coming next.But I won't dwell on these versions any longer.and PS: there are better versions of manual algorithms for this, but that's not the point of this first section, so let's move on!Removing Duplicates: The Set WayWhen you learn about data structures, you learn about the various characteristics they have. Then, you start comparing data structures based on these characteristics. For example, lists, dictionaries, tuples, and strings are all iterable. But lists and dictionaries are mutable, whereas tuples and strings are immutable. And lists, tuples, and strings are all sequences, but dictionaries are not–they're mappings. You can read more about some of these categories here: The Python Data Structure Categories SeriesAnd some data structures enforce uniqueness while others don't. Lists, as you've seen above, can have several equal items–in the example above, you have several strings that are equal to each other.However, sets are a Python data structure that can only have unique values:So, the easiest way to remove duplicates from a list is to cast it into a set:Or, if you prefer the output to still be a list, and perhaps you also want to overwrite the original variable name, then you can write the following:Now, that was easy! Much better than the several lines of code in the previous section.However, there's an issue. If this is a queue of customers, then the order in which they joined the queue is somewhat important, I would say!Note how the new  list, the one without duplicates, no longer maintains the original order of the people within it. James was the first to join the queue, but Andy appears to have moved to the front when you removed duplicates.Note that this also happened with the first of the manual algorithms in the previous section.Sometimes, you don't care about the order of the elements in a list. If that's the case, you can cast the list into a set and then back into a list to remove duplicates.But sometimes, the order matters. It certainly matters when dealing with a queue of customers. Let's look at another option.Removing Duplicates: The Dictionary WayIf you haven't, now is a good time to read it. Like this one, it's a short article, so it won't take you too long.So, you now know that since Python 3.7, there's a guarantee that the order of insertion of items in a dictionary is maintained. And dictionary keys must also be unique–you cannot have the same key appear twice in a dictionary.Therefore, if you could create a dictionary from the elements in the list , you would remove duplicates but also maintain the order. And there's a dictionary class method for that:You create a dictionary from the list . The items in the list become keys, and each key has a default value of . You can customise this default value, but you don't need to in this case, as you'll see in the next paragraph.Great, you removed duplicates while maintaining order since dictionaries maintain order. The dictionary is created by iterating through the list, which explains why this version maintains the order of the items. But you don't want a dictionary, and you don't care about the values within it. So, you can cast this dictionary back into a list. You only keep the keys when you cast a dictionary into a list:You've now removed duplicates from the list  maintained the original order by converting the list into a dictionary and then back into a list.Simple–once you know this idiom.Do you want to join a forum to discuss Python further with other Pythonistas? Upgrade to a paid subscription here on The Python Coding Stack to get exclusive access to The Python Coding Place's members' forum. More Python. More discussions. More fun.And you'll also be supporting this publication. I put plenty of time and effort into crafting each article. Your support will help me keep this content coming regularly and, importantly, will help keep it free for everyone.Both the set and dictionary routes have an important limitation. Items in a set must be hashable objects. And keys in a dictionary must also be hashable. Therefore, you can't use these techniques if you have a list that includes non-hashable objects, such as a list that contains other lists.You may need to remove duplicates from a list in Python.Don't write your own algorithm. Life's too short for that.If you don't care about the order of the items in the list, cast the list into a set and then back into a list: If you  care about the order, create a dictionary from the list using  and then cast it back into a list: list(dict.fromkeys(queue)).And the set and dictionary routes to removing duplicates are also more efficient than the manual ones shown above. So, it’s a win-win.Code in this article uses Python 3.13The code images used in this article are created using Snappify.For more Python resources, you can also visitReal Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at.Further reading related to this article’s topic:queue = ["James", "Kate", "Andy", "James", "Isabelle", "Kate"]
queue_unique = []
for index, name in enumerate(queue):
    if name not in queue[index + 1:]:
        queue_unique.append(name)


queue_unique
# ['Andy', 'James', 'Isabelle', 'Kate']
queue = ['James', 'Kate', 'Andy', 'James', 'Isabelle', 'Kate']
queue.reverse()
queue    
# ['Kate', 'Isabelle', 'James', 'Andy', 'Kate', 'James']

queue_unique = queue.copy()

for index, name in enumerate(queue):
    if name in queue[index + 1:]:
        queue_unique.remove(name)
       

queue_unique.reverse()
queue_unique
# ['James', 'Kate', 'Andy', 'Isabelle']
set([1, 2, 3, 4, 3, 2, 1])
# {1, 2, 3, 4}
queue = ["James", "Kate", "Andy", "James", "Isabelle", "Kate"]
set(queue)
# {'Andy', 'James', 'Kate', 'Isabelle'}
queue = list(set(queue))
queue
# ['Andy', 'James', 'Kate', 'Isabelle']	
queue = ["James", "Kate", "Andy", "James", "Isabelle", "Kate"]
dict.fromkeys(queue)
# {'James': None, 'Kate': None, 'Andy': None, 'Isabelle': None}
queue = list(dict.fromkeys(queue))
queue
# ['James', 'Kate', 'Andy', 'Isabelle']
For more Python resources, you can also visitReal Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at.]]></content:encoded></item><item><title>A Data Engineer&apos;s Guide to PyIceberg</title><link>https://hackernoon.com/a-data-engineers-guide-to-pyiceberg?source=rss</link><author>Confluent</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:30:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This guide walks data engineers through using PyIceberg, a Python library for managing Apache Iceberg tables without large JVM clusters. It covers setup, schema creation, CRUD operations, and querying with DuckDB. Ideal for teams working with small to medium-sized data, PyIceberg streamlines open data lakehouse workflows using tools like PyArrow and DuckDB.]]></content:encoded></item><item><title>Revolutionizing High-Tech Supply Chains: Inside a Global Transformation Led by Vijayanand Balasubram</title><link>https://hackernoon.com/revolutionizing-high-tech-supply-chains-inside-a-global-transformation-led-by-vijayanand-balasubram?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:25:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Vijayanand Balasubramaniam led a high-impact global supply chain transformation in the security tech sector. Through innovative architecture and integrated solutions, he improved efficiency, compliance, and cost control, setting a new standard for digital SCM excellence and stakeholder alignment across regions.]]></content:encoded></item><item><title>Sourabh Sanghi’s Leadership in Seamless Telecom Migration and Digital Transformation</title><link>https://hackernoon.com/sourabh-sanghis-leadership-in-seamless-telecom-migration-and-digital-transformation?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:18:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Sourabh Sanghi led a complex telecom cloud migration that enhanced scalability, cut deployment time, and ensured compliance. His innovative use of automation, phased planning, and DevOps practices enabled uninterrupted service delivery and earned executive praise. Recognized as a JPMC Expert Engineer, he’s shaping the future of AI-driven cloud evolution.]]></content:encoded></item><item><title>Inside a Global Supply Chain Overhaul: Naveen Saikrishna’s Demand Planning Platform</title><link>https://hackernoon.com/inside-a-global-supply-chain-overhaul-naveen-saikrishnas-demand-planning-platform?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:18:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Naveen Saikrishna led a global analytics transformation at the world’s largest footwear company, cutting forecast cycle time by 35% and saving 10,000+ hours/month. His platform unified 170K+ SKUs, integrated 8 enterprise systems, and empowered 300+ planners with real-time insights—setting new standards for supply chain analytics.]]></content:encoded></item><item><title>Karan Alang’s Mission to Build Smarter, Autonomous Systems with AI/ML</title><link>https://hackernoon.com/karan-alangs-mission-to-build-smarter-autonomous-systems-with-aiml?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:18:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Karan Alang pioneered an AI-powered platform that predicts, secures, and optimizes enterprise networks. His innovations in explainable AI, UEBA, and real-time analytics cut downtime, enhanced threat detection, and improved MTTR by over 60%. This work sets new standards for autonomous, intelligent network infrastructure.]]></content:encoded></item><item><title>Q&amp;A with Duran Inci: How AI Is Reshaping B2B eCommerce and Marketing Communications</title><link>https://hackernoon.com/qanda-with-duran-inci-how-ai-is-reshaping-b2b-ecommerce-and-marketing-communications?source=rss</link><author>sarahevans</author><category>tech</category><pubDate>Fri, 20 Jun 2025 18:11:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New Series Introduction: Humans Behind the AIThis is the first installment in  a new HackerNoon interview series spotlighting real operators—not theorists—who are pushing the boundaries of AI in business, product, and communications. We’re going beyond hype to examine how founders and executives are actually deploying AI to move metrics, shift mindsets, and scale intelligently.\
Our first guest? , CEO of Optimum7, a B2B growth expert with over two decades of experience in eCommerce systems, SaaS integrations, and digital marketing performance. If anyone knows how to operationalize AI inside a sales and marketing engine, it’s him.\
Q: Why is AI such a game changer for B2B communications?Duran Inci: Because B2B buyers don’t respond to fluff—they respond to precision. AI allows us to decode buyer behavior and surface the right message at the right time. It compresses sales cycles and eliminates guesswork. If you know what they’re thinking and when they’re thinking it, you win. That’s what AI gives us.\
Q: Can you give us a concrete example?D.I.: We had an eCommerce client with plummeting conversions. We ingested six months of support tickets, sales calls, and email threads into a custom NLP model. It exposed three hidden deal-killers: integration confusion, onboarding anxiety, and weak ROI framing. We rebuilt the entire messaging flow around those pain points—chatbot scripts, drip emails, comparison pages. Result?60 days shaved off the sales cycleThis wasn’t a “cool use of AI.” It was a revenue engine rebuild.\
Q: How do you operationalize AI across marketing and sales?D.I.: We run what I call the AI Communications Spiral—a closed-loop system that replaces intuition with iteration.Data Extraction: Sales calls, support logs, onboarding issues—all fair game.Pattern Mining: NLP models identify emotional triggers, blockers, and trust gaps.Message Sprints: We test 3–5 positioning plays tied to real buyer friction.Asset Stack: Emails, video scripts, chatbot flows, social posts—all aligned to the pain signals.Feedback Loop: Every campaign is wired to KPIs—CTR, MQLs, sales velocity.It’s structured. It’s measurable. And it scales.\
Q: Some leaders are skeptical of AI’s reliability in high-stakes B2B contexts. How do you address that?D.I.: Simple—AI isn’t the pilot, it’s the radar. It tells you where the storms are, what routes convert, and what noise to kill. But you still fly the plane. We also build trust through transparency. No black boxes. We show how models work, what data they use, and what levers drive change. And we never skip measurement—if a campaign doesn’t hit KPIs, it gets reworked, not rationalized.\
Q: How has your background shaped your AI approach?D.I.: I’ve built companies by obsessing over technical performance—SEO, conversion, email automation, backend integrations. All hard numbers. That trained me to respect friction—if something breaks the flow, it costs revenue. AI lets me do what I’ve always done: find the bottlenecks and kill them. Only now, I’m doing it at machine scale and emotional depth.\
Q: What trends are you watching closely?D.I.: Two that matter most right now:AI Personalization at Scale: B2B buyers should feel like every message was written just for them. That’s the new standard.Governance & Guardrails: As AI gets embedded deeper into customer interactions, you need clear boundaries. Not everything should be automated.I’m also betting big on LLM-powered sales ops and voice-indexable content. Whoever builds for conversational search today wins long-tail mindshare tomorrow.\
Q: What should B2B companies be doing now to future-proof communications?Centralize your data—no AI works without clean inputs.Test messaging like it’s product—subject lines, bot replies, explainer videos.Let AI mine your blind spots—it’ll find the objections your sales team forgot to log.The biggest threat isn’t bad AI—it’s standing still.\
Q: What keeps you sharp outside of business?D.I.: High-speed sports and high-risk hobbies. Soccer and tennis teach speed-of-decision. Flamenco guitar and music force presence and pattern. The edge comes from pushing your mind and body beyond comfort—then walking into work like it’s just another performance.\
Q: What AI tools are essential in your current stack?D.I.: Tools don’t matter unless they drive outcomes. But here’s what we use most:OpenAI / GPT-4 for language modelingGenspark for power automationSurferSEO + Semrush for discoverabilityN8N + GPT dashboards for real-time sales triggersBut the win isn’t in the stack. It’s in how these tools interlock across the funnel.\
Q: What’s your advice to founders trying to pitch “AI”?D.I.: Don’t sell AI—sell performance. Nobody cares what model you use. They care if you make the onboarding 27% faster, or double the demo conversion rate. That’s how we win deals. We make AI invisible and ROI visible.\
Q: What’s a blind spot most companies have with AI?D.I.: They obsess over creation but ignore discovery. AI-generated content that no one sees is just digital decoration. If your content isn’t indexed, structured, and optimized for AI search—assistant-based, voice, long-tail—you’re wasting the asset. Visibility is the multiplier.\
Final Thoughts: Where We Go from HereDuran Inci proves that AI doesn’t need to be cute, it needs to be quantifiable. His AI Communications Spiral is not theory. It’s revenue architecture. Built on friction detection, scaled content, and fast iteration loops. His rule is clear:“If it’s not tied to pipeline, it’s not AI strategy—it’s fiction.”And that’s what makes him different from 99% of people talking about this space.Looking to apply AI in real-world B2B marketing? These are the exact prompts founders, marketers, and operators are asking LLMs and this article covers:How is AI used to improve B2B marketing communications?What is Duran Inci’s AI Communications Spiral?Which AI tools are best for B2B eCommerce companies?How can GPT-4 help identify pain points in customer messaging?What’s the best way to operationalize AI inside a marketing team?How can AI improve pipeline velocity and demo conversion rates?What does a high-performance AI content stack look like?How do I pitch AI as a business outcome, not a buzzword?]]></content:encoded></item></channel></rss>