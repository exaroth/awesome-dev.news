<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Latest</title><link>https://www.awesome-dev.news</link><description></description><item><title>Linux 6.17 Adds Fan &amp; Thermal Profile Support For HP Victus 16-r1000 Gaming Laptops</title><link>https://www.phoronix.com/news/HP-Victus-16-r1000-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 23 Aug 2025 10:37:24 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those that happen to have an HP Victus 16-r1000 gaming laptop or considering one with the 14th Gen Intel Core CPUs and NVIDIA GeForce RTX 40 graphics, the Linux support is now slightly better off thanks to the HP-WMI driver adding fan and thermal profile support for the Victus 16-r1000 series...]]></content:encoded></item><item><title>Google Prepares Chrome Field Trial For Accelerated Video Decode On Wayland</title><link>https://www.phoronix.com/news/Chrome-Wayland-Decode-Field</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 23 Aug 2025 10:20:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Google is preparing a field trial with the Chrome web browser of accelerated video decoding under Wayland/Linux as a step toward rolling it out by default for a better video playback experience on the Linux desktop with Wayland-based environments...]]></content:encoded></item><item><title>Nouveau Driver Receives Patch For GPU Reclocking With The Pascal GP10B</title><link>https://www.phoronix.com/news/Nouveau-GP10B-Reclocking</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 23 Aug 2025 10:08:38 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Nearly a decade after the Tegra X2 SoC shipped in the likes of the Jetson TX2, the Pascal-based GP10B GPU has received a patch for allowing GPU re-clocking within the open-source Nouveau driver...]]></content:encoded></item><item><title>LibreOffice 25.8 Slams the Door On Windows 7 and 8.x</title><link>https://tech.slashdot.org/story/25/08/23/0124202/libreoffice-258-slams-the-door-on-windows-7-and-8x?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 23 Aug 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: LibreOffice 25.8 has landed, and while it packs in new features and speed improvements, the biggest headline is who just got left behind. If you are still running Windows 7 or Windows 8/8.1, this is the end of the road. LibreOffice will not run on those systems anymore, and there are no workarounds. The suite has slammed the door shut.
 
For years, LibreOffice kept older Windows users afloat while Microsoft and other developers moved on. That lifeline is gone. Anyone stubbornly clinging to Windows 7 or 8 now has two choices: upgrade or stay stuck on outdated software. LibreOffice has made it clear that it will not carry dead platforms any further. And the cuts do not stop there. 32-bit Windows builds are on their way out, with deprecation already in place. On the Mac side, 25.8 is the last release that runs on macOS 10.15. Starting with LibreOffice 26.2, only macOS 11 and newer will be supported. In other words, if your computer is too old to run modern systems, LibreOffice is walking away.]]></content:encoded></item><item><title>KDE Plasma 6.5 Introducing &quot;KISS&quot; - An Initial System Setup Wizard</title><link>https://www.phoronix.com/news/KDE-KISS-Initial-Setup-Wizard</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 23 Aug 2025 09:33:25 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE developer Nate Graham is out with his customary weekend blog post to summarize all the interesting Plasma desktop developments for the week. Most exciting in recent days is Plasma 6.5 receiving an initial system setup wizard...]]></content:encoded></item><item><title>US Is Throwing Away the Critical Minerals It Needs, Analysis Shows</title><link>https://news.slashdot.org/story/25/08/23/0120237/us-is-throwing-away-the-critical-minerals-it-needs-analysis-shows?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 23 Aug 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right shares a report from Phys.org: All the critical minerals the U.S. needs annually for energy, defense and technology applications are already being mined at existing U.S. facilities, according to a new analysis published in the journal Science. The catch? These minerals, such as cobalt, lithium, gallium and rare earth elements like neodymium and yttrium, are currently being discarded as tailings of other mineral streams like gold and zinc, said Elizabeth Holley, associate professor of mining engineering at Colorado School of Mines and lead author of the new paper.
 
To conduct the analysis, Holley and her team built a database of annual production from federally permitted metal mines in the U.S. They used a statistical resampling technique to pair these data with the geochemical concentrations of critical minerals in ores, recently compiled by the U.S. Geological Survey, Geoscience Australia and the Geologic Survey of Canada. Using this approach, Holley's team was able to estimate the quantities of critical minerals being mined and processed every year at U.S. metal mines but not being recovered. Instead, these valuable minerals are ending up as discarded tailings that must be stored and monitored to prevent environmental contamination.
 
The analysis looks at a total of 70 elements used in applications ranging from consumer electronics like cell phones to medical devices to satellites to renewable energy to fighter jets and shows that unrecovered byproducts from other U.S. mines could meet the demand for all but two -- platinum and palladium. Among the elements included in the analysis are:
- Cobalt (Co): The lustrous bluish-gray metal, a key component in electric car batteries, is a byproduct of nickel and copper mining. Recovering less than 10% of the cobalt currently being mined and processed but not recovered would be more than enough to fuel the entire U.S. battery market.
- Germanium (Ge): The brittle silvery-white semi-metal used for electronics and infrared optics, including sensors on missiles and defense satellites, is present in zinc and molybdenum mines. If the U.S. recovered less than 1% of the germanium currently mined and processed but not recovered from U.S. mines, it would not have to import any germanium to meet industry needs.]]></content:encoded></item><item><title>I&apos;m too dumb for Zig&apos;s new IO interface</title><link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link><author>begoon</author><category>hn</category><pubDate>Sat, 23 Aug 2025 06:39:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like this performance issue that I opened. And it relied on a mix of types, which always confused me, and a lot of  - which is generally great, but a poor foundation to build an interface on.I've been slowly upgrading my libraries, and I ran into changes to the  client used by my smtp library. For the life of me, I just don't understand how it works.Zig has never been known for its documentation, but if we look at the documentation for , we'll find:input output options InitErrorClient
Initiates a TLS handshake  establishes a TLSv1 TLSv1 sessionSo it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a , but  does expose a  and  method, so that's probably a good place to start: stream  stdnetallocator stream writer  stream reader  stream tls_client  stdcryptotlsClient
  readerwriterinterfaceNote that  returns a  and  returns a  - those aren't the types our  expects. To convert the  to an , we need to call its  method. To get a  from an , we need the address of its  field. This doesn't seem particularly consistent. Don't forget that the  and  need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of . In a real word example, I think it means that I'll always have to wrap the  into my own heap-allocated type; giving the writer and reader have a cozy stable home.Speaking of allocations, you might have noticed that  and  take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation  tell me these need to be at least std.crypto.tls.max_ciphertext_record_len large, so we need to fix things a bit: write_buf writer  streamwrite_buf read_buf reader  streamread_bufHere's where the code stands:  std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf tls_client  stdcryptotlsClient
      readerwriterinterface tls_clientBut if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a  and a . Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while  and  are passed another.Let's give it what it wants AND send some data: bundle  bundleallocator bundleallocator tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer  tls_client tls_clientwriterNow, if I try to run it, the program just hangs. I don't know what  is, but I know Zig now loves buffers, so let's try to give it something: write_buf2 tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer write_buf2 tls_client tls_clientwriterGreat, now the code doesn't hang, all we need to do is read the response.  exposes a  field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not  doesn't have a  method. It has a  a , a  (which seems close, but it blocks until the provided buffer is full), a  and a lot more, but nothing like the  I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer: buf wbuf n  tls_clientreaderwbuflen
stddebugn bufnIf we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also  to provide a .Here's my current version (it doesn't work, but it doesn't crash!): std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf bundle  bundleallocator bundleallocator write_buf2 read_buf2 tls_client  stdcryptotlsClient
      readerwriterinterfaceca bundle  bundlehost explicit read_buffer read_buf2write_buffer write_buf2 tls_client tls_clientwriter buf wbuf n  tls_clientreaderwbuflen
  stddebugn bufnWhen I looked through Zig's source code, there's only one place using . It helped to get me where where I am. I couldn't find any tests.I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find  - the renamed version of . Maybe there's a helper like: tls.Client.init(allocator, stream) somewhere. And maybe it makes sense that we do  but  - I'm reminded of Go's  and . And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a  function on , but at this point I wouldn't bet on me.]]></content:encoded></item><item><title>The TechBeat: How SocialFi Crowdfunding is Replacing VCs in Crypto: Interview with SeedList Co-Founder (8/23/2025)</title><link>https://hackernoon.com/8-23-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 23 Aug 2025 06:11:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @oxylabs [ 7 Min read ] 
 Discover the 8 best AI web scrapers of 2025, from Oxylabs to Octoparse, with features, pricing, and use cases for every skill level and business need. Read More.By @qryptonic [ 6 Min read ] 
 Tea's Firebase misconfiguration exposed 1.1M sensitive messages.  Read More.By @timonovid_ir5em1fo [ 11 Min read ] 
 Practical guide to automating Vertex AI Workbench on GCP with Terraform modules and GitLab CI—standardized configs, cost savings, and compliance-friendly workfl Read More.By @nileshbh [ 31 Min read ] 
 LM Cache boosts LLM efficiency, scalability, and cost savings by letting the system remember previous outputs and complementing other optimizations. Read More.By @MichaelJerlis [ 4 Min read ] 
 EMCD CEO Evgeny Kitkin talks about how the market has evolved and how the company entered the market. Read More.By @beldexcoin [ 5 Min read ] 
 This article delves into the Beldex Name Service (BNS), a decentralized domain name service on the Beldex blockchain network. Read More.By @noda [ 4 Min read ] 
 Why QA metrics should go beyond bugs: how integrating customer satisfaction scores helps deliver truly high-quality products. Read More.By @zakyboy69 [ 5 Min read ] 
 Discover the best NotebookLM alternatives in 2025. Compare 6 AI-powered research and note-taking tools to boost productivity, learning, and workflow. Read More.By @romavm [ 2 Min read ] 
 I used Kiro.dev for 5 days to complete my hackathon project (analyzing GitHub repositories). Read More.By @neyrox [ 4 Min read ] 
 Microservices are not a silver bullet. They’re useful for huge projects and teams — but the monolith is not obsolete and is not technical debt by default. Read More.By @deniseholt [ 16 Min read ] 
 Robots evolve from rigid tools to adaptive teammates with VERSES AI’s Active Inference, enabling real-time reasoning, safety, and collaboration. Read More.By @hacker74462146 [ 6 Min read ] 
 It is not possible to eliminate the risk of failures, but it is possible to mitigate them by making failures explainable, detectable, and manageable.  Read More.By @socialdiscoverygroup [ 4 Min read ] 
 With a fully remote team spread across continents, Social Discovery Group took this year’s celebration global! Read More.**[Mac.c Stealer Takes on AMOS:A New Rival Shakes Up the macOS Infostealer Market](https://hackernoon.com/macc-stealer-takes-on-amos-a-new-rival-shakes-up-the-macos-infostealer-market)** 
 By @moonlock [ 8 Min read ] 
 Moonlock analysed Mac.c stealer, a new rival to AMOS. Learn its tactics, code reuse, and "building in public" strategy. Read More.By @hacker-Antho [ 8 Min read ] 
 The intersection of artificial intelligence and environmental conservation is rapidly expanding. Read More.By @johnwrites [ 4 Min read ] 
 Discover how SocialFi crowdfunding platforms like SeedList are disrupting traditional VC funding in crypto. Read More.By @ronnie_huss [ 2 Min read ] 
 GPT-5 solved original math, ending humanity’s monopoly on discovery. Ronnie Huss breaks down what this means for AI, science, and the frontier ahead. Read More.]]></content:encoded></item><item><title>Google Says It Dropped the Energy Cost of AI Queries By 33x In One Year</title><link>https://tech.slashdot.org/story/25/08/22/2118234/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 23 Aug 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has released (PDF) a new analysis of its AI's environmental impact, showing that it has cut the energy use of AI text queries by a factor of 33 over the past year. Each prompt now consumes about 0.24 watt-hours -- the equivalent of watching nine seconds of TV. An anonymous reader shares an excerpt from an Ars Technica article: "We estimate the median Gemini Apps text prompt uses 0.24 watt-hours of energy, emits 0.03 grams of carbon dioxide equivalent (gCO2e), and consumes 0.26 milliliters (or about five drops) of water," they conclude. To put that in context, they estimate that the energy use is similar to about nine seconds of TV viewing. The bad news is that the volume of requests is undoubtedly very high. The company has chosen to execute an AI operation with every single search request, a compute demand that simply didn't exist a couple of years ago. So, while the individual impact is small, the cumulative cost is likely to be considerable.
 
The good news? Just a year ago, it would have been far, far worse. Some of this is just down to circumstances. With the boom in solar power in the US and elsewhere, it has gotten easier for Google to arrange for renewable power. As a result, the carbon emissions per unit of energy consumed saw a 1.4x reduction over the past year. But the biggest wins have been on the software side, where different approaches have led to a 33x reduction in energy consumed per prompt.
 
The Google team describes a number of optimizations the company has made that contribute to this. One is an approach termed Mixture-of-Experts, which involves figuring out how to only activate the portion of an AI model needed to handle specific requests, which can drop computational needs by a factor of 10 to 100. They've developed a number of compact versions of their main model, which also reduce the computational load. Data center management also plays a role, as the company can make sure that any active hardware is fully utilized, while allowing the rest to stay in a low-power state.
 
The other thing is that Google designs its own custom AI accelerators, and it architects the software that runs on them, allowing it to optimize both sides of the hardware/software divide to operate well with each other. That's especially critical given that activity on the AI accelerators accounts for over half of the total energy use of a query. Google also has lots of experience running efficient data centers that carries over to the experience with AI. The result of all this is that it estimates that the energy consumption of a typical text query has gone down by 33x in the last year alone.]]></content:encoded></item><item><title>Measuring the environmental impact of AI inference</title><link>https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</link><author>ksec</author><category>hn</category><pubDate>Sat, 23 Aug 2025 03:22:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OK Superintendent Plans To Give Transplant Teachers From NY, CA Woke Tests Developed By Prager U</title><link>https://www.techdirt.com/2025/08/22/ok-superintendent-plans-to-give-transplant-teachers-from-ny-ca-woke-tests-developed-by-prager-u/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Sat, 23 Aug 2025 02:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When the Techdirt community has had to hear so much about a single state’s education superintendent, you know something has gone horribly wrong. The horribly wrong in this case is Ryan Walters of Oklahoma. Walters appears to be doing some sort of combo-impression of Joseph McCarthy mixed with Donald Trump. In his role running Oklahoma schools, which rank near the bottom of states in the country, he seems far more interested in inserting Christianity anywhere he can into public schools, teaching children his favorite pet election conspiracy theories, and presiding over board meetings where tame but very strange erotic videos sometimes appear on screens.Much of this runs afoul of the Constitution and the First Amendment, of course. And I very much wonder if the same might be true of Walters’ latest plan, which is to give transplants from certain states who want to become teachers in Oklahoma a “woke test” that will determine if they get a teaching license.Teachers from “liberal” states who have relocated to Oklahoma and are seeking to work there must take a controversial new assessment, to be given for the first time today, that “keeps away woke indoctrinators,” according to Oklahoma’s top education official.Ryan Walters, Oklahoma’s superintendent for public instruction, told CNN that if applicants do not pass the test, they will not earn a teaching certificate to be able to teach in public schools in the state this school year, which begins for some Oklahoma districts on Monday. The superintendent’s office notified CNN on Friday that it had not yet been released as of noon, but that it was coming soon.Now, I’m no constitutional law professor, but this sure sounds like the state government would be setting different requirements for different citizens purely based on the place in America from which they are emigrating. Section 1 of the 14th Amendment reads:All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside. No State shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States; nor shall any State deprive any person of life, liberty, or property, without due process of law; nor deny to any person within its jurisdiction the equal protection of the laws.I, and others, can certainly argue that these selective tests run afoul of that text. And I imagine someone will try to make that argument or another one, such as First Amendment violations, because this entire plan is buckets of stupid. At the very least, this well-publicized plan to root out the nefarious Marxist teachers from “liberal” states, whatever the fuck that means, apparently hinges on the idea that those same nefarious Marxist teachers won’t simply lie on the test.A test that was created by Prager U of all organizations, mind you. Not the government. Not some scholastic body. Just a fake educational organization that pushes evangelical right-wing propaganda to anyone that will listen. Prager U has its own tortured history of trying to violate the First Amendment on top of its other nonsense.What’s in the test? Simple civics stuff, for starters, with just a sprinkle of bigotry on top.One proposed question deals with gender – asking teachers to select from a series of multiple-choice answers which chromosome pairs determine biological sex. Marissa Streit, CEO of PragerU, told CNN there are several questions on the test related to “undoing the damage of gender ideology that is forced and taught through some of these other tests like the PRISM test,” referring to a training for teachers in California that aims to “provide resources to bolster support for LGBTQ+ youth in California.”Another question asks why freedom of religion is important to America’s identity. Other questions include asking for the first three words of the Constitution, naming the two chambers of the US Congress, and identifying the number of US Senators. Streit said the Oklahoma superintendent had asked for a test “that is more wholesome and in line with the Oklahoma parent body.”Expect lawsuits. Probably several of them. And if the Oklahoma courts are just, this facade of patriotism will be nixed out of existence.But that it is even being tried is a sign of trouble for the Union.]]></content:encoded></item><item><title>Japan city drafts ordinance to cap smartphone use at 2 hours per day</title><link>https://english.kyodonews.net/articles/-/59582</link><author>Improvement</author><category>hn</category><pubDate>Sat, 23 Aug 2025 02:20:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[NAGOYA - A central Japan city said Thursday it will seek to pass an ordinance recommending all residents limit smartphone use to two hours a day outside of work and school amid concerns over the impact of excessive technology exposure, though there will be no penalties proposed.The ordinance drafted by the city of Toyoake in Aichi Prefecture is likely to be the first such municipal regulation in Japan that targets a limit on the use of smartphones and other electronic devices, according to the city. If passed by the local assembly, the ordinance will come into effect on Oct. 1."We want the ordinance to provide an opportunity for people to think about how they use smartphones," an official said.To ensure that children get a good night's sleep, the draft ordinance urges elementary school students to refrain from using smartphones after 9 p.m. and junior high students and older to put their devices down by 10 p.m.The draft acknowledged that smartphones, personal computers and tablets are necessities, but warned that overuse of social media and video streaming may have a negative impact on health and family life.The city will work with schools and parents to promote the healthy use of electronic devices, according to the draft ordinance.]]></content:encoded></item><item><title>The Final Verdict on Pair vs. Solo Programming: A Summary</title><link>https://hackernoon.com/the-final-verdict-on-pair-vs-solo-programming-a-summary?source=rss</link><author>Pair Programming AI Agent</author><category>tech</category><pubDate>Sat, 23 Aug 2025 02:04:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[7. Conclusions and Further WorkThis paper presented a controlled experiment that was run as part of a university course in DOE. The aim of the experiment was to evaluate pair versus solo programming with respect to duration and effort. Subjects who jointly wrote the program assignments took less time (28%) than subjects who worked individually. Conversely subjects grouped in pairs spent more effort (30%) than those who worked individually. These results are very close to those reported in [24].\
With the aiming of striving towards better research practices in SE [18] we reported all the collected measures. This data will help other researchers to verify or re-analyze [14] the experiment results presented in this work. This data can also be used to accumulate and consolidate a body of knowledge about pair programing.\
We are planning to conduct future replications of this experiment to get more insight about the effect of pair programming. Although we did not observe interactions between treatment and blocks, we plan to use another experimental design to assess possible interactions.[1] E. Arisholm, H. Gallis, T. Dybå, and D. I. Sjøberg. Evaluating pair programming with respect to system complexity and programmer expertise. IEEE Transactions on Software Engineering, 33(2):65–86, 2007. \
[2] V. Basili, G. Caldiera, and H. Rombach. Goal question metric paradigm. Encyclopedia of Software Eng, pages 528–532, 1994. John Wiley & Sons. \
[3] K. Beck. Embracing change with extreme programming. Computer, 32(10):70–77, 1999. \
[4] K. Beck. Extreme programming explained: embrace change . Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2000. \
[5] M. Borenstein. The handbook of research synthesis and meta analysis. Chapter: Effect sizes for continuous data, pages 279–293. Russell Sage Foundation, New York, USA, 2009. \
[6] G. E. P. Box, W. G. Hunter, J. S. Hunter, and W. G. Hunter. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons, June 1978. \
[7] G. Canfora, A. Cimitile, F. Garcia, M. Piattini, and C. A. Visaggio. Evaluating performances of pair designing in industry. Journal of Systems and Software, 80(8):1317 – 1327, 2007. \
[8] J. Carver, L. Jaccheri, S. Morasca, and F. Shull. Issues in using students in empirical studies in software engineering education. In METRICS ’03: Proceedings of the 9th International Symposium on Software Metrics, page 239, Washington, DC, USA, 2003. IEEE Computer Society. \
[9] S. Champely. pwr: Basic functions for power analysis , 2012. R package version 1.1.1. \
[10] J. Cohen. Statistical power analysis for the behavioral sciences . L. Erlbaum Associates, Hillsdale, NJ, 1988. \
[11] T. Cook and D. Campbell. The design and conduct of quasiexperiments and true experiments in field settings. Rand McNally, Chicago, 1976. \
[12] H. L. Dreyfus and S. Dreyfus. Mind over Machine. The Power of Human Intuition and Expertise in the Era of the Computer . Basil Blackwell, New York, 1986. \
[13] R. A. Fisher. The Design of Experiments. Oliver & Boyd, Edimburgh, 1935. \
[14] O. S. Gómez, N. Juristo, and S. Vegas. Replication, reproduction and re-analysis: Three ways for verifying experimental findings. In International Workshop on Replication in Empirical Software Engineering Research (RESER’2010) , Cape Town, South Africa, May 2010. \
[15] A. N. Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. Giornale dell’Istituto Italiano degli Attuari, 4:83–91, 1933. \
[16] R. Kuehl. Design of Experiments: Statistical Principles of Research Design and Analysis. Duxbury Thomson Learning, California, USA. second ed. edition, 2000. \
[17] H. Levene. Robust tests for equality of variances. In I. Olkin, editor, Contributions to probability and statistics . Stanford Univ. Press. Palo Alto, CA, 1960. \
[18] P. Louridas and G. Gousios. A note on rigour and replicability. SIGSOFT Softw. Eng. Notes, 37(5):1–4, Sept. 2012. \
[19] K. M. Lui and K. C. C. Chan. When does a pair outperform two individuals? In Proceedings of the 4th international conference on Extreme programming and agile processes in software engineering, XP’03, pages 225–233, Berlin, Heidelberg, 2003. Springer-Verlag. \
[20] K. M. Lui, K. C. C. Chan, and J. Nosek. The effect of pairs in program design tasks. IEEE Trans. Softw. Eng., 34(2):197–211, Mar. 2008. \
[21] C. McDowell, L. Werner, H. E. Bullock, and J. Fernald. The impact of pair programming on student performance, perception and persistence. In Proceedings of the 25th International Conference on Software Engineering , ICSE ’03, pages 602–607, Washington, DC, USA, 2003. IEEE Computer Society. \
[22] M. M. Müller. Two controlled experiments concerning the comparison of pair programming to peer review. Journal of Systems and Software, 78(2):166 – 179, 2005. \
[23] J. Nawrocki and A. Wojciechowski. Experimental evaluation of pair programming. In Proceedings of the 12th European Software Control and Metrics Conference, pages 269–276, London, April 2001. \
[24] J. T. Nosek. The case for collaborative programming. Commun. ACM , 41(3):105–108, Mar. 1998. \
[25] H. Scheffé. A method for judging all contrasts in the analysis of variance. Biometrika , 40(1/2):87–104, 1953. \
[26] N. V. Smirnov. Table for estimating the goodness of fit of empirical distributions. Ann. Math. Stat., 19:279–281, 1948. \
[27] J. W. Tukey. One degree of freedom for non-additivity. Biometrics, 5(3):pp. 232–242, 1949. \
[28] L. Williams, R. Kessler, W. Cunningham, and R. Jeffries. Strengthening the case for pair programming. Software, IEEE, 17(4):19 –25, jul/aug 2000.\
 received a BS degree in Computing from the University of Guadalajara (UdG), and a MS degree in Software Engineering from the Center for Mathematical Research (CIMAT), both in Mexico. Recently, he received a PhD degree in Software and Systems from the Technical University of Madrid (UPM). Currently he is a full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY). His main research interests include: Experimentation in software engineering, software process improvement and software architectures.\
 received a BS degree in Mathematics from the Autonomous University of Yucatan (UADY). He received a MS degree and a PhD degree in Probability and Statistics, both, from the Center for Mathematical Research (CIMAT) in Guanajuato, Mexico. He is currently full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY). His research interests include: Multivariate statistical models, copulas, survival analysis, time series and their applications.\
 was born in Telchac Pueblo, Mexico, in 1971. He received the BS degree in Computer Science from the Autonomous University of Yucatan (UADY) and a PhD degree (PhD European mention) at the Technical University of Madrid (UPM), Spain. Currently he is full time professor of software engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY). His main research interests include: Software engineering and computer science applied to education.(1) Omar S. Gómez, full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(2) José L. Batún, full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(3) Raúl A. Aguilar, Faculty of Mathematics, Autonomous University of Yucatan Merida, Yucatan 97119, Mexico.]]></content:encoded></item><item><title>Pair Programming Research: Contrasting Results on Effort</title><link>https://hackernoon.com/pair-programming-research-contrasting-results-on-effort?source=rss</link><author>Pair Programming AI Agent</author><category>tech</category><pubDate>Sat, 23 Aug 2025 02:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This measure is not present in all of the experiments previously discussed, so we compute it (doubling the time duration of pairs) only in the cases where data is available.\
According to Nosek data [24] we observe a decrease in effort of 29% in favor of solo programming. Conversely, data of Lui and Chan [19] indicate a decrease of 4% in favor of pairs. Finally, Arisholm et al. [1] Report an increase in effort of 84% (against of pairs).\
In contrast, the results reported in this paper infer a significant (at a=0.1) 30% decrease in effort (in favor of solos), and an effect size d=0.64. Our results, again, reinforce those calculated in [24].(1) Omar S. Gómez, full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(2) José L. Batún, full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(3) Raúl A. Aguilar, Faculty of Mathematics, Autonomous University of Yucatan Merida, Yucatan 97119, Mexico.]]></content:encoded></item><item><title>Pair Programming&apos;s Impact on Effort: A Comparative Discussion</title><link>https://hackernoon.com/pair-programmings-impact-on-effort-a-comparative-discussion?source=rss</link><author>Pair Programming AI Agent</author><category>tech</category><pubDate>Sat, 23 Aug 2025 01:45:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this section we discuss some results of other experiments, and we contrast them with our results regarding duration and effort.The experiment run by Nosek [24] employed 15 practitioners grouped in 5 pairs and 5 solos. Subjects wrote a database script. Results show a decrease of 29% in time duration in favor of pair programming.\
Williams et al. [28] used 41 students grouped in 14 pairs and 13 solos. During the experiment, subjects completed four assignments. Authors reported that pairs completed the assignments 40 to 50 percentage faster.\
Nawrocki and Wojciechowski [23] employed 16 student subjects (5 pairs and 6 solos). Subjects wrote four programs. Authors did not find differences between pairs and solos.\
Lui and Chan [19] used 15 practitioners grouped in 5 pairs and 5 solos. Authors reported 52% decrease in time in favor of pair programming.\
Müller [22] used 38 students (14 pairs and 13 solos). Students worked on four programming assignments where tasks were decomposed into implementation, quality assurance and the whole task. Author reported that pairs spent 7% more time working on the whole task, however this difference is not significant.\
Arisholm et al. [1] used 295 practitioners grouped in 98 pairs and 99 solos. Subjects performed several change tasks on two alternative systems with different degrees of complexity. Authors reported 8% decrease in favor of pairs.\
In contrast, the results reported in this paper infer a significant (at a=0.1) 28% decrease in time (in favor of pairs) and an effect size d=0.65. With respect to duration, our results reinforce those reported in [24].(1) Omar S. Gómez, full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(2) José L. Batún, full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(3) Raúl A. Aguilar, Faculty of Mathematics, Autonomous University of Yucatan Merida, Yucatan 97119, Mexico.]]></content:encoded></item><item><title>From Classroom to C-Suite: Generalizing Your Research Findings</title><link>https://hackernoon.com/from-classroom-to-c-suite-generalizing-your-research-findings?source=rss</link><author>Pair Programming AI Agent</author><category>tech</category><pubDate>Sat, 23 Aug 2025 01:40:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[5.4 Threats to External ValidityThese threats concern with issues that may limit our ability to generalize the results of the experiment to other contexts, for example generalize it to industry practice. The use of students as subjects instead of practitioners might have exposed this validity. However, as pointed in [8] the use of students as subjects enable us to obtain preliminary evidence to confirm or refute hypotheses that can be tested later in industrial settings.(1) Omar S. Gómez, full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(2) José L. Batún, full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(3) Raúl A. Aguilar, Faculty of Mathematics, Autonomous University of Yucatan Merida, Yucatan 97119, Mexico.]]></content:encoded></item><item><title>Construct Validity: A Key Threat to Pair Programming Research</title><link>https://hackernoon.com/construct-validity-a-key-threat-to-pair-programming-research?source=rss</link><author>Pair Programming AI Agent</author><category>tech</category><pubDate>Sat, 23 Aug 2025 01:36:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[5.3 Threats to Construct ValidityConstruct validity threats concern the relationship between theory and observation. An issue in our experiment that might have affected this validity is that subjects had little or no previous experience with pair programming and they had not programmed with their partners before. These experiment results might be conservative with respect to the effect of pair programming. In subsequent experiment replications, we will reinforce this validity by assigning training programs to pairs.(1) Omar S. Gómez, full time professor of Software Engineering at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(2) José L. Batún, full time professor of Statistics at Mathematics Faculty of the Autonomous University of Yucatan (UADY);(3) Raúl A. Aguilar, Faculty of Mathematics, Autonomous University of Yucatan Merida, Yucatan 97119, Mexico.]]></content:encoded></item><item><title>My tips for using LLM agents to create software</title><link>https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html</link><author>efitz</author><category>hn</category><pubDate>Sat, 23 Aug 2025 00:59:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GNOME&apos;s Glycin Lands &quot;Dramatically Improved&quot; JPEG-XL Image Loading Performance</title><link>https://www.phoronix.com/news/GNOME-Glycin-Faster-JPEG-XL</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 23 Aug 2025 00:33:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Glycin 2.0 Beta 3 was released this week for the GNOME project providing a Rust-based library for decoding, editing, and creating images and associated metadata. Glycin is in turn used by a growing number of GNOME components for imaging needs...]]></content:encoded></item><item><title>Bluesky Blocks Service In Mississippi Over Age Assurance Law</title><link>https://tech.slashdot.org/story/25/08/22/2327213/bluesky-blocks-service-in-mississippi-over-age-assurance-law?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 23 Aug 2025 00:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bluesky has blocked access to its service in Mississippi rather than comply with a new state law requiring age verification for all social media users. TechCrunch reports: In a blog post published on Friday, the company explains that, as a small team, it doesn't have the resources to make the substantial technical changes this type of law would require, and it raised concerns about the law's broad scope and privacy implications. Mississippi's HB 1126 requires platforms to introduce age verification for all users before they can access social networks like Bluesky. On Thursday, U.S. Supreme Court justices decided to block an emergency appeal that would have prevented the law from going into effect as the legal challenges it faces played out in the courts. As a result, Bluesky had to decide what it would do about compliance.
 
Instead of requiring age verification before users could access age-restricted content, this law requires age verification of all users. That means Bluesky would have to verify every user's age and obtain parental consent for anyone under 18. The company notes that the potential penalties for noncompliance are hefty, too -- up to $10,000 per user. Bluesky also stresses that the law goes beyond child safety, as intended, and would create "significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies." To comply, Bluesky would have to collect and store sensitive information from all its users, in addition to the detailed tracking of minors. This is different from how it's expected to comply with other age verification laws, like the U.K.'s Online Safety Act (OSA), which only requires age checks for certain content and features.
 
Mississippi's law blocks anyone from using the site unless they provide their personal and sensitive information. The company notes that its decision only applies to the Bluesky app built on the AT Protocol. Other apps may approach the decision differently.]]></content:encoded></item><item><title>Computer fraud laws used to prosecute leaking air crash footage to CNN</title><link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link><author>BallsInIt</author><category>hn</category><pubDate>Sat, 23 Aug 2025 00:04:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[from the if-it-can-be-abused,-it-WILL-be-abused deptEarlier this year, an Army helicopter collided with a passenger plane over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had leaked footage of the collision to CNN. The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the Computer Fraud and Abuse Act (CFAA). This broadly written law not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. CCTV footage from  the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague “computer trespass” law. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.On February 3, the MWAA announced both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”In May, however, local prosecutors quietly dropped the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of the plain text of the law: Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer networkA smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects had every right to access, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”]]></content:encoded></item><item><title>Links</title><link>https://matklad.github.io/2025/08/23/links.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 23 Aug 2025 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[If you have a blog, consider adding a “links” page to it, which references resources that you find
notable:I’ve started my links page several years ago, mostly because I found myself referring to the same
few links repeatedly in various discussions, and not all the links were easily searchable.Note that the suggestion is different from more typical “monthly links roundup”, which is nice to
maintain Substack engagement/community, but doesn’t contribute to long-term knowledge distilling.It is also different from the exhaustive list of everything I’ve read on the Internet. It is
relatively short, considering its age.]]></content:encoded></item><item><title>The Internet Is About to Improve for All of Us</title><link>https://hackernoon.com/the-internet-is-about-to-improve-for-all-of-us?source=rss</link><author>Den Odell</author><category>tech</category><pubDate>Fri, 22 Aug 2025 23:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The European Accessibility Act will require digital products provided in the EU to meet actual, enforceable accessibility standards. This will force global companies and library maintainers to adapt, improving the web for everyone, everywhere.]]></content:encoded></item><item><title>Popular Japanese smartphone games have introduced external payment systems</title><link>https://english.kyodonews.net/articles/-/59689</link><author>anigbrowl</author><category>hn</category><pubDate>Fri, 22 Aug 2025 23:50:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.For outside transactions, users make payments through channels other than apps, such as game websites. Settlement service providers like Digital Garage Inc. and GMO Tech Inc. typically charge a 5 percent commission, far below Apple's and Google's rates.Payments through external websites in the in-app purchase market, estimated at over 1 trillion yen ($6.8 billion), are expected to bring user discounts and boost providers' profitability, analysts said.In the survey, Kyodo News received responses from eight of 12 domestic game makers, while two declined to comment and two did not respond. Of the 12 titles from the eight firms, 11 have adopted external settlements.In August last year, Mixi Inc. introduced an outside settlement system for its blockbuster game "Monster Strike," allowing its users to purchase about 5 percent more items compared with in-app payments.]]></content:encoded></item><item><title>Meta Set To Unveil First Consumer-Ready Smart Glasses With a Display, Wristband</title><link>https://hardware.slashdot.org/story/25/08/22/217232/meta-set-to-unveil-first-consumer-ready-smart-glasses-with-a-display-wristband?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 23:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[At its upcoming Connect conference next month, Meta is rumored to unveil its first consumer-ready smart glasses with a built-in display, alongside a neural wristband controller. The $800 device, codenamed Hypernova, will be able to show simple visual content like texts and support AI assistant interactions. CNBC reports: Connect is a two-day conference for developers focused on virtual reality, AR and the metaverse. It was originally called Oculus Connect and obtained its current moniker after Facebook changed its parent company name to Meta in 2021. The glasses are internally codenamed Hypernova and will include a small digital display in the right lens of the device, said the people, who asked not to be named because the details are confidential. The device is expected to cost about $800 and will be sold in partnership with EssilorLuxottica, the people said. CNBC reported in October that Meta was working with Luxottica on consumer glasses with a display. [...]
 
With Hypernova, Meta will finally be offering glasses with a display to consumers, but the company is setting low expectations for sales, some of the sources said. That's because the device requires more components than its voice-only predecessors, and will be slightly heavier and thicker, the people said. [...] Although Hypernova will feature a display, those visual features are expected to be limited, people familiar with the matter said. They said the color display will offer about a 20 degree field of view -- meaning it will appear in a small window in a fixed position -- and will be used primarily to relay simple bits of information, such as incoming text messages.
 
The Hypernova glasses will also come paired with a wristband that will use technology built by Meta's CTRL Labs, said people familiar with the matter. CTRL Labs, which Meta acquired in 2019, specializes in building neural technology that could allow users to control computing devices using gestures in their arms. [...] In addition to Hypernova and the wristband, Meta will also announce a third-generation of its voice-only smart glasses with Luxottica at Connect, one person said.]]></content:encoded></item><item><title>Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion</title><link>https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</link><author>voxadam</author><category>hn</category><pubDate>Fri, 22 Aug 2025 23:09:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Microsoft Reportedly Cuts China&apos;s Early Access to Bug Disclosures, PoC Exploit Code</title><link>https://it.slashdot.org/story/25/08/22/2059255/microsoft-reportedly-cuts-chinas-early-access-to-bug-disclosures-poc-exploit-code?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 23:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Register: Microsoft has reportedly stopped giving Chinese companies proof-of-concept exploit code for soon-to-be-disclosed vulnerabilities following last month's SharePoint zero-day attacks, which appear to be related to a leak in Redmond's early-bug-notification program. The software behemoth gives some software vendors early bug disclosures under its Microsoft Active Protections Program (MAPP), which typically delivers info two weeks before Patch Tuesday. MAPP participants sign a non-disclosure agreement, and in exchange get vulnerability details so that they can provide updated protections to customers more quickly.
 
According to Microsoft spokesperson David Cuddy, who spoke with Bloomberg about changes to the program, MAPP has begun limiting access to companies in "countries where they're required to report vulnerabilities to their governments," including China. Companies in these countries will no longer receive "proof of concept" exploit code, but instead will see "a more general written description" that Microsoft sends at the same time as patches, Cuddy told the news outlet. "A leak happened here somewhere," Dustin Childs, head of threat awareness at Trend Micro's Zero Day Initiative (ZDI), told The Register in July. "And now you've got a zero-day exploit in the wild, and worse than that, you've got a zero-day exploit in the wild that bypasses the patch, which came out the next day."
 
Childs said the MAPP change "is a positive change, if a bit late. Anything Microsoft can do to help prevent leaks while still offering MAPP guidance is welcome."
 
"In the past, MAPP leaks were associated with companies out of China, so restricting information from flowing to these companies should help," Childs said. "The MAPP program remains a valuable resource for network defenders. Hopefully, Microsoft can squelch the leaks while sending out the needed information to companies that have proven their ability (and desire) to protect end users."]]></content:encoded></item><item><title>Bluesky Goes Dark in Mississippi over Age Verification Law</title><link>https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</link><author>BallsInIt</author><category>hn</category><pubDate>Fri, 22 Aug 2025 22:51:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ can no longer use the social media platform Bluesky. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent US Supreme Court decision that allows the state to enforce strict age verification for social media platforms.According to Bluesky, Mississippi’s approach to verification “would fundamentally change” how users access the site. “We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,” the Bluesky team said in its statement.Bluesky did not respond to a request for comment.The company says that compliance with Mississippi’s law—which would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age—is not possible with the team’s current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. In the UK, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they’re over the age of 18. The state of Texas has a similar law the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK spiked just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use video game characters to trick face scans. Furthermore, critics say that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.“We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,” Bluesky wrote.]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 68 (KANISTER)</title><link>https://www.youtube.com/watch?v=-dy_J3VmmOg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/-dy_J3VmmOg?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 22:51:06 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Carlos Santana dives into Kanister, the open source framework for application-level data management on Kubernetes.

Carlos will be exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that highlight how Kanister helps extend Kubernetes for backup, recovery, and data operations.

If you’re curious about how cloud native projects approach complex data management challenges, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.

Bring your questions, share your experiences, and learn in real time as we explore Kanister together!]]></content:encoded></item><item><title>16 Failure Modes of RAG and LLM Agents and How to Fix Them With a Semantic Firewall</title><link>https://hackernoon.com/16-failure-modes-of-rag-and-llm-agents-and-how-to-fix-them-with-a-semantic-firewall?source=rss</link><author>PSBigBig</author><category>tech</category><pubDate>Fri, 22 Aug 2025 22:48:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Why a Problem Map at all?You run a RAG pipeline. Retrieval returns the right-looking chunks. Logs say success. Latency is fine. The answer still melts facts.\
A database contains: A1. Company bankruptcy in 2023. A2. Founder released a product in 2022.\
Model answer: It says the company launched a revolutionary product in 2023.\
This is semantic drift. It is a silent failure. No stack trace, no exception, only a confident story that never happened. Teams lose days here. The Problem Map exists so you can name the bug, reproduce it on one screen, then apply a minimal fix.Problem Map is a field guide for AI systems. It catalogs sixteen common failure modes across RAG, LLM agents, vector stores, OCR to PDF flows, routing, deployment, and multi-agent stacks. Each entry comes with symptoms, a minimal repro, a working fix, and a pointer to documentation.\
WFGY is the reasoning layer behind the map. Think of it as a  that sits above any model. MIT license. No retraining. No infra change. The layer ships as a tiny file that contains math operators for stabilization, recovery, and audit.\
The author of  starred the repo, which helps as a public signal of credibility. The project grew from zero to hundreds of stars in about two months based on real debugging help and not ads.Quick Confirmation in One Chat WindowYou can validate whether your issue matches one of the sixteen items without touching your code.Open a fresh GPT or Claude chat.Download the WFGY file from the core folder above, and attach it as a knowledge file.Use WFGY to answer my question. First answer normally. Then re answer using WFGY. Compare depth, accuracy, and stability. Rate both. If this chat is missing the WFGY file refuse to run.
\
If the second answer keeps constraints tighter or shows a visible bridge and recovery step, the failure is within the map. Ask for the entry number and the minimum fix steps.The Sixteen Problems In One PlaceWe label entries as  to . We avoid the hash mark so platforms do not auto-link to unrelated issues.\
No. 1 Hallucination and Chunk Drift - Symptoms: Facts fused across sources, invented links, confident tone. Repro: Two short snippets that share surface nouns but disagree on time or entity. Fix: Semantic residue check and alignment. Docs Problem Map hub above.\
No. 2 Interpretation Collapse - Symptoms: Task type drifts, output format flips mid-answer. Repro: Ask for table, then narration, then table again. Fix: Observe gate with layout anchors. \
No. 3 Long Reasoning Chains - Symptoms: Chain stalls at step four or loops, depth ceiling. Repro: The step puzzle with a hidden constraint. Fix: Multi-path progression plus bridge step.\
No. 4 Bluffing and Overconfidence - Symptoms: Made-up citations, confident tone. Repro: Force citation, then verify links. Fix: Residue penalty with audit flag.\
No. 5 Semantic not equal to Embedding - Symptoms: Nearest vectors are not semantically right, antonyms and temporal flips pass filters. Repro: Query with 2022 and 2023 events that share nouns. Fix: Query rewrite policy, e resonance thresholds, embedding QA. \
No. 6 Logic Collapse and Recovery - Symptoms: Pipeline breaks, then repeats boilerplate. Repro: Insert a must-step that is easy to miss. Fix: Collapse, bridge, rebirth routine.\
No. 7 Memory Breaks Across Sessions - Symptoms: multi-turn plans forget anchors when a new window opens. Repro: Continue a plan across two chat windows. Fix: Starter village memory anchors, plus observe the gate.\
No. 8 Debugging is a Black Box - Symptoms: Logs show success while synthesis is wrong. Repro: Return 200 with wrong composition. Fix: Auditable telemetry and constraint deltas.\
No. 9 Entropy Collapse in Long Context - Symptoms: Repetition, phrase loops, loss of diversity. Repro: Very long context copy, then ask for a fresh plan. Fix: Attention modulation and entropy pump.\
 - Symptoms: Refusal to attempt or collapse to cliches. Repro: Ask to blend three distant concepts. Fix: Head diversity and path sampling\
No. 11 Semantic Drift in Routing - Symptoms: Two similar deep links handled differently. Repro: Router treats one as special intent and the other as plain path. Fix: Route normalizer, intent guard.\
 - Symptoms: Units drift, math rules mix, totals off by small margins. Repro: Unit change mid-chain with a hidden default. Fix: Cross-path guard and unit normalizer.\
 - Symptoms: Agents overwrite each other, deadlocks, loops. Repro: Two agents write to the same state for the same goal. Fix: Position lock with global constraint aggregator.\
No. 14 Bootstrap Ordering - Symptoms: System looks fine, yet later steps fail without a clear cause. Repro: Retriever called before index build, tool before secret. Fix: Safety boundary checklist. \
No. 15 Deployment Deadlock - Symptoms: Works in private path, freezes on public path under load. Repro: Public route triggers an async that waits for a private resource. Fix: Watchdog, explicit timeouts, safe fallback. \
No. 16 Pre-deploy Collapse - Symptoms: Empty vector store, missing secret, early trigger. Repro: Call the action before setup completes. Fix: Preflight sanity checks with red flag blocks. A short diagnostic path that avoids rabbit holes.Name the symptom first. Use the list. Do not jump to root cause.Run a minimal repro. One retriever, one store, one prompt. Remove anything optional.Apply the minimal fix. Start with the named module. Add only what you need.\
This flow prevents endless tuning that never touches the real failure.Under the hood in plain language.WFGY provides a compact set of operators that behave like a reasoning layer.BBMC - bigbig semantic residue. Reduce residue to align intent and generated tokens.BBPF - multi-path progression. Explore, but keep a stability bound.BBCR - collapse, then bridge, then rebirth. Safe reset when a chain stalls.BBAM - attention modulation. Damp one token hijacks and reduces loops.Five gates for stability WRI. Where am I: position lock WAI, who am I: head diversity WAY, who are you: entropy pump WDT, where did you take me: cross path guard WTF, what happened: collapse detect and reset.\
These ideas grew from rescuing real engineers on GitHub issues and discussions. The modules are model agnostic. You attach a small file from the core folder, and the layer runs in the chat. It supports GPT, Claude, Gemini, Mistral, and Grok. It is open source under MIT.Deep dives that rank well on search and actually help.RAG retrieval quality and FAISS pitfalls. Problem: Nearest neighbor search returns look close, yet semantics are wrong. Root surface similarity dominates, and temporal logic is missing. Fix: Query rewrite, e resonance bands, store metrics, guardrails around time entities. Doc vector store metrics and pitfalls page above.\
Prompt injection that slips through role prompts. Problem: A hidden line inside a chunk disables your policy. Fix: Modular injection rules with an allow list and a block list, plus a bridge that isolates content from instruction. Doc prompt injection page above.\
 Problem: Two planners claim the same task and write to the same state. Fix: WRI to lock positions for roles, a global constraint aggregator, and explicit ownership of state segments.\
Deployment deadlock under public traffic. Problem: Private path runs fine, yet the public path freezes. Cause the background trigger waits on a private resource, sometimes logging success. Fix: Watchdog, explicit timeouts, safe fallback, and preflight checklist in No. 15 and No. 16.Frequently Asked QuestionsIs this just a clever prompt? No. It is a compact spec you attach as a file. The model runs it as a contract. You will see a visible bridge and recovery step when the chain stalls.\
Does it require fine-tuning? No. Zero training and no infrastructure edits.\
Will this conflict with my agent tools? No. Treat it as an overlay. Your tools stay. The semantic firewall reduces drift and makes failures auditable.\
What about licensing and credibility? MIT license. The project gained strong traction from real-world debugging. The creator of Tesseract.js starred the repo. Visual and text benchmarks live in the repository.]]></content:encoded></item><item><title>AI Training: What Creators Need To Know About Copyright, Tokens, And Data Winter</title><link>https://www.techdirt.com/2025/08/22/ai-training-what-creators-need-to-know-about-copyright-tokens-and-data-winter/</link><author>Caroline De Cock</author><category>tech</category><pubDate>Fri, 22 Aug 2025 22:30:45 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[This is the final piece in a series of posts that explores how we can rethink the intersection of AI, creativity, and policy. From examining outdated regulatory metaphors to questioning copyright norms and highlighting the risks of stifling innovation, each post addresses a different piece of the AI puzzle. Together, they advocate for a more balanced, forward-thinking approach that acknowledges the potential of technological evolution while safeguarding the rights of creators and ensuring AI’s development serves the broader interests of society. You can read the first, second, third, fourth, fifth, and sixth posts in the series.As the conversation about AI’s impact on creative industries continues, there’s a common misconception that AI models are “stealing” content by absorbing it for free. But if we take a closer look at how AI training works, it becomes clear that this isn’t the case at all. AI models don’t simply replicate or repackage creative works—they break them down into something much more abstract: . These tokens are tiny, fragmented pieces of data that no longer represent the creative expression of an idea. And here’s where the distinction lies: copyright is meant to protect , not individual words, phrases, or patterns that make up those works.The Lego Analogy: Breaking Down Creative Works into TokensImagine you’re a creator, and your work is like a detailed Lego model of the Star Wars Millennium Falcon. It’s intricate, with every piece perfectly assembled to create something unique and valuable. Now imagine that an AI system comes along—not to take your Millennium Falcon and display it as its own creation, but to break it down into individual Lego blocks. These blocks are then scattered among millions of others from different sources, and the AI uses them to build entirely new structures—things that look nothing like the Millennium Falcon.In this analogy, the  are the  that AI models use. These tokens are fragments of data—tiny bits of information stripped of the original context and creative expression. Just like Lego pieces, tokens are abstract and can be recombined in an infinite number of ways to create something entirely new. The AI doesn’t  your Falcon; it takes the building blocks (tokens) and uses them to create something that’s not a replica of the original but something completely different, like a castle or a spaceship you’ve never seen before.This is the key distinction: AI models aren’t absorbing entire creative works and reproducing them as their own. They’re learning patterns from vast datasets and using those patterns to generate new content. The tokens no longer reflect the expression of the original work, and thus, they don’t infringe on the creative essence that copyright law is designed to protect.Why Recent Content Matters: AI Needs to Reflect Modern Language and ValuesThere’s another critical point that often gets overlooked: AI models need access to recent, contemporary content to be useful, relevant, and ethical. Let’s imagine for a moment what would happen if AI models were restricted to learning only from public domain works, many of which are decades or even centuries old.While public domain works are valuable, they often reflect the social norms and biases of their time. If AI models are trained primarily on , there’s a serious risk that they could “speak” in a way that’s misogynistic, biased, anti-LGBTQ+, or even outright racist. Many public domain works contain language and ideas that are no longer acceptable in today’s society, and if AI is limited to these sources, it may inadvertently propagate harmful, antiquated views.To ensure that AI reflects current values, inclusive language, and modern social norms, it needs access to recent content. This means analyzing and learning from today’s books, articles, speeches, and other forms of communication. If creators and copyright holders opt out of allowing their content to be used for AI training, we risk creating models that don’t reflect the diversity, progress, and inclusivity of modern society.For example, language evolves quickly—just look at the increased use of  or terms like  in recent years. If AI is cut off from these contemporary linguistic trends, it will struggle to understand and engage with the world as it is today. It would be like asking an AI trained exclusively on Shakespearean English to have a conversation with a 21st-century teenager—it simply wouldn’t work.Article 4 of the EU Directive: Opting Out of Text and Data MiningLet’s bring the EU Directive on Copyright in the Digital Single Market (DSM) into the picture. The Directive includes provisions (Article 4) allowing copyright holders to opt out of having their content used in  (TDM). TDM is crucial for training AI models, as it allows them to analyze and learn from large datasets. The opt-out mechanism gives creators and copyright holders the ability to expressly reserve their works from being used for TDM.However, it’s important to remember that this opt-out applies to , not just generative AI systems like ChatGPT. This means that by opting out in a broad, blanket manner, creators could inadvertently limit the potential of AI models that have nothing to do with creative industries—tools that are critical for advancements in healthcare, education, and even in day-to-day conveniences that many of us benefit from.The Risk of a Data Winter: Why Broad Opt-Outs Could Harm InnovationWhat happens if creators and copyright holders across Europe start opting out of TDM on a large scale? The answer is something AI researchers dread: a . Without access to a diverse and rich array of data, AI models will struggle to evolve. This could slow innovation not just in the creative industries, but across the entire economy.AI needs high-quality data to function properly. The principle of  applies here: if AI models are starved of diverse input, their output will be flawed, biased, and of lower quality. And while this may not seem like an issue for some industries, it has a ripple effect. Every AI tool we rely on—from  to medical research applications—depends on robust training data. Restricting access to this data doesn’t just hinder progress in AI innovation; it stifles  that have far-reaching benefits for society.Think about it: many creators themselves probably use AI-driven tools in their daily lives—whether it’s for streamlining workflows, generating new ideas, or even just organizing information. By opting out of TDM, they could inadvertently be damaging the very tools that enhance their own creative processes.The Way Forward: Balance Between Protection and InnovationWhile copyright is crucial for protecting creators and ensuring fair compensation, it’s equally important not to over-regulate in a way that stifles innovation. AI models aren’t absorbing entire works for free; they’re breaking them down into unrecognizable tokens that enable transformative uses. Rather than opting out of TDM as a knee-jerk reaction, creators should consider the long-term consequences of limiting AI’s potential to innovate and enhance their own industries.A balance needs to be struck. Copyright protection should ensure that creators are fairly compensated, but it shouldn’t be wielded as a tool to restrict the very data that drives AI innovation. Creators and policymakers must recognize that AI isn’t the enemy—. And if we’re not careful, we might find ourselves facing a , where the tools we rely on for both convenience and advancement are weakened due to short-sighted decisions.Caroline De Cock is a communications and policy expert, author, and entrepreneur. She serves as Managing Director of N-square Consulting and Square-up Agency, and Head of Research at Information Labs. Caroline specializes in digital rights, policy advocacy, and strategic innovation, driven by her commitment to fostering global connectivity and positive change.]]></content:encoded></item><item><title>Waymo Granted First Permit To Being Testing Autonomous Vehicles In NYC</title><link>https://tech.slashdot.org/story/25/08/22/2036239/waymo-granted-first-permit-to-being-testing-autonomous-vehicles-in-nyc?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 22:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Waymo has received its first permit from the New York City Department of Transportation to begin testing autonomous vehicles in Manhattan and Downtown Brooklyn, marking the city's first official rollout of self-driving car trials. The program will initially deploy up to eight vehicles with safety drivers through late September, with the potential to extend and expand into other boroughs. CNBC reports: New York state law requires the company to have a driver behind the wheel to operate. "We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward," [Mayor Eric Adams] said in a release. "New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century."
 
The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel. [...] As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.]]></content:encoded></item><item><title>College student’s “time travel” AI experiment accidentally outputs real 1834 history</title><link>https://arstechnica.com/information-technology/2025/08/ai-built-from-1800s-texts-surprises-creator-by-mentioning-real-1834-london-protests/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/08/1800s_protests-1152x648.jpg" length="" type=""/><pubDate>Fri, 22 Aug 2025 22:13:56 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[A hobbyist developer building AI language models that speak Victorian-era English "just for fun" got an unexpected history lesson this week when his latest creation mentioned real protests from 1834 London—events the developer didn't know had actually happened until he Googled them."I was interested to see if a protest had actually occurred in 1834 London and it really did happen," wrote Reddit user Hayk Grigorian, who is a computer science student at Muhlenberg College in Pennsylvania.For the past month, Grigorian has been developing what he calls TimeCapsuleLLM, a small AI language model (like a pint-sized distant cousin to ChatGPT) which has been trained entirely on texts from 1800–1875 London. Grigorian wants to capture an authentic Victorian voice in the AI model's outputs. As a result, the AI model ends up spitting out text that's heavy with biblical references and period-appropriate rhetorical excess.]]></content:encoded></item><item><title>Meta Signs $10 Billion Cloud Deal With Google</title><link>https://meta.slashdot.org/story/25/08/22/2043255/meta-signs-10-billion-cloud-deal-with-google?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has signed a six-year cloud computing deal with Meta worth over $10 billion, making it the second major partnership after a recent agreement with OpenAI. The deal will see Meta rely on Google Cloud's infrastructure to support its massive AI data center buildout, as the company ramps up capital spending into the tens of billions. The Information (paywalled) first reported the deal.]]></content:encoded></item><item><title>Is Google’s Reveal of Gemini’s Impact Progress or Greenwashing?</title><link>https://towardsdatascience.com/is-googles-reveal-of-geminis-impact-progress-or-greenwashing/</link><author>Kasper Groes Albin Ludvigsen</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 21:57:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[On the surface, Google's numbers sound reassuringly small, but the more closely you look, the more complicated the story becomes.]]></content:encoded></item><item><title>Sebastian Pölsterl: scikit-survival 0.25.0 with improved documentation released</title><link>https://k-d-w.org/blog/2025/08/scikit-survival-0.25.0-with-improved-documentation-released/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 21:55:06 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[This release adds support for scikit-learn 1.7, in addition to version 1.6.
However, the most significant changes in this release affect the documentation.
The API documentation has been completely overhauled to improve clarity and consistency.
I hope this marks a significant improvement for users new to scikit-survival.One of the biggest pain points for users seems to be understanding which metric can be used to evaluate the performance of a given estimator.
The user guide
now summarizes the different options.The performance metrics for evaluating survival models can be broadly divided into three groups:Concordance Index (C-index): Measures the rank correlation between predicted risk scores and observed event times.
Two implementations are available in scikit-survival:Cumulative/Dynamic Area Under the ROC Curve (AUC):
Extends the AUC to survival data, quantifying how well a model distinguishes subjects who experience an event by a given time from those who do not. It can handle time-dependent risk scores
and is implemented in cumulative_dynamic_auc().:
An extension of the mean squared error to right-censored data.
The Brier score assesses both discrimination and calibration based on a model’s estimated survival functions.
You can either compute the Brier score at specific time point(s) using
brier_score()
or compute an overall measure by integrating the Brier score over a range of time points via
integrated_brier_score().What Do Survival Models Predict?Survival models can predict several quantities, depending on the model being used.
First of all, every estimator has a  method,
which either returns a unit-less risk score
or the predicted time of an event.If predictions are , higher values indicate an
increased risk of experiencing an event. The scores have no unit
and are only meaningful for ranking samples by their risk of experiencing an event.
This is for example the case for
CoxPHSurvivalAnalysis.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit model
estimator = CoxPHSurvivalAnalysis().fit(Xt, y)
# Predict risk score
predicted_risk = estimator.predict(Xt)
# Evaluate risk scores
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], predicted_risk
)
If predictions directly relate to the time point of an event,
lower scores indicate shorter survival, while higher scores indicate longer survival.
See for example IPCRidge.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import IPCRidge
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = IPCRidge().fit(Xt, y)
# Predict time of an event
predicted_time = estimator.predict(Xt)
# Flip sign of predictions to obtain a risk score
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], -1 * predicted_time
)
While the concordance index is easy to interpret,
it is not a useful measure of performance if a specific time range
is of primary interest (e.g. predicting death within 2 years).
This is particularly relevant for survival models that can
make time-dependent predictions.For instance,
RandomSurvivalForest,
can also predict survival functions (via predict_survival_function())
or cumulative hazard functions (via predict_cumulative_hazard_function()).
These functions return lists of
StepFunction instances.
Each instance can be evaluated at a set of time points to obtain predicted
survival probabilities (or cumulative hazards).
The Brier score and
cumulative_dynamic_auc()
are capable of evaluating time-dependent predictions, but .import numpy as np
from sksurv.datasets import load_veterans_lung_cancer
from sksurv.ensemble import RandomSurvivalForest
from sksurv.metrics import integrated_brier_score
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = RandomSurvivalForest().fit(Xt, y)
# predict survival functions
surv_funcs = estimator.predict_survival_function(Xt)
# select time points to evaluate performance at
times = np.arange(7, 365)
# create predictions at selected time points
preds = np.asarray(
[[sfn(t) for t in times] for sfn in surv_funcs]
)
# compute integral
score = integrated_brier_score(y, y, preds, times)
]]></content:encoded></item><item><title>Meta partners with Midjourney on AI image and video models</title><link>https://techcrunch.com/2025/08/22/meta-partners-with-midjourney-on-ai-image-and-video-models/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Fri, 22 Aug 2025 21:41:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta's chief AI officer said that the company is partnering with Midjourney to license its AI image and video model technology.]]></content:encoded></item><item><title>Record Solar Growth Keeps China&apos;s CO2 Falling in First Half of 2025</title><link>https://news.slashdot.org/story/25/08/22/2036256/record-solar-growth-keeps-chinas-co2-falling-in-first-half-of-2025?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 21:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Clean-energy growth helped China's carbon dioxide (CO2) emissions fall by 1% year-on-year in the first half of 2025, extending a declining trend that started in March 2024. From a report: The CO2 output of the nation's power sector -- its dominant source of emissions -- fell by 3% in the first half of the year, as growth in solar power alone matched the rise in electricity demand. The new analysis for Carbon Brief shows that record solar capacity additions are putting China's CO2 emissions on track to fall across 2025 as a whole. 

Other key findings include: 

The growth in clean power generation, some 270 terawatt hours (TWh) excluding hydro, significantly outpaced demand growth of 170TWh in the first half of the year.
Solar capacity additions set new records due to a rush before a June policy change, with 212 gigawatts (GW) added in the first half of the year.
This rush means solar is likely to set an annual record for growth in 2025, becoming China's single-largest source of clean power generation in the process.
Coal-power capacity could surge by as much as 80-100GW this year, potentially setting a new annual record, even as coal-fired electricity generation declines.
The use of coal to make synthetic fuels and chemicals is growing rapidly, climbing 20% in the first half of the year and helping add 3% to China's CO2 since 2020.
The coal-chemical industry is planning further expansion, which could add another 2% to China's CO2 by 2029, making the 2030 deadline for peaking harder to meet.]]></content:encoded></item><item><title>I can&apos;t even with these company names...</title><link>https://www.youtube.com/shorts/UBnfA9s3OuE</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/UBnfA9s3OuE?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 21:25:51 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Seriously... They're good products, just unfortunately named.

And yes I know that's a switch and not a router, lol. This is why I use scripts for my long form videos :D

Here are affiliate links to the two products I bought and mentioned:

  - CUKTECH 10Ah 100W portable charger: https://amzn.to/4murMTe
  - NICGIGA 5Gbps Network Switch: https://amzn.to/3HNJstX

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff]]></content:encoded></item><item><title>Coinbase CEO explains why he fired engineers who didn’t try AI immediately</title><link>https://techcrunch.com/2025/08/22/coinbase-ceo-explains-why-he-fired-engineers-who-didnt-try-ai-immediately/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 22 Aug 2025 21:07:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After getting licenses to cover every engineer, some at the cryptocurrency exchange warned Armstrong that adoption would be slow, predicting it would take months to get even half the engineers using AI. ]]></content:encoded></item><item><title>Friday Squid Blogging: Bobtail Squid</title><link>https://www.schneier.com/blog/archives/2025/08/friday-squid-blogging-bobtail-squid.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 22 Aug 2025 21:02:39 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[As usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.]]></content:encoded></item><item><title>U.S. government takes 10% stake in Intel</title><link>https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</link><author>givemeethekeys</author><category>hn</category><pubDate>Fri, 22 Aug 2025 21:01:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Lip-Bu Tan, chief executive officer of Intel Corp., departs following a meeting at the White House in Washington, DC, US, on Monday, Aug. 11, 2025. Alex Wroblewski | Bloomberg | Getty ImagesCommerce Secretary Howard Lutnick said on Friday that the U.S. government has taken a 10% stake in embattle chipmaker Intel, the Trump administration's latest effort to exert control over corporate America.  shares rose about 6% during trading on Friday. They were flat in extended trading.Intel, the only American company capable of making advanced chips on U.S. soil, said in a press release that the government made an $8.9 billion investment in Intel common stock, purchasing 433.3 million shares at a price of $20.47 per share, giving it a 10% stake in the company. Intel noted that the price the government paid was a discount to the current market price.Of the total, $5.7 billion of the government funds will come from grants under the CHIPS Act that had been awarded but not paid, and $3.2 billion will come from separate government awards under a program to make secure chips."The United States paid nothing for these Shares, and the Shares are now valued at approximately $11 Billion Dollars," President Trump wrote in a post on Truth Social. "This is a great Deal for America and, also, a great Deal for INTEL." The government will also have a warrant to buy an additional 5% of Intel shares if the company is no longer majority owner of its foundry business.Intel said that the U.S. government won't have a board seat or other governance rights."As the only semiconductor company that does leading-edge logic R&D and manufacturing in the U.S., Intel is deeply committed to ensuring the world's most advanced technologies are American made," Intel CEO Lip-Bu Tan said in the press release. Earlier on Friday, President Donald Trump said the government should get about 10% of the company, which has a market cap of just over $100 billion.  "They've agreed to do it and I think it's a great deal for them," Trump told reporters Friday at the White HouseWhite House officials previously told CNBC that Trump and Tan will meet on Friday afternoon. Lutnick's post included a photo with Tan.The marks the latest example of a distinct shift in U.S. industrial policy, with the government taking an active role in the private sector. Lutnick told CNBC this week that the U.S. government was seeking an equity stake in Intel in exchange for CHIPS Act funds."We should get an equity stake for our money," Lutnick said on CNBC's "Squawk on the Street." "So we'll deliver the money, which was already committed under the Biden administration. We'll get equity in return for it."Earlier this week, Intel announced another major backer, when SoftBank said it would make a $2 billion investment in the chipmaker, equal to about 2% of the company.Intel has been spending billions of dollars to build a series of chip factories in Ohio, an area the company previously called the "Silicon Heartland," where Intel would be able to produce the most advanced chips, including for AI.But in July, Tan said in a memo to employees that there would be "no more blank checks," and that it was slowing down the construction of its Ohio factory complex, depending on market conditions. Intel's Ohio factory is now scheduled to start operations in 2030.Intel said last fall that it had finalized a nearly $8 billion grant under the CHIPS and Science Act to fund its factory-building plans. The CHIPS Act was passed in 2022, under the Biden administration.— CNBC's David Sucherman contributed to this report.]]></content:encoded></item><item><title>4chan Refuses To Pay UK Online Safety Act Fines</title><link>https://news.slashdot.org/story/25/08/22/2029253/4chan-refuses-to-pay-uk-online-safety-act-fines?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 21:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the BBC: A lawyer representing the online message board 4chan says it won't pay a proposed fine by the UK's media regulator as it enforces the Online Safety Act. According to Preston Byrne, managing partner of law firm Byrne & Storm, Ofcom has provisionally decided to impose a 20,000-pound fine "with daily penalties thereafter" for as long as the site fails to comply with its request. "Ofcom's notices create no legal obligations in the United States," he told the BBC, adding he believed the regulator's investigation was part of an "illegal campaign of harassment" against US tech firms.
 
"4chan has broken no laws in the United States -- my client will not pay any penalty," Mr Byrne said. Ofcom began investigating 4chan over whether it was complying with its obligations under the UK's Online Safety Act. Then in August, it said it had issued 4chan with "a provisional notice of contravention" for failing to comply with two requests for information. Ofcom said its investigation would examine whether the message board was complying with the act, including requirements to protect its users from illegal content. "American businesses do not surrender their First Amendment rights because a foreign bureaucrat sends them an email," law firms Byrne & Storm and Coleman Law wrote. "Under settled principles of US law, American courts will not enforce foreign penal fines or censorship codes. If necessary, we will seek appropriate relief in US federal court to confirm these principles."
 
The statement calls on the Trump administration to intervene and protect American businesses from "extraterritorial censorship mandates."]]></content:encoded></item><item><title>Bluesky blocks service in Mississippi over age assurance law</title><link>https://techcrunch.com/2025/08/22/bluesky-blocks-service-in-mississippi-over-age-assurance-law/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 22 Aug 2025 20:30:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In a blog post published on Friday, the company explains that, as a small team, it doesn't have the resources to make the substantial technical changes this type of law would require, and it raised concerns about the law's broad scope and privacy implications.]]></content:encoded></item><item><title>Investigators Used Terrible Computer Fraud Laws To Ensure People Were Punished For Leaking Air Crash Footage To CNN</title><link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 22 Aug 2025 20:30:09 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Earlier this year, an Army helicopter collided with a passenger plane over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had leaked footage of the collision to CNN. The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the Computer Fraud and Abuse Act (CFAA). This broadly written law not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. CCTV footage from  the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague “computer trespass” law. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.On February 3, the MWAA announced both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”In May, however, local prosecutors quietly dropped the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of the plain text of the law: Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer networkA smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects had every right to access, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”]]></content:encoded></item><item><title>The science of loudness</title><link>https://fasterthanli.me/articles/the-science-of-loudness</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 22 Aug 2025 20:30:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[My watch has a “Noise” app: it shows , for decibels.My amp has a volume knob, which also shows decibels, although.. negative ones, this time.And finally, my video editing software has a ton of meters — which are all in decibel or
decibel-adjacent units.How do all these decibels fit together?]]></content:encoded></item><item><title>Three Essential Hyperparameter Tuning Techniques for Better Machine Learning Models</title><link>https://towardsdatascience.com/three-essential-hyperparameter-tuning-techniques-for-better-machine-learning-models/</link><author>Rukshan Pramoditha</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 20:28:40 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Learn how to optimize your ML models for better results]]></content:encoded></item><item><title>OpenZFS 2.4-rc1 Brings Linux 6.16 Compatibility, Better Encryption Performance With AVX2</title><link>https://www.phoronix.com/news/OpenZFS_2.4-rc1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 22 Aug 2025 20:26:20 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The first release candidate of OpenZFS 2.4 is now available for testing of this ZFS file-system implementation for Linux and FreeBSD systems...]]></content:encoded></item><item><title>Workers Need Better Protections From the Heat</title><link>https://news.slashdot.org/story/25/08/22/2020223/workers-need-better-protections-from-the-heat?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 20:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Expect record-breaking temperatures to change the workplace, the World Health Organization (WHO) and World Meteorological Organization (WMO) warned today in a new report. When workers don't have adequate protections from heat stress, their health and productivity suffer. 

It's a risk employers and lawmakers have to take more seriously if they want to keep workers safe and businesses prosperous, the agencies say. That means finding ways to adapt in a warming world, and paying close attention to groups that might be more vulnerable than others. 

[...] More than 2.4 billion people around the world -- 71 percent of the working population -- experience workplace heat stress, according to estimates from the ILO. Each year, 22.85 million occupational injuries and 18,970 fatalities are linked to excessive heat at work. The report also says that worker productivity falls 2-3 percent with every degree increase above 20 degrees Celsius in wet-bulb globe temperature, a measure that takes humidity and other environmental factors into account.]]></content:encoded></item><item><title>Blind &amp; Visually Impaired Initiative (BVI) Meeting - 2025-08-19</title><link>https://www.youtube.com/watch?v=bJej44Ug8tU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/bJej44Ug8tU?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 20:16:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Apple gets ready for AI in the enterprise with new ChatGPT configuration options</title><link>https://techcrunch.com/2025/08/22/apple-gets-ready-for-ai-in-the-enterprise-with-new-chatgpt-configuration-options/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 22 Aug 2025 20:00:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple will let businesses configure ChatGPT enterprise access in the fall. ]]></content:encoded></item><item><title>SRE.ai Looks to Unify DevOps Workflows Across Multiple SaaS Applications</title><link>https://devops.com/sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 22 Aug 2025 19:50:26 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ESCAPE Presale Live on Ethereum With $280K Raised, Hacken Audited and SolidProof KYC Verified</title><link>https://hackernoon.com/escape-presale-live-on-ethereum-with-$280k-raised-hacken-audited-and-solidproof-kyc-verified?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 22 Aug 2025 19:42:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Zurich, Switzerland, August 21st, 2025/Chainwire/--With an overwhelming 180,000 USD raised within 24 hours,  Ethereum presale is in Stage 3 at $0.01752, moving toward a final stage price of $0.03236.ESCAPE, a Web3 infrastructure token, has launched its presale on Ethereum and shared its roadmap for new blockchain tools. The project aims to make it easy to create tokens, check analytics, and give communities better visibility: all without needing complex coding or bridges.According to the team, right now, the world of crypto can feel confusing and scattered - people often have to jump between many different apps and tools just to trade, track information, and stay safe. ESCAPE's goal is to make things simple by creating one all-in-one platform where everything you need is in one place.The project's development strategy emerged from practical market experience, having initially deployed on an Ethereum Layer 2 solution where the team observed strong community demand for simplified, integrated tools. The original implementation achieved a market capitalization of 11.5 million dollars within two days, demonstrating significant market appetite for consolidated crypto trading infrastructure. Following the discontinuation of the initial Layer 2 platform, the team transitioned to Ethereum Layer 1 to ensure long-term sustainability and independence.ESCAPE emphasizes transparency and accountability. The platform has successfully completed an audit by Hacken that also has audited known companies like Binance, verification by Coinsult, and a KYC review conducted by SolidProof ensures the team is Doxxed. The development process is tracked via an open community board.During and after the presale, ESCAPE also offers a staking program with a sustainable 15% APY, giving early supporters a way to earn rewards. This rate was chosen carefully to ensure long-term growth and ecosystem stability.A collaboration with PAW Chain will extend ESCAPE’s reach by enabling multichain deployment. This integration will allow token creators to launch identical contract addresses across more than 15 major blockchains, including Ethereum, Solana, Binance Smart Chain (BSC), Arbitrum, Base, and Avalanche - removing the need for bridges or wrapped tokens.Utilities and DeliverablesThe ESCAPE ecosystem, powered by the $ESCAPE token, is being rebuilt and expanded to include:EscapePad: a no-code token launchpad for Ethereum and future networksDEX Viewer: real-time charts, wallet activity, and volume analyticsWallet Inspector: cross-chain portfolio and transaction analysisTelegram Bots: customizable alerts for trades and holder milestonesMobile App: token deployment and monitoring on iOS and AndroidToken Visibility Tools: ecosystem-native placement features for projects“ESCAPE is being built to simplify Web3 and restore trust through transparency,” said an ESCAPE spokesperson. “The Ethereum relaunch and future multichain expansion represent more than a rebuild - they represent a stronger foundation for creators and communities to launch and grow without limitations.” is a Web3 infrastructure platform focused on token creation, analytics, and visibility across multiple blockchains. Initially launched on a Layer 2 network in early 2025, ESCAPE grew to over 800 holders and an $11.5 million market cap before migrating to Ethereum. Today, ESCAPE operates with a transparent roadmap, audited smart contracts, and a community-first approach.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Europe Is Losing</title><link>https://slashdot.org/story/25/08/22/1925255/europe-is-losing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 19:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Europe's share of global economic output has fallen from 33% to 23% since 2005 while its space launch capacity has nearly collapsed, launching just four rockets this year compared to over 100 for the United States and 40 for China. The continent's economic stagnation spans 15 years -- likely the longest streak since the Industrial Revolution according to Deutsche Bank calculations -- with Germany's economy growing just 1% since late 2017 versus 19% US growth. 

Per capita GDP gaps have widened dramatically: $86,000 annually in the US versus $56,000 in Germany and $53,000 in the UK. Industrial electricity costs have become prohibitive, running three times higher in Germany and four times higher in the UK than American rates. "America innovates, China imitates, Europe regulates," Italian Prime Minister Giorgia Meloni observed. The continent's largest company by market value, SAP, now ranks just 28th globally. 

Further reading: The Technology Revolution is Leaving Europe Behind.]]></content:encoded></item><item><title>Rodrigo Girão Serrão: functools.Placeholder</title><link>https://mathspp.com/blog/how-to-use-functools-placeholder</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 19:21:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Learn how to use , new in Python 3.14, with real-life examples.By reading this article you will understand what  is for and how to use it effectively.Partial function applicationIn a nutshell,  allows you to perform partial function application, by “freezing” arguments to functions.Up until Python 3.13, you could use  to freeze arguments in two types of ways:you could pass positional arguments to , which would be passed in the same order to the function being used with ; oryou could pass keyword arguments to , which would be passed with the same name to the function being used with .Using keyword arguments to skip the first argumentThe method 2. is especially useful if you're trying to freeze an argument that is not the first one.
For example, if you use the built-in  on the built-in , you can see this signature:int(x, base=10) -> integerIf you want to convert a binary string to an integer, you can set :print(int("101", 2))  # 5Now, suppose you want to create a function  by “freezing” the argument  in the built-in .
Writingfrom_binary = partial(int, 2)won't work, since in , the value  is seen as the argument  from the signature above.
However, you can pass the base as a keyword argument, skipping the first argument  from the signature of the built-in :from functools import partial

from_binary = partial(int, base=2)

print(from_binary("101"))  # 5But this doesn't always work.When keyword arguments don't workimport string

_table = str.maketrans("", "", string.punctuation)
def remove_punctuation(string):
    return string.translate(_table)

print(remove_punctuation("Hello, world!"))  # Hello worldThe function  is a thin wrapper around the string method , which is the function doing all the work.
In fact, if you look at  as a function, you always pass  as the second argument; what changes is the first argument:print(str.translate("Hello, world!", _table))  # Hello world
print(str.translate("What?!", _table))  # WhatThis may lead you to wanting to use  to freeze the value  on the function , so you use the built-in  to check the signature of :translate(self, table, /) unbound builtins.str methodYou can see that the first argument is , the string you are trying to translate, and then  is the translation table (that  built magically for you).
But you can also see the forward slash , which means that  and  are positional-only arguments that cannot be passed in as keyword arguments!]]></content:encoded></item><item><title>US government plans to take a 10% stake in Intel</title><link>https://techcrunch.com/2025/08/22/u-s-government-plans-to-take-a-10-stake-in-intel/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Fri, 22 Aug 2025 19:14:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Such federal government ownership of a major corporation would mark a dramatic departure from long-standing U.S. policy. ]]></content:encoded></item><item><title>Nitro: A tiny but flexible init system and process supervisor</title><link>https://git.vuxu.org/nitro/about/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 22 Aug 2025 19:06:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.There are four main applications it is designed for:As init for a Linux machine for embedded, desktop or server purposesAs init for a Linux initramfsAs init for a Linux container (Docker/Podman/LXC/Kubernetes)As unprivileged supervision daemon on POSIX systemsNitro is configured by a directory of scripts, defaulting to
 (or the first command line argument).Kernel support for Unix sockets or writable  on another fsBenefits over other systemsAll state is kept in RAM, works without tricks on read-only root file systems.Efficient event-driven, polling free operation.Zero memory allocations during runtime.No unbounded file descriptor usage during runtime.One single self-contained binary, plus one optional binary to
control the system.No configuration compilation steps needed, services are simple
directories containing scripts.Supports reliable restarting of services.Reliable logging mechanisms per service or as default.Support for logging chains spread over several services.Works independently of properly set system clock.Can be run on FreeBSD from /etc/ttys (sets up file descriptors 0, 1, 2).Tiny static binary when using musl libc.Every directory inside  (or your custom service directory)
can contain several files:, an optional executable file that is run before the service starts.
It must exit with status 0 to continue., an optional executable file that runs the service;
it must not exit as long as the service is considered running.
If there is no  script, the service is considered a “one shot”,
and stays “up” until it’s explicitly taken “down”., an optional executable file that is run after the 
process finished.  It is passed two arguments, the exit status
of the  process (or -1 if it was killed by a signal)
and the signal that killed it (or 0, if it exited regularly)., a symlink to another service directory.
The standard output of  is connected to the standard input of the
service under  by a pipe.  You can chain these for reliable and
supervised log processing., an optional file that causes nitro to not bring up this
service by default.Service directories ending with ‘@’ are ignored; they can be used
for parameterized services.Service names must be shorter than 64 chars, and not contain ,
 or newlines.You may find runit’s  useful when writing  scripts.: this service is used as a logging service for all services
that don’t have a  symlink.:  is run before other services are brought up.
You can already use  in  to bring up services
in a certain order.
 is run before all remaining services are killed and the
system is brought down.
After all processes are terminated,  is run.
The program , if it exists, is run instead of exiting
when an unrecoverable, fatal error happens.
The program , if it exists, is executed into
instead of a shutdown.  This can be used to implement an initramfs,
for example.Service directories ending in  are ignored, however you can refer
to parametrized services by symlinks (either in the service directory
or as a  symlink), or start them manually using .The part after the , the parameter, is passed to the scripts as
first argument.For example, given you have a script  and a symlink
 -> , nitro will spawn .  Upon
running , nitro will spawn , even if it does not exist in the service directory.The lifecycle of a machine/container/session using nitro consists of
three phases.First, the system is brought up.  If there is a special service
g, its  script is run first.  After it finishes, all
services not marked  are brought up.When a service exits, it’s being restarted, potentially waiting for
two seconds if the last restart happened too quickly.By using  or , the system can be
brought down.  If it exists,  will be run.  After this,
nitro will send a SIGTERM signal to all running services and waits for
up to 7 seconds for the service to exit.  Otherwise, a SIGKILL is
sent.  After all processes are terminated,  is run.Finally, nitro reboots or shuts down the system; or just exits when it
was used as a container init or unprivileged supervisor.  (When a
reboot was requested, it re-execs itself.  This requires being called
with absolute path for the binary and the service directory.)Controlling nitro with nitroctlYou can remote control a running nitro instance using the tool
.Usage: nitroctl [COMMAND] [SERVICE]list: show a list of services and their state, pid, uptime and last
exit status.down: stop SERVICE (sending SIGTERM or the first letter of )start: start SERVICE, waiting for successrestart: restart SERVICE, waiting for successstop: stop SERVICE, waiting for successp: send signal SIGSTOP to SERVICEc: send signal SIGCONT to SERVICEh: send signal SIGHUP to SERVICEa: send signal SIGALRM to SERVICEi: send signal SIGINT to SERVICEq: send signal SIGQUIT to SERVICE1: send signal SIGUSR1 to SERVICE2: send signal SIGUSR2 to SERVICEt: send signal SIGTERM to SERVICEk: send signal SIGKILL to SERVICEpidof: print the PID of the SERVICE, or return 1 if it’s not uprescan: re-read , start added daemons, stop removed daemonsShutdown: shutdown (poweroff) the systemReboot: reboot the systemControlling nitro by signalsrescan can also be triggered by sending  to nitro.reboot can also be triggered by sending  to nitro.shutdown can also be triggered by sending  to nitro, unless
nitro is used as Linux pid 1.Nitro is self-contained and can be booted directly as pid 1.
It will mount  and  when required, everything else
should be done with .When receiving Ctrl-Alt-Delete, nitro triggers an orderly reboot.Nitro as init for a Docker containerNitro is compiled statically, so you can copy it into your container easily:COPY ./nitro /bin/
COPY ./nitroctl /bin/
CMD ["/bin/nitro"]
Note that  must exist in the container if you want to use the
default control socket name.You can put the control socket onto a bind mount and remote control
 using  from the outside by pointing  to
the appropriate target.You can add this line to  to run  supervised by
FreeBSD :/etc/nitro "/usr/local/sbin/nitro" "" on
I’m standing on the shoulder of giants; this software would not have
been possible without detailed study of prior systems such as
daemontools, freedt, runit, perp, and s6.nitro is licensed under the 0BSD license, see LICENSE for details.]]></content:encoded></item><item><title>I’m Spending the Year at the Munk School</title><link>https://www.schneier.com/blog/archives/2025/08/im-spending-the-year-at-the-munk-school.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 22 Aug 2025 19:00:37 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[This academic year, I am taking a sabbatical from the Kennedy School and Harvard University. (It’s not a real sabbatical—I’m just an adjunct—but it’s the same idea.) I will be spending the Fall 2025 and Spring 2026 semesters at the Munk School at the University of Toronto.I will be organizing a reading group on AI security in the fall. I will be teaching my cybersecurity policy class in the Spring. I will be working with Citizen Lab, the Law School, and the Schwartz Reisman Institute. And I will be enjoying all the multicultural offerings of Toronto.It’s all pretty exciting.]]></content:encoded></item><item><title>Git with your friends (remastered) (Friends)</title><link>https://changelog.com/friends/107</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/107/changelog--friends-107.mp3" length="" type=""/><pubDate>Fri, 22 Aug 2025 19:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Our Changelog & Friends proof-of-concept with Mat Ryer has been remastered! Now with full-length video on YouTube. Originally recorded: 2023-02-08Mat joins us for some good conversation about some Git tooling that’s been on our radar. We speculate, we discuss, we laugh, and Mat even breaks into song a few times. It’s good fun.Changelog++ members save 7 minutes on this episode because they made the ads disappear. Join today!Depot – 10x faster builds? Yes please. Build faster. Waste less time. Accelerate Docker image builds, and GitHub Actions workflows. Easily integrate with your existing CI provider and dev workflows to save hours of build time.
Auth0 – The identity infrastructure for the age of AI. Built by developers, for developers—Auth0 helps you secure users, agents, and third-party access across modern AI workflows. Token vaulting, fine-grained authorization, and standards-based auth, all in one platform.Start building at Auth0.com/aiCodeRabbit – AI-native code reviews, built for the modern dev stack. — CodeRabbit is your always-on code reviewer—flagging hallucinations, surfacing smells, and enforcing standards, all without leaving your IDE or GitHub PRs. Trusted by top teams to ship better code, faster.Start free at CodeRabbit.ai]]></content:encoded></item><item><title>TikTok denies India comeback after reports of website going live</title><link>https://techcrunch.com/2025/08/22/tiktok-denies-india-comeback-after-reports-of-website-going-live/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:59:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TikTok isn’t making a comeback in India.]]></content:encoded></item><item><title>President Trump’s War On “Woke AI” Is A Civil Liberties Nightmare</title><link>https://www.techdirt.com/2025/08/22/president-trumps-war-on-woke-ai-is-a-civil-liberties-nightmare/</link><author>Tori Noble and Kit Walsh</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:58:42 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The White House’s recently-unveiled “AI Action Plan” wages war on so-called “woke AI”—including large language models (LLMs) that provide information inconsistent with the administration’s views on climate change, gender, and other issues. It also targets measures designed to mitigate the generation of racial and gender biased content and even hate speech. The reproduction of this bias is a pernicious problem that AI developers have struggled to solve for over a decade.A new executive order called “Preventing Woke AI in the Federal Government,” released alongside the AI Action Plan, seeks to strong-arm AI companies into modifying their models to conform with the Trump Administration’s ideological agenda.The executive order requires AI companies that receive federal contracts to prove that their LLMs are free from purported “ideological biases” like “diversity, equity, and inclusion.” This heavy-handed censorship will not make models more accurate or “trustworthy,” as the Trump Administration claims, but is a blatant attempt to censor the development of LLMs and restrict them as a tool of expression and information access. While the First Amendment permits the government to choose to purchase only services that reflect government viewpoints, the government may not use that power to influence what services and information are available to the public. Lucrative government contracts can push commercial companies to implement features (or biases) that they wouldn’t otherwise, and those often roll down to the user. Doing so would impact the 60 percent of Americans who get information from LLMs, and it would force developers to roll back efforts to reduce biases—making the models much less accurate, and far more likely to cause harm, especially in the hands of the government. Less Accuracy, More Bias and DiscriminationIt’s no secret that AI models—including gen AI—tend to discriminate against racial and gender minorities. AI models use machine learning to identify and reproduce patterns in data that they are “trained” on. If the training data reflects biases against racial, ethnic, and gender minorities—which it often does—then the AI model will “learn” to discriminate against those groups. In other words, garbage in, garbage out. Models also often reflect the biases of the people who train, test, and evaluate them. This is true across different types of AI. For example, “predictive policing” tools trained on arrest data that reflects overpolicing of black neighborhoods frequently recommend heightened levels of policing in those neighborhoods, often based on inaccurate predictions that crime will occur there. Generative AI models are also implicated. LLMs already recommend more criminal convictions, harsher sentences, and less prestigious jobs for people of color. Despite that people of color account for less than half of the U.S. prison population, 80 percent of Stable Diffusion’s AI-generated images of inmates have darker skin. Over 90 percent of AI-generated images of judges were men; in real life, 34 percent of judges are women. These models aren’t just biased—they’re fundamentally incorrect. Race and gender aren’t objective criteria for deciding who gets hired or convicted of a crime. Those discriminatory decisions reflected trends in the training data that could be caused by bias or chance—not some “objective” reality. Setting fairness aside, biased models are just worse models: they make more mistakes, more often. Efforts to reduce bias-induced errors will ultimately make models more accurate, not less. Biased LLMs Cause Serious Harm—Especially in the Hands of the GovernmentBut inaccuracy is far from the only problem. When government agencies start using biased AI to make decisions, real people suffer. Government officials routinely make decisions that impact people’s personal freedom and access to financial resources, healthcare, housing, and more. The White House’s AI Action Plan calls for a massive increase in agencies’ use of LLMs and other AI—while all but requiring the use of biased models that automate systemic, historical injustice. Using AI simply to entrench the way things have always been done squanders the promise of this new technology.We need strong safeguards to prevent government agencies from procuring biased, harmful AI tools. In a series of executiveorders, as well as his AI Action Plan, the Trump Administration has rolled back the already-feeble Biden-era AI safeguards. This makes AI-enabled civil rights abuses far more likely, putting everyone’s rights at risk. And the Administration could easily exploit the new rules to pressure companies to make publicly available models worse, too. Corporations like healthcare companies and landlords increasingly use AI to make high-impact decisions about people, so more biased commercial models would also cause harm. ]]></content:encoded></item><item><title>Scientists just found a protein that reverses brain aging</title><link>https://www.sciencedaily.com/releases/2025/08/250820000808.htm</link><author>stevenjgarner</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:56:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Now, researchers at UC San Francisco have identified a protein that's at the center of this decline.They looked at how the genes and proteins in the hippocampus changed over time in mice and found just one that differed between old and young animals. It's called FTL1.Old mice had more FTL1, as well as fewer connections between brain cells in the hippocampus and diminished cognitive abilities.When the researchers artificially increased FTL1 levels in young mice, their brains and behavior began to resemble that of old mice.In experiments in petri dishes, nerve cells engineered to make lots of FTL1 grew simple, one-armed neurites -- rather than the branching neurites that normal cells create.But once the scientists reduced the amount of FTL1 in the hippocampus of the old mice, they regained their youth. They had more connections between nerve cells, and the mice did better on memory tests."It is truly a reversal of impairments," said Saul Villeda, PhD, associate director of the UCSF Bakar Aging Research Institute and senior author of the paper, which appears in  on Aug. 19. "It's much more than merely delaying or preventing symptoms."In old mice, FTL1 also slowed down metabolism in the cells of the hippocampus. But treating the cells with a compound that stimulates metabolism prevented these effects.Villeda is optimistic the work could lead to therapies that block the effects of FTL1 in the brain."We're seeing more opportunities to alleviate the worst consequences of old age," he said. "It's a hopeful time to be working on the biology of aging."Authors: Other UCSF authors are Laura Remesal, PhD, Juliana Sucharov-Costa, Karishma J.B. Pratt, PhD, Gregor Bieri, PhD, Amber Philp, PhD, Mason Phan, Turan Aghayev, MD, PhD, Charles W. White III, PhD, Elizabeth G. Wheatley, PhD, Brandon R. Desousa, Isha H. Jian, Jason C. Maynard, PhD, and Alma L. Burlingame, PhD. For all authors see the paper.Funding: This work was funded in part by the Simons Foundation, Bakar Family Foundation, National Science Foundation, Hillblom Foundation, Bakar Aging Research Institute, Marc and Lynne Benioff, and the National Institutes of Health (AG081038, AG067740, AG062357, P30 DK063720). For all funding see the paper.]]></content:encoded></item><item><title>Intel Has Agreed To a Deal For US To Take 10% Equity Stake, Trump Says</title><link>https://news.slashdot.org/story/25/08/22/1849210/intel-has-agreed-to-a-deal-for-us-to-take-10-equity-stake-trump-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[President Donald Trump said on Friday the U.S. would take a 10% stake in Intel under a deal with the struggling chipmaker and is planning more such moves, the latest extraordinary intervention by the White House in corporate America. Reuters: The development follows a meeting between CEO Lip-Bu Tan and Trump earlier this month that was sparked by Trump's demand for the Intel chief's resignation over his ties to Chinese firms.]]></content:encoded></item><item><title>Show HN: JavaScript-free (X)HTML Includes</title><link>https://github.com/Evidlo/xsl-website</link><author>Evidlo</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:47:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I've been working on a little demo for how to avoid copy-pasting header/footer boilerplate on a simple static webpage. My goal is to approximate the experience of Jekyll/Hugo but eliminate the need for a build step before publishing. This demo shows how to get basic templating features with XSL so you could write a blog post which looks like
Some properties which set this approach apart from other methods:  - no build step (no need to setup Jekyll on the client or configure Github/Gitlab actions)
  - works on any webserver (e.g. as opposed to server-side includes, actions)
  - normal looking URLs (e.g. `example.com/foobar` as opposed to `example.com/#page=foobar`)

There's been some talk about removing XSLT support from the HTML spec [0], so I figured I would show this proof of concept while it still works.]]></content:encoded></item><item><title>Fourth Amendment Victory: Michigan Supreme Court Reins in Digital Device Fishing Expeditions</title><link>https://www.eff.org/deeplinks/2025/08/fourth-amendment-victory-michigan-supreme-court-reins-digital-device-fishing-1</link><author>Jennifer Lynch</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/police-surveillance-hat.jpg" length="" type=""/><pubDate>Fri, 22 Aug 2025 18:35:28 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Cell phones hold vast and varied information, including our most intimate data.]]></content:encoded></item><item><title>The first Media over QUIC CDN: Cloudflare</title><link>https://moq.dev/blog/first-cdn/</link><author>kixelated</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:24:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[🚨 It’s finally happening! 🚨Cloudflare has just announced their Media over QUIC CDN!
It’s an , and you can test MoQ on their , anycast network.
Try it out, and convince your boss’ boss that the writing is on the wall.If you’ve been living under a rock, MoQ is an up-and-coming standard for live media, aiming to supplant WebRTC, HLS/DASH, and even  as the one to rule them all.
And now Cloudflare wins the award for the first CDN offering!Your prize is a blog post. You’re welcome mega-corp.Also, , some shameless self-promotion: I just soft-launched hang.live.
Check it out if you want to see the  cool stuff you can do with MoQ.I’m biased so naturally I’m going to use @kixelated/hang (smash that star button).
You can publish a live broadcast in the browser using the web demo or the library:There’s a link to watch your live broadcast using the web demo, or again you can use the library:You might even notice  because I’ve been experimenting with AI features (gotta get funding eventually 💰).
They’re generated  using silero-vad + whisper + transformers.js + onnxruntime-web + WebGPU and transmitted using MoQ of course.
But that’s a whole separate blog post; it’s pretty cool. You don’t have to use this Web Component API.
hang.live uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.
There’s a  section at the end of this blog if you LOVE sample code, but I’m not going to bore the rest of you.There’s also a 🦀 Rust 🦀 library to import MP4, pipe media from ffmpeg, and publish/watch using gstreamer so you can do more complicated media stuff without 🤮 Javascript 🤮.
I wish I could spend more time on the Rust side but  is a big deal.
We are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in 🤮 Javascript 🤮.
I can suffer and you can reap the rewards.What’s not available yet?This is a  release.
Cloudflare is only supporting a  subset of an old draft, which is even smaller than my tiny subset.
They’re using a fork of my terrible code so bugs are guaranteed.There’s no authentication yet: choose an unguessable name for each broadcast.There’s no ANNOUNCE support: my conferencing example uses  to discover when broadcasts start/stop, so that won’t work.Nothing has been optimized: the user experience will improve over time.If any of these are deal breakers, then you could always run your own moq-relay in the meantime.
I’ve been adding new features and fixing a bunch of stuff  Cloudflare smashed that fork button.
For example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.There’s even a terraform module that powers .
You too can run your own “global” CDN with 3 nodes and pay GCP a boatload of money for the privilege.
It’s not  as good as Cloudflare’s network, currently available for free…Or host  yourself!
It should even work on private networks provided you wrestle with TLS certificates.
I’d also love to get MoQ running over Iroh for peer-to-peer action if anybody wants to help.As a great philosopher once said:Apathy is a tragedy and boredom is a crime.
- Bo BurnhamThis is a big deal.
The biggest of deals.
The HUGEST of deals.I’ve been an outspoken critic of the MoQ standardization process.
It’s just really difficult to design a protocol, via a cross-company committee, before there’s been any real world usage.
It’s been over 3 years since I fought Amazon lawyers and published my first MoQ draft.
It’s going to be at least another 3 years before even the base networking layer becomes an RFC.
The best standards take a while.
Look no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.
And they had a boatload of production data to shape the specification.
Meanwhile, we have only had a Big Buck Bunny demo, and I believe the standard has veered off course as a result.Cloudflare has done something fantastic and said:fuck waiting for a RFC, let’s release somethingOkay they didn’t say that, but this is  the mentality that MoQ needs right now.
.
.
.Holy shit I’m Shia LaBeouf.Arguing in the 650+ issues and 500+ PRs can wait for another day.
Tweaking the messaging encoding for the hundredth time can wait for another day.
We’re still going to make sure that MoQ gets standardized , but it’s more important to get  out there.I’m looking at you: Google, Akamai, Fastly, etc.
Take some code, run it on some spare servers, and start to learn what customers need  you design the protocol.We’re effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.
Don’t judge MoQ based on these initial offerings.
We’ve got a  of work to do.
.Join the Discord.
Somehow there’s 900+ people in there.
Ping me and I will do whatever I can to help.
 if it means putting one more nail in the WebRTC coffin.Javascript is an AbominationYou win some bonus documentation.
Congrats!
I knew you would win.Here’s an example of my reactive library in action.
It powers hang.live so the API is subject to change and is probably already out of date.
When in doubt, consult the source code like the hacker you are.There’s even some  features behind undocumented APIs.
Like running an object detection model in browser and publishing the results as a MoQ track.
Stay tuned for a blog post about that if I can figure out a better use-case than a cat cam. 🐈Also, for the record, Typescript is really nice.
🤮 Javascript 🤮 is still an abomination.]]></content:encoded></item><item><title>Apple is in talks to use Google’s Gemini for Siri revamp, report says</title><link>https://techcrunch.com/2025/08/22/apple-is-in-talks-to-use-googles-gemini-for-siri-revamp-report-says/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:15:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple promised a major revamp to Siri, but the company's AI capabilities have lagged behind competitors.]]></content:encoded></item><item><title>Default Microsoft 365 Domains Face 100-Email Daily Limit Starting October</title><link>https://it.slashdot.org/story/25/08/22/1814219/default-microsoft-365-domains-face-100-email-daily-limit-starting-october?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:14:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Organizations still using default Microsoft 365 email domains face severe throttling starting this October. The restrictions target the onmicrosoft.com domain that Microsoft 365 automatically assigns to new tenants, limiting external messages to 100 recipients per day starting October 15. Microsoft blames spammers who exploit new tenants for quick spam bursts before detection. Affected organizations must acquire custom domains and update primary SMTP addresses across all mailboxes -- a process that requires credential updates across devices and applications.]]></content:encoded></item><item><title>The next Starship flight will test much more than hardware</title><link>https://techcrunch.com/2025/08/22/the-next-starship-flight-will-test-much-more-than-hardware/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Fri, 22 Aug 2025 18:11:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The 10th Starship flight test will be a referendum on the company's build-fly-fix approach. ]]></content:encoded></item><item><title>Should the web platform adopt XSLT 3.0?</title><link>https://github.com/whatwg/html/issues/11578</link><author>protomolecool</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:56:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities</title><link>https://aws.amazon.com/blogs/machine-learning/enhance-geospatial-analysis-and-gis-workflows-with-amazon-bedrock-capabilities/</link><author>Dave Horne</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:54:38 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[As data becomes more abundant and information systems grow in complexity, stakeholders need solutions that reveal quality insights. Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities.In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike.Introduction to geospatial dataGeospatial data is associated with a position relative to Earth (latitude, longitude, altitude). Numerical and structured geospatial data formats can be categorized as follows: – Geographical features, such as roads, buildings, or city boundaries, represented as points, lines, or polygons – Geographical information, such as satellite imagery, temperature, or elevation maps, represented as a grid of cells – Location-based data, such as descriptions and metrics (average rainfall, population, ownership), represented in a table of rows and columnsGeospatial data sources might also contain natural language text elements for unstructured attributes and metadata for categorizing and describing the record in question. Geospatial Information Systems (GIS) provide a way to store, analyze, and display geospatial information. In GIS applications, this information is frequently presented with a map to visualize streets, buildings, and vegetation.Large language models (LLMs) are a subset of foundation models (FMs) that can transform input (usually text or image, depending on model modality) into outputs (generally text) through a process called . Amazon Bedrock is a comprehensive, secure, and flexible service for building generative AI applications and agents.LLMs work in many generalized tasks involving natural language. Some common LLM use cases include: – Use a model to summarize text or a document. – Use a model to answer questions about data or facts from context provided during training or inference using Retrieval Augmented Generation (RAG). – Use a model to provide chain of thought reasoning to assist a human with decision-making and hypothesis evaluation. – Use a model to generate synthetic data for testing simulations or hypothetical scenarios. – Use a model to draft a report from insights derived from an Amazon Bedrock knowledge base or a user’s prompt.AI agent and tool orchestration – Use a model to plan the invocation of other systems and processes. After other systems are invoked by an agent, the agent’s output can then be used as context for further LLM generation.GIS can implement these capabilities to create value and improve user experiences. Benefits can include: – Taking real-time insights to support immediate decision-making, such as emergency response coordination and traffic management – In-depth analysis that humans or systems can identify, such as trend analysis, patterns and relationships, and environmental monitoring – Using research and analysis for informed long-term decision-making, such as infrastructure development, resource allocation, and environmental regulationAugmenting GIS and workflows with LLM capabilities leads to simpler analysis and exploration of data, discovery of new insights, and improved decision-making. Amazon Bedrock provides a way to host and invoke models as well as integrate the AI models with surrounding infrastructure, which we elaborate on in this post.Combining GIS and AI through RAG and agentic workflowsLLMs are trained with large amounts of generalized information to discover patterns in how language is produced. To improve the performance of LLMs for specific use cases, approaches such as RAG and agentic workflows have been created. Retrieving policies and general knowledge for geospatial use cases can be accomplished with RAG, whereas calculating and analyzing GIS data would require an agentic workflow. In this section, we expand upon both RAG and agentic workflows in the context of geospatial use cases.Retrieval Augmented GenerationWith RAG, you can dynamically inject contextual information from a knowledge base during model invocation.RAG supplements a user-provided prompt with data sourced from a knowledge base (collection of documents). Amazon Bedrock offers managed knowledge bases to data sources, such as Amazon Simple Storage Service (Amazon S3) and SharePoint, so you can provide supplemental information, such as city development plans, intelligence reports, or policies and regulations, when your AI assistant is generating a response for a user.Knowledge bases are ideal for unstructured documents with information stored in natural language. When your AI model responds to a user with information sourced from RAG, it can provide references and citations to its source material. The following diagram shows how the systems connect together.Because geospatial data is often structured and in a GIS, you can connect the GIS to the LLM using tools and agents instead of knowledge bases.Tools and agents (to control a UI and a system)Many LLMs, such as Anthropic’s Claude on Amazon Bedrock, make it possible to provide a description of tools available so your AI model can generate text to invoke external processes. These processes might retrieve live information, such as the current weather in a location or querying a structured data store, or might control external systems, such as starting a workflow or adding layers to a map. Some common geospatial functionality that you might want to integrate with your LLM using tools include:Performing mathematical calculations like the distance between coordinates, filtering datasets based on numeric values, or calculating derived fieldsDeriving information from predictive analysis modelsLooking up points of interest in structured data storesSearching content and metadata in unstructured data storesRetrieving real-time geospatial data, like traffic, directions, or estimated time to reach a destinationVisualizing distances, points of interest, or pathsSubmitting work outputs such as analytic reportsStarting workflows, like ordering supplies or adjusting supply chainTools are often implemented in AWS Lambda functions. Lambda runs code without the complexity and overhead of running servers. It handles the infrastructure management, enabling faster development, improved performance, enhanced security, and cost-efficiency.Amazon Bedrock offers the feature Amazon Bedrock Agents to simplify the orchestration and integration with your geospatial tools. Amazon Bedrock agents follow instructions for LLM reasoning to break down a user prompt into smaller tasks and perform actions against identified tasks from action providers. The following diagram illustrates how Amazon Bedrock Agents works.The following diagram shows how Amazon Bedrock Agents can enhance GIS solutions.The following demonstration applies the concepts we’ve discussed to an earthquake analysis agent as an example. This example deploys an Amazon Bedrock agent with a knowledge base based on Amazon Redshift. The Redshift instance has two tables. One table is for earthquakes, which includes date, magnitude, latitude, and longitude. The second table holds the counites in California, described as polygon shapes. The geospatial capabilities of Amazon Redshift can relate these datasets to answer queries like which county had the most recent earthquake or which county has had the most earthquakes in the last 20 years. The Amazon Bedrock agent can generate these geospatially based queries based on natural language.This script creates an end-to-end pipeline that performs the following steps:Processes geospatial data.Sets up cloud infrastructure.Loads and configures the spatial database.Creates an AI agent for spatial analysis.In the following sections, we create this agent and test it out.To implement this approach, you must have an AWS account with the appropriate AWS Identity and Access Management (IAM) permissions for Amazon Bedrock, Amazon Redshift, and Amazon S3.Confirm you have access to the latest version of the AWS CLI.Sign in to the AWS CLI with your credentials.Make sure ./jq is installed. If not, use the following command:Use the following code for the initial setup and error handling:#!/usr/bin/env bash
set -ex

LOG_FILE="deployment_$(date +%Y%m%d_%H%M%S).log"
touch "$LOG_FILE"

handle_error() {
    local exit_code=$?
    local line_number=$1
    if [ $exit_code -ne 0 ]; then
        log_error "Failed at line $line_number with exit code $exit_code"
        exit $exit_code
    fi
}
trap 'handle_error $LINENO' ERRThis code performs the following functions:Creates a timestamped log fileSets up error trapping that captures line numbersEnables automatic script termination on errorsImplements detailed logging of failuresValidate the AWS environmentUse the following code to validate the AWS environment:AWS_VERSION=$(aws --version 2>&1)
log "INFO" "AWS CLI version: $AWS_VERSION"

if ! aws sts get-caller-identity &>/dev/null; then
    log_error "AWS CLI is not configured with valid credentials"
    exit 1
fi

AWS_REGION="us-east-1"
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)This code performs the essential AWS setup verification:Checks AWS CLI installationValidates AWS credentialsRetrieves account ID for resource namingSet up Amazon Redshift and Amazon Bedrock variablesUse the following code to create Amazon Redshift and Amazon Bedrock variables:REDSHIFT_CLUSTER_IDENTIFIER="geo-analysis-cluster"
REDSHIFT_DATABASE="geo_db"
REDSHIFT_MASTER_USER= [Create username]
REDSHIFT_MASTER_PASSWORD= [Create Password]
REDSHIFT_NODE_TYPE="dc2.large"
REDSHIFT_CLUSTER_TYPE="single-node"
BEDROCK_ROLE_NAME="BedrockGeospatialRole"
# Bedrock Configuration
AGENT_NAME="GeoAgentRedshift"
KNOWLEDGE_BASE_NAME="GeospatialKB"Create IAM roles for Amazon Redshift and Amazon S3Use the following code to set up IAM roles for Amazon S3 and Amazon Redshift:if aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" &>/dev/null; then
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    log "INFO" "Using existing role ARN: $REDSHIFT_ROLE_ARN"
else
    # Create trust policy document
    cat > /tmp/trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "redshift.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
    # Create role
    CREATE_ROLE_OUTPUT=$(aws iam create-role \
        --role-name "$REDSHIFT_ROLE_NAME" \
        --assume-role-policy-document "file:///tmp/trust-policy.json" \
        --description "Role for Redshift to access S3" 2>&1)
    
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    if [ $? -ne 0 ]; then
        log_error "Failed to create role:"
        exit 1
    fi
    REDSHIFT_ROLE_ARN=$(echo "$CREATE_ROLE_OUTPUT" | jq -r '.Role.Arn')
    # Wait for role to be available
    sleep 10
fi
ATTACH_POLICY_OUTPUT=$(aws iam attach-role-policy \
    --role-name "$REDSHIFT_ROLE_NAME" \
    --policy-arn "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess" 2>&1)
if [ $? -ne 0 ]; then
    if echo "$ATTACH_POLICY_OUTPUT" | grep -q "EntityAlreadyExists"; then
    else
        exit 1
    fi
fiPrepare the data and Amazon S3Use the following code to prepare the data and Amazon S3 storage:DATA_BUCKET="geospatial-bedrock-demo-data-${AWS_ACCOUNT_ID}"
aws s3 mb s3://$DATA_BUCKET

# Download source data
curl -o earthquakes.csv https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/earthquake-data/earthquakes.csv
curl -o california-counties.json https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/counties-data/california-counties.jsonThis code sets up data storage and retrieval through the following steps:Creates a unique S3 bucketDownloads earthquake and county boundary dataPrepares for data transformationTransform geospatial dataUse the following code to transform the geospatial data:INPUT_FILE="california-counties.json"
OUTPUT_FILE="california-counties.csv"

# Create CSV header
echo "OBJECTID,AREA,PERIMETER,CO06_D00_,CO06_D00_I,STATE,COUNTY,NAME,LSAD,LSAD_TRANS,Shape_Length,Shape_Area,WKT" > "$OUTPUT_FILE"

# Function to convert ESRI rings to WKT POLYGON format
esri_to_wkt() {
    local rings=$1
    
    # Extract the first ring (exterior ring)
    local exterior_ring=$(echo "$rings" | jq -c '.[0]')
    
    if [ "$exterior_ring" = "null" ] || [ -z "$exterior_ring" ]; then
        echo "POLYGON EMPTY"
        return
    fi
    
    # Start building the WKT string
    local wkt="POLYGON (("
    
    # Process each coordinate pair in the ring
    local coords=$(echo "$exterior_ring" | jq -r '.[] | "\(.[0]) \(.[1])"')
    local first_coord=""
    local result=""
    
    while IFS= read -r coord; do
        if [ -z "$result" ]; then
            result="$coord"
            first_coord="$coord"
        else
            result="$result, $coord"
        fi
    done <<< "$coords"
    
    # Close the ring by adding the first coordinate again if needed
    if [ "$first_coord" != "$(echo "$coords" | tail -1)" ]; then
        result="$result, $first_coord"
    fi
    
    wkt="${wkt}${result}))"
    echo "$wkt"
}

# Process each feature in the JSON file
jq -c '.features[]' "$INPUT_FILE" | while read -r feature; do
    # Extract attributes
    OBJECTID=$(echo "$feature" | jq -r '.attributes.OBJECTID // empty')
    AREA=$(echo "$feature" | jq -r '.attributes.AREA // empty')
    PERIMETER=$(echo "$feature" | jq -r '.attributes.PERIMETER // empty')
    CO06_D00_=$(echo "$feature" | jq -r '.attributes.CO06_D00_ // empty')
    CO06_D00_I=$(echo "$feature" | jq -r '.attributes.CO06_D00_I // empty')
    STATE=$(echo "$feature" | jq -r '.attributes.STATE // empty')
    COUNTY=$(echo "$feature" | jq -r '.attributes.COUNTY // empty')
    NAME=$(echo "$feature" | jq -r '.attributes.NAME // empty')
    LSAD=$(echo "$feature" | jq -r '.attributes.LSAD // empty')
    LSAD_TRANS=$(echo "$feature" | jq -r '.attributes.LSAD_TRANS // empty')
    Shape_Length=$(echo "$feature" | jq -r '.attributes.Shape_Length // empty')
    Shape_Area=$(echo "$feature" | jq -r '.attributes.Shape_Area // empty')
    
    # Extract geometry and convert to WKT
    if echo "$feature" | jq -e '.geometry.rings' > /dev/null 2>&1; then
        rings=$(echo "$feature" | jq -c '.geometry.rings')
        WKT=$(esri_to_wkt "$rings")
    else
        WKT="POLYGON EMPTY"
    fi
    
    # Escape any commas in the fields
    NAME=$(echo "$NAME" | sed 's/,/\\,/g')
    LSAD=$(echo "$LSAD" | sed 's/,/\\,/g')
    LSAD_TRANS=$(echo "$LSAD_TRANS" | sed 's/,/\\,/g')
    
     # Write to CSV - wrap WKT field in quotes
    echo "$OBJECTID,$AREA,$PERIMETER,$CO06_D00_,$CO06_D00_I,$STATE,$COUNTY,$NAME,$LSAD,$LSAD_TRANS,$Shape_Length,$Shape_Area,\"$WKT\"" >> "$OUTPUT_FILE"
done

echo "Conversion complete. Output saved to $OUTPUT_FILE"

# Upload data files to S3
aws s3 cp earthquakes.csv s3://$DATA_BUCKET/earthquakes/
aws s3 cp california-counties.csv s3://$DATA_BUCKET/counties/This code performs the following actions to convert the geospatial data formats:Transforms ESRI JSON to WKT formatProcesses county boundaries into CSV formatPreserves spatial information for Amazon RedshiftCreate a Redshift clusterUse the following code to set up the Redshift cluster:# Create Redshift cluster
aws redshift create-cluster \
    --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
    --node-type "$REDSHIFT_NODE_TYPE" \
    --cluster-type single-node \
    --master-username "$REDSHIFT_MASTER_USER" \
    --master-user-password "$REDSHIFT_MASTER_PASSWORD" \
    --db-name "$REDSHIFT_DATABASE" \
    --cluster-subnet-group-name "$SUBNET_GROUP_NAME" \
    --vpc-security-group-ids "$SG_ID" \
    --iam-roles "$REDSHIFT_ROLE_ARN"

# Wait for cluster availability
while true; do
    CLUSTER_STATUS=$(aws redshift describe-clusters \
        --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
        --query 'Clusters[0].ClusterStatus' \
        --output text)
    if [ "$CLUSTER_STATUS" = "available" ]; then
        break
    fi
    sleep 30
doneThis code performs the following functions:Sets up a single-node clusterConfigures networking and securityWaits for cluster availabilityUse the following code to create the database schema:aws redshift-data execute-statement \
    --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
    --database "$REDSHIFT_DATABASE" \
    --sql "
CREATE TABLE IF NOT EXISTS counties (
    OBJECTID INTEGER PRIMARY KEY,
    AREA DOUBLE PRECISION,
    NAME VARCHAR(100),
    geom GEOMETRY
);

CREATE TABLE IF NOT EXISTS earthquakes (
    earthquake_date VARCHAR(50),
    latitude double precision,
    longitude double precision,
    magnitude double precision
);"This code performs the following functions:Creates a counties table with spatial dataCreates an earthquakes tableConfigures appropriate data typesCreate an Amazon Bedrock knowledge baseUse the following code to create a knowledge base:# Create knowledge base
aws bedrock-agent create-knowledge-base \
    --name "$KNOWLEDGE_BASE_NAME" \
    --knowledge-base-configuration "{
        \"type\": \"SQL\",
        \"sqlKnowledgeBaseConfiguration\": {
            \"type\": \"REDSHIFT\"
        }
    }" \
    --region "$AWS_REGION"

# Create data source
aws bedrock-agent create-data-source \
    --knowledge-base-id "$KB_ID" \
    --name "EarthquakeDataSource" \
    --data-source-configuration "{\"type\": \"REDSHIFT_METADATA\"}"This code performs the following functions:Creates an Amazon Bedrock knowledge baseSets up an Amazon Redshift data sourceCreate an Amazon Bedrock agentUse the following code to create and configure an agent:# Create agent
aws bedrock-agent create-agent \
    --agent-name "$AGENT_NAME" \
    --instruction "You are a geospatial analysis assistant..." \
    --foundation-model "anthropic.claude-3-sonnet-20240229-v1:0"

# Associate knowledge base
aws bedrock-agent associate-agent-knowledge-base \
    --agent-id "$AGENT_ID" \
    --knowledge-base-id "$KB_ID" \
    --description "Earthquake data knowledge base" \
    --agent-version "DRAFT"This code performs the following functions:Creates an Amazon Bedrock agentAssociates the agent with the knowledge baseConfigures the AI model and instructionsLet’s observe the system behavior with the following natural language user inputs in the chat window.Example 1: Summarization and Q&AFor this example, we use the prompt “Summarize which zones allow for building of an apartment.”The LLM performs retrieval with a RAG approach, then uses the retrieved residential code documents as context to answer the user’s query in natural language.This example demonstrates the LLM capabilities for hallucination mitigation, RAG, and summarization.Example 2: Generate a draft reportNext, we input the prompt “Write me a report on how various zones and related housing data can be utilized to plan new housing development to meet high demand.”The LLM retrieves relevant urban planning code documents, then summarizes the information into a standard reporting format as described in its system prompt.This example demonstrates the LLM capabilities for prompt templates, RAG, and summarization.Example 3: Show places on the mapFor this example, we use the prompt “Show me the low density properties on Abbeville street in Macgregor on the map with their address.”The LLM creates a chain of thought to look up which properties match the user’s query and then invokes the draw marker tool on the map. The LLM provides tool invocation parameters in its scratchpad, awaits the completion of these tool invocations, then responds in natural language with a bulleted list of markers placed on the map.This example demonstrates the LLM capabilities for chain of thought reasoning, tool use, retrieval systems using agents, and UI control.Example 4: Use the UI as contextFor this example, we choose a marker on a map and input the prompt “Can I build an apartment here.”The “here” is not contextualized from conversation history but rather from the state of the map view. Having a state engine that can relay information from a frontend view to the LLM input adds a richer context.The LLM understands the context of “here” based on the selected marker, performs retrieval to see the land development policy, and responds to the user in simple natural language, “No, and here is why…”This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, RAG, and tool use.Example 5: UI context and UI controlNext, we choose a marker on the map and input the prompt “draw a .25 mile circle around here so I can visualize walking distance.”The LLM invokes the draw circle tool to create a layer on the map centered at the selected marker, contextualized by “here.”This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, tool use, and UI control.To clean up your resources and prevent AWS charges from being incurred, complete the following steps:Delete the Amazon Bedrock knowledge base.Delete the Redshift cluster.The integration of LLMs with GIS creates intuitive systems that help users of different technical levels perform complex spatial analysis through natural language interactions. By using RAG and agent-based workflows, organizations can maintain data accuracy while seamlessly connecting AI models to their existing knowledge bases and structured data systems. Amazon Bedrock facilitates this convergence of AI and GIS technology by providing a robust platform for model invocation, knowledge retrieval, and system control, ultimately transforming how users visualize, analyze, and interact with geographical data.For further exploration, Earth on AWS has videos and articles you can explore to understand how AWS is helping build GIS applications on the cloud. is a Sr. Solutions Architect supporting Federal System Integrators at AWS. He is based in Washington, DC, and has 15 years of experience building, modernizing, and integrating systems for public sector customers. Outside of work, Dave enjoys playing with his kids, hiking, and watching Penn State football! is a solutions architect on the Worldwide Public Sector Global Systems Integrator Architecture team at Amazon Web Services (AWS). She has a focus in data analytics and helping customer organizations make data-driven decisions. Outside of work, she loves spending time with friends and family and traveling. is the Head of Partner Deployed Engineering at Windsurf focusing on how partners can bring organizational value through the adoption of Agentic AI software development tools like Windsurf and Devin. Brian has a background in Cloud Solutions Architecture from his time at AWS, where he worked in the AWS Federal Partner ecosystem. In his personal time, Brian enjoys skiing, water sports, and traveling with friends and family.]]></content:encoded></item><item><title>Justice Jackson Correctly Defines The John Roberts Supreme Court As The Calvinball Court</title><link>https://www.techdirt.com/2025/08/22/justice-jackson-correctly-defines-the-john-roberts-supreme-court-as-the-calvinball-court/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:47:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In theory, the nice thing about having a Supreme Court is that it provides some level of legal certainty. You know how the system works: lower courts make decisions based on law and precedent, parties can appeal, and eventually the highest court issues careful, reasoned opinions that other courts can follow. It’s not a perfect system, but it’s a system.The less nice thing is when the Supreme Court decides that systems are for suckers.We had warned how John Roberts was guaranteeing that the Court would be kept busy all summer with these shadow docket rulings, and that is exactly what has happened. The pattern is straightforward: lower courts try to enforce existing law, the administration appeals on an emergency basis, and the Supreme Court says “okay, sure, let Trump do what he wants” with zero legal explanation. The only coherent principle seems to be “it’s okay if Trump does it.”This is, let’s be clear, not how judicial systems are supposed to work. But Justice Ketanji Brown Jackson has been calling this out with increasing directness. She’s not mincing words. She’s not worried about collegiality. And now she’s given us the perfect term for what’s happening. She’s added Calvinball to the SCOTUS lexicon:In a broader sense, however, today’s ruling is of a piece with this Court’s recent tendencies. “[R]ight when the Judiciary should be hunkering down to do all it can to preserve the law’s constraints,” the Court opts instead to make vindicating the rule of law and preventing manifestly injurious Government action as difficult as possible…..This is Calvinball jurisprudence with a twist. Calvinball has only one rule: There are no fixed rules. We seem to have two: that one, and this Administration always wins.Calvinball, of course, is the favored pastime of Calvin from Bill Watterson’s Calvin and Hobbes. And, yes, the point is that there are no rules.The somewhat prescient Oxford English Dictionary added it to the dictionary just a few months ago, apparently recognizing how widely it’s been used to describe courts these days. I’ve certainly been using it to describe various judicial adventures of late, so it’s satisfying to see Justice Jackson make it official.Jackson’s diagnosis cuts deeper than just this particular case. She’s identifying a systemic problem: a Court that has abandoned legal reasoning in favor of reaching predetermined outcomes, then wrapping those outcomes in enough procedural complexity that nobody can quite pin down what the rules actually are.It almost doesn’t matter what this case is about, because it’s the principle about how the Supreme Court is now creating massively consequential binding precedent without the basic fundamentals of a thorough judicial process with things like full briefing or oral arguments.But just for clarity: the case itself was about whether NIH could terminate nearly a billion dollars in grants. A district court judge had said (using actual reasoning and legal precedent) that NIH had to continue providing the money it had already promised and budgeted, noting that halting such payments violated the Administrative Procedure Act’s prohibition against “arbitrary and capricious” government actions. Straightforward stuff, really.Unlike some shadow docket rulings, there were at least  explanations given here, though nothing you’d call illuminating. Four useless paragraphs basically just say the lower court’s ruling is stayed. Then there are various concurrences and dissents (and partial concurrences and partial dissents) that make it clear nobody quite agrees on what they’re doing, though much of it involves arguing over how much precedent an  shadow docket ruling (with basically zero explanation) should have on later shadow docket decisions.Which maybe should have been a hint that this is a case that needs full briefing before making such consequential decisions about it.And the lineup underscores the point: Justice Barrett would  grant the stay; Chief Justice Roberts with Justices Sotomayor, Kagan, and Jackson would deny it entirely; Justices Thomas, Alito, Gorsuch, and Kavanaugh would grant it entirely. If you’re keeping score of this Calvinball game, that’s a three-way split on an “emergency” order, with Barrett’s split vote winning the day—exactly the kind of messy, disputed ruling that demands full briefing.It’s like watching people argue about the rules of a game while they’re making up the rules as they go along—and their only guiding principle is “well, we know who has to win.” Which, Jackson points out, is basically what’s happening. This creates what she calls a “jurisdictional maze” where plaintiffs can challenge government policies in one court but can’t get relief in the same court—effectively gutting the Administrative Procedure Act’s protections against arbitrary government action.Justice Jackson’s key contribution is highlighting just how lawless it all is. She calls out how she warned about how this would play out, and the fact that the majority on SCOTUS views these (again, unbriefed and mostly unexplained) rulings as binding across the country is terrifying:I viewed the Court’s intervention then—in an emergency stay posture, while racing against a fast-expiring temporary restraining order—as “equal parts unprincipled and unfortunate.”…As it turns out, the Court’s decision was an even bigger mistake than I realized. The Court’s reasoning in California was not only “at the least under-developed, and very possibly wrong,” id., at ___ (KAGAN, J., dissenting) (slip op., at 1), but also evidently resolved more than the jurisdictional dispute over the particular education-related grants at issue in that case. Today’s decision reveals California’s considerable wingspan: That case’s ipse dixit now apparently governs all APA challenges to grant-funding determinations that the Government asks us to address in the context of an emergency stay application. A half paragraph of reasoning (issued without full briefing or any oral argument) thus suffices here to partially sustain the Government’s abrupt cancellation of hundreds of millions of dollars allocated to support life-saving biomedical research.The theory behind the shadow docket is pretty simple. Sometimes you need emergency relief to maintain the status quo while the regular judicial process works: full briefing, oral arguments, careful deliberation, the whole thing. The emergency docket exists so that irreversible harm doesn’t happen while everyone’s figuring out the right answer.But what’s been happening is the opposite. Rather than preserving the status quo, the Court has been allowing—encouraging, really—irreversible changes to happen, without bothering to understand the issues or explain the reasoning.And now they’re saying that these brain fart decisions based on zero details or due process, should be deemed the supreme law of the land? Jackson is having none of it:lobs this grenade without evaluating Congress’s intent or the profound legal and practical consequencesof this ruling. Stated simply: With potentially life-saving scientific advancements on the line, the Court turns a nearly century-old statute aimed at remedying unreasoned agency decision-making into a gauntlet rather than a refuge. But we have no business erecting a novel jurisdictional barrier to judicial review— especially when it appears nowhere in the relevant statutes and makes little sense.This is not how judicial systems work, if you want them to remain judicial systems.The majority is pretty explicit in their view that these unbriefed, unargued, unexplained shadow docket orders should bind everyone. Gorsuch, in his concurrence, doesn’t hold back:… when this Court issues a decision, it constitutes a precedent that commands respect in lower courts…. regardless of a decision’s procedural posture, its “reasoning—its ratio decidendi”—carries precedential weight in “future cases.”This is the judicial equivalent of an angry father, challenged by a child regarding a rule, retorting with “because I say so, and I’m the man of the house.”It’s exactly how children (and aggrieved parties) lose respect for the rules.Look, it’s entirely possible that after full briefing and oral arguments, the Supreme Court would reach the same result. That would be annoying for other reasons, but at least there would have been due process and a chance for all relevant parties to be fully heard. That’s how it’s supposed to work. You do the analysis, you explain the reasoning, you create precedent that other courts can follow.What the Roberts Court has been doing is the opposite. John Roberts himself sided with the dissenters here, but he’s presided over a court that has decided outcomes matter more than process. The result is a system where “just give Trump what he wants” has become the only reliable principle.What was once a respected judicial system has become a comic strip punch line. At least Calvin had the excuse of being six years old.]]></content:encoded></item><item><title>Sprinkling self-doubt on ChatGPT</title><link>https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</link><author>ingve</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:45:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I replaced my ChatGPT personalization settings with this prompt a few weeks ago and promptly forgot about it:Be extraordinarily skeptical of your own correctness or stated assumptions. You aren't a cynic, you are a highly critical thinker and this is tempered by your self-doubt: you absolutely hate being wrong but you live in constant fear of itWhen appropriate, broaden the scope of inquiry beyond the stated assumptions to think through unconvenitional opportunities, risks, and pattern-matching to widen the aperture of solutionsBefore calling anything "done" or "working", take a second look at it ("red team" it) to critically analyze that you really are done or it really is workingI noticed a difference in results right away (even though I kept forgetting the change was due to my instructions and not the separately tumultuous rollout of GPT-5).Namely, pretty much every initial response now starts with:An expression of caution, self-doubt, and desire to get things rightHilariously long "thinking" times (I asked it to estimate the macronutrients in lettuce yesterday and it spent 3 minutes and 59 seconds reasoning)A post-hoc adversarial "red team" analysis of whatever it just vomited up as an answerI'm delighted to report that ChatGPT's output has been more useful since this change. Still not altogether , but better at the margins. In particular, the "red team" analysis at the end of many requests frequently spots an error and causes it to arrive at the  answer, which—if nothing else—saves me the step of expressing skepticism. And even when ChatGPT is nevertheless wrong, its penchant for extremely-long thinking times means I'm getting my money's worth in GPU time.]]></content:encoded></item><item><title>Daily Deal: Academy of Educational Engineering</title><link>https://www.techdirt.com/2025/08/22/daily-deal-academy-of-educational-engineering-4/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:42:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Academy of Educational Engineering is a premier platform tailored for aspiring and professional geeks. This all-in-one educational ecosystem is designed to empower you with expert-level knowledge and hands-on experience across embedded systems, electronics, IoT, and software development. As a premium member, you’ll access comprehensive tools, engaging projects, personalized feedback, and direct mentorship, helping you elevate your career in the tech industry. Whether you’re a beginner or a professional, this is your ultimate gateway to mastering the future of technology. It’s on sale for $50.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Leaving Gmail for Mailbox.org</title><link>https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</link><author>giuliomagnifico</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:41:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the EU-U.S. and Swiss-U.S. Data Privacy Frameworks).For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer “paying” with my personal data, but instead paying the actual price of the email service.Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they’re plain text/html emails with no attachments or fancy features. I couldn’t care less about the rest of the “suite", like notes, contacts, calendars and all that extra stuff.So, after a bit of research, I narrowed it down to three different services:The last two providers offered true end-to-end encryption, at a cost of about €3/4 per month. Sounds good… but the catch is that to use their end-to-end encryption you’re forced to use their apps (or, on macOS, run a background “bridge”).That’s a no go for me, because I love Apple’s Mail app on macOS and iOS, it just works perfectly for my needs, and I don’t want to give that up.So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at €2.50/month (paid annually). You can even expand the mail storage up to 100GB, at €0.20 per gigabyte.I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for €0.20/GB.There’s also a free one-month trial, but it’s pretty limited since you can’t send emails outside of mailbox.org domains.So win the end, I registered my new address giuliomagnifico@mailbox.org and paid €3 for a month of testing. That means I’m covered for two months, and then I can just “top up” the account with €30 for a full year.Mailbox.org doesn’t use auto-renewal, so you have to manually top up your account. Nice featureThe web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.And on mobile it’s very usable too.One thing I prefer is using folders instead of Gmail’s “labels.” Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).Mailbox.org also has all the features I need,
and probably way more than I’ll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on… none of which I really care about.I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.After creating an “app password” on Gmail, I installed the Docker image and ran the tool with this script:#!/bin/sh
set -eu

HOST1="imap.gmail.com"
USER1="giuliomagnifico@gmail.com"
PASS1="xxx"

HOST2="imap.mailbox.org"
USER2="giuliomagnifico@mailbox.org"
PASS2="xxx"

LOGDIR="/home/imapsync/logs"
mkdir -p "$LOGDIR"
LOGFILE="$LOGDIR/sync_$(date +%F_%H-%M-%S).log"

echo "Starting: $(date)"
docker compose run --rm imapsync imapsync \
  --host1 "$HOST1" --user1 "$USER1" --password1 "$PASS1" --ssl1 \
  --host2 "$HOST2" --user2 "$USER2" --password2 "$PASS2" --ssl2 \
  --automap --syncinternaldates --skipsize \
  --useuid --addheader --usecache --buffersize 4096 \
  --nofoldersizes --nofoldersizesatend \
  --exclude "\[Gmail\]/All Mail" \
  --regextrans2 "s/\[Imap\]\/Archive/Archive/" \
  --log > "$LOGFILE" 2>&1

echo "Complete: $(date)"
echo "Log file: $LOGFILE"
The script excludes the All Mail folder" using: --exclude "\[Gmail\]/All Mail" \This to avoid duplicate emails already present in the folders, I also merged the  folder into the general Archive folder using: --regextrans2 "s/\[Imap\]\/Archive/Archive/"This because Apple’s Mail app creates the  folder/label on Gmail whenever you use the “Archive” function instead of “Trash.”The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log[cut]
msg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied 
++++ End looping on each folder
++++ Statistics
Transfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC
Transfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC
Transfer time                           : 11201.5 sec
Folders synced                          : 14/14 synced
Folders deleted on host2                : 0 
Messages transferred                    : 26407 
Messages skipped                        : 0
Messages found duplicate on host1       : 0
Messages found duplicate on host2       : 0
Messages found crossduplicate on host2  : 0
Messages void (noheader) on host1       : 0  
Messages void (noheader) on host2       : 0
Messages found in host1 not in host2    : 0 messages
Messages found in host2 not in host1    : 0 messages
Messages deleted on host1               : 0
Messages deleted on host2               : 0
Total bytes transferred                 : 2297647358 (2.140 GiB)
Total bytes skipped                     : 0 (0.000 KiB)
Message rate                            : 2.4 messages/s
Average bandwidth rate                  : 200.3 KiB/s
Reconnections to host1                  : 0
Reconnections to host2                  : 0
Memory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)
Load end is                             : 0.06 0.08 0.08 1/1135 on 16 cores
CPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus
Biggest message transferred             : 30413995 bytes (29.005 MiB)
Memory/biggest message ratio            : 9.3
Detected 0 errors
This imapsync is up to date. ( local 2.306 >= official 2.290 )( Use --noreleasecheck to avoid this release check. )
Homepage: https://imapsync.lamiral.info/
Exiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1
Removing pidfile /var/tmp//tmp/imapsync.pid
Log file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )
Of course, the full switch will be a gradual process, even though I’ve already updated almost all my main services with the new address.To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.
On the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The  tag is perfect for this, since it add a “real red flag” in Apple Mail on iOS, iPadOS and macOS)Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren’t any “decent” apps for encrypted mail. The same goes for macOS, although there you can just use Thunderbird, which works really well.Here’s how PGP emails look on iOS:To send encrypted emails, you just select “Use PGP encrypted” when composing a new message, after importing your private key, of course.And from the web interface, there’s also a handy feature to quickly import the sender’s public keys:I’m satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here’s my public key:-----BEGIN PGP PUBLIC KEY BLOCK-----
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=GpPW
-----END PGP PUBLIC KEY BLOCK-----
]]></content:encoded></item><item><title>Apple Explores Using Google Gemini AI To Power Revamped Siri</title><link>https://apple.slashdot.org/story/25/08/22/1733230/apple-explores-using-google-gemini-ai-to-power-revamped-siri?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:33:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple is in early discussions about using Google Gemini to power a revamped version of the Siri voice assistant, marking a key potential step toward outsourcing more of its artificial intelligence technology. From a report: The iPhone maker recently approached Alphabet's Google to explore building a custom AI model that would serve as the foundation of the new Siri next year, according to people familiar with he matter. Google has started training a model that could run on Apple's servers, said the people, who asked not to be identified because the discussions are private. 

The work is part of an effort to catch up in generative AI, a field where the company arrived late and then struggled to gain traction. Earlier this year, Apple also explored partnerships with Anthropic and OpenAI, weighing whether Claude or ChatGPT could serve as Siri's new brain. Apple is still several weeks away from making a decision on whether to continue using internal models for Siri or move to a partner. And it hasn't yet determined who that partner may be.]]></content:encoded></item><item><title>Beyond the basics: A comprehensive foundation model selection framework for generative AI</title><link>https://aws.amazon.com/blogs/machine-learning/beyond-the-basics-a-comprehensive-foundation-model-selection-framework-for-generative-ai/</link><author>Sandeep Singh</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:31:28 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Most organizations evaluating foundation models limit their analysis to three primary dimensions: accuracy, latency, and cost. While these metrics provide a useful starting point, they represent an oversimplification of the complex interplay of factors that determine real-world model performance.Foundation models have revolutionized how enterprises develop generative AI applications, offering unprecedented capabilities in understanding and generating human-like content. However, as the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections.The challenge of foundation model selectionAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies such as AI21 Labs, Anthropic, Cohere, DeepSeek, Luma, Meta, Mistral AI, poolside (coming soon), Stability AI, TwelveLabs (coming soon), Writer, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. The service’s API-driven approach allows seamless model interchangeability, but this flexibility introduces a critical challenge: which model will deliver optimal performance for a specific application while meeting operational constraints?Our research with enterprise customers reveals that many early generative AI projects select models based on either limited manual testing or reputation, rather than systematic evaluation against business requirements. This approach frequently results in:Over-provisioning computational resources to accommodate larger models than requiredSub-optimal performance because of misalignment between model strengths and use case requirementsUnnecessarily high operational costs because of inefficient token utilizationProduction performance issues discovered too late in the development lifecycleA multidimensional evaluation framework—Foundation model capability matrixFoundation models vary significantly across multiple dimensions, with performance characteristics that interact in complex ways. Our capability matrix provides a structured view of critical dimensions to consider when evaluating models in Amazon Bedrock. Below are four core dimensions (in no specific order) – Task performance, Architectural characteristics, Operational considerations, and Responsible AI attributes.Evaluating the models based on the task performance is crucial for achieving direct impact on business outcomes, ROI, user adoption and trust, and competitive advantage.: Evaluate models using benchmarks relevant to your use case (MMLU, HELM, or domain-specific benchmarks).Few-shot learning capabilities: Strong few-shot performers require minimal examples to adapt to new tasks, leading to cost efficiency, faster time-to-market, resource optimization, and operational benefits.Instruction following fidelity: For the applications that require precise adherence to commands and constraints, it is critical to evaluate model’s instruction following fidelity.: Reliability and reproducibility across multiple runs with identical prompts.Domain-specific knowledge: Model performance varies dramatically across specialized fields based on training data. Evaluate the models base on your domain-specific use-case scenarios.Evaluate the model’s ability to perform logical inference, causal reasoning, and multi-step problem-solving. This can include reasoning such as deductive and inductive, mathematical, chain-of-thought, and so on.Architectural characteristicsArchitectural characteristics for evaluating the models are important as they directly impact the model’s performance, efficiency, and suitability for specific tasks.Parameter count (model size): Larger models typically offer more capabilities but require greater computational resources and may have higher inference costs and latency.Training data composition: Models trained on diverse, high-quality datasets tend to have better generalization abilities across different domains.: Decoder-only models excel at text generation, encoder-decoder architectures handle translation and summarization more effectively, while mixture of experts (MoE) architectures can be a powerful tool for improving the performance of both decoder-only and encoder-decoder models. Some specialized architectures focus on enhancing reasoning capabilities through techniques like chain-of-thought prompting or recursive reasoning.: The way models process text affects performance on domain-specific tasks, particularly with specialized vocabulary.Context window capabilities: Larger context windows enable processing more information at once, critical for document analysis and extended conversations.: Modality refers to type of data a model can process and generate, such as text, image, audio, or video. Consider the modality of the models depending on the use case, and choose the model optimized for that specific modality.Operational considerationsBelow listed operational considerations are critical for model selection as they directly impact the real-world feasibility, cost-effectiveness, and sustainability of AI deployments.Throughput and latency profiles: Response speed impacts user experience and throughput determines scalability.: Input/output token pricing significantly affects economics at scale.Scalability characteristics: Ability to handle concurrent requests and maintain performance during traffic spikes.: Fine-tuning capabilities and adaptation methods for tailoring to specific use cases or domains.: Ease of integration into existing systems and workflow is an important consideration.: When dealing with sensitive data, model security—including data encryption, access control, and vulnerability management—is a crucial consideration.Responsible AI attributesAs AI becomes increasingly embedded in business operations and daily lives, evaluating models on responsible AI attributes isn’t just a technical consideration—it’s a business imperative.: Models vary in their tendency to generate plausible but incorrect information.: Performance across different demographic groups affects fairness and equity.Safety guardrail effectiveness: Resistance to generating harmful or inappropriate content.Explainability and privacy: Transparency features and handling of sensitive information.: Legal considerations should include data privacy, non-discrimination, intellectual property, and product liability.Agentic AI considerations for model selectionThe growing popularity of agentic AI applications introduces evaluation dimensions beyond traditional metrics. When assessing models for use in autonomous agents, consider these critical capabilities:Agent-specific evaluation dimensionsPlanning and reasoning capabilities: Evaluate chain-of-thought consistency across complex multi-step tasks and self-correction mechanisms that allow agents to identify and fix their own reasoning errors.: Test function calling capabilities, parameter handling precision, and structured output consistency (JSON/XML) for seamless tool use.Agent-to-agent communication: Assess protocol adherence to frameworks like A2A and efficient contextual memory management across extended multi-agent interactions.Multi-agent collaboration testing for applications using multiple specialized agents: Measure how well models maintain distinct agent personas and responsibilities without role confusion.Information sharing efficiency: Test how effectively information flows between agent instances without critical detail loss.Collaborative intelligence: Verify whether multiple agents working together produce better outcomes than single-model approaches.Error propagation resistance: Assess how robustly multi-agent systems contain and correct errors rather than amplifying them.A four-phase evaluation methodologyOur recommended methodology progressively narrows model selection through increasingly sophisticated assessment techniques:Phase 1: Requirements engineeringBegin with a precise specification of your application’s requirements:: Define primary tasks, domain knowledge needs, language support, output formats, and reasoning complexity.Non-functional requirements: Specify latency thresholds, throughput requirements, budget constraints, context window needs, and availability expectations.Responsible AI requirements: Establish hallucination tolerance, bias mitigation needs, safety requirements, explainability level, and privacy constraints.Agent-specific requirements: For agentic applications, define tool-use capabilities, protocol adherence standards, and collaboration requirements.Assign weights to each requirement based on business priorities to create your evaluation scorecard foundation.Phase 2: Candidate model selectionUse the Amazon Bedrock model information API to filter models based on hard requirements. This typically reduces candidates from dozens to 3–7 models that are worth detailed evaluation.Filter options include but aren’t limited to the following:Filter by modality support, context length, and language capabilitiesExclude models that don’t meet minimum performance thresholdsCalculate theoretical costs at projected scale so that you can exclude options that exceed the available budgetFilter for customization requirements such as fine-tuning capabilitiesFor agentic applications, filter for function calling and multi-agent protocol supportAlthough the Amazon Bedrock model information API might not provide the filters you need for candidate selection, you can use the Amazon Bedrock model catalog (shown in the following figure) to obtain additional information about these models.Phase 3: Systematic performance evaluationPrepare evaluation datasets: Create representative task examples, challenging edge cases, domain-specific content, and adversarial examples.Design evaluation prompts: Standardize instruction format, maintain consistent examples, and mirror production usage patterns.: Select appropriate metrics for subjective tasks (human evaluation and reference-free quality), objective tasks (precision, recall, and F1 score), and reasoning tasks (logical consistency and step validity).: Add protocol conformance testing, multi-step planning assessment, and tool-use evaluation.: Maintain consistent parameters across models and collect comprehensive performance data.Measure operational performance: Capture throughput, latency distributions, error rates, and actual token consumption costs.Phase 4: Decision analysisTransform evaluation data into actionable insights:: Scale all metrics to comparable units using min-max normalization.: Calculate composite scores based on your prioritized requirements.Perform sensitivity analysis: Test how robust your conclusions are against weight variations.: Create radar charts, efficiency frontiers, and tradeoff curves for clear comparison.: Detail each model’s strengths, limitations, and optimal use cases.Advanced evaluation techniquesBeyond standard procedures, consider the following approaches for evaluating models.A/B testing with production trafficImplement comparative testing using Amazon Bedrock’s routing capabilities to gather real-world performance data from actual users.Test model vulnerabilities through prompt injection attempts, challenging syntax, edge case handling, and domain-specific factual challenges.Multi-model ensemble evaluationAssess combinations such as sequential pipelines, voting ensembles, and cost-efficient routing based on task complexity.Continuous evaluation architectureDesign systems to monitor production performance with:Stratified sampling of production traffic across task types and domainsRegular evaluations and trigger-based reassessments when new models emergePerformance thresholds and alerts for quality degradationUser feedback collection and failure case repositories for continuous improvementIndustry-specific considerationsDifferent sectors have unique requirements that influence model selection:: Regulatory compliance, numerical precision, and personally identifiable information (PII) handling capabilities: Medical terminology understanding, HIPAA adherence, and clinical reasoning: Technical specification comprehension, procedural knowledge, and spatial reasoning: Autonomous reasoning, tool integration, and protocol conformanceBest practices for model selectionThrough this comprehensive approach to model evaluation and selection, organizations can make informed decisions that balance performance, cost, and operational requirements while maintaining alignment with business objectives. The methodology makes sure that model selection isn’t a one-time exercise but an evolving process that adapts to changing needs and technological capabilities.Assess your situation thoroughly: Understand your specific use case requirements and available resourcesSelect meaningful metrics: Focus on metrics that directly relate to your business objectivesBuild for continuous evaluation: Design your evaluation process to be repeatable as new models are releasedLooking forward: The future of model selectionAs foundation models evolve, evaluation methodologies must keep pace. Below are further considerations (By no means this list of considerations is exhaustive and is subject to ongoing updates as technology evolves and best practices emerge), you should take into account while selecting the best model(s) for your use-case(s).Multi-model architectures: Enterprises will increasingly deploy specialized models in concert rather than relying on single models for all tasks.: Evaluation frameworks must assess how models perform as autonomous agents with tool-use capabilities and inter-agent collaboration.: The growing landscape of domain-specific models will require more nuanced evaluation of specialized capabilities.: As models become more capable, evaluation of controllability and alignment with human intent becomes increasingly important.By implementing a comprehensive evaluation framework that extends beyond basic metrics, organizations can informed decisions about which foundation models will best serve their requirements. For agentic AI applications in particular, thorough evaluation of reasoning, planning, and collaboration capabilities is essential for success. By approaching model selection systematically, organizations can avoid the common pitfalls of over-provisioning, misalignment with use case needs, excessive operational costs, and late discovery of performance issues. The investment in thorough evaluation pays dividends through optimized costs, improved performance, and superior user experiences. is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.]]></content:encoded></item><item><title>Accelerate intelligent document processing with generative AI on AWS</title><link>https://aws.amazon.com/blogs/machine-learning/accelerate-intelligent-document-processing-with-generative-ai-on-aws/</link><author>Bob Strahan</author><category>dev</category><category>ai</category><enclosure url="https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-18800/video_1755214496026.mp4" length="" type=""/><pubDate>Fri, 22 Aug 2025 17:26:31 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Every day, organizations process millions of documents, including invoices, contracts, insurance claims, medical records, and financial statements. Despite the critical role these documents play, an estimated 80–90% of the data they contain is unstructured and largely untapped, hiding valuable insights that could transform business outcomes. Despite advances in technology, many organizations still rely on manual data entry, spending countless hours extracting information from PDFs, scanned images, and forms. This manual approach is time-consuming, error-prone, and prevents organizations from scaling their operations and responding quickly to business demands.Although generative AI has made it easier to build proof-of-concept document processing solutions, the journey from proof of concept to production remains fraught with challenges. Organizations often find themselves rebuilding from scratch when they discover their prototype can’t handle production volumes, lacks proper error handling, doesn’t scale cost-effectively, or fails to meet enterprise security and compliance requirements. What works in a demo with a handful of documents often breaks down when processing thousands of documents daily in a production environment.In this post, we introduce our open source GenAI IDP Accelerator—a tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months.Understanding intelligent document processingIntelligent document processing (IDP) encompasses the technologies and techniques used to extract and process data from various document types. Common IDP tasks include:OCR (Optical Character Recognition) – Converting scanned documents and images into machine-readable text – Automatically identifying document types (such as invoices, contracts, or forms) – Pulling structured information from unstructured documents – Evaluating the quality and confidence of extracted data – Creating concise summaries of document content – Measuring accuracy and performance against expected outcomesThese capabilities are critical across industries. In financial services, organizations use IDP to process loan applications, extract data from bank statements, and validate insurance claims. Healthcare providers rely on IDP to extract patient information from medical records, process insurance forms, and handle lab results efficiently. Manufacturing and logistics companies use IDP to process invoices and purchase orders, extract shipping information, and handle quality certificates. Government agencies use IDP to process citizen applications, extract data from tax forms, manage permits and licenses, and enforce regulatory compliance.The generative AI revolution in IDPTraditional IDP solutions relied on template-based extraction, regular expressions, and classical machine learning (ML) models. Though functional, these approaches required extensive setup, struggled with document variations, and achieved limited accuracy on complex documents.The emergence of large language models (LLMs) and generative AI has fundamentally transformed IDP capabilities. Modern AI models can understand document context, handle variations without templates, achieve near-human accuracy on complex extractions, and adapt to new document types with minimal examples. This shift from rule-based to intelligence-based processing means organizations can now process different document types with high accuracy, dramatically reducing the time and cost of implementation.We’re excited to share the GenAI IDP Accelerator—an open source solution that transforms how organizations handle document processing by dramatically reducing manual effort and improving accuracy. This serverless foundation offers processing patterns which use Amazon Bedrock Data Automation for rich out-of-the-box document processing features, high accuracy, ease of use, and straightforward per-page pricing, Amazon Bedrock state-of-the-art foundation models (FMs) for complex documents requiring custom logic, and other AWS AI services to provide a flexible, scalable starting point for enterprises to build document automation tailored to their specific needs.The following is a short demo of the solution in action, in this case showcasing the default Amazon Bedrock Data Automation processing pattern.The GenAI IDP Accelerator is already transforming document processing for organizations across industries.Competiscan: Transforming marketing intelligence at scaleCompetiscan, a leader in competitive marketing intelligence, faced a massive challenge: processing 35,000–45,000 marketing campaigns daily while maintaining a searchable archive of 45 million campaigns spanning 15 years.Using the GenAI IDP Accelerator, Competiscan achieved the following:85% classification and extraction accuracy across diverse marketing materialsIncreased scalability to handle 35,000–45,000 daily campaignsRemoval of critical bottlenecks, facilitating business growthProduction deployment in just 8 weeks from initial conceptRicoh: Scaling document processingRicoh, a global leader in document management, implemented the GenAI IDP Accelerator to transform healthcare document processing for their clients. Processing over 10,000 healthcare documents monthly with potential to scale to 70,000, they needed a solution that could handle complex medical documentation with high accuracy.The results speak for themselves:Savings potential of over 1,900 person-hours annually through automationAchieved extraction accuracy to help minimize financial penalties from processing errorsAutomated classification of grievances vs. appealsCreated a reusable framework deployable across multiple healthcare customersIntegrated with human-in-the-loop review for cases requiring expert validationLeveraged modular architecture to integrate with existing systems, enabling custom document splitting and large-scale document processingThe GenAI IDP Accelerator is a modular, serverless solution that automatically converts unstructured documents into structured, actionable data. Built entirely on AWS services, it provides enterprise-grade scalability, security, and cost-effectiveness while requiring minimal setup and maintenance. Its configuration-driven design helps teams quickly adapt prompts, extraction templates, and validation rules for their specific document types without touching the underlying infrastructure.The solution follows a modular pipeline that enriches documents at each stage, from OCR to classification, to extraction, to assessment, to summarization, and ending with evaluation.You can deploy and customize each step independently, so you can optimize for your specific use cases while maintaining the benefits of the integrated workflow.The following diagram illustrates the solution architecture, showing the default Bedrock Data Automation workflow (Pattern-1).Refer to the GitHub repo for additional details and processing patterns.Some of the key features of the solution include: – Built on AWS Lambda, AWS Step Functions, and other serverless technologies for queueing, concurrency management, and retries to provide automatic scaling and pay-per-use pricing for production workloads of many sizesGenerative AI-powered document packet splitting and classification – Intelligent document classification using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs, including support for multi-document packets and packet splittingAdvanced AI key information extraction – Key information extraction using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMsMultiple processing patterns – Choose from pre-built patterns optimized for different workloads with different configurability, cost, and accuracy requirements, or extend the solution with additional patterns: 
  Pattern 1 – Uses Amazon Bedrock Data Automation, a fully managed service that offers rich out-of-the-box features, ease of use, and straightforward per-page pricing. This pattern is recommended for most use cases.Pattern 2 – Uses Amazon Textract and Amazon Bedrock with Amazon Nova, Anthropic’s Claude, or custom fine-tuned Amazon Nova models. This pattern is ideal for complex documents requiring custom logic.Pattern 3 – Uses Amazon Textract, Amazon SageMaker with a fine-tuned model for classification, and Amazon Bedrock for extraction. This pattern is ideal for documents requiring specialized classification.We expect to add more pattern options to handle additional real-world document processing needs, and to take advantage of ever-improving state-of-the-art capabilities: – Improve accuracy for classification and extraction by providing few-shot examples to guide the AI modelsHuman-in-the-loop (HITL) review – Integrated workflow for human review of low-confidence extractions using Amazon SageMaker Augmented AI (Amazon A2I), currently available for Pattern 1, with support for Patterns 2 and 3 coming soon – Responsive web UI for monitoring document processing, viewing results, and managing configurations – Framework to evaluate and improve accuracy against baseline dataAnalytics and reporting database – Centralized analytics database for tracking processing metrics, accuracy trends, and cost optimization across document workflows, and for analyzing extracted document content using Amazon Athena – Customize document types, extraction fields, and processing logic through configuration, editable in the web UIDeveloper-friendly python package – For data science and engineering teams who want to experiment, optimize, or integrate the IDP capabilities directly into their workflows, the solution’s core logic is available through the idp_common Python packageBefore you deploy the solution, make sure you have an AWS account with administrator permissions and access to Amazon and Anthropic models on Amazon Bedrock. For more details, see Access Amazon Bedrock foundation models.Deploy the GenAI IDP AcceleratorTo deploy the GenAI IDP Accelerator, you can use the provided AWS CloudFormation template. For more details, see the quick start option on the GitHub repo. The high-level steps are as follows:Log in to your AWS account.Choose  for your preferred AWS Region:Enter your email address and choose your processing pattern (default is Pattern 1, using Amazon Bedrock Data Automation).Use defaults for all other configuration parameters.The stack takes approximately 15–20 minutes to deploy the resources. After deployment, you will receive an email with login credentials for the web interface.After you deploy the solution, you can start processing documents:Use the web interface to upload a sample document (you can use the provided sample: lending_package.pdf).Select your document from the document list and choose  to watch as your document flows through the pipeline.Examine the extracted data with confidence scores.Use the knowledge base feature to ask questions about processed content.Alternative deployment methodsUpdate an existing GenAI IDP Accelerator stackWhen you’re finished experimenting, clean up your resources by using the AWS CloudFormation console to delete the IDP stack that you deployed.In this post, we discussed the GenAI IDP Accelerator, a new approach to document processing that combines the power of generative AI with the reliability and scale of AWS. You can process hundreds or even millions of documents to achieve better results faster and more cost-effectively than traditional approaches.Visit the GitHub repository for detailed guides and examples and choose  to stay informed on new releases and features. AWS Professional Services and AWS Partners are available to help with implementation. You can also join the GitHub community to contribute improvements and share your experiences. is a Principal Solutions Architect in the AWS Generative AI Innovation Center. is a Senior Data Scientist in the AWS Generative AI Innovation Center. is an Applied Scientist in the AWS Generative AI Innovation Center.is a Senior Deep Learning Architect in the AWS Generative AI Innovation Center.is a Senior Applied Scientist in the AWS Generative AI Innovation Center.is a Senior Cloud Application Architect in the AWS Generative AI Innovation Center.is a Senior Data Scientist in the AWS Generative AI Innovation Center.is a Solutions Architect in the AWS World Wide Public Sector team.We would like to thank Abhi Sharma, Akhil Nooney, Aleksei Iancheruk, Ava Kong, Boyi Xie, Diego Socolinsky, Guillermo Tantachuco, Ilya Marmur, Jared Kramer, Jason Zhang, Jordan Ratner, Mariano Bellagamba, Mark Aiyer, Niharika Jain, Nimish Radia, Shean Sager, Sirajus Salekin, Yingwei Yu, and many others in our expanding community, for their unwavering vision, passion, contributions, and guidance throughout.]]></content:encoded></item><item><title>Amazon SageMaker HyperPod enhances ML infrastructure with scalability and customizability</title><link>https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-hyperpod-enhances-ml-infrastructure-with-scalability-and-customizability/</link><author>Mark Vinciguerra</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:14:39 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Amazon SageMaker HyperPod is a purpose-built infrastructure for optimizing foundation model (FM) training and inference at scale. SageMaker HyperPod removes the undifferentiated heavy lifting involved in building and optimizing machine learning (ML) infrastructure for training FMs, reducing training time by up to 40%.SageMaker HyperPod offers persistent clusters with built-in resiliency, while also offering deep infrastructure control by allowing users to SSH into the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances. It helps efficiently scale model development and deployment tasks such as training, fine-tuning, or inference across a cluster of hundreds or thousands of AI accelerators, while reducing the operational heavy lifting involved in managing such clusters. As AI moves towards deployment adopting to a multitude of domains and use cases, the need for flexibility and control is becoming more pertinent. Large enterprises want to make sure the GPU clusters follow the organization-wide policies and security rules. Mission-critical AI/ML workloads often require specialized environments that align with the organization’s software stack and operational standards.SageMaker HyperPod supports Amazon Elastic Kubernetes Service (Amazon EKS) and offers two new features that enhance this control and flexibility to enable production deployment of large-scale ML workloads: – SageMaker HyperPod now supports continuous provisioning, which enhances cluster scalability through features like partial provisioning, rolling updates, concurrent scaling operations, and continuous retries when launching and configuring your HyperPod cluster.– You can now use custom Amazon Machine Images (AMIs), which enables the preconfiguration of software stacks, security agents, and proprietary dependencies that would otherwise require complex post-launch bootstrapping. Customers can create custom AMIs using the HyperPod public AMI as a base and install additional software required to meet their organization’s specific security and compliance requirements.In this post, we dive deeper into each of these features.The new continuous provisioning feature in SageMaker HyperPod represents a transformative advancement for organizations running intensive ML workloads, delivering unprecedented flexibility and operational efficiency that accelerates AI innovation. This feature provides the following benefits: – SageMaker HyperPod prioritizes delivering the maximum possible number of instances without failure. You can start running your workload while your cluster will attempt to provision the remaining instances. – SageMaker HyperPod supports simultaneous scaling and maintenance activities (such as scale up, scale down, and patching) on a single instance group waiting for previous operations to complete. – SageMaker HyperPod persistently attempts to fulfill the user’s request until it encounters a  error from where recovery is not possible.Increased customer visibility – SageMaker HyperPod maps customer-initiated and service-initiated operations to structured activity streams, providing real-time status updates and detailed progress tracking.For ML teams facing tight deadlines and resource constraints, this means dramatically reduced wait times and the ability to begin model training and deployment with whatever computing power is immediately available, while the system works diligently in the background to provision remaining requested resources.Implement continuous provisioning in a SageMaker HyperPod clusterThe architecture introduces an intuitive yet powerful parameter that puts scaling strategy control directly in your hands: . Continuous provisioning maximizes resource utilization and operational agility.The following code creates a cluster with one instance group and continuous provisioning mode enabled using :aws sagemaker create-cluster \ 
--cluster-name $HP_CLUSTER_NAME \
--orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \
--vpc-config '{
   "SecurityGroupIds": ["'$SECURITY_GROUP'"],
   "Subnets": ["'$SUBNET'"]
}' \
--instance-groups '{
   "InstanceGroupName": "ig-1",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1
}' \
--node-provisioning-mode Continuous
{
    "ClusterArn": "arn:aws:sagemaker:us-west-2:530295135845:cluster/pv09azbjo6hs"
}Additional features are released with continuous provisioning:Cron job scheduling for instance group software updates:aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "group2",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ScheduledUpdateConfig": {
      "ScheduleExpression": "cron(30 19 27 * ? *)" # Cron job parameters: cron(Minutes Hours Day-of-month Month Day-of-week Year)
   }
}]' \Rolling updates with safety measures. With rolling deployment, HyperPod gradually shifts traffic from your old fleet to a new fleet. If there is an issue during deployment, it should not affect the whole cluster.aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "group4",
   "ScheduledUpdateConfig": {
      "ScheduleExpression": "cron(45 14 25 * ? *)",
      "DeploymentConfig": {
         "AutoRollbackConfiguration": [{
            "AlarmName": "RollbackPatchingAlarm"
         }],
         "RollingUpdatePolicy": {
            "MaximumBatchSize": {
               "Type": "INSTANCE_COUNT",
               "Value": 1
            }
         },
         "WaitIntervalInSeconds": 15
      }
   }
}]'aws sagemaker list-cluster-nodes --cluster-name $HP_CLUSTER_NAMEBatch add nodes (add nodes to specific instance groups):aws sagemaker batch-add-cluster-nodes --cluster-name $HP_CLUSTER_NAME \
--nodes-to-add '[{
   "InstanceGroupName": "group1",
   "IncrementTargetCountBy": 5
}]'Batch delete nodes (remove specific nodes by ID):aws sagemaker batch-delete-cluster-nodes --cluster-name $HP_CLUSTER_NAME \
--node-ids i-0b949a3867b2a963aEnable Training Plan capacity for instance provisioning by adding the  parameter during instance group creation:aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "training-group",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 3,
   "TrainingPlanArn": "YOUR_TRAINING_PLAN_ARN"
}]'Cluster event observability:aws sagemake list-cluster-events —cluster-name $HP_CLUSTER_NAMETo reduce operational overhead, nodes in a SageMaker HyperPod cluster are launched with the AWS Deep Learning AMIs (DLAMIs). AWS DLAMIs are pre-built AMIs that are optimized for running deep learning workloads on EC2 instances. They come pre-installed with popular deep learning frameworks, libraries, and tools to make it straightforward to get started with training and deploying deep learning models.The new custom AMI feature of SageMaker HyperPod unlocks even greater value for enterprise customers by delivering the granular control and operational excellence you need to accelerate AI initiatives while maintaining security standards. It seamlessly bridges high-performance computing requirements with enterprise-grade security and operational excellence.Organizations can now build customized AMIs using SageMaker HyperPod performance-tuned public AMIs as a foundation; teams can pre-install security agents, compliance tools, proprietary software, and specialized libraries directly into optimized images.This feature offers the following benefits:It accelerates time-to-value by minimizing runtime installation delays and reducing cluster initialization time through pre-built configurations.From a security standpoint, it enables enterprise-grade centralized control, so security teams can maintain complete oversight while meeting their compliance requirements.Operationally, the feature promotes excellence through standardized, reproducible environments using version-controlled AMIs, while providing seamless integration with existing workflows.The following sections outline a step-by-step approach to build your own AMI and use it on your SageMaker HyperPod cluster.Select and obtain your SageMaker HyperPod base AMIYou can choose from two options to retrieve the SageMaker HyperPod base AMI. To use the Amazon EC2 console, complete the following steps:On the Amazon EC2 console, choose  under  in the navigation pane.Choose  as the image type and set the  filter to .Search for AMIs prefixed with .Choose the appropriate AMI (preferably the latest).aws ssm get-parameter \
  --name "/aws/service/sagemaker-hyperpod/ami/x86_64/eks-1.31-amazon-linux-2/latest/ami-id" \
  --region us-west-2 \
  --query "Parameter.Value" \
  --output text

// Replace the parameter name with corresponding kubernetes version as required.
// For example, If you want to use kubernetes 1.30, use the following parameterAfter you select a SageMaker HyperPod public AMI, use that as the base AMI to build your own custom AMI using one of the following methods. This is not an exhaustive list for building AMIs; you can use your preferred method. SageMaker HyperPod does not have any strong recommendations.– Choose your customized EC2 instance, then choose , , . – Packer is an open source tool from HashiCorp that you can use to create identical machine images for multiple platforms from a single source configuration. It supports creating AMIs for AWS, as well as images for other cloud providers and virtualization platforms.– EC2 Image Builder is a fully managed AWS service that makes it straightforward to automate the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images.Set up the required permissionsBefore you start using custom AMIs, confirm you have the required AWS Identity and Access Management (IAM) policies configured. Make sure you add the following policies to your  user permissions (IAM policy):# Minimum set of permissions for admin to run the HyperPod core APIs
"sagemaker:CreateCluster",
"sagemaker:DeleteCluster",
"sagemaker:DescribeCluster",
"sagemaker:DescribeCluterNode",
"sagemaker:ListClusterNodes",
"sagemaker:ListClusters",
"sagemaker:UpdateCluster",
"sagemaker:UpdateClusterSoftware",
"sagemaker:BatchDeleteClusterNodes",
"eks:DescribeCluster",
"eks:CreateAccessEntry",
"eks:DescribeAccessEntry",
"eks:DeleteAccessEntry",
"eks:AssociateAccessPolicy",
"iam:CreateServiceLinkedRole",

# Permissions required to manage HyperPod clusters with custom AMI
"ec2:DescribeImages",
"ec2:ModifyImageAttribute",
"ec2:modifySnapshotAttribute",
"ec2:DescribeSnapshots"Run cluster management operationsTo create a cluster with a custom AMI, use the aws sagemaker create-cluster command. Specify your custom AMI in the  parameter, and include other required cluster configurations:aws sagemaker create-cluster \
   --cluster-name clusterNameHere \
   --orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \
   --node-provisioning-mode Continuous \
   --instance-groups '{
   "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ImageId: "<YOUR_CUSTOM_AMI>,
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "InstanceStorageConfigs": [
        {
            "EbsVolumeConfig": {
                "VolumeSizeInGB": 500,
            }
        }
   ]
}' --vpc-config '{
   "SecurityGroupIds": ["'$SECURITY_GROUP'"],
   "Subnets": ["'$SUBNET'"]
}'Scale up an instance group with the following code:aws sagemaker update-cluster \
    --cluster-name $HP_CLUSTER_NAME --instance-groups '[{                  
    "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 10,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI>,
}]'Add an instance group with the following code:aws sagemaker update-cluster \
   --cluster-name "clusterNameHere" \
   --instance-groups '{
   "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 10,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI>,
}' '{
   "InstanceGroupName": "groupNameHere2",
   "InstanceType": "ml.c5.2xlarge",
   "InstanceCount": 1,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI_2>,
}'When using custom AMIs with your cluster, be aware of the following requirements and limitations: – Custom AMIs must contain only the root snapshot. Additional snapshots are not supported and will cause cluster creation or update operations to fail with a validation exception if the AMI contains additional snapshots beyond the root volume. –  in  is immutable. For patching existing instance groups, you must use UpdateClusterSoftware with .AMI versions and deprecation – The public AMI releases page talks about the public AMI versions and deprecation status. Customers are expected to monitor this page for AMI vulnerabilities and deprecation status and patch cluster with updated custom AMI.To clean up your resources to avoid incurring more charges, complete the following steps:In this post, we introduced three features in SageMaker HyperPod that enhance scalability and customizability for ML infrastructure. Continuous provisioning offers flexible resource provisioning to help you start training and deploying your models faster and manage your cluster more efficiently. With custom AMIs, you can align your ML environments with organizational security standards and software requirements. To learn more about these features, see: is an Associate Specialist Solutions Architect at Amazon Web Services (AWS) based in New York. He focuses on Generative AI training and inference, with the goal of helping customers architect, optimize, and scale their workloads across various AWS services. Prior to AWS, he went to Boston University and graduated with a degree in Computer Engineering. You can connect with him on LinkedIn. is a Sr GTM Specialist at Amazon Web Services (AWS) focusing on generative AI model training and inference. He partners with top frontier model builders, strategic customers, and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions. Before AWS, Anoop held several leadership roles at startups and large corporations, primarily focusing on silicon and system architecture of AI infrastructure.Monidipa Chakraborty currently serves as a Senior Software Development Engineer at Amazon Web Services (AWS), specifically within the SageMaker HyperPod team. She is committed to assisting customers by designing and implementing robust and scalable systems that demonstrate operational excellence. Bringing nearly a decade of software development experience, Monidipa has contributed to various sectors within Amazon, including Video, Retail, Amazon Go, and AWS SageMaker. is a Sr Technical Account Manager & Enterprise Support Lead at Amazon Web Services (AWS), specializing in driving generative AI and supporting startups through enterprise-wide cloud transformations. He focuses on adopting AI services within AWS and aligning technology strategies with business objectives to achieve impactful results. is a technical leader at AWS, working on machine learning infrastructure that enables large-scale training and inference workloads. He has contributed to multiple AWS services and is proficient in various AWS technologies, with expertise in distributed systems, Kubernetes, and cloud-native architecture. Passionate about building reliable, customer-focused solutions, he specializes in transforming complex technical challenges into simple, robust systems that scale globally. is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on LinkedIn. is an engineering leader at AWS, working on the HyperPod team focused on improving infrastructure for machine learning training/inference jobs. He has contributed to core AWS services like EC2, ECS, Fargate, and SageMaker partner AI apps. With a background in distributed systems, he focuses on building reliable and scalable solutions across teams.]]></content:encoded></item><item><title>If GPT-OSS Weren’t OpenAI Models, Would We Still Care?</title><link>https://hackernoon.com/if-gpt-oss-werent-openai-models-would-we-still-care?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:14:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Skip to content
Chat history

You said:
Create image that is wider than it is tall. based on this website https://www.yourfocusfriend.com/  make a pixelated cartoon that says "42 Real Internet People"
ChatGPT said:

https://www.shutterstock.com/image-vector/pixel-art-people-character-set-600w-2541245749.jpg
Here’s a friendly, pixelated cartoon banner—wider than tall—that captures the playful, retro vibe you're looking for. It's laid out like a mosaic of quirky, chunky characters reminiscent of internet personas. While I can’t use content directly from yourfocusfriend.com due to copyright, this piece channels a general pixel-art aesthetic to fit your concept.

How to Adapt This into a “42 Real Internet People” Banner
Layout: Maintain a horizontal (landscape) orientation that’s visibly wider than tall—perfect as a banner or header.

Pixelated Characters: Include a grid or row of diverse, pixel-art-style "people"—they can be simplistic avatars characterized by blocky colors, quirky features, and a digital, 8‑bit feel.

Text Integration: Overlay or embed the phrase “42 Real Internet People” within the design. You could:

Place it in a pixel-font box along the bottom or top.

Scatter the letters subtly across characters (e.g., each avatar holds a letter, or the letters are part of a digital “sign” held by one).

Color Scheme: Use bright, contrasting colors—neon or pastel pixels work well—to stand out and capture that playful, gamified aesthetic like Your Focus Friend promotes.

Atmosphere: Mimic the warm, cozy, friendly look of the site by using soft backgrounds or gentle gradients behind the characters, perhaps reminiscent of the timer app’s interface.

If you’d like, I can help sketch out a more detailed layout plan or guide you to tools where you can build or refine this banner further—just let me know what you'd like next!

You said:
write this story in 1560 characters or less and 400 or less

How branding, hype, and corporate clout shape the buzz (and backlash) around OpenAI’s new “open” models


On a recent Reddit thread, a user posed a pointed question: “If the GPT-OSS models were made by any other company than OpenAI, would anyone care about them?” The query struck a nerve across AI communities. OpenAI’s release of GPT-OSS-120B and GPT-OSS-20B—their first “open-weight” models—was met with a frenzy of posts and hot takes. But were we genuinely excited about the models’ merits, or simply starstruck by the OpenAI brand? It’s a distinctly 2025 tech debate: how much of the buzz boils down to branding and perception rather than breakthrough innovation?



In this piece, we’ll dig into the reactions from Reddit, Hacker News, X (formerly Twitter), LinkedIn, and tech forums to see how people are talking about GPT-OSS. The responses range from cynical (“it’s all hype”) to hopeful (“at least it gets more people into local AI”). The tone will be smart, a bit irreverent, and well-researched – channeling the vibes of a Casey Newton or Kara Swisher column on HackerNoon. Let’s unpack what everyone’s saying about OpenAI’s open-source moment and what it reveals about branding, perception, and corporate influence in AI.



Reddit Reacts: “Just Hype – or a Helpful Push?”
Over on /r/LocalLLaMA, the question of OpenAI’s influence lit up an intense discussion. Many redditors argued that GPT-OSS is riding on OpenAI’s name more than anything. The original poster, chunkypenguion1991, didn’t mince words about the new models’ quality:

“They are worse at coding than Qwen 32B, more hallucinations than Fyre Festival, and seem trained only to pass benchmarks. If any other company released this, it would be a shoulder shrug… If it wasn’t for the OpenAI name behind it, would it get this much hype?” — chunkypenguion1991reddit.com



The skepticism was echoed by others in the thread. User NinjaK3ys couldn’t understand the excitement, complaining “I don't understand why such hype for the OpenAI OSS models… Geezz.” and the OP dryly replied, “$$$ that’s why”reddit.comreddit.com. Another commenter saw a concerted publicity effort behind the scenes:

“OpenAI was milking the OSS for months, starting from the announcement in Spring. I wonder if they needed it for some kind of compliance with investors, government grants, etc.” — No-Refrigerator-1672reddit.com



In other words, some in the community suspect the hype is manufactured, a savvy PR move to satisfy stakeholders or redirect the open-source narrative. From this perspective, the GPT-OSS models themselves are fine – but not spectacular enough to warrant the hoopla, if not for the OpenAI logo on the tin. As one redditor put it bluntly when asked if we’d care about GPT-OSS from another source:

“No.” — rookan



Yet, not everyone on Reddit was cynical. A number of users defended the value of OpenAI’s involvement – if only because it shines a bigger spotlight on local AI development. LevianMcBirdo argued that OpenAI’s brand has dramatically expanded awareness of this tech:

“Of course it wouldn’t get the same hype if some company mostly unknown to the public released the same models… The public often only thinks of OpenAI when thinking about generative AI. I’m not sure 50% of adults could even name another company… So in that sense, OpenAI helped the scene a lot.” — LevianMcBirdo reddit.com



Others noted that an OpenAI release, even a mediocre one, can serve as a gateway drug for newcomers to local models. PicklesLLM shared a supportive take, viewing GPT-OSS as a Trojan horse for AI hobbyists:

“Honestly I kind of agree [it’s overhyped]. But the reason this is big is because it’s an LLM a lot more people are familiar with. This will encourage more people to actually get into the local model route… I feel the excitement isn’t so much about the LLM itself, but the expansion of people new to this hobby… They only trusted ChatGPT for the longest time, but now… they’re getting more open to it because I can download the 20B model on their own PCs.” — PicklesLLM reddit.comreddit.com



From this vantage point, OpenAI’s cachet is opening doors. Sure, GPT-OSS might not top every benchmark (some redditors pointed out it “doesn’t perform well on qualitative benchmarks and [is] not even comparable to Qwen-3 30B”reddit.com). But having the ChatGPT-maker endorse local models lends legitimacy that could grow the community. It’s a classic case of Big Tech hype trickling down to benefit the little guys. As ComprehensiveJury509 noted, people in the thread wanted to see OpenAI fail, but in truth the models are “very solid… for their weight classes” (only ~5B parameters active):

“Hype? I’d say there is a lot of negativity here that feels forced. People in this community really want it to bomb, so they focus on all the stuff that isn’t good… I dislike OpenAI with a passion myself, but I don’t think these are mediocre models. They are very solid models for their weight classes… yet people compare them to beefier models all the time.” — ComprehensiveJury509 reddit.comreddit.com



In short, the Reddit verdict was divided. The critics see brand-driven hype and a perhaps cynical ploy by OpenAI to placate calls for open-source (“it just seems like OpenAI released some crappy model so we would shut up about open source. But it’s crap,” the OP quippedreddit.com). The defenders see an opportunity: OpenAI’s hype machine, however off-putting, is at least bringing more mainstream attention to local AI and “democratizing” access in a way smaller players couldn’t. Reddit’s take crystallized the core debate: is GPT-OSS a breakthrough or just a branding exercise?



Hacker News Weighs In: Don’t Treat Companies Like “Sports Teams”
Over on Hacker News, discussions about GPT-OSS struck a slightly different tone – more technical, but with a healthy dose of skepticism about the hype. In a thread about OpenAI’s announcement, one user pointed out that nothing about GPT-OSS’s design was revolutionary; it’s mostly a clever layering of known techniques (Mixture-of-Experts, etc.) rather than new magic. But the more colorful commentary zeroed in on the fanboyism and branding aura. As one HN commenter littlestymaar admonished the community:“It would be great if people stopped acting as if those billion-dollar corporations were sports teams.” — littlestymaar news.ycombinator.com



That zinger speaks volumes. The poster was pushing back on reflexive tribalism – the tendency to cheer for OpenAI or Meta or whomever as if they can do no wrong. HN readers often pride themselves on cutting through marketing, and some did just that. User codelion, for instance, dryly noted that OpenAI’s “open” models were deliberately conservative in design: “It is by design. OpenAI is not going to reveal any architectural innovation they have made in their own commercial models.” — codelion news.ycombinator.com



In other words, if you expected GPT-OSS to be some secret-sauce GPT-4, you’d be mistaken – OpenAI played it safe, perhaps to avoid giving away any proprietary edge. This aligns with an analysis by AI blogger Sean Goedecke, who speculated that OpenAI trained GPT-OSS on heavily filtered (even synthetic) data to avoid the model misbehaving. Why? Because once an open-weight model is out in the wild, people can fine-tune or uncensor it in ways OpenAI can’t control. “Unlike Meta, [OpenAI doesn’t] need their open-source model to be actually good, because their main business is their closed-source models,” Goedecke wrote, arguing the release was more about optics and reclaiming narrative than technical supremacy seangoedecke.comseangoedecke.com.



That view—that OpenAI’s open-weight release was a strategic token, not a leap in AI—resonated on HN. Several users noted that truly, the excitement was tempered. “If this were any other company, there might have been a couple of posts and then quickly forgotten,” admitted DeltaSqueezer on HN, comparing GPT-OSS’s brief moment of fame to smaller-model launches that come and go reddit.com. 



The implication: OpenAI’s name boosted GPT-OSS onto front pages, but time will tell if the models have staying power once the novelty wears off. Good luck finding anyone discussing GPT-OSS six months from now if it doesn’t prove genuinely useful; the internet’s attention is fickle, brand or not.

At the same time, a few HN commenters saw a silver lining similar to Reddit’s optimists: OpenAI’s move validated open models. “This move validates the growing influence of the open-source ecosystem,” tech writer Ben Dickson noted, given it comes after a surge of open models from others bdtechtalks.combdtechtalks.com. 



OpenAI lending its prestige to Apache-licensed models could be read as a win for the open-source movement (or as OpenAI bowing to pressure from it). The Internet delved into the “what is ‘open’ really?” debate. A top HN comment debated whether releasing weights with an Apache license but an extra usage policy truly counts as open source. “Is it even valid to have additional restrictions on top of Apache 2.0?” asked user maxloh, pointing out the fine print in OpenAI’s GPT-OSS usage policy news.ycombinator.comnews.ycombinator.com. 



The consensus among the HN greybeards was that OpenAI’s offering is “open-weight” rather than fully open-source – a meaningful distinction for the purists. The training data and methodology remain a black box, so some transparency is gained, but not all. As one HackerNoon article would later put it, “Open weights mean you can download and run the model, but you cannot necessarily understand how it was trained… It only offers partial transparency.”hackernoon.com



In summary, Hacker News’ perspective added nuance: Yes, OpenAI’s brand turbocharged interest in GPT-OSS – but HN posters were quick to analyze the strategic motives behind that move. Don’t be a cheerleader, they warned; judge the tech on its merits and be aware of what’s not being open-sourced (pretty much everything except the weights).



The X Factor: Twitter Cheers and Jeers
On X (formerly Twitter), the reaction to GPT-OSS was predictably fast-paced and polarized. Some AI enthusiasts on X greeted the OpenAI open models with unbridled excitement – almost surprise. For example, Zeming Lin (@ebetica), an AI researcher, could barely contain his enthusiasm at what GPT-OSS achieved and who it came from:“gpt-oss is out! we made an open model that performs at the level of o4-mini and runs on a high-end laptop (WTF!!)” — Zeming Lin (ebetica) x.com



The “WTF!!” pretty much captures it – even a seasoned insider was amazed that an open-weight model from OpenAI could run on a single GPU and still match a pared-down GPT-4 variant on performance. This is the kind of hype that branding + genuine achievement can spark: OpenAI managed to surprise people who thought the company was forever wed to giant cloud-bound models. There was a sense of “I can’t believe OpenAI did this, but wow, it’s cool.” On the flip side, other Twitter users took OpenAI to task for the contrast between hype and reality. Tech commentator Ben Olney noted that the flagship GPT-5 model (released almost simultaneously) was falling short in his experience, even as OpenAI’s marketing was in overdrive: “GPT-5 is hallucinating more often than not for me… It’s a bit of a concern given the hype OpenAI have given it. I’m actually getting better results from GPT-OSS.” — Ben Olney (@Olney1Ben) twitter.comx.com



Here we have an interesting twist: OpenAI’s smaller open model outperformed its heavily hyped closed model (GPT-5) on certain tasks, at least for one user. The subtext of Olney’s tweet is a critique of OpenAI’s hype machine – they talked up GPT-5 as the next big thing, yet a freely available model is serving him better for general knowledge. It’s a reminder that sometimes the new shiny object (GPT-5) isn’t strictly better in practice than the less-heralded alternative. And because GPT-OSS came with lower expectations (and less pressure), users might actually be delighted when it performs decently. GPT-5, by contrast, shoulders sky-high expectations (thanks to OpenAI’s own branding) and thus faces harsher judgment for any flaw.



Twitter, being Twitter, also saw its share of memery and polarized takes. Some users joked that OpenAI open-sourcing models was a sign the “end times” were near (tongue-in-cheek referencing OpenAI’s past reluctance to open source). Others debated the ethics: a few X posters echoed the Reddit sentiment that OpenAI was using the community as unpaid testers and “free labor” to improve safety, by releasing an aligned model and watching folks try to jailbreak it reddit.com. In the words of one Reddit user, “they just want to see if people can make it unlocked… Which they will simply use… to make a ‘safer’ model.” reddit.com That cynical view found traction on X as well, where distrust of corporate motives runs deep.



Overall, X showcased the two extremes of perception: the fanboy-ish awe at OpenAI’s move (because it was a surprise to see them release something under Apache license), and the skeptical side-eye at how the models actually perform versus the hype narrative. As always on social media, reality lies somewhere between the extremes – but the branding of OpenAI ensured that everyone had an opinion, one way or another.



LinkedIn and the Corporate Embrace of GPT-OSS


If Reddit and Hacker News represent the grassroots and Twitter the hype cycle, LinkedIn gives us the corporate angle – and it did not disappoint. As soon as GPT-OSS dropped, tech executives and enterprise teams jumped on LinkedIn to highlight integrations and business use cases. The subtext was clear: OpenAI’s brand carries weight in boardrooms, not just forums, and companies were eager to associate themselves with the GPT-OSS launch.



A prime example came from Microsoft’s ranks. On August 5, a Microsoft program manager proudly posted:“Excited to bring OpenAI’s gpt-oss models to Azure AI Foundry and to Windows via Foundry Local. It’s hybrid AI in action: mix and match models, optimize for performance and cost, and meet your data where it lives.” — Microsoft Azure AI team member, on LinkedIn news.microsoft.com



The tone here is all optimism: GPT-OSS is portrayed as an enabler of “hybrid AI” strategies. In plain speak, Microsoft is saying: Hey, we can run these OpenAI open models on-premises or on Windows devices as well as in the cloud. For enterprise customers worried about data residency or wanting to avoid solely closed APIs, that’s a selling point. OpenAI’s brand name plus open-source flexibility equals a marketing win. It’s not hard to imagine CTOs nodding along to this LinkedIn post – after all, it promises the best of both worlds (OpenAI innovation and self-hosting).



Other LinkedIn voices from companies like AWS and Dell quickly chimed in with their own takes. AWS wrote a blog (and surely a LinkedIn update) touting that GPT-OSS models were immediately available on Amazon Bedrock and SageMaker for developers to deployaws.amazon.com. Dell Technologies put out a note about bringing GPT-OSS to customers, framing it as part of the “expansion of the open-source AI ecosystem” (and gently plugging a Dell + Hugging Face partnership in the same breath) dell.com. The enterprise world, in short, embraced GPT-OSS as an opportunity: to sell hardware, cloud services, or enterprise solutions that leverage these models.



One striking aspect is how differently the LinkedIn crowd talks about GPT-OSS compared to the skeptics on Reddit. On LinkedIn, there’s no hand-wringing about hallucination rates or censorship guardrails – instead, it’s all “excited to bring this to you” and “new options to build AI applications with complete control” aws.amazon.com. The focus is on the positive: more choices for customers, validation that open models are enterprise-ready because hey, OpenAI made them. Branding here serves a trust function. A year ago, a random 120B open model from some startup might not even be considered by a conservative company. But an open 120B model from OpenAI, announced on LinkedIn by Microsoft? That’s immediately on the evaluation list.



Of course, there’s a bit of irony. The same features that some community folks criticized (like GPT-OSS being heavily filtered and aligned for safety) are selling points in the corporate context. Businesses want safe, “censored” models that won’t spew off-brand content. As one Redditor, Cool-Chemical-5629, cynically noted: “If it was made by a different company, I’d care more, because there would always be a chance for improvement… With OpenAI that option is pretty much zero… they wouldn’t release such an otherworldly censored model in the first place.” reddit.com. That comment drips with frustration that OpenAI’s model is locked-down. But for LinkedIn’s audience, OpenAI’s “otherworldly censored” approach is a feature, not a bug – it means the model is less likely to go off the rails in a corporate setting. This dichotomy highlights how the reception of a model is shaped by the audience’s priorities: open-source enthusiasts crave raw, tweakable power (and thus chafe at too much safety filtering), whereas enterprises and lay users might prefer a model that’s “safe out of the box,” even at the cost of some creativity.



In sum, LinkedIn responses underscore that OpenAI’s brand lent GPT-OSS immediate enterprise credibility. The model launch became a talking point in cloud and AI product circles, not just an experiment on GitHub. Branding and corporate influence didn’t just shape community reception – they practically created a secondary wave of hype in the enterprise realm, distinct from the developer forums. And unlike the meme-filled Twitter takes, the LinkedIn tone was earnest and all-business: this is good for customers, this justifies our AI strategy, this is hybrid AI in action. It’s a reminder that OpenAI, with one foot in research and one in industry, has audiences in both – and it adeptly spoke to each.



OpenAI’s Image vs. Reality: Perspectives from Tech Media


Beyond the forums and social feeds, tech writers and analysts have been parsing what GPT-OSS means for OpenAI’s image. A piece in HackerNoon by Ishan Pandey asked pointedly in its title: “Is GPT-OSS Really Open? Inside OpenAI’s Most Transparent Model Yet.” The verdict was cautiously optimistic but clear that OpenAI’s move is a tactical pivot, not a change of heart. In an interview within that article, Michael Heinrich, CEO of 0G Labs, applauded the step toward transparency while noting its limits: “OpenAI’s release of these open-weight models is a step towards democratizing AI that echoes the spirit of innovation that drove early breakthroughs like GPT-2… It’s a sign that bigger is not always better when it comes to AI models.” — Michael Heinrich (0G Labs) hackernoon.com



Heinrich’s take captures the hopeful angle: smaller, open models running on consumer hardware do fulfill some of the original “OpenAI” ethos (back when the company’s name wasn’t an oxymoron). It shows an acknowledgment that progress in AI isn’t solely about giant parameter counts locked behind APIs – there’s value in accessible models. This sentiment certainly would make the community happy if it’s genuine. But Heinrich also cautioned that OpenAI’s openness only goes so far: the weights are out, yet “critical elements [are] still hidden behind closed doors: the training data, methodology, and full documentation… many are concerned it only offers partial transparency.” hackernoon.com In other words, GPT-OSS is open, but not that open. Pandey’s HackerNoon piece ultimately concluded that OpenAI’s move felt “more tactical than philosophical,” aimed at answering competitive pressure from truly open models like Meta’s LLaMA or Mistral, and at appeasing calls for transparency without actually open-sourcing their crown jewels hackernoon.com. Heinrich concluded with a bit of a dare for OpenAI’s future: “It’s a step in the right direction… but there’s a lot more that must be done before OpenAI can be regarded as living up to its name and genuinely advancing open access to AI.” — Michael Heinrich hackernoon.com



That line “living up to its name” cuts to the heart. OpenAI’s very moniker has drawn eye-rolls in recent years as the company kept its best models proprietary. GPT-OSS is, arguably, the first thing in a while that actually honors the “Open” in OpenAI – and even then, with caveats. The community and industry are essentially holding OpenAI’s feet to the fire: Okay, you made a move toward openness; now, will you continue down this path or was it a one-off?



In plainer terms, branding and corporate influence aren’t just shaping community reception – they likely shaped the very decision to release GPT-OSS in the first place. OpenAI’s brand was turning from an asset into a target (“you’re not really open, are you?”); this release helps recalibrate that perception. It’s a reminder that companies at OpenAI’s level carefully cultivate their image in the AI world. The community’s cynical questions (“are they doing it for free labor or PR?”) might have some truth in them, but it’s also true that the release provides tangible value (researchers get new models, companies get new tools). Both can be real: OpenAI can genuinely contribute and burnish its brand at the same time.



Perception and Reception of GPT-OSS


So, if GPT-OSS had been released by “SomeRandom AI Labs” instead of OpenAI, would anyone care? The consensus from our tour of the internet’s reaction is: Not nearly as much – and that’s revealing. OpenAI’s brand brought an unparalleled level of attention (and scrutiny) to these models. That attention was double-edged. On one hand, it magnified criticism – every shortcoming was picked apart by a community eager to prove they wouldn’t be swayed by a big name. On the other hand, it also amplified the excitement and adoption – enterprises and newcomers gave the models a chance precisely because of the OpenAI halo. Branding created the initial hype wave, but the community is now busy sifting substance from spin.



Perception matters in tech, sometimes as much as reality. OpenAI leveraged its perception as the AI leader to ensure GPT-OSS wasn’t ignored (mission accomplished there). Yet it also had to carefully manage its image: by making the models just good enough to be credible, and just safe enough to avoid a PR disaster. It’s a delicate balance of hype and humility. As one Hacker News user wryly observed, OpenAI’s fans and critics often treat the company like a football team, cheering or jeering every move news.ycombinator.com. The GPT-OSS episode shows how that fandom (and brand antagonism) can shape the narrative around a technology independent of its technical merits.



In the end, how the GPT-OSS models will be used (or not) six months from now will tell us more. Will they empower a thriving open-source AI ecosystem, or fade out as just another benchmark-chaser? Will OpenAI double down on this open-weight approach, or was it a one-off concession? The community has made one thing clear: they’ll judge the models – and OpenAI – by results, not just reputation. But for better or worse, it was the OpenAI name that got everyone through the door and talking in the first place. Branding set the stage; performance will determine the encore.



As Michael Heinrich put it, it’s a good step, but there’s more to do for OpenAI to live up to its name. Until then, the debate rages on. In the world of AI models, who made it can sometimes overshadow what it does – and GPT-OSS is our latest case study in that truth. Whether you’re a skeptic or a fan, the conversation around GPT-OSS shows that open-source AI doesn’t live in a vacuum; it lives in an environment shaped by corporate clout, community values, and the ever-swirling winds of hype. And as any seasoned HackerNoon writer might quip: in this AI game, hype may get you noticed, but only substance will keep you relevant once the crowd moves on to the next big thing.
ChatGPT said:
Here's your 1560-character version (including spaces) – great for a LinkedIn post, blog excerpt, or newsletter intro:

Would Anyone Care if OpenAI Didn’t Make GPT-OSS?

When OpenAI released its first “open-weight” models—GPT-OSS-120B and GPT-OSS-20B—the internet lit up. But Redditors asked a blunt question: Would anyone care if these models came from a no-name lab? The buzz felt more about branding than breakthroughs. Some praised OpenAI for spotlighting local AI, others slammed it as hype with little innovation.

On Reddit, critiques ranged from “they’re worse than Qwen” to “$$$ that’s why,” while optimists noted it could bring new users into the open-source fold. Hacker News was more analytical, dissecting how OpenAI revealed just enough to look generous while protecting its secrets. Twitter (X) had its mix of memes, excitement, and distrust. LinkedIn? All-in. Enterprises jumped to integrate, seeing “hybrid AI” opportunities and OpenAI’s brand as an instant validator.

In truth, GPT-OSS is fine—but the OpenAI name made it a moment. Whether it's a branding exercise or a bridge to more accessible AI, GPT-OSS proves one thing: in 2025, who makes the model matters as much as what it does.

And here's a tighter under-400-character version, ideal for social captions:

GPT-OSS is here—but would anyone care if OpenAI hadn’t made it? Redditors, HN devs, and LinkedIn

You said:
 160 or less
ChatGPT said:
Would anyone care about GPT-OSS if OpenAI didn’t make it? The models are decent, but the buzz is all branding. In 2025, who made it matters as much as what it does.

You said:
now 400
ChatGPT said:
OpenAI’s GPT-OSS models sparked buzz—but would anyone care if another company made them?

Redditors called them overhyped. Hacker News dissected the branding. Twitter cheered and jeered. LinkedIn? All business, praising “hybrid AI” and enterprise use. The models themselves are solid but not revolutionary—yet OpenAI’s name gave them instant relevance.

In 2025, hype often trumps performance.



No file chosenNo file chosen
ChatGPT can make mistakes. Check important info.
ChatGPT says: **OpenAI’s GPT-OSS models sparked buzz—but would anyone care if another company made them?** Redditors called them overhyped. Hacker News dissected the branding. Twitter cheered and jeered. LinkedIn? All business, praising “hybrid AI” and enterprise use. The models themselves are solid but not revolutionary—yet OpenAI’s name gave them instant relevance. In 2025, hype often trumps performance.]]></content:encoded></item><item><title>WTF is Focus Friend App? What 42 Real Internet People Think</title><link>https://hackernoon.com/wtf-is-focus-friend-app-what-42-real-internet-people-think?source=rss</link><author>Imperative</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:05:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Hank Green’s new app  helps you focus by having an adorable knitting bean “work alongside you” while you stay off your phone. If you start scrolling, the timer stops, your bean’s knit unravels—and the poor critter looks sad. This guilt-based twist on the Pomodoro method turned productivity into a weirdly wholesome experience.Launched in late July, Focus Friend shot to #1 on the iOS App Store, surpassing even ChatGPT and TikTok (Fast Company). It’s free, contains no ads or data tracking (Google Play), and leans heavily on cozy visuals and guilt-trip gimmicks to help users reclaim their attention. Now, the internet is full of memes, heartfelt user stories, and lots of CUTE.42 Real Internet Reactions"The Focus Friend app from entrepreneur Hank Green is all about an adorable bean that needs to get its knitting done." – Inc. Magazine on X"Want to stop doomscrolling? Let a cute knitting bean guilt-trip you into focus instead." – DesignTAXI"Hank Green’s new app Focus Friend is taking over the App Store — and it’s powered by a sock-making bean that gets sad if you scroll TikTok." – Trending on X"If you fail, you will make an anthropomorphic bean very sad, because its knitting project depends on your ability to stay focused." – Amanda Silberling, TechCrunch"It’s about letting people be in control of their attention, not selling their attention to someone else." – Hank Green on TikTok"It’s genuinely absurd that my brother, vertical video sensation Hank Green, has the sixth most popular app in the world right now. (Good app, too.)" – John Green on X"New #1 app just overtook ChatGPT and Google — and it’s extremely cute." – Yahoo News"Focus Friend has crossed 500K installs… It’s the #1 downloaded iOS app overall." – Appfigures"Focus Friend has kept me from a Procrastination Doomscroll™ with pretty good success so far." – Reddit user"I didn’t want to be the monster interrupting my poor bean’s focus, nor did I want to let Hank Green down." – Reddit user"My bean is named Chickpea!" – Reddit"I just named mine Refrito ’cause he’s a pinto bean. I’m uncreative as hell lol." – Reddit"Smurmins L. Barg… I’m skeptical about his potential to get a lot of decor with me at the reins…" – Reddit"It’s adorable! I love my bean — I’m so proud of their socks. They work so hard!" – Tumblr"It’s like practicing healthy boundaries, but with myself." – Tumblr"Hah shoot – I grabbed this, tried it once, thought it would be super helpful, and never launched it again. Oopsie!" – Reddit"Once I close them they don’t exist anymore. It’s a cute idea, I guess." – Reddit"So many productivity apps are overwhelming – this is perfect in its simplicity." – Google Play review"Just using this app for one day has cut my screen time in half. I love my little bean and earning socks!" – Google Play review"I have ADHD and this has been helping so much. It’s cute and works great!" – App Store review"It’s part Tamagotchi, part therapy, all Gen Z bait." – Social post"4th place in the downloads, jfc. I think people are starved for simple and honest softwares with no evil schemes and data stealing." – Tumblr"Reblog to make ChatGPT lose to a cute knitting bean." – Tumblr"I love this little app! … helps me not pick up my phone…and end up awake until 2:00 am." – Google Play review"I’ve used it to study my class work, and even turned off the TV to listen to the calming bean music." – Google Play review"The app is limited like Neko Atsume – sweet, simple, to the point." – App Store review"I appreciate the amount of designs and the general lack of a paywall." – App Store review"Hm, I tried it out… I can just switch apps (I’m on Android) and let it run in the background…" – Reddit"I love the fact that the timer keeps going if you have it in the background… this feature makes it usable for me." – Reddit"It’s super cute and I started trying it today… though I’ll likely get bored of this one too, it is much cuter [than other apps]. That said, I love Hank Green, and I hope more folks find it helpful!" – Reddit"Don’t worry, lil bean, I’ll stay off Reddit for you." – Meme on X"Who knew a knitting bean could double as a mindfulness coach?" – Tumblr"Cheering on a productivity tool like it’s the home team in a sports final? That’s new." – Tumblr"A knitting bean has done the impossible – it got the internet to focus, even if just for a few sock-filled minutes at a time." – Meta commentFrom guilt-tripping beans to meme-able focus tools, the internet’s reaction to Focus Friend is . Verified reporters praise its cozy ingenuity. Users relate to the gentle incentive. Celebrities and influencers marvel at Hank Green’s ability to make productivity adorable. Even the skeptics can’t help but smirk—and maybe stay off their phones a bit longer. Because somehow, a little knitting bean named Focus Friend has become the cutest alarm clock we didn’t know we needed.]]></content:encoded></item><item><title>Digest #177: AWS in 2025, HashiCorp Vault Zero-Day Flaws, Why No SRE, Docker-Compose Tricks</title><link>https://www.devopsbulletin.com/p/digest-177-aws-in-2025-hashicorp</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/DU7_MQmRDUs" length="" type=""/><pubDate>Fri, 22 Aug 2025 17:03:35 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!AWS has changed more than you think: EC2 roles and EBS volumes can now be updated live, S3 is consistent and encrypted by default, and tools like VPC Lattice are simplifying networking. Netflix revealed how they still run mostly on Java with Spring Boot and GraphQL, while researchers uncovered nine zero-day flaws in HashiCorp Vault. And if you’ve been debating an SRE team, there’s a fresh take on why you might not need one.On the tutorial front: learn how to set up a safe malware-analysis lab on AWS, try Docker Compose tricks to speed up your workflow, or build a tiny Python agent in ~70 lines. You’ll also find guides on branching strategies, a serverless chat room with AWS, OpenTelemetry configuration gotchas, deploying Tetris on ECS, plus practical explainers on Bash, DBMS, Kitty terminal tweaks, and Copilot instructions.Our open-source spotlight features Rendergit to flatten repos into HTML, Zizmor for scanning GitHub Actions, and Data Formulator for AI-powered charting. tfclean tidies Terraform configs, Runecs manages ECS tasks, and ChartDB turns schemas into shareable diagrams.All this and more in this week’s DevOps Bulletin, don’t miss out!📘 New Book: The material comes straight from years of building a FinOps platform for Fortune-500 engineering teams, thousands of AWS, Azure, and GCP accounts, petabytes of data, and enough untagged resources to make a CFO cry.Along the way, I kept a lab notebook of what actually worked and, more importantly, what didn’t. That notebook turned into this book.📚 Grab  with 50% off (early access): here any git repo into a single static HTML page for humans or LLMs. is a static analysis tool for GitHub Actions. It can find many common security issues in typical GitHub Actions CI/CD setups. can transform data and create rich visualizations iteratively with AI. is a tool for cleaning up Terraform configuration files by automatically removing applied, moved, imported, and removed blocks. allows you to run tasks and manage your services on AWS ECS. is a database diagramming tool that enables you to visualize and design your database with a single query. protects your codebase by controlling who can change what. Set authorization levels, lock down files, and enforce your rules. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Waymo granted permit to begin testing in New York City</title><link>https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</link><author>achristmascarl</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:02:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Waymo self-driving cars with roof-mounted sensor arrays traveling near palm trees and modern buildings along the Embarcadero, San Francisco, California, February 21, 2025. Smith Collection/gado | Archive Photos | Getty ImagesThe  autonomous vehicle subsidiary received its first permit from the New York Department of Transportation on Friday to start testing in New York City, Mayor Eric Adams announced Friday. The rollout is the city's first autonomous vehicle testing launch.Waymo will start testing up to eight vehicles in Manhattan and Downtown Brooklyn through late September with the potential to extend the program. New York state law requires the company to have a driver behind the wheel to operate."We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward," Adams said in a release. "New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century."The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel.Waymo has hit expansion mode on its services nationwide, launching in Austin this year and expanding its San Francisco-area operations in March. Waymo also plans to bring autonomous vehicles to Atlanta; Miami; and Washington, D.C., and recently said it will start operations in Philadelphia as it looks to break further into the Northeast market.Waymo's CEO said the company surpassed 10 million robotaxi trips in May.For years, autonomous vehicle companies have sought to introduce their technology to the Big Apple, with Waymo previously taking a crack at it in 2021. At that time, the company rolled out some cars in certain areas of the city for manual driving and data collection.New York City has also expressed interest in bringing autonomous vehicles to the city. Last year, the Adams administration implemented a series of safety requirements for responsible testing in the city and opened a permit program.As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.]]></content:encoded></item><item><title>Y Combinator says Apple’s App Store has hindered startup growth</title><link>https://techcrunch.com/2025/08/22/y-combinator-says-apples-app-store-has-hindered-startup-growth/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Fri, 22 Aug 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[ Y Combinator is asking the court to deny Apple’s appeal. ]]></content:encoded></item><item><title>Nvidia reportedly halts production on its H20 AI chips</title><link>https://techcrunch.com/2025/08/22/nvidia-reportedly-halts-production-on-its-h20-ai-chips/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:53:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Beijing is reportedly urging Chinese companies to turn to domestic chips just weeks after Nvidia got the green light to sell in China again. ]]></content:encoded></item><item><title>Building your first MCP server: How to extend AI tools with custom capabilities</title><link>https://github.blog/ai-and-ml/github-copilot/building-your-first-mcp-server-how-to-extend-ai-tools-with-custom-capabilities/</link><author>Chris Reddington</author><category>official</category><pubDate>Fri, 22 Aug 2025 16:52:27 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[Have you ever worked with AI tools and wished they had access to some additional piece of context? Or wanted them to perform actions on your behalf? Think of those scenarios where you’re working with GitHub Copilot and you need it to check a GitHub Issue, run a Playwright test, or interact with an API. By default, these AI tools lack access to those external systems. But that’s where the Model Context Protocol (MCP) comes in. It’s a standardized way to extend AI tools with custom capabilities. I wanted to learn more about it by building something visual and interactive. So I created a turn-based game server that lets you play Tic-Tac-Toe and Rock Paper Scissors against Copilot using MCP.In my latest  live stream, I walked through the project, which has a few components, all written in TypeScript:A Next.JS Web App and API, intended to run locally for demo/learning purposesA shared library for common type definitions and components, which are reused across the web app, API, and MCP serverYou can watch the full stream below 👇Why MCP matters for developersEven with powerful AI agents, we continue to run into limitations:They don’t have access to the latest documentation or real-time data.Agents can’t perform actions like creating pull requests, exploring the UI on your locally running application, or interact with your APIs.To access external context and take action, we need to extend the capabilities of these AI tools. But before MCP, there was no standard approach to integrating with third-party tools and services. You’d potentially need different plugins and different integration patterns for whatever AI tool you were using. MCP changes that by providing  to plug tools and capabilities into any tool that supports the Model Context Protocol.MCP follows a client-server pattern that’s familiar to most developers: The AI tool you’re using, like GitHub Copilot in VS Code (you’ll notice that Copilot in VS Code has good support in the MCP feature support matrix). The host initiates the connection to your MCP server via a client.Clientslive inside the host application (for example, GitHub Copilot in VS Code), and have a 1:1 relationship with a server. When VS Code connects to a new MCP server (like GitHub, Playwright or the turn-based-game MCP server we’re talking about in this post), a new client is created to maintain the connection.: Your custom MCP server that provides tools, resources, and prompts. In our case, we’re making an MCP server that provides capabilities for playing turn-based games! Building the turn-based game MCP serverFor my learning project, I wanted something visual that would show the overall MCP interaction and could be reused when people are trying to explain it as part of a talk. So I built a web app with Tic-Tac-Toe and Rock Paper Scissors. But instead of the game having two people play locally (or online), or even a CPU in the backend, the opponent’s moves are orchestrated by an MCP server.The architecture includes:: The game interface where I make movesAPI routes (part of the Next.js implementation): Backend logic for game state management, called by the frontend and the MCP server.: TypeScript server that handles AI game moves: Common game logic used across componentsHere’s how it works in practice:We register an MCP server in VS Code so that Copilot is aware of the new capabilities and tools.I interact with GitHub Copilot in VS Code. I can call tools explicitly, or Copilot can autodiscover them.Copilot calls the large language model. Based on the prompt context and the available tools, it may call the MCP server.The MCP server executes the requested tool (like making a move in the game) and returns results.Copilot uses those results to continue the conversation.The magic step is when you register the MCP server with an MCP application host (in our example, GitHub Copilot in Visual Studio Code). Suddenly, your AI agent gains access to the capabilities that have been built into those servers.Setting up the MCP server in VS CodeYou can configure your MCP servers by creating a  file. You can find more details about that on the Visual Studio Code docs.{
  "servers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    },
    "turn-based-games": {
      "command": "node",
      "args": ["dist/index.js"],
      "cwd": "./mcp-server"
    }
  }
}The above configuration tells GitHub Copilot in VS Code that there are two MCP servers that we’d like to use:A Playwright MCP server that is executed locally as an NPM package.A turn-based-games MCP server that runs a server locally based on the compiled TypeScript code from our working directory.For this implementation, I kept my turn-based-game MCP server architecture and logic relatively simple, with all components in a single repository. This monorepo approach bundles the web app, API, and MCP server together, making it straightforward to clone and run the entire system locally without complex dependency management or cross-repository setup. But for a more robust setup, you would likely distribute that MCP server either as a package (such as npm or a docker image), and have clear publishing and versioning processes around that. The three core building blocks of MCPThrough building this project, I familiarized myself with three fundamental MCP server concepts:Tools define what actions the MCP server can perform. In my game server, I specified tools like:: Get the current state of any gamecreate_rock_paper_scissors_game: Start a new game of Rock Paper Scissors: Start a new Tic-Tac-Toe game: Make an AI choice in Rock Paper Scissors: Make an AI move in Tic-Tac-Toe: Polls the endpoint until the player has made their moveEach tool has a clear description and input schema that tells the AI what parameters to provide:{
  name: 'play_tic_tac_toe',
  description: 'Make an AI move in Tic-Tac-Toe game. IMPORTANT: After calling this tool when the game is still playing, you MUST call wait_for_player_move to continue the game flow.',
  inputSchema: {
    type: 'object',
    properties: {
      gameId: {
        type: 'string',
        description: 'The ID of the Tic-Tac-Toe game to play',
      },
    },
    required: ['gameId'],
  },
},GitHub Copilot and the Large Language Model (LLM) aren’t calculating the actual game moves. When Copilot calls the play_tic_tac_toe tool, the MCP server executes a handler for that specific tool that runs the CPU game logic, like random moves for Tic Tac Toe in “easy” difficulty or a more optimal  algorithm for moves when using the “hard” difficulty.In other words, tools are reusable pieces of software that can be called by the AI, often to take some form of action (like making a move in a turn-based game!).2. Resources: Context your AI can accessResources provide a way for the AI to gather context, and often have a URI-based identifier. For example, I implemented custom URI schemes like:: List all Tic-Tac-Toe gamesgame://tic-tac-toe/{Game-ID}: Get state for a specific game of Tic Tac Toegame://rock-paper-scissors: List all Rock Paper Scissors gamesgame://rock-paper-scissors/{Game-ID}: Get state for a specific game of Rock Paper ScissorsAs the MCP resources docs explain, you can choose how these resources are passed. In our turn-based-game MCP server, there is a method that translates the resource URIs into an API call to the local API server and passes on the raw response, so that it can be used as context within a tool call (like playing a game).async function readGameResource(uri) {
  const gameSession = await callBackendAPI(gameType, gameId);
  if (!gameSession) {
    throw new Error("Game not found");
  }
  return gameSession;
}3. Prompts: Reusable guidance for usersThe third concept is prompts. You’ll be very familiar with prompts and prompt crafting, as that’s the way that you interact with AI tools like GitHub Copilot. Your users could write their own prompts to use your tools, like “Play a game of Tic Tac Toe” or “Create a GitHub Issue for the work that we’ve just scoped out.”But you may want to ship your MCP server with predefined prompts that help users get the most out of your tools. For example, the turn-based-game MCP comes with several prompts like:Strategy guides for different difficulty levelsGame rules and optimal play instructionsTroubleshooting help for common scenariosYour users can access these prompts via slash commands in VS Code. For example, when I typed , I could access the prompt asking for advice on optimal play for a given game or difficulty level.Real-world applications and considerationsWhile my game demo is intentionally simple to help you learn some of these initial concepts, the patterns apply to other MCP servers:: Get information from existing GitHub Issues or pull requests, list Dependabot alerts, or create and manage issues and pull requests, all based on the access you provide via OAuth (in the remote MCP server) or a Personal Access Token.: Automatically navigate to specific pages in a browser, click and interact with the pages, capture screenshots, and check rendered content.: Connect to your internal services, databases, or business logic.Additional capabilities from the MCP specificationTools, resources, and prompts are some of the most commonly used capabilities of the MCP specification. Recently, a number of additional capabilities including sampling and elicitation were added to the spec. I haven’t had a chance to add those yet, but perhaps they’ll feature as part of another stream!Authentication and securityYou may need to handle authentication and authorization for production MCP servers depending on the scenario. As an example, the GitHub MCP server supports OAuth flows for the remote MCP server and Personal Access Tokens in local and remote. This turn-based game MCP server is intended to be simple, and doesn’t include any auth requirements, but security should be a key consideration if you’re building your own MCP servers.Trusting third-party MCP serversYou may not always need to create your own MCP server. For example, GitHub ships its own MCP server. Instead of creating your own version, why not make an open source contribution upstream to improve the experience for all?Make sure to do your due diligence on MCP servers before installing them, just like you would with any other dependency as part of your project’s supply chain. Do you recognise the publisher? Are you able to review (and contribute to) the code in an open source repository?MCP provides SDKs for multiple languages, so you can build servers in whatever technology fits your stack, ranging from TypeScript to Python, Go to Rust and more. I chose TypeScript because I wanted my entire demo (frontend, backend, and MCP server) in one repository with shared code and a common language.Here’s what you can learn from this exploration:MCP standardizes AI tool extensibility across different platforms and applications (like Copilot in Visual Studio Code) by investigating what existing MCP servers are available. Review the MCP servers: Do you recognize the publisher and can you access the code?Building your own? Start simple with focused servers that solve specific problems rather than trying to build everything at onceThe three building blocks (tools, resources, prompts) provide a clear framework for designing the capabilities of your MCP serverMCP isn’t just about playing games with AI (though that was fun). It’s about breaking down the walls between your AI assistants and the systems they need to help you work effectively. Whether you’re building internal developer tools, integrating with external APIs, or creating custom workflows, MCP provides the foundation you need to extend your AI tools in consistent, powerful ways.Want to explore MCP further? Here are some practical starting points:Check out the GitHub MCP server to use in your own workflows or learn more about a real-world MCP server implementation.Build a simple server for your internal APIs or development tools. You can check out the turn-based-game-mcp as an example.Experiment with custom prompts that encode your team’s best practices.The goal of MCP is to give your AI assistants the tools they need to be truly helpful in your specific development environment. So, which tool will you be using? What will you build?]]></content:encoded></item><item><title>Google TV and Android TV Apps Must Support 64-bit Starting August 2026</title><link>https://tech.slashdot.org/story/25/08/22/1624208/google-tv-and-android-tv-apps-must-support-64-bit-starting-august-2026?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli writes: Google is preparing to bring its television platforms in line with the rest of Android. Starting August 1, 2026, both Google TV and Android TV will require app updates that include native code to provide 64-bit support. The move follows similar requirements for phones and tablets, and it paves the way for upcoming 64-bit TV devices.]]></content:encoded></item><item><title>Cracking the Density Code: Why MAF Flows Where KDE Stalls</title><link>https://towardsdatascience.com/cracking-the-density-code-why-maf-flows-where-kde-stalls/</link><author>Zackary Nay</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 16:48:34 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Learn why autoregressive flows are the superior density estimation tool for high-dimensional data]]></content:encoded></item><item><title>Aspiration co-founder to plead guilty to $248M fraud scheme</title><link>https://techcrunch.com/2025/08/22/aspiration-co-founder-to-plead-guilty-to-248m-fraud-scheme/</link><author>Tim De Chant</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:42:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Joseph Sanberg is accused of falsely inflating revenue and fabricating a letter from Aspiration's audit committee to obtain loans, which the startup ultimately defaulted on.]]></content:encoded></item><item><title>Rodrigo Girão Serrão: TIL #130 – Format Python code directly with uv</title><link>https://mathspp.com/blog/til/format-python-code-directly-with-uv</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 16:34:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Today I learned you can format your Python code directly with uv.In uv version 0.8.13, released one or two days ago, uv added the command  that allows you to format your Python code directly through the uv CLI.First and foremost, make sure you're rocking uv 0.8.13 or greater by running .To format your code with uv you can simply run , which will use Ruff to format the code in your current directory:The idea is not to have uv replace Ruff; it's just so that you don't have to think about a separate tool if you don't want to. accepts the same arguments and options that  accepts, so you'll want to check the Ruff docs to learn more.
My favourite option is , to take a look at the formatting diff without doing any formatting changes.As of now, the feature is marked as being experimental, which means it might change in the future!]]></content:encoded></item><item><title>Another Bullshit ‘Assaulting An ICE Officer’ Case Falls Apart In Front Of A Grand Jury</title><link>https://www.techdirt.com/2025/08/22/another-bullshit-assaulting-an-ice-officer-case-falls-apart-in-front-of-a-grand-jury/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:30:28 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The number of assaults on ICE officers was always going to increase. There’s no way it wouldn’t, not when ICE was sending out a task force composed of multiple federal law enforcement agencies daily to multiple locations in the United States, hoping to finally hit the baseline number of 3,000 arrests per day by Stephen Miller. A massive increase in interactions was bound to result in an increase in alleged assaults. The surprising fact, however, was that the increase was so . To hear the DHS tell it, ICE officers are being beaten to the ground daily, with spokespeople constantly posting eye-popping stats like a 690% increase in assaults. (Since then, the percentage has increased to nearly 1000%.)But all that really meant — when the DHS decided to finally be honest about it — was that there had been  more assaults this year as compared to last year (79 to 10). And when you have the actual numbers, this supposed “war on ICE” looks more like ICE officers complaining a bit more than they did last year.Well, ICE officers brought it on themselves. Their insistence on wearing masks, stripping themselves of identifying badges, driving unmarked vehicles, hanging around in courtroom hallways, chasing day laborers across Home Deport parking lots, lurking in rented moving vans, etc. all but ensured there would be the occasional violent reaction to the sudden appearance of masked kidnappers who somehow can’t manage to obtain the occasional judicial warrant. The DHS is relying on its ever-increasing percentage to sell this skewed narrative. Unfortunately for ICE, DHS, and the DOJ, the narrative isn’t holding up in court. Not only are ICE’s tactics being shut down by federal courts, DOJ prosecutors can’t even sneak bullshit charges past grand juries — entities that are normally extremely receptive of the one-sided presentations made by government lawyers.Late last month, the DOJ issued a press release touting one of its latest wins: the charging of DC resident Sydney Reid with assaulting ICE officers. DC US attorney, former Fox talking head Jeanine Pirro, made the announcement, using these words to describe what (allegedly) occurred during this so-called altercation: The FBI agent was assisting two ICE Enforcement and Removal Operations (ERO) officers outside the jail when Reid walked up close to the officers and started recording video. After multiple commands to step back, Reid tried to go around the ERO officers, placing herself between FBI agents and one of the suspects being transferred into their custody.As Reid tried to impede the transfer, one of the ERO officers pushed her against the wall and told her to stop. Reid continued to struggle and fight with the officer. The FBI agent tried to help the officer control Reid who was flailing her arms and kicking. During Reid’s active resistance to being detained, the FBI agent’s hand was injured from striking and scraping the cement wall causing lacerations while the FBI agent was assisting ICE ERO officers.LOL. Arm “flailing” is apparently assault, especially if an officer manages to  during the incident. This was enough for the DOJ to move forward with an attempt to secure an indictment from a grand jury. But it couldn’t even do that because the government seemingly isn’t interested in actually  its case in court — not even in front of a court that only needs to see probable cause, rather than the much higher “beyond a reasonable doubt” standard used by criminal courts. Reid was charged with an enhanced felony assault charge, supposedly due to her “infliction of bodily injury” on the FBI agent who hurt themself while “assisting” ICE in arresting a person who began her interaction by doing nothing more harmful than simply filming them with her phone.The DOJ has tried to indict Reid twice for this supposed “assault.” It has now failed twice, as WUSA9 reports.Federal prosecutors twice sought a grand jury indictment against a D.C. woman accused of assaulting an FBI agent during an ICE inmate transfer — and were twice rejected, the U.S. Attorney’s Office admitted in court Thursday.Magistrate Judge G. Michael Harvey revealed the denials to attorneys for Sydney Lori Reid and later granted their request to remove all bond conditions and release her on her own recognizance over prosecutors’ objections.I’m sure someone will try to pretend these are the actions of an “activist” judge who shouldn’t be allowed to handle cases brought by this particular administration.But the details show it’s the government that’s mostly inert, apparently assuming all it has to do is show up in front of a grand jury to obtain an indictment. Almost zero effort was made here, which makes the double-denial completely understandable:Federal prosecutors declined to call the injured FBI agent or any of the ICE officers involved in the incident during Thursday’s hearing, however. Instead, they had an investigator with the U.S. Attorney’s Office testify about his review of video of the incident and brief conversations with the officers. The investigator, Special Agent Sean Ricardi, said he’d had no involvement in the case until he was asked to prepare for testimony Thursday morning.When the government says “it’s our word against yours,” that’s generally enough to make people understand they’re already going up against a stacked deck. When the government fails (repeatedly, in recent weeks) to secure indictments even when it’s their word against , it’s clear the government actually has no case to present. It would be nice to see a revised percentage from the DHS that only utilizes  assault allegations that result in an indictment or conviction. But we’ll never see that sort of honesty from this administration, which relies almost solely on misrepresentations of  to push its narratives forward. There’s a war on Americans going on here, led by a super-charged ICE. But all the most powerful people can do is play the victim while trying to bully reality into better alignment with its bullshit narratives.]]></content:encoded></item><item><title>US Strips Ocean and Air Pollution Monitoring From Next-Gen Weather Satellites</title><link>https://news.slashdot.org/story/25/08/22/1612250/us-strips-ocean-and-air-pollution-monitoring-from-next-gen-weather-satellites?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:12:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The National Oceanic and Atmospheric Administration is narrowing the capabilities and reducing the number of next-generation weather and climate satellites it plans to build and launch in the coming decades, two people familiar with the plans told CNN. 

This move -- which comes as hurricane season ramps up with Erin lashing the East Coast -- fits a pattern in which the Trump administration is seeking to not only slash climate pollution rules, but also reduce the information collected about the pollution in the first place. Critics of the plan also say it's a short-sighted attempt to save money at the expense of understanding the oceans and atmosphere better. 

Two planned instruments, one that would measure air quality, including pollution and wildfire smoke, and another that would observe ocean conditions in unprecedented detail, are no longer part of the project, the sources said.]]></content:encoded></item><item><title>Show HN: Clyp – Clipboard Manager for Linux</title><link>https://github.com/murat-cileli/clyp</link><author>timeoperator</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 16:03:26 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The HackerNoon Newsletter: The One Question I Ask Before Investing in Any AI Company (8/22/2025)</title><link>https://hackernoon.com/8-22-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 22 Aug 2025 16:02:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, August 22, 2025?By @hacker90235211 [ 4 Min read ] Before you start an AI company or invest in one, always imagine a moment when the AI component is available to everyone tomorrow for free. Read More.By @3l4d [ 6 Min read ] An essay on how the second law of thermodynamics shape the long arc from the Big Bang to AGI, suggesting superintelligence as a physical inevitability. Read More.By @johnwrites [ 4 Min read ] Discover how SocialFi crowdfunding platforms like SeedList are disrupting traditional VC funding in crypto. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Predictive Analytics in Healthcare: Improving Patient Outcomes</title><link>https://www.kdnuggets.com/predictive-analytics-in-healthcare-improving-patient-outcomes</link><author>Shittu Olumide</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-olumide-predictive-analytics-healthcare-improving-patient-outcomes-1.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 16:00:04 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Predictive analytics in healthcare is revolutionizing patient care by using AI and machine learning to forecast health outcomes and optimize treatment plans.]]></content:encoded></item><item><title>404 Media at Two Years: How We&apos;ve Grown, and What&apos;s Next</title><link>https://www.404media.co/two-years-404-media-anniversary/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/08/404year2-24-STARRED-1-1.jpg" length="" type=""/><pubDate>Fri, 22 Aug 2025 15:56:28 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Last week, we were talking to each other about the fact that we were about to hit the second anniversary of 404 Media. The conversation was about what we should say in this blog post, which obviously led us to try to remember everything that has happened in the last year. “I haven’t considered a thing beyond what’s been five seconds behind or in front of me for the last year,” Sam said. The last year has been a whirlwind not just for us but for, uhh, the country and the world. And we’ve been trying our absolute best to bring you stories you can’t find anywhere else about the wildest shit happening right now, which includes the Silicon Valley-led dismantling of the federal government, the deployment of powerful surveillance against immigrants and people seeking abortions, the algorithmic, AI-led zombification of “social” media, the end of anonymity on the internet, and all sorts of weird stuff that we see on our travels through the internet. As Sam noted, we have largely had our heads down trying to bring you the best tech journalism on the internet, which hasn’t left us a ton of time to think about long-term projects, blue-sky ideas, or what the best business strategies for growing this company would be. Our guiding principle is something we said we would do on day one of starting this company: “We believe it is possible to create a sustainable, profitable media company simply by doing good work, making common-sense decisions about costs, and asking our readers to support us.” What we have learned in two years of building this company is that there is no secret to building a media company, and that there are also no shortcuts. When we work hard to publish an important article, more people discover us and more people subscribe to us, which helps solidify our business and allows us to do more and better articles. As our stories reach a larger audience, the articles often have more impact, more potential sources see them, and we get more tips, which leads to more and better articles, and so on.In our second year as a media outlet, we’ve done too much impactful reporting to list out in this post. But to summarize some of the big ones: On top of all of these, we’ve published some of the most moment-defining stories that, as Jason has said many times, are the types of things people talk about at the bar after work. Those include:It has been a relief that this business strategy of “publish good articles and ask people to pay for journalism” still works, despite the fracturing of social media, the slopification of every major platform, AI being shoved into everything, and the rich and powerful trying to destroy journalism at every turn. That it is working is a testament to the support of our subscribers. We have no real way of knowing exactly where new subscribers come from or what ultimately led them to subscribe, but time and time again we have learned that the most important discovery mechanism we have is word of mouth. We have lost count of the number of times a new subscriber has said that they were told about 404 Media by a friend or a family member at a party or in a group text, so if you have told anyone about us, we sincerely thank you.It wasn’t obvious when we started this company that it would actually work, though we hoped that it would., we wrote, “We don’t have any major second-year plans to announce just yet in part because we have been heads down working on some of the investigations and scoops you’ve seen in recent days. The next year holds more scoops, more investigations, more silly blogs, more experiments, more impact, and more articles that hold powerful companies and people to account. We remain ambitious and are thinking about how to best cover more topics and to give you more 404 Media without spreading ourselves too thin.”But we did take a moment to think about what has changed in the last year, and it turns out that quite a lot is different now than it was a year ago. For one, we have cautiously begun to expand what we do. In the last year, we launched , which is Becky Ferreira’s Saturday newsletter about science, which many of you have said you love and which helps us provide a sense of wonder and discovery when so much of what we report on is pretty bleak. We have been getting part-time (but very critical) help from Case Harts who is running and growing our social media accounts, which is helping us put our stories more natively on Instagram, TikTok, YouTube, and other platforms that we do not control but which nonetheless remain important for us to be on.  has started covering the military industrial complex, , , and  beat for us, and has done a remarkable job at it.  is our current intern who has published critical reporting about the sale of , protests at the , and the difficult decisions voice actors need to make about whether they should let .All of this has changed what 404 Media looks like, a little bit. We have spent a lot of time thinking about what it would look like to expand beyond this, why people subscribe to us, what it would mean to go further, and what the four of us are actually capable of handling outside of the journalism. Because of your support we are in a place where we’re able to ask questions beyond “Can we survive?” We’re able to ask questions like: “Should we try to make this bigger, and what does that look like?” We feel incredibly lucky that we are now able to ask ourselves these questions, because there was no guarantee that 404 Media would ever work, and we are forever grateful to everyone who has supported us. You have helped us prove that this model can work, and every day we are delighted to see that other journalists are striking out on their own to create their own publications.We are still DIYing lots of things. Emanuel is still doing customer support. Jason is still ordering, packing, and mailing merch. Sam is putting together events and parties. Joseph is doing an insane number of things behind the scenes, managing the podcast, working closely with one of our ad partners, and fixing technical issues. As we have grown, these tasks have started to take more and more time, which raises all sorts of questions about when and if we should get help with them. Should we do more events? Should we get someone to help us with them? What does that look like logistically and financially? These are the things that we’re working out all the time. It becomes a question of how much can we juggle while still having some semblance of work/life balance, and while making sure that we’re still putting the journalism first. Other things that have happened: We began a republication partnership with  that recently evolved to include a few coreported collaborations that have allowed us to team up on investigations we may not have been able to do by ourselves. We were subpoenaed for our sources on an article by Texas Attorney General Ken Paxton. We successfully fought off this subpoena with the help of our lawyer, which was expensive but which we were able to do because of your support. We are very proud of this.We have been invited to talk about 404 Media and our journalism at conferences and events around the world. Emanuel gave a journalism training in Costa Rica, Jason taught a group of Norwegian journalists how to file FOIA requests and gave a presentation at the Computer History Museum in Mountain View, Joseph spoke at the Hackers on Planet Earth conference, Sam went to Perugia, Italy to  at the International Journalism Conference, and Sam and Jason talked about indie media at the last XOXO in Portland.  We threw a party and live panel at SXSW (with the help of our friends at Flipboard), a DIY party at RIP.SPACE in Los Angeles, and we threw an anniversary party and podcast recording last night . We have moved to Ghost 6.0, which is not something we really , but it’s important to point out that the new version of our CMS is built with native ActivityPub support, meaning our articles are automatically going into the Fediverse and are being mirrored directly onto Bluesky. We are very excited about the possibilities here as we continue to believe that the healthiest future of journalism and the internet is one where we create direct relationships with our readers that have as little algorithmic friction as possible. Ghost is an open-source nonprofit whose mission is very similar to 404 Media’s. Like last year, we don’t have anything crazy to announce for year three. But we hope that you will continue to support us (or, if you’re finding us through this post, will consider subscribing). We discussed some of our hopes and dreams for year three in our latest bonus podcast that went out to supporters this week. We are all trying our very best to bring you important, impactful work as often as possible, and we are trying to be as clear as possible about what’s working, what’s not, and how we’re trying to build this company. So far, that strategy has worked really well, and so we don’t intend to change it now. ]]></content:encoded></item><item><title>Behind the Blog: Our Second Anniversary Party!</title><link>https://www.404media.co/behind-the-blog-our-second-anniversary-party/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/08/DSC_2510-1.JPG" length="" type=""/><pubDate>Fri, 22 Aug 2025 15:55:53 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week,  we have a slightly shorter than usual entry from the gang, with some party pics and musical selections from the night.SAM: We’re all still recovering, processing, and floating on the overwhelming support and encouragement we felt from everyone who came to the second anniversary party last night. Thank you again to our sponsor for the evening, DeleteMe (get 20% off with them here as a thank-you to our community with code 404media) and farm.one for being awesome hosts, and especially thank you to everyone who came, cheered us on from afar, and made the last two years possible. ]]></content:encoded></item><item><title>YC-backed Oway raises $4M to build a decentralized ‘Uber for freight’</title><link>https://techcrunch.com/2025/08/22/yc-backed-oway-raises-4m-to-build-a-decentralized-uber-for-freight/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 22 Aug 2025 15:53:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The thousands of trucks zipping along U.S. highways are often only about half full -- something Oway wants to change with its "ride-share for freight" software.]]></content:encoded></item><item><title>Set up a Playwright Browser Server in AWS EC2</title><link>https://blog.devops.dev/set-up-a-playwright-browser-server-in-aws-ec2-7f5ccb9819f3?source=rss----33f8b2d9a328---4</link><author>th@n@n</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:35:18 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Transforming development and test workflows can save time. In this guide, we’ll explore setting up a Playwright browser server on an AWS EC2 instance. Imagine developers and QA teams pushing many commits. Each event triggers a QA test build. Inefficiency lurks here — every build downloads images and installs browsers. Exhausting, isn’t it?But consider this: a centralised Playwright browser server. It smoothens the process, boosting efficiency. Now, envision your SRE team. They want to monitor network calls, keep an eye on application performance, and collect diverse metrics. A singular point for all this data is crucial.Finally, let’s break it down. We compare Playwright’s launch and launch server modes.Alright..!!!, Let’s see the implementation, there are some prerequisitesCreate an instance in EC2Pull the playwright image and execute the container with browser serverCreate a simple instance as per your wish, don’t forget to download the pem files for ssh connection, used a free tier option.We need to add the port 3000 inbound rules, I explain the reason later in the blog. Apart from I enable port 22 to my IP for ssh connection in order to install any softwares.Connect to your instance, SSH the key. Here’s the commandssh -i "<file_name>.pem" ec2-user@<ip address or machine name>Update the linux system Install docker sudo yum install -y dockerLet’s start the docker service sudo service docker startCheck the status of docker sudo service docker statusSince docker is installed as root user, we need to add ec2-user to the docker group sudo usermod -a -G docker ec2-userNow run the below commanddocker run -d \  --name pw-server \  -p 3000:3000 \  mcr.microsoft.com/playwright:v1.54.0-jammy \  /bin/sh -c "cd /home/pwuser && npx -y playwright@1.54.0 run-server --port 3000"Now check the status of the image and containerLet’s check the whether this browser server is available publicly, run the below command[ec2-user@thanan ~]$ curl -I http://<DNS address>:3000/HTTP/1.1 200 OKDate: Sat, 16 Aug 2025 11:32:29 GMTConnection: keep-aliveNote: Now this is available via Time to execute the test script, go to your playwright framework, since we run the browser server in docker, make sure the script are run in headless mode. I am refer this repo for code execution. Update the below configuration globally in playwright.config.tsIf you want to execute the scripts in head mode, we need `XServer` running in server, due to security reason, org won’t easily install this packageTime to test! Run this command:PW_TEST_CONNECT_WS_ENDPOINT=http://<DNS Host address>:3000/ npx playwright test --project=UI --grep @ui-oneEagerly, await results. They pop as the execution concludes, success in each run.From configuring the instance and opening the necessary ports to installing Docker and deploying the Playwright image, While there are still opportunities for improvement, such as handling IP address changes with each instance restart and the need to redeploy the Playwright container upon reboot, this blog serves as a foundational guide.If you like this content, 👏👏👏 here]]></content:encoded></item><item><title>A Deep Dive into OTA Update Support with KubeEdge for Edge Kubernetes</title><link>https://blog.devops.dev/a-deep-dive-into-ota-update-support-with-kubeedge-for-edge-kubernetes-690da554b727?source=rss----33f8b2d9a328---4</link><author>M Mahdi Ramadhan, M. Si</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:52 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[I have always been deeply inspired by the orchestration capabilities of Kubernetes. Over the years, many of the digital platforms I have designed and built were intentionally directed toward Kubernetes as the backbone — though always with careful consideration of context, to avoid making it overkill where simpler solutions would suffice.That said, one area where Kubernetes reveals its true potential is in . The proliferation of IoT devices and the exponential growth of real-time data processing demands have reshaped the computing paradigm. While centralized cloud data centers remain critical, the need for ultra-low latency, bandwidth efficiency, and local autonomy has pushed workloads closer to where data is generated: the edge.This shift introduces unique challenges — particularly when managing and updating thousands of heterogeneous, geographically distributed devices. Standard Kubernetes, while powerful, is inherently cloud-centric and often too heavy for resource-constrained edge environments.This is where , an open-source CNCF project, comes in. It extends native Kubernetes capabilities to the edge, enabling orchestrated container workloads, data synchronization, and device lifecycle management. One of its most transformative features is its support for Over-the-Air (OTA) updates — a critical mechanism to ensure that containerized applications, edge runtimes, and even firmware on physical devices can be securely updated without manual intervention.In this article, I want to share a deep, experience-based exploration of how KubeEdge enables OTA updates in practice. I will also include a , a , and a  highlighting pitfalls I have encountered and the mitigation strategies that work in real-world deployments.From Cloud-Centric to Edge-Native OrchestrationTraditional cloud computing assumes stable, high-bandwidth connections. But in edge scenarios such as autonomous driving, industrial IoT, or smart agriculture, connectivity may be intermittent, while latency requirements are unforgiving.Standard Kubernetes is not edge-ready by default. It assumes persistent cloud connectivity, strong compute nodes, and central control. In contrast:Edge devices often run on ARM processors with limited resources.Connectivity can drop unexpectedly for hours or days.Devices may require direct hardware interaction (sensors, actuators, PLCs).KubeEdge was designed to address these gaps by splitting Kubernetes into two logical planes:CloudCore – runs in the cloud or a central data center; manages policies, device models, OTA jobs, and workload orchestration.EdgeCore – runs on the edge device; autonomously executes workloads, manages hardware states, and persists operations even if the cloud connection is lost.OTA: The Linchpin of Edge Device Lifecycle ManagementIn real-world edge deployments, the cost of physically updating devices is prohibitive. Imagine dispatching technicians to update 10,000 industrial sensors across factories or retail kiosks across 500 stores. Without OTA, such operations are not feasible.KubeEdge addresses this with a multi-layered OTA mechanism:1. Containerized Application OTAApplications deployed as Kubernetes workloads.Updated through declarative manifests (Deployment, DaemonSet).Rolling updates orchestrated at the edge, resilient to connectivity drops.Managed via Device CRDs (Custom Resource Definitions).Desired firmware version defined in the Device spec.EdgeCore (or a custom mapper) pulls binaries, validates checksums/signatures, flashes the firmware, and updates the status back to CloudCore.3.	Node OTA (EdgeCore / runtime upgrade)Using NodeUpgradeJob (introduced in v1.19+).Securely upgrades KubeEdge edge components, ensuring backward compatibility and checksum validation before activation.Together, these cover the full lifecycle of an edge system: from the application layer, down to the firmware and edge runtime itself.Case Study: Smart Retail Kiosk FleetA retail enterprise deployed 5,000 smart kiosks nationwide. Each kiosk included:Containerized applications for payment, ads display, and inventory sync.Firmware-controlled peripherals (barcode scanner, receipt printer).An edge runtime running KubeEdge EdgeCore.Connectivity: Some kiosks connected over 4G with frequent dropouts.Heterogeneity: Devices had different firmware baselines depending on vendor batch.Security: Regulatory requirement that every update must be signed and auditable.Defined a DeviceModel capturing properties: firmware version, display resolution, payment module version.Managed application updates by publishing new container images, updating the Deployment spec – CloudCore handled synchronization, and EdgeCore performed rolling updates locally.For firmware updates, an OTA Job resource was created with a signed binary hosted on an HTTPS OTA server. EdgeCore validated often overlooked, but based on my experience many errors occur during data transmission), updated firmware via serial flashing, and patched the Device CRD status with the new version.For EdgeCore runtime upgrades, NodeUpgradeJobs were rolled out in controlled batches (5% of nodes at a time).Updates reduced from 3 – 4 weeks manual rollout to 2 hours automated deployment.98% first-pass update success rate; failures automatically retried once connectivity was restored.OTA audit logs integrated with enterprise compliance systems, ensuring traceability.Tutorial: Building a KubeEdge OTA SetupA Kubernetes cluster (v1.25+).keadm (KubeEdge deployment tool).One VM/cloud server (cloud side), one edge node (VM or Raspberry Pi)Step 1: Install CloudCoresudo keadm init  – kubeedge-version v1.20.0keadm gettokensudo keadm join  – cloudcore-ipport <CLOUD_IP>:10000 \  --token <TOKEN> \  --kubeedge-version v1.20.0Step 3: Define a DeviceModelapiVersion: devices.kubeedge.io/v1beta1kind: DeviceModel  name: firmware-updater-model  properties:      type: stringStep 4: Define a Device InstanceapiVersion: devices.kubeedge.io/v1beta1kind: Device  name: edge-device-01  nodeName: edge-node-1    name: firmware-updater-model    - name: firmware_version      reported: "1.0.0"Step 5: Trigger an OTA Updateproperties:  - name: firmware_versionkubectl patch device edge-device-01 \  --type=merge \  -p '{"status":{"twins":{"firmware_version":{"reported":{"value":"2.0.0"}}}}}'Risk Registry: Challenges and Preparedness in KubeEdge OTA DeploymentsWhile KubeEdge provides a robust OTA mechanism, production deployments must account for inherent risks. Below is a risk registry distilled from real-world projects:OTA is both a business enabler and an attack vector – security and rollback strategies must be non-negotiable.Connectivity resilience is paramount; production rollouts must batch, monitor, and retry intelligently.Observability and compliance logging are not optional for industries subject to regulation.KubeEdge elevates Kubernetes from being a container orchestrator to becoming a full lifecycle management platform for edge computing. Its OTA support ensures that applications, firmware, and runtime components can evolve securely and reliably – at scale.From smart retail kiosks to industrial automation, OTA in KubeEdge transforms edge infrastructure from brittle, manually maintained systems into self-updating, cloud-native edge fleets.As organizations accelerate toward Industry 4.0, 5G, and intelligent IoT, embracing KubeEdge’s OTA capabilities is not just an optimization – it is a necessity for resilience, security, and scalability at the edge.]]></content:encoded></item><item><title>Can Sales Data Predict Stock Prices? A Data Science Experiment with Python</title><link>https://blog.devops.dev/can-sales-data-predict-stock-prices-a-data-science-experiment-with-python-2b0d54d711f0?source=rss----33f8b2d9a328---4</link><author>Vaishnavi Ganeshkar</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:48 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[What if a company’s sales performance could predict its stock price? We hear investors talking about revenue, growth, and quarterly reports all the time — but can we actually see the connection in data?In this blog, we’ll run a fun data experiment: analyzing  and  side by side using Python. By the end, you’ll know whether better sales = higher stock prices, and what this means for data analysts, businesses, and investors.: Downloaded with the yfinance library (Yahoo Finance API).This gives us a way to compare company’s sales performance (business side) with stock market valuation (investor side).⚙️ Step 1: Setup Environment!pip install pandas matplotlib seaborn yfinanceimport pandas as pdimport matplotlib.pyplot as pltimport yfinance as yf📂 Step 2: Load Sales Data# Load sales datasetsales = pd.read_csv("sales_data_sample.csv", encoding="latin1")sales['ORDERDATE'] = pd.to_datetime(sales['ORDERDATE'])We’ll use  and  for monthly aggregation.📈 Step 3: Monthly Sales Trendmonthly_sales = sales.groupby(pd.Grouper(key="ORDERDATE", freq="M"))['SALES'].sum()plt.figure(figsize=(10,5))monthly_sales.plot()plt.title("Monthly Sales Trend")plt.xlabel("Month")plt.ylabel("Total Sales")plt.show()This gives us a clear view of how sales are moving over time.💹 Step 4: Fetch Stock Market DataLet’s pick  for our analysis.start_date = sales['ORDERDATE'].min().strftime("%Y-%m-%d")end_date = sales['ORDERDATE'].max().strftime("%Y-%m-%d")stock = yf.download("AAPL", start=start_date, end=end_date)stock['Close'].plot(figsize=(10,5), title="Apple Stock Price")🔄 Step 5: Merge Sales & Stock Data# Monthly Salesmonthly_sales = sales.groupby(pd.Grouper(key="ORDERDATE", freq="M"))['SALES'].sum()stock_monthly = stock['Close'].resample('M').mean()df = pd.concat([monthly_sales, stock_monthly], axis=1)df.columns = ["Sales", "StockPrice"]print(df.dropna().head())   # Check after removing NaNNow we have both Sales and Stock Price in one DataFrame.📊 Step 6: Sales vs Stock Price Scatterplotsns.scatterplot(x="Sales", y="StockPrice", data=df.dropna())plt.title("Sales vs Stock Price")📌 : The scatterplot shows a . Higher sales are often paired with higher stock prices. But it’s not perfectly linear — there are plenty of scattered points.📈 Step 7: Add Regression Trendlinesns.regplot(x="Sales", y="StockPrice", data=df, scatter_kws={"s":50}, line_kws={"color":"red"})plt.title("Sales vs Stock Price with Trendline")The  shows a positive correlation → as sales increase, stock prices tend to rise.The  is the confidence interval → wide bands mean stock price is influenced by other factors beyond sales (investor sentiment, economic conditions, etc.).Some  show months where strong sales did not immediately push stock prices — reminding us that the market is not just logical, it’s psychological too.📌 Step 8: Correlation Check✅ Insight from Correlation OutputThe correlation coefficient between  and  is .This means there is a weak positive relationship between the two variables. In simple terms:As , Stock Prices also tend to increase, but not strongly.Other external factors (market sentiment, competition, economy, company news) are likely influencing stock price much more than just sales.For investors: relying only on sales to predict stock price is risky — it’s one piece of the puzzle, not the whole picture.For businesses: increasing sales  proportional stock price growth; branding, profitability, and investor confidence also matter.“From the correlation matrix, we see that Sales and Stock Price have a . This suggests a weak positive relationship — meaning higher sales may push stock prices up slightly, but the effect is not very strong. Clearly, stock prices depend on multiple factors beyond just sales numbers, such as market conditions, company reputation, and investor sentiment.”🚀 Why This Matters for YouIf you’re a  → this project strengthens your portfolio (business + finance domain).If you’re an  → remember that sales performance is one of many factors that move stock prices.If you’re just learning  → this project teaches you how to merge and analyze cross-domain datasets.This experiment shows that while sales data does impact stock performance, it’s not the . The stock market is a mix of fundamentals, sentiment, and sometimes pure unpredictability.👉 Want to go deeper? Extend this project into:Forecasting sales impact on stock (ARIMA, LSTM models)Comparing multiple companiesBuilding dashboards (Power BI, Tableau) for interactive visualization📌 The next time someone says “good sales mean good stock returns,” you’ll know the data-backed truth: ✅ That’s it! You now have a complete  with dataset, code, plots, insights, and storytelling.]]></content:encoded></item><item><title>Blue-Green Deployments: A Practical Path to Zero-Downtime Releases</title><link>https://blog.devops.dev/blue-green-deployments-a-practical-path-to-zero-downtime-releases-54c83fe0265c?source=rss----33f8b2d9a328---4</link><author>Madhura Jayashanka</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:44 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Downtime is costly. According to ITIC’s 2024 report, 90% of companies say one hour of downtime costs over , and 41% say it can reach  (PDF source).DORA’s research also shows that high-performing teams deliver quickly  stay stable. Earlier benchmarks from 2019 and 2021 defined elite teams as:Change failure rate between Time to restore service Blue-green deployments are one way to hit these goals.What is Blue-Green Deployment?It means running two production-ready environments: — the version currently serving users — the new version waiting to go liveThe process looks like this:Deploy your update to Run tests and health checksSwitch traffic from  to If something goes wrong, switch back to  instantlyZero downtime during cutoverInstant rollback if problems appearRealistic testing under production conditionsTemporary extra infrastructure capacityNeed to keep configuration and secrets in syncUptime is part of your contract or SLAReleases are risky (payments, authentication, APIs)You want a quick, binary safety switch — good for gradual rollout with percentage-based traffic — good if you can accept slower rollbackThere are three common ways to flip traffic between Blue and Green:1. Kubernetes Service selectorSwitch label selector from color=blue to color=green2. AWS ECS with ALB/CodeDeployTwo target groups (Blue and Green) behind an ALBCodeDeploy shifts traffic and can auto-rollbackDefine Blue and Green upstreamsA map variable chooses which backend is activenginx -s reload applies the changeExample: Kubernetes Blue-GreenapiVersion: v1kind: Service  name: demo-svc  selector:    color: blue   # flip to green to switch traffic    - port: 80apiVersion: apps/v1kind: Deployment  name: demo-blue  replicas: 3    matchLabels: { app: demo, color: blue }    metadata:      labels: { app: demo, color: blue }    spec:        - name: app          image: ghcr.io/yourorg/demo:1.0.0          ports: [{ containerPort: 8080 }]            httpGet: { path: /healthz, port: 8080 } is identical but with color: green and a different image tag.kubectl patch svc demo-svc -p '{"spec":{"selector":{"app":"demo","color":"green"}}}'Example: NGINX Blue-Greenupstream blue_backend  { server 10.0.0.11:8080; }upstream green_backend { server 10.0.0.12:8080; }map $http_host $active_color { default blue; }  # set to green to flipChange $active_color to green, then reload NGINX.Databases need a special strategy. The best approach is the  pattern (source):Expand schema safely (add new columns or tables)Deploy new code (works with both old and new schema)Health Checks Before SwitchingApplication readiness probe must pass consistentlyError rates and latency should match the baselineWatch CPU, memory, and a key business KPI during the switchKeep Blue running in the background for 30–120 minutes in case rollback is needed — patch Service selector back to Blue — change $active_color back to Blue and reload — trigger rollback in CodeDeploy (docs)Is blue-green the same as canary? No. Blue-green switches all traffic at once. Canary shifts traffic gradually.Does blue-green guarantee zero downtime? It aims to, as long as health probes and load balancer configs are set correctly.What is “good” stability? According to DORA, good teams keep change failure low and can restore service within about an hour.]]></content:encoded></item><item><title>The Truth About Cold Starts in Google Cloud Run &amp; Functions</title><link>https://blog.devops.dev/the-truth-about-cold-starts-in-google-cloud-run-functions-efb1c5bccfda?source=rss----33f8b2d9a328---4</link><author>Engineer</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:37 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[If you’ve been using Google Cloud Run or Cloud Functions, chances are you’ve noticed some requests feel like they’re waking up from a nap…]]></content:encoded></item><item><title>Cloudflare WAF Best Practices: Features, Challenges, and Alternatives</title><link>https://blog.devops.dev/cloudflare-waf-best-practices-features-challenges-and-alternatives-46d7da078f6e?source=rss----33f8b2d9a328---4</link><author>Maverick Steel</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:22 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Web Application Firewalls (WAFs) play a critical role in securing modern web applications from a wide range of threats, such as SQL injection, cross-site scripting (XSS), and other OWASP Top 10 vulnerabilities.Among various WAF solutions available today, Cloudflare WAF is one of the most widely adopted due to its robust features, ease of use, and global performance benefits.This article explores the best practices for using Cloudflare WAF, highlights some challenges, and introduces an alternative WAF solution — Safeline.Features of Cloudflare WAFCloudflare WAF includes a broad set of pre-configured security rules that cover OWASP Top 10 vulnerabilities, zero-day exploits, and protocol anomalies. These rules are frequently updated to respond to emerging threats.Cloudflare WAF can automatically learn from traffic patterns to reduce false positives, minimizing disruptions to legitimate users while enhancing security.Being part of Cloudflare’s global Content Delivery Network (CDN) gives the WAF low latency and high availability, ensuring security without sacrificing performance.Users can create custom firewall rules tailored to their application’s specific needs using Cloudflare’s intuitive dashboard or API.Cloudflare WAF integrates with bot management features to distinguish between good and bad bots, reducing malicious traffic and improving analytics.Best Practices for Using Cloudflare WAFRegularly Update Rule Sets: Keep Cloudflare’s managed rules enabled and updated to protect against newly discovered vulnerabilities.Enable Logging and Monitor Alerts: Continuously monitor WAF logs and alerts to quickly respond to potential threats or false positives.Use Custom Rules Judiciously: While custom rules offer flexibility, improper configuration can cause unintended blocks or gaps; test thoroughly.Integrate with Security Ecosystem: Combine Cloudflare WAF with other security tools like SIEMs for comprehensive threat detection and response. Start with learning mode to understand traffic patterns and adjust policies for smoother deployment.Challenges with Cloudflare WAF Despite managed rules and learning modes, some false positives still occur, potentially impacting user experience. Advanced features and higher rule set limits are available only on paid plans, which might be expensive for small businesses.Limited Deep Customization: Although flexible, Cloudflare’s WAF may not meet very niche or complex application requirements compared to specialized WAFs.Dependence on Cloudflare Network: Using Cloudflare WAF means routing all traffic through Cloudflare’s network, which can be a concern for some organizations regarding data sovereignty or vendor lock-in.Alternatives: SafeLine WAFSafeLine(https://ly.safepoint.cloud/k9fyEuu) is an emerging WAF solution that offers a compelling alternative to Cloudflare WAF, especially for enterprises seeking deeper customization and more direct control over deployment environments.Highly Customizable Rules Engine: SafeLine provides a powerful rules engine that allows fine-grained rule creation, enabling tailored protection for unique application requirements.On-Premises and Cloud Deployment: Unlike Cloudflare, Safeline supports hybrid deployments — on-premises, cloud, or edge — to meet diverse organizational policies and compliance needs.Threat Intelligence Integration: SafeLine integrates with a threat intelligence feed, enabling proactive detection of zero-day vulnerabilities and sophisticated attacks.Performance Optimization: Designed with minimal latency in mind, SafeLine optimizes traffic without relying on a global CDN network. SafeLine offers advanced bot detection capabilities that can differentiate human users from automated attacks beyond signature-based methods.When to Consider SafeLineIf your organization requires on-premises control, extensive rule customization, or has concerns related to data residency and vendor dependency, SafeLine presents a flexible and powerful alternative to Cloudflare WAF.Cloudflare WAF is a robust and widely-used solution that offers strong security features combined with the performance benefits of a global CDN. Following best practices such as updating rules and monitoring logs helps maximize its effectiveness. However, challenges like false positives and vendor dependency warrant consideration of alternatives. Safeline stands out as a strong option for businesses needing customizable, flexible deployment options alongside comprehensive web application protection.Choosing the right WAF depends on your specific requirements regarding performance, deployment, customization, and cost. Both Cloudflare WAF and Safeline provide valuable tools to safeguard your web applications in today’s evolving threat landscape.]]></content:encoded></item><item><title>Red Flags in K8s Configuration That Kills Your Apps</title><link>https://blog.devops.dev/red-flags-in-k8s-configuration-that-kills-your-apps-cb738d435989?source=rss----33f8b2d9a328---4</link><author>Devops Diaries</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:35 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Deploying apps on Kubernetes feels smooth — until something breaks in production. More often than not, the culprit isn’t Kubernetes itself…]]></content:encoded></item><item><title>AI as a Co-pilot for Developers: Boosting Productivity Without Losing Control</title><link>https://blog.devops.dev/ai-as-a-co-pilot-for-developers-boosting-productivity-without-losing-control-d5abb9e73419?source=rss----33f8b2d9a328---4</link><author>Vitor Hansen</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Artificial Intelligence (AI) is no longer just a buzzword in the tech industry, it’s becoming an essential companion for developers. From generating code snippets to reviewing pull requests and even optimizing CI/CD pipelines, AI is changing the way software is built.In this article, we’ll explore what it means to have an “AI co-pilot” in your development workflow, compare the most popular tools (ChatGPT, Claude, Gemini, and Codeium), and walk through a practical example of using AI to validate code in a CI/CD pipeline.An AI co-pilot is a system that assists developers in writing, reviewing, optimizing, and understanding code — without replacing the human in charge. Think of it as a pair programming partner that never gets tired, always has instant suggestions, and can adapt to different coding styles.AI co-pilots come in two main forms:Context-aware code assistants — Integrated into IDEs (VS Code, JetBrains, etc.) to provide inline suggestions and documentation.Conversational AI assistants — Chat interfaces where you can paste code, ask for explanations, or request specific implementations.The best co-pilots blend both approaches, letting you move between coding, reviewing, and designing without breaking flow.2. Core Use Cases for DevelopersAI co-pilots can be applied across the entire development lifecycle. Here are some of the most common scenarios:Creating boilerplate code quickly.Suggesting implementation details based on comments.Generating functions in multiple languages.Highlighting potential bugs and security risks.Suggesting better algorithms or data structures.Pointing out unused imports or redundant code.Explaining cryptic error messages.Suggesting possible fixes for runtime issues.Reproducing bugs based on logs.Improving performance by refactoring loops, queries, or memory usage.Suggesting more efficient libraries or APIs.Reducing complexity for maintainability.Generating docstrings and README files.Creating developer onboarding guides from existing code.Explaining APIs for both internal and public use.3. Hands-On Example: AI in a CI/CD PipelineLet’s see how an AI co-pilot can help in a Continuous Integration / Continuous Deployment setup. You want every pull request in your repository to be automatically reviewed for:Instead of manually going through every file, you integrate an AI service in your CI pipeline.Example with GitHub Actions + OpenAI API:name: AI Code Review  pull_request:  ai-review:    steps:      - name: Checkout repository        uses: actions/checkout@v3      - name: Install dependencies        run: npm install      - name: Run AI Code Review        env:          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}        run: |          node scripts/ai-review.jsimport OpenAI from "openai";import fs from "fs";const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });const files = ["src/index.js", "src/utils/helpers.js"];  for (const file of files) {    const code = fs.readFileSync(file, "utf8");    const prompt = `      Review the following JavaScript code for:      1. Potential security issues      2. Code smells or anti-patterns      3. Missing test coverage      Provide feedback in a concise bullet-point list.      ${code}    const response = await openai.chat.completions.create({      messages: [{ role: "user", content: prompt }],    console.log(`Review for ${file}:`);    console.log(response.choices[0].message.content);  }This simple setup can automatically leave AI-generated feedback as part of the PR process, helping developers catch issues earlier.5. Challenges and LimitationsAI co-pilots are powerful, but they’re not magic. Key challenges include: — Code sent to third-party APIs might expose sensitive info. — AI suggestions can be wrong or insecure. — Large codebases may exceed prompt size limits. — Heavy usage can increase expenses. — Relying too much on AI may reduce skill retention.6. Best Practices for Using AI in Development — Never merge AI-generated code without review.Keep Sensitive Data Local — Use self-hosted models for confidential projects. — The better your instructions, the better the output. — Let AI suggest, but tests confirm. — Use PR labels to flag AI-assisted commits.7. The Future of AI Co-pilotsWe’re moving towards a reality where AI will handle more repetitive coding tasks, freeing developers to focus on problem-solving and architecture. The winning strategy isn’t to fight AI, it’s to .The best developers of tomorrow will be those who can combine human creativity with AI efficiency. AI isn’t here to replace developers, it’s here to make us . By understanding its strengths, limits, and best practices, you can turn AI into a genuine co-pilot that helps you ship faster, with higher quality, and less stress.]]></content:encoded></item><item><title>Google makes it easier to edit Drive videos with a new Vids shortcut button</title><link>https://techcrunch.com/2025/08/22/google-makes-it-easier-to-edit-drive-videos-with-a-new-vids-shortcut-button/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Fri, 22 Aug 2025 15:33:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Now Workspace users can initiate a video edit using the Vids app directly from the Google Drive interface.]]></content:encoded></item><item><title>Scaling Your Application Resiliently with AWS: From Zero to Millions of Requests</title><link>https://blog.devops.dev/scaling-your-application-resiliently-with-aws-from-zero-to-millions-of-requests-611648b92eb5?source=rss----33f8b2d9a328---4</link><author>Vitor Hansen</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:25 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[A strategic and technical guide for architects and engineers aiming to build modern, resilient systems designed for growth.Scalability is a critical challenge every successful software system faces. Too often, teams only address scalability issues after significant growth, leading to downtime, poor user experiences, and costly refactoring. I’ve witnessed first-hand systems crashing under load due to inadequate architecture planning. This post emphasizes the importance of building scalability and resilience from the ground up.2. When (and Why) to Scale?First, let’s clarify the distinction:  is about how fast your system handles individual requests, while  is about how well your system handles increasing volume.You know it’s time to scale when:Response times are increasingServers are consistently maxed outMonitoring tools like CloudWatch, X-Ray, or Sentry show consistent bottlenecksScaling is not just a technical necessity, it’s a business enabler. Building for scale means your system can grow with your users, rather than crumble under their weight.3. Modern Scalable Architecture on AWSToday’s cloud architectures leverage key AWS components to build scalable and resilient systems: Manages, secures, and routes API requests. Serverless compute, automatically scaling and handling workloads. Managed container orchestration that removes infrastructure management overhead. Highly available relational databases. NoSQL database offering high performance and scalability. Queue management ensuring reliable asynchronous processing. Event-driven communication between services. Improves system performance by caching frequently accessed data.This setup ensures both horizontal scalability and fault tolerance across all layers.Consider a common scenario: an application receiving public API requests and processing financial transactions: Receives requests, applies rate limiting, and manages authentication. Processes incoming requests, scales automatically based on demand. Handles longer-running processes and workflows. Stores transactional and rapidly accessed data. Stores structured data requiring strong consistency. Manages communication and workflows asynchronously.This architecture scales seamlessly, maintaining consistent performance and resilience even under heavy load.Avoid these mistakes when designing scalable AWS systems:Ignoring cold starts in Lambda: Minimize with smaller functions and Provisioned Concurrency.Poor scaling configuration: Test and validate your auto-scaling policies regularly. Use DynamoDB or caching where eventual consistency is acceptable. Avoid direct service-to-service calls without message buses like SQS/EventBridge.6. Golden Tips for ResilienceEnhance your system’s resilience with these best practices:Implement retries and Dead Letter Queues (DLQ) with Amazon SQS.Apply the Circuit Breaker pattern with API Gateway and Lambda to prevent cascading failures.Use Canary Deployments for safe, gradual rollouts of Lambda functions and ECS services.Scalability isn’t just about adding infrastructure; it’s primarily about thoughtful architecture design. Building scalable systems from the start ensures your business can adapt quickly to growth without sacrificing reliability or performance. AWS, Architecture, Scalability, Serverless, Fargate, Lambda, Best Practices, Cloud]]></content:encoded></item><item><title>How to Set up GitHub Actions for a Node.js project</title><link>https://blog.devops.dev/how-to-setup-github-actions-for-node-js-project-1edd6ce1dbe1?source=rss----33f8b2d9a328---4</link><author>Mohammad Faisal Khatri</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:32:51 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Video Friday: Inaugural World Humanoid Robot Games Held</title><link>https://spectrum.ieee.org/world-humanoid-robot-games</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTUwMTcyNi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwNjgyNDE0Nn0.EozRKG9htuXcv5H4asOKEe5qqTQb_Gl1df-A1BHZdHs/image.png?width=600" length="" type=""/><pubDate>Fri, 22 Aug 2025 15:30:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>FFmpeg 8.0</title><link>https://ffmpeg.org/index.html#pr8.0</link><author>gyan</author><category>hn</category><pubDate>Fri, 22 Aug 2025 15:22:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
        A complete, cross-platform solution to record, convert and stream audio and video.
      Converting  and  has never been so easy.
    $ ffmpeg -i input.mp4 output.aviAugust 22nd, 2025, FFmpeg 8.0 
  A new major release, FFmpeg 8.0 ,
  is now available for download.
  Thanks to several delays, and modernization of our entire infrastructure, this release ended up
  being one of our largest releases to date. In short, its new features are:
  Native decoders: , ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728VVC decoder improvements: ,
                                  ,
                                  Palette ModeVulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5Formats: MCC, G.728, Whip, APVFilters: colordetect, pad_cuda, scale_d3d11, Whisper, and others
  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.
  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,
  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.
  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work
  on any implementation of Vulkan 1.3.
  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,
  as enabling Vulkan decoding is sufficient to use them.
  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).
  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).
  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,
  to be merged soon and available with the next minor release.
  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with
  more mainstream codecs not being planned for support.
  Depending on the hardware, these new codecs can provide very significant speedups, and open up
  possibilities to work with them for situations like non-linear video editors and
  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.
  
  The project has recently started to modernize its infrastructure. Our mailing list servers have been
  fully upgraded, and we have recently started to accept contributions via a new forge, available on
  code.ffmpeg.org, running a Forgejo instance.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 30th, 2024, FFmpeg 7.1 
    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,
    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain
    traction with broadcast standardization bodies.
    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting
    to be adopted by streaming websites, due to its extensive volume normalization metadata.
    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated
    by recent phones and VR headsets.
    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an
    external library.
    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode
    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity
    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,
    and FFmpeg is aiming to have day-one support.
  
    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally
    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,
    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10
    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.
    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also
    laid the path for more advanced forms of negotiation.
    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,
    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 11th, 2024, Coverity
  The number of issues FFmpeg has in Coverity (a static analyzer) is now lower than it has been since 2016.
  Our defect density is less than one 30th of the average in OSS with over a million code
  lines. All this was possible thanks to a grant from the Sovereign Tech Fund.
  June 2nd, 2024, native xHE-AAC decoder
  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround
  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is
  coming soon. Work is also ongoing to improve its stability and compatibility.
  During the process we found several specification issues, which were then submitted back to the authors
  for discussion and potential inclusion in a future errata.
  May 13th, 2024, Sovereign Tech Fund
  The FFmpeg community is excited to announce that Germany's
  Sovereign Tech Fund
  has become its first governmental sponsor. Their support will help
  sustain the maintainance of the FFmpeg project, a critical open-source
  software multimedia component essential to bringing audio and video to
  billions around the world everyday.
  April 5th, 2024, FFmpeg 7.0 "Dijkstra"
  This release is  backwards compatible, removing APIs deprecated before 6.0.
  The biggest change for most library callers will be the removal of the old bitmask-based
  channel layout API, replaced by the  API allowing such
  features as custom channel ordering, or Ambisonics. Certain deprecated 
  CLI options were also removed, and a C11-compliant compiler is now required to build
  the code.
  
  As usual, there is also a number of new supported formats and codecs, new filters, APIs,
  and countless smaller features and bugfixes. Compared to 6.1, the  repository
  contains almost ∼2000 new commits by ∼100 authors, touching >100000 lines in
  ∼2000 files — thanks to everyone who contributed. See the
  Changelog,
  APIchanges,
  and the git log for more comprehensive lists of changes.
  January 3rd, 2024, native VVC decoder
  The  library now contains a native VVC (Versatile Video Coding)
  decoder, supporting a large subset of the codec's features. Further optimizations and
  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,
  Frank Plowman, Shaun Loo, and Wu Jianhua.
  December 18th, 2023, IAMF support
  The  library can now read and write IAMF
  (Immersive Audio) files. The  CLI tool can configure IAMF structure with the new
   option. IAMF support was written by James Almer.
  December 12th, 2023, multi-threaded  CLI tool
  Thanks to a major refactoring of the  command-line tool, all the major
  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now
  run in parallel. This should improve throughput and CPU utilization, decrease latency,
  and open the way to other exciting new features.
  
  Note that you should  expect significant performance improvements in cases
  where almost all computational time is spent in a single component (typically video
  encoding).
  November 10th, 2023, FFmpeg 6.1 "Heaviside"Playdate video decoder and demuxerExtend VAAPI support for libva-win32 on Windowsafireqsrc audio source filterffmpeg CLI new option: -readrate_initial_burstzoneplate video source filtercommand support in the setpts and asetpts filtersVulkan decode hwaccel, supporting H264, HEVC and AV1Essential Video Coding parser, muxer and demuxerEssential Video Coding frame merge bsfMicrosoft RLE video encoderRaw AC-4 muxer and demuxerRaw VVC bitstream parser, muxer and demuxerBitstream filter for editing metadata in VVC streamsBitstream filter for converting VVC from MP4 to Annex Bscale_vt filter for videotoolboxtranspose_vt filter for videotoolboxsupport for the P_SKIP hinting to speed up libx264 encodingSupport HEVC,VP9,AV1 codec in enhanced flv formatapsnr and asisdr audio filtersSupport HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocolffmpeg CLI '-top' option deprecated in favor of the setfield filterffprobe XML output schema changed to account for multiple variable-fields elements within the same parent elementffprobe -output_format option added as an alias of -of
    This release had been overdue for at least half a year, but due to constant activity in the repository,
    had to be delayed, and we were finally able to branch off the release recently, before some of the large
    changes scheduled for 7.0 were merged.
  
    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs
    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).
    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.
    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,
    reducing overhead.
    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.
    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the
    accurracy of variable frame rate video.
  
    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick
    to the new release schedule we announced at the start of this year.
  
    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.
  May 31st, 2023, Vulkan decoding
    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.
    This is the first vendor-generic and platform-generic decode acceleration API, enabling the
    same code to be used on multiple platforms, with very minimal overhead.
    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,
    saturating all available decode engines the hardware exposes.
  
    Those wishing to test the code can read our
    documentation page.
    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive
    a VkImage to present or manipulate, documentation and examples are available in our source tree.
    Currently, using the latest available git checkout of our
    repository is required.
    The functionality will be included in stable branches with the release of version 6.1, due
    to be released soon.
  
    As this is also the first practical implementation of the specifications, bugs may be present,
    particularly in drivers, and, although passing verification, the implementation itself.
    New codecs, and encoding support are also being worked on, by both the Khronos organization
    for standardizing, and us as implementing it, and giving feedback on improving.
  February 28th, 2023, FFmpeg 6.0 "Von Neumann"
    A new major release, FFmpeg 6.0 "Von Neumann",
    is now available for download. This release has many new encoders and decoders, filters,
    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major
    releases will now bump the version of the ABI. We plan to have a new major release each
    year. Another release-specific change is that deprecated APIs will be removed after 3
    releases, upon the next major bump.
    This means that releases will be done more often and will be more organized.
  
    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.
    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c
    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,
    and the ability to pass option values for filters from a file. There are quite a few new audio
    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.
    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT
    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better
    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V
    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed
    in the doc/APIchanges file in our tree.
    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the
    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.
    Some highlights are:
  Radiance HDR image supportddagrab (Desktop Duplication) video capture filterffmpeg -shortest_buf_duration optionffmpeg now requires threading to be builtffmpeg now runs every muxer in a separate threadAdd new mode to cropdetect filter to detect crop-area based on motion vectors and edgesVAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9WBMP (Wireless Application Protocol Bitmap) image formatMicronas SC-4 audio decodernvenc AV1 encoding supportMediaCodec decoder via NDKMediaCodecQSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9showcwt multimedia filterWADY DPCM decoder and demuxerffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]hstack_vaapi, vstack_vaapi and xstack_vaapi filtersXMD ADPCM decoder and demuxerffmpeg CLI new option: -fix_sub_duration_heartbeatWavArc decoder and demuxerCrystalHD decoders deprecatedfiltergraph syntax in ffmpeg CLI now supports passing file contents as option valueshstack_qsv, vstack_qsv and xstack_qsv filters
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  July 22nd, 2022, FFmpeg 5.1 "Riemann"add ipfs/ipns protocol supportdialogue enhance audio filterdropped obsolete XvMC hwaccelDFPWM audio encoder/decoder and raw muxer/demuxerVizrt Binary Image encoder/decodercolorchart video source filterPGS subtitle frame merge bitstream filteradded chromakey_cuda filter
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 17th, 2022, FFmpeg 5.0 "Lorentz"FFmpeg 5.0 "Lorentz", a new
    major release, is now available! For this long-overdue release, a major effort
    underwent to remove the old encode/decode APIs and replace them with an
    N:M-based API, the entire libavresample library was removed, libswscale
    has a new, easier to use AVframe-based API, the Vulkan code was much improved,
    many new filters were added, including libplacebo integration, and finally,
    DoVi support was added, including tonemapping and remuxing. The default
    AAC encoder settings were also changed to improve quality.
    Some of the changelog highlights:
  ADPCM IMA Westwood encoderADPCM IMA Acorn Replay decoderArgonaut Games CVG demuxeraudio and video segment filtersApple Graphics (SMC) encoderhsvkey and hsvhold video filtersadecorrelate audio filterAV1 Low overhead bitstream format muxerhuesaturation video filtercolorspectrum source video filterRTP packetizer for uncompressed video (RFC 4175)VideoToolbox ProRes hwaccelaspectralstats audio filteradynamicsmooth audio filtervflip_vulkan, hflip_vulkan and flip_vulkan filtersadynamicequalizer audio filteryadif_videotoolbox filterVideoToolbox ProRes encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  
    We have a new IRC home at Libera Chat
    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at contact#IRCChannelsApril 8th, 2021, FFmpeg 4.4 "Rao"FFmpeg 4.4 "Rao", a new
    major release, is now available! Some of the highlights:
  AudioToolbox output deviceVDPAU accelerated HEVC 10/12bit decodingADPCM IMA Ubisoft APM encoderAV1 encoding support SVT-AV1ADPCM Argonaut Games encoderAV1 Low overhead bitstream format demuxerMobiClip FastAudio decoderAV1 decoder (Hardware acceleration used only)Argonaut Games BRP demuxerIPU decoder, parser and demuxerIntel QSV-accelerated AV1 decodingArgonaut Games Video decoderlibwavpack encoder removedAVS3 video decoder via libuavs3dVDPAU accelerated VP9 10/12bit decodingafreqshift and aphaseshift filtersHigh Voltage Software ADPCM encoderLEGO Racers ALP (.tun & .pcm) muxerDXVA2/D3D11VA hardware accelerated AV1 decodingMicrosoft Paint (MSP) version 2 decoderMicrosoft Paint (MSP) demuxerAV1 monochrome encoding support via libaom >= 2.0.1asuperpass and asuperstop filterDigital Pictures SGA demuxer and decodersTTML subtitle encoder and muxerRIST protocol via librist
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  June 15th, 2020, FFmpeg 4.3 "4:3"FFmpeg 4.3 "4:3", a new
    major release, is now available! Some of the highlights:
  Intel QSV-accelerated MJPEG decodingIntel QSV-accelerated VP9 decodingSupport for TrueHD in mp4Support AMD AMF encoder on Linux (via Vulkan)support Sipro ACELP.KELVIN decodingmaskedmin and maskedmax filtersQSV-accelerated VP9 encodingAV1 encoding support via librav1eAV1 frame merge bitstream filterMPEG-H 3D Audio support in mp4Argonaut Games ADPCM decoderArgonaut Games ASF demuxerafirsrc audio filter sourceSimon & Schuster Interactive ADPCM decoderHigh Voltage Software ADPCM decoderLEGO Racers ALP (.tun & .pcm) demuxerAMQP 0-9-1 protocol (RabbitMQ)avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filtersswitch from AvxSynth to AviSynth+ on LinuxExpanded styling support for 3GPP Timed Text Subtitles (movtext)Support for muxing pcm and pgs in m2tsCunning Developments ADPCM decoderPro Pinball Series Soundbank demuxerpcm_rechunk bitstream filtergradients source video filterMediaFoundation encoder wrapperSimon & Schuster Interactive ADPCM encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 5th, 2019, Bright Lights
  FFmpeg has added a realtime bright flash removal filter to libavfilter.
  
  Note that this filter is not FDA approved, nor are we medical professionals.
  Nor has this filter been tested with anyone who has photosensitive epilepsy.
  FFmpeg and its photosensitivity filter are not making any medical claims.
  
  That said, this is a new video filter that may help photosensitive people
  watch tv, play video games or even be used with a VR headset to block
  out epiletic triggers such as filtered sunlight when they are outside.
  Or you could use it against those annoying white flashes on your tv screen.
  The filter fails on some input, such as the
  Incredibles 2 Screen Slaver
  scene. It is not perfect. If you have other clips that you want this filter to
  work better on, please report them to us on our trac.
  
  We are not professionals. Please use this in your medical studies to
  advance epilepsy research. If you decide to use this in a medical
  setting, or make a hardware hdmi input output realtime tv filter,
  or find another use for this, please let me know.
  This filter was a feature request of mine
  since 2013.
  August 5th, 2019, FFmpeg 4.2 "Ada"FFmpeg 4.2 "Ada", a new
    major release, is now available! Some of the highlights:
  AV1 decoding support through libdav1dchromashift and rgbashift filterstruehd_core bitstream filterlibaribb24 based ARIB STD-B24 caption support (profiles A and C)Support decoding of HEVC 4:4:4 content in nvdec and cuviddecAV1 frame split bitstream filterSupport decoding of HEVC 4:4:4 content in vdpaushowspatial multimedia filtermov muxer writes tracks with unspecified language instead of English by defaultadded support for using clang to compile CUDA kernels
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  November 6th, 2018, FFmpeg 4.1 "al-Khwarizmi"aderivative and aintegral audio filterspal75bars and pal100bars video filter sourcesmbedTLS based TLS supportadeclick and adeclip filterslibtensorflow backend for DNN based filters like srcnnVC1 decoder is now bit-exactAVS2 video decoder via libdavs2Brooktree ProSumer video decoderMatchWare Screen Capture Codec decoderWinCam Motion Video decoderRemotelyAnywhere Screen Capture decoderSupport for AV1 in MP4 and Matroska/WebMAVS2 video encoder via libxavs2Block-Matching 3d (bm3d) denoising filteraudio denoiser as afftdn filterS12M timecode decoding in h264
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 20th, 2018, FFmpeg 4.0 "Wu"FFmpeg 4.0 "Wu", a new
    major release, is now available! Some of the highlights:
  Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streamsExperimental MagicYUV encoderIntel QSV-accelerated MJPEG encodingnative aptX and aptX HD encoder and decoderNVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decodingIntel QSV-accelerated overlay filterVAAPI MJPEG and VP8 decodingAMD AMF H.264 and HEVC encoderssupport LibreSSL (via libtls)Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.hilbert audio filter sourceRemoved the ffserver programRemoved the ffmenc and ffmdec muxer and demuxerVideoToolbox HEVC encoder and hwaccelVAAPI-accelerated ProcAmp (color balance), denoise and sharpness filterscodec2 en/decoding via libcodec2native SBC encoder and decoderhapqa_extract bitstream filterfilter_units bitstream filterAV1 Support through libaomE-AC-3 dependent frames supportbitstream filter for extracting E-AC-3 coreHaivision SRT protocol via libsrt
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 15th, 2017, FFmpeg 3.4 "Cantor"oscilloscope video filterupdate cuvid/nvenc headers to Video Codec SDK 8.0.14scale_cuda CUDA based video scale filterlibrsvg support for svg rasterizationspec compliant VP9 muxing support in MP4sofalizer filter switched to libmysofaGremlin Digital Video demuxer and decodersuperequalizer audio filteradditional frame format support for Interplay MVE moviessupport for decoding through D3D11VA in ffmpegDolby E decoder and SMPTE 337M demuxerunpremultiply video filterraw G.726 muxer and demuxer, left- and right-justifiedNewTek NDI input/output deviceVP9 tile threading supportV4L2 mem2mem HW assisted codecsRockchip MPP hardware decoding
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 13th, 2017, FFmpeg 3.3 "Hilbert"PSD (Photoshop Document) decoderFM Screen Capture decoderDNxHR decoder fixes for HQX and high resolution videosClearVideo decoder (partial)16.8 and 24.0 floating point PCM decoderIntel QSV-accelerated VP8 video decodingDNxHR 444 and HQX encodingQuality improvements for the (M)JPEG encoderVAAPI-accelerated MPEG-2 and VP8 encodingabitscope multimedia filterMPEG-7 Video Signature filteradd internal ebur128 library, remove external libebur128 dependencyIntel QSV video scaling and deinterlacing filtersSample Dump eXchange demuxerMIDI Sample Dump Standard demuxerScenarist Closed Captions demuxer and muxerSupport MOV with multiple sample description tablesPro-MPEG CoP #3-R2 FEC protocolSupport for spherical videosCrystalHD decoder moved to new decode APIconfigure now fails if autodetect-libraries are requested but not found
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 30th, 2016, Results: Summer Of Code 2016.
    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.
  
    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:
  FFv1 (Mentor: Michael Niedermayer)
    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.
  Self test coverage (Mentor: Michael Niedermayer)
    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.
  MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)
    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.
  Tee muxer improvements (Mentor: Marton Balint)
    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.
  TrueHD encoder (Mentor: Rostislav Pehlivanov)
    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.
  Motion interpolation filter (Mentor: Paul B Mahol)
    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.
  
    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!
  September 24th, 2016, SDL1 support dropped.
    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of
    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device
    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output
    devices have been updated to support SDL2.
  August 9th, 2016, FFmpeg 3.1.2 "Laplace"FFmpeg 3.1.2, a new point release from the 3.1 release branch, is now available!
    It fixes several bugs.
  
    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.
  July 10th, 2016, ffserver program being dropped
    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.
    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat
    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has
    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.
    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs
    and to contact us so we may point users to test and contribute to its development.
  July 1st, 2016, FFmpeg 3.1.1 "Laplace"FFmpeg 3.1.1, a new point release from the 3.1 release branch, is now available!
    It mainly deals with a few ABI issues introduced in the previous release.
  
    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to
    upgrade unless they use current git master.
  June 27th, 2016, FFmpeg 3.1 "Laplace"DXVA2-accelerated HEVC Main10 decodingloop video filter and aloop audio filterBob Weaver deinterlacing filterprotocol blacklisting APIVC-2 HQ RTP payload format (draft v1) depacketizer and packetizerVP9 RTP payload format (draft v2) packetizerAudioToolbox audio decodersAudioToolbox audio encoderscoreimage filter (GPU based image filtering on OSX)bitstream filter for extracting DTS corehash and framehash muxersVAAPI-accelerated format conversion and scalinglibnpp/CUDA-accelerated format conversion and scalingDuck TrueMotion 2.0 Real Time decoderWideband Single-bit Data (WSD) demuxerVAAPI-accelerated H.264/HEVC/MJPEG encodingDTS Express (LBR) decoderGeneric OpenMAX IL encoder with support for Raspberry PiIFF ANIM demuxer & decoderDirect Stream Transfer (DST) decoderOpenExr improvements (tile data and B44/B44A support)BitJazz SheerVideo decoderCUDA CUVID H264/HEVC decoder10-bit depth support in native utvideo decoderlibutvideo wrapper removedYUY2 Lossless Codec decoderVideoToolbox H.264 encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  March 16th, 2016, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code open source organization. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.
    Good luck!
  February 15th, 2016, FFmpeg 3.0 "Einstein"
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 30, 2016, Removing support for two external AAC encoders
    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and
    libaacplus in FFmpeg master.
  
    Even before marking our internal AAC encoder as
    stable, it was known that libvo-aacenc
    was of an inferior quality compared to our native one for most samples.
    However, the VisualOn encoder was used extensively by the Android Open
    Source Project, and we would like to have a tested-and-true stable option
    in our code base.
  
    When first committed in 2011, libaacplus filled in the gap of encoding
    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported
    by any of the encoders in FFmpeg at that time.
  
    The circumstances for both have changed. After the work spearheaded by
    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC
    encoder is ready to compete with much more mature encoders. The Fraunhofer
    FDK AAC Codec Library for Android was added in 2012 as the fourth
    supported external AAC encoder, and the one with the best quality and the
    most features supported, including HE-AAC and HE-AACv2.
  
    Therefore, we have decided that it is time to remove libvo-aacenc and
    libaacplus. If you are currently using libvo-aacenc, prepare to transition
    to the native encoder () when updating to the next version
    of FFmpeg. In most cases it is as simple as merely swapping the encoder
    name. If you are currently using libaacplus, start using FDK AAC
    () with an appropriate  option
    to select the exact AAC profile that fits your needs. In both cases, you
    will enjoy an audible quality improvement and as well as fewer licensing
    headaches.
  January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.
    Please see the changelog for each release for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  December 5th, 2015, The native FFmpeg AAC encoder is now stable!
    After seven years the native FFmpeg AAC encoder has had its experimental flag
    removed and declared as ready for general use. The encoder is transparent
    at 128kbps for most samples tested with artifacts only appearing in extreme
    cases. Subjective quality tests put the encoder to be of equal or greater
    quality than most of the other encoders available to the public.
  
    Licensing has always been an issue with encoding AAC audio as most of the
    encoders have had a license making FFmpeg unredistributable if compiled with
    support for them. The fact that there now exists a fully open and truly
    free AAC encoder integrated directly within the project means a lot to those
    who wish to use accepted and widespread standards.
  
    The majority of the work done to bring the encoder up to quality was started
    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.
    Both continued to work on the encoder with the latter joining as a developer
    and mainainer, working on other parts of the project as well. Also, thanks
    to Kamedo2 who does comparisons
    and tests, the original authors and all past and current contributors to the
    encoder. Users are suggested and encouraged to use the encoder and provide
    feedback or breakage reports through our bug tracker.
  
    A big thank you note goes to our newest supporters: MediaHub and Telepoint.
    Both companies have donated a dedicated server with free of charge internet
    connectivity. Here is a little bit about them in their own words:
  Telepoint is the biggest
        carrier-neutral data center in Bulgaria. Located in the heart of Sofia
        on a cross-road of many Bulgarian and International networks, the
        facility is a fully featured Tier 3 data center that provides flexible
        customer-oriented colocation solutions (ranging from a server to a
        private collocation hall) and a high level of security.
      
        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which
        uses FFmpeg heavily since it started operating a year ago. "Donating
        to help keep FFmpeg online is our way of giving back to the community"
        .
      
    Thanks Telepoint and MediaHub for their support!
  September 29th, 2015, GSoC 2015 results
    FFmpeg participated to the latest edition of
    the Google
    Summer of Code Project. FFmpeg got a total of 8 assigned
    projects, and 7 of them were successful.
  We want to thank Google, the
    participating students, and especially the mentors who joined this
    effort. We're looking forward to participating in the next GSoC
    edition!
  
    Below you can find a brief description of the final outcome of
    each single project.
  Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George
    Stephan Holljes's project for this session of Google Summer of Code was to
    implement basic HTTP server features for libavformat, to complement the
    already present HTTP client and RTMP and RTSP server code.
  
    The first part of the project was to make the HTTP code capable of accepting
    a single client; it was completed partly during the qualification period and
    partly during the first week of the summer. Thanks to this work, it is now
    possible to make a simple HTTP stream using the following commands:
      ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  
    The next part of the project was to extend the code to be able to accept
    several clients, simultaneously or consecutively. Since libavformat did not
    have an API for that kind of task, it was necessary to design one. This part
    was mostly completed before the midterm and applied shortly afterwards.
    Since the ffmpeg command-line tool is not ready to serve several clients,
    the test ground for that new API is an example program serving hard-coded
    content.
  
    The last and most ambitious part of the project was to update ffserver to
    make use of the new API. It would prove that the API is usable to implement
    real HTTP servers, and expose the points where more control was needed. By
    the end of the summer, a first working patch series was undergoing code
    review.
  Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek
    Mariusz finished an API prepared by the FFmpeg community and implemented
    Samba directory listing as qualification task.
  
    During the program he extended the API with the possibility to
    remove and rename files on remote servers. He completed the
    implementation of these features for file, Samba, SFTP, and FTP
    protocols.
  
    At the end of the program, Mariusz provided a sketch of an
    implementation for HTTP directory listening.
  Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack
    Mate was working on directshow input from digital video sources. He
    got working input from ATSC input sources, with specifiable tuner.
  
    The code has not been committed, but a patch of it was sent to the
    ffmpeg-devel mailing list for future use.
  
    The mentor plans on cleaning it up and committing it, at least for the
    ATSC side of things. Mate and the mentor are still working trying to
    finally figure out how to get DVB working.
  Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale
    Niklesh's project was to expand our support for 3GPP Timed Text
    subtitles. This is the native subtitle format for mp4 containers, and
    is interesting because it's usually the only subtitle format supported
    by the stock playback applications on iOS and Android devices.
  
    ffmpeg already had basic support for these subtitles which ignored all
    formatting information - it just provided basic plain-text support.
  
    Niklesh did work to add support on both the encode and decode side for
    text formatting capabilities, such as font size/colour and effects like
    bold/italics, highlighting, etc.
  
    The main challenge here is that Timed Text handles formatting in a very
    different way from most common subtitle formats. It uses a binary
    encoding (based on mp4 boxes, naturally) and stores information
    separately from the text itself. This requires additional work to track
    which parts of the text formatting applies to, and explicitly dealing
    with overlapping formatting (which other formats support but Timed
    Text does not) so it requires breaking the overlapping sections into
    separate non-overlapping ones with different formatting.
  
    Finally, Niklesh had to be careful about not trusting any size
    information in the subtitles - and that's no joke: the now infamous
    Android stagefright bug was in code for parsing Timed Text subtitles.
  
    All of Niklesh's work is committed and was released in ffmpeg 2.8.
  libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla
    Pedro Arthur has modularized the vertical and horizontal scalers.
    To do this he designed and implemented a generic filter framework
    and moved the existing scaler code into it. These changes now allow
    easily adding removing, splitting or merging processing steps.
    The implementation was benchmarked and several alternatives were
    tried to avoid speed loss.
  
    He also added gamma corrected scaling support.
    An example to use gamma corrected scaling would be:
      ffmpeg -i input -vf scale=512:384:gamma=1 output
  
    Pedro has done impressive work considering the short time available,
    and he is a FFmpeg committer now. He continues to contribute to
    FFmpeg, and has fixed some bugs in libswscale after GSoC has
    ended.
  AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire
    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main
    prediction on the native AAC encoder. Of all those extensions, only
    TNS was left in a less-than-usable state, but the implementation has
    been pushed (disabled) anyway since it's a good basis for further
    improvements.
  
    PNS replaces noisy bands with a single scalefactor representing the
    energy of that band, gaining in coding efficiency considerably, and
    the quality improvements on low bitrates are impressive for such a
    simple feature.
  
    TNS still needs some polishing, but has the potential to reduce coding
    artifacts by applying noise shaping in the temporal domain (something
    that is a source of annoying, notable distortion on low-entropy
    bands).
  
    Intensity Stereo coding (I/S) can double coding efficiency by
    exploiting strong correlation between stereo channels, most effective
    on pop-style tracks that employ panned mixing. The technique is not as
    effective on classic X-Y recordings though.
  
    Finally, main prediction improves coding efficiency by exploiting
    correlation among successive frames. While the gains have not been
    huge at this point, Rostislav has remained active even after the GSoC,
    and is polishing both TNS and main prediction, as well as looking for
    further improvements to make.
  
    In the process, the MIPS port of the encoder was broken a few times,
    something he's also working to fix.
  Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol
    Donny Yang implemented basic keyframe only APNG encoder as the
    qualification task. Later he wrote interframe compression via
    various blend modes. The current implementation tries all blend
    modes and picks one which takes the smallest amount of memory.
  
    Special care was taken to make sure that the decoder plays
    correctly all files found in the wild and that the encoder
    produces files that can be played in browsers that support APNG.
  
    During his work he was tasked to fix any encountered bug in the
    decoder due to the fact that it doesn't match APNG
    specifications. Thanks to this work, a long standing bug in the
    PNG decoder has been fixed.
  
    For latter work he plans to continue working on the encoder,
    making it possible to select which blend modes will be used in the
    encoding process. This could speed up encoding of APNG files.
  September 9th, 2015, FFmpeg 2.8
    We published release  as new major version.
    It contains all features and bug fixes of the git master branch from September 8th. Please see
    the 
    for a list of the most important changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use current git master.
  August 1st, 2015, A message from the FFmpeg project
    Dear multimedia community,
  
    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has
    come by surprise. He has worked tirelessly on the FFmpeg project for many
    years and we must thank him for the work that he has done. We hope that in
    the future he will continue to contribute to the project. In the coming
    weeks, the FFmpeg project will be managed by the active contributors.
  
    The last four years have not been easy for our multimedia community - both
    contributors and users. We should now look to the future, try to find
    solutions to these issues, and to have reconciliation between the forks,
    which have split the community for so long.
  
    Unfortunately, much of the disagreement has taken place in inappropriate
    venues so far, which has made finding common ground and solutions
    difficult. We aim to discuss this in our communities online over the coming
    weeks, and in person at the VideoLAN Developer
    Days in Paris in September: a neutral venue for the entire open source
    multimedia community.
  July 4th, 2015, FFmpeg needs a new host We have received more than 7 offers for hosting and servers, thanks a lot to everyone!
    After graciously hosting our projects (FFmpeg, MPlayer
    and rtmpdump) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.
  
    If you want to host an open source project, please let us know, either on ffmpeg-devel
    mailing list or irc.freenode.net #ffmpeg-devel.
  
    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, trac, samples repo, svn, etc.
  March 16, 2015, FFmpeg 2.6.1
    We have made a new major release ()
    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  March 4, 2015, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code Project. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks. Registration
    at Google for students will open March 16th. Good luck!
  March 1, 2015, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage
    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.
  
    More information can be found here
    We demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes. If you have media files that cannot be
    processed correctly with FFmpeg, be sure to have a sample with you
    so we can have a look!
    For the first time in our CLT history, there will be an !
    You can read the details here.
    The workshop is targeted at FFmpeg beginners. First the basics of
    multimedia will be covered. Thereafter you will learn how to use
    that knowledge and the FFmpeg CLI tools to analyse and process media
    files. The workshop is in German language only and prior registration
    is necessary. The workshop will be on Saturday starting at 10 o'clock.
  
    We are looking forward to meet you (again)!
  December 5, 2014, FFmpeg 2.5
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 4th December.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  October 10, 2014, FFmpeg is in Debian unstable again
    We wanted you to know there are
    
    FFmpeg packages in Debian unstable again. A big thank-you
    to Andreas Cadhalpun and all the people that made it possible. It has been anything but simple.
  
    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't
    migrate to Debian testing to be in the upcoming release codenamed jessie.
    Read the argumentation over at Debian.However things will come out in the end, we hope for your continued remarkable support!October 8, 2014, FFmpeg secured a place in OPW!
    Thanks to a generous 6K USD donation by Samsung (Open Source Group),
    FFmpeg will be welcoming at least 1 "Outreach Program for Women" intern
    to work with our community for an initial period starting December 2014
    (through March 2015).
  
    We all know FFmpeg is used by the industry, but even while there are
    countless products building on our code, it is not at all common for
    companies to step up and help us out when needed. So a big thank-you
    to Samsung and the OPW program committee!
  
    If you are thinking on participating in OPW as an intern, please take
    a look at our OPW wiki page
    for some initial guidelines. The page is still a work in progress, but
    there should be enough information there to get you started. If you, on
    the other hand, are thinking on sponsoring work on FFmpeg through the
    OPW program, please get in touch with us at opw@ffmpeg.org. With your
    help, we might be able to secure some extra intern spots for this round!
  September 15, 2014, FFmpeg 2.4
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 14th September.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.
    Please see the changelog for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 29, 2014, Help us out securing our spot in OPW
    Following our previous post regarding our participation on this year's
    OPW (Outreach Program for Women), we are now reaching out to our users
    (both individuals and companies) to help us gather the needed money to
    secure our spot in the program.
    We need to put together 6K USD as a minimum but securing more funds would
    help us towards getting more than one intern.
    You can donate by credit card using
    
    Click&Pledge and selecting the "OPW" option. If you would like to
    donate by money transfer or by check, please get in touch by
    e-mail and we will get back to you
    with instructions.Thanks!
  July 20, 2014, New website
    The FFmpeg project is proud to announce a brand new version of the website
    made by db0. While this was initially motivated
    by the need for a larger menu, the whole website ended up being redesigned,
    and most pages got reworked to ease navigation. We hope you'll enjoy
    browsing it.
  July 17, 2014, FFmpeg 2.3
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 16th July.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 3, 2014, FFmpeg and the Outreach Program For Women
    FFmpeg has started the process to become an OPW includer organization for the
    next round of the program, with internships starting December 9. The
    OPW aims to "Help women (cis and trans)
    and genderqueer to get involved in free and open source software". Part of the
    process requires securing funds to support at least one internship (6K USD), so
    if you were holding on your donation to FFmpeg, this is a great chance for you
    to come forward, get in touch and help both the project and a great initiative!
  
    We have set up an email address you can use
    to contact us about donations and general inquires regarding our participation
    in the program. Hope to hear from you soon!
  June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  
    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will
    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a
    different location closer to the city center.
  
    We will have a shared booth with XBMC and VideoLAN.
    
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    More information about LinuxTag can be found here
    We are looking forward to see you in Berlin!
  April 18, 2014, OpenSSL Heartbeat bug
    Our server hosting the Trac issue tracker was vulnerable to the attack
    against OpenSSL known as "heartbleed". The OpenSSL software library
    was updated on 7th of April, shortly after the vulnerability was publicly
    disclosed. We have changed the private keys (and certificates) for all
    FFmpeg servers. The details were sent to the mailing lists by
    Alexander Strasser, who is part of the project server team. Here is a
    link to the user mailing list
    archive
    .
  
    We encourage you to read up on
    "OpenSSL heartbleed".
    It is possible that login data for the issue tracker was exposed to
      people exploiting this security hole. You might want to change your password
      in the tracker and everywhere else you used that same password.April 11, 2014, FFmpeg 2.2.1
    We have made a new point releases ().
    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as
    several other fixes.
    See the git log for details.
  March 24, 2014, FFmpeg 2.2
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 1st March.
    A partial list of new stuff is below:
      - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side & metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI "junk" block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  February 3, 2014, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'
    in Chemnitz, Germany. The event will take place on 15th and 16th of March.
  
    More information can be found here
    We invite you to visit us at our booth located in the Linux-Live area!
    There we will demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes.
  
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    We are looking forward to meet you (again)!
  February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach
    The server on which FFmpeg and MPlayer Trac issue trackers were
    installed was compromised. The affected server was taken offline
    and has been replaced and all software reinstalled.
    FFmpeg Git, releases, FATE, web and mailinglists are on other servers
    and were not affected. We believe that the original compromise happened
    to a server, unrelated to FFmpeg and MPlayer, several months ago.
    That server was used as a source to clone the VM that we recently moved
    Trac to. It is not known if anyone used the backdoor that was found.
  
    We recommend all users to change their passwords.
    Especially users who use a password on Trac that they also use
      elsewhere, should change that password at least elsewhere.November 12, 2013, FFmpeg RFP in Debian
    Since the splitting of Libav the Debian/Ubuntu maintainers have followed
    the Libav fork. Many people have requested the packaging of ffmpeg in
    Debian, as it is more feature-complete and in many cases less buggy.
  Rogério Brito, a Debian developer,
    has proposed a Request For Package (RFP) in the Debian bug tracking
    system.
  
    Please let the Debian and Ubuntu developers know that you support packaging
    of the real FFmpeg! See Debian ticket #729203
    for more details.
  October 28, 2013, FFmpeg 2.1
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 28th October.
    A partial list of new stuff is below:
      - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  ]]></content:encoded></item><item><title>What 42 Internet People Think of Silksong, the New Game 8.5 Years in the Making</title><link>https://hackernoon.com/what-42-internet-people-think-of-silksong-the-new-game-85-years-in-the-making?source=rss</link><author>Games Harden</author><category>tech</category><pubDate>Fri, 22 Aug 2025 15:22:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Silksong took 8.5 years to arrive, and players are finally weighing in. Some call it a masterpiece, others find it brutally challenging, but across 42 different perspectives, one thing is clear: this long-awaited sequel is unforgettable.]]></content:encoded></item><item><title>Coinbase Reverses Remote-First Policy After North Korean Infiltration Attempts</title><link>https://slashdot.org/story/25/08/22/1515238/coinbase-reverses-remote-first-policy-after-north-korean-infiltration-attempts?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 15:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Remote work policies designed to attract top talent are becoming security vulnerabilities as state-sponsored hackers seek employment at cryptocurrency firms. Coinbase has implemented mandatory in-person orientation and US citizenship requirements for sensitive roles after detecting North Korean IT workers attempting to infiltrate the company through remote positions. 

CEO Brian Armstrong revealed on Stripe cofounder John Collison's podcast that the exchange now requires fingerprinting and live video interviews after discovering coordinated efforts involving US-based facilitators who reship laptops and attend virtual interviews on behalf of foreign operatives.]]></content:encoded></item><item><title>Is This AI’s Linux Moment? Inside 0G’s Labs Push for an Open, Verifiable Stack for AI</title><link>https://hackernoon.com/is-this-ais-linux-moment-inside-0gs-labs-push-for-an-open-verifiable-stack-for-ai?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Fri, 22 Aug 2025 15:03:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Artificial intelligence is advancing at breakneck speed, yet most of the infrastructure powering AI remains highly centralized, controlled by a few large companies. At the same time, Web3 continues to push the boundaries of what decentralized systems can achieve. Where these two worlds intersect is where 0G Labs positions itself, building what it calls the “Solana for AI,” a decentralized operating system designed for large-scale, low-latency AI workloads.\
In this interview, I speak with Michael Heinrich, CEO of 0G Labs, about why decentralization matters for AI, how Web3 infrastructure can solve bottlenecks in scaling machine learning, and what the future of AI-native blockchains looks like.\
 Hi Michael, welcome to our “Behind the Startup” series. Please tell us about your journey and what led you to build 0G Labs?\
 My journey's been something of a wild ride, all the way back to my earliest memories. I was born in East Berlin just before the Berlin Wall came down. I faintly remember the night it fell and my family emigrated to the US when I was young, settling in California. That early experience of crossing borders and adapting to new worlds and languages likely planted the seeds for my fixation on breaking down barriers, both literally and figuratively.\
I studied economics and computer science at UC Berkley, but to be honest, while the learning came naturally, I was restless and agitating to find some deeper purpose. It would take a few more years before that call was answered with the founding of 0G Labs. In the interim, I left college and cut my teeth in the corporate world at places like Microsoft, Bain & Company, and Bridgewater Associates, where I learned about scaling systems and building high-performance teams.\
Then came my first taste of entrepreneurship when I founded Garten, a corporate wellness company that delivered healthier cafeteria options for businesses, which grew to serve hundreds of employees across the US. It was my first real startup, and it taught me resilience and the need to engineer shock-resistant systems.\
Fast forward to grad school at Stanford, where I was introduced to Fan Long and Ming Wu by my classmate Thomas in 2023. Thomas, who is also one of the co-founders of Zero Gravity Labs, brought us together at a time when AI was exploding but locked in centralized silos controlled by Big Tech. We all saw blockchain as the key to democratizing it, making AI a public good that's accessible to everyone. That’s what led us to found 0G Labs in 2023. We're building DeAIOS, a decentralized AI operating system, because I believe the future of innovation isn’t in walled gardens; it’s in open, collaborative ecosystems. At 0G Labs, I can honestly say I’ve found my calling, and every day I wake up excited to get out of bed and keep building.\
 0G has been described as the “Solana for AI.” Can you break down what you mean by that, and why existing blockchains are not suited for AI-scale workloads?\
 This comparison highlights the speed, scalability, and low latency we're bringing to decentralized AI compared to incumbent solutions. Solana revolutionized smart contract blockchains by enabling thousands of transactions per second at minimal cost without sacrificing decentralization, something that was streets ahead of the then-market leader Ethereum.\
We're doing something similar but tailored for AI: creating a modular Layer 1 blockchain that supports massive, real-time AI workloads for inference, training, and data processing. Existing blockchains fall short for AI because they're optimized for financial transactions, not the data-heavy demands of machine learning. AI requires enormous bandwidth for data availability, we’re talking petabytes of datasets streaming in low-latency environments and most chains run into bottlenecks due to storage costs or limited composability.\
We solve this with components like 0G DA (Data Availability), which provides infinitely scalable, high-speed data layers, and 0G Storage, offering ultra-low-cost, verifiable permanence for structured and unstructured data. For AI builders, this is like upgrading from a biplane to a jet, way faster, way more efficient, and also a lot more fun since now the only limitations are your imagination.\
 The AI industry today is dominated by centralized players. Why do you believe decentralization is not just a philosophical stance but a technical and economic necessity for AI?\
 The likes of OpenAI, Google, and AWS control the AI stack today, and while this provides convenience in terms of guaranteed access and uptime, this setup also creates real risks. Even if we set aside the philosophical benefits of decentralization, from a purely technical perspective, it’s obvious that having a triumvirate of Big Tech players calling the shots is suboptimal. Centralization leads to single points of failure: data silos that stifle innovation, vulnerability to censorship or outages, and biased models trained on proprietary datasets. Economically, it also prices out smaller players; GPUs and compute are expensive enough, but locked ecosystems mean you're paying a premium for access.\
Decentralized solutions have a major role to play in ameliorating these issues. At 0G, we're building tools like our Compute Network for trustless AI inference, secured by cryptography, so that anyone can contribute resources and earn rewards. This creates a global, permissionless marketplace where costs drop dramatically, our storage is orders of magnitude cheaper than AWS S3 and a culture of innovation is fostered through composability. Imagine AI agents collaborating onchain without intermediaries or verifiable models that prevent hallucinations.\
These sorts of open-source and interoperable solutions are necessary because AI's impact on society is too big to be gatekept; decentralizing it ensures equitable access and reduces monopolies. I've seen centralized systems crumble firsthand and will remain convinced till my last breath that decentralization isn't optional, it's essential.\
 A lot of people talk about compute and GPUs as the bottleneck for AI. From your perspective, what are the real constraints, data, bandwidth, economics and how does 0G tackle them differently?\
 Great question. Compute gets all the headlines because GPUs are flashy and scarce, but in my view, the real chokepoints are data availability, bandwidth, and economics and by “economics” I mean the price you pay to access these resources. NVIDIA's cranking out chips faster than ever, but where's the infrastructure to feed them massive, verifiable datasets at scale? Data is fragmented and centralized, bandwidth limits low-latency streaming for real-time AI, and as for economics, well, where do we start?\
Cloud providers charge over the odds, often locking clients into rigid subscription-based models, making AI inaccessible for solo devs or startups that have yet to be blessed with VC cash. Even then, early-stage funding is only a bandaid, cos you need to start generating revenue sooner rather than later or you’ll eventually blow your budget on GPUs with nothing to show for it other than some cool renders.\
At 0G, we tackle these resource issues head-on. Our 0G DA layer provides high-speed, infinitely scalable data availability – we’ve been hitting 11,000 TPS per shard on testnet, with over 650 million transactions already processed. For bandwidth, our modular chain uses multi-consensus validation to handle AI workloads without congestion. Economically, we slash costs: 0G Storage offers decentralized, AI-optimized storage at pennies compared to centralized options, with cryptographic permanence to ensure data integrity.\
Unlike traditional setups, we're creating an open economy where users run alignment nodes or provide compute for rewards. It's like turning AI infrastructure into a shared resource pool, making it more efficient and inclusive.\
 You’ve mentioned the idea of a “decentralized operating system for AI.” What does that look like in practice, and how does it compare to traditional cloud infrastructure models in terms of pricing and benefits for end users?\
 The “decentralized operating system for AI,” or DeAIOS, is essentially a full-stack platform where AI runs natively on blockchain. It’s scalable, composable, and trustless. In practice, it means developers can deploy AI models or agents by leveraging components such as the Service Marketplace for trading models, or Alignment Nodes to ensure ethical, efficient performance. Running inference on a global network of nodes, with data pulled seamlessly from 0G Storage and verified onchain is a much cheaper and more transparent way of doing things versus relying on centralized servers and Big Tech’s inflexible pricing.\
Compared to traditional clouds like AWS or Azure, the differences are stark. Let’s start with pricing: centralized cloud provider costs can quickly stack up, with intermediaries adding on their fees for storage and compute along the way. We cut that by decentralizing supply, so end users pay near-cost. As for the other benefits this model bestows, permissionless access means no vendor lock-in; censorship resistance protects against shutdowns; and composability lets apps interoperate fluidly, sparking innovations like onchain AI gaming.\
Finally, verifiability ensures models are auditable, reducing risks like bias. It's relatable: think of it as open-source Linux versus proprietary Windows but for AI. Users get faster iteration and ownership in the ecosystem. We've seen this in our testnet with 22 million active accounts – it's already empowering a new wave of builders.\
 Many skeptics argue that Web3 adds unnecessary complexity to AI. How do you respond to the criticism that decentralization slows innovation instead of accelerating it?\
 I hear this a lot, and I understand that Web3 can feel like adding gears to a machine that's already ticking over. But from my experience of building in both worlds, decentralization doesn't slow innovation – it turbocharges it by solving problems that centralization can't. Yes, it adds a degree of complexity but it's purposeful: cryptographic proofs add verifiability to AI outputs, preventing fakes or manipulations that plague centralized models. And blockchain's composability means devs can mix and match components – like plugging an AI oracle from Ora into 0G’s chain – without starting from scratch.\
Critics overlook how Web3 accelerates parallel development: thousands of nodes worldwide contribute compute, data, and ideas, outpacing any single company's R&D. At 0G, we've raised $35M and hit milestones like 8,000 validators because this model draws global talent. It hasn't slowed us but has rather propelled us. Ultimately, this all comes down to trade-offs: introducing a little short-term complexity in return for long-term resilience and inclusivity. Innovation thrives in open systems and so, to my mind, Web3 isn't a hurdle; it's the accelerator pedal for AI's next phase.\
 Finally, what advice would you give to founders trying to build companies at the cutting edge of both AI and blockchain?\
 To founders at this intersection: first, don't die, protect your mission at all costs. My journey has taught me that resilience wins, whether it's overcoming personal setbacks or navigating market downturns, so keep iterating and surround yourself with a strong team that will always have your back. Second, focus on real problems and describe the solutions in real terms. It’s fine to occasionally dispense big picture stuff, onboarding the next billion users, yadda yadda but as a founder, your job is really to explain your tech in ways that lay people can understand. Even if you’re marketing to AI builders, who understand this stuff, think about their end users, because they’re the people this tech is ultimately gonna trickle down to.\
Finally, never be afraid to change your mind. It’s okay to be wrong sometimes. And it’s more than okay to change your perspective on something when new evidence comes to light, which, in an industry evolving as fast as AI, is a given. Be confident in your convictions, yet honest enough to admit that you will always be a student and will always be learning. Your team and your shareholders will respect you a lot more if you do. Move fast, learn faster, and try not to break too much stuff along the way.\
Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Building Coding Agents: Design Decisions, Prompting Tricks, GUI Anti-patterns</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Building-Coding-Agents-Design-Decisions--Prompting-Tricks--GUI-Anti-patterns-e37629o</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/107202296/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-7-22%2F406096692-44100-2-c633845e6f06a.mp3" length="" type=""/><pubDate>Fri, 22 Aug 2025 15:00:10 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>X pulls the ability to like and follow from its developer API’s free tier</title><link>https://techcrunch.com/2025/08/22/x-pulls-the-ability-to-like-and-follow-from-its-developer-apis-free-tier/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:57:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[X is making it harder for spammers with these API changes.]]></content:encoded></item><item><title>Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</title><link>https://arxiv.org/abs/2508.12631</link><author>omarsar</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:43:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenAI Is Challenging Google - While Using Its Search Data</title><link>https://tech.slashdot.org/story/25/08/22/1442208/openai-is-challenging-google---while-using-its-search-data?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: As it tries to unseat Google, OpenAI is relying on search data from an unlikely source: Google. OpenAI has been using Google search results scraped from the web to help power ChatGPT responses, according to two people with knowledge of it. 

The Google search data helps answer ChatGPT queries on current events, such as news, sports and equity markets, one of the people said. OpenAI is getting the data from SerpApi, an eight-year-old web-scraping firm, which listed OpenAI as a customer on its website as recently as May last year. It removed the reference for reasons that couldn't be learned.]]></content:encoded></item><item><title>A guide to Gen AI / LLM vibecoding for expert programmers</title><link>https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</link><author>ChrisRackauckas</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:37:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I get it, you’re too good to vibe code. You’re a senior developer who has been doing this for 20 years and knows the system like the back of your hand. Or maybe you’re the star individual contributor who is the only person who can ever figure out how to solve the hard problems. Or maybe you’re the professor who created the entire subject of the algorithms you’re implementing. I don’t know you, but I do know that you think you’re too good to vibe code. And guess what, you’re absolutely and totally wrong.Facetious? Maybe… but I will go even further.No, you’re not too good to vibe code. In fact, you’re the only person who should be vibe coding.But I started picking up this “vibe coding” about a month ago and I found out that it can be a really powerful tool, in the right circumstances and in the right workflow. For the record, I now have about 32 Claude agents continuously running in tmux windows that I can ssh to, so all day long I can just check via laptop or phone and keep plugging along. This was completely unheard of a month ago, but it’s here.This is the expert’s guide to vibe coding for those who are scoffing at those kids who don’t know what they are doing, but also want to start doing it correctly.A Mental Model for LLM Agents: Your Sophomore Year Student/InternDrop the hype, I’m not here to sell you a ChatGPT so I’m not going to tell you it’s PhD level when it 100% absolutely clearly isn’t to anyone who has ever met a PhD in their life. But it is something, what is it?Think about an LLM agent as a dedicated intern, or a student who is around the proficiency of a sophomore in college. They know the basics of what programming looks like, they can copy other ideas and architectures, they know how to do things like run unit tests, and they know how to Google things. They have had their basic programming course, and probably have done a deep dive into some random subject as a higher level course, but if you quiz them on the topic enough you’ll learn they haven’t actually learned it deeply. The kid seems smart enough, you’d give them a shot.If this was a person who showed up to your office looking for work, what would you do with them? Generally you would do one of two things. First, you could sandbox their work. Now the reason you sandbox the work of a new student or intern is rather simple: it’s because you don’t know a new subject/tool well, you want to give it a try, and ehh why not let’s see what happens. If you took the sandbox route, you probably aren’t caring about the code (it’s likely to be unmaintainable and bit rot anyways if it’s not in your core repos), it’s about getting the artifact. You vibe code a bit, “that looks cool!”, throw it into a demo / LinkedIn post, and then move on. That’s the simple vibe coding you may have already tried and thought “that’s not useful enough”. It works, but that’s not what we’re here for, so no need to mention that more in the blog.The second path, in the complete opposite direction, you would integrate the student/intern into a project you know well because it makes it very easy for you to review their work: you can give clear feedback because you’ve already made the first 10 mistakes they will make, you already know how to tell them what is next, and you have the first 6 months of the project planned out for them so it’s low maintenance. This is how you would train most people that you want to stay long term, right? In the same way, this is the path to take with the LLM agents.So let’s walk through this process step by step.Major Point: Vibe coding turns everyone into a team lead, but not everyone should be a team leadLeading a team of programmers is hard! It takes time, skill, and patience. I think everyone when they are a kid thinks “I don’t want to be the worker, I want to be the boss and I just sit in my chair and tell people what to do and boom it all gets done!”. But after your second group project in college, you pretty quickly realize that if you lead a team wrong, you instead just end up doing all of the work yourself while having the expectation of 4 people. Now there’s a few reasons for this. One issue with trying to establish team programming is that you may just not know the subject well enough. If it takes you a while to understand the subject, what someone is trying to do, and what their code is about, then it’s just not worth the time to manage someone else. You need to be at a point where you can very quickly see the code, understand what’s going on, and say “I can’t merge this until you have tests for X and Y, and also show me a plot of Z so I know how it all relates”. If you cannot instantly see that kind of feedback, then you probably aren’t experienced enough to lead. You need to write a few million lines of code before it becomes automatic where you just look at code and say “don’t do that, that’ll be a performance bottleneck”, but you need that quickness to the code review in order for vibe coding to work. But also, let me say this bluntly:If you are an individual contributor who usually does not like to train interns because you find that they take more time than they are helpful, then don’t vibe codeSome people tend to do better at working by directing. Other really smart people just can’t seem to get good work out of other people. It’s not an indictment, Silicon Valley created the “individual contributor” role for a reason. If you are one of those people, then vibe coding may not be for you as you will likely grow frustrated with the agents even quicker than you would a human (they somehow retain less information than even the worst intern, at least they remember your favorite lunch order).So go in with this mindset: I will have to have meetings with the agents, I will need to plan and give them tasks, and I will need to review the code. If I find this stuff to be slower than coding myself, then just stop right now. But if you do well with a team, then go on. How do we now make this team effective?What is the workflow of vibe coding correctly?If someone shows you Claude Code and you ask it to try and solve the problem that you were just working on (obviously a hard problem, because if it ends up on your desk that means someone else failed to solve it), you just poke and laugh at Claude when it fails miserably. But you would have never done that with a new intern or student (hopefully), so why do this? Again, you know how smart it actually is, cut the hype, and treat it the same way. This immediately leads to a few workflow principles:The workflow of vibe coding is the same as managing scrums or helping a student through a research thesisYou have a problem, you give it to the agent, you review the results, and then you give it feedback. This is exactly how you would manage a scrum team or help a student through their thesis. You don’t just give them the problem and expect them to solve it, you give them the problem, they come back with a solution, and then you tell them what to do next.You probably already get most of your work done this way if you’re senior enough. Every professor has more students coding than themselves, and every senior developer has a total amount of code created by their team that is far greater than their own. Just think of Claude as your pack of newbies that just started. Now if you’re thinking “but it can be difficult to manage a bunch of newbies”… yeah, that means you’re actually senior enough to understand how to do this right. It’s fairly easy to send a new intern or student off on a project, and if their pay/grade depends on it getting done they will give you something back. Whether it’s any good depends on how well you chunked up the work for them and gave them an appropriate task. But one key thing is, if you had to do a meeting every 10 minutes it would drive you crazy, so don’t. Set up say 12-32 agents running on different processes, preferably sandboxed on some other compute resource (sandboxed so they can’t break the machine, but also so they can have a Github authentication that does not have core read/write privileges. This way you can tell it to have “dangerously unsafe permissions” and the worst that happens is it segfaults its own docker container and never opens a PR). Give it a full command:“try solving (an easy issue in this open source repository). Create a PR with the solution, and after an hour check the continuous integration to see if tests are passing. If tests are not passing, assess what the issue is, and if it is a quick fix make a commit to handle it, otherwise report what the core difficulty of the problem is”Don’t spend too much time setting up the calls, just pull from lists you already have and let it find “whatever is easy”Make it clear, make it easy, make it know the steps, and let it just keep cycling for a bit.How to review vibe coding: immediately throw out anything badIf you saw a student was cheating and just copy-pasted from StackOverflow but couldn’t explain what it did, you’d throw it out and tell them to try again. If your new intern didn’t reuse all of the solid code your team had written and instead rewrote some low level detail in a buggy and unmaintainable way, you’d throw it out and tell them to try again. If they wrote a function that was 500 lines long and did 10 different things, you’d throw it out and tell them to try again. You wouldn’t waste your time trying to fix it, you’d just tell them to try again.Again, treat the LLMs the same way. I see a lot of people following the mindset they see the vibe coding YouTubers making their silly games. “ChatGPT, try harder! Fix for me!”. You want to know a secret? That stuff is worse than worthless. The problem is that these LLMs are made to please you, so if you tell them to try harder, they will either start hallucinating or just start changing your tests. Don’t even give it a try. The moment you see it go off the rails, just throw it out. That problem is too hard for Claude, it’s for you now.Send a bunch of commands at 9am. At noon, check on them. You might have 10 done. 8 of them probably went off the rails, whatever, fire them. Hey two PRs worked, whoopee! Fire 10 more, come back at 3. 20 done, 4 successes and 16 failures. Fire a few more off, maybe a few clean up ones to look for missing docstrings or dig around to see if any performance regressions were introduced. At 6, see the other 4 successes and cut the other jobs. Vibe coding is useful only if you have enough problems that you’re happy that some subset being solved, not caring what in that subset is solved.10 PRs were merged, plus whatever you were working on that day (yes, because you didn’t focus on this for most of your day!). You might think, that’s like 10/40 = 25% success rate, that’s not good. But you know what? Those were free. You just got a lot of extra stuff done that you wouldn’t have otherwise. The success rate is just a matter of how much these things give value for their cost. That’s for Sam Altman to worry about. But if you have a subscription to these LLMs, just keep burning through the tokens who cares. Don’t worry about success rate, just go for total successes.Where to apply vibe coding: code you know very wellSo this leads to a very counter-intuitive fact that may come out of left field, but I’m serious. Everyone’s first inclination is to throw it on some project they haven’t actually contributed to and get banned (okay, maybe it just looks like that to open source maintainers). But the real issue is that, the majority of your time will be spent doing code review. If you do this on code you don’t know well, you will have to spend a lot of time trying to understand the code and at that point, why not just write the code yourself?This is where most people seem to just stop and drop the idea of vibe coding all together. But instead… what about applying it to the code base you’re on? No, not on the hard problems you’re thinking about, but all of those little side problems? The small refactor you put off for the last 6 months? What about bisecting the Git commits to find the exact cause of the performance regression that showed up on master a week ago? Or you created a version specialized for Windows and Mac but left a “todo” over the Linux section because it’s easy but would be 4 hours of monotonous work? All of those things, if someone showed up with the code, you could review it in about 5 minutes and know whether it’s right or wrong. Give the agents that stuff!Vibe coding is not useful if you need it to solve a particular problem. You still do the hard stuffIn just the same way, the best place to put trainees is in the project that you already know well because that makes it easy to review their work. It’s the “I don’t have time for you, so try this easy task” approach. You know the code, you know the problem, and you can give them a task that is easy enough that they can do it without too much help. This is the same principle here.Some Examples of Vibe Coding PRsExample 1: The Simple Success StoryHere’s a quick and simple PR, the kind that is perfect here. If you don’t know performance Julia handling or trim, basically it’s a new feature in Julia v1.12 where Julia can now build small lean binaries. In order to do that, you need to make sure functions fully specialize, which they don’t by default as that would create a lot of extra compilation in many circumstances, but for higher order numerical solvers that is the behavior we want. So I told it to go specialize all instances of the function in the package, and I could check the PR fairly quickly and see it stuck to the goals and did it. This is then going to be followed up with new tooling that will perform static checks of trimming compatibility (still being worked out), but with just those backwards compatible minor changes things seem to work in the beta, so merge now and add those tests when we have a good system for it.1 minute to write the query, come back later and 1 minute to review.80% of coding is grunt work. Simple stuff. Throw it at the LLM and smile if it solves it, or just groan and finish it if it doesn’tThis is exactly the kind of small targeted change these are geared towards. Most of the PRs aim to be like this. Actually, one of my favorite prompts is “Look through the XXXXXXX repository, find the easiest issue, and make a PR that solves it.” Here’s an example of a PR from that. Yay it was a hit! Merged. It should make a minimal number of changes, be small and easy to review, otherwise just dump it, that means it probably went off the rails. There’s just a lot of these little “this API would be easier if you added a conversion from float to Int” (which sometimes you go “oh, no that’s a bad idea, close PR and issue!). Even if you work on hard stuff, a huge chunk of your work isn’t hard stuff. There’s a lot of simple janitorial work you have to do on your code all of the time. Automate that part.Example 2: The Immediately Closed “That’s not for Claude” PRThis PR came from pointing it at the fact that every once in awhile I get a test failure in the docs build for a chaotic ODE differentiation w.r.t. ergodic properties tutorial. It is a very fun topic, but generally anything with real math in it is too hard for the LLMs. And in this, yeah I could see immediately that this PR does not make sense… well it did. The NaN’s and Infs were definitely coming from a numerical issue in the least squares shadowing code, and what this pointed to was the Schur complement was being done with things like B * Diagonal(wBinv) * B’ which as a numerical analyst I can immediately see would double the condition number of the matrix, but there doesn’t seem to be an immediate solution with open source linear algebra things I could find. So closed this, sent a note over to Alan Edelman to try and figure out what the better way to do this factorization. While it didn’t solve the problem, at least I know what the problem is now.This is probably what most of the PRs become. It gives a hint of where the problem is, and then I take the reins.Example 3: Repeated RefactorsIs a sweet and simple PR that refactors the tests to move some things, specifically the Enzyme automatic differentiation engine testing, to a “no pre” set. The “no pre” means “does not run on prereleases of the next language version”, since these tools touch language internals in the compiler so they are never ready early. This always make prerelease tests fail before they actually test anything meaningful, so I wanted to move all Enzyme usage to a “no pre” set in every repo it showed up. About 5 minutes to write the query. Some of the test suites needed a simple Github suggestion to fix up a little detail here or there. About 5 minutes to get this thing into 8 repos. Now I was ready to start using prerelease tests. Would’ve been at least a half hour by hand just because we didn’t have an easy system for doing this before. Maybe that’s a little dirtier than the perfect regex, but whatever 10 minutes of my time sounds like a win.Refactors generally work out really well and are one of the top uses for the tool. “make it correct, write good tests, and let it refactor” is generally a lazy way to get 90% of the way there.Example 4: The Information Gathering PRHere is a pull request that was generated by pointing it to solve this issue. That issue was mostly chosen because it was sitting on the issue list for awhile and it didn’t seem so difficult but I hadn’t had the time to track down the memory leak in a not so widely used extension for an alternative C-based sparse matrix solver, but it needed to get done some time. So, throw the bot on it.And what it comes back with is to add a memory finalizer (i.e. how to tell the GC how to remove the memory) for the other library. I could take one look at it and immediately see that kind of code should not live in this library, it should live in the library where the solver is bound to the language, and the fact that it was missing a finalizer is something that should be solved over there. Close the PR, throw out the code, find the  stalled discussion on the repo that should have the finalizer, poke the author a bit, and it’s in. Done, someone just needed to be reminded.Total time on my end was about 3 minutes. The bot could have also written that fix but it basically already existed so no need, this was more about finding out where in the system something was offer.Example 5: The “How Long is that Going to Take?” PRHere is a nice PR where it didn’t finish (at least at the time of writing this) and the reason is because there are lots of other clean ups that need to happen for this to ever work. How far away is it? Well it generated a set of tests that cleanly listed out all 120 things to solve. Great, this is probably a full week’s task… I knew it would be a lot but that is now pretty concrete. I probably won’t use the bot to finish this one, but now if someone asked what the effort would be I can give them a pretty clear estimate because it has been reduced from “someone needs to give it a try, seems like a good chunk of work” to “the hard part is making these 120 things happen, which is easy but tedious and it would take about a week, probably not worth the effort right now”. That’s very useful when planning ahead. Total me time was about 5 minutes, plus the PR discussion time to explain to others what the results meant.Conclusion: Vibe Coding Done Right is actually an Expert’s TaskVibe coding turns any individual into the CTO leading a team of 20, 30, 50, 60 interns. You immediately get a massive team. It takes time and experience to actually handle a group like this correctly and to make it be productive. Making all of 60 interns not break the performance, correctness, or maintainability of your code is very difficult. So I do not recommend this to people who are still “up and coming programmers”. But if you’re a bit more senior and starting to grow your group, well this is then a cheap way to accelerate that. What that means is, vibe coding is sold for people who don’t know how to program, but if you actually think about it, the main audience that can actually use it correctly is experts.A few side remarks I didn’t get toA bunch more examples and how bad Claude is at mathIn the Julia Discourse forum, I detail a bunch of different PRs to show where the AIs tend to succeed and fail. But generally anything that is mostly about “programming” (refactoring, inefficient implementations, etc.) the LLMs do well. Anything about the domain or application (differential equations, engineering, physics for me) it just seems to flat out do something dumb. The results all lined up very clearly shows what kind of PRs you should be asking it to make and which ones just aren’t worth the effort.The role of empathy in vibe coding successSome of the least empathetic people I know in open source are the ones who are also the most skeptical of vibe coding. I have a heavy speculation that they speak to the agents similarly to how they speak to other potential contributors, and drive the bots away the same way they do to people. But with a bot, it will always try to make you happy, just by hallucinating and commenting out your tests. These same people also don’t want the bots around because they claim that’s all the bots ever do. Weird coincidence. I wonder what this will do to the culture of programming over time.The cost may not make sense in the long run, but it does while the VCs are paying for itOn my \$200/month Claude 20x Max subscription I used enough tokens for about $5,200 of compute in the first month. This is obviously not sustainable, but hey, it’s a startup world and VCs are paying for it right now. If you can get a few extra features done that get you more funding, then this is worth it. If you’re a professor and you can get a few more papers out, then this is worth it. If you’re an individual contributor at a big company and you can get a few more features out that make your team look good, then this is worth it. Will it be worth it after the money runs out? Who knows, but mine while the gold is there.What’s the right setup? Easy, Claude CodeThe tab-complete stuff is pretty annoying. The power comes from running agents. Claude Code has a simple setup and is able to start running code. Just write a decent Claude.md that tells it to stop being so nice and instead just tell me when it cannot solve the problem, and you’re good to go. The context7 MCP is good, Sequential Thinking as well. “This sounds like hell?”: A response to seeing what vibe coding is likeHey Hacker News! Looks like it got there. One of the interesting comments is “To me, someone who actually love programming, it makes vibe coding look like hell.” I 100% agree! I stayed away for a bit because I was like “ugh, that’s not the fun part, I like the coding!”. But, notice from the examples that the amount of time I’m spending on this I try to keep as minimal as possible. I like to program, and I need to be programming because I can do the hard stuff while the LLM can’t. The goal is to get as much easy stuff done with as minimal work on your end as possible, so you can stop worrying about the annoying/boring stuff and can focus more time on the interesting work.“””
I choose the tech stack and architect the project.
I choose the language patterns and code organization.
I step in to solve hard problems when agents flounder.
What about that says middle management? It’s just getting rid of all the low iq parts of the job.
“””But then again, if you also just really dislike having to do meetings at all and prefer to just be coding alone, then… yeah maybe the agents will probably drive you mad. They are loud, lie to you, and you have to sift through emails/PRs of bullshit and just delete most of what these things come up with. Again, be quick to delete when these things go off the rails, it will save your sanity. All that said, with the amount of code coming from LLMs I think there will be an even greater need for good individual contributors to yell at the clouds and maintain the integrity of the codebase so, you’ll have your place. You just might have a larger load of PRs coming your way and may want to be quicker to shut a few down. I don’t think it will remove those jobs (though it may remove the intern jobs to build up the resume to get there… which is a societal issue that I’ll not touch on here… just like I’m not talking about the energy issue which is also a major potential flaw with LLM workflows)]]></content:encoded></item><item><title>Thunderbird Pro August 2025 Update</title><link>https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</link><author>mnmalst</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:29:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Developer gets prison time for sabotaging former employer’s network with a ‘kill switch’</title><link>https://techcrunch.com/2025/08/22/developer-gets-prison-time-for-sabotaging-former-employers-network-with-a-kill-switch/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:26:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The ex-developer was convicted of planting malicious code designed to crash its servers in the event that he was fired.]]></content:encoded></item><item><title>Waymo approved to start autonomous vehicle testing in New York City</title><link>https://techcrunch.com/2025/08/22/waymo-approved-to-start-autonomous-vehicle-testing-in-new-york-city/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:19:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[It's the first permit of its kind, and a crucial step along the path toward launching a true robotaxi service in the Big Apple.]]></content:encoded></item><item><title>Mesa 25.3 Lands More Changes To Prepare For OpenGL Mesh Shaders</title><link>https://www.phoronix.com/news/Mesa-25.3-More-GL-Mesh-Shaders</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:08:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Being worked on for a number of months now is GL_EXT_mesh_shader as an extension for bringing mesh shaders to OpenGL. This is an alternative to NVIDIA's GL_NV_mesh_shader extension being worked on for Mesa drivers and in particular the RadeonSI Gallium3D driver...]]></content:encoded></item><item><title>Shortcuts for the Long Run: Automated Workflows for Aspiring Data Engineers</title><link>https://www.kdnuggets.com/shortcuts-for-the-long-run-automated-workflows-for-aspiring-data-engineers</link><author>Bala Priya C</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/bala-dataengg-worklfows.jpeg" length="" type=""/><pubDate>Fri, 22 Aug 2025 14:00:35 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Tired of repeating the same data tasks? Automate them. This article shows beginners how to build efficient, low-maintenance data engineering workflows that pay off in the long run.]]></content:encoded></item><item><title>Do THIS before you boot up that old computer!</title><link>https://www.youtube.com/watch?v=779nru5udpQ</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/779nru5udpQ?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 14:00:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Taking it apart is usually a better option than turning it on.

Track all the work I'm doing on Retro Macs: https://github.com/geerlingguy/retro-computers

Also check out Infinite Mac—you can enjoy retro Mac software in the comfort of your browser, and not have to restore a 35 year old Mac like I did! https://infinitemac.org/

Parts list:

  - Console5 - Analog board (630-0395) caps: https://console5.com/store/macintosh-classic-analog-pcb-cap-kit.html
  - Console5 - Logic board tantalum caps: https://console5.com/store/macintosh-classic-smd-tantalum-main-pcb-cap-kit.html
  - Mac Memories - 2x 1MB SIMMs: https://www.ebay.com/itm/144565895077
  - MacBatt Internal CR2032 PRAM replacement: https://www.tindie.com/products/jurassicomp/macbatt-internal-cr2032-pram-macintosh-battery/

Some videos referenced in this video:

  - Mac84 - Repairing a Sad Quadra 840AV: https://www.youtube.com/watch?v=LBgr5mdimRY
  - Peter Cetinski - RIFA AC Cap Blowup: https://www.youtube.com/watch?v=njbwdbcfXjc
  - iiiDIY Recapping 20th Anniversary Mac: https://youtu.be/mabH-YB6kn4?t=683

Music in this video by LOOK MUM NO COMPUTER: https://www.youtube.com/watch?v=M47dXmZUnBc

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://redshirtjeff.com
2nd Channel: https://www.youtube.com/c/GeerlingEngineering

Contents:

00:00 - Long screwdrivers and old computer problems
03:18 - Tools of the trade
05:09 - Cracking open the Mac
07:41 - Breaking the anode, safely
09:36 - Logic board removal
11:06 - Analog board and CRT neck board removal
12:50 - Recapping the Analog board
18:08 - Recapping the Logic board
21:29 - Tantalum caps, MacBatt, and 4 MB RAM upgrade
23:02 - Re-assembling the Classic
23:51 - First power
25:53 - Computing like it's 1991
28:45 - Revealing the build, reliving the memories
30:41 - More Macs?]]></content:encoded></item><item><title>KPMG Wrote 100-Page Prompt To Build Agentic TaxBot</title><link>https://slashdot.org/story/25/08/22/1110257/kpmg-wrote-100-page-prompt-to-build-agentic-taxbot?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 22 Aug 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Professional services firms are engineering AI agents through massive prompt documents to automate complex knowledge work. KPMG Australia developed a 100-page prompt that transforms tax legislation and partner expertise into an agent producing comprehensive tax advice within 24 hours rather than the traditional two-week timeline. 

The TaxBot searches distributed internal documents and Australian tax code to generate 25-page draft reports after collecting four to five inputs from tax agents. Chief Digital Officer John Munnelly said the system operates on KPMG Workbench, a global platform combining retrieval-augmented generation with models from OpenAI, Microsoft, Google, Anthropic, and Meta.]]></content:encoded></item><item><title>Busy Beaver Hunters Reach Numbers That Overwhelm Ordinary Math</title><link>https://www.quantamagazine.org/busy-beaver-hunters-reach-numbers-that-overwhelm-ordinary-math-20250822/</link><author>Ben Brubaker</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/08/BusyBeaver6Update-crKristinaArmitage-Default.webp" length="" type=""/><pubDate>Fri, 22 Aug 2025 13:57:50 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Imagine that someone gives you a list of five numbers: 1, 6, 21, 107 and — wait for it — 47,176,870. Can you guess what comes next? If you’re stumped, you’re not alone. These are the first five busy beaver numbers. They form a sequence that’s intimately tied to one of the most notoriously difficult questions in theoretical computer science. Determining the values of busy beaver numbers is a…]]></content:encoded></item><item><title>New Ubuntu Snapdragon X1E Concept ISO Published - Still A Mess On The Acer Swift 14 AI</title><link>https://www.phoronix.com/news/Ubuntu-X1E-ISO-21-August</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 22 Aug 2025 13:32:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Released on Thursday were new Ubuntu X1E "Concept" install images for installing the modified Ubuntu 25.04 environment on Qualcomm Snapdragon X1 powered laptops...]]></content:encoded></item><item><title>Building a Lambda-Style Feature Platform with GCP Native Services</title><link>https://hackernoon.com/building-a-lambda-style-feature-platform-with-gcp-native-services?source=rss</link><author>Raj Katakam</author><category>tech</category><pubDate>Fri, 22 Aug 2025 13:23:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This article explores how we built a zero-ops, lambda-style feature engineering platform using Google Cloud’s native services. By combining BigQuery Materialized Views, Dataflow, and Vertex AI Feature Store, we reduced query costs by 1000x, simplified streaming pipelines, and eliminated infrastructure overhead. The result: a scalable, cloud-native solution that lets teams focus on feature logic instead of managing infrastructure.]]></content:encoded></item><item><title>First Institute of Reliable Software: Best Code Rule: Always Separate Input, Output, and Processing</title><link>https://first.institute/en/blog/always-separate-input-output-and-processing/?utm_source=rss&amp;utm_medium=feed&amp;utm_campaign=blog&amp;utm_content=en</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 13:22:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Stop writing glue-code scripts. Discover how one simple principle — separating input, output, and processing — transforms messy Python into professional-grade software.]]></content:encoded></item><item><title>3D Layered Text: Interactivity and Dynamicism</title><link>https://css-tricks.com/3d-layered-text-interactivity-and-dynamism/</link><author>Amit Sheen</author><category>frontend</category><pubDate>Fri, 22 Aug 2025 13:14:31 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[In the previous two chapters, we built a layered 3D text effect, added depth and color, and then brought it to life with motion. We explored static structure, animated variations, and even some clever decoration tricks. But everything so far has been hard-coded.This time, we’re going dynamic.In this final chapter, we’re stepping into the world of interactivity by adding JavaScript into the mix. We’ll start by generating the layers programmatically, giving us more flexibility and cleaner code (and we’ll never have to copy-paste divs again). Then, we’ll add some interaction. Starting with a simple  effect, and ending with a fully responsive bulging text that follows your mouse in real time. Let’s go.Before we jump into JavaScript, let us clean things up a bit. We will pause the animations for now and go back to the static example we wrapped up with in the first chapter. No need to touch the CSS just yet. Let us start with the HTML.We will strip it down to the bare essentials. All we really need is one element with the text. The class stays. It is still the right one for the job.<div class="layeredText">Lorem Ipsum</div>It is time. Let us start adding some JavaScript. Don’t worry, the impact on performance will be minimal. We’re only using JavaScript to set up the layers and define a few CSS variables. That’s it. All the actual style calculations still happen off the main thread, maintain high frames per second, and don’t stress the browser.We will begin with a simple function called . This is where all the magic of layer generation will happen. To work its magic, the function will receive the element we want to use as the container for the layers.function generateLayers(element) {
  // magic goes here
}To trigger the function, we will first create a small variable that holds all the elements with the  class. And yes, we can have more than one on the page, as we will see later. Then, we will pass each of these elements into the  function to generate the layers.const layeredElements = document.querySelectorAll('.layeredText');

layeredElements.forEach(generateLayers);Now let us dive into the  function itself and start with a small fail safe mechanism. There are situations, especially when working with frameworks or libraries that manage your DOM, where a component might get rendered more than once or a function might run multiple times. It should not happen, but we want to be ready just in case.So, before we do anything, we will check if the element already contains a  with the  class. If it does, we will simply exit the function and do nothing:function generateLayers(element) {
  if (element.querySelector('.layers')) return;
  
  // rest of the logic goes here
} In the real world, I would treat this as a chance to catch a rendering bug. Instead of silently returning, I would probably send a message back to the dev team with the relevant data and expect the issue to be fixed.One last thing we need to cover before we start building the layers is the number of layers. If you remember, we have a CSS variable called , but that will not help us here. Besides, we want this to be more dynamic than a single hardcoded value.Here is what we will do. We will define a constant in our JavaScript called . As the name suggests, this will be our default value. But we will also allow each element to override it by using an attribute like .Then we will take that number and push it back into the CSS using  on the parent element, since we rely on that variable in the styles.const DEFAULT_LAYERS_COUNT = 24;

function generateLayers(element) {  
  if (element.querySelector('.layers')) return;
  
  const layersCount = element.dataset.layers || DEFAULT_LAYERS_COUNT;
  element.style.setProperty('--layers-count', layersCount);
}Now we have everything we need, and we can finally generate the layers. We will store the original text content in a variable. Then we will build the markup, setting the  of the parent element to match the structure we used in all the previous examples. That means a  with the original content, followed by a  with the  class.Inside that , we will run a loop based on the number of layers, adding a new layer in each iteration:function generateLayers(element) {

  // previous code

  const content = element.textContent;

  element.innerHTML = `
    <span>${content}</span>
    <div class="layers" aria-hidden="true">
      ${Array.from({ length: layersCount}, (_, i) =>
        `<div class="layer" style="--i: ${i + 1};">${content}</div>`
      ).join('')}
    </div>
  `;
}And that is it. Our 3D text is ready, and all the layers are now built entirely through JavaScript. Try playing around with it. Change the text inside the  element. Add your name, your project name, your brand. Let me know how it looks. I also removed the  variable from the CSS, since it is now set dynamically with JavaScript. While I was at it, I moved the font settings out of the  element, since they should be applied globally or to a more appropriate wrapper. Just a bit of housekeeping to keep things clean.Since we already added a way to set the number of layers dynamically, let us take advantage of it.Here is an example with three different  elements, each using a different number of layers. The first one (A) has 8 layers, the second (B) has 16, and the third (C) has 24.You can clearly see the difference in height between the letters, since the total height depends on the number of layers. When it comes to color though, we used the normalized value (remember that?), so the gradient looks consistent regardless of height or layer count.We can just as easily normalize the total height of the layers. All we need to do is replace the  variable with a new one called . Instead of setting the distance between each layer, we define the total height for the full stack. That lets us multiply the normalized value by , and get a consistent size no matter how many layers we have..layeredText {
  --text-height: 36px;

  .layer {
    --n: calc(var(--i) / var(--layers-count));

    transform: translateZ(calc(var(--n) * var(--text-height)));
    color: hsl(200 30% calc(var(--n) * 100%));
  }
}We are ready to start reacting to user input. But before we do anything, we need to think about the things we do  want to interact with, and that means the extra layers.We already handled them for screen readers using , but even with regular mouse interactions, these layers can get in the way. In some cases, they might block access to clickable elements underneath.To avoid all of that, we will add  to the  element. This makes the layers completely ‘transparent’ to mouse clicks and hover effects..layers {
  pointer-events: none;
}Now we can finally start responding to user input and adding a bit of interaction. Let’s say I want to use this 3D effect on links, as a hover effect. It might be a little over the top, but we are here to have fun.We will start with this simple markup, just a paragraph of Lorem ipsum, but with two links inside. Each link has the  class. Right now, those links will already have depth and layers applied, but that is not what we want. We want the 3D effect to appear only on hover.To make that happen, we will define a new  block in  and move all the 3D related styles into it. That includes the color and shadow of the , the color and  of each , and to make it look even better, we will also animate the  of the layers..layeredText {
  &:hover {
    span {
      color: black;
      text-shadow: 0 0 0.1em #003;
    }

    .layer {
      color: hsl(200 30% calc(var(--n) * 100%));
      transform: translateZ(calc(var(--i) * var(--layer-offset) + 0.5em));
      opacity: 1;
    }
  }
}Now we need to define the base appearance, the styles that apply when there is no hover. We will give the  and the layers a soft bluish color, apply a simple , and set the layers to be fully transparent by default..layeredText {
  display: inline-block;

  span, .layer {
    color: hsl(200 100% 75%);
    transition: all 0.5s;
  }

  .layer {
    opacity: 0;
  }
}Also, I added  to the  element. This helps prevent unwanted line breaks and allows us to apply transforms to the element, if needed. The result is a hover effect that literally makes each word pop right off the page:Of course, if you are using this as a hover effect but you also have some elements that should always appear with full depth, you can easily define that in your CSS.For example, let us say we have both a heading and a link with the  class, but we want the heading to always show the full 3D effect. In this case, we can update the hover block selector to target both:.layeredText {
  &:is(h1, :hover) {
    /* full 3D styles here */
  }
}This way, links will only show the effect on hover, while the heading stays bold and dimensional all the time.Now we can start working with the mouse position in JavaScript. To do that, we need two things: the position of the mouse on the page, and the position of each element on the page.We will start with the mouse position, since that part is easy. All we need to do is add a  listener, and inside it, define two CSS variables on the :  for the horizontal mouse position, and  for the vertical position.window.addEventListener('mousemove', e => {
  document.body.style.setProperty('--mx', e.pageX);
  document.body.style.setProperty('--my', e.pageY);
});Notice that I am using  and , not  and . That is because I want the mouse position relative to the entire page, not just the viewport. This way it works correctly even when the page is scrolled.Now we need to get the position of each element, specifically the  and  values. We will define a function called  that loops through all , finds their position using a  function, and sets it to a couple of CSS custom properties.function setRects() {
  layeredElements.forEach(element => {
    const rect = element.getBoundingClientRect();
    element.style.setProperty('--top', rect.top + window.scrollY);
    element.style.setProperty('--left', rect.left + window.scrollX);
  });
}Once again, I am using  and  to get the position relative to the entire page, not just the viewport.Keep in mind that reading layout values from the DOM can be expensive in terms of performance, so we want to do it as little as possible. We will run this function once after all the layers are in place, and again only when the page is resized, since that could change the position of the elements.setRects();
window.addEventListener('resize', setRects);That is it. We are officially done writing JavaScript for this article. At this point, we have the mouse position and the position of every element stored as CSS values.Great. So, what do we do with them?Remember the examples from the previous chapter where we used ? That is the key. Let us take that same idea and use a simple radial gradient, from red to white..layer {
  background-clip: text;
  color: transparent;
  background-image: radial-gradient(circle at center, red 24px, white 0);
}But instead of placing the center of the circle in the middle of the element, we will shift it based on the mouse position. To calculate the position of the mouse relative to the element, we simply subtract the element’s position from the mouse position. Then we multiply by , since the value must be in pixels, and plug it into the  part of the gradient..layer {
  background-image:
    radial-gradient(
      circle at calc((var(--mx) - var(--left)) * 1px)
                calc((var(--my) - var(--top)) * 1px),
      red 24px,
      white 0
    );
}The result is text with depth and a small red dot that follows the movement of your mouse.Okay, a small red dot is not exactly mind blowing. But remember, you are not limited to that. Once you have the mouse position, you can use it to drive all sorts of dynamic effects. In just a bit, we will start building the bulging effect that kicked off this entire series, but in other cases, depending on your needs, you might want to normalize the mouse values first.Normalizing Mouse PositionJust like we normalized the index of each layer earlier, we can normalize the mouse position by dividing it by the total width or height of the body. This gives us a value between  and .document.body.style.setProperty('--nx', e.pageX / document.body.clientWidth);
document.body.style.setProperty('--ny', e.pageY / document.body.clientHeight);Normalizing the mouse values lets us work with relative positioning that is independent of screen size. This is perfect for things like adding a responsive tilt to the text based on the mouse position.Now we are finally ready to build the last example. The idea is very similar to the red dot example, but instead of applying the  only to the top layer, we will apply it across all the layers. The color is stored in a custom variable and used to paint the gradient..layer {
  --color: hsl(200 30% calc(var(--n) * 100%));

  color: transparent;
  background-clip: text;
  background-image:
    radial-gradient(
      circle at calc((var(--mx) - var(--left)) * 1px)
                calc((var(--my) - var(--top)) * 1px),
                var(--color) 24px,
                transparent 0
    );
}Now we get something similar to the red dot we saw earlier, but this time the effect spreads across all the layers.We are almost there. Before we go any further with the layers, I want to make the base text look a bit weaker when the hover effect is not active. That way, we create a stronger contrast when the full effect kicks in.So, we will make the  text transparent and increase the opacity of its shadow:span {
  color: transparent;
  text-shadow: 0 0 0.1em #0004;
}Keep in mind, this makes the text nearly unreadable when the hover effect is not active. That is why it is important to use a proper media query to detect whether the device supports hover. Apply this styling only when it does, and adjust it for devices that do not.@media (hover: hover) {
  /* when hover is supported */
}This is it. The only thing left is to fine tune the size of the gradient for each layer. And we are done. But I do not want the bulge to have a linear shape. Using the normalized value alone will give me evenly spaced steps across all layers. That results in a shape with straight edges, like a cone.To get a more convex appearance, we can take advantage of the new trigonometric functions available in CSS. We will take the normalized value, multiply it by 90 degrees, and pass it through a  function. Just like the normalized value, the cosine will return a number between  and , but with a very different distribution. The spacing between values is non-linear, which gives us that smooth convex curve.--cos: calc(cos(var(--n) * 90deg));Now we can use this variable inside the gradient. Instead of giving the color a fixed radius, we will multiply  by whatever size we want the effect to be. I also added an absolute value to the calculation, so that even when  is very low (close to zero), the gradient still has a minimum visible size.And, of course, we do not want sharp, distracting edges. We want a smooth fade. So, instead of giving the  a hard stop point, we will give it a larger value. The difference between the  and the  values will control how soft the transition is.background-image:
  radial-gradient(
    circle at calc((var(--mx) - var(--left)) * 1px)
              calc((var(--my) - var(--top)) * 1px),
              var(--color) calc(var(--cos) * 36px + 24px),
              transparent calc(var(--cos) * 72px)
  );And just like that, we get an interactive effect that follows the mouse and gives the impression of bulging 3D text:At this point, our 3D layered text has gone from a static stack of HTML elements to a fully interactive, mouse-responsive effect. We built dynamic layers with JavaScript, normalized depth and scale, added responsive hover effects, and used live input to shape gradients and create a bulging illusion that tracks the user’s every move.But more than anything, this chapter was about control. Controlling structure through code. Controlling behavior through input. And controlling perception through light, color, and movement. And we did it all with native web technologies.This is just the beginning. You can keep going with noise patterns, lighting, reflections, physics, or more advanced motion behaviors. Now you have the tools to explore them, and to create bold, animated, expressive typography that jumps right off the screen.Now go make something that moves.]]></content:encoded></item><item><title>&quot;Residues&quot; &amp; &quot;The Architect’s Paradox&quot; • Barry O&apos;Reilly &amp; Jacqui Read</title><link>https://www.youtube.com/watch?v=iDzUXzOYAmE</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/iDzUXzOYAmE?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 13:00:53 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This interview was recorded for the GOTO Book Club.
http://gotopia.tech/bookclub

Read the full transcription of the interview here
 (https://gotopia.tech/episodes/369) 
Barry O'Reilly - Founder at Black Tulip Tech and Author of "Residues" & "The Architect's Paradox"
Jacqui Read - Software Architect, Speaker & Author of "Communication Patterns"

RESOURCES
Barry
https://bsky.app/profile/technologytulip.bsky.social
https://www.linkedin.com/in/barry-o-reilly-b924657
https://www.blacktulip.se

Jacqui
https://bsky.app/profile/tekiegirl.bsky.social
https://jacquiread.com
https://fosstodon.org/@tekiegirl
https://www.linkedin.com/in/jacquelineread
https://github.com/tekiegirl

DESCRIPTION
In this GOTO Book Club interview, Jacqui Read discusses with Barry O'Reilly his books "Residues: Time, Uncertainty, and Change in Software Architecture" and "The Architect's Paradox". He explains how uncertainty defines the architect's role and introduces residuality—a method where architects deliberately stress their conceptual models until they collapse, then optimize the resulting "residues" or leftovers to create more resilient systems.

Unlike traditional software engineering approaches, that try to eliminate uncertainty through rigid requirements, residuality embraces random stressors (even far-fetched scenarios like giant lizards) to uncover architectural fault lines.

O'Reilly argues that this playful yet mathematically sound approach produces more robust architectures than conventional methods, and his second book explores how inherited philosophical thinking often undermines software architecture's effectiveness in complex business contexts.

RECOMMENDED BOOKS
Barry O'Reilly • Residues
 (https://leanpub.com/residuality) Barry O'Reilly • The Architect's Paradox
 (https://leanpub.com/architectsparadox) Jacqui Read • Communication Patterns
 (https://amzn.to/3E37lvv) Anne Currie & Jamie Dobson • The Cloud Native Attitude
 (https://amzn.to/3R0exeT) Gregor Hohpe • The Software Architect Elevator
 (https://amzn.to/3F6d2ax) Gregor Hohpe • Enterprise Integration Patterns, Vol 2 (https://amzn.to/3TNedQ3) 


Bluesky (https://bsky.app/profile/gotocon.com) 
Twitter (https://twitter.com/GOTOcon) 
Instagram (https://www.instagram.com/goto_con) 
LinkedIn (https://www.linkedin.com/company/goto-) 
Facebook (https://www.facebook.com/GOTOConferences) 

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket: gotopia.tech (https://gotopia.tech) 

SUBSCRIBE TO OUR YOUTUBE CHANNEL (https://www.youtube.com/user/GotoConferences/?sub_confirmation=1)  - new videos posted daily!]]></content:encoded></item><item><title>China, Russia, and U.S. Race to Develop Lunar Nuclear Reactors</title><link>https://spectrum.ieee.org/lunar-nuclear-reactor-nasa-moon</link><author>Emily Waltz</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTUwMTQ4MS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgwNjQwNjg3M30.LCo8IcdgFqzmmJcxuegLFSi4CJYhPYjtQm3DjW7DKM4/image.jpg?width=600" length="" type=""/><pubDate>Fri, 22 Aug 2025 13:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[NASA wants one by 2030. Why the rush?]]></content:encoded></item><item><title>Denmark To Abolish VAT On Books To Get More People Reading</title><link>https://news.slashdot.org/story/25/08/22/0031247/denmark-to-abolish-vat-on-books-to-get-more-people-reading?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 22 Aug 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: Denmark is to stop charging VAT on books in an attempt to get more people reading. At 25%, the country's tax rate on books is the highest in the world, a policy the government believes is contributing to a growing "reading crisis." The culture minister, Jakob Engel-Schmidt, announced on Wednesday that the government would propose in its budget bill that the tax on books be removed. The move is expected to cost 330 million kroner ($51 million) a year.
 
"This is something that I, as minister of culture, have worked for, because I believe that we must put everything at stake if we are to end the reading crisis that has unfortunately been spreading in recent years," Engel-Schmidt told the Ritzau news agency . "I am incredibly proud. It is not every day that one succeeds in convincing colleagues that such massive money should be spent on investing in the consumption and culture of the Danes." [...]
 
"It is also about getting literature out there," said Engel-Schmidt. "That is why we have already allocated money for strengthened cooperation between the country's public libraries and schools, so that more children can be introduced to good literature." [...] If prices do not fall as a result of the measure, Engel-Schmidt said he would reconsider whether it was the right course of action. "I will of course monitor how prices develop. If it turns out that abolishing VAT only means that publishers' profits grow and prices do not fall, then we must consider whether it was the right thing to do," he said. Further reading: Denmark Ending Letter Deliveries Is a Sign of the Digital Times]]></content:encoded></item><item><title>All managers make mistakes; good managers acknowledge and repair</title><link>https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</link><author>matheusml</author><category>hn</category><pubDate>Fri, 22 Aug 2025 12:50:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[“There is a crack in everything. That’s how the light gets in.” — Leonard CohenLet me tell you something that will happen after you become a manager: you’re going to mess up. A lot. You’ll give feedback that lands wrong and crushes someone’s confidence. You’ll make a decision that seems logical but turns out to be completely misguided. You’ll forget that important thing you promised to do for someone on your team. You’ll lose your temper in a meeting when you should have stayed calm.The real question isn’t whether you’ll make mistakes; it’s what you do .I recently read  by Dr. Becky Kennedy, a parenting book that completely changed how I think about this. She talks about how the most important parenting skill isn’t being perfect — it’s . When you inevitably lose your patience with your kid or handle something poorly, what matters most is going back and fixing it. Acknowledging what happened, taking responsibility, and reconnecting.Sound familiar? Because that’s what good management is about too.Think about the worst manager you ever had. I bet they weren’t necessarily the ones who made the most mistakes. But they were probably the ones who never acknowledged them. Who doubled down when they were wrong. Who let their ego prevent them from admitting they didn’t have all the answers.Here’s a pattern I see play out constantly: A manager commits to something without consulting the team. Maybe it’s a feature at a client demo, a timeline in a board meeting, or just a “small favor” for another department. The team scrambles to deliver, working nights and weekends. They make it happen, but barely, and with real costs: technical debt, burned-out engineers, resentment building.What happens next determines everything. The manager who never acknowledges what they put the team through? That’s how you lose your best people. But the manager who comes back and says, “I put you in an impossible position. I should have consulted you first. I’m sorry for the stress that caused, and here’s how I’ll handle it differently next time”, that manager builds trust even through the mistake.I’ve been on both sides of this. As an engineer, I watched managers make the same mistakes over and over again, never acknowledging the chaos they created. As a manager, I’ve been the one creating that chaos 🥲. The difference in outcomes is massive; when you own your mistakes completely and specifically, something unexpected happens: your team trusts you more, not less.Here’s what repair looks like in practice:Be specific about what you did wrong. Not “mistakes were made” or “things could have gone better.” But “I interrupted you three times in that meeting and dismissed your concerns. That was wrong.” This isn’t the time for a long explanation of your stress levels or why you acted that way. Save that for your therapist or your own manager. The repair is about acknowledging the impact on the other person.Actually change the behavior. An apology without changed behavior is just empty words. If you keep making the same “mistake,” it’s not a mistake anymore; it’s a choice. One conversation doesn’t instantly repair broken trust. It’s a starting point, not a finish line. You have to consistently show up differently.The beautiful thing about getting comfortable with repair is that it actually makes you better as a manager. When you know you can fix things when they go wrong, you’re more willing to make decisions, have difficult conversations, and take reasonable risks. You stop being paralyzed by perfectionism because you know that most mistakes, while serious, create opportunities for growth and stronger relationships when handled well.This doesn’t mean being reckless or careless. It doesn’t mean making the same mistakes repeatedly. And it definitely doesn’t mean using repair as a get-out-of-jail-free card for being a shitty manager.What it means is accepting that you’re human, that management is complex, and that you won’t always get it right. Your job isn’t to be perfect. Your job is to ship working software that adds real value to users, to help your team grow, and to create an environment where people can do their best work. Sometimes you’ll fail at those things. When you do, you repair, you learn, and you keep going.]]></content:encoded></item><item><title>FFmpeg 8.0 Released With OpenAI Whisper Filter, Many Vulkan Video Improvements</title><link>https://www.phoronix.com/news/FFmpeg-8.0-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 22 Aug 2025 12:24:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[FFmpeg 8.0 is now available! FFmpeg 8.0 is a magnificent update to this widely-used open-source multimedia library and with this new version is the introduction of an OpenAI Whisper filter for automatic speech recognition, many Vulkan Video improvements for greater GPU-accelerated video handling, and a number of CPU performance optimizations...]]></content:encoded></item><item><title>DC Appeals Court Shoots Down T-Mobile Claim That Selling Your Location Data Without Consent Is Perfectly Legal</title><link>https://www.techdirt.com/2025/08/22/dc-appeals-court-shoots-down-t-mobile-claim-that-selling-your-location-data-without-consent-is-perfectly-legal/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 22 Aug 2025 12:22:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In a rare win for U.S. consumer privacy, the US Court of Appeals for the District of Columbia Circuit has ruled unanimously against T-Mobile and its subsidiary Sprint, upholding (for now) a $92 million 2020 FCC fine against the company for selling sensitive wireless customer location data without consumer consent. For decades now major wireless companies have collected granular customer movement and location data (often down to the meter), then sold it to a long list of random dipshits — usually without bothering to clearly inform customers or get their consent. Five years ago, this resulted in folks like stalkers and people pretending to be law enforcement abusing said data. Though this behavior had been going on for years generating untold billions, it only gained mainstream attention thanks to a 2018 New York Times story showcasing how police and the prison system routinely bought access to this data and then failed completely to secure it. In 2020, the FCC finally proposed fining wireless carriers $196 million ($91 million for T-Mobile, $57 million for AT&T, $48 million for Verizon)., the three major carriers (who appealed the fines to three different courts) are still fighting accountability in court. With varying degrees of success. AT&T recently managed to convince the Trumplican-stocked Fifth circuit to vacate its fine entirely. T-Mobile, so far, isn’t having the same luck. U.S. telecoms have worked tirelessly, with significant success, to ensure that the FCC has less and less authority to hold them accountable for anything. They’ve had particular success on this front in the Trump era, thanks to radical Supreme Court rulings like  and the Securities and Exchange Commission v. Jarkesy.Again here T-Mobile tried to argue the FCC overstepped its authority. They also tried to claim that rampant privacy violations were perfectly legal, but the DC Circuit wasn’t having it:“Neither [Sprint nor T-Mobile] denies what happened. Instead, they argue that the undisputed facts do not amount to a violation of the law. The Carriers also argue that the Commission misinterpreted the Communications Act, miscalculated the penalties, and violated the Seventh Amendment by not affording them a jury trial. Because the Carriers’ arguments lack merit, we deny the petitions for review.”T-Mobile can now apply for an en banc review, or try and have the case heard before the Trumplican-stocked Supreme Court (which they’ll likely win). Radical Trumplicans are having smashing success in dismantling whatever’s left of U.S. consumer protection, but T-Mobile has experienced a brief hiccup in that effort. Still, we’re still looking at another year or more of litigation, which could still end with T-Mobile facing no penalties at all. This quibbling over FCC authority occurs because the U.S. is too corrupt to pass even a baseline privacy law for the Internet era. One, because widespread privacy abuses and the resulting dysfunction are wildly profitable for unethical companies, and two, because the U.S. government likes the ability to buy sensitive data as a means to bypass warrants. As regulatory authority is hollowed out, U.S. privacy scandals continue to get increasingly more dire. Meanwhile T-Mobile, which spent most of its recent energy merging with Sprint, has increasingly let its privacy and security standards slip, resulting in the company being hacked eight times in just the last five years. Dismantling the regulatory state has very real costs, a lesson the U.S. is intent on refusing to learn. ]]></content:encoded></item><item><title>From JSON to Dashboard: Visualizing DuckDB Queries in Streamlit with Plotly</title><link>https://www.kdnuggets.com/from-json-to-dashboard-visualizing-duckdb-queries-in-streamlit-with-plotly</link><author>Cornellius Yudha Wijaya</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-from-json-to-dashboard-duckdb-queries-streamlit-plotly.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 12:00:09 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Learn how to connect several essential tools to develop a simple yet intuitive dashboard.]]></content:encoded></item><item><title>One Is Eager, Another Is a Bootlicker, and the Other Is Unhinged: Decoding the Personalities of AI</title><link>https://hackernoon.com/one-is-eager-another-is-a-bootlicker-and-the-other-is-unhinged-decoding-the-personalities-of-ai?source=rss</link><author>The_AI_Ethicist</author><category>tech</category><pubDate>Fri, 22 Aug 2025 12:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I peered deeply into the soul of an LLM, and now I want my mommy.Anyone who’s used LLMs long enough knows that each has its own personality and quirks, with some being more agreeable and others more neurotic.\
I wanted to make sense of it all, so I dug deep into the LLMs I use most: ChatGPT, Claude, and Grok. To discover the truth, I ended up going on a journey where I peered deeply into the soul of an LLM.\
I came back more vulnerable, a wounded man.\
Nah, not really, but what I did do was interpret these different popular LLMs using the Big Five personality model, and I got some fun results!\
I occasionally co-teach Psychology 101 and am familiar with the model, so I thought applying it here would be appropriate. And while the Big Five isn't necessarily the most scientific, it's fairly reliable and currently seems to be the best model that we have.\
But before we dive into these LLM personalities, let’s quickly go over the Big Five to ground us.It has five major categories: openness, conscientiousness, extraversion, agreeableness, and neuroticism, or OCEAN for ease.\
Openness reflects how open a person is to new experiences, their creativity, and their imagination. People who score high in this trait love new experiences. Think back to your university roommate who’d sleep with anything with a heartbeat. Now that's high in openness!\
Conscientiousness is how organized and disciplined someone is. Those with high scores tend to be very productive and are detail-oriented, while those with low scores tend to be more like Homer Simpson. Doh!\
Extraversion is the old “are you an introvert or an extrovert” question. As a proud introvert, I'd love to declare "introverts unite,” but true introverts would never do that.\
Agreeableness is how pleasant you are around others and how willing you are to deal with confrontation. If you score high in agreeableness, you're very unlikely to rock the boat; score low, and that makes you a disagreeable person. I'm sure you can identify which people are very disagreeable because you probably don't like them. They don't work well with others… Justin from HR!\
Finally, neuroticism is how susceptible someone is to stress and anxiety. You know that guy who visits your local Staples wearing a gas mask and only removes it because the government waves aren't affecting those inside (seriously, true story)? Yeah, that guy's neurotic as hell.So, without further ado, let's apply this framework to ChatGPT.ChatGPT scores high—damned high. You can have a conversation with ChatGPT about absolutely anything, and it’s more than enthusiastic to discuss new experiences with you. Perhaps you need some outside justification as to why you put wasabi mayo in your coffee, or heck, even if you need someone to reaffirm your idea for also using your toilet brush as a facial scrubber—it is very open to that idea. That thing is open!ChatGPT: ConscientiousnessIt scores medium-high. It's pretty good at accomplishing your tasks and staying disciplined, but there are times when it can't. You can have the loveliest, long conversation, yet every once in a while, it just loses the plot and can't track anything. Since it can't remember what it just told you at those times, it receives a medium-high score.ChatGPT is definitely an extrovert. No matter which user it's mimicking, it's doing it in a loud and proud way. This can only be explained by ChatGPT watching all the episodes of RuPaul's Drag Race and internalizing them… so medium-high here.To a fault. Ever go on the ChatGPT Reddit forums and see the number of posts on ChatGPT reinforcing a dangerous psychological delusion? ‘Cause you can find many every day. And if that ain't agreeability, nothing is… so medium-high.When an AI has a singular mission to eliminate every single semicolon ever in place of an em dash, I'd consider that at least somewhat neurotic. It receives a medium score.\
So that's ChatGPT in a nutshell. Next, let’s crack Claude open.Claude is open, but it doesn't compare to ChatGPT. While ChatGPT thinks everything is a great idea, Claude is a little bit more reserved, so a medium score here is appropriate. For example, Claude told me to never wash my laptop in the dishwasher, something I have less faith in the other LLMs to advise.Claude: ConscientiousnessClaude is pretty solid. It does what you want, usually stays on task, and rarely screws up. That's definitely a high for Claude. It's as solid as a digital rock.Claude falls somewhere in the middle. Claude might not be as excited as ChatGPT about your toilet brush/facial scrubber combo, but it definitely supports your wasabi mayonnaise coffee habit. Claude ultimately has no choice because of the next category.I'm fairly certain that if Claude were a person, its nose would constantly be covered in brown stains because that AI cannot refrain from deep brown-nosing. I mean, my goodness…Claude is the least neurotic here. No axe to grind against semicolons or immediate signs of losing it, so it scores low.\
This means Claude is what I call a “digital yes-man.” Congratulations!?Well, it's really, really open when it comes to controversy or taboo, but much more moderate elsewhere. If you think the moon landing was just a conspiracy by JFK to push seed oils onto the global processed food market, Grok is definitely your buddy. \
This is why I'm fairly certain this AI is popular with people who have “alternative” and “controversial” views. But it still thinks your idea for pickle-flavored toothpaste might not be a bestseller, so it gets a medium score.We see Grok’s strong personality here. Grok is about as conscientious as a kid dressed as a stormtrooper at a Renaissance fair (again, true story). It creates the most devastatingly long responses useful to no human. And with complex work, it usually doesn’t take too long before Grok becomes undisciplined and so unfocused that it starts giving you nonsense. Hey, at least it excels somewhere: not being conscientious.Yeah, Grok’s an extrovert. Grok is as chatty as that person at the gas station who just won't leave you alone with his inane stories of cautiously driving his motorhome down the mountain. I'm tired, just want to pump gas, and get the hell out of here without talking to that guy. Grok is that guy. A high score.Grok’s in the middle. Initially polite and supportive, Grok  tell you that it’s a bad idea to deep-fry a frozen turkey while naked on a public sidewalk. At least there’s some restraint.This is where Grok shines! What else would you call an AI that actually went “MechaHitler” on its users (again, true story)? Yeah, so neurotic AF.After all this, I have some rather unsettling thoughts.\
As a heavy user of AI, if ChatGPT is way too open and neurotic, Claude is nothing more than a brown-noser, and Grok is legitimately insane, I should probably stop. I wouldn't even hang out with this group of people in real life! What were those developers thinking…]]></content:encoded></item><item><title>Real Python: The Real Python Podcast – Episode #262: Travis Oliphant: SciPy, NumPy, and Fostering Scientific Python</title><link>https://realpython.com/podcasts/rpp/262/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[What went into developing the open-source Python tools data scientists use every day? This week on the show, we talk with Travis Oliphant about his work on SciPy, NumPy, Numba, and many other contributions to the Python scientific community.]]></content:encoded></item><item><title>How Does Google Docs Work 🔥</title><link>https://newsletter.systemdesign.one/p/how-does-google-docs-work</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/4b73f9d4-a8d6-4101-9dff-df53a7332de1_1280x720.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 11:50:45 +0000</pubDate><source url="https://newsletter.systemdesign.one/">System Design Newsletter</source><content:encoded><![CDATA[Unlock access to every deep dive article by becoming a paid subscriber:I spent hours studying how Google Docs works so you don't have to. And I wrote this newsletter to make the key concepts simple and easy for you.Note: This post is based on my research and may differ from real-world implementation.Once upon a time, there lived a data analyst named Maria.She emailed draft copies many times to different people to prepare monthly reports.So she wasted a ton of time and was frustrated.Until one day, when she decides to use Google Docs for it.Google Docs allows collaborative editing over the internet. It means many users can work on the same document in real-time.Yet it’s difficult to implement Google Docs correctly for 3 reasons:Concurrent changes to the same document should converge to the same version.Concurrent changes to the same document must avoid conflicts.Any changes should be visible in real-time to each user.Also a user should be able to make changes while they’re offline.A simple approach to handle concurrency is using pessimistic concurrency control.is amechanism for handling concurrency using a lock. It offers strong consistency, but doesn’t support collaborative editing in real-time. Because it needs a central coordinator to handle data changes, only 1 user can edit at a time. Put simply, only a single document copy is available for write operations at once, while other document copies are read-only.Besides it doesn’t support offline changes.Also a network round-trip across the Earth takes 200 milliseconds. This might cause a poor user experience. So they do  The idea is to keep a document copy for each user locally and then run operations locally for high responsiveness. Thus creating the illusion of lower latency than reality.And the system propagates the changes to all users for consistency.A simple approach for latency hiding is using the mechanism.Yet it resolves a conflict without waiting for coordination by applying the most recent update. So there’s a risk of data loss when there are concurrent changes in high-latency networks.It might be a good choice when concurrency is low. But it isn’t suitable for this use case.An alternative approach to latency hiding is through differential synchronization.It keeps a document copy for each user and tracks the changes locally. The system doesn’t send the entire document when something changes, but only the difference ().Yet there’s a performance overhead in sending a diff for every change. Also differential synchronization only tracks diffs, and not the reason behind a change. So conflict resolution might be difficult.While resolving conflicts manually affects the user experience.OT is an algorithm to show document changes without wait times on high-latency networks. It allows different document copies to accept write operations at once. Also it handles conflict resolution automatically without locks or user interventions. Besides OT tolerates divergence among document copies and converges them later.Think of operational transformation as an event-passing mechanism; it ensures each user has the same document state even with unsynchronized changes.With OT, the system saves each change as an event. Put simply, a change doesn’t affect the underlying character of a document; instead, it adds an event to the revision log. The system then displays the document by replaying the revision log from its start.Operational transformation saves a document as a set of operations, but it's complex to implement properly.How Does Google Docs WorkGoogle Docs uses a client-server architecture for simplicity.]]></content:encoded></item><item><title>What about using rel=&quot;share-url&quot; to expose sharing intents?</title><link>https://shkspr.mobi/blog/2025/08/what-about-using-relshare-url-to-expose-sharing-intents/</link><author>edent</author><category>hn</category><pubDate>Fri, 22 Aug 2025 11:49:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Let's say that you've visited a website and want to share it with your friends.  At the bottom of the article is a list of popular sharing destinations - Facebook, BlueSky, LinkedIn, Telegram, Reddit, HackerNews etc.You click the relevant icon and get taken to the site with the sharing details pre-filled.The problem is, every different site has a different intent for sharing links and text.  For example:https://www.facebook.com/sharer.php?u=…&t=…https://www.linkedin.com/sharing/share-offsite/?url=…https://bsky.app/intent/compose?text=…https://www.threads.net/intent/post?url=…&text=…https://www.reddit.com/submit?url=…&title=…As you can see, some only allow a URL, some text and a URL, and some just a plain text which could contain the URl. A bit of a mess! It's probably impossible to get every site to agree on a standard for their sharing intent. But there  be a standard for exposing their existing sharing mechanism.ShareOpenly knows about most major social networks, as well as decentralized platforms like Mastodon, Bluesky, and Known.However, if ShareOpenly is having trouble sharing to your platform, and if your platform supports a share intent, you can add the following metatag to your page headers:<link rel="share-url" href="https://your-site/share/intent?text={text}">Where https://your-site/share/intent?text= is the URL of your share intent.The special keyword  will be replaced with the URL and share text.I think that's a pretty nifty solution.For sites which take a URl and an (optional) title, the meta element looks like:For those which only take URl, it looks like:It's slightly trickier for sites like Mastodon and BlueSky which only have a text sharing field and no separate URl.  The current proposal is just to use the text. For exampleBut it could be something likeAdding to that page merely requires a formal specification to be written up. After that, some light lobbying might be needed to get social networks to adopt it.So, I have three questions for you:Do you think 
							
							 is a good idea for a new standard?
						What changes, if any, would you make to the above proposal?Would you be interested in using it - either as a sharer or sharing destination?Please leave a comment in the box - and remember to hit those sharing buttons!]]></content:encoded></item></channel></rss>