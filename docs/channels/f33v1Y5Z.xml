<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Latest</title><link>https://www.awesome-dev.news</link><description></description><item><title>How Autonomy Saved One of Spotify’s Most Loved Features • Joakim Sunden • YOW! 2024</title><link>https://www.youtube.com/watch?v=6bzS5GW6Ad4</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/6bzS5GW6Ad4?version=3" length="" type=""/><pubDate>Sun, 23 Feb 2025 13:00:15 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at YOW! Australia 2024. #GOTOcon #YOW
https://yowcon.com

Joakim Sundén - Founding Partner at Better Product Work & Co-Creator of the Spotify Model @joakimsunden-crisp 

ORIGINAL TALK TITLE
How Autonomy Saved One of Spotify’s Most Loved Features From Being Killed

RESOURCES
https://bsky.app/profile/joakimsunden.bsky.social
https://twitter.com/joakimsunden
https://linkedin.com/in/joakimsunden
https://joakimsunden.com

ABSTRACT
"I would have killed that if it was just me, 100%,” said Spotify founder and CEO Daniel Ek about Discover Weekly, a feature that would become one of Spotify’s most loved product features, almost a brand in itself.

Designers and senior engineers were equally skeptical, but the team was still able to ship the feature.

In this talk, you’ll learn how Spotify’s organisational culture of Agile management and autonomous teams enables innovation, using the Discover Weekly feature as an example. [...]

TIMECODES
00:00 Intro
02:14 Discover Weekly
05:16 Spotify HQ
06:12 Radio
06:50 Discover
09:54 Re-connecting the dots
12:44 "Traditional" organizations
15:06 Autonomous squad
16:53 4 Risks in product development
18:12 Focused on outcomes, not output
19:11 Activities, epics, features, stories
19:27 These bets have a low success rate
21:03 Moving from opinions to data
20:38 Alignment – Autonomy
21:37 Aligned autonomy
22:57 Gatekeeping vs facilitating context
25:29 Iterate, iterate, iterate
27:07 Daniel Ek
27:32 Celebrate failure
27:55 Limit blast radius
28:37 Watching the data
28:57 Watching the buzz
29:28 Experiments
32:59 Winning formula
33:36 Conclusions
38:13 Innovation can't be forced
38:24 Outro

Download slides and read the full abstract here:
https://yowcon.com/brisbane-2024/sessions/3442

RECOMMENDED BOOKS
Joakim Sunden & Marcus Hammarberg • Kanban in Action • https://amzn.to/3n1Fmna
Ron Kohavi, Diane Tang & Ya Xu • Trustworthy Online Controlled Experiments • https://amzn.to/3JlY4NZ
Gene Kim, Jez Humble, Nicole Forsgren, Patrick Debois & John Willis • The DevOps Handbook • https://amzn.to/3C0Rj0C
Matthew Skelton & Manuel Pais • Team Topologies • http://amzn.to/3sVLyLQ
Jennifer Petoff, Niall Murphy, Betsy Beyer & Chris Jones • Site Reliability Engineering • https://amzn.to/3xYac2Q
Marty Cagan • Inspired • https://amzn.to/3dPz7iq
Marty Cagan • Empowered • https://amzn.to/42kuKAj
Reed Hastings & Erin Meyer • No Rules Rules • https://amzn.to/3Cjottx
Forsgren, Humble & Kim • Accelerate: The Science of Lean Software and DevOps • https://amzn.to/3tCz1xO
Fred Brooks Jr. • The Mythical Man-Month • https://amzn.to/31NJc5C

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#Spotify #SpotifyFeature #Autonomy #SoftwareEngineering #Programming #DiscoverWeekly #Agile #AgileDevelopment #JoakimSunden #Crisp #Kanban #Data #ContinuousImprovement #AlignedAutonomy #YOWcon

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Flawed Diamonds Are a Quantum Sensor’s Best Friend</title><link>https://spectrum.ieee.org/quantum-sensors-2671182149</link><author>Dina Genkina</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjU2MjQ4MC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwMTk5NzcyN30.loo801SNgzyVATCIIVnex4SB2jlSslJIGlThemv8LaQ/image.png?width=600" length="" type=""/><pubDate>Sun, 23 Feb 2025 13:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Tiny faults could find big applications for chips, art history, and more]]></content:encoded></item><item><title>Unexpected Shape of Lead-208 Nucleus of May Force Scientists to Reevaluate Atomic Nuclei Models</title><link>https://science.slashdot.org/story/25/02/23/0051225/unexpected-shape-of-lead-208-nucleus-of-may-force-scientists-to-reevaluate-atomic-nuclei-models?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Feb 2025 12:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["An international research collaboration led by the University of Surrey's Nuclear Physics Group has overturned the long-standing belief that the atomic nucleus of lead-208 is perfectly spherical," reports Phys.org. 

They add that the discovery "challenges fundamental assumptions about nuclear structure and has far-reaching implications for our understanding of how the heaviest elements are formed in the universe..."


[A] new study published in Physical Review Letters used a high-precision experimental probe to examine its shape and found that rather than being perfectly spherical, the nucleus of lead-208 is slightly elongated, resembling a rugby ball (prolate spheroid)... Using the state-of-the-art GRETINA gamma-ray spectrometer at Argonne National Laboratory in Illinois, U.S., scientists bombarded lead atoms with high-speed particle beams accelerated to 10% of the speed of light — equivalent to circling the Earth every second. The interactions created unique gamma-ray fingerprints of the properties of excited quantum states in lead-208 nuclei — in other words, the nuclei were energized — which, in turn, were used to determine its shape. 


Theoretical physicists, including those at the Surrey Nuclear Theory Group, are now re-examining the models used to describe atomic nuclei, as the experiments suggest that nuclear structure is far more complex than previously thought.
]]></content:encoded></item><item><title>Quiz: How to Use sorted() and .sort() in Python</title><link>https://realpython.com/quizzes/python-sort/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Sun, 23 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[By working through this quiz, you’ll revisit how to sort various types of data in different data structures, customize the order, and work with two different ways of sorting in Python.]]></content:encoded></item><item><title>AMD Preparing New GPU Support For Their Kernel Graphics Driver In Linux 6.15</title><link>https://www.phoronix.com/news/AMDGPU-Linux-6.15</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Feb 2025 11:51:11 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD has sent out their initial pull request of "new stuff" for their AMDGPU kernel graphics driver and AMDKFD compute driver of feature additions they want to make for the upcoming Linux 6.15 kernel. Most notable from this week's submission to DRM-Next is preparing a lot of new GPU hardware support...]]></content:encoded></item><item><title>OneXPlayer Linux Driver Catching Up To The Windows Monitoring Driver</title><link>https://www.phoronix.com/news/OneXPlayer-Linux-Driver-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Feb 2025 11:36:57 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[OneXPlayer produces a line of handheld gaming consoles powered by AMD or Intel SoCs. These devices ship with Windows out-of-the-box but  given they are x86_64 software have worked alright with Linux and there's been a OneXPlayer Linux driver for supporting sensor readings and other device-specific information from these handhelds. In a big patch series this weekend, that OneXPlayer Linux driver is catching up to its official Windows counterpart...]]></content:encoded></item><item><title>Digest #161: GitLab 300GB Loss, DIY Data Center, OAuth Attacks, Netflix AWS Security, Docker Hub Limits &amp; More!</title><link>https://www.devopsbulletin.com/p/digest-161-gitlab-300gb-loss-diy</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd377709-8e11-4b69-9726-1029a1b30588_1447x991.jpeg" length="" type=""/><pubDate>Sun, 23 Feb 2025 11:03:19 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!First, see how GitLab lost 300GB of production data and look into what it takes to build your data center. We also share the findings from six months of research into OAuth application attacks, update you on Docker Hub’s new limits, explain Cortex Cloud’s merge with Prisma Cloud, and show how monday.com is managing its trace volume.Discover how to secure thousands of AWS accounts in our featured podcast without slowing down developers from Netflix’s cloud security engineer.In the tutorials section, learn how to replace Docker Compose with Quadlet for servers and organize Terraform code for better scalability to refactoring code with GitHub Copilot, understand Azure Data Transfer pricing, fix AWS Serverless image handlers, set up cross-region disaster recovery on AWS, explore multi-cluster fault tolerance with k8gb, storing Terraform state in Azure, and even a look at HTTP3.We also highlight some cool open-source devtools:•  – like Wireshark for Docker, letting you see all network requests.•  – a handy script to switch from Docker to Podman.•  – a tool to track which Chrome extensions make suspicious DNS requests.All this and more in this week’s DevOps Bulletin—don’t miss out! is an open-source project management platform focused on simplicity and efficiency. is Wireshark for your Docker containers. It lets devs see all incoming and outgoing requests to resolve production issues faster. is a small bash script that helps you migrate from Docker to Podman. helps track which Chrome extensions are making suspicious DNS requests. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Wireshark 4.4.4 Released, (Sun, Feb 23rd)</title><link>https://isc.sans.edu/diary/rss/31712</link><author></author><category>infosec</category><pubDate>Sun, 23 Feb 2025 10:38:27 +0000</pubDate><source url="https://isc.sans.edu/">Sans Edu Diaries</source><content:encoded><![CDATA[Wireshark release 4.4.4 fixes 1 vulnerability (%%CVE:2025-1492%%) and 12 bugs.

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>DataWars.io: 7 free Machine Learning projects to practice using Python | DataWars</title><link>https://www.datawars.io/articles/7-free-machine-learning-projects-to-practice-using-python</link><author></author><category>dev</category><category>python</category><pubDate>Sun, 23 Feb 2025 09:48:16 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SVT-AV1 3.0 Released With Faster CPU-Based AV1 Encoding</title><link>https://www.phoronix.com/news/SVT-AV1-3.0-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Feb 2025 09:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[SVT-AV1 as the open-source, CPU-based AV1 encoder that was started by Intel software engineers and now led by the Alliance for Open Media is out this week with the big SVT-AV1 3.0 release. Here's some details on SVT-AV1 3.0 as well as some initial performance benchmarks for this speedy AV1 encoder, especially on modern Intel and AMD processors...]]></content:encoded></item><item><title>Amazon Is Killing the Ability to Download eBooks to Your Computer</title><link>https://news.slashdot.org/story/25/02/23/0529220/amazon-is-killing-the-ability-to-download-ebooks-to-your-computer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Feb 2025 08:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Amazon has long allowed you to download its ebooks to your computer," notes PCMag.com, "where they can serve as a backup or be transferred to other devices. 
"However, that feature will end on February 26, 2025, along with the ability to transfer books from your computer to your Kindle via USB."
If you attempt to download your ebooks right now, a message says: "Starting February 26, 2025, the 'Download & Transfer via USB' option will no longer be available. You can still send Kindle books to your Wi-Fi-enabled devices by selecting the 'Deliver or Remove from Device' option." After February 26, you will still be able to download Kindle books [onto your Kindle] from the Kindle Store via Wi-Fi, and you can also use the Send to Kindle page on Amazon to send a variety of files to your Kindle. 

Should you want to transfer your titles from your Kindle to your computer while you still can, go to Amazon.com, sign in, and click Accounts & Lists > Content Library > Books. Navigate to the book you want to download and click More actions > Download & transfer via USB.
 

Tom's Guide shares their reaction:

Most people probably won't notice this latest example of an Amazon service getting worse, but the feature has existed for over a decade and is useful for backing up your purchases or converting them to formats compatible with other non-Kindle e-Readers or devices. It's also useful for those times when you don't have access to Wi-Fi, and of course, there's peace of mind knowing you have copies of your books... All in all it is a reminder that you don't actually own many or most of your digital purchases, as what you are typically actually "buying" are licenses to use content that can be revoked at any time. 

If you find this decision annoying and want to find alternatives, here are a few. To start, might we recommend the Libby app which lets you borrow ebooks from your local library. You can also borrow audiobooks... You can also try purchasing books from places like Google Books and Apple Books, both of which offer a number of ebooks. eBooks.com offers DRM free books and EPUB formats. For those looking for free ebooks there is always Project Gutenberg which has over 75,000 free books largely those in the public domain though there are some more recent titles as well.
]]></content:encoded></item><item><title>Half-Life</title><link>https://www.filfre.net/2024/12/half-life/</link><author>dmazin</author><category>hn</category><pubDate>Sun, 23 Feb 2025 08:18:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Around twenty years ago, people would have laughed if you told them that videogames would end up at the Smithsonian, but the Half-Life team really did want to make games that were more than just throwaway toys. The rule against cinematics — which made our jobs much harder and also ended up leaving a lot of my favorite work out of the game — was a kind of ideological stake in the ground: we really did want the game and the story to be the same thing. It was far from flawless, but it was really trying to push the boundaries of a young medium.— Valve artist Steve TheodoreBy 1998, the first-person shooter was nearing its pinnacle of popularity. In June of that year,  magazine could list fourteen reasonably big-budget, high-profile FPS’s earmarked for release in the next six months alone. And yet the FPS seemed rather to be running to stand still. Almost all of the innovation taking place in the space was in the realm of technology rather than design.To be sure, progress in the former realm was continuing apace. Less than five years after id Software had shaken the world with , that game’s low-resolution 2.5D graphics and equally crude soundscapes had become positively quaint. Aided and abetted by a fast-evolving ecosystem of 3D-graphics hardware from companies like 3Dfx, id’s  engine had raised the bar enormously in 1996; ditto  in 1997. These were the cutting-edge engines that everyone with hopes of selling a lot of shooters scrambled to license. Then, in May of 1998, with  not scheduled for release until late the following year, Epic MegaGames came along with their own  engine, boasting a considerably longer bullet list of graphical effects than . In thus directly challenging id’s heretofore unquestioned supremacy in the space,  ignited a 3D-graphics arms race that seemed to promise even faster progress in the immediate future.Yet whether they sported the name  or  or something else on their boxes, single-player FPS’s were still content to hew to the “shooting gallery” design template laid out by . You were expected to march through a ladder-style campaign consisting of a set number of discrete levels, each successive one full of more and more deadly enemies to kill than the last, perhaps with some puzzles of the lock-and-key or button-mashing stripe to add a modicum of variety. These levels were joined together by some thread of story, sometimes more elaborate and sometimes — usually — less so, but so irrelevant to what occurred inside the levels that impatient gamers could and sometimes did skip right over the intervening cutscenes or other forms of exposition in order to get right back into the action.This was clearly a model with which countless gamers were completely comfortable, one which had the virtue of allowing them maximal freedom of choice: follow along with the story or ignore it, as you liked. Or, as id’s star programmer John Carmack famously said: “Story in a game is like a story in a porn movie. It’s expected to be there, but it’s not that important.”But what if you could build the story right into the gameplay, such that the two became inseparable? What if you could eliminate the artificial division between exposition and action, and with it the whole conceit of a game as a mere series of levels waiting to be beaten one by one? What if you could drop players into an open-ended space where the story was taking place all around them?This was the thinking that animated an upstart newcomer to the games industry that went by the name of Valve L.L.C. The game that resulted from it would prove the most dramatic conceptual advance in the FPS genre since , with lessons and repercussions that reached well beyond the borders of shooter country.Mike Harrington, who left Valve in 2000 because there were other fish in the sea.The formation of Valve was one of several outcomes of a dawning realization inside the Microsoft of the mid-1990s that computer gaming was becoming a very big business. The same realization led a highly respected Microsoft programmer named Michael Abrash to quit his cushy job in Redmond, Washington, throw his tie into the nearest trashcan, and move to Mesquite, Texas, to help John Carmack and the other scruffy id boys make . It led another insider named Alex St. John to put together the internal team who made DirectX, a library of code that allowed game developers and players to finally say farewell to creaky old MS-DOS and join the rest of the world that was running Windows 95. It led Microsoft to buy an outfit called Ensemble Studios and their promising real-time-strategy game  as a first wedge for levering their way into the gaming market as a publisher of major note. And it led to Valve Corporation.In 1996, Valve’s future co-founders Gabe Newell and Mike Harrington were both valued employees of Microsoft. Newell had been working there in project-management roles since 1983; he had played an important part in the creation of the early versions of Windows before moving on to Microsoft Office and other high-profile applications. Harrington was a programmer whose tenure had been shorter, but he had made significant contributions to Windows NT, Microsoft’s business- and server-oriented operating system.As Newell tells the tale, he had an epiphany when he was asked to commission a study to find out just how many computers were currently running Microsoft Windows in the United States. The number of 20 million that he got back was impressive. Yet he was shocked to learn that Windows wasn’t the most popular single piece of software to be found on American personal computers; that was rather a game called . Newell and Harrington had long enjoyed playing games. Now, it seemed, there were huge piles of money to be earned from making them. Doing so struck them as a heck of a lot more fun than making more operating systems and business software. It was worth a shot, at any rate; both men were wealthy enough by now that they could afford to take a flier on something completely different.So, on August, 24, 1996, the pair quit Microsoft to open an office for their new company Valve some five miles away in Kirkland, Washington. At the time, no one could have imagined — least of all them — what a milestone moment this would go down as in the history of gaming. “On the surface, we should have failed,” says Gabe Newell. “Realistically, both Mike and I thought we would get about a year into it, realize we’d made horrible mistakes, and go back to our friends at Microsoft and ask for our jobs back.”Be that as it may, they did know what kind of game they wanted to make. Or rather Newell did; from the beginning, he was the driving creative and conceptual force, while Harrington focused more on the practical logistics of running a business and making quality software. Like so many others in the industry he was entering, Newell wanted to make a shooter. Yet he wanted his shooter to be more immersive and encompassing of its player than any of the ones that were currently out there, with a story that was embedded right into the gameplay rather than standing apart from it. Valve wasted no time in licensing id’s  engine to bring these aspirations to life, via the game that would come to be known as .As the id deal demonstrates, Newell and Harrington had connections and financial resources to hand that almost any other would-be game maker would have killed for; both had exited Microsoft as millionaires several times over. Yet they had no access to gaming distribution channels, meaning that they had to beat the bush for a publisher just like anyone else with a new studio. They soon found that their studio and their ambitious ideas were a harder sell than they had expected. With games becoming a bigger business every year, there were a lot of deep-pocketed folks from other fields jumping into the industry with plans to teach all of the people who were already there what they were doing wrong; such folks generally had no clue about what it took to do games right. Seasoned industry insiders had a name for these people, one that was usually thoroughly apt: “Tourists.” At first glance, the label was easy to apply to Newell and Harrington and Valve. Among those who did so was Mitch Lasky, then an executive at Activision, who would go on to become a legendary gaming venture capitalist. He got “a whiff of tourism” from Valve, he admits. He says he still has “post-traumatic stress disorder” over his decision to pass on signing them to a publishing deal — but by no means was he the only one to do so.Newell and Harrington finally wound up pitching to Sierra, a publisher known primarily for point-and-click adventure games, a venerable genre that was now being sorely tested by all of the new FPS’s. In light of this, Sierra was understandably eager to find a horse of the new breed to back. The inking of a publishing deal with Valve was one of the last major decisions made by Ken Williams, who had founded Sierra in his living room back in 1980 but was now in the process of selling the business he had built from the ground up. As a man with such deep roots in adventure games, he found Valve’s focus on story both refreshing and appealing. Still, there was a lot of wrangling between the parties, mainly over the ultimate disposition of the rights to the  name; Williams wanted them to go to Sierra, but Newell and Harrington wanted to retain them for themselves. In the end, with no other publishers stepping up to the plate, Valve had to accept what Sierra was offering, a capitulation that would lead to a lengthy legal battle a few years down the road. For now, though, they had their publisher.As for Ken Williams, who would exit the industry stage left well before  was finished:Now that I’m retired, people sometimes ask me what I used to do. I usually just say, “I had a game company back in the old days.” That inevitably causes them to say, “Did you make any games I might have heard of?” I answer, “Leisure Suit Larry.” That normally is sufficient, but if there is no glimmer of recognition I pull out the heavy artillery and say, “Half-Life.” Unless they’ve been sleeping under a rock for the last couple of decades, that always lights up their eyes.One can imagine worse codas to a business career…In what could all too easily be read as another sign of naïve tourism, Newell and Harrington agreed to a crazily optimistic development timeline, promising a finished game for the Christmas of 1997, which was just one year away. To make it happen, they hired a few dozen level designers, programmers, artists, and other creative and technical types, many of whom had no prior professional experience in the games industry, all of whom recognized what an extraordinary opportunity they were being handed and were willing to work like dogs to make the most of it. The founders tapped a fertile pool of recruits in the online  and  modding scenes, where amateurs were making names for themselves by bending those engines in all sorts of new directions. They would now do the same on a professional basis at Valve, even as the programmers modified the  engine itself to suit their needs, implementing better lighting and particle effects, and adding scripting and artificial-intelligence capabilities that the straightforward run-and-shoot gameplay in which id specialized had never demanded. Gabe Newell would estimate when all was said and done that 75 percent of the code in the engine had been written or rewritten at Valve.In June of 1997, Valve took  to the big E3 trade show, where it competed for attention with a murderers’ row of other FPS’s, including early builds of , , , , and . Valve didn’t even have a booth of their own at the show. Nor were they to be found inside Sierra’s;  was instead shown in the booth of 3Dfx. Like so many of Valve’s early moves, this one was deceptively clever, because 3Dfx was absolutely huge at the time, with as big a buzz around their hardware as id enjoyed around their software.  walked away from the show with the title of “Best Action Game.”The validation of E3 made the unavoidable moment of reckoning that came soon after easier to stomach. I speak, of course, about the moment when Valve had to recognize that they didn’t have a ghost of a chance of finishing the game that they wanted to make within the next few months. Newell and Harrington looked at the state of the project and decided that they could probably scrape together a more or less acceptable but formulaic shooter in time for that coming Christmas. Or they could keep working and end up with something amazing for the  Christmas. To their eternal credit, they chose the latter course, a decision which was made possible only by their deep pockets. For Sierra, who were notorious for releasing half-finished games, certainly did not intend to pay for an extra year of development time. The co-founders would have to foot that bill themselves. Nevertheless, to hear Gabe Newell tell it today, it was a no-brainer: “Late is just for a little while. Suck is forever.”The anticipation around  didn’t diminish in the months that followed, not even after the finished  took the world by storm in May of 1998. Despite being based on a two-plus-year-old engine in a milieu that usually prized the technologically new and shiny above all else, Valve’s “shooter with a story” had well and truly captured the imaginations of gamers. During the summer of 1998, a demo of the game consisting of the first three chapters — including the now-iconic opening scenes, in which you ride a tram into a secret government research facility as just another scientist on the staff headed for another day on the job — leaked out of the offices of a magazine to which it had been sent. It did more to promote the game than a million dollars worth of advertising could have; the demo spread like wildfire online, raising the excitement level to an even more feverish pitch.  was different enough to have the frisson of novelty in the otherwise homogeneous culture of the FPS, whilst still being readily identifiable  an FPS. It was the perfect mix of innovation and familiarity.So, it was no real surprise when the full game turned into a massive hit for Valve and Sierra after its release on November 19, 1998. The magazines fell all over themselves to praise it. , normally the closest thing the hype-driven journalism of gaming had to a voice of sobriety, got as high on s supply as anyone. The magazine’s long-serving associate editor Jeff Green took it upon himself to render the official verdict.Everything you’ve heard, everything you’ve hoped for — it’s all true. Half-Life, Valve Software’s highly anticipated first-person shooter, is not just one of the best games of the year. It’s one of the best games of any year, an instant classic that is miles better than any of its immediate competition, and, in its single-player form, is the best shooter since DOOM. Plus, despite the fact that it’s “just” a shooter, Half-Life provides one of the best examples ever of how to present an interactive movie — and a great, scary movie at that. sold its first 200,000 copies in the United States before Christmas — i.e., before glowing reviews like the one above even hit the newsstands. But this was the barest beginning to its success story. In honor of its tenth birthday in 2008, Guinness of world-records fame would formally anoint  as the best-selling single FPS in history, with total sales in the neighborhood of 10 million copies across all platforms and countries. For Newell and Harrington, it was one hell of a way to launch a game-development studio. For Sierra, who in truth had done very little for  beyond putting it in a box and shipping it out to stores, it was a tsunami of cash that seemed to come out of nowhere, the biggest game they had ever published almost by an order of magnitude. One does hope that somebody in the company’s new management took a moment to thank Ken Williams for this manna from heaven. has come to occupy such a hallowed, well-nigh sacrosanct position in the annals of gaming that any encounter with the actual artifact today seems bound to be slightly underwhelming. Yet even when we take into account the trouble that any game would have living up to a reputation as elevated as this one’s, the truth is that there’s quite a lot here for the modern skeptical critic to find fault with — and, Lord knows, this particular critic has seldom been accused of lacking in skepticism.Judged purely as a shooter, the design shows its age. It’s sometimes amazingly inspired, but more often no better than average for its time. There’s a lot of crawling through anonymous vents that serve no real purpose other than to extend the length of the game, a lot of places where you can survive only by dying first so as to learn what’s coming, a lot of spots where it’s really not clear at all what the game wants from you. And then there are an  lot of jumping puzzles, shoehorned into a game engine that has way more slop in it than is ideal for such things. I must say that I had more unadulterated fun with LucasArts’s , the last shooter I played all the way through for these histories, than I did with . There the levels are constructed like thrill rides straight out of the  films, with a through-line that seems to just intuitively come to you; looking back, I’m still in awe of their subtle genius in this respect.  is not like that. You really have to work to get through it, and that’s not as appealing to me.Then again, my judgment on these things should, like that of any critic, be taken with a grain of salt. Whether you judge a game good or bad or mediocre hinges to a large degree on what precisely you’re looking for from it; we’ve all read countless negative reviews reflective not so much of a bad game as one that isn’t the game that that reviewer wanted to play. Personally, I’m very much a tourist in the land of the FPS. While I understand the appeal of the genre, I don’t want to expend too many hours or too much effort on it. I want to blast through a fun and engaging environment without too much friction. Make me feel like an awesome action hero while I’m at it, and I’ll probably walk away satisfied, ready to go play something else.  on easy mode gave me that experience;  did not, demanding a degree of careful attention from me that I wasn’t always eager to grant it. If you’re more hardcore about this genre than I am, your judgment of the positives and negatives in these things may very well be the opposite of mine. Certainly  is more typical of its era than  — an era when games like this were still accepted and even expected to be harder and more time-consuming than they are today. C’est la vie et vive la différence!But of course, it wasn’t the granular tactical details of the design that made  stand out so much from the competition back in the day. It was rather its brilliance as a storytelling vehicle that led to its legendary reputation. And don’t worry, you definitely won’t see me quibbling that said reputation isn’t deserved. Even here, though, we do need to be sure that we understand exactly what it did and did not do that was so innovative at the time.Contrary to its popular rep then and now,  was by no means the first “shooter with a story.” Technically speaking, even  has a story, some prattle about a space station and a portal to Hell and a space marine who’s the only one that can stop the demon spawn. The story most certainly isn’t , but it’s there. wasn’t even the shooter at the time of its release with the inarguably best or most complicated story. LucasArts makes a strong bid for the title there. Both  and the aforementioned , released in 1995 and 1997 respectively, weave fairly elaborate tales into the fabric of the existing  universe, drawing on its rich lore, inserting themselves into the established chronology of the original trilogy of films and the “Expanded Universe” series of  novels.Like that of many games of this era, s story betrays the heavy influence of the television show , which enjoyed its biggest season ever just before this game was released. We have the standard nefarious government conspiracy involving extraterrestrials, set in the standard top-secret military installation somewhere in the Desert Southwest. We even have a direct equivalent to Cancer Man, s shadowy, nameless villain who is constantly lurking behind the scenes. “G-Man” does the same in ; voice actor Michael Shapiro even opted to give him a “lizard voice” that’s almost a dead ringer for Cancer Man’s nicotine-addled croak.All told, s story is more of a collection of tropes than a serious exercise in fictional world-building. To be clear, the sketchiness is by no means an automatically bad thing, not when it’s judged in the light of the purpose the story actually needs to serve. Mark Laidlaw, the sometime science-fiction novelist who wrote the script, makes no bones about the limits to his ambitions for it. “You don’t have to write the whole story,” he says. “Because it’s a conspiracy plot, everybody knows more about it than you do. So you don’t have to answer those questions. Just keep raising questions.”Once the shooting starts, plot-related things happen, but it’s all heat-of-the-moment stuff. You fight your way out of the complex after its been overrun by alien invaders coming through a trans-dimensional gate that’s been inadvertently opened, only to find that your own government is now as bent on killing you as the aliens are in the name of the disposal of evidence. Eventually, in a plot point weirdly reminiscent of , you have to teleport yourself onto the aliens’ world to shut down the portal they’re using to reach yours.Suffice to say that, while  may be slightly further along the continuum toward  than  is, it’s only slightly so. Countless better, richer, deeper stories were told in games before this one came along. When people talk about  as “the FPS with a story,” they’re really talking about something more subtle: about its way of  its story. Far from diminishing the game, this makes it more important, across genres well beyond the FPS. The best way for us to start to come to grips with what  did that was so extraordinary might be to look back to the way games were deploying their stories before its arrival on the scene.Throughout the 1980s, story in games was largely confined to the axiomatically narrative-heavy genres of the adventure game and the CRPG. Then, in 1990, Origin Systems released Chris Roberts’s , a game which was as revolutionary in the context of its own time as  was in its. In terms of gameplay,  was a “simulation” of outer-space dog-fighting, not all that far removed in spirit from the classic . What made it stand out was what happened when you weren’t behind the controls of your space fighter. Between missions, you hung out in the officers’ lounge aboard your mother ship, collecting scuttlebutt from the bartender, even flirting with the fetching female pilot in your squadron. When you went into the briefing room to learn about your next mission, you also learned about the effect your last one had had on the unfolding war against the deadly alien Kilrathi, and were given a broader picture of the latest developments in the conflict that necessitated this latest flight into danger. The missions themselves remained shooting galleries, but the story that was woven around them gave them resonance, made you feel like you were a part of something much grander. Almost equally importantly, this “campaign” provided an easy way to structure your time in the game and chart your improving skills; beat all of the missions in the campaign and see the story to its end, and you could say that you had mastered the game as a whole.People  this;  became by far the most popular computer-gaming franchise of the young decade prior to the smashing arrival of  at the end of 1993. The approach it pioneered quickly spread across virtually all gaming genres. In particular, both the first-person-shooter and the real-time strategy genres — the two that would dominate over all others in the second half of the decade — adopted it as their model for the single-player experience. Even at its most rudimentary, a ladder-style campaign gave you a goal to pursue and a framework of progression to hang your hat on.Yet the same approach created a weirdly rigid division between gameplay and exposition, not only on the playing side of the ledger but to a large extent on the development side as well. It wasn’t unusual for completely separate teams to be charged with making the gameplay part of a game and all of the narrative pomp and circumstance that justified it. The disconnect could sometimes verge on hilarious; in , which went so far as to film real humans acting out a B-grade  movie between its levels, the protagonist has a beard in the cutscenes but is clean-shaven during the levels. By the late 1990s, the pre-rendered-3D or filmed-live-action cutscenes sometimes cost more to produce than the game itself, and almost always filled more of the space on the CD.As he was setting up his team at Valve, Gabe Newell attempted to eliminate this weird bifurcation between narrative and gameplay by passing down two edicts to his employees, the only non-negotiable rules he would ever impose upon them.  had to have a story — not necessarily one worthy of a film or a novel, but one worthy of the name. And at the same time, it couldn’t ever, under any conditions, from the very first moment to the very last, take control out of the hands of the player. Everything that followed cascaded from these two simple rules, which many a game maker of the time would surely have seen as mutually contradictory. To state the two most obvious and celebrated results, they meant no cutscenes whatsoever and no externally imposed ladder of levels to progress through — for any sort of level break did mean taking control out of the hands of the player, no matter how briefly.Adapting to such a paradigm the  engine, which had been designed with a traditional FPS campaign in mind, proved taxing but achievable. Valve set up the world of  as a spatial grid of “levels” that were now better described as zones; pass over a boundary from one zone into another, and the new one would be loaded in swiftly and almost transparently. Valve kept the discrete zones small so as to minimize the loading times, judging more but shorter loading breaks to be better than fewer but longer ones. The hardest part was dealing with the borderlands, so to speak; you needed to be able to look into one zone from another, and the enemies and allies around you had to stay consistent before and after a transition. But Valve managed even this through some clever technical sleight of hand — such as by creating overlapping areas that existed in both of the adjoining sets of level data — and through more of the same on the design side, such as by placing the borders whenever possible at corners in corridors and at other spots where the line of sight didn’t extend too far. The occasional brief loading message aside — and they’re  brief, or even effectively nonexistent, on modern hardware —  really does feel like it all takes place in the same contiguous space.
Every detail of  has been analyzed at extensive, exhaustive length over the decades since its release. Such analysis has its place in fan culture, but it can be more confusing than clarifying when it comes to appreciating the game’s most important achievements. The ironic fact is that you can learn almost everything that really matters about  as a game design just by playing it for an hour or so, enough to get into its third chapter. Shall we do so together now? hews to Gabe Newell’s guiding rules from the moment you click the “New Game” button on the main menu and the iconic tram ride into the Black Mesa Research Center begins. The opening credits play over this sequence, in which you are allowed to move around and look where you like. There are reports that many gamers back in the day didn’t actually realize that they were already in control of the protagonist — reports that they just sat there patiently waiting for the “cutscene” to finish, so ingrained was the status quo of bifurcation.The protagonist himself strikes an artful balance between being an undefined player stand-in — what  called an “AFGNCAAP,” or “Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person” — and a fully fleshed-out character. As another result of Newell’s guiding rules, you never see him in the game unless you look in a mirror; you only see the world through his eyes. You do, however, hear security guards and colleagues refer to him — or, if you like, to you — as “Gordon” or “Mr. Freeman.” The manual and the intertitles that appear over the opening sequence of the game explain that his full name is indeed Gordon Freeman, and that he’s a 27-year-old theoretical physicist with a PhD from MIT who has been recently hired to work at Black Mesa. The game’s loading screen and its box art show us a rather atypical FPS protagonist, someone very different from the muscle-bound, cigar-chomping Duke Nukem or the cocky budding Jedi knight Kyle Katarn: a slim, studious-looking fellow with Coke-bottle eyeglasses and a token goatee. The heart of the computer-gaming demographic being what it was in 1998, he was disarmingly easy for many of the first players of  to identify with, thus adding just that one more note of immersion to the symphony. Small wonder that he has remained a favorite with cosplayers for decades. In fact, cipher though he almost entirely is, Gordon Freeman has become one of the most famous videogame characters in history.The tram eventually arrives at its destination and a security guard welcomes you to the part of the complex where you work: “Morning, Mr. Freeman. Looks like you’re running late.” Passing through the double blast doors, you learn from your colleagues inside that it’s already been a rough morning: the main computer has crashed, which has thrown a wrench into an important test that was planned for today. Mind you, you don’t learn this through dialog menus, which Valve judged to qualify as taking control away from the player. You can’t speak at all, but if you approach the guards and scientists, they’ll say things to you, leaving you to imagine your own role in the conversation. Or you can stand back and listen to the conversations they have with one another.You can wander around as you like in this office area. You can look in Gordon’s locker to learn a little bit more about him, buy a snack from the vending machine, even blow it up by microwaving it for too long. (“My God!” says your colleague in reaction. “What are you doing?”) All of this inculcates the sense of a lived-in workspace better than any amount of external exposition could have done, setting up a potent contrast with the havoc to come.When you get bored fooling around with lockers and microwaves, you put on your hazardous-environment suit and head down to where the day’s test is to be conducted. It isn’t made clear to you the player just what the test is meant to accomplish; it isn’t even clear that Gordon himself understands the entirety of the research project to which he’s been assigned. All that matters is that the test goes horribly wrong, creating a “resonance cascade event” that’s accompanied by a lot of scary-looking energy beams flying through the air and explosions popping off everywhere. You’ve now reached the end of the second chapter without ever touching a weapon. But that’s about to change, because you’re about to find out that hostile alien lifeforms are now swarming the place. “Get to the surface as soon as you can and let someone know we’re stranded down here!” demand your colleagues. So, you pick up the handy crowbar you find lying on the floor and set off to batter a path through the opposition.This was a drill with which 1990s shooter fans were a lot more familiar, but there are still plenty of new wrinkles. The scientists and guards who were present in the complex before all hell broke loose don’t just disappear. They’re still around, mostly cowering in the corners in the case of the former, doing their best to fight back in that of the latter. The scientists sometimes have vital information to share, while the guards will join you as full-blown allies, firing off their pop-gun pistols at your side, although they tend not to live very long. Allies were a new thing under the FPS sun in 1998, an idea that would quickly spread to other games. (Ditto the way that the guards here are almost better at shooting you in the back than they are at shooting the rampaging aliens. The full history of “allies” in the FPS genre is a fraught one…)As you battle your way up through the complex, you witness plenty of pre-scripted scenes to go along with the emergent behavior of the scientists, guards, and aliens. Ideally, you won’t consciously notice any distinction between the two. You see a scientist being transformed into a zombie by an alien “head crab” behind the window of his office; see several hapless sad sacks tumbling down an open elevator shaft; see a dying guard trying and just failing to reach a healing station. These not only add to the terror and drama, but sometimes have a teaching function. The dying guard, for example, points out to you the presence of healing stations for ensuring that you don’t come to share his fate.It’s the combination of emergent and scripted behaviors, on the part of your enemies and even more on that of your friends, that makes  come so vividly alive. I’m tempted to use the word “realism” here, but I know that Gabe Newell would rush to correct me if I did. Realism, he would say, is boring. Realistically, a guy like Gordon Freeman — heck, even one like Duke Nukem — wouldn’t last ten minutes in a situation like this one. Call it verisimilitude instead, a sign of a game that’s absolutely determined to stay true to its fictional premise, never mind how outlandish it is. The world  presents really is a living one; Newell’s rule of thumb was that five seconds should never pass without something happening near the player. Likewise, the world has to react to anything the player does. “If I shoot the wall, the wall should change, you know?” Newell said. “Similarly, if I were to throw a grenade at a grunt, he should react to it, right? I mean, he should run away from it or lay down on the ground and duck for cover. If he can’t run away from it, he should yell ‘Shit!’ or ‘Fire in the hole!’ or something like that.” In , he will indeed do one or all of these things.The commitment to verisimilitude means that most of what you see and hear is, to use the language of film, diegetic, or internal to the world as Gordon Freeman is experiencing it. Even the onscreen HUD is the one that Gordon is seeing, being the one that’s built into his hazard suit. The exceptions to the diegetic rule are few: the musical soundtrack that plays behind your exploits; the chapter names and titles which flash on the screen from time to time; those credits that are superimposed over the tram ride at the very beginning. These exceptions notwithstanding, the game’s determination to immerse you in an almost purely diegetic sensory bubble is the reason I have to strongly differ with Jeff Green’s description of  as an “interactive movie.” It’s actually the polar opposite of such a stylized beast. It’s an exercise in raw immersion which seeks to eliminate any barriers between you and your lived experience rather than making you feel like you’re doing anything so passive as watching or even guiding a movie. One might go so far as to take  as a sign that gaming was finally growing up and learning to stand on its own two feet by 1998, no longer needing to take so many of its cues from other forms of media.We’ve about reached the end of our hour in  now, so we can dispense with the blow-by-blow. This is not to say that we’ve seen all the game has to offer. Betwixt and between the sequences that I find somewhat tedious going are more jaw-dropping dramatic peaks: the moment when you reach the exit to the complex at long last, only to learn that the United States Army wants to terminate rather than rescue you; the moment when you discover a tram much like the one you arrived on and realize that you can drive it through the tunnels; the moment when you burst out of the complex completely and see the bright blue desert sky above. (Unfortunately, it’s partially blotted out by a big Marine helicopter that also wants to kill you).In my opinion,  could have been an even better game if it had been about half as long, made up of only its most innovative and stirring moments — “all killer, no filler,” as they used to say in the music business. Alas, the marketplace realities of game distribution in the late 1990s militated against this. If you were going to charge a punter $40 or $50 for a boxed game, you had to make sure it lasted more than six or seven hours. If  was being made today, Valve might very well have made different choices.Again, though, mileages will vary when it comes to these things. The one place where  does fall down fairly undeniably is right at the end. Your climactic journey into Xen, the world of the aliens, is so truncated by time and budget considerations as to be barely there at all, being little more than a series of (infuriating) jumping puzzles and a couple of boss fights. Tellingly, it’s here that  gives in at last and violates its own rules of engagement, by delivering — perish the thought! — a cutscene containing the last bits of exposition that Valve didn’t have time to shoehorn into their game proper. The folks from Valve almost universally name the trip to Xen as their biggest single regret, saying they wish they had either found a way to do it properly or just saved it for a sequel. Needless to say, I can only concur.Yet the fact remains that  at its best is so audacious and so groundbreaking that it almost transcends such complaints. Its innovations have echoed down across decades and genres. We’ll be bearing witness to that again and again in the years to come as we continue our journey through gaming history. Longtime readers of this site will know that I’m very sparing in my use of words like “revolutionary.” But I feel no reluctance whatsoever to apply the word to this game.Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like. The books Rocket Jump: Quake and the Golden Age of First-Person Shooters by David L. Craddock, Masters of DOOM: How Two Guys Created an Empire and Transformed Pop Culture by David Kushner, Not All Fairy Tales Have Happy Endings: The Rise and Fall of Sierra On-Line by Ken Williams, and Game Design: Theory & Practice (2nd edition) by Richard Rouse III.  149; the  special issue “Trigger Happy”;  of December 1998, April 1999, and June 1999;  of June 1998, December 1998, and February 1999; Sierra’s newsletter  of Fall 1997;  of September 1998.]]></content:encoded></item><item><title>Mesa&apos;s Venus Now Exposes Vulkan 1.4 Support</title><link>https://www.phoronix.com/news/Mesa-Venus-Vulkan-1.4</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Feb 2025 08:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Mesa Venus driver code for use with VirtIO-GPU for exposing accelerated Vulkan API support within virtualized environments (VMs) now is advertising Vulkan 1.4 API support...]]></content:encoded></item><item><title>The TechBeat: The Stupidest Requests on the Dark Web Come from Regular People (2/23/2025)</title><link>https://hackernoon.com/2-23-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sun, 23 Feb 2025 07:10:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @rootstock_io [ 3 Min read ] 
 Rootstock merges Bitcoin’s security with Ethereum’s flexibility, enabling AI-driven blockchain apps for trustless governance, security, and fraud detection. Read More.By @stellar [ 5 Min read ] 
 Regulatory shifts in 2025 will shape crypto wallets. Learn how compliance, DeFi, and Stellar’s Soroban ecosystem will impact the future of Web3 wallets. Read More.By @buzzpy [ 7 Min read ] 
 This article is dedicated to any developer who wants to try something new or exciting (which is game development, yes). Read More.By @noda [ 4 Min read ] 
 2025 is the tipping point for pay-by-bank. Lower fees, instant payments, and new regulations make it the future of digital transactions.  Read More.By @mexcmedia [ 7 Min read ] 
 Altcoin season sees altcoins outperform Bitcoin, offering lucrative opportunities. Learn key signs, strategies, & how to benefit from MEXC’s diverse offerings. Read More.By @2077research [ 11 Min read ] 
 Explore how multidimensional EIP-1559 updates Ethereum's gas efficiency by separating resource costs, enhancing scalability, and improving network utilization. Read More.By @blackheart [ 1 Min read ] 
 As someone who’s spent time digging through dark web marketplaces, forums, and Telegram groups, I’ve seen it all. Here’s what no one tells you. Read More.By @mexcmedia [ 6 Min read ] 
 Liquidity is key to crypto trading, ensuring price stability, seamless transactions, and reduced slippage. Learn how MEXC excels in liquidity management. Read More.By @hayday [ 4 Min read ] 
 Is the rise of vibe coding also the end of software engineering? How will vibeware change the nature of the software entrepreneur, and the meaning of work? Read More.By @2077research [ 11 Min read ] 
 Explore the evolution of crypto options and perpetual futures, diving into innovations like panoptions, liquidity challenges, and decentralized trading. Read More.By @mexcmedia [ 5 Min read ] 
 Discover key crypto trends of 2025, from Bitcoin’s surge to MEXC’s role in shaping the future of digital asset trading with liquidity, security, & innovation. Read More.By @proflead [ 5 Min read ] 
 In this guide, I will walk you through a method using GitHub and Git that allows you to keep your notes in sync without spending a dime. Read More.By @alexandersimonov [ 6 Min read ] 
 AI and cats can be random. Learn why AI isn’t always deterministic, how stochastic processes shape its decisions, and why it self-corrects and hallucinates. Read More.By @linearization [ 6 Min read ] 
 This study evaluates 44 DPFL methods, comparing accuracy, efficiency, and generalizability in power system computations. Read More.By @oleksiijko [ 13 Min read ] 
 The project is built on the principle of microservice architecture, which allows you to divide functionality into independent services.  Read More.By @antonvoichenkovokrug [ 4 Min read ] 
 While the whole world watches with interest as ChatGPT and OpenAI progress toward creating AGI, Nirvanic has announced  an even more ambitious goal. Read More.By @ekaterinaandreeva [ 15 Min read ] 
 How to level up your IT career? A step-by-step guide, proven career tracks, and practical checklists for confident growth – read the full article Read More.By @dataengonline [ 6 Min read ] 
 Firms increasingly make use of artificial intelligence (AI) infrastructures to host and manage autonomous workloads. Read More.]]></content:encoded></item><item><title>Lithium Batteries Reignited Tuesday at the Moss Landing Power Plant Fire Site</title><link>https://hardware.slashdot.org/story/25/02/23/039220/lithium-batteries-reignited-tuesday-at-the-moss-landing-power-plant-fire-site?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Feb 2025 05:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Remember that battery plant fire last month in Moss Landing, California? Tuesday night local firefighters "determined that a group of lithium batteries in an area that had previously burned during the January 16 fire had smoldered and reignited," reports SFGate. 

 Fire Chief Joel Mendoza said the flames burned at varying intensities throughout Tuesday night before the fire burned itself out at about 8 a.m. on Wednesday.

Additional flare-ups at the site are expected due to weather exposure and damage to the remaining batteries. "Rekindling is very, very likely — almost a certainty," said EPA onsite coordinator Eric Sandusky, adding that rain and humidity can interact with the damaged batteries, leading to short circuits and reignition. To further reduce fire risk, Sandusky said the EPA is working with Vistra to begin "de-linking the batteries," a process that disconnects them to lower the risk of propagation and prevent a large-scale fire... 

"Vistra said that since the January 16 fire, they have brought in a private fire crew that is on-site at all times to monitor the Moss 300 building," according to a local news site. 

 Fire Chief Joel Mendoza shared more details with the digital newspaper Lookout Santa Cruz. "We've been saying all along that batteries exposed to heat that didn't burn can ignite. We were hoping that it wouldn't happen, but it did."]]></content:encoded></item><item><title>AI May Not Impact Tech-Sector Employment, Projects US Department of Labor</title><link>https://it.slashdot.org/story/25/02/23/0034221/ai-may-not-impact-tech-sector-employment-projects-us-department-of-labor?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Feb 2025 02:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[America's Labor Department includes the fact-finding Bureau of Labor Statistics — and they recently explained how AI impacts their projections for the next 10 years. Their conclusion, writes Investopedia, was that "tech workers might not have as much to worry about as one might think."

Employment in the professional, scientific, and technical services sector is forecast to increase by 10.5% from 2023 to 2033, more than double the national average. According to the BLS, the impact AI will have on tech-sector employment is highly uncertain. For one, AI is adept at coding and related tasks. But at the same time, as digital systems become more advanced and essential to day-to-day life, more software developers, data managers, and the like are going to be needed to manage those systems. "Although it is always possible that AI-induced productivity improvements will outweigh continued labor demand, there is no clear evidence to support this conjecture," according to BLS researchers. 
Their employment projections through 2033 predict the fastest-growing sector within the tech industry will be computer system design, while the fastest-growing occupation will be data scientist. 

And they also project that from 2023 through 2033 AI will "primarily affect occupations whose core tasks can be most easily replicated by GenAI in its current form." So over those 10 years they project a 4.7% drop in employment of medical transcriptionists and a 5.0% drop in employment of customer service representatives.


Other occupations also may see AI impacts, although not to the same extent. For instance, computer occupations may see productivity impacts from AI, but the need to implement and maintain AI infrastructure could in actuality boost demand for some occupations in this group.
 

They also project decreasing employment for paralegals, but with actual lawyers being "less affected."]]></content:encoded></item><item><title>Wine Staging 10.2 Adds Support For AF_UNIX Sockets</title><link>https://www.phoronix.com/news/Wine-Staging-10.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Feb 2025 01:18:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following Friday's release of Wine 10.2, Wine Staging 10.2 is out for testing as this more experimental/leading-edge version of Wine that is shipping with 300+ extra patches for testing...]]></content:encoded></item><item><title>Meta slashes staff stock awards as group embarks on AI spending drive</title><link>https://www.ft.com/content/67a4c030-a7f6-47af-bab0-a998f0a09506</link><author>apical_dendrite</author><category>hn</category><pubDate>Sun, 23 Feb 2025 01:12:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Clang Static Analyzer and the Z3 constraint solver (2022)</title><link>https://www.cambus.net/clang-static-analyzer-and-the-z3-constraint-solver/</link><author>davikr</author><category>hn</category><pubDate>Sun, 23 Feb 2025 01:03:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As far as static analyzers are concerned, one of the most important point
to consider is filtering out false positives as much as possible, in order
for the reports to be actionable.This is an area on which  did an excellent job, and likely a
major reason why they got so popular within the open source community,
despite being a closed-source product.LLVM has the  build option, which allows building
LLVM against the Z3 constraint solver.It is documented as follow:The option is enabled in the Debian 11 package (clang-tools-11), but not
in Fedora 36 or Ubuntu 22.04 ones. I added a build option (not enabled by
default) to the llvm and clang packages in Pkgsrc, and successfully
built Z3 enabled packages on NetBSD.For Pkgsrc users, add the following in , and build :For each method, we can use Clang directly on a given translation
unit or use .The first way is using Z3 as an external constraint solver:This is a lot slower than the default, and the commit which documented
the feature mentions a ~15x slowdown over the built-in constraint solver.The second way is using the default range based solver but having Z3 do
refutation to filter out false positives, which is a lot faster:Again, no bugs found. How boring.We can verify what happens if we run the analyzer without involving Z3 at all:We get a false positive, because the default constraint solver cannot
reason about bitwise operations (among other things), and report an
unreachable NULL pointer dereference.]]></content:encoded></item><item><title>Digital Services Playbook</title><link>https://playbook.usds.gov/</link><author>ronbenton</author><category>hn</category><pubDate>Sun, 23 Feb 2025 00:54:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Understand what people needWe must begin digital projects by exploring and pinpointing the needs of the people who will use the service, and the ways the service will fit into their lives. Whether the users are members of the public or government employees, policy makers must include real people in their design process from the beginning. The needs of people — not constraints of government structures or silos — should inform technical and design decisions. We need to continually test the products we build with real people to keep us honest about what is important.Early in the project, spend time with current and prospective users of the serviceUse a range of qualitative and quantitative research methods to determine people’s goals, needs, and behaviors; be thoughtful about the time spentTest prototypes of solutions with real people, in the field if possibleDocument the findings about user goals, needs, behaviors, and preferencesShare findings with the team and agency leadershipCreate a prioritized list of tasks the user is trying to accomplish, also known as “user stories”As the digital service is being built, regularly test it with potential users to ensure it meets people’s needsWho are your primary users?What user needs will this service address?Why does the user want or need this service?Which people will have the most difficulty with the service?Which research methods were used?What were the key findings?How were the findings documented? Where can future team members access the documentation?How often are you testing with real people?Address the whole experience, from start to finishWe need to understand the different ways people will interact with our services, including the actions they take online, through a mobile application, on a phone, or in person. Every encounter — whether it’s online or offline — should move the user closer towards their goal.Understand the different points at which people will interact with the service – both online and in personIdentify pain points in the current way users interact with the service, and prioritize these according to user needsDesign the digital parts of the service so that they are integrated with the offline touch points people use to interact with the serviceDevelop metrics that will measure how well the service is meeting user needs at each step of the serviceWhat are the different ways (both online and offline) that people currently accomplish the task the digital service is designed to help with?Where are user pain points in the current way people accomplish the task?Where does this specific project fit into the larger way people currently obtain the service being offered?What metrics will best indicate how well the service is working for its users?Make it simple and intuitiveUsing a government service shouldn’t be stressful, confusing, or daunting. It’s our job to build services that are simple and intuitive enough that users succeed the first time, unaided.Use the design style guide consistently for related digital servicesGive users clear information about where they are in each step of the processFollow accessibility best practices to ensure all people can use the serviceProvide users with a way to exit and return later to complete the processUse language that is familiar to the user and easy to understandUse language and design consistently throughout the service, including online and offline touch pointsWhat primary tasks are the user trying to accomplish?Is the language as plain and universal as possible?What languages is your service offered in?If a user needs help while using the service, how do they go about getting it?How does the service’s design visually relate to other government services?Build the service using agile and iterative practicesWe should use an incremental, fast-paced style of software development to reduce the risk of failure. We want to get working software into users’ hands as early as possible to give the design and development team opportunities to adjust based on user feedback about the service. A critical capability is being able to automatically test and deploy the service so that new features can be added often and be put into production easily.Ship a functioning “minimum viable product” (MVP) that solves a core user need as soon as possible, no longer than three months from the beginning of the project, using a “beta” or “test” period if neededRun usability tests frequently to see how well the service works and identify improvements that should be madeEnsure the individuals building the service communicate closely using techniques such as launch meetings, strategy rooms, daily standups, and team chat toolsKeep delivery teams small and focused; limit organizational layers that separate these teams from the business ownersRelease features and improvements multiple times each monthCreate a prioritized list of features and bugs, also known as the “feature backlog” and “bug backlog”Use a source code version control systemGive the entire project team access to the issue tracker and version control systemUse code reviews to ensure qualityHow long did it take to ship the MVP? If it hasn’t shipped yet, when will it?How long does it take for a production deployment?How many days or weeks are in each iteration/sprint?Which version control system is being used?How are bugs tracked and tickets issued? What tool is used?How is the feature backlog managed? What tool is used?How often do you review and reprioritize the feature and bug backlog?How do you collect user feedback during development? How is that feedback used to improve the service?At each stage of usability testing, which gaps were identified in addressing user needs?Structure budgets and contracts to support deliveryTo improve our chances of success when contracting out development work, we need to work with experienced budgeting and contracting officers. In cases where we use third parties to help build a service, a well-defined contract can facilitate good development practices like conducting a research and prototyping phase, refining product requirements as the service is built, evaluating open source alternatives, ensuring frequent delivery milestones, and allowing the flexibility to purchase cloud computing resources.The TechFAR Handbook provides a detailed explanation of the flexibilities in the Federal Acquisition Regulation (FAR) that can help agencies implement this play.Budget includes research, discovery, and prototyping activitiesContract is structured to request frequent deliverables, not multi-month milestonesContract is structured to hold vendors accountable to deliverablesContract gives the government delivery team enough flexibility to adjust feature prioritization and delivery schedule as the project evolvesContract ensures open source solutions are evaluated when technology choices are madeContract specifies that software and data generated by third parties remains under our control, and can be reused and released to the public as appropriate and in accordance with the lawContract allows us to use tools, services, and hosting from vendors with a variety of pricing models, including fixed fees and variable models like “pay-for-what-you-use” servicesContract specifies a warranty period where defects uncovered by the public are addressed by the vendor at no additional cost to the governmentContract includes a transition of services period and transition-out planWhat is the scope of the project? What are the key deliverables?What are the milestones? How frequent are they?What are the performance metrics defined in the contract (e.g., response time, system uptime, time period to address priority issues)?Assign one leader and hold that person accountableThere must be a single product owner who has the authority and responsibility to assign tasks and work elements; make business, product, and technical decisions; and be accountable for the success or failure of the overall service. This product owner is ultimately responsible for how well the service meets the needs of its users, which is how a service should be evaluated. The product owner is responsible for ensuring that features are built and managing the feature and bug backlogs.A product owner has been identifiedAll stakeholders agree that the product owner has the authority to assign tasks and make decisions about features and technical implementation detailsThe product owner has a product management background with technical experience to assess alternatives and weigh tradeoffsThe product owner has a work plan that includes budget estimates and identifies funding sourcesThe product owner has a strong relationship with the contracting officerWho is the product owner?What organizational changes have been made to ensure the product owner has sufficient authority over and support for the project?What does it take for the product owner to add or remove a feature from the service?Bring in experienced teamsWe need talented people working in government who have experience creating modern digital services. This includes bringing in seasoned product managers, engineers, and designers. When outside help is needed, our teams should work with contracting officers who understand how to evaluate third-party technical competency so our teams can be paired with contractors who are good at both building and delivering effective digital services. The makeup and experience requirements of the team will vary depending on the scope of the project.Member(s) of the team have experience building popular, high-traffic digital servicesMember(s) of the team have experience designing mobile and web applicationsMember(s) of the team have experience using automated testing frameworksMember(s) of the team have experience with modern development and operations (DevOps) techniques like continuous integration and continuous deploymentMember(s) of the team have experience securing digital servicesA Federal contracting officer is on the internal team if a third party will be used for development workA Federal budget officer is on the internal team or is a partnerThe appropriate privacy, civil liberties, and/or legal advisor for the department or agency is a partnerChoose a modern technology stackThe technology decisions we make need to enable development teams to work efficiently and enable services to scale easily and cost-effectively. Our choices for hosting infrastructure, databases, software frameworks, programming languages and the rest of the technology stack should seek to avoid vendor lock-in and match what successful modern consumer and enterprise software companies would choose today. In particular, digital services teams should consider using open source, cloud-based, and commodity solutions across the technology stack, because of their widespread adoption and support by successful consumer and enterprise technology companies in the private sector.Choose software frameworks that are commonly used by private-sector companies creating similar servicesWhenever possible, ensure that software can be deployed on a variety of commodity hardware typesEnsure that each project has clear, understandable instructions for setting up a local development environment, and that team members can be quickly added or removed from projectsWhat is your development stack and why did you choose it?Which databases are you using and why did you choose them?How long does it take for a new team member to start contributing?Deploy in a flexible hosting environmentOur services should be deployed on flexible infrastructure, where resources can be provisioned in real-time to meet spikes in traffic and user demand. Our digital services are crippled when we host them in data centers that market themselves as “cloud hosting” but require us to manage and maintain hardware directly. This outdated practice wastes time, weakens our disaster recovery plans, and results in significantly higher costs.Resources are provisioned on demandResources scale based on real-time user demandResources are provisioned through an APIResources are available in multiple regionsWe only pay for resources we useStatic assets are served through a content delivery networkApplication is hosted on commodity hardwareWhere is your service hosted?What hardware does your service use to run?What is the demand or usage pattern for your service?What happens to your service when it experiences a surge in traffic or load?How much capacity is available in your hosting environment?How long does it take you to provision a new resource, like an application server?How have you designed your service to scale based on demand?How are you paying for your hosting infrastructure (e.g., by the minute, hourly, daily, monthly, fixed)?Is your service hosted in multiple regions, availability zones, or data centers?In the event of a catastrophic disaster to a datacenter, how long will it take to have the service operational?What would be the impact of a prolonged downtime window?What data redundancy do you have built into the system, and what would be the impact of a catastrophic data loss?How often do you need to contact a person from your hosting provider to get resources or to fix an issue?Automate testing and deploymentsToday, developers write automated scripts that can verify thousands of scenarios in minutes and then deploy updated code into production environments multiple times a day. They use automated performance tests which simulate surges in traffic to identify performance bottlenecks. While manual tests and quality assurance are still necessary, automated tests provide consistent and reliable protection against unintentional regressions, and make it possible for developers to confidently release frequent updates to the service.Create automated tests that verify all user-facing functionalityCreate unit and integration tests to verify modules and componentsRun tests automatically as part of the build processPerform deployments automatically with deployment scripts, continuous delivery services, or similar techniquesConduct load and performance tests at regular intervals, including before public launchWhat percentage of the code base is covered by automated tests?How long does it take to build, test, and deploy a typical bug fix?How long does it take to build, test, and deploy a new feature into production?How frequently are builds created?What test tools are used?Which deployment automation or continuous integration tools are used?What is the estimated maximum number of concurrent users who will want to use the system?How many simultaneous users could the system handle, according to the most recent capacity test?How does the service perform when you exceed the expected target usage volume? Does it degrade gracefully or catastrophically?What is your scaling strategy when demand increases suddenly?Manage security and privacy through reusable processesOur digital services have to protect sensitive information and keep systems secure. This is typically a process of continuous review and improvement which should be built into the development and maintenance of the service. At the start of designing a new service or feature, the team lead should engage the appropriate privacy, security, and legal officer(s) to discuss the type of information collected, how it should be secured, how long it is kept, and how it may be used and shared. The sustained engagement of a privacy specialist helps ensure that personal data is properly managed. In addition, a key process to building a secure service is comprehensively testing and certifying the components in each layer of the technology stack for security vulnerabilities, and then to re-use these same pre-certified components for multiple services.The following checklist provides a starting point, but teams should work closely with their privacy specialist and security engineer to meet the needs of the specific service.Contact the appropriate privacy or legal officer of the department or agency to determine whether a System of Records Notice (SORN), Privacy Impact Assessment, or other review should be conductedDetermine, in consultation with a records officer, what data is collected and why, how it is used or shared, how it is stored and secured, and how long it is keptDetermine, in consultation with a privacy specialist, whether and how users are notified about how personal information is collected and used, including whether a privacy policy is needed and where it should appear, and how users will be notified in the event of a security breachConsider whether the user should be able to access, delete, or remove their information from the service“Pre-certify” the hosting infrastructure used for the project using FedRAMPUse deployment scripts to ensure configuration of production environment remains consistent and controllableDoes the service collect personal information from the user?  How is the user notified of this collection?Does it collect more information than necessary? Could the data be used in ways an average user wouldn’t expect?How does a user access, correct, delete, or remove personal information?Will any of the personal information stored in the system be shared with other services, people, or partners?How and how often is the service tested for security vulnerabilities?How can someone from the public report a security issue?Use data to drive decisionsAt every stage of a project, we should measure how well our service is working for our users. This includes measuring how well a system performs and how people are interacting with it in real-time. Our teams and agency leadership should carefully watch these metrics to find issues and identify which bug fixes and improvements should be prioritized. Along with monitoring tools, a feedback mechanism should be in place for people to report issues directly.Monitor system-level resource utilization in real timeMonitor system performance in real-time (e.g. response time, latency, throughput, and error rates)Ensure monitoring can measure median, 95th percentile, and 98th percentile performanceCreate automated alerts based on this monitoringTrack concurrent users in real-time, and monitor user behaviors in the aggregate to determine how well the service meets user needsPublish metrics internallyPublish metrics externallyUse an experimentation tool that supports multivariate testing in productionWhat are the key metrics for the service?How have these metrics performed over the life of the service?Which system monitoring tools are in place?What is the targeted average response time for your service? What percent of requests take more than 1 second, 2 seconds, 4 seconds, and 8 seconds?What is the average response time and percentile breakdown (percent of requests taking more than 1s, 2s, 4s, and 8s) for the top 10 transactions?What is the volume of each of your service’s top 10 transactions? What is the percentage of transactions started vs. completed?What is your service’s monthly uptime target?What is your service’s monthly uptime percentage, including scheduled maintenance? Excluding scheduled maintenance?How does your team receive automated alerts when incidents occur?How does your team respond to incidents? What is your post-mortem process?Which tools are in place to measure user behavior?What tools or technologies are used for A/B testing?How do you measure customer satisfaction?When we collaborate in the open and publish our data publicly, we can improve Government together. By building services more openly and publishing open data, we simplify the public’s access to government services and information, allow the public to contribute easily, and enable reuse by entrepreneurs, nonprofits, other agencies, and the public.Offer users a mechanism to report bugs and issues, and be responsive to these reportsProvide datasets to the public, in their entirety, through bulk downloads and APIs (application programming interfaces)Ensure that data from the service is explicitly in the public domain, and that rights are waived globally via an international public domain dedication, such as the “Creative Commons Zero” waiverCatalog data in the agency’s enterprise data inventory and add any public datasets to the agency’s public data listingEnsure that we maintain the rights to all data developed by third parties in a manner that is releasable and reusable at no cost to the publicEnsure that we maintain contractual rights to all custom software developed by third parties in a manner that is publishable and reusable at no costWhen appropriate, create a versioned API for third parties and internal users to interact with the service directlyWhen appropriate, publish source code of projects or components onlineWhen appropriate, share your development process and progress publiclyHow are you collecting user feedback for bugs and issues?If there is an API, what capabilities does it provide? Is it versioned? Who uses it? How is it documented?If the codebase has not been released under an open source license, explain why.What components are made available to the public as open source?What datasets are made available to the public?]]></content:encoded></item><item><title>Penn to reduce graduate admissions, rescind acceptances amid research cuts</title><link>https://www.thedp.com/article/2025/02/penn-graduate-student-class-size-cut-trump-funding</link><author>strangeloops85</author><category>hn</category><pubDate>Sun, 23 Feb 2025 00:37:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The unexpected way in which conditional types constrain type variables in TypeScript</title><link>https://2ality.com/2025/02/conditional-type-constraints.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[The TypeScript handbook makes an interesting statement: “Often, the checks in a conditional type will provide us with some new information. Just like narrowing with type guards can give us a more specific type, the true branch of a conditional type will further constrain generics by the type we check against.”In this blog post, we’ll see that this goes further than you may think.]]></content:encoded></item><item><title>Ask for no, don&apos;t ask for yes (2022)</title><link>https://www.mooreds.com/wordpress/archives/3518</link><author>mooreds</author><category>hn</category><pubDate>Sat, 22 Feb 2025 23:55:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I think it is important to have a bias for action. Like anything else, this is something you can make a habit of. Moving forward allows you to make progress. I don’t know about you, but I’ve frozen up in the past not knowing what the right path was for me. Moving forward, even the smallest possible step, helped break that stasis.One habit I like is to ask for no, not yes. Note that this is based on my experience at small companies (< 200 employees) where a lot of my experience has been. I’m not sure how it’d work in a big company, non-profit, or government.When you have something you want to do and that you feel is in scope for your position, but you want a bit of reassurance or to let the boss know what you are up to, it’s common to reach out and ask them for permission. Don’t. Don’t ask for a yes. Instead, offer a chance to say no, but with a deadline.Let’s see how this works.Suppose I want to set up a new GitHub action that I feel will really improve the quality of our software. This isn’t whimsy, I’ve done some research and tested it locally. I may have even asked a former colleague how they used this GitHub action.But I’m not quite sure. I want to let my boss know that I’ll be modifying the repository.I could say “hey, boss, can we install action X? It’ll help with the XYZ problems we’ve been having.”If you have a busy boss (and most people do), this is going to require a bit of work on their part to say “yes”.They’ll want to review the XYZ problem, think about how X will solve it and maybe do some thinking or prioritization about how this fits in with other work. Or maybe they’ll want you to share what you know. It may fall off their plate. You will probably have to remind them a few times to get around to saying “yes”. It might be a more pressing issue for youNow, let’s take the alternative approach.”Hey, boss, I am going to install action X, which should solve the XYZ problems we’ve been having. Will take care of this on Monday unless I hear differently from you.”Do you see the change in tone?You are saying (without being explicit) that you “got it” and are going to handle this issue. The boss can still weigh in if they want to, but they don’t have to. If they forget about it or other issues pop up, you still proceed. This lets you keep moving forward and solving problems while keeping the boss informed and allowing them to add their two cents if it is important enough.You can also use this approach with a group of people.By the way, the deadline is critical too. Which would you respond to more quickly, if it was Jan 15, all other things being equal and assuming a response was needed?“I’m going to do task X.”“I’m going to do task X on Jan 17.”“I’m going to do task X on Feb 15.”I would respond to the second one, which has a deadline in the near future. I think that is the way most folks work.Again, pursue this approach for problems you feel are in the scope of your role but that you want to inform the boss about. It’s great when you want to  a chance for feedback, but you are confident enough in the course of action that you don’t  feedback.]]></content:encoded></item><item><title>Will Consumer Data Collection Lead to Algorithm-Adjusted &apos;Surveillance Pricing&apos;?</title><link>https://yro.slashdot.org/story/25/02/22/230253/will-consumer-data-collection-lead-to-algorithm-adjusted-surveillance-pricing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 23:03:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from the Washington Post's "Tech Brief":


Last fall, reports that Kroger was considering bringing facial recognition technology into its stores sparked outcry from lawmakers and customers. They worried personalized data could be used to charge different prices for different customers based on their shopping habits, financial circumstances or appearance. Kroger, the country's largest supermarket chain, had already been using digital price tags in its stores. 

Kroger told lawmakers that it doesn't use facial recognition to help it set prices, a stance the company reiterated to the Tech Brief on Thursday. Still, the uproar helped to spark a push by consumer advocates who warn that the threat of invasive, personalized pricing schemes is real. Now, Democratic lawmakers in several states are working to ban so-called "surveillance pricing" — when businesses charge customers more or less for the same item based on their personal information. 

Besides a bill in California, three more bill were introduced this month in Colorado, Georgia, and Illinois that also ban "surveillance wages," which the article defines as employers adjusting wages based on how much data an employee collects. "Both surveillance pricing and surveillance wages really disrupt fundamental ideals of fairness," University of California, Irvine law professor Veena Dubal tells the Washington Post. 

Dubal is one of the consumer advocates behind a new report which notes information released last month by America's consumer-protecting FTC that "suggests that surveillance pricing tools are being actively developed and marketed across a range of industries, including consumer-facing businesses like 'grocery stores, apparel retailers, health and beauty retailers, home goods and furnishing stores, convenience stores, building and hardware stores, and general merchandise retailers such as department or discount stores." The consumer advocates (which include the Electronic Privacy Information Center) put it this way. 
"Imagine walking into a grocery store and seeing a price for milk that's higher than what the next shopper pays because an algorithm calculated that you're willing to spend more..."]]></content:encoded></item><item><title>Did xAI lie about Grok 3’s benchmarks?</title><link>https://techcrunch.com/2025/02/22/did-xai-lie-about-grok-3s-benchmarks/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Sat, 22 Feb 2025 22:55:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Debates over AI benchmarks — and how they’re reported by AI labs — are spilling out into public view. This week, an OpenAI employee accused Elon Musk’s AI company, xAI, of publishing misleading benchmark results for its latest AI model, Grok 3. One of the co-founders of xAI, Igor Babushkin, insisted that the company was […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Vine: A programming language based on Interaction Nets</title><link>https://vine.dev/docs/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:43:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New EV Batteries are Making Electric Cars Cheaper and Safer</title><link>https://hardware.slashdot.org/story/25/02/22/1840228/new-ev-batteries-are-making-electric-cars-cheaper-and-safer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 22:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Washington Post looks at a new kind of battery that "could make American EVs cheaper and safer, experts say."

If you bought an EV with a lithium iron phosphate (LFP) battery, you could expect lower car payments, less fire risk and more years of use out of your car — but you wouldn't be able to go as far on a single charge as you could with the nickel manganese cobalt (NMC) batteries commonly found in American and European electric cars. That trade-off has made LFP batteries the go-to choice for standard-range EVs in China, helping to make electric cars more affordable and limit pollution. Now, American companies are starting to build their own LFP batteries to catch up to their Chinese rivals... But there are plenty of barriers for U.S. companies that want to adopt a technology dominated by Chinese firms. Tariffs and tax credit restrictions have made it too expensive for most American automakers to import LFP batteries from China, and national security concerns have made it hard for American companies to partner with Chinese battery makers to build factories in the United States... 


Although American scientists invented LFP batteries in 1997, U.S. automakers didn't invest in the technology. Instead, they bet on NMC batteries because they have longer range, a big concern for American EV buyers. "Everyone in the West thought LFP was a nonstarter five or six years ago," said Adrian Yao, who founded STEER, a technology research group within Stanford University. "We really did have a myopic focus on" range, he added. That left the door open for Chinese companies to perfect LFP batteries, which have a few advantages. Instead of pricey nickel and cobalt, they use iron, which makes them 20 percent cheaper than NMC batteries, according to the International Energy Agency. While NMC batteries can be recharged up to about 1,000 times before they go kaput — which is enough to put 200,000 miles on most EVs — LFP batteries can last two or three times as long, according to Moura. Plus, LFP batteries' chemistry makes them less likely to catch fire and easier to extinguish. An NMC battery, on the other hand, is so flammable that "you could put it underwater or in space, and it'll keep burning because the oxygen it needs to keep the flame going is embedded within itself," Moura said. 

That safety advantage is key, because Chinese firms figured out they could pack LFP cells closer together inside a battery pack without risking a fire. That meant they could cram more energy into LFP batteries and nearly catch up to the range of NMC batteries. Last year, the Chinese battery giant CATL made the first LFP battery with more than 600 miles of range. Since LFP batteries are made from common materials and last longer, they also have a smaller environmental footprint than NMC batteries. 
Ford used LFP batteries in its Mach-E sedan (2023) and F-150 Lightning pickup trucks (2024), according to the article, "while Rivian began using them in the basic trims of its R1S SUV and R1T pickup truck this year... American LFP factories are slated to open this year in St. Louis and next year in Arizona."
And an environmental engineering professor at the University of California at Berkeley predicts LFP battery factories in the U.S. will "grow quite rapidly over the next five to 10 years."]]></content:encoded></item><item><title>Gig workers worked more but earned less in 2024: study</title><link>https://www.businessinsider.com/uber-lyft-instacart-gig-workers-saw-earnings-fall-2024-2025-2</link><author>wallflower</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:22:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Gig workers for Uber, Instacart, and other services made less money for their time in 2024.Even when delivery and ride-hailing drivers made more, their hours rose, too, a new report found.Gig workers have said their jobs have gotten more competitive and less lucrative in recent years.Gig workers for Uber, Instacart, and other services made less money on average in 2024 — even as the number of hours that they worked rose, in some cases.Uber hide-hailing drivers saw their earnings for 2024 fall 3.4% on average to $513 a week, according to a study released Tuesday by data analytics company Gridwise. At the same time, Uber drivers worked 0.8% more hours in 2024.Lyft drivers, meanwhile, worked 5.4% fewer hours in 2024, but saw their pay decline at a faster clip of 13.9% to $318 a week.Workers who shop and deliver orders for Instacart saw their pay for the year decline 8% to $194. Their hours worked fell 4.9%."Drivers are earning less across all of the platforms," Ryan Green, the CEO of Gridwise, told Business Insider.Meantime at DoorDash, gross weekly earnings rose 4.8% to $240 in 2024. Hourly earnings for those on the app fell, though, as the number of hours that gig workers spent on the app rose 5.2%.Amazon Flex workers were in a similar situation. Their earnings soared 18.1% to $413 a week — just as their hours increased 20.4%.Uber Eats workers made $178 a week, or 5.1% more than 2023. Average worker hours on the app rose 2.1%, though.The only app where workers earned significantly more money for the same or less work was Favor, a service owned by Texas supermarket H-E-B that delivers online orders for the chain. There, workers saw their pay rise 3.4% to $155 a week in 2024 as their hours worked fell 13.1%.In response to the report, an Uber spokesperson told BI that its drivers make more than $30 an hour on average.A Lyft spokesperson referred BI to comments that CEO David Risher made this month on the company's earnings call, including that ride-hailing drivers on the app earned a collective $9 billion in 2024. That was "the highest amount of combine driver earnings on our platform ever," Risher said."Several of the claims made in the Gridwise report related to driver earnings and market share are likely based on different methodologies, painting an incomplete picture," a spokesperson from Lyft told BI.An Instacart spokesperson called the report's findings "inaccurate and misleading.""Shopper earnings remain steady across the Instacart platform, and we continue to hear from shoppers that Instacart creates rewarding, flexible earnings opportunities that allow them to earn on their own time and their own terms," the spokesperson said.DoorDash declined to comment. Amazon and Favor did not respond to requests for comment.We want to hear from you. Are you a gig worker? What are the biggest benefits or challenges of gig work that you'd be comfortable sharing with a reporter? Please fill out this Gridwise obtained the data for the report using its own app, which it markets to gig workers to track their earnings and expenses. The company analyzed 171 million trips and $1.9 billion worth of gig worker earnings documented by the app to compile its findings for 2024.The report also found that the average restaurant delivery worker relied on tips for a majority — 53.4% — of their earnings. For grocery delivery workers, 45.7% of earnings came from tips.Tips were much less significant for ride-hailing drivers, Gridwise found. Gratuities made up just 10.4% of earnings, per the report.Gig workers have told BI that claiming good-paying rides and orders on the apps has gotten more competitive. Some workers have even set up their own businesses to offer rides or deliver restaurant food in hopes of making more money than they do on the apps.Consumers, meanwhile, told Gridwise that they plan to keep using ride-hailing and delivery services despite the lingering effects of inflation on many items in Americans' monthly budgets.Majorities of the 1,000 customers surveyed by Gridwise in January said that they thought prices on both ride-hailing apps like Uber and Lyft as well as grocery delivery apps like Instacart were "reasonable.""They talk about being price-sensitive, but their actions reflect differently," Green said.Do you work for Uber, Lyft, DoorDash, Instacart, or another service that uses gig workers and have a story idea to share? Reach out to this reporter at abitter@businessinsider.com]]></content:encoded></item><item><title>OpenBSD Innovations</title><link>https://www.openbsd.org/innovations.html</link><author>angristan</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:08:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
    This is a list of software and ideas developed or maintained by the OpenBSD
    project, sorted in order of approximate introduction. Some of them are
    explained in detail in our research papers.
ipsec(4):
	Started by John Ioannidis, Angelos D. Keromytis, Niels Provos, and
	Niklas Hallqvist, imported February 20, 1997.  OpenBSD was the first
	free operating system to provide an IPSec stack.
    inet6(4):
	First complete integration and adoption of IPv6 led by
	"Itojun" (Dr. Junichiro Hagino) [WIDE/KAME], Craig Metz [NRL], and
	Angelos D. Keromytis starting Jan 6, 1999.
	Almost fully operational Jun 6, 1999 during the
	first OpenBSD hackathon.
	OpenBSD 2.7.
    :
	Related to the work on privilege separation, some programs were refactored
	to drop privileges while holding onto a tricky resource such as a raw socket,
	reserved port, or modification-locked bpf(4) descriptor,
	for example
	ping(8),
	traceroute(8),
	etc.
    :
	Developed since 2001 as "propolice" by Hiroaki Etoh. Integrated, and
	implemented for additional hardware platforms, by Federico G. Schwindt,
	Miod Vallat and Theo de Raadt.  OpenBSD 3.3 was the first operating
	system to enable it systemwide by default.
    :
	First used for sparc, sparc64, alpha, and hppa in OpenBSD 3.3.
	Strictly enforced by default since OpenBSD 6.0: a program can only
	violate it if the executable is marked with 
	and it is located on a filesystem mounted with the mount(8) option.
     by ld.so:
	first done as part of the W^X work in OpenBSD 3.3, by Dale Rahn and
	Theo de Raadt. The GOT and PLT regions are read-only outside of ld.so
	itself. Extended to the .init/.fini sections (constructors and
	destructors) in OpenBSD 3.4.
    :
	OpenBSD 3.4 was the first widely used operating system to
	provide it by default.
    gcc-local(1)
	__attribute__((__bounded__)) static analysis annotation
	and checking mechanism:
	Started by Anil Madhavapeddy on June 26, 2003
	and ported to GCC 4 by Nicholas Marriott.
	First released with OpenBSD 3.4.
    malloc(3)
	randomization implemented by Thierry Deval. Guard pages and randomized (delayed) free added by Ted Unangst.
	Reimplemented by Otto Moerbeek
	for OpenBSD 4.4.
    Position-independent executables (PIE):
	OpenBSD 5.3 was the first widely used operating system to enable it
	globally by default, on seven hardware platforms.
	Implemented in November 2008 by
	Kurt Miller
	and enabled by default by
	Pascal Stumpf
	in August 2012.
    :
	the ability to specify that a variable should be initialized
	at load time with random byte values (placed into a new ELF
	 section) was implemented in
	OpenBSD 5.3 by Matthew Dempsky.
    Stack protector per shared object:
	using the random-data memory feature, each shared object was given its
	own stack protector cookie in OpenBSD 5.3 by Matthew Dempsky.
    :
	Position-independent static binaries for /bin, /sbin and ramdisks.
	Implemented for OpenBSD 5.7 by Kurt Miller and Mark Kettenis.
    
	(sigreturn(2)
	oriented programming) mitigation: attacks researched by
	Eric Bosman
	and Herbert Bos in 2014, solution implemented by Theo de Raadt in May 2016,
	enabled by default since OpenBSD 6.0.
    Library order randomization:
	In rc(8), re-link
	, , and 
	on startup, placing the objects in a random order.
	Theo de Raadt and Robert Peichaer, May 2016,
	enabled by default since OpenBSD 6.0 and 6.2.
    Kernel-assisted lazy-binding for W^X safety in multi-threaded programs.
	A new syscall kbind(2)
	permits lazy-binding to be W^X safe in multi-threaded programs.
	Implemented for OpenBSD 5.9 by Philip Guenther in July 2015.
    Process layouts in memory tightened to remove execute permission from
	all segmented, non-instruction data and to remove write permission from
	data that is only modified during loading and relocation.
	By combining the RELRO (Read-Only after Relocation) design from the
	GNU project with the original ASLR work from OpenBSD 3.3 and
	strict lazy-binding work from OpenBSD 5.9, this is applied to not
	just a subset of programs and libraries but rather to all programs
	and libraries.
	Implemented for OpenBSD 6.1 by Philip Guenther in August 2016.
    Use of fork+exec in privilege separated programs. The
	strategy is to give each process a fresh & unique address space for
	ASLR, stack protector -- as protection against address space discovery attacks.
	Implemented first by
	Damien Miller (sshd(8) 2004),
	Claudio Jeker (bgpd(8), 2015),
	Eric Faurot (smtpd(8), 2016),
	Rafael Zalamena (various, 2016), and others.
    :
	Reduction of incidental NOP instructions/sequences in the instruction
	stream which could be useful potentially for ROP attack methods to
	inaccurately target gadgets. These NOP sequences are converted into
	trap sequences where possible. Todd Mortimer and Theo de Raadt, June
	2017.
    :
	the .o files of the kernel are relinked in random order from a
	link-kit, before every reboot. This provides substantial interior
	randomization in the kernel's text and data segments for layout and
	relative branches/calls.  Basically a unique address space for each
	kernel boot, similar to the userland fork+exec model described above
	but for the kernel.  Theo de Raadt, June 2017.
    Rearranged i386/amd64 register allocator order in
	clang(1)
	to reduce polymorphic RET instructions:
	Todd Mortimer, November 20, 2017.
    Reencoding of i386/amd64 instruction sequences to avoid
	embedded polymorphic RET instructions.  Enhancements to
	clang(1)
	Todd Mortimer, April 28, 2018 and onwards.
     addition to
	mmap(2)
	allows opportunistic verification that the stack-register
	points at stack memory, therefore catching pivots to non-stack
	memory (sometimes used in ROP attacks).
	Theo de Raadt, April 12, 2018.
     is a replacement for the 
	which uses a per-function random cookie (located in the read-only ELF
	 section) to consistency-check the
	return address on the stack.  Implemented for amd64 and arm64
	by Todd Mortimer in OpenBSD 6.4, for mips64 in OpenBSD 6.7, and
	powerpc/powerpc64 in OpenBSD 6.9.  amd64 system call stubs also
	protected in OpenBSD 7.3.
     addition to
	mmap(2)
	disallows memory pages to be written to core dumps, preventing
	accidental exposure of private information.
	Theo de Raadt, Mark Kettenis and Scott Soule Cheloha,
	February 2, 2019.
    Similar to the opportunistic verification in ,
	system-calls can no longer be performed from PROT_WRITE memory.
	Theo de Raadt, June 2, 2019.
    System calls may only be performed from selected code regions
        (main program, ld.so, libc.so, and sigtramp).  The libc.so region
	is setup by msyscall(2).
	Theo de Raadt, November 28, 2019.
	This mechanism was removed because later work on immutable memory +
	pinned system calls was even better.
    Permissions (RWX, MAP_STACK, etc) on address space regions can be
	made immutable, so that mmap(2),
	mprotect(2) or
	munmap(2) fail with
	EPERM. Most of the program static address space is now automatically
	immutable (main program, ld.so, main stack, load-time shared libraries,
	and dlopen()'d libraries mapped without RTLD_NODELETE).  Programmers
	can request non-immutable static data using the "openbsd.mutable" section,
	or manually bring immutability to (page aligned heap objects) using
	mimmutable(2).
	Theo de Raadt, Dec 4, 2022.
    sshd random relinking at boot. Theo de Raadt. Jan 18, 2023.
    Some architectures now have non-readable code ("xonly"), both from
	the perspective of userland reading its own memory, or the kernel
	trying to read memory in a system call.  Many sloppy practices in
	userland code had to be repaired to allow this.  The linker option
	 is enabled by default. In order of
	development: arm64, riscv64, hppa, amd64,
	powerpc64, powerpc (G5 only), octeon.
	sparc64 (sun4u only, unfinished).
	Mark Kettenis, Theo de Raadt, Visa Hankala, Miod Vallat,
	Dave Voutila, George Koehler in kernel and base, and
	Theo Buehler, Robert Nagy, Christian Weisgerber in ports.
	Dec 2022 - Feb 2023, still ongoing.
    On all architectures which lack hardware-enforcement of xonly,
	system calls are now prevented from reading (via copyin/copyinst)
	inside the program's main text, ld.so text, sigtramp text, or
	libc.so text.
	Theo de Raadt, Jan 2023.
    Architectures which lack xonly mmu-enforcement can still benefit
	from switching to --execute-only binaries if the cpu generates
	different traps for instruction-fetch versus data-fetch.  The
	VM system will not allow memory to be read before it was
	executed which is valuable together with library relinking.
	Architectures switched over include loongson.
	Theo de Raadt, Feb 2023.
    ld.so and crt0 register the location of the
        execve(2)
	libc syscall stub with the kernel using
        pinsyscall(2),
	after which the kernel only accepts an execve call from that
	specific location. Theo de Raadt, Feb 2023. Made redundant by
        pinsyscalls(2)
        which handles all system calls.
    Mandatory enforcement of indirect branch targets (BTI on arm64,
        IBT on Intel amd64), unless a linker flag (-Wl,-z,nobtcfi) requests
        no enforcement.
    The kernel and ld.so register the precise entry location of
	every system call used by a program, as described in the
	new ELF section  inside ld.so and
	libc.so.  ld.so uses the new syscall
        pinsyscalls(2)
        to tell the kernel the precise entry location of system calls in libc.so.
	Since all syscall entries are now known to the kernel, the
	pininsyscall(SYS_execve) interface becomes redundant.
        msyscall(2) mechanism
	also becomes redundant (and is removed a bit later), because immutable
	memory + pinsyscalls together are cheaper and more effective targeting.
	Theo de Raadt, Jan 2024.
     is a clang extension that, upon return from a function
	cleans the return value off the stack (one of many information leaks which
	can be used to determine where functions in a different DSO reside).
	The kernel, libc, libcrypto, and ld.so(1) are compiled with this option.
	amd64 only, for now.
strtonum(3):
	Ted Unangst, Todd Miller, and Theo de Raadt, May 3, 2004, OpenBSD 3.6
    imsg:
	Message passing API, written by Henning Brauer.
	In libutil since May 26, 2010, OpenBSD 4.8;
	used by various daemons before that.
    ohash:
	Written and maintained by Marc Espie.
	In libutil since May 12, 2014, OpenBSD 5.6;
	used by make(1) and m4(1) before that.
    asr:
	Replacement resolver written and maintained by Eric Faurot.
	Imported April 14, 2012; activated on March 26, 2014, OpenBSD 5.6.
    reallocarray(3):
	Theo de Raadt and Ted Unangst, April 22, 2014, OpenBSD 5.6
    getentropy(2):
	Matthew Dempsky and Theo de Raadt, June 13, 2014, OpenBSD 5.6
    pledge(2):
	Theo de Raadt, July 19, 2015, OpenBSD 5.9
    recallocarray(3):
	Otto Moerbeek, Joel Sing and Theo de Raadt, March 6, 2017, OpenBSD 6.1
    freezero(3):
	Otto Moerbeek, April 10, 2017, OpenBSD 6.2
    unveil(2):
	Theo de Raadt and Bob Beck, July 13, 2018, OpenBSD 6.4
    ober:
        ASN.1 basic encoding rules API, written by Claudio Jeker and
        Reyk Flöter, maintained by Rob Pierce and Martijn van Duren;
        started in 2006/07, moved to libutil on May 11, 2019, OpenBSD 6.6
ypserv(8):
	Started by Mats O. Jansson in 1994.
	Imported October 23, 1995 and first released with OpenBSD 2.0.
    mopd(8):
	Started by Mats O. Jansson in 1993.
	Imported September 21, 1996 and first released with OpenBSD 2.0.
    AnonCVS:
	Designed and implemented by Chuck Cranor and Theo de Raadt in 1995
	(paper,
	slides)
    aucat(1):
	Started by Kenneth Stailey.
	Imported January 2, 1997 and first released with OpenBSD 2.1.
	Now maintained by Alexandre Ratchov.
    mg(1):
	Started by Dave Conroy in November 1986.
	Imported February 25, 2000 and first released with OpenBSD 2.7.
	Now maintained by Mark Lumsden.
    m4(1):
	Originally implemented by Ozan Yigit and Richard A. O'Keefe for 4.3BSD-Reno.
	Considerably extended and maintained by Marc Espie since 1999.
    pf(4),
	pfctl(8),
	pflogd(8),
	authpf(8),
	ftp-proxy(8):
	Started by Daniel Hartmeier as a replacement for the non-free ipf by
	Darren Reed. Imported June 24, 2001 and first released with OpenBSD
	3.0. Now maintained by Henning Brauer.
    systrace(4),
	systrace(1):
	Started by Niels Provos.
	Imported June 4, 2002 and first released with OpenBSD 3.2.
	Deleted after OpenBSD 5.9 because
	pledge(2) is even better.
    spamd(8):
	Written by Bob Beck. Imported December 21, 2002 and first released with
	OpenBSD 3.3.
    dc(1):
	Written and maintained by Otto Moerbeek.
	Imported September 19, 2003 and first released with OpenBSD 3.5.
    bc(1):
	Written and maintained by Otto Moerbeek.
	Imported September 25, 2003 and first released with OpenBSD 3.5.
    sensorsd(8):
	Started by Henning Brauer.
	Imported September 24, 2003 and first released with OpenBSD 3.5.
	Reworked by Constantine A. Murenin.
    pkg_add(1):
	Written and maintained by Marc Espie.
	Imported October 16, 2003 and first released with OpenBSD 3.5.
    carp(4):
	Written by Mickey Shalayeff, Markus Friedl, Marco Pfatschbacher,
	and Ryan McBride.
	Imported October 17, 2003 and first released with OpenBSD 3.5.
    OpenBGPD
	including bgpd(8)
	and bgpctl(8):
	Written and maintained by Henning Brauer and Claudio Jeker,
	and also maintained by Peter Hessler.
	Imported December 17, 2003 and first released with OpenBSD 3.5.
    dhclient(8):
	Started by Ted Lemon and Elliot Poger in 1996.
	Imported January 18, 2004 and first released with OpenBSD 3.5.
	Reworked by Henning Brauer.
	Now maintained by Kenneth Westerback.
    dhcpd(8):
	Started by Ted Lemon in 1995.
	Imported April 13, 2004 and first released with OpenBSD 3.6.
	Reworked by Henning Brauer.
	Now maintained by Kenneth Westerback.
    hotplugd(8):
	Started by Alexander Yurchenko.
	Imported May 30, 2004 and first released with OpenBSD 3.6.
    OpenNTPD
	including ntpd(8)
	and ntpctl(8):
	Written and maintained by Henning Brauer.
	Imported May 31, 2004 and first released with OpenBSD 3.6.
	Portable version maintained by Brent Cook.
    dpb(1):
	Started by Nikolay Sturm on August 10, 2004; first available for OpenBSD 3.6.
	Rewritten and maintained by Marc Espie since August 20, 2010.
    ospfd(8),
	ospfctl(8):
	Started by Esben Norby and Claudio Jeker.
	Imported January 28, 2005 and first released with OpenBSD 3.7.
    ifstated(8):
	Started by Marco Pfatschbacher and Ryan McBride.
	Imported January 23, 2004 and first released with OpenBSD 3.8.
    bioctl(8):
	Started by Marco Peereboom.
	Imported March 29, 2005 and first released with OpenBSD 3.8.
    hostapd(8):
	Written by Reyk Flöter.
	Imported May 26, 2005 and first released with OpenBSD 3.8.
    watchdogd(8):
	Started by Marc Balmer.
	Imported August 8, 2005 and first released with OpenBSD 3.8.
    sdiff(1):
	Written by Ray Lai.
	Imported December 27, 2005 and first released with OpenBSD 3.9.
    dvmrpd(8),
	dvmrpctl(8):
	Started by Esben Norby.
	Imported June 1, 2006 and first released with OpenBSD 4.0.
    ripd(8),
	ripctl(8):
	Started by Michele Marchetto.
	Imported October 18, 2006 and first released with OpenBSD 4.1.
    pkg-config(1):
	Started by Chris Kuethe and Marc Espie.
	Imported November 27, 2006 and first released with OpenBSD 4.1.
	Now maintained by Jasper Lievisse Adriaanse.
    relayd(8)
	with relayctl(8):
	Started by Pierre-Yves Ritschard and Reyk Flöter.
	Imported December 16, 2006 and first released with OpenBSD 4.1.
	Now maintained by Sebastian Benoit.ospf6d(8),
	ospf6ctl(8):
	Started by Esben Norby and Claudio Jeker.
	Imported October 8, 2007 and first released with OpenBSD 4.2.
    libtool(1):
	Written by Steven Mestdagh and Marc Espie.
	Imported October 28, 2007 and first available for OpenBSD 4.3.
	Now maintained by Marc Espie, Jasper Lievisse Adriaanse,
	and Antoine Jacoutot.
    snmpd(8):
	Started by Reyk Flöter.
	Imported December 5, 2007 and first released with OpenBSD 4.3.
	Now maintained by Martijn van Duren.
    sysmerge(8):
	Written and maintained by Antoine Jacoutot,
	originally forked from mergemaster by Douglas Barton.
	Imported April 22, 2008, first released with OpenBSD 4.4.
    ypldap(8):
	Started by Pierre-Yves Ritschard.
	Imported June 26, 2008 and first released with OpenBSD 4.4.
    OpenSMTPD
	including smtpd(8),
	smtpctl(8),
	makemap(8):
	Started by Gilles Chehade.
	Imported November 1, 2008 and first released with OpenBSD 4.6.
	Now maintained by Gilles Chehade and Eric Faurot.
    tmux,
	tmux(1):
	Started in 2007 and maintained by Nicholas Marriott.
	Imported June 1, 2009, first released with OpenBSD 4.6.
    ldpd(8),
	ldpctl(8):
	Started by Michele Marchetto.
	Imported June 1, 2009 and first released with OpenBSD 4.6.
	Now maintained by Claudio Jeker.
    ldapd(8),
	ldapctl(8):
	Written by Martin Hedenfalk.
	Imported May 31, 2010 and first released with OpenBSD 4.8.
    OpenIKED
	including iked(8)
	and ikectl(8):
	Started by Reyk Flöter.
	Imported June 3, 2010 and first released with OpenBSD 4.8.
	Now maintained by Tobias Heider.
    iscsid(8),
	iscsictl(8):
	Written and maintained by Claudio Jeker.
	Imported September 24, 2010 and first released with OpenBSD 4.9.
    rc.d(8),
	rc.subr(8):
	Written and maintained by Robert Nagy and Antoine Jacoutot.
	Imported October 26, 2010 and first released with OpenBSD 4.9.
    tftpd(8):
	Written and maintained by David Gwynne.
	Imported March 2, 2012 and first released with OpenBSD 5.2.
    npppd(8),
	npppctl(8):
	Started by Internet Initiative Japan Inc.
	Imported January 11, 2010, first released with OpenBSD 5.3.
	Maintained by YASUOKA Masahiko.
    ldomd(8),
	ldomctl(8):
	Written and maintained by Mark Kettenis.
	Imported October 26, 2012 and first released with OpenBSD 5.3.
    sndiod(8):
	Written and maintained by Alexandre Ratchov.
	Imported November 23, 2012 and first released with OpenBSD 5.3.
    cu(1):
	Written and maintained by Nicholas Marriott.
	Imported July 10, 2012 and first released with OpenBSD 5.4.
    identd(8):
	Written and maintained by David Gwynne.
	Imported March 18, 2013 and first released with OpenBSD 5.4.
    slowcgi(8):
	Written and maintained by Florian Obser.
	Imported May 23, 2013 and first released with OpenBSD 5.4.
    signify(1):
	Written and maintained by Ted Unangst.
	Imported December 31, 2013 and first released with OpenBSD 5.5.
    htpasswd(1):
	Written and maintained by Florian Obser.
	Imported March 17, 2014 and first released with OpenBSD 5.6.
    LibreSSL:
	Started by Ted Unangst, Bob Beck, Joel Sing, Miod Vallat, Philip Guenther,
	and Theo de Raadt on April 13, 2014, as a fork of OpenSSL 1.0.1g.
	First released with OpenBSD 5.6.
	Portable version maintained by Brent Cook.
    httpd(8):
	Started by Reyk Flöter.
	Imported July 12, 2014 and first released with OpenBSD 5.6.
    rcctl(8):
	Written and maintained by Antoine Jacoutot.
	Imported August 19, 2014 and first released with OpenBSD 5.7.
    file(1):
	Rewritten from scratch and maintained by Nicholas Marriott.
	Imported April 24, 2015 and first released with OpenBSD 5.8.
    doas(1):
	Written and maintained by Ted Unangst.
	Imported July 16, 2015 and first released with OpenBSD 5.8.
    radiusd(8):
	Written and maintained by YASUOKA Masahiko.
	Imported July 21, 2015 and first released with OpenBSD 5.8.
    eigrpd(8),
	eigrpctl(8):
	Written and maintained by Renato Westphal.
	Imported October 2, 2015 and first released with OpenBSD 5.9.
    vmm(4),
	vmd(8),
	vmctl(8):
	Written by Mike Larkin and Reyk Flöter.
	Imported November 13, 2015 and first released with OpenBSD 5.9.
    pdisk(8):
	Originally written by Eryk Vershen in 1996-1998,
	rewritten and maintained by Kenneth Westerback since January 11, 2016
	and first released with OpenBSD 5.9.
    mknod(8):
	Original version from Version 6 AT&T UNIX (1975),
	last rewritten by Marc Espie on March 5, 2016
	and first released with OpenBSD 6.0.
    audioctl(1):
	Originally written by Lennart Augustsson in 1997,
	rewritten and maintained by Alexandre Ratchov since June 21, 2016
	and first released with OpenBSD 6.0.
    acme-client(1):
	Written by Kristaps Dzonsons, imported August 31, 2016; released
	with OpenBSD 6.1.
    syspatch(8):
	Written and maintained by Antoine Jacoutot.
	Imported September 5, 2016; released with OpenBSD 6.1.
    ping(8):
	Restructured to include IPv6 functionality and maintained by Florian Obser.
	The separate
	ping6(8)
	was superseded on September 17, 2016,
	and the new, combined version was released with OpenBSD 6.1.
    xenodm(1):
	Cleaned-up fork of
	xdm(1)
	maintained by Matthieu Herrb.
	Imported October 23, 2016; released with OpenBSD 6.1.
    ocspcheck(8):
	Written and maintained by Bob Beck.
	Imported January 24, 2017; released with OpenBSD 6.1.
    slaacd(8):
	Written and maintained by Florian Obser.
	Imported March 18, 2017; released with OpenBSD 6.2.
    rad(8):
	Written and maintained by Florian Obser.
	Imported July 10, 2018; released with OpenBSD 6.4.
    unwind(8):
	Written and maintained by Florian Obser.
	Imported January 23, 2019; released with OpenBSD 6.5.
    openrsync(1):
	Written by Kristaps Dzonsons.
	Imported February 10, 2019; released with OpenBSD 6.5.
    sysupgrade(8):
        Written by Christian Weisgerber, Florian Obser, and Theo de Raadt.
        Imported April 25, 2019; released with OpenBSD 6.6.
    snmp(1):
        Written and maintained by Martijn van Duren.
        Imported August 9, 2019; released with OpenBSD 6.6.
    rpki-client(8):
        Written by Kristaps Dzonsons; maintained by Claudio Jeker,
        Theo Buehler, and Job Snijders.
        Imported June 17, 2019; released with OpenBSD 6.7.
    resolvd(8):
        Written and maintained by Florian Obser and Theo de Raadt.
        Imported February 24, 2021; released with OpenBSD 6.9.
    dhcpleased(8):
        Written and maintained by Florian Obser.
        Imported February 26, 2021; released with OpenBSD 6.9.
Projects maintained by OpenBSD developers outside OpenBSDsudo:
	Started by Bob Coggeshall and Cliff Spencer around 1980.
	Imported November 18, 1999, first released with OpenBSD 2.7.
	Now maintained by Todd Miller.
    femail:
	Written and maintained by Henning Brauer.
	Started in 2005, port available since September 22, 2005.
    midish:
	Written and maintained by Alexandre Ratchov.
	Started in 2003, port available since November 4, 2005.
    fdm:
	Written and maintained by Nicholas Marriott.
	Started in 2006, port available since January 18, 2007.
    toad:
	Written and maintained by Antoine Jacoutot.
	Started in 2013, port available since October 8, 2013.
    docbook2mdoc:
	Started by Kristaps Dzonsons in 2014, maintained by Ingo Schwarze.
	Port available since April 3, 2014.
    portroach:
	Written and maintained by Jasper Lievisse Adriaanse,
	originally forked from FreeBSD's portscout.
	Started in 2014, port available since September 5, 2014.
    cvs2gitdump:
	Written and maintained by YASUOKA Masahiko.
	Started in 2012, port available since August 1, 2016.
    Game of Trees:
	Written and maintained by Stefan Sperling.
	Started in 2017, port available since August 9, 2019.
]]></content:encoded></item><item><title>This Week In Techdirt History: February 16th – 22nd</title><link>https://www.techdirt.com/2025/02/22/this-week-in-techdirt-history-february-16th-22nd/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sat, 22 Feb 2025 21:45:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust Developer Survey Finds Increasing Usage, Especially on Linux</title><link>https://developers.slashdot.org/story/25/02/22/042227/rust-developer-survey-finds-increasing-usage-especially-on-linux?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 21:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[This year's "State of Rust" survey was completed by 7,310 Rust developers. DevClass note some key findings:

When asked about their biggest worries for Rust's future, 45.5 percent cited "not enough usage in the tech industry," up from 42.5 percent last year, just ahead of the 45.2 percent who cited complexity as a concern... Only 18.6 percent declared themselves "not worried," though this is a slight improvement on 17.8 percent in 2023... 
Another question asks whether respondents are using Rust at work. 38.2 percent claimed to use it for most of their coding [up from 34% in 2023], and 13.4 percent a few times a week, accounting for just over half of responses. At the organization level there is a similar pattern. 45.5 percent of organizations represented by respondents make "non-trivial use of Rust," up from 38.7 percent last year. 

More details from I Programmer:

On the up are "Using Rust helps us achieve or goals", now 82% compared to 72% in 2022; "We're likely to use Rust again in the future", up 3% to 78%; and "Using Rust has been worth the cost of Adoption". Going down are "Adopting Rust has been challenging", now 34.5% compared to 38.5% in 2022; and "Overall adopting Rust has slowed down our team" down by over 2% to 7%. 



"According to the survey, organizations primarily choose Rust for building correct and bug-free software (87.1%), performance characteristics (84.5%), security and safety properties (74.8%), and development enjoyment (71.2%)," writes The New Stack:

 Rust seems to be especially popular for creating server backends (53.4%), web and networking services, cloud technologies and WebAssembly, the report said. It also seems to be gaining more traction for embedded use cases... Regarding the preferred development environment, Linux remains the dominant development platform (73.7%). 

However, although VS Code remains the leading editor, its usage dropped five percentage points, from 61.7% to 56.7%, but the Zed editor gained notable traction, from 0.7% to 8.9%. Also, "nine out of 10 Rust developers use the current stable version, suggesting strong confidence in the language's stability," the report said... 

Overall, 82% of respondents report that Rust helped their company achieve its goals, and daily Rust usage increased to 53% (up four percentage points from 2023). When asked why they use Rust at work, 47% of respondents cited a need for precise control over their software, which is up from 37% when the question was asked two years ago.]]></content:encoded></item><item><title>Infocon: green</title><link>https://isc.sans.edu/diary.html?rss</link><author></author><category>infosec</category><pubDate>Sat, 22 Feb 2025 21:30:03 +0000</pubDate><source url="https://isc.sans.edu/">Sans Edu Diaries</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>US AI Safety Institute could face big cuts</title><link>https://techcrunch.com/2025/02/22/us-ai-safety-institute-could-face-big-cuts/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 22 Feb 2025 21:22:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The National Institute of Standards and Technology could fire as many as 500 staffers, according to multiple reports — cuts that further threaten a fledgling AI safety organization. Axios reported this week that the US AI Safety Institute (AISI) and Chips for America, both part of NIST, would be “gutted” by layoffs targeting probationary employees […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Kaneo – An open source project management platform</title><link>https://kaneo.app/</link><author>saturn5k</author><category>hn</category><pubDate>Sat, 22 Feb 2025 20:52:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
An open source project management platform focused on simplicity
                and efficiency. Self-host it, customize it, make it yours.

Free and open source forever

Full control over your data
]]></content:encoded></item><item><title>How I Podcast: Summer Album / Winter Album’s Jody Avirgan</title><link>https://techcrunch.com/2025/02/22/how-i-podcast-summer-album-winter-albums-jody-avirgan/</link><author>Brian Heater</author><category>tech</category><pubDate>Sat, 22 Feb 2025 20:41:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The beauty of podcasting is that anyone can do it. It’s a rare medium that’s nearly as easy to make as it is to consume. And as such, no two people do it exactly the same way. There are a wealth of hardware and software solutions open to potential podcasters, so setups run the gamut […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>In Defense of Text Labels</title><link>https://www.chrbutler.com/in-defense-of-text-labels</link><author>delaugust</author><category>hn</category><pubDate>Sat, 22 Feb 2025 20:40:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
Why Icons Alone Aren’t Enough

I’m a firm believer in text labels.

Interfaces are over-stuffed with icons. The more icons we have to scan over, the more brain power we put toward making sense of them rather than using the tools they represent. This slows us down, not just once, but over and over again.

While it may feel duplicative to add a text label, the reality is that few icons are self-sufficient in communicating meaning.

The Problems that Icons Create
1. Few icons communicate a clear, singular meaning immediately
It’s easy to say that a  icon will communicate meaning — or that if an icon needs a text label, it’s not doing its job. But that doesn’t take into consideration the burden that icons —  or  — put on people trying to navigate interfaces.

Even the simplest icons can create ambiguity. While a trash can icon reliably communicates “delete,” what about the common pencil icon. Does it mean create? Edit? Write? Draw? Context can help with disambiguation, but not always, and that contextual interpretation requires additional cognitive effort.

When an icon’s meaning isn’t immediately clear, it slows down our orientation within an interface and the use of its features. Each encounter requires a split-second of processing that might seem negligible but accumulates across interactions.
2. The more icons within an interface, the more difficult it can be to navigate.
As feature sets grow, we often resort to increasingly abstract or subtle visual distinctions between icons. What might have worked with 5-7 core functions becomes unmanageable at 15-20 features. Users must differentiate between various forms of creation, sharing, saving, and organizing - all through pictorial representation alone.

The burden of communication increases for each individual icon as an interface’s feature set expands. It becomes increasingly difficult to communicate specific functions with icons alone, especially when distinguishing between similar actions like creating and editing, saving and archiving, or uploading and downloading.
3. Icons function as an interface-specific language within a broader ecosystem.
Interfaces operate within other interfaces. Your application may run within a browser that also runs within an operating system. Users must navigate multiple levels of interface complexity, most of which you cannot control. When creating bespoke icons, you force users to learn a new visual language while still maintaining awareness of established conventions.

This creates particular challenges with standardized icon sets. When we use established systems like Google’s Material Design, an icon that represents one function in our interface might represent something entirely different in another application. This cross-context confusion adds to the cognitive load of icon interpretation.

Why Text Labeling Helps Your Interface
1. Text alone is usually more efficient.
Our brains process familiar words holistically rather than letter-by-letter, making them incredibly efficient information carriers. We’ve spent our lives learning to recognize words instantly, while most app icons require new visual vocabulary.

Scanning text is fundamentally easier than scanning icons. A stacked list of text requires only a one-directional scan (top-to-bottom), while icon grids demand bi-directional scanning (top-to-bottom and left-to-right). This efficiency becomes particularly apparent in mobile interfaces, where similar-looking app icons can create a visually confusing grid.
2. Text can make icons more efficient.
The example above comes from Magnolia, an application I designed. On the left is the side navigation panel without labels. On the right is the same panel with text labels. Magnolia is an extremely niche tool with highly specific features that align with the needs of research and planning teams who develop account briefs. Without the labels, the people who we created Magnolia for would likely find the navigation system confusing.

Adding text labels to icons serves two purposes: it clarifies meaning and provides greater creative freedom. When an icon’s meaning is reinforced by text, users can scan more quickly and confidently. Additionally, designers can focus more on the unity of their interface’s visual language when they’re not relying on icons alone to communicate function.
3. Icons are effective anchors in text-heavy applications.
Above is another example from Magnolia. Notice how the list of options on the right (Export, Regenerate, and History) stands out because of the icons, but the text labels make it immediately clear what these things do.

See, this isn’t an argument for eliminating icons entirely. Icons serve an important role as visual landmarks, helping to differentiate functional areas from content areas. Especially in text-heavy applications, icons help pull the eye toward interactive elements.

The combination of icon and text label creates clearer affordances than either element alone.

Every time we choose between an icon and a text label, we’re making a choice about cognitive load. We’re deciding how much mental energy people will spend interpreting our interfaces rather than using them. While a purely iconic interface might seem simple and more attractive, it often creates an invisible tax on attention and understanding.

The solution, of course, isn’t found in a “perfect” icon, nor in abandoning icons entirely. Icons remain powerful tools for creating visual hierarchy and differentiation. Instead, we need to be more thoughtful about when and how we deploy them. The best interfaces recognize that icons and text aren’t competing approaches but complementary tools that work best in harmony.

This means considering not just the immediate context of our own interfaces, but the broader ecosystem in which they exist. Our applications don’t exist in isolation — they’re part of a complex digital environment where users are constantly switching between different contexts, each with its own visual language.

The next time you’re tempted to create yet another icon, or to remove text labels, remember: the most elegant solution isn’t always the one that  simple — it’s the one that makes communication and understanding  simple.
]]></content:encoded></item><item><title>Glitches for Windows 11 Update Include Breaking File Explorer</title><link>https://tech.slashdot.org/story/25/02/22/183257/glitches-for-windows-11-update-include-breaking-file-explorer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 20:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Five days ago on Patch Tuesday, Microsoft released patch KB5051987 for Windows 11 version 24H2, writes the XDA Developers site. 

But "As reported by Windows Latest and various communities like Reddit and Microsoft's help forum, many users have encountered a major issue..." 
Some have reported that, in addition to File Explorer failing to launch, they're unable to open folders from the desktop, save Office files, or even download files. Clicking on a folder icon may display its subfolders, but the contents within remain inaccessible... Some users on Microsoft's help forum and Reddit have also reported that the KB5051987 patch fails to install entirely. The update gets stuck at a certain percentage for hours before eventually displaying an error code. While these are among the most widely reported issues, others have surfaced as well, including problems with Taskbar preview animations, the camera, and more. 

"Microsoft keeps running into brick walls with the 2024 version of Windows 11," writes ZDNet. "Each new update designed to fix the outstanding bugs ends up introducing other problems..."
Among the glitches resolved were ones that affected digital audio converters, USB audio drivers, USB cameras, and passkeys. The update also patched several security vulnerabilities, including some that were deemed critical.... 

Other glitches that may pop up include a stuttering mouse, an undetectable camera, .NET apps that cannot be installed inside the Windows Sandbox, and the Taskbar's new preview animation that does not work properly. You may also encounter other roadblocks. One person in the Windows Feedback Hub said that after installing the update, the battery life shows only 2.5 hours versus 6 hours previously. Another person found that the clipboard history no longer copies items from Microsoft Word... 
Each annual Windows update can suffer from bugs, especially after being rolled out to millions of users. However, Windows 11 24H2 has been more problematic than usual. Since its official launch last October, the 2024 version has carried with it a host of known issues, many of which still haven't been resolved.
]]></content:encoded></item><item><title>The pain of discontinued items, and the thrill of finding them online</title><link>https://techcrunch.com/2025/02/22/the-pain-of-discontinued-items-and-the-thrill-of-finding-them-online/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sat, 22 Feb 2025 20:02:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We’ve all been there. A favorite item is suddenly unavailable for purchase. Couldn’t the manufacturer have given you advance warning? Whether owing to low sales, changing habits, production costs, or even because something is a little wrong with your favorite product (shh), discontinued items are part of life. In a weekend piece, the New York […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>September 17, 1787: &quot;A Republic, If You Can Keep It&quot;</title><link>https://www.nps.gov/articles/000/constitutionalconvention-september17.htm</link><author>037</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:58:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Monday, September 17, 1787: The Convention TodayThe day began with a prepared speech from Franklin (PA) who, eighty-one years old and painfully afflicted with gout and kidney stone, was unable to read it himself and delegated that task to Wilson (PA). While the speech was formally addressed to Washington (VA), the Convention’s president, its purpose was to convince the three delegates who had announced their refusal to sign the Constitution—Gerry (MA), Randolph (VA), and Mason (VA)—to abandon their opposition. Franklin began on a note of humility. “I confess that there are several parts of this Constitution which I do not at present approve, but I am not sure I shall never approve them. For having lived long, I have experienced many instances of being obliged by better information, or fuller consideration, to change opinions even on important subjects, which I once thought right, but found to be otherwise. It is therefore that, the older I grow, the more apt I am to doubt my own judgment, and to pay more respect to the judgment of others.” “In these sentiments, Sir, I agree to this Constitution, with all its faults, if they are such; because I think a General Government necessary for us, and there is no form of government, but what may be a blessing to the people if well administered; and believe further, that this is likely to be well administered for a course of years, and can only end in despotism, as other forms have done before it, when the people shall become so corrupted as to need despotic government.”  He didn’t think another Convention (which Mason and Randolph had argued for) would do any better than the first had. He admitted that the men in the room were all well-reasoned and had a diversity of opinions, making it difficult to find common ground. “From such an assembly can a perfect production be expected? It therefore astonishes me, sir, to find this system approaching so near to perfection as it does.... Thus I consent, Sir, to this Constitution, because I expect no better, and because I am not sure, that it is not the best.” “On the whole, Sir, I cannot help expressing a wish that every member of the Convention, who may still have objections to it, would with me, on this occasion, doubt a little of his own infallibility, and to make manifest our unanimity, put his name to this instrument.” Franklin then moved for the form of the signing to be such: “Done in Convention by the unanimous consent of the States present, the seventeenth of September, &c. In witness whereof, we have hereunto subscribed our names.” This form had actually been thought up by Gouverneur Morris (PA), who had given it to Franklin so that Franklin’s esteem would lend it credence. The wording of the form doesn’t explicitly state that the signer is endorsing the Constitution. It only means that the signer is affirming that the states present in the Convention unanimously approved the Constitution. The idea was to get Gerry, Mason, and Randolph to sign by making their personal objections irrelevant to their signatures. Gorham (MA) then motioned for Congress to be given the power to increase the size of the House of Representatives from one representative for every 40,000 people to one for every 30,000. (Mind that Congress would not have been required to increase the House to such a size, but just given the option to do so.) Proposals such as this one had repeatedly failed, but King (MA) and Carroll (MD) seconded him. Now, on this last day of the Convention, Washington (VA) spoke for the only time. While he said it was typically inappropriate for him, as president of the Convention, to offer his opinion, he felt called to support Gorham’s motion. He thought increasing the size of the House of Representatives would increase the “security of the rights and the interests of the people.” After Washington’s speech, no one spoke in opposition to the motion, and it passed unanimously. Jacob Shallus, the scribe who had the day before handwritten the engrossed copy of the Constitution, corrected the text to reflect this final amendment. Randolph gave a brief speech where (much like one from two days earlier) he was almost apologetic about refusing to sign the Constitution, but left open the possibility that he might support the Constitution when Virginia considered ratifying it. He stated, “Nine States [the minimum number for the Constitution to take effect] will fail to ratify the plan, and confusion must ensue.” G. Morris and Williamson (NC) gave speeches encouraging the holdouts to sign. Hamilton (NY) spoke similarly, with Madison (VA) summarizing him thus: “No man’s ideas were more remote from the plan than his own were known to be; but is it possible to deliberate between anarchy and convulsion on one side, and the chance of good to be expected from the plan on the other?” Blount (NC) stated that his signature should not be taken as a sign of his support for the Constitution but just as his affirmation that the Constitution had been unanimously approved by the states at the Convention. Franklin gave a second speech where he personally begged Randolph to sign. Randolph said that Franklin’s proposed form for the signatures didn’t make a difference: signing the Constitution would imply that he supported it, and he didn’t. Madison writes that “He [Randolph] repeated, that, in refusing to sign the Constitution, he took a step which might be the most awful of his life; but it was dictated by his conscience, and it was not possible for him to hesitate, — much less, to change.” Randolph thought that presenting the Constitution to the American people to only accept or reject in total, without amendments, would cause all the “anarchy and civil convulsions” which the soon-to-be signers professed to want to avoid. Gerry gave a speech to the same effect. Charles Cotesworth Pinckney (SC) did not like the ambiguity in Franklin’s proposed form for the signatures. He supported the Constitution and intended his signature to be a sign of that support. Ingersoll (PA) took a middle position: his signature would not indicate his support for the Constitution, but neither would it merely be his attestation to the Convention’s unanimity. His signature would be his “recommendation” that the Convention’s final product, “all things considered, was the most eligible.” Franklin’s motion (related to the form of the signing) passed 10–1, with South Carolina’s vote divided on account of C.C. Pinckney and Butler wanting the form to be more emphatically supportive. The Convention then voted to deposit their official journals (which ended up being much less detailed than Madison’s personal notes) with Washington.  The delegates then proceeded to sign the engrossed copy of the United States Constitution. Thirty-eight men signed thirty-nine names—Dickinson (DE) was ailing with a headache and had asked Read (DE) to sign for him two days earlier. Despite so many personal appeals, Gerry, Randolph, and Mason still refused to sign. Hamilton, as the only New Yorker present at this point, signed in a personal capacity, since New York could not be effectively represented in the Convention by only one delegate. The signatures were grouped by state, with Pennsylvania’s eight being the most. The listing of state names next to the signatures appears to be in the hand of Hamilton. Rhode Island, the only state not to send delegates to the Convention, is not listed. After the signing, the Convention adjourned for a final time. The signatures did not have any legal significance. The Constitution was clear: it would only go into effect when nine of the thirteen states chose to ratify it. As hard as the past four months had been, the real challenge lay ahead, in convincing the American people to embrace the government that these men had authored. As the last names were being signed, Franklin, in a personal aside to some other members, made an observation about the chair that Washington had been sitting in as he presided over the Convention. The chair had an emblem of half of a sun. Franklin noted that artists often have a hard time distinguishing between a rising and a setting sun in their artwork. “I have often and often, in the course of the session, and the vicissitudes of my hopes and fears as to its issue, looked at that behind the President, without being able to tell whether it was rising or setting: but now at length, I have the happiness to know, that it is a rising and not a setting sun.” ]]></content:encoded></item><item><title>Utah Bill Aims to Make Officers Disclose AI-Written Police Reports</title><link>https://www.eff.org/deeplinks/2025/02/utah-bill-aims-make-officers-disclose-ai-written-police-reports</link><author>hn_acker</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:53:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>California Sues Data-Harvesting Company NPD, Enforcing Strict Privacy Law</title><link>https://yro.slashdot.org/story/25/02/22/1512258/california-sues-data-harvesting-company-npd-enforcing-strict-privacy-law?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 19:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[California sued to fine a data-harvesting company, reports the Washington Post, calling it "a rare step to put muscle behind one of the strongest online privacy laws in the United States."

Even when states have tried to restrict data brokers, it has been tough to make those laws stick. That has generally been a problem for the 19 states that have passed broad laws to protect personal information, said Matt Schwartz, a policy analyst for Consumer Reports. He said there has been only 15 or so public enforcement actions by regulators overseeing all those laws. Partly because companies aren't held accountable, they're empowered to ignore the privacy standards. "Noncompliance is fairly widespread," Schwartz said. "It's a major problem." 

 That's why California is unusual with a data broker law that seems to have teeth. To make sure state residents can order all data brokers operating in the state to delete their personal records [with a single request], California is now requiring brokers to register with the state or face a fine of $200 a day. The state's privacy watchdog said Thursday that it filed litigation to force one data broker, National Public Data, to pay $46,000 for failing to comply with that initial phase of the data broker law. NPD declined to comment through an attorney... This first lawsuit for noncompliance, Schwartz said, shows that California is serious about making companies live up to their privacy obligations... "If they can successfully build it and show it works, it will create a blueprint for other states interested in this idea," he said. 
Last summer NPD "spilled hundreds of millions of Americans' Social Security Numbers, addresses, and phone numbers online," according to the blog Krebs on Security, adding that another NPD data broker sharing access to the same consumer records "inadvertently published the passwords to its back-end database in a file that was freely available from its homepage..." 
California's attempt to regulate the industry inspired the nonprofit Consumer Reports to create an app called Permission Slip that reveals what data companies collect and, for people in U.S. states, will "work with you to file a request, telling companies to stop selling your personal information." 

Other data-protecting options suggested by The Washington Post:
 Use Firefox, Brave or DuckDuckGo, "which can automatically tell websites not to sell or share your data. Those demands from the web browsers are legally binding or will be soon in at least nine states."
Use Privacy Badger, an EFF browser extension which the EFF says "automatically tells websites not to sell or share your data including where it's required by state law."]]></content:encoded></item><item><title>The Birth of Chipzilla</title><link>https://www.abortretry.fail/p/the-birth-of-chipzilla</link><author>rbanffy</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:04:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I do not expect to join any company which is simply a manufacturer of semiconductors. I would rather try to find some small company which is trying to develop some product or technology which no one has yet done. To stay independent (and small) I might form a new company, after a vacation. just called me on the phone. We'd been friends for a long time. Documents? There was practically nothing. Noyce's reputation was good enough. We put out a page-and-a-half little circular, but I'd raised the money even before people saw it.Intel’s structure in these early days had Rock as Chairman, Noyce as CEO, Moore as Executive VP, Grove as director of engineering, and Vadász as the director of MOS design.Before the company could begin operations, they needed a place to work. Over on Middlefield Road in Mountain View, Union Carbide was vacating a building. They’d really just begun that process, but a conference room was available. Intel rented it. As Intel took over the building, Moore felt the space was larger than what they really needed, but whether he was right or wrong, that wouldn’t be the case for too long. Official operations of Intel began in August of 1968.For Intel’s first few months, the company had quite a bit of work. They needed to build a manufacturing process, figure out their product line, and hire enough people to get products made and sold. This involved them needing to build Fab1 as quickly as possible. The goal was to get things done by the end of December, and this meant that they wouldn’t be building every tool they needed. At Wescon, they actually bought some equipment right off the show floor. A few things had to be adapted to meet their needs, but most of their equipment wasn’t made by them. This wasn’t solely due to the timing constraints either. The founders of Intel had learned from their prior experiences in this industry. While a single company might be able to innovate in some areas, it wasn’t long before competitors replicated that success. Then, those competitors would also be iterating and innovating. A single company had a limited customer base, but the market as a whole was larger and innovations could come from anywhere. They wanted everyone’s expertise, so they bought what they could from whomever had the best tools. Moore worked closely with Intel’s suppliers, and he even made some investments in them personally. While Moore, Noyce, and Rock felt little fear at Intel, this wasn’t true for Grove. At Fairchild, Grove had been strictly R&D. He didn’t really see the rest of the company, and he’d been hired by Moore out of graduate school. At Intel, he had the chance to really see the entire company and its operations. He constantly feared that the company would go bankrupt, but perhaps as a result of that fear, he became quite focused on manufacturing. He wanted the company to be able to do the same things every day to make a uniform product successfully, affordably, and reliably. He quickly began handle much of the company’s operations. The team that was assembled to undertake these early efforts was phenomenal, and Intel met its goal completing the Fab1 setup by the 31st of December.Before Fab1 was even completed, Noyce had visited Sharp in Japan. They spoke Tadashi Sasaki with the hope of gaining (at least some of) the company’s business for ICs. Sasaki then asked Rockwell, who did their manufacturing work, if they could give Intel some of their business, and Rockwell said no; they wanted to enforce their contract’s exclusivity. Sasaki then invested 40 million yen into his friends Yoshio Kojima’s company, Bijikon Kabushiki-gaisha (or Busicom), in Nara, Japan which gave him quite a bit of influence there, and allowed him to work on a CMOS calculator with Intel. Marcian Edward Hoff Jr (or Ted), Intel employee twelve, was the person initially tasked with this project on the Intel side of things. He worked as both an engineer and the liaison between the two companies.In April of 1969, Intel released the 3101 SRAM. The 3101 held 64 bits of data (eight letters of sixteen digits), and sold for $99.50 (about $850 in 2025 dollars). Even at this time, the chip had far too little storage capacity to be useful as main memory (especially with its high price), but it was incredibly fast with an access time of fifty nanoseconds and it dissipated just six milliwatts per bit. With the chip’s high speed, low power draw, and small size, it was quite useful for processor registers in minicomputers and mainframes. The company had been hoping to gain a Honeywell contract with the 3101. That didn’t happen, but the chip was moderately successful. This chip was, in fact, successful enough that Intel’s work with Busicom fell on the back burner.Busicom had initially provided Intel with a twelve chip, PMOS, decimal arithmetic design for their calculator hardware, and by June, Masatoshi Shima had refined this to seven chips. He didn’t have a full schematic, but it was something to work with. Shima then visited Intel in June with two other Busicom engineers. When Shima arrived, he was a bit dismayed. Almost no progress had been made, and he discovered that Intel had some serious expertise in hardware and memory design, but they didn’t have any serious expertise in logic design. Stan Mazor began to act as a buffer between Shima and Hoff, and over three months, there were many meetings about the design, but almost no progress.At this time, Intel was just a memory company. They had packaging to accommodate early memories and nothing more. This limited the design of the chips to 4bit allowing the chip to fit in a 16-pin or 18-pin DIP. These restrictions were not initially given to Busicom, but Hoff knew them, and they influenced what he presented. Unknown to Intel, Sasaki had also come up with a four chip design and gave this to Busicom. With both sides now agreeing to a four chip design, it would seem that things could have progressed, but unknown to Busicom, no more progress would be made that year.In July of 1969, Intel released the 1101. This was first MOS memory chip. The 1101 was slower than the 3101, but had a capacity of 256 bits, used silicon gates, and allowed for far higher memory densities than had previously been possible.In December of 1969, CTC provided Intel the CPU requirements and preliminary designs for a processor they planned to use in their Datapoint 2200 terminal. Intel was then supposed to implement these in LSI. At Intel this project became the 1201, but much like the Busicom project, it wasn’t given any priority. Intel was a memory company. Frustrated, CTC turned to Texas Instruments who also failed, and CTC eventually built the processor out of discreet components.For 1969, Intel ended the year with revenues of $565,874 and a loss for the year of $1,912,833 which meant a loss per share of $1.66. While this sounds terrible, the company had set expectations that this would more or less be the case.Back on the Busicom project, Hoff eventually brought a proposal chip design to Shima that would use a general purpose 4bit CPU with binary logic, a ROM chip, and a RAM chip. Shima then added a 10bit static shift register for use with the printer and keyboard interface of the calculator. They improved the instruction set, the bus, and by December of 1969, Shima was able to write the functional specification. The agreement for Intel to build the chips was officially entered into on the 6th of February in 1970.In April of 1970, Federico Faggin joined Intel after having left Fairchild. He’d previously worked with Vadász at Fairchild, and his skills, especially his having developed silicon gate technology, were well known to the executives at Intel. Their company was essentially based upon Faggin’s work. Given both Faggin’s background and Intel’s needs, his request to work on chip design was immediately accepted. Faggin now had just six months to produce working silicon from a specification, and he didn’t have anyone to help him in the endeavor. Shima returned to Intel just a few days later and was rather unhappy to discover that effectively zero further work had been done on the calculator chips. Faggin started working 12 to 16 hours each day. This was particularly hard for him given he had a month old daughter at home and a loving wife. During this time, his wife went to Italy with her sister as he couldn’t be around to help. I am sure that Faggin somewhat enjoyed the work, but the hours, the pace, the separation from his family, and the deadline were probably dreadful. Adding to this, when Faggin reviewed the specification, he thought it to be a poor design. There was much that he wished to change, but he didn’t have the time. More over, when he asked questions, he was told that this was his project and he simply needed to figure it out. He verified that the architecture was free of errors, but otherwise didn’t change it. He had too much to do. There was still logic design to be done, circuit design, layout, cutting, masking, initial fabrication, testing, fixing any problems found in testing, and then the tasks related to production.PDP-11In December of 1970, the Busicom chips came back for testing. None of them worked. Faggin began checking them with a microscope, and he realized that one of the masks hadn’t been applied. He noted this, and requested another run of chips be made. Those came back three weeks later. In January of 1971, Federico Faggin completed testing of the first single-chip microprocessor ever made. It was his second self-designed computer, but this one was built with multiple technologies he had pioneered. He was just 29 years old. The four chips he created were the 4001, 4002, 4003, and the 4004. The 4001 was a ROM of 256 bytes with a built-in 4bit I/O port. The 4002 was a RAM chip of 40 bytes, which would, in this case, hold 80 4bit words. The 4003 was a 10bit shift register, and in the system for which this chip was designed, it would handle the keyboard and printer. The Intel 4004 was a 4bit microprocessor that ran at 750kHz. It was built of 2300 transistors on a 10 micron process and packaged in a 16-pin DIP. It used 8bit instructions, had 4bit data words, and 12bit addressing. This rather unusual arrangement meant that the 4004 could address 4120 bits of RAM which would mean 1280 4bit characters. It could address 32768 bits of ROM which would mean 4096 8bit words. The 4004 had 46 instructions and 16 registers.Intel’s revenues for 1970 were $3,932,517 but the company posted another loss (though far smaller) of $969,915 which meant a loss of $0.69 per share.1971 was a big year. The Intel employee head count hit 460, the 4004 became available, the 1103 was selling well, and the company released the 1601 in January. The 1601 was the first commercially available PROM built on P-channel silicon gate MOS. The company also began selling fully assembled circuit boards populated with memory chips for mainframes and minicomputers. Intel completed the build out a new headquarters of 78,000 square feet in Santa Clara providing them about three times the space they had previously. This, naturally, allowed them to dramatically increase production. Unlike the previous location, the Santa Clara location had chemical and solvent disposal systems in place from day one preventing this from later becoming a superfund site. In September, Intel released the first EPROM, the 1701. This was an improvement over the 1601 in that it could be reprogrammed. The package had a black plastic cover over the chip. With that cover removed, shining ultra violet light on the chip would erase it and allow it to be programmed again. All of these factors combined allowed Intel to post its first profit at $1,015,080 on revenues of $9,411,821. Share holders saw $0.17 earned on each share.Q1 CorporationOf course, Intel now had Faggin. Immediately after wrapping up the 4004, he was tasked with bringing this new 8bit design into the world, and essentially all of the problems from the 4004 project were present. The design wasn’t great, he had very little time, and he had very little staff. The 1201 became the Intel 8008 which was made available commercially in April of 1972. The 8008 was an 8 bit CPU with a 14 bit address width. It was manufactured on a 10 micron process and built of 3500 transistors. It was packaged in an 18-pin DIP, which resulted in 30 TTL chips being required to interface memory and I/O. This packaging was dictated by Grove and Vadász despite protest against it and meant that the chip did not perform as well as it could have otherwise. The 8008 could achieve around 29,000 operations per second. True to his word, Alroy bought a bunch of 8008s and delivered the world’s first complete, pre-assembled, commercially produced, microcomputer system to the Litcom Division of Litton Industries on Long Island on the 11th of December in 1972.In June of 1973, Intel introduced the Intellec 4 and the Intellec 8 microcomputers at the National Computer Conference in the New York Coliseum. These were sold to software and hardware developers in limited numbers. For the Intellec 8, the price started at $2395. These machines had resident monitors in ROM and they had a PL/M compiler (also a new product). The Intellec series supported a teletype at 110 baud, paper tape, and glass teletypes at 1200 baud. The Intellec 4 shipped with 1K RAM and could support up to 4K instruction RAM. Data memory was just 320 4bit words, and could be expanded to 2560 words. The 8, on the other hand, shipped with 5K RAM and could support up to 16K. These machines were offered either as boards or as fully assembled rack-mountable units with front panel and power supply. The fully assembled systems weighed in at 31 pounds.Shortly after the release of the 8008, Federico Faggin began pushing for a new general purpose processor that wouldn’t have the same limitations as the 8008. Intel had developed a new N-channel MOS process for 4K DRAM, and this is what Faggin wanted to use. He also wanted a new bus architecture, a new interrupt structure, more instructions, and a 40-pin DIP; all while keeping the chip source-code compatible with the 8008. He wanted the instruction cycle time to be 2 microseconds which would put the product within the realm occupied by minicomputers. Approval took time but once he had it, Faggin hired Masatoshi Shima to help with logic design and firmware. Throughout 1973, the company introduced various memory chips, support chips for memories and processors, and even an LCD driver. Revenues for the year were $65,593,000 and profit was $9,214,000 or $2.12 per share.The 4004 and 8008 suggested it, but the 8080 made it realZilogDespite a rough second half of the year, Intel still faced little danger. All of their debts had been paid, they owned their facilities with no mortgages, they owned all of their production equipment, and they required no equity financing having nearly $2 million in cash. In December of 1974, the company announced that Gordon Moore would become president and CEO, Robert Noyce would become chairman of the board, Arthur Rock would become vice chairman, Ed Gelbach would become senior vice president and general manager of the components division, Jack Carsten the vice president of marketing, Gene Flath the vice president of manufacturing, Les Vadász the vice president of engineering, and finally, Andrew Grove would become executive vice president. The company had grown and the restructuring was necessary, and it positioned Intel quite well for the changes and challenges yet to come.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.]]></content:encoded></item><item><title>A map of torii around the world</title><link>https://www.google.com/maps/d/viewer?mid=1RNaaTlz7U2FgjlvFARZQWHsMeWsTc2S1&amp;hl=en</link><author>ilamont</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:01:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Open full screen to view more]]></content:encoded></item><item><title>Amazon now discloses you&apos;re buying a license to view Kindle eBooks</title><link>https://blog.the-ebook-reader.com/2025/02/22/amazon-now-openly-discloses-youre-buying-a-license-to-view-kindle-ebooks/</link><author>DavideNL</author><category>hn</category><pubDate>Sat, 22 Feb 2025 18:50:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Amazon recently changed the wording on their website when it comes to buying Kindle ebooks.As the screenshot above shows, they now have a disclaimer under the buy now button that says, “By placing your order, you’re purchasing a license to the content and you agree to the Kindle Store Terms of Use”.It also says that same thing when shopping for Kindle ebooks directly from the store on Kindle ereaders and Kindle apps.The funny thing is they only appear to be doing that in the US. I checked Amazon UK and Amazon CA and both show the old disclaimer that just says, “By clicking the above button, you agree to the Kindle Store Terms of Use”.If you click the terms of use link you’ll find a page full of legal disclaimers indicating you’re buying a license to access Kindle content, but they don’t openly disclose it on product pages under the buy button like on the Amazon US website.I read somewhere about a new law that was passed in California where companies have to “conspicuously” disclose that customers are buying a license when it comes to digital media like ebooks, so that’s likely the reason why Amazon made the change.But not all ebook stores are following the same path. Kobo still just has a link to their terms of sale page when you go to checkout. Apple doesn’t say anything about licenses at all when trying to buy an ebook from them. Google doesn’t say anything about it on their ebook product pages, but they do say you’re purchasing a license before confirming the purchase, with a link to their terms of service.Some people still don’t know that when buying digital content you’re buying a license to view said content, not the actual content itself. You don’t actually “own” the ebooks you purchase; you just own the rights to view them. It’s a distinction that applies to digital media since you can’t physically own it. I think it’s a good idea for companies to openly disclose that fact before buying. Nobody is going to read through those ridiculous TOS pages before purchasing something.]]></content:encoded></item><item><title>ArcaOS (OS/2 Warp OEM) 5.1.1 Has Been Released</title><link>https://tech.slashdot.org/story/25/02/22/0748224/arcaos-os2-warp-oem-511-has-been-released?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["IBM stopped supporting OS/2 at the end of 2006," write the makers of ArcaOS, an OEM distribution of OS/2's discontinued Warp operating system. 

And now long-time Slashdot reader martiniturbide tells us that ArcaOS 5.1.1 has been released, and that many of it's components have been updated too. From this week's announcement:

ArcaOS 5.1.1 continues to support installation on the latest generation of UEFI-based systems, as well as the ability to install to GPT-based disk layouts. This enables ArcaOS 5.1.1 to install on a wide array of modern hardware. Of course, ArcaOS 5.1.1 is just as much at home on traditional BIOS-based systems, offering enhanced stability and performance across both environments.... 
Need more convincing? How about a commercial operating system which doesn't spy on you, does not report your online activity to anyone, and gives you complete freedom to choose the applications you want to use, however you want to use them? How about an operating system which isn't tied to any specific hardware manufacturer, allowing you to choose the platform which is right for you, and fits perfectly well in systems with less than 4GB of memory or even virtual machines?
]]></content:encoded></item><item><title>The fallout from HP’s Humane acquisition</title><link>https://techcrunch.com/2025/02/22/the-fallout-of-hps-humane-acquisition/</link><author>Cody Corrall</author><category>tech</category><pubDate>Sat, 22 Feb 2025 18:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome back to Week in Review. This week we’re looking at the internal chaos surrounding HP’s $116 million acquisition of AI Pin maker Humane; Mira Murati’s new AI venture coming out of stealth; Duolingo killing its iconic owl mascot with a Cybertruck; and more! Let’s get into it. Humane’s AI pin is dead. The hardware […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Ask HN: Is anyone still using Dreamweaver?</title><link>https://news.ycombinator.com/item?id=43141368</link><author>gillytech</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:59:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[When I was learning to build websites in 2010 Dreamweaver was the go-to. I remember it thoroughly confused the heck out of me. Anyone here able to use it effectively?]]></content:encoded></item><item><title>Spectacular Connection Between LLMs, Quantum Systems, and Number Theory</title><link>https://www.datasciencecentral.com/spectacular-connection-between-llms-quantum-systems-and-number-theory/</link><author>Vincent Granville</author><category>dev</category><category>ai</category><pubDate>Sat, 22 Feb 2025 17:44:20 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Microsoft Makes More Of Their DirectX Compiler Code Open-Source</title><link>https://www.phoronix.com/news/DirectXShaderCompiler-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 22 Feb 2025 17:37:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Back in 2017 was the initial open-source DirectX Shader Compiler milestone and since then Microsoft has continued iterating on it with better Linux support, new features, and ironing out other gaps in this "DirectXShaderCompiler" project. On Friday they released the newest version of this DirectX Shader Compiler that features another newly open-sourced component...]]></content:encoded></item><item><title>Encrypted Messages Are Being Targeted, Google Security Group Warns</title><link>https://it.slashdot.org/story/25/02/22/0724228/encrypted-messages-are-being-targeted-google-security-group-warns?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google's Threat Intelligence Group notes "the growing threat to secure messaging applications." While specifically acknowledging "wide ranging efforts to compromise Signal accounts," they add that the threat "also extends to other popular messaging applications such as WhatsApp and Telegram, which are also being actively targeted by Russian-aligned threat groups using similar techniques. 

"In anticipation of a wider adoption of similar tradecraft by other threat actors, we are issuing a public warning regarding the tactics and methods used to date to help build public awareness and help communities better safeguard themselves from similar threats." 

Computer Weekly reports:

Analysts predict it is only a matter of time before Russia starts deploying hacking techniques against non-military Signal users and users of other encrypted messaging services, including WhatsApp and Telegram. Dan Black, principal analyst at Google Threat Intelligence Group, said he would be "absolutely shocked" if he did not see attacks against Signal expand beyond the war in Ukraine and to other encrypted messaging platforms... 

Russia-backed hackers are attempting to compromise Signal's "linked devices" capability, which allows Signal users to link their messaging account to multiple devices, including phones and laptops, using a quick response (QR) code. Google threat analysts report that Russia-linked threat actors have developed malicious QR codes that, when scanned, will give the threat actor real-time access to the victim's messages without having to compromise the victim's phone or computer. In one case, according to Black, a compromised Signal account led Russia to launch an artillery strike against a Ukrainian army brigade, resulting in a number of casualties... Google also warned that multiple threat actors have been observed using exploits to steal Signal database files from compromised Android and Windows devices. 

The article notes that the attacks "are difficult to detect and when successful there is a high risk that compromised Signal accounts can go unnoticed for a long time." And it adds that "The warning follows disclosures that Russian intelligence created a spoof website for the Davos World Economic Forum in January 2025 to surreptitiously attempt to gain access to WhatsApp accounts used by Ukrainian government officials, diplomats and a former investigative journalist at Bellingcat." 

Google's Threat Intelligence Group notes there's a variety of attack methods, though the "linked devices" technique is the most widely used. "We are grateful to the team at Signal for their close partnership in investigating this activity," Google's group says in their blog post, adding that "the latest Signal releases on Android and iOS contain hardened features designed to help protect against similar phishing campaigns in the future. Update to the latest version to enable these features."]]></content:encoded></item><item><title>Private antitrust cases are going through the courts</title><link>https://www.thebignewsletter.com/p/the-people-take-antitrust-into-their</link><author>toomuchtodo</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:30:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The $1.5B Bybit Hack</title><link>https://blog.trailofbits.com/2025/02/21/the-1.5b-bybit-hack-the-era-of-operational-security-failures-has-arrived/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:05:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Two weeks ago at the DeFi Security Summit, Trail of Bits’ Josselin Feist (@Montyly) was asked if we’d see a billion-dollar exploit in 2025. His response: “If it happens, it won’t be a smart contract, it’ll be an operational security issue.”Today, that prediction was validated.On February 21, 2025, cryptocurrency exchange Bybit suffered the largest cryptocurrency theft in history when attackers stole approximately $1.5B from their multisig cold storage wallet. At this time, it appears the attackers compromised multiple signers’ devices, manipulated what signers saw in their wallet interface, and collected the required signatures while the signers believed they were conducting routine transactions.This hack is one of many that represent a dramatic shift in how centralized exchanges are compromised. For years, the industry has focused on hardening code and improving their technical security practices, but as the ecosystem’s secure development life cycle has matured, attackers have shifted to targeting the human and operational elements of cryptocurrency exchanges and other organizations.These attacks reveal an escalating pattern, with each compromise building on the last:In each case, the attackers didn’t exploit smart contract or application-level vulnerabilities. Instead, they compromised the computers used to manage those systems using sophisticated malware to manipulate what users saw versus what they actually signed.The DPRK’s Cryptocurrency Theft InfrastructureThe attack chain typically begins with aggressive social engineering campaigns targeting multiple employees simultaneously within an organization. The RGB identifies key personnel in system administration, software development, and treasury roles, then creates detailed pretexts - often elaborate job recruitment schemes - customized to each target’s background and interests. These aren’t mass phishing campaigns; they’re meticulously crafted approaches designed to compromise specific individuals with access to critical systems.What makes these attacks particularly concerning is their repeatability. The RGB has built a sophisticated cross-platform toolkit that can:Operate seamlessly across Windows, MacOS, and various wallet interfacesShow minimal signs of compromise while maintaining persistenceFunction as backdoors to execute arbitrary commandsDownload and execute additional malicious payloadsManipulate what users see in their interfacesEach successful compromise has allowed the RGB to refine their tools and techniques. They’re not starting from scratch with each target - they’re executing a tested playbook that’s specifically engineered to defeat standard cryptocurrency security controls when those controls are used in isolation.Organizations below a certain security threshold are now at serious risk. Without comprehensive security controls including:Air-gapped signing systemsMultiple layers of transaction verificationEndpoint detection and response (EDR) systems like CrowdStrike or SentinelOneRegular security training and war gamesThey are likely to face an adversary that has already built and tested the exact tools needed to defeat their existing protections.The New Reality of Cryptocurrency SecurityThis attack highlights a fundamental truth: no single security control, no matter how robust, can protect against sophisticated attackers targeting operational security. While secure code remains crucial, it must be part of a comprehensive security strategy.Organizations must adopt new processes and controls that operate under the assumption that their infrastructure will eventually face compromise:Infrastructure Segmentation: Critical operations like transaction signing require both physical and logical separation from day-to-day business operations. This isolation ensures that a breach of corporate systems cannot directly impact signing infrastructure. Critical operations should use dedicated hardware, separate networks, and strictly controlled access protocols. Security controls must work in concert - hardware wallets, multi-signature schemes, and transaction verification tools each provide important protections, but true security emerges from their coordinated operation. Organizations need multiple, overlapping controls that can detect and prevent sophisticated attacks.Organizational Preparedness: Technical controls must be supported by comprehensive security programs that include:Thorough threat modeling incorporating both technical and operational risksRegular third-party security assessments of infrastructure and proceduresWell-documented and frequently tested incident response plansOngoing security awareness training tailored to specific rolesWar games and attack simulations that test both systems and personnelThese principles aren’t new - they represent hard-won lessons from years of security incidents in both traditional finance and cryptocurrency. Trail of Bits has consistently advocated for this comprehensive approach to security, providing concrete guidance through several key publications:These publications show a clear pattern that’s echoed by recent attacks: sophisticated attackers are increasingly targeting operational security vulnerabilities rather than technical flaws.The cryptocurrency industry’s resistance to implementing traditional corporate security controls, combined with the high value of potential targets and this group’s sophisticated capabilities, suggests these attacks are likely to continue unless significant changes are made to how cryptocurrency companies approach operational security.The Bybit hack marks a new era in cryptocurrency security. Industry participants need to recognize the evolving threat landscape and invest additional resources in improving their operational security. No one understands this reality better than the security researchers who have been tracking these attacks.Tay @tayvano_, a renowned security researcher known for exposing on-chain thefts, dissecting DPRK crypto hacks, and fiercely advocating for better blockchain security practices, summarized the current reality bluntly:For all these reasons and more, it’s my opinion that once they get on your device, you’re fucked. The end.
If your keys are hot or in AWS, they fuck you immediately.
If they aren’t, they work slightly harder to fuck you.
But no matter what, you’re going to get fucked.Organizations must protect themselves through a comprehensive defense strategy combining isolation, verification, detection, and robust operational security controls. However, the time for basic security measures has passed. Organizations holding significant cryptocurrency assets must take immediate action:Implement dedicated, air-gapped signing infrastructureEngage with security teams experienced in defending against sophisticated state actorsBuild and regularly test incident response plansThe next billion-dollar hack isn’t a matter of if, but when. The only question is: will your organization be ready?]]></content:encoded></item><item><title>Exult: Recreating Ultima VII for modern operating systems</title><link>https://exult.sourceforge.io/index.php</link><author>nateb2022</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:56:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Ultima VII turns 30 - Exult releases version 1.830 years ago Origin released Ultima VII - The Black Gate and many still
	hold this game in a very high regard. We certainly do!
	Instead of waiting another decade this is the perfect opportunity to release
	our next version. Because of the measures we took to eliminate weird crashes
	it was important to make a new release available.
	By default we will now play the extended intro of Serpent Isle, that we restored
	from an old recording of it. It was cut and slimmed down for the release of
	Ultima VII's Part 2 to conserve precious floppy disk space. A shout out to
	Denis Loubet for creating this masterpiece.
	Two important things were always missing from Exult in regards to the Black
	Gate:
	The sound effects in the intro and the congratulations screen! Both are now
	present!
	Currently we are in the process of merging Ken Cecka's fork of Exult for
	 (*). While this is not yet finished, Ken created a 1.8 version
	based on our release code.

	You can find the Android, Windows and macOS releases on our
	download page (**).
	Please play with it and tell us of any problem you encounter using the
	forum or
	bug tracker.A brief list of the most important changes in Exult since v1.6:Windows builds are by default 64bitExult Studio modernized by porting it to GTK+ 3 (thanks Dragon Baroque)Exult Studio now offered as a download for macOSExult shows the proper ending screen for BG (you have beaten Ultima VII in nn days...)Sound effects in the BG intro were missingExtended intro for SI implementedMore crashes eliminated caused by cached out objects"Gumps pause game" no longer delays usecode eventsSmooth virtual joystick movement on iOSiOS shows the mouse cursor when a real mouse device is usedScreenshots will now be saved in the PNG formatStatus bars can now also be vertical on both sides of the screenUpdated Windows installer to download and install the audio packs and our modsOur options have been reorderedBut we do have known issues:Antimagic rain caused by the cube generator is still not dissipating as fast as it shouldExult does not return to the game menu after beating the game (instead it quits)Some schedules need more detailed loveSeveral bugs that need more in-depth looking at but no plot-stopping bugs(**) All files have been scanned and cleared with
	Virus Total's online scanner, however Avast
	falsely flags our InnoSetup based installer for Windows.]]></content:encoded></item><item><title>Trump administration reportedly shutting down federal EV chargers nationwide</title><link>https://techcrunch.com/2025/02/22/trump-administration-reportedly-shutting-down-federal-ev-chargers-nationwide/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 22 Feb 2025 16:55:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The General Services Administration, the agency that manages buildings owned by the federal government, is planning to shut down its entire network of electric vehicle chargers, according to a report in The Verge. The GSA reportedly operates a network of hundreds of EV chargers with a total of 8,000 plugs that can be used to […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>FFmpeg School of Assembly Language</title><link>https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md</link><author>davikr</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:52:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Recovering priceless audio and lost languages from old decaying tapes</title><link>https://theconversation.com/how-were-recovering-priceless-audio-and-lost-languages-from-old-decaying-tapes-248116</link><author>PaulHoule</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:45:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Remember cassettes? If you’re old enough, you might remember dropping one into a player, only to have it screech at you when you pressed “play”. We’ve fixed that problem. But why would we bother?Before the iPod came along, people recorded their favourite tunes straight from the radio. Some of us made home recordings with our sibling and grandparents – precious childhood snippets. And a few of us even have recordings from that time we travelled to a village in Vanuatu, some 40 years ago, and heard the locals performing in a language that no longer exists.In the field of linguistics, such recordings are beyond priceless – yet often out of reach, due to the degradation of old cassettes over time. With a new tool, we are able to repair those tapes, and in doing so can recover the stories, songs and memories they hold. A digital humanities telescopeOur digital archive, PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures) contains thousands of hours of audio – mainly from musicological or linguistic fieldwork. This audio represents some 1,360 languages, with a major focus on languages of the Pacific and Papua New Guinea. The PARADISEC research project was started in 2003 as a collaboration between the universities of Melbourne and Sydney, and the Australian National University. Like a humanities telescope, PARADISEC allows us to learn more about the language diversity around us, as we explained in a 2016 Conversation article.While many of the tapes we get are in good condition and can be readily played and digitised, others need special care, and the removal of mould and dirt.We work with colleagues at agencies such as the Solomon Islands National Museum, for whom we recently repaired a set of cassettes that were previously unplayable and just screeched. We’ll be taking those cassettes, now repaired and digitised, back to Honiara in February and expect to pick up more for further treatment. Screeching happens when a tape is dried out and can’t move through the mechanism easily. The screeching covers the audio signal we want to capture. In 2019, my colleague Sam King built (with the help of his colleague Doug Smith) a cassette-lubricating machine while working at the Australian Institute of Aboriginal and Torres Strait Islander Studies. This machine – likely the first of its kind in Australia – allowed us to play many previously unplayable tapes.Last year, Sam built two versions of an updated machine called the LM-3032 Tape Restorator for PARADISEC, improving on the previous model. Between hand building some parts, 3D printing others and writing code for the controllers, it took him more than a year.Preserving culture and heritageThe LM-3032 Tape Restorator works by applying cyclomethicone (a silicone-based solvent used in cosmetics) to the length of a tape. This leaves behind an extremely thin film of lubrication that allows smoother playback, making digitisation possible. See more details here.Tests have shown this process has no negative long-term effects on the tape. In fact, tapes treated with this method five years ago still play without issues. This technological wizardry allows us to salvage precious analogue recordings before it’s too late. For many languages, these may be the only known recordings – stored on a single cassette, in a single location, and virtually inaccessible. Some of the primary research records digitised by PARADISEC have survived long periods of neglect in offices, garages and attics.The audio below is from a tape that was kept at Fitzroy Crossing in the Kimberley for 40 years. It features beautiful singing in the local Walmajarri language, with guitar accompaniment. The first seven seconds are from the untreated tape, while the rest is from the treated version.
        Singing in Walmajarri, with guitar accompaniment. A side-by-side comparison of a tape treated with the LM-3032 Tape Restorator.
        Our experience has shown community members truly value finding records in their own languages, and we’re committed to making this process easier for them.Here’s one testimonial from E’ava Geita, Papua New Guinea’s current acting Solicitor General. In 2015, Geita was overjoyed to hear digitised records capturing PNG’s Koita language:If only you witnessed and captured the reaction in me going through the recordings at home! It is quite an amazing experience! From feeling of awe to emotion to deep excitement! The feeling of knowing that your language has been documented or recorded in a structured way, kept safely somewhere in the world, hearing it spoken 50–60 years ago and by some people you haven’t seen but whose names you only hear in history is quite incredible. It is most heartwarming to know that it is possible to sustain the life of my language. Thank you once again for the opportunity to listen to the records. Acknowlegement: I’d like to thank Sam King for the technical information provided in this article.]]></content:encoded></item><item><title>James Bond&apos;s Next Assignment: Amazon Pays $1 Billion for Full Creative Control</title><link>https://entertainment.slashdot.org/story/25/02/22/0638211/james-bonds-next-assignment-amazon-pays-1-billion-for-full-creative-control?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Deadline reports:
It's taking around $1 billion to have 007 stewards Barbara Broccoli and Michael G. Wilson cede creative oversight of their family's storied James Bond franchise to Amazon MGM Studios, sources tell us. Amazon originally overpaid on its purchase of MGM in a deal orchestrated by then-MGM board chair Kevin Ulrich. Though valued between $3.5 billion-$4 billion, the legendary motion picture studio was absorbed by the streamer for $8.5 billion, the hefty sum propped up by the potential access of the 007 franchise. However, Amazon couldn't fully freely develop Bond with Broccoli and Wilson in the mix. Hence, it took another $1 billion to ensure that they could fully steer and exploit the Ian Fleming IP. 
The article suggests Broccoli's long hold-out came from "Amazon's desire to expand the James Bond franchise into its own universe akin to Marvel or Star Wars."
In the past, filmmakers including Quentin Tarantino and Christopher Nolan have expressed an interest in putting their stamp on Bond; both however, required complete creative control, which wasn't possible under the reign of Broccoli and Wilson. Now, with the producers on the side, Amazon can move forward to attract a top-tier director. 

Also available to come to life in the new deal finally are a slew of Bond villains and women in their own series or features. The last time an attempt was made to spin off the Bond franchise was in 2003 with a stand-alone movie about the spy's girlfriend Jinx, played by Halle Berry in Die Another Day. Bond scribes Neal Purvis and Rob Wade were attached to pen that, with Stephen Frears circling, but Broccoli and Wilson put the kibosh to the idea due to creative differences. 
In a related note, the article adds that Amazon "is looking to have an international theatrical distribution arm fully operational by some time in 2026." 

Jeff Bezos asked his followers on X.com who should play James Bond in the next movie, reports IGN, "and the answer was loud and clear." On X.com the "clear fan favorite" was DC Extended Universe actor Henry Cavill. (Besides playing Superman, Cavill also appeared in the 2024 film spy action-comedy Argyle, and fought Tom Cruise's character in 2018's Mission Impossible: Fallout — and played Geralt of Rivia in the Netflix series The Witcher.)]]></content:encoded></item><item><title>The HackerNoon Newsletter: The Secret Hidden Power of Questions (2/22/2025)</title><link>https://hackernoon.com/2-22-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sat, 22 Feb 2025 16:05:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 22, 2025?By @scottdclary [ 8 Min read ] Most people are running on default questions they never chose to install. Read More.By @textmodels [ 4 Min read ] A smarter way to allocate computing resources in AI transformers is making them faster and more efficient. Read More.By @luminousmen [ 3 Min read ] Discover different archetypes of data engineers and how their collaboration drives data-driven success. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>O.XYZ Launches OCEAN – Cerebras-Powered AI Engine, 10x Faster Than ChatGPT</title><link>https://hackernoon.com/oxyz-launches-ocean-cerebras-powered-ai-engine-10x-faster-than-chatgpt?source=rss</link><author>o.xyz</author><category>tech</category><pubDate>Sat, 22 Feb 2025 16:00:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
O.XYZ is thrilled to announce the official launch of , a next-generation decentralized AI assistant powered by the industry-leading Cerebras CS-3 wafer-scale chips. On February 22, the company unveiled this new platform, touting speeds ten times faster than ChatGPT and positioning OCEAN as a truly transformative solution in both B2C and B2B markets. From blazing response times to a broad feature set that includes voice interaction and a commitment to decentralization, OCEAN represents a major leap forward in how users worldwide can engage with AI.\
OCEAN’s speed and real-time response capabilities are central to its unique value. Ahmad Shadid, Founder of O and IO, explains that one of the reasons for such swift performance is the adoption of Cerebras’s cutting-edge hardware.\
"The Cerebras CS-3 chip, known as the Wafer Scale Engine (WSE-3), has 900,000 AI-optimized cores and four trillion transistors on a single chip. Traditional GPU-based systems often require complex orchestration and distributed programming to handle large models, but the Cerebras approach offers simplicity and scalability." – claims Shadid.\
Cerebras can scale from one billion to 24 trillion parameters without requiring code changes, freeing users from the delays commonly associated with AI assistants. With 21 PB/s of memory bandwidth, Cerebras-based processing provides low-latency, high-efficiency performance that far surpasses traditional GPU systems.\
While speed and performance are crucial, OCEAN brings more than just high-octane AI. Shadid refers to OCEAN as “the world’s fastest AI search engine,” emphasizing that beyond raw computational power, the platform delivers a sleek, intuitive user experience. This includes a voice interaction system that will allow users to speak their prompts directly to “Miss O,” who can respond in audio format. This conversational style, along with advanced AI agent capabilities slated for future versions, puts OCEAN on a path that goes beyond just answering text-based queries.\
From a product standpoint, OCEAN adopts a dual approach, serving both individual consumers and enterprises. For users who simply want a next-level AI-powered search engine, the app offers quick responses, privacy-focused features, and a decentralized framework to ensure data security. For business clients, OCEAN plans to roll out an API service powered by the very same Cerebras infrastructure that underpins its consumer operations.\
The O community has gained exclusive access to the closed testnet of the OCEAN Assistant, and initial tests  it may run up to 20 times faster than popular AI services like ChatGPT and DeepSeek. X is already flooded with dozens of comparison videos demonstrating the remarkable speed difference, prompting widespread excitement and curiosity about the assistant’s capabilities.\
In the next five years, O.XYZ envisions OCEAN becoming a fully integrated platform with advanced routing intelligence. The proprietary “O Routing Intelligence” (ORI), developed by O.RESEARCH, will dynamically route subtasks to the most suitable model, whether it is an open-source option or a specialized AI for more complex requirements. ORI will help optimize costs without compromising speed or accuracy. This first-of-its-kind technology lays the groundwork for building a massive AI library featuring hundreds of thousands of models. Over time, this expansion is expected to bring OCEAN closer to artificial general intelligence (AGI), while still emphasizing security and user data ownership. With ORI, the team presents a unified intelligence technology similar to the one that OpenAI  this February. ORI will select from over 100,000 open-source models, routing tasks to the best one in real time. ORI will be integrated into OCEAN in spring 2025.\
ORI will be the central hub for AI innovation, seamlessly integrating multiple AI models into a unified intelligence. Through its integration with OCEAN, users will effortlessly harness the power of diverse AI models in one place. aims to reshape artificial intelligence by developing systems independent of corporate control. It focuses on making AI technology accessible, transparent, and community-driven, ensuring superintelligence serves humanity's interests.\
O.XYZ's technical foundation centers on building an AI ecosystem designed to be shutdown-resistant and self-led. Their key initiatives include developing 'Sovereign Super Intelligence,' creating decentralized infrastructure, and researching hyper-fast AI systems.\
The project operates under the O.Systems Foundation, led by Ahmad Shadid. Shadid, who previously founded IO.NET– a $4.5B Solana DePIN – brings his experience to O.XYZ's work on building an autonomous, community-led AI ecosystem.]]></content:encoded></item><item><title>The Secret Hidden Power of Questions</title><link>https://hackernoon.com/the-secret-hidden-power-of-questions?source=rss</link><author>Scott D. Clary</author><category>tech</category><pubDate>Sat, 22 Feb 2025 16:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We all think we're good at asking questions.\
In reality, most of us are terrible at it.\
Not because we lack curiosity or intelligence, but because we've been trained to focus on answers. School rewards students who memorize facts, not those who challenge assumptions. Work promotes people who execute known solutions, not those who explore unknown possibilities.\
Yet, look at anyone who has achieved extraordinary results in any field. The difference between good and great isn't in having more answers—it's in asking better questions.\
When Elon Musk questioned why rockets cost so much, he didn't start by studying aerospace engineering. He started by breaking down the raw material costs of rockets and asking why each component was so expensive. This fundamental questioning led to SpaceX revolutionizing the space industry.\
The same pattern shows up everywhere. Jeff Bezos asked why people couldn't buy any book they wanted instantly. Steve Jobs asked why computers couldn't be beautiful and intuitive. They didn't begin with solutions. They began with questions that challenged basic assumptions.\
But here's what nobody tells you about questions.\
They're not just tools for learning—they're tools for transformation.\
The right question can instantly shift your perspective. It can take a problem that seemed impossible and make the solution obvious. It can turn confusion into clarity. Overwhelm into focus. Stagnation into momentum.\
The quality of your life is directly proportional to the quality of questions you regularly ask yourself.\
Think about that for a second.\
What questions do you ask yourself when you wake up? When you face a challenge? When you're stuck? When you're deciding what to do with your time?\
Most people unconsciously ask questions that keep them trapped. Questions like "Why is this happening to me?" or "What if I fail?" or "What will others think?"\
These questions program your brain to look for evidence of problems, failures, and judgments. They create a self-fulfilling prophecy of mediocrity.\
The path to extraordinary results starts with extraordinary questions.Your mind is running on questions whether you realize it or not.\
Think of questions as lines of code in your mental software. Some questions create bugs in your thinking. Others unlock new capabilities you didn't know you had.\
Most people are running on default questions they never chose to install.\
These are the mental programs you inherited from parents, teachers, and society. They might have served a purpose once, but now, they're outdated. Running in the background. Draining your mental energy and limiting your potential.\
Want to spot the questions running your life? Look at your results.\
If you're constantly stressed about money, you might be running questions like "How can I avoid going broke?" Instead of "How can I create more value?"\
If your relationships are unfulfilling, you might be asking "Why don't people understand me?" Instead of "How can I understand others more deeply?"\
Questions shape reality by directing your focus.\
When you ask better questions, you literally upgrade your mental operating system. Your brain starts noticing opportunities it was blind to before. Solutions appear that were invisible when you were asking surface-level questions.\
This isn't just theory. It's how breakthroughs happen.\
Breaking through plateaus in any area comes down to asking questions nobody else is asking. Questions that challenge core assumptions. Questions that reframe the entire problem.\
The gap between 1x and 1000x performance isn't in having more answers. It's in having better questions.Let's talk about returns.\
Most people play small because they ask small questions. They stay stuck at 1x thinking because their questions never challenge their fundamental assumptions about what's possible.\
The jump from 1x to 10x starts with questions that break limiting patterns.\
Instead of asking "How can I get more clients?" you ask "Why do clients need what I'm selling in the first place?" This simple shift can reveal entire markets you were missing.\
The 10x to 100x leap happens when your questions reframe the entire game.\
This is where you stop asking "How can I be the best player?" and start asking "How can I change the rules?" These questions break you out of competition and into creation.\
But the real magic happens in the jump from 100x to 1000x.\
This is where Lady Luck enters the picture. But luck isn't random - it's attracted to certain types of questions.\
Questions that combine fields nobody has combined before. Questions that challenge assumptions are so basic that nobody sees them as assumptions. Questions that make people uncomfortable because they threaten the status quo.\
Warren Buffett didn't just ask "What stocks should I buy?" He asked, "What businesses are so fundamental to society that they'll be valuable for the next 50 years?" This question led him to invest in Coca-Cola, American Express, and other companies that have generated astronomical returns.\
When everyone else is asking "How can I compete?" you need to be asking "What game should I be playing instead?"\
The right question doesn't just solve problems - it eliminates them entirely.Most people think asking better questions is about being smarter.\
It's about being more precise.\
Vague questions create vague results. When you ask "How can I be more successful?" your brain has nothing concrete to work with. It's like trying to build a house without blueprints.\
But ask "What specific skills would make me irreplaceable in my industry?" and your mind immediately starts generating actionable insights.\
This is the art of question architecture.\
Start with "what" instead of "why." "Why" questions often lead to rationalizations and excuses. "What" questions lead to observations and actions.\
"Why am I stuck?" becomes "What small step would create momentum?" "Why don't I have enough time?" becomes "What am I currently spending time on that doesn't serve my goals?"\
The more specific your question, the more useful the answer.\
Your internal dialogue is just a series of questions and answers. Most people let this run on autopilot. But when you consciously architect your questions, you transform your mental landscape.\
Think of questions as doorways. A poorly designed doorway leads to a closet. A well-designed doorway opens into a universe of possibilities.\
The goal isn't to find the one perfect question. It's to develop a framework for generating better questions.\
This is how you turn confusion into clarity. Overwhelm into action. Stagnation into progress.\
Master learners don't just ask better questions. They have a system for generating them.\
First, they recognize that timing matters. There's a massive difference between asking questions to understand and asking questions to act.\
Understanding questions open up possibilities. Action questions narrow them down.\
When you're exploring a new field, you want broad questions that challenge basic assumptions. "What if everything I know about this is wrong?" This creates space for genuine insight.\
But when it's time to execute, you need focused questions that drive specific outcomes. "What's the smallest step I can take right now that makes all other steps easier?"\
The key is knowing which mode you're in.\
Most people get stuck because they ask action questions during exploration mode, or exploration questions during action mode. They try to optimize before they understand. Or they keep exploring when they should be executing.\
Master learners also build a question database. They collect powerful questions like others collect answers.\
"What would this look like if it were easy?" "What am I not seeing?" "What would I do if I knew I couldn't fail?"\
These aren't just motivational quotes. They're mental tools that break you out of limited thinking patterns.\
But the real power comes from creating your own questions.\
Questions that address your specific blindspots. Questions that challenge your deepest assumptions. Questions that force you to think in new ways.Here's what most people miss about questions.\
They're not just tools for solving problems. They're tools for living.\
Questions determine how you experience reality itself.\
When you ask "What's wrong with my life?" you'll find endless problems. When you ask "What's working in my life?" you'll find endless opportunities. Both questions reveal truth, but they reveal different truths.\
This isn't positive thinking. It's about understanding how your mind constructs reality.\
Every answer closes doors. It settles something. Finalizes it. Puts it in a box. But . They create possibilities that didn't exist before.\
Smart people often fall into the trap of being "answer-oriented." They pride themselves on knowing things. On being right. On having it figured out.\
But wisdom comes from maintaining a state of question.\
Think about it. The most profound experiences in life don't come from finding answers. They come from encountering better questions.\
Questions that make you reevaluate everything. Questions that expand your sense of what's possible. Questions that connect you to something larger than yourself.\
The goal isn't to eliminate uncertainty. It's to get better at dancing with it.\
Life becomes more interesting when you stop demanding answers and start embracing questions. When you stop trying to be certain and start getting curious.You're probably wondering what to do with all this.\
Take the questions you ask yourself every day and upgrade them. Don't try to force massive changes. Just make them slightly better.\
Small shifts in your questions create massive shifts in your life.\
Instead of asking "What do I have to do today?" ask "What's the most important thing I could accomplish today?"\
Pay attention to warning signs of poor questions. When you feel stuck, stressed, or overwhelmed, pause and notice what questions arerunning through your mind.\
Are they empowering or limiting? Are they specific or vague? Are they opening new possibilities or closing them off?\
The beauty of questions is that you can change them instantly.\
Your brain is like a search engine. It will find answers to whatever questions you feed it. Feed it better questions, and you'll get better answers.\
But there's something even more powerful at work here.\
Questions compound. Each better question leads to better insights, which leads to better questions. It's an upward spiral of understanding and capability.\
This is how you future-proof yourself.\
In a world of artificial intelligence and rapid change, the ability to ask better questions becomes increasingly valuable. AI can give you answers, but it can't tell you what questions to ask.\
That's where the real opportunity lies.\
Not in having all the answers, but in knowing how to find the questions that matter.]]></content:encoded></item><item><title>Despite 2024 Layoffs, Tech Jobs Expected to Take Off</title><link>https://spectrum.ieee.org/tech-jobs</link><author>Gwendolyn Rak</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjU2Mjc1OS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0OTA5MTQ5OX0.INl7Nce1q6TXLdMcV5dbkJPmcPgnRjMqUxidu7HD8yw/image.png?width=600" length="" type=""/><pubDate>Sat, 22 Feb 2025 16:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Big data and AI specialist roles are expanding rapidly]]></content:encoded></item><item><title>Torvalds: Rust Kernel Code Isn&apos;t Forced In Over Maintainers&apos; Objections</title><link>https://linux.slashdot.org/story/25/02/22/0524210/torvalds-rust-kernel-code-isnt-forced-in-over-maintainers-objections?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 22 Feb 2025 15:54:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Linus Torvalds responded Thursday to kernel developer Christoph Hellwig, who had claimed Torvalds merged Rust code into the kernel even over his objections as the original C code's maintainer. Highlights from Torvalds' response:

The fact is, the pull request you objected to DID NOT TOUCH THE DMA LAYER AT ALL. It was literally just another user of it, in a completely separate subdirectory, that didn't change the code you maintain in _any_ way, shape, or form... Honestly, what you have been doing is basically saying "as a DMA maintainer I control what the DMA code is used for". 

And that is not how *any* of this works. What's next? Saying that particular drivers can't do DMA, because you don't like that device, and as a DMA maintainer you control who can use the DMA code? That's _literally_ exactly what you are trying to do with the Rust code. You are saying that you disagree with Rust — which is fine, nobody has ever required you to write or read Rust code. But then you take that stance to mean that the Rust code cannot even use or interface to code you maintain... 

You don't have to like Rust. You don't have to care about it. That's been made clear pretty much from the very beginning, that nobody is forced to suddenly have to learn a new language, and that people who want to work purely on the C side can very much continue to do so. So to get back to the very core of your statement: 

 "The document claims no subsystem is forced to take Rust" 

that is very much true. You are not forced to take any Rust code, or care about any Rust code in the DMA code. You can ignore it... 

You can't have it both ways. You can't say "I want to have nothing to do with Rust", and then in the very next sentence say "And that means that the Rust code that I will ignore cannot use the C interfaces I maintain".... So when you change the C interfaces, the Rust people will have to deal with the fallout, and will have to fix the Rust bindings. That's kind of the promise here: there's that "wall of protection" around C developers that don't want to deal with Rust issues in the promise that they don't *have* to deal with Rust. 

But that "wall of protection" basically goes both ways. If you don't want to deal with the Rust code, you get no *say* on the Rust code. Put another way: the "nobody is forced to deal with Rust" does not imply "everybody is allowed to veto any Rust code".
 
Torvalds also made sure to add some kind remarks, including "I respect you technically, and I like working with you."]]></content:encoded></item><item><title>Discover the IndieWeb, one blog post at a time</title><link>https://indieblog.page/</link><author>vinhnx</author><category>hn</category><pubDate>Sat, 22 Feb 2025 15:42:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
        This website lets you randomly explore the IndieWeb. Simply click the button below and you
        will be redirected to a random post from a personal blog.
    
        Disclaimer: the content linked to is aggregated automatically. I neither endorse nor necessarily agree with
        the linked sites. Use at your own risk.
     you can drag the button to your bookmarks and have it always available when you want to be
        inspired.
    ]]></content:encoded></item><item><title>Explore the online world of Apple TV’s ‘Severance’</title><link>https://techcrunch.com/2025/02/22/explore-the-online-world-of-apple-tvs-severance/</link><author>Sarah Perez</author><category>tech</category><pubDate>Sat, 22 Feb 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple has been steadily working to expand the world of the Apple TV+ series “Severance,” through online materials, e-books, podcasts, and other content – and so have its fans. Taking advantage of its platform power, the Cupertino tech giant has been able to easily distribute supplemental material that adds to the show’s storytelling abilities, offering […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>The Challenge with Voice Agents</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/The-Challenge-with-Voice-Agents-e2v7kj7</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/98865191/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-22%2F395336812-44100-2-5e30b9d18237d.mp3" length="" type=""/><pubDate>Sat, 22 Feb 2025 14:50:55 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I Built a Visual Workflow Automation Platform – FlowRipple</title><link>https://flowripple.com/</link><author>shivsarthak34</author><category>dev</category><category>hn</category><pubDate>Sat, 22 Feb 2025 14:20:04 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Watch how easy it is to build powerful automation workflows using our visual builder. Drag, drop, and connect nodes to create your perfect workflow.]]></content:encoded></item><item><title>Asahi Linux&apos;s Honeykrisp Vulkan Driver Gains Sparse Support In Mesa 25.1</title><link>https://www.phoronix.com/news/Honeykrisp-Sparse-Mesa-25.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 22 Feb 2025 14:14:47 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Alyssa Rosenzweig has carried out a fresh sync of the Asahi Linux AGX Gallium3D and Honeykrisp Vulkan driver changes of the work that was being carried by the Asahi Linux development tree and now upstreamed to Mesa proper...]]></content:encoded></item><item><title>Mozambique Retools Weather Tech for Impactful Forecasting</title><link>https://spectrum.ieee.org/disaster-response-tech-mozambique</link><author>Maurizio Arseni</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjU1ODQ1Mi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2NzYxNjQ4M30.BkrN-0JVFqGthtNaOHUl1-7PJN95BugC26I-YqQOVcY/image.jpg?width=600" length="" type=""/><pubDate>Sat, 22 Feb 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Drones and situation rooms inform responses to floods and cyclones]]></content:encoded></item><item><title>Scientists Discover Ancient Farms in the Deep Sea</title><link>https://www.404media.co/scientists-discover-ancient-farms-in-the-deep-sea/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/image4-1.jpg" length="" type=""/><pubDate>Sat, 22 Feb 2025 14:00:01 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to the Abstract! It’s hard to keep up with all the news about all the giant gassy orbiters out there. I’m speaking, of course, about hot Jupiters, a class of planets that takes the concept of “inhospitable” to dazzling and creative new levels, and which had an epic news week.Then, what did scientists find in cores taken from deep-sea trenches? The answer might surprise you. Next, mice administer “first aid.” Last, fish can see you for who you really are (though yummy treats will certainly not be refused). Hot Jupiters Are So Hot Right Now (and at All Other Times)Hot Jupiters are the low-hanging fruit of exoplanet discoveries. As the name implies, they are Jupiter-sized worlds that orbit extremely close to their stars, a proximity that makes them—you guessed it—hot. Given that they are both giant in scale and have short years lasting only hours or days, hot Jupiters are the easiest exoplanets to spot, which is why our catalog of distant worlds is packed with them. In fact, a study came out  that identified seven new ones.But while it’s not all that novel to discover these worlds (which is kind of amazing in itself), scientists have now peered deep into the atmosphere of the hot Jupiter WASP-121, nicknamed Tylos, which is about 850 light years from Earth. It’s the first time several distinct atmospheric layers and processes have been observed on an exoplanet.   “Ultra-hot Jupiters, an extreme class of planets not found in our solar system, provide a unique window into atmospheric processes,” said researchers led by Julia Seidel of the European Southern Observatory (ESO). “Here we show a dramatic shift in atmospheric circulation in an ultra-hot Jupiter” including “the first vertical characterization of a high-altitude, super-rotational atmospheric jet stream.”  Tylos is slightly bigger than Jupiter, but it is so close to its star that its year lasts only 30 hours. As a consequence, it is tidally locked, meaning that one side is always facing the star, and the other always faces away. The star-lit side is about 2,300°C (4,200°F) which is, as advertised, quite hot. Using the ESO’s Very Large Telescope, the researchers spotted the aforementioned equatorial jet stream and saw flows of hot gas moving from the hot day side to the cooler night side—which is still pretty hot at around 700°C (1,340°F). The weather report on Tylos is permanently fatal with a chance of titanium rain, according to a third study that came out this week (that’s a hot Jupiter hat-trick). Taken together, the research represents a new emerging era of exoplanet observations in which astronomers can peek under the hood of these distant atmospheres and start to get a real vertical cross-section of otherworldly skies. Down the line, this will lead to better characterizations of the atmospheres of potentially habitable exoplanets, which could contain detectable signs of alien life. But for now, on this late winter weekend, let's be satisfied with warming ourselves  into certain oblivion in the bellies of hot Jupiters. From the Hadal to the Grave To cool off, we shall now dive straight into the deepest parts of the ocean, the hadal zone, where strange things are inherently afoot. Scientists took sediment cores from seafloors at depths of over 4.6 miles in the Japan Trench which is, in my opinion, asking for trouble. But in this case, the results revealed an activity that you might not expect to find in one of the most inhospitable places on Earth—farming.   I should just say, the “farmers” are probably invertebrates, like sea cucumbers or bivalves, that cultivate microbes that help break down organic matter for them. Still, a basic form of “agrichnial” farming is preserved in trace fossils, like burrows, the team found in the cores. “The hadal zone, >6 km deep, remains one of the least understood ecosystems on Earth,” said researchers led by Jussi Hovikoski of the Geological Survey of Finland. The cores open a rare window into this otherworldly region and reveal “slender spiral, lobate and deeply penetrating straight and ramifying burrow systems…interpreted to include burrows of microbe farming and chemosymbiotic invertebrates.” The study also gets points for its title, “Bioturbation in the hadal zone,” which sounds like an early aughts prog rock album. \m/ Somebody Call an EMT! (Emergency Mouse Technician)Humans produce a lot of selfish psychos, if you hadn’t noticed, but one nice thing about our species is we generally share a prosocial instinct to help people during a medical crisis. As it turns out, we’re not alone in this behavior, according to a new study that monitored the reactions of mice to ailing, unconscious, or dead conspecifics. “Anecdotal observations across several species in the wild, including nonhuman primates, dolphins, and elephants have reported intriguing behaviors of animals toward unresponsive conspecifics that have collapsed because of sickness, injury, or death,” said researchers led by  Wenjian Sun of the University of Southern California. “These animals…display various behavioral responses, including touching, grooming, nudging, and sometimes even more intense physical actions, such as striking, toward the collapsed peers. Some of these actions toward incapacitated conspecifics are reminiscent of human emergency responses, especially those involving sensory stimulation.”To bring these anecdotal reports in an experimental setting, the team videotaped mice responding to cagemates that had been anesthetized into unconsciousness, as well as their reactions  to dead mice. The r mice interacted with unconscious cage-mates  about ten times as much as with an active partner, and may have even performed basic versions of first aid.“Our results suggest that the actions of mouth/ tongue biting and tongue pulling may have rescue-like effects, reminiscent of human first aid efforts in reviving unconscious individuals with physical stimulation and airway maintenance,” the researchers said.  “The consequences of the behaviors, such as improved airway opening or clearance and expedited recovery, are clearly beneficial to the recipient,” they added, though they also cautioned that “it is challenging to determine the motivational needs behind these distinctive ‘reviving-like’ behaviors.”  Familiarity played a strong role in the experiment's outcome; mice heaped much more attention on dead or unconscious cage-mates that they knew well compared to strangers. At the risk of anthropomorphizing, it’s kind of sad to think about these mice being confronted with their passed-out or dead friends, but the silver lining is an empirical validation of widespread prosocial behaviors. I’m also going to assume it means that the Disney franchise , starring mice humanitarians, is a documentary.The Adventures of Left Hump and FriendsThe next time you go for an ocean swim, why not introduce yourself to some neighboring fish? They might learn to recognize you as an individual and start following you around, especially if you give them something nice to eat. That’s the conclusion of a new study that found fish can tell individual divers apart based on visual cues—and that they rapidly learn which divers are generous with treats (in this case: shrimp).Researchers Maëlan Tomasek and Katinka Soller conducted several dives at the STARESO research station in Corsica, France. Soller was the designated shrimp dispenser, and the wild fish “volunteers” rapidly learned to distinguish her visually from Tomasek, the shrimp miser.“Two species voluntarily took part in our experiments: saddled sea bream and black sea bream ,” said the researchers. “Of specific individuals, the saddled bream (Bernie) was first identified at dive 5 of the training, four black bream at dives 12 (Left Hump), 15 (Kasi), 19 (Alfi), 21 (Julius) and the last black bream (Geraldine) on the first session of experiment 1. Note that this marks the moment from which we were able to reliably identify them (i.e. identify with absolute certainty at each apparition from one dive to the next) but that they most likely appeared several days prior to this.”First of all, fantastic names. I’m already shipping Julius and Geraldine as a celebrity fish couple called Juladine. Left Hump will officiate the wedding. But setting aside the fish fanfic, the team demonstrated that the fish learned to visually tell the researchers apart, leading to a clear preference for following Soller. “The fact that wild bream can discriminate between divers adds scientific evidence to the numerous accounts suggesting differentiated relationships between fish and specific humans,” the team said. “Our study thus encourages a reappraisal of the methodological avenues to study cognitive abilities of wild fish under natural conditions.” “It also demonstrates a potential difficulty when conducting such experiments that could be disturbed by fish following specific experimenters,” the researchers said, concluding with an implied wink: “Researchers might not always want to be followed all around by fish, but if they do, they will not be disappointed.”Thanks for reading! See you next week.]]></content:encoded></item><item><title>Florida insurers steered money to investors while claiming losses, study says</title><link>https://www.tampabay.com/news/florida-politics/2025/02/22/florida-insurance-profits-desantis-regulation-investors-crisis/</link><author>howard941</author><category>hn</category><pubDate>Sat, 22 Feb 2025 13:25:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[TALLAHASSEE — While Florida insurers claimed to be losing money in the wake of hurricanes Irma and Michael, their parent companies and affiliates were making billions of dollars, according to a study obtained by the Times/Herald.The start of the state’s insurance market meltdown came on the heels of those two storms between 2017 and 2019, as companies justified big rate increases to cover their losses.But those financial hardships don’t tell the full story, according to the 2022 study that has never been made public and was released to the Times/Herald after a two-year wait for public records.The report, the most in-depth dive into the byzantine finances of Florida’s homeowners insurance market, reveals that as the industry was ailing and companies were losing money, executives distributed $680 million in dividends to shareholders while diverting billions more to affiliate companies.Executives with most Florida-based insurers were removing so much money fromtheir companies that they violated state regulations, the study’s author concluded.The result left some insurers financially weaker — and potentially unable to pay claims — heading into the depths of the state’s insurance crisis.State lawmakers never saw the report.The state’s then-insurance commissioner and Gov. Ron DeSantis focused on legal reforms making it harder to sue insurers.“These companies are crying poverty in order to raise premiums or justify insolvency: ‘It’s litigation, it’s fraud,‘” Quinn said. “This is money shifting from their left pocket to the right, and crying poverty while their right pocket bulges.”While the report is an incomplete picture of insurers' money,Florida’s Office of Insurance Regulation said in a statement, the study affirms that the reforms the office wants are warranted.ButPaul Handerhan, founder of the trade group Federal Association for Insurance Reform, whose members include insurance companies,disputed the idea that executives were deliberately moving money around.“This notion that they’re fleecing their policyholders and offshoring the money to their affiliates is just not happening,” Handerhan said. “None of these guys did this as a strategy.”During debates in the Legislature over how best to respond to the insurance crisis between 2018 and 2023, some lawmakers asked what role affiliate companies played.Rep. Hillary Cassel, R-Dania Beach, said lawmakers and observers had a lack of data about affiliates, known in the industry as “managing general agents,” when they were voting on legislation.“All of us informed on the issues knew (managing general agents) were a problem,” said Cassel, a former lawyer for insurance companies who now sues them.The Office of Insurance Regulation said in a statement that the study was not given to lawmakers because it was “not a formal examination report.”It was produced months before lawmakers met in emergency legislative sessions in 2022 and left in a “draft” status.“Our office does not release every internal analysis of companies to the Legislature,” the office said.The Times/Herald requested the report in November 2022, but the office did not turn over the executive summary until December 2024.The affiliate structure is nothing new in Florida.Profits of insurance companies are limited by regulators to about 4.5% — hardly enticing to investors, considering the risk of hurricanes.However, insurance executives in Florida have used financial workarounds to reward investors and themselves.While the profits and executive compensation of the insurance company are capped, the profits of affiliate and parent companies are not.So executives create sister companies that charge the insurance company for basic services, such as claims handling, underwriting, accounting and issuing policies. (Large national insurers typically handle all of those services internally.)Arrangements between insurance companies and affiliates must be approved by the state, and regulations say they must be “fair and reasonable,” which isn’t defined in state law.When Southern Fidelity Insurance dissolved that year, it was one of six companies under the same umbrella, and its holdings included a hunting lodge maintained at a cost of $485,000 per year. State officials were investigating whether the company “took active measures to conceal these costs” from regulators, according to an October insolvency report.That’s because the owner of the insurance company also owns the affiliates, creating an incentive for executives to overcharge the insurance company for services.Still,it’s unlikely Floridaregulators knew the full scope of insurance money-shifting arrangements until 2021, when lawmakers gave them the ability to demand more information from insurers and affiliates.Using that power,the state’s then-commissioner, David Altmaier, paid a Connecticut-based consultantnearly $150,000to parse through the information the companies provided. Several companies turned over incomplete data, according to the study.Between 2017 and 2019, the insurers in the study (minus a couple of outliers) showed a net loss of $432 million.Their affiliate companies showed a net income of $1.8 billion.With all 53 companies included, the industry recorded $61 million in net income, and affiliates made about $14 billion in net income, according to the study. Those figures likely include national carriers that also provide auto insurance.The author noted that the affiliates of Florida-based companies were profitable even after they injected$485 million back into the insurers andwaived $208 million in fees during the three years, steps made to help keep the insurers afloat.In the author’s opinion, 19 of the 30 Florida-based companies that provided data were paying feesto their affiliate companies that were “not fair and reasonable.”The numbers in the study are “eye-popping” and raise questions about why regulators would allow such financial arrangements, said Birny Birnbaum, executive director of the Center for Economic Justice and a former chief economist at the Texas Department of Insurance.“It’s unclear why (the Office of Insurance Regulation) isn’t doing anything about it,” Birnbaum said.Regulators this year are asking lawmakers to define “fair and reasonable” to include the actual cost of the service provided, the overall health of the insurer and how much in dividends were paid out. Regulators asked for that in 2023 but lawmakers rejected it, claiming it would “upset the apple cart” of Florida’s insurance industry.The office’s proposed legislation would also require fees to affiliates be paid in dollar amounts, instead of percentages. Affiliates will typically charge the insurance company fees of between 20% and 34% of premiums, which results in more money for the affiliates when premiums go up.The office has canceled or modified some companies’ agreements. In 2023, for example, one company’s contract with an affiliate was canceled after regulators discovered that the affiliate was charging the insurer additional fees on top of the cost of the services being provided, according to the office.]]></content:encoded></item></channel></rss>