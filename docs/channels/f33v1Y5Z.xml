<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Latest</title><link>https://www.awesome-dev.news</link><description></description><item><title>Could a Faint Glow in the Milky Way Be Dark Matter?</title><link>https://science.slashdot.org/story/25/11/01/2118210/could-a-faint-glow-in-the-milky-way-be-dark-matter?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 21:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["A nearby galaxy once thought to be dominated by dark matter seems to have a surprise supermassive black hole at its centre," reports New Scientist. 

Yet scientists "are convinced dark matter is out there," writes Space.com. "The quest to detect it arguably remains both one of the most frustrating and most exhilarating challenges in modern physics." 

And now they report that the century-old mystery of dark matter — the invisible glue thought to hold galaxies together — "just got a modern clue."

Scientists say they may be one step closer to confirming the existence of this elusive material, thanks to new simulations suggesting that a faint glow at the center of the Milky Way could be dark matter's long-sought signature. "It's very hard to actually prove, but it does seem likely," Moorits Muru of the Leibniz Institute for Astrophysics Potsdam in Germany, who led the new study, told Space.com... 

The findings, show that dark matter near the Milky Way's center might not form a perfect sphere as scientists long thought. Instead, it appears flattened, almost egg-shaped, and that shape closely mirrors the pattern of mysterious gamma rays observed by NASA's Fermi Gamma-ray Space Telescope... Using powerful supercomputers, [the researchers] recreated how the Milky Way formed, including billions of years of violent collisions and mergers with smaller galaxies. Those violent events, the researchers found, left deep "fingerprints" on the way dark matter is distributed in the galactic core.... matching the pattern of gamma-ray emission Fermi has observed, the new study reports... 

If the excess truly arises from dark matter collisions, it would mark the first indirect evidence that weakly interacting massive particles [WIMPs], a leading dark matter candidate, really exist...
 

"We have run dozens of direct detection experiments around the globe hunting for WIMPS," notes
Phys.org, in an article titled "The Empty Search for Dark Matter."

We have run dozens of direct detection experiments around the globe hunting for WIMPS — dark matter particles in this particular mass range. And they're not all the same kind of experiments. There are also the scintillators, which use a giant vat of liquefied noble gas, like several tons of xenon. They wait for a dark matter particle to strike the xenon and cause it to scintillate, which is a fancy science word for "sparkle." We see the sparkle; we detect dark matter... 

They're just one example of a broader class of dark matter candidates, with delightful names like Q-balls, WIMPzillas, and sterile neutrinos. We've tuned our different experiments to capture different mass ranges or interaction strengths to cover as much of that wide dark matter spectrum as possible. We've even tried to manufacture various kinds of dark matter in our particle collider experiments.
 
And we've found nothing.

]]></content:encoded></item><item><title>Debian&apos;s APT Will Soon Begin Requiring Rust: Debian Ports Need To Adapt Or Be Sunset</title><link>https://www.phoronix.com/news/Debian-APT-Will-Require-Rust</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 21:23:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Debian developer Julian Andres Klode sent out a message on Halloween that may give some Debian Linux users and developers a spook: the APT packaging tool next year will begin requiring a Rust compiler. This will place a hard requirement by Debian Linux on Rust support for all architectures. Debian CPU architectures with ports currently but lacking Rust support will either need to see support worked on or be sunset...]]></content:encoded></item><item><title>Employees Are the New Hackers: 1Password Warns AI Use Is Breaking Corporate Security</title><link>https://it.slashdot.org/story/25/11/01/2047217/employees-are-the-new-hackers-1password-warns-ai-use-is-breaking-corporate-security?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 20:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader BrianFagioli writes: Password manager 1Password's 2025 Annual Report: The Access-Trust Gap exposes how everyday employees are becoming accidental hackers in the AI era. The company's data shows that 73% of workers are encouraged to use AI tools, yet more than a third admit they do not always follow corporate policies. Many employees are feeding sensitive information into large language models or using unapproved AI apps to get work done, creating what 1Password calls "Shadow AI." At the same time, traditional defenses like single sign-on (SSO) and mobile device management (MDM) are failing to keep pace, leaving gaps in visibility and control. The report warns that corporate security is being undermined from within. More than half of employees have installed software without IT approval, two-thirds still use weak passwords, and 38% have accessed accounts at previous employers. Despite rising enthusiasm for passkeys and passwordless authentication, 1Password says most organizations still depend on outdated systems that were never built for cloud-native, AI-driven work. The result is a growing "Access-Trust Gap" that could allow AI chaos and employee shortcuts to dismantle enterprise security from the inside.]]></content:encoded></item><item><title>GitHub Game Off 2025 theme announcement</title><link>https://github.blog/company/github-game-off-2025-theme-announcement/</link><author>Lee Reilly</author><category>official</category><pubDate>Sat, 1 Nov 2025 20:37:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[Get ready for the annual Game Off, our month-long game jam that has inspired thousands of developers to make, share, and play games since 2012. Whether you’re a first-time jammer or a returning champion, this November is your chance to make something unforgettable.The theme for this year? !You have until December 1, 2025, at 13:37 PST to build a game  based on the theme. How you interpret it is entirely up to you. Don’t overthink it. Just ride the creative wave and see where it takes you. 🏄🏻Need inspiration? Here are a few concept ideasA space shooter where you fly through gravitational waves and wormholes.A survival game where you build a coastal base and A tower defense game where you battle waves of increasingly powerful baddies.A skateboard game where you ride a sine wave, shredding through peaks and troughs. A rhythm game where you catch the beat and ride the wave.A racing game where you drift through vaporwave skylines and a totally tubular synthwave soundtrack.A physics puzzler where you bounce, reflect, and refract energy waves.A remake of a class you enjoyed when you were younger resulting in endless waves of nostalgia.Whatever form your game takes, whether it crashes, ripples, or totally wipes out… we can’t wait to see it.Stuck for ideas? GitHub Copilot might be able to help. Try asking, “What are some fun games I could create with the game jam theme, WAVES?”Work alone or on a team. Use whatever programming languages, game engines, or libraries you like. Create a free GitHub account if you don’t have one. Hop onto the itch.io Game Off 2025 page. If you don’t already have an itch.io account, you can sign in with your GitHub account.Create a public repository. Store your source code on GitHub. Push your game before Submit your game on itch.io. Once submitted, you’ll be able to play other entries and cast your votes.After the submission period ends, participants will vote on each other’s games. Entries will be evaluated in the following categories:Voting will end on January 8, 2026, at 13:37 PST. Winners will be announced on the GitHub Blog and social channels on January 10, 2026, at 13:37 PST.Game Off is intentionally relaxed, but here are a few simple guidelines to keep things fair and fun:Your game must live in a GitHub repository. You should start from scratch, but you can use templates. The vast majority of the work should be done in the game jam period.License it however you like. Open source is encouraged, but not required. Work however you’re most comfortable.Use any tools or assets you prefer. Open source, commercial, or your own creations are all welcome.AI-assisted development is allowed.That’s it. Keep it creative, respectful, and fun, and remember to push your code before the deadline.You don’t need to be an expert. Many participants build their first game during Game Off. Some use popular engines, others build their own, and a few even create games for classic hardware like the NES, Game Boy, or ZX Spectrum. However you make it, there’s no wrong way to play.Here are a few engines you might want to explore:: Great for 2D and 3D games. Open source, lightweight, and beginner-friendly.: Ideal for 3D or mobile games with plenty of tutorials and asset packs available.: Best for cinematic visuals, complex 3D games, and high-end experiences.: Good choice for browser-based 2D arcade or platformer games.: A solid option for learning game development basics or prototyping ideas quickly.: Modern, data-driven engine for developers who like performance and clean ECS design.: Lightweight and fast, good for 2D games and creative coding projects.: Works well for mobile-first 2D games if you already use Flutter.: Simple and powerful engine for 2D games written in Go.: Cross-platform 2D engine with built-in tools and an active indie community.: A familiar choice for developers coming from Java or Android backgrounds.: Great for retro-style 2D games, platformers, and jam projects.The Game Off 2025 Community is a great place to ask questions or look for teammates. There’s also a friendly community-run Discord server.Game Off is the perfect opportunity to check it out (version control pun intended).Whether your build floats or sinks, you’re part of something swell. Join thousands of developers around the world for a month of creativity, learning, and code-powered fun. Let’s hang ten on your keyboard  🌊 🤙 and make some WAVES together.]]></content:encoded></item><item><title>Elon Musk wants you to know that Sam Altman got a refund for his Tesla Roadster</title><link>https://techcrunch.com/2025/11/01/elon-musk-wants-you-to-know-that-sam-altman-got-a-refund-for-his-tesla-roadster/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 1 Nov 2025 20:01:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk and Sam Altman are still taking swipes at each other on Musk’s social media platform X.]]></content:encoded></item><item><title>NASA Seeks Backup Plan for Carrying Astronauts to the Moon</title><link>https://science.slashdot.org/story/25/11/01/1737240/nasa-seeks-backup-plan-for-carrying-astronauts-to-the-moon?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 19:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from CNN:
[C]iting delays in Starship's development and competitive pressure from China, NASA asked SpaceX and Blue Origin — which holds a separate lunar lander contract with the space agency — to submit plans to expedite development of their respective spacecraft by October 29. Both companies have responded. But the space agency is also asking the broader commercial space industry to detail how they might get the job done more quickly, hinting that NASA leadership is prepared to sideline its current partners. CNN spoke with half a dozen companies about how they plan to respond to NASA's call to action, which the agency will formally issue once the government shutdown ends, according to a source familiar with the matter. 

One possibility is Lockheed Martin...


Notably, as a legacy NASA contractor, the company built the $20.4 billion Orion spacecraft that astronauts will ride when they take off from Earth... Now, Lockheed says it can piece together a two-stage lunar lander that uses spare parts harvested from Orion. The company would make use of Space Shuttle-era OMS-E engines — which are also used on Orion — to serve as the propulsion for an "ascent stage" of the lunar lander, providing the thrust for the vehicle to lift off the moon after a mission is completed. But the vehicle also needs a descent stage to get down to the lunar surface in the first place... 

Other commercial space companies contacted by CNN — including Firefly Aerospace and Northrop Grumman — said simply that they were "ready to support" NASA in its endeavor to find a faster way to complete the Artemis III mission. They did not confirm whether they would formally respond to the space agency's anticipated request for companies to submit proposals. 

The more important goal, argue some experts, is to pave the way for a permanent lunar base where astronauts can live and work...

[P]erhaps the true winner will be the country that is able to build lasting infrastructure, experts say.
"It makes great press fodder to frame this as competition," said one space policy source, who was among several that spoke to CNN on the condition of anonymity to discuss controversial issues. "But this is about the long game and the sustainability."
]]></content:encoded></item><item><title>This Week In Techdirt History: October 26th – November 1st</title><link>https://www.techdirt.com/2025/11/01/this-week-in-techdirt-history-october-26th-november-1st/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sat, 1 Nov 2025 19:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude Code Can Debug Low-Level Cryptography</title><link>https://words.filippo.io/claude-debugging/</link><author>Bogdanp</author><category>hn</category><pubDate>Sat, 1 Nov 2025 18:41:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Over the past few days I wrote a new Go implementation of ML-DSA, a post-quantum signature algorithm specified by NIST last summer. I livecoded it all over four days, finishing it on Thursday evening. Except… Verify was always rejecting valid signatures.$ bin/go test crypto/internal/fips140/mldsa
--- FAIL: TestVector (0.00s)
    mldsa_test.go:47: Verify: mldsa: invalid signature
    mldsa_test.go:84: Verify: mldsa: invalid signature
    mldsa_test.go:121: Verify: mldsa: invalid signature
FAIL
FAIL     crypto/internal/fips140/mldsa   2.142s
FAIL
I was exhausted, so I tried debugging for half an hour and then gave up, with the intention of coming back to it the next day with a fresh mind.On a whim, I figured I would let Claude Code take a shot while I read emails and resurfaced from hyperfocus. I mostly expected it to flail in some maybe-interesting way, or rule out some issues.Instead, it rapidly figured out a fairly complex low-level bug in my implementation of a relatively novel cryptography algorithm. I am sharing this because it made me realize I still don’t have a good intuition for when to invoke AI tools, and because I think it’s a fantastic case study for anyone who’s still skeptical about their usefulness.Full disclosure: Anthropic gave me a few months of Claude Max for free. They reached out one day and told me they were giving it away to some open source maintainers. Maybe it’s a ploy to get me hooked so I’ll pay for it when the free coupon expires. Maybe they hoped I’d write something like this. Maybe they are just nice. Anyway, they made no request or suggestion to write anything public about Claude Code. Now you know.I started Claude Code v2.0.28 with Opus 4.1 and no system prompts, and gave it the following prompt (typos included):I implemented ML-DSA in the Go standard library, and it all works except that verification always rejects the signatures. I know the signatures are right because they match the test vector.YOu can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaLook for potential reasons the signatures don’t verify. ultrathinkI spot-checked and w1 is different from the signing one.Maybe I shouldn’t be surprised! Maybe it would have been clear to anyone more familiar with AI tools that this was a good AI task: a well-scoped issue with failing tests. On the other hand, this is a low-level issue in a fresh implementation of a complex,  algorithm.It figured out that I had merged  and  into a single function for using it from Sign, and then reused it from Verify where  already produces the high bits, effectively taking the high bits of w1 twice in Verify.Looking at the log, it loaded the implementation into the context and then  figured it out, without any exploratory tool use! After that it wrote itself a cute little test that reimplemented half of verification to confirm the hypothesis, wrote a mediocre fix, and checked the tests pass.I threw the fix away and refactored  to take high bits as input, and changed the type of the high bits, which is both clearer and saves a round-trip through Montgomery representation. Still, this 100% saved me a bunch of debugging time.A second synthetic experimentOn Monday, I had also finished implementing signing with failing tests. There were two bugs, which I fixed in the following couple evenings.I figured these would be an interesting way to validate Claude’s ability to help find bugs in low-level cryptography code, so I checked out the old version of the change with the bugs (yay Jujutsu!) and kicked off a fresh Claude Code session with this prompt:I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector it looks like it goes into an infinite loop, probably because it always rejects in the Fiat-Shamir with Aborts loop.You can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaFigure out why it loops forever, and get the tests to pass. ultrathinkIt gave up after fixing that bug even if the tests still failed, so I started a fresh session (on the assumption that the context on the wrong constants would do more harm than good investigating an independent bug), and gave it this prompt:I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector they don’t match.You can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaFigure out what is going on. ultrathinkIt’s interesting how Claude found the “easier” bug more difficult. My guess is that maybe the large random-looking outputs of the failing tests did not play well with its attention.The fix it proposed was updating only the allocation’s length and not its capacity, but whatever, the point is finding the bug, and I’ll usually want to throw away the fix and rewrite it myself anyway.Three out of three one-shot debugging hits with no help is . Importantly, there is no need to trust the LLM or review its output when its job is just saving me an hour or two by telling me where the bug is, for me to reason about it and fix it.As ever, I wish we had better tooling for using LLMs which didn’t look like chat or autocomplete or “make me a PR.” For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it?Enjoy the silliest floof. Surely this will help redeem me in the eyes of folks who consider AI less of a tool and more of something to be hated or loved.My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)
Here are a few words from some of them!Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.]]></content:encoded></item><item><title>Scientists Say &apos;Dueling Dinosaurs&apos; Fossil Confirms a Smaller Tyrannosaur Species, Not a Teenaged T. Rex</title><link>https://science.slashdot.org/story/25/11/01/0740245/scientists-say-dueling-dinosaurs-fossil-confirms-a-smaller-tyrannosaur-species-not-a-teenaged-t-rex?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from NPR:

It's known as the "Dueling Dinosaurs" fossil: A triceratops and a tyrannosaur, skeletons entangled, locked in apparent combat right up until the moment of their mutual demise... That discovery in 2006 now appears to have overturned decades of dinosaur dogma about Tyrannosaurus rex, the fearsome giant long thought to be the sole top predator stalking the late Cretaceous. In a paper in the journal Nature, paleontologists Lindsay Zanno and James Napoli conclude that some of the bones from that specimen belong not to a teenage T. rex, but to a fully grown individual of a different tyrannosaur species — Nanotyrannus lancensis.... 


One of the first of those red flags in the new specimen was the arm bones. They looked completely different than T. rex's puny appendages... "These are powerful arms with large claws, large hands. They were using them for prey capture." Contrast that with T. rex, "an animal that's a mouth on legs." There were additional clues. The animal had fewer tail vertebrae and more teeth than T. rex. Zanno and Napoli considered other lines of evidence. They created 3D models of numerous purported T. rexes against which they compared their specimen. They looked at the growth stages of the cranial nerves and sinuses of close living relatives of dinosaurs, features that were visible in the fossilized skeleton. 

"But maybe the most important and damning thing that we did was we were able to figure out that our animal is not a juvenile at all," she says. This conclusion was based on slicing through the fossil's limb bones to examine the growth rings. That work demonstrated that this animal was mature and done growing when it died around the age of 20. "That means it's half the size and a tenth of the mass of a full grown Tyrannosaurus rex," says Zanno... In addition, while making models of all those other alleged T. rex skeletons, Zanno says they identified another new species of tyrannosaur, one they're calling Nanotyrannus lethaeus... 

"It tells us that these end-Cretaceous ecosystems right before the asteroid hit were flourishing," says Zanno. "They had an abundance of different predators. And refutes this idea that dinosaurs were in decline before the asteroid struck."

]]></content:encoded></item><item><title>Visible from space, Sudan&apos;s bloodied sands expose a massacre of thousands</title><link>https://www.telegraph.co.uk/world-news/2025/10/28/sudan-bloodied-sands-massacre-thousands/</link><author>wslh</author><category>hn</category><pubDate>Sat, 1 Nov 2025 17:50:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Reports indicated that the RSF was deliberately forcing displaced civilians eastward into areas under its control, away from humanitarian hubs such as Tawila, where some international agencies were operating.According to Jeremy Konyndyk, president of Refugees International, the RSF was preventing people from fleeing the town in other directions, specifically blocking movement south and west, and compelling them to move east, where there was no safety or access to aid.Yvette Cooper, the Foreign Secretary, said: “We are witnessing a deeply disturbing pattern of abuses in El Fasher, including systematic killings, torture, and sexual violence.”The UN Human Rights Office said it had received “multiple, alarming reports that the RSF are carrying out atrocities, including summary executions”.Volker Türk, the UN human rights chief, said the risk of further large-scale, ethnically motivated violations and atrocities in El Fasher was “mounting by the day”.]]></content:encoded></item><item><title>Show HN: Why write code if the LLM can just do the thing? (web app experiment)</title><link>https://github.com/samrolken/nokode</link><author>samrolken</author><category>hn</category><pubDate>Sat, 1 Nov 2025 17:45:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I spent a few hours last weekend testing whether AI can replace code by executing directly. Built a contact manager where every HTTP request goes to an LLM with three tools: database (SQLite), webResponse (HTML/JSON/JS), and updateMemory (feedback). No routes, no controllers, no business logic. The AI designs schemas on first request, generates UIs from paths alone, and evolves based on natural language feedback. It works—forms submit, data persists, APIs return JSON—but it's catastrophically slow (30-60s per request), absurdly expensive ($0.05/request), and has zero UI consistency between requests. The capability exists; performance is the problem. When inference gets 10x faster, maybe the question shifts from "how do we generate better code?" to "why generate code at all?"]]></content:encoded></item><item><title>Ubuntu Will Use Rust For Dozens of Core Linux Utilities</title><link>https://news.slashdot.org/story/25/11/01/079206/ubuntu-will-use-rust-for-dozens-of-core-linux-utilities?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 1 Nov 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[ Ubuntu "is adopting the memory-safe Rust language," reports ZDNet, citing remarks at this year's Ubuntu Summit from Jon Seager, Canonical's VP of engineering for Ubuntu:

. Seager said the engineering team is focused on replacing key system components with Rust-based alternatives to enhance safety and resilience, starting with Ubuntu 25.10. He stressed that resilience and memory safety, not just performance, are the principal drivers: "It's the enhanced resilience and safety that is more easily achieved with Rust ports that are most attractive to me". This move is echoed in Ubuntu's adoption of sudo-rs, the Rust implementation of sudo, with fallback and opt-out mechanisms for users who want to use the old-school sudo command. 


In addition to sudo-rs, Ubuntu 26.04 will use the Rust-based uutils/coreutils for Linux's default core utilities. This setup includes ls, cp, mv, and dozens of other basic Unix command-line tools. This Rust reimplementation aims for functional parity with GNU coreutils, with improved safety and maintainability. 

On the desktop front, Ubuntu 26.04 will also bring seamless TPM-backed full disk encryption. If this approach reminds you of Windows BitLocker or MacOS FileVault, it should. That's the idea. 

In other news, Canonical CEO Mark Shuttleworth said "I'm a believer in the potential of Linux to deliver a desktop that could have wider and universal appeal." (Although he also thinks "the open-source community needs to understand that building desktops for people who aren't engineers is different. We need to understand that the 'simple and just works' is also really important.") 


Shuttleworth answered questions from Slashdot's readers in 2005 and 2012.]]></content:encoded></item><item><title>OpenAI Moves to Complete Potentially the Largest Theft in Human History</title><link>https://thezvi.substack.com/p/openai-moves-to-complete-potentially</link><author>paulpauper</author><category>hn</category><pubDate>Sat, 1 Nov 2025 17:25:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Coinbase CEO Brian Armstrong trolls the prediction markets</title><link>https://techcrunch.com/2025/11/01/coinbase-ceo-brian-armstrong-trolls-the-prediction-markets/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:59:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While Armstrong may have helped some Kalshi and Polymarket users make a little money, he was also illustrating how easily these markets can be manipulated.]]></content:encoded></item><item><title>Studies increasingly find links between air pollutants and dementia</title><link>https://www.nytimes.com/2025/11/01/health/alzheimers-dementia-air-pollution.html</link><author>quapster</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:54:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chat Control proposal fails again after public opposition</title><link>https://andreafortuna.org/2025/11/01/chat-control-proposal-fails-again-after-massive-public-opposition/</link><author>speckx</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:42:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The  has once again retreated from its controversial Chat Control proposal, a plan that would have required widespread scanning of encrypted messages. The withdrawal by the current  represents yet another chapter in a long-running battle between privacy advocates and lawmakers who believe they can compromise encryption in the name of public safety. While this latest defeat is a victory for digital rights, the fight is far from over, and the fundamental misunderstanding of encryption technology continues to plague policy discussions across Europe.A zombie proposal that refuses to dieSince its introduction in 2022, Chat Control has become what privacy advocates call a , repeatedly resurrected despite consistent opposition from civil society, technical experts, and the public. The Electronic Frontier Foundation and more than 80 civil society organizations have strongly opposed the legislation, which would mandate client-side scanning of encrypted communications under the guise of combating child sexual abuse material.The pattern has become predictable. EU lawmakers introduce the proposal, claiming it includes safeguards for privacy. Technical experts explain why those safeguards are illusory. Public pressure mounts. The proposal is withdrawn or modified. Then, after a brief hiatus, it returns with minor tweaks, and the cycle begins anew. This latest withdrawal by the  follows the same script, but the underlying issues remain unresolved.What makes this particularly frustrating is that the fundamental problem with Chat Control has never been addressed. The proposal seeks to create what privacy experts call a “backdoor” into encryption, allowing authorities to scan messages before they’re encrypted or after they’re decrypted. Proponents argue this preserves encryption while enabling content moderation, but this reveals a dangerous misunderstanding of how encryption actually works. Creating any mechanism to access encrypted content inherently weakens the entire system, making it vulnerable not just to authorized access but to malicious actors as well.The technical impossibility of “safe” scanningThe core issue with Chat Control and similar proposals lies in a fundamental misunderstanding of encryption technology. End-to-end encryption works because only the sender and recipient possess the keys to decrypt messages. Any third party, whether a government agency or a tech company, cannot read the contents. This is not a design choice but a mathematical certainty that ensures the security of billions of communications daily.Client-side scanning, the technical approach favored by Chat Control advocates, attempts to circumvent this limitation by analyzing messages on users’ devices before encryption or after decryption. While this might sound like a clever workaround, it fundamentally breaks the security model of encryption. If a device can scan and report on message content, so can malware, hackers, or authoritarian governments who might compel tech companies to expand the scope of scanning.Security researchers have repeatedly demonstrated that there is no way to create a scanning system that only works for “good guys.”  learned this lesson the hard way in 2021 when it proposed a similar system for detecting child abuse imagery in iCloud photos. The backlash from security experts was swift and devastating, forcing the company to abandon the plan. The same security vulnerabilities that would enable Chat Control would inevitably be exploited by malicious actors, putting everyone at greater risk.Moreover, the scope creep inherent in surveillance technologies is well documented. A system initially designed to detect illegal content could easily be expanded to monitor political dissent, religious expression, or any other communication governments deem problematic. Countries around the world are watching the EU’s actions closely. If Chat Control were to pass, it would set a dangerous precedent that authoritarian regimes would eagerly exploit, claiming they’re simply following Europe’s lead in implementing “reasonable” content moderation.Public pressure and the power of resistanceThe withdrawal of Chat Control demonstrates the critical importance of sustained public engagement in technology policy. Unlike previous instances where technical proposals sailed through legislative processes with little public awareness, this fight has been characterized by unprecedented mobilization from civil society organizations, technology companies, security researchers, and ordinary citizens concerned about their digital rights.Organizations like the Electronic Frontier Foundation, , and numerous national privacy advocacy groups have played a crucial role in educating the public about the risks of Chat Control. Their efforts have included detailed technical explanations, legal analysis, and coordination of opposition campaigns that have reached millions of Europeans. This groundswell of opposition has made it politically toxic for lawmakers to support the proposal, at least in its current form.The effectiveness of this resistance offers important lessons for future policy battles. First, technical expertise matters. When security researchers speak with a unified voice about the impossibility of safe backdoors, it becomes harder for politicians to dismiss concerns as alarmist. Second, coalition-building across different sectors strengthens opposition. When civil liberties groups, tech companies, and individual users all oppose a policy, it suggests the problems are real and widespread. Third, sustained pressure is essential because, as Chat Control demonstrates, bad proposals rarely die on the first attempt.However, this victory should be tempered with realism. The forces pushing for Chat Control have not given up, and the underlying political dynamics that gave rise to the proposal remain unchanged. Politicians face genuine pressure to be seen as “doing something” about online harms, particularly regarding child safety. Until alternative approaches that don’t compromise encryption gain political traction, proposals like Chat Control will continue to resurface.The path forward requires education and alternativesThe repeated resurrection of Chat Control points to a deeper problem in how technology policy is made. Many lawmakers genuinely believe they can have both strong encryption and government access to encrypted content. This belief persists despite unanimous opposition from the cryptographic community because the political incentives favor appearing tough on crime over understanding complex technical realities.Breaking this cycle requires a fundamental shift in how we approach online safety. Rather than seeking technological magic bullets that promise security without trade-offs, policymakers need to invest in solutions that actually work. This includes better funding for law enforcement training and tools that don’t require breaking encryption, improved international cooperation on criminal investigations, and addressing the root causes of online exploitation through social programs and education. also bear responsibility for developing and promoting genuinely privacy-preserving safety features. End-to-end encrypted platforms can implement abuse prevention measures that don’t involve content scanning, such as metadata analysis, user reporting systems, and account-level restrictions for suspicious behavior. While these approaches may be less comprehensive than mass surveillance, they achieve meaningful safety improvements without the catastrophic privacy trade-offs of backdoors.Looking ahead, the privacy community cannot simply celebrate the withdrawal of Chat Control and move on. The next presidency of the EU Council will bring new opportunities for the proposal to resurface in yet another modified form. Sustained vigilance, continued public education, and proactive development of alternative safety measures will be essential. The fight to protect encryption is not a single battle but an ongoing campaign that requires long-term commitment from everyone who values digital privacy and security.The withdrawal of Chat Control is a victory, but it’s a temporary one. The fundamental challenge remains: convincing policymakers that some trade-offs are not worth making, and that breaking encryption to combat illegal content creates far more problems than it solves. Until that message truly sinks in, the zombie proposal will keep rising from the grave, and the privacy community must remain ready to defeat it again and again.]]></content:encoded></item><item><title>Did a Weather Balloon, Not a Mysteryious Space Object, Strike That United Airlines Flight?</title><link>https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysteryious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader joshuark shares this report from SFGate:


The mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... 

WindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... "At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further," [WindBorne's CEO John Dean posted on social media...]
 WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. 

WindBorne "has conducted more than 4,000 launches," the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus "The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight."


We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.]]></content:encoded></item><item><title>Did a Weather Balloon, Not a Mysterious Space Object, Strike That United Airlines Flight?</title><link>https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysterious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader joshuark shares this report from SFGate:


The mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... 

WindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... "At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further," [WindBorne's CEO John Dean posted on social media...]
 WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. 

WindBorne "has conducted more than 4,000 launches," the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus "The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight."


We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.]]></content:encoded></item><item><title>GHC now runs in the browser</title><link>https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169</link><author>kaycebasques</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:29:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Data centers contribute to high prices as energy bills electrify local politics</title><link>https://www.wsj.com/economy/consumers/surging-power-costs-are-putting-the-squeeze-on-customers-f8b2c04b</link><author>moose_man</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:16:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rising energy prices put AI and data centers in the crosshairs</title><link>https://techcrunch.com/2025/11/01/rising-energy-prices-put-ai-and-data-centers-in-the-crosshairs/</link><author>Tim De Chant</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A majority of consumers say they’re worried about data centers driving up electricity costs. Is the industry prepared for a possible backlash?]]></content:encoded></item><item><title>Elaborate Hoaxes in the Age of AI</title><link>https://hackernoon.com/elaborate-hoaxes-in-the-age-of-ai?source=rss</link><author>Jacob Landry</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This week, I’ve seen a lot of over-dramatization of very simple factual events that seem to be fueled by AI in many ways. Now, these aren’t “caused” by AI; I’m not referring to hoaxes that people have used AI specifically to spread, but things that AI has made worse by the ease with which fake information can be made to look very, very real.Fifteen years ago, this problem existed, but in my opinion, was severely muted. The concept of biased news is not new by any means and has been an issue for as long as the news has existed. There have always been audiences that are more susceptible to believing in these invented stories, scenarios, and scams, and the media has always catered to them, guiding them to the water they wish them to drink. \
My concern, and reason for this brain dump, is that with AI, these evil parties seem to be able to cast a much wider net than they could before. They can twist real news into something it’s not with fake videos made by AI; they can pump the internet full of AI-generated content that says whatever they want and cites other AI-generated sources, and they can mobilize an army of influencers that spread their filth like wildfire in an instant.\
I’ve found that recently, a huge chunk of my time when consuming any form of media is spent asking, “Is this real?” I consistently have to find multiple sources and manually scan them, looking for clues that it was AI-generated, a task that is getting harder and harder by the week. The videos are getting more realistic, the content is written better, and the sources I'm used to relying on are less and less trustworthy.A group of protestors (in this case, trolls) showed up at Chicago’s Bean with claims that there was a man trapped inside. The protestors claimed that they had found evidence that a wealth of life-support systems had been purchased during the making of the Bean statue, and also attempted to make a connection to a potential missing person (a baby, I believe) around the time of its construction. Trolls exist. They always have and always will. That’s not the issue at hand here. The issue is what happened next. This group was clearly trying to be funny, just causing a stir with some radical idea for their own amusement, but the internet used AI to take the country by storm.\
While scrolling, I started to see dozens of videos with screenshots of these purchase records, x-ray footage of a person floating inside the Bean, and “eyewitness” reports from someone who claimed they could hear knocking or scratching coming from inside the structure. There were also videos of the Bean being constructed, where you could clearly see the equipment being placed inside. Most, if not all, of these were generated by AI and are completely fake. I knew this, being a sensible human, but I had to admit that the quality created compelling evidence. With less common sense, I would have been easily duped.The recent discovery of 3I/ATLAS has been a goldmine for AI generators and the conspiracy-loving masses. From what I could find, which wasn’t much because it seems a lot of this information is being controlled to stop the spread of disinformation, all we know is that a comet is passing through our solar system. This comet looks like a comet and acts like a comet, but is slightly faster and is not orbiting our sun. One scientist ventured a challenge to the “it’s just a comet” consensus to encourage more critical thinking, theorizing that it was, of course, possible for it to be an alien craft. This set the AI-loving conspiracy nuts on fire.\
My Instagram feed was on absolute fire with fake videos of this comet with lights being emitted from the sides like a ship, with exhaust clearly venting into space, and fake X-ray shots that showed the internal ship structure and beings inside. Countless videos of influencers pretended to be experts on the matter and talked about potential alien invasions. The worst part of all of this was that each one cited different sources and pulled from different video content. It was easy to assume that a “potential alien invasion” was fake; however, I have to admit, the video content they provided was stunningly believable. The “experts” talking were confident and had plenty of “research” to back them up.\
The most alarming thing about this entire situation is how fast this misinformation is able to spread and how absolutely believable it can make it. I know this isn’t a controlled situation, and we’ve always had irresponsible people running social media accounts to susceptible individuals, but the use of AI in these fields is making the problem more abundant and harder to discern.It’s not all bad… but it’s pretty bad.AI is a wonderful tool that has the potential to make our lives easier. I don’t believe that it is ready for constant use yet, despite it being shoved down our throats around every turn. It consistently hallucinates and makes false claims; it slows my work down more than it speeds it up, and has become a barrier to productivity in most situations I’ve tried to use it. \
However, I can admit it has potential, and there are small automation tasks that I do find small uses for it. That being said, there’s always going to be a heated conversation around ethics and how we should be using AI. \
I don’t believe there will be any disagreement, however, that the above cases are the wrong way to use AI. The use of AI to generate content to fuel conspiracy theories and spread them to the masses as facts is dangerous and, frankly, terrifying. I believe we’re only seeing the tip of the iceberg, and I worry that we’re headed for a future where we’ll no longer be able to discern the difference between truth and fiction.]]></content:encoded></item><item><title>The Pearson Correlation Coefficient, Explained Simply</title><link>https://towardsdatascience.com/pearson-correlation-coefficient-explained-simply/</link><author>Nikhil Dasari</author><category>dev</category><category>ai</category><pubDate>Sat, 1 Nov 2025 16:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[A simple explanation of the Pearson correlation coefficient with examples]]></content:encoded></item><item><title>Security Holes Found in OpenAI&apos;s ChatGPT Atlas Browser (and Perplexity&apos;s Comet)</title><link>https://it.slashdot.org/story/25/11/01/054213/security-holes-found-in-openais-chatgpt-atlas-browser-and-perplexitys-comet?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The address bar/ChatGPT input window in OpenAI's browser ChatGPT Atlas "could be targeted for prompt injection using malicious instructions disguised as links," reports SC World, citing a report from AI/agent security platform NeuralTrust:

NeuralTrust found that a malformed URL could be crafted to include a prompt that is treated as plain text by the browser, passing the prompt on to the LLM. A malformation, such as an extra space after the first slash following "https:" prevents the browser from recognizing the link as a website to visit. Rather than triggering a web search, as is common when plain text is submitted to a browser's address bar, ChatGPT Atlas treats plain text as ChatGPT prompts by default. 

An unsuspecting user could potentially be tricked into copying and pasting a malformed link, believing they will be sent to a legitimate webpage. An attacker could plant the link behind a "copy link" button so that the user might not notice the suspicious text at the end of the link until after it is pasted and submitted. These prompt injections could potentially be used to instruct ChatGPT to open a new tab to a malicious website such as a phishing site, or to tell ChatGPT to take harmful actions in the user's integrated applications or logged-in sites like Google Drive, NeuralTrust said. 

Last month browser security platform LayerX also described how malicious prompts could be hidden in URLs (as a parameter) for Perplexity's browser Comet. And last week SquareX Labs demonstrated that a malicious browser extension could spoof Comet's AI sidebar feature and have since replicated the proof-of-concept (PoC) attack on Atlas. 



But another new vulnerability in ChatGPT Atlas "could allow malicious actors to inject nefarious instructions into the artificial intelligence (AI)-powered assistant's memory and run arbitrary code," reports The Hacker News, citing a report from browser security platform LayerX:



"This exploit can allow attackers to infect systems with malicious code, grant themselves access privileges, or deploy malware," LayerX Security Co-Founder and CEO, Or Eshed, said in a report shared with The Hacker News. The attack, at its core, leverages a cross-site request forgery (CSRF) flaw that could be exploited to inject malicious instructions into ChatGPT's persistent memory. The corrupted memory can then persist across devices and sessions, permitting an attacker to conduct various actions, including seizing control of a user's account, browser, or connected systems, when a logged-in user attempts to use ChatGPT for legitimate purposes.... 

"What makes this exploit uniquely dangerous is that it targets the AI's persistent memory, not just the browser session," Michelle Levy, head of security research at LayerX Security, said. "By chaining a standard CSRF to a memory write, an attacker can invisibly plant instructions that survive across devices, sessions, and even different browsers. In our tests, once ChatGPT's memory was tainted, subsequent 'normal' prompts could trigger code fetches, privilege escalations, or data exfiltration without tripping meaningful safeguards...." 


LayerX said the problem is exacerbated by ChatGPT Atlas' lack of robust anti-phishing controls, the browser security company said, adding it leaves users up to 90% more exposed than traditional browsers like Google Chrome or Microsoft Edge. In tests against over 100 in-the-wild web vulnerabilities and phishing attacks, Edge managed to stop 53% of them, followed by Google Chrome at 47% and Dia at 46%. In contrast, Perplexity's Comet and ChatGPT Atlas stopped only 7% and 5.8% of malicious web pages. 

From The Conversation:

Sandboxing is a security approach designed to keep websites isolated and prevent malicious code from accessing data from other tabs. The modern web depends on this separation. But in Atlas, the AI agent isn't malicious code — it's a trusted user with permission to see and act across all sites. This undermines the core principle of browser isolation. 


Thanks to Slashdot reader spatwei for suggesting the topic.]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-25/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:34 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-24/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:33 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-23/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:27 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-22/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:26 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-21/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:23 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-20/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:19 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-19/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:18 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-18/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-17/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:11 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-16/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:10 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Beyond Brute Force: 4 Secrets to Smaller, Smarter, and Dramatically Cheaper AI</title><link>https://hackernoon.com/beyond-brute-force-4-secrets-to-smaller-smarter-and-dramatically-cheaper-ai?source=rss</link><author>Anthony Laneau</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Large Language Models (LLMs) are incredibly powerful generalists, but transforming them into specialized experts is a major challenge. The process of training a model on new, specific knowledge like internal company documents or a complex reasoning task is notoriously expensive, time-consuming, and fraught with pitfalls. We want smaller, more efficient models that can master a domain without the compute budget of a tech giant.\
The core idea behind making smaller models smarter is a concept called "distillation." In this process, a smaller "student" model learns from a larger, more capable "teacher" model. The student doesn't just learn from a static textbook of examples; it learns to mimic the teacher's thought process. This is a powerful shortcut for transferring expertise.\
Until now, however, engineers have faced a frustrating trade-off. One approach, on-policy reinforcement learning (RL), forces the student to learn from its own mistakes, which is relevant but painfully slow. The alternative, off-policy distillation, is much faster but dangerously flawed; the student learns from the teacher's ideal examples, which often occur in contexts the student will never encounter on its own, causing errors to compound. This has been the bottleneck for creating specialized AI; until now.\
A powerful technique called "on-policy distillation" combines the best of both worlds. By having a teacher model provide dense, token-by-token feedback on the student model's own attempts, we can achieve breakthroughs in training efficiency and capability. Here are the four most surprising and impactful takeaways from this approach.A Smarter Feedback Loop Makes AI Training Up to 100x CheaperThe fundamental difference between Reinforcement Learning (RL) and Distillation lies in the density of the feedback. To understand this, imagine learning to play chess. is like learning chess by only being told if you won or lost at the very end of a match. The feedback is directly related to your actions, but it's sparse. You know you lost, but you don't know if it was because of your opening, a mid-game blunder, or a weak endgame. is like watching a grandmaster play. You observe brilliant moves, but they are made in complex board positions that you, as a novice, will rarely find yourself in. The feedback is dense, but the context is often irrelevant to your own learning path. provides the best of both worlds. It's like having an expert coach who grades every single one of your moves in your own games, telling you if a move was a "blunder," "inaccuracy," or "brilliant." The feedback is both dense and perfectly relevant to your current skill level.\
This smarter feedback loop has a massive impact on efficiency. In a direct back-to-back comparison where a student model learned from a teacher trained via RL, on-policy distillation allowed the student to reach the teacher's performance level 7-10 times faster in terms of gradient steps. This translates to a staggering 50-100x improvement in cumulative compute efficiency.\
The reason for this dramatic speedup is that on-policy distillation provides more useful information (more "bits per episode") for the model to learn from. Because this dense, token-level feedback reduces gradient noise, it allows for training with shorter contexts and smaller, more efficient batch sizes, further slashing the overall computational cost.You Can Cure “AI Amnesia” When Teaching New KnowledgeA common and frustrating problem in AI is "catastrophic forgetting." When you take a pre-trained model and fine-tune it on new, specialized information (like your company's internal knowledge base), it often degrades or completely forgets its original, general-purpose skills, such as the ability to follow instructions.\
Consider an experiment to create an "internal assistant." Researchers started with the Qwen3-8B model, which had a strong instruction-following score of 85%. After fine-tuning it on a 70-30 mix of internal company documents and general chat data:Its knowledge about the documents improved significantly (from 18% to 36% on a QA evaluation).However, its instruction-following skill degraded badly, dropping from 85% down to 79%.\
The solution was a brief phase of on-policy distillation after the initial fine-tuning. By using the original version of the model as the teacher, researchers could restore the lost behavior. The results were powerful:Instruction-following performance was almost fully recovered, jumping back up to 83%.Crucially, this happened without losing the newly acquired knowledge. In fact, the knowledge score even improved slightly to 41%.\
This finding is a game-changer for "continual learning," aka the ability to update models with new information over time without having to perform expensive, full-scale retraining from scratch. It provides a reliable way to teach an AI new facts without it forgetting its core skills.An AI Can Master a Reasoning Skill From Just One ExampleThis finding is highly counterintuitive. In most AI training methods, repeatedly training a model on the exact same prompt is a recipe for failure; the model simply memorizes the answer instead of learning the underlying skill.\
However, an experiment with on-policy distillation turned this assumption on its head. Researchers trained a student model on a math reasoning task using only a single, randomly chosen prompt. They trained on this one prompt for 20 consecutive steps, each with a batch of 256 rollouts, generating 5,120 total learning sequences.\
The remarkable outcome turns conventional wisdom on its head: the student model was able to approximately match the performance of the expert teacher model on the AIME'24 math benchmark, despite only ever having seen that one problem.\
This works because on-policy distillation teaches the model to approximate the teacher's entire thought process; its full probability distribution for what the next best token should be at every step, rather than just memorizing a final answer. This means that for certain skills, the bottleneck isn't finding thousands of examples, but creating a single, perfectly-guided learning experience.Why "Practicing" on Its Own Samples Can Make an AI DumberIt seems logical that if a model produces a high-quality output, you could feed that output back into its training data to reinforce good behavior. This method, known as supervised fine-tuning (SFT) on on-policy data, is like having the model "practice" on its own best work.\
But researchers found the opposite to be true. When they trained a model using a dataset composed of its own samples, its performance on an instruction-following evaluation actually degraded.\
The technical reason for this failure is subtle but critical. While the dataset of the model's own outputs might be perfectly on-policy on average, every finite batch of data exhibits a slightly different distribution. Training on these batches causes the model’s internal policy to drift away from its original state. This process turns training on its own samples into a form of off-policy training over time, leading to the same compounding error and divergence seen in other flawed methods.\
In contrast, on-policy distillation is completely stable in this self-distillation scenario. Because the teacher model remains a fixed, consistent target, the student can robustly converge on the desired behavior without degrading. This further cements on-policy distillation as a superior and more reliable tool for behavior refinement and continual learning.The Future of AI is Smaller, Faster, and More PersonalOn-policy distillation is more than just another training technique; it's a foundational shift in how we create specialized, expert AI. By combining the direct relevance of learning from one's own actions with the incredible efficiency of dense, token-by-token feedback, it solves some of the biggest challenges in applied AI.\
The benefits are clear: massive compute savings, a cure for catastrophic forgetting, and unbelievable data efficiency. This is a key enabling technology that lowers the barrier to entry, unlocking the ability for more teams to build and maintain custom models that possess deep domain knowledge without sacrificing core capabilities. This democratization of expert AI will fuel new business models and create competitive advantages previously reserved for frontier labs.]]></content:encoded></item><item><title>AI researchers ’embodied’ an LLM into a robot – and it started channeling Robin Williams</title><link>https://techcrunch.com/2025/11/01/ai-researchers-embodied-an-llm-into-a-robot-and-it-started-channeling-robin-williams/</link><author>Julie Bort</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI researchers at Andon Labs embedded various LLMs in a vacuum robot to test how ready they were to be embodied. And hilarity ensued.]]></content:encoded></item><item><title>Updated practice for review articles and position papers in ArXiv CS category</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>dw64</author><category>hn</category><pubDate>Sat, 1 Nov 2025 14:58:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux Kernel Ported To WebAssembly - Demo Lets You Run It In Your Web Browser</title><link>https://www.phoronix.com/news/Linux-Kernel-WebAssembly</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:40:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Open-source developer Joel Severin today announced his work on porting the Linux kernel to WebAssembly and has successffully gotten the kernel up and running within WASM-capable web browsers...]]></content:encoded></item><item><title>MIT Physicists Find a Way To See Inside Atoms That May Aid Search For Antimatter</title><link>https://science.slashdot.org/story/25/11/01/0545231/mit-physicists-find-a-way-to-see-inside-atoms-that-may-aid-search-for-antimatter?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Traditionally, exploring the interior of atomic nuclei requires enormous particle accelerators that stretch for kilometers and propel beams of electrons at extremely high speeds," writes SciTechDaily. 

But MIT physicists have unveiled a groundbreaking alternative that "used the atom's own electrons as probes to momentarily enter the nucleus..."



In research published in Science, a team of MIT physicists achieved exceptionally precise measurements of the energy of electrons orbiting a radium atom that had been chemically bonded with a fluoride atom to form radium monofluoride. By studying these molecules, the researchers created a kind of miniature particle collider. Within this environment, the electrons surrounding the radium atom were confined closely enough to occasionally slip into the nucleus before returning to their usual orbits... When those electrons returned to their outer paths, they retained the altered energy, effectively carrying a "message" from within the nucleus that could be decoded to reveal its internal arrangement... 

[The researchers] trapped and cooled the molecules and sent them through a system of vacuum chambers, into which they also sent lasers, which interacted with the molecules. In this way, the researchers were able to precisely measure the energies of electrons inside each molecule. When the researchers analyzed their measurements, they noticed that the electrons carried slightly different energies than expected if they had remained outside the nucleus. The difference was incredibly small, only about one millionth of the energy of the laser photon used to excite the molecules, but it was clear evidence that the electrons had entered the radium nucleus and interacted with its protons and neutrons... 

The researchers plan to use this new technique to create a detailed map of how forces are distributed inside the nucleus... to chart the nucleus with greater precision and search for possible violations of fundamental symmetries in nature. 

"It is thought that additional sources of fundamental symmetry violation are required to explain the almost complete absence of antimatter in our universe," the article points out. "Such violations could be seen within the nuclei of certain atoms such as radium... 


"Unlike most atomic nuclei, which are spherical in shape, the radium atom's nucleus has a more asymmetrical configuration, similar to a pear. Scientists predict that this pear shape could significantly enhance their ability to sense the violation of fundamental symmetries, to the extent that they may be potentially observable."]]></content:encoded></item><item><title>The Story Of an App That Was Slow</title><link>https://blog.devops.dev/the-story-of-an-app-that-was-slow-3844f1af9250?source=rss----33f8b2d9a328---4</link><author>Amita Pal Singh</author><category>devops</category><pubDate>Sat, 1 Nov 2025 14:12:02 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Once upon a time, there was an app — , it had beautiful forms, users loved it. Soon hundreds of people started visiting it. But then it started struggling. With 1500 daily users… slow load times and outages started causing chaos!Bandwidth is the maximum amount of data that can pass through the network at any given time.Throughput is the average amount of data that actually passes through over a given period of time.It is measured in bps = bits per secondNetwork latency is measured in milliseconds by calculating the time interval between the initiation of a send operation from a source system and the completion of the matching receive operation by the target system.This measurement helps developers understand how quickly a webpage or application will load for users.A ping rate of less than 100ms is considered acceptable but latency in the range of 30–40ms is desirable.The amount of time it takes for a response to reach a client device after a client request. It is double the Latency, plus processing time at server.Expected Daily users = 1500Expected Peak traffic = 100–200 form submissions per hourRequired Bandwidth ≥300 Mbps for 200–300 RPS (~1 MB each).Throughput ≥250 Mbps (80% of Bandwidth) for smooth data flow.Required Latency <100ms for snappy form rendering.RTT <200ms for quick submissions.Upgrade Bandwidth to 500 Mbps, enough for 200–300 simultaneous 1 MB form submissionsUse QoS rules to prioritize form traffic and upgrade load balancersDeploy a CDN to cache form assets closer to users and optimize server codeOptimize Db Queries, Add IndexesWant to create your own networking adventure? Track Bandwidth, Latency, Throughput, and RTT. Use observability tools, scale smart, and aim for these metrics to keep your users happy!]]></content:encoded></item><item><title>The Hidden Ledger of Code: Tracking the Carbon Debt Inside Our Software</title><link>https://hackernoon.com/the-hidden-ledger-of-code-tracking-the-carbon-debt-inside-our-software?source=rss</link><author>Jacob Wolinsky</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:00:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every line of code carries an invisible cost. As software scales, so does the energy it consumes and the emissions it generates.This growing footprint forms what many engineers now call carbon debt: the accumulation of energy waste caused by inefficient architecture, redundant compute, or neglected cleanup.The problem isn’t limited to theory anymore. Global data workloads are rising faster than the efficiency gains meant to offset them, and few teams have the tools to measure what their systems actually emit.Because engineers control how and where code runs, real progress starts inside development workflows, not in boardrooms.As carbon visibility moves closer to the code itself, software projects may soon be judged not only by speed and stability, but by how responsibly they use the power behind them.Teams talk about technical debt every sprint. They track code smells, refactoring needs, module complexity, and build bloat. But almost no one tracks the energy drain built into their systems, and this makes that blind spot real.\
Every inefficiency in code, like extra loops, redundant database fetches, and idle background tasks, translates into power use. Run thousands or millions of times per day, and what feels trivial becomes measurable emissions. Researchers have begun quantifying this: for example, the Green Algorithms framework shows that compute time, memory usage, and data center efficiency can be converted into carbon equivalent estimates for any computational task.\
At the data center scale, inefficiencies amplify. One white-paper found that servers may draw 60% to 90% of their peak power even while idle. Multiply that across dozens of servers, and weeks of wasted cycles become dozens of kilograms of CO2 equivalent.\
Every product team now operates with an invisible balance sheet, one that records carbon alongside complexity.The term carbon debt originates in environmental accounting, where it describes the accumulated emissions a system or entity has “borrowed” against future budgets with insufficient offsets. (It’s rooted in the broader notion of ecological or climate debt.) Now, technologists are borrowing that phrase to describe software systems whose inefficiencies accrue hidden energy costs over time.\
In software, carbon debt grows when layers of redundant code, over-provisioned infrastructure, and heavy frameworks persist unchecked. A module that spawns unnecessary background jobs, or a service that overfetches data, burns CPU cycles, which burn power.\
When infrastructure is sized with generous headroom “just in case,” that slack often stays underutilized, yet still draws baseline power. Servers and services often draw between 27% and 36% of peak power even under light load.\
As your system advances with more users, more services, and more replicas, each inefficiency multiplies. What once was a single wasted cycle becomes thousands per second. That energy waste endures unless addressed, compounding like interest owed on an invisible balance.\
Next, we’ll trace how code builds up emissions so you can see where the debt really comes from.How Code Accrues EmissionsThe energy footprint of software often hides in the smallest details of its logic. A loop that runs one step too long or a recursive function that never terminates efficiently can keep processors active far longer than needed. Each extra millisecond of compute draws power, and the effect multiplies when thousands of users trigger the same function at once.How Tiny Loops Turn Into Big CostsResearch on mobile software shows that energy code smells can dramatically increase consumption, and in some cases, they can consume up to 87x more energy than clean versions. Follow-up work found that fixing these patterns delivered 4% to 30% efficiency gains in practice. These results reinforce the broader point: repetitive, seemingly minor patterns accumulate real power draw over time.\
Similar waste appears in everyday engineering habits: redundant database queries, unnecessary front-end re-renders, and dormant API endpoints all keep processors active, drawing power without improving performance.\
Over-sized build artifacts and idle background tasks deepen the impact by holding memory and storage resources active long after they’re useful. When these patterns run across millions of daily transactions, the emissions scale from grams to kilograms of CO2. Quantifying that footprint is the next challenge, and few teams yet have the tools to do it precisely.Measuring What We Don’t SeeTracking how much energy software really uses is harder than it sounds. The Software Carbon Intensity (SCI) framework from the Green Software Foundation is one of the first real attempts to make that measurable, like mapping compute time, memory use, and data transfer against actual energy data.\
Tools such as Cloud Carbon Footprint and CodeCarbon are now taking that formula a step further, embedding energy estimates directly into build pipelines and dashboards so developers can see environmental impact alongside performance metrics. This aligns with broader conversations inside the DevOps community, where teams are beginning to explore practical ways to embed sustainability into build and deployment workflows.\
The challenge is translating code execution into physical terms. Every watt drawn depends on processor type, cooling efficiency, and the carbon intensity of the grid that powers the data center. The same workload might have a fraction of the emissions on renewable-heavy infrastructure compared to fossil-fueled grids.\
The logic behind these tools isn’t far from how predictive analytics is being used to expose hidden operational costs in other industries, turning guesswork into measurable insight. Until this kind of visibility becomes standard in developer environments, most teams will keep optimizing performance while staying blind to the energy behind it.The Governance Gap: Why Carbon Isn’t Yet a Coding MetricSustainability still sits outside most engineering workflows. In many companies, carbon reporting lives with facilities or operations teams, not with the people writing or deploying code.\
As a result, the energy cost of a release is rarely discussed in sprint planning or post-mortems. Agile ceremonies track velocity, story points, and error rates, but not emissions.Few DevOps environments include “carbon sprints” or carbon budgets, even though they could be tracked the same way as uptime or latency. A report based on responses from over 2,000 software practitioners has found that most organizations are still in the early stages of measuring software-related emissions. Others echoed this, noting that sustainability metrics remain largely absent from continuous-integration and delivery pipelines.\
That gap is beginning to close. Some open-source communities have started experimenting with “green commits” to tag energy-efficient changes, and enterprise dashboards are beginning to surface sustainability data next to performance KPIs. As this visibility improves, design priorities are shifting toward decay and restraint by building systems that know when to slow down, scale back, or shut off entirely.Designing for Decay: Making Efficiency a DefaultArchitects concerned with long-lived systems often speak of architectural erosion or design decay, like the gradual divergence between intended structure and runtime reality. Architecture erosion is a well-known risk in systems as features accumulate and shortcuts proliferate. One way to counter that drift is to build systems that self-optimize or sunset unused processes automatically, pruning inactive modules or trimming underutilized services based on real usage signals.Treating code decay as a feature means embedding routines that perform periodic cleanup: archiving stale APIs, retiring dormant modules, or enforcing dependency hygiene. Frameworks may require that libraries unused for X releases be flagged or removed. Over time, the shift moves from “unlimited scaling” toward sustainable scaling, systems designed to shrink or sleep when load is low rather than running flat out forever.\
Engineers can use runtime profiling, build monitoring, and garbage-collection heat maps as signals. If a microservice’s CPU utilization stays near zero for weeks, it raises a refactor or archive flag. If build artifacts grow without change, they are flagged for pruning.\
This philosophy sets the stage for what’s next: making carbon visibility part of everyday decision-making, and bringing engineering metrics and emissions metrics into the same ecosystem.The Road to Carbon TransparencyImagine an IDE where each file, function, or commit carries a live “emissions counter”; you write a loop, and you see how much energy it might cost. That’s the direction software tooling is heading. Build tools could come to flag carbon-heavy changes before they’re merged.\
CI/CD pipelines will evolve to flag carbon-intensive builds, perhaps even rejecting code that spikes emissions far above baseline. With tighter integration, carbon metrics will merge with performance dashboards, showing build time, throughput, and CO2 cost in one pane.Cloud Dashboards & Deployment TransparencyCloud providers may expose per-deployment carbon cost insights, mapping workload emissions to regions, instance types, and schedules. The same principle underpins the idea of carbon-aware computing, where workloads shift dynamically to regions or times with cleaner grids. Integrating that into the same console where devs monitor CPU, bandwidth, and billing makes sustainability part of everyday trade-offs.\
With visibility in place, engineers will begin to optimize not just for latency or memory, but for carbon as a first-class metric. Those insights will shape budgeting decisions, drive architecture choices (edge, serverless, off-peak scheduling), and enforce sustainable defaults in code.\
Ahead lies a time when your pull request comes with a carbon delta and teams judge changes not only by correctness or performance, but by how much energy they add or save.Engineering AccountabilitySustainability in software doesn’t start in a server farm, but it starts at the keyboard. Every query, commit, and deployment decision shapes the energy profile of the systems we run. For years, efficiency meant speed and uptime, and now it also means restraint.\
Across the industry, teams are beginning to treat carbon debt the same way they treat technical debt: as something that compounds if ignored. Cleaning up unused code, right-sizing infrastructure, or pausing idle jobs are no longer side tasks; they’re acts of maintenance that protect performance and the planet.\
As tooling matures, carbon visibility will become part of normal governance, sitting next to reliability and security in every build report. The responsibility won’t rest with operations alone but with every engineer who touches code. Because in modern software, clean code and clean energy belong to the same conversation, and writing one well means caring about the other.]]></content:encoded></item><item><title>Go: Can It Mitigate Supply Chain Attacks?</title><link>https://hackernoon.com/go-can-it-mitigate-supply-chain-attacks?source=rss</link><author>Go [Technical Documentation]</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:00:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern software engineering is collaborative, and based on reusing Open Source software. That exposes targets to supply chain attacks, where software projects are attacked by compromising their dependencies.\
Despite any process or technical measure, every dependency is unavoidably a trust relationship. However, the Go tooling and design help mitigate risk at various stages.There is no way for changes in the outside world—such as a new version of a dependency being published—to automatically affect a Go build.\
Unlike most other package managers files, Go modules don’t have a separate list of constraints and a lock file pinning specific versions. The version of every dependency contributing to any Go build is fully determined by the  file of the main module.\
Since Go 1.16, this determinism is enforced by default, and build commands (, , , , …) will fail if the go.mod is incomplete. The only commands that will change the  (and therefore the build) are  and . These commands are not expected to be run automatically or in CI, so changes to dependency trees must be made deliberately and have the opportunity to go through code review.\
This is very important for security, because when a CI system or new machine runs , the checked-in source is the ultimate and complete source of truth for what will get built. There is no way for third parties to affect that.\
Moreover, when a dependency is added with , its transitive dependencies are added at the version specified in the dependency’s  file, not at their latest versions, thanks to Minimal version selection. The same happens for invocations of go install example.com/cmd/devtoolx@latest, the equivalents of which in some ecosystems bypass pinning. In Go, the latest version of  will be fetched, but then all the dependencies will be set by its  file.\
If a module gets compromised and a new malicious version is published, no one will be affected until they explicitly update that dependency, providing the opportunity to review the changes and time for the ecosystem to detect the event.Version contents never changeAnother key property necessary to ensure third parties can’t affect builds is that the contents of a module version are immutable. If an attacker that compromises a dependency could re-upload an existing version, they could automatically compromise all projects that depend on it.\
That’s what the  file is for. It contains a list of cryptographic hashes of each dependency that contributes to the build. Again, an incomplete  causes an error, and only  and  will modify it, so any changes to it will accompany a deliberate dependency change. Other builds are guaranteed to have a full set of checksums.\
This is a common feature of most lock files. Go goes beyond it with the Checksum Database (sumdb for short), a global append-only cryptographically-verifiable list of go.sum entries. When  needs to add an entry to the  file, it fetches it from the sumdb along with cryptographic proof of the sumdb integrity. This ensures that not only every build of a certain module uses the same dependency contents, but that every module out there uses the same dependency contents!\
The sumdb makes it impossible for compromised dependencies or even Google-operated Go infrastructure to target specific dependents with modified (e.g. backdoored) source. You’re guaranteed to be using the exact same code that everyone else who’s using e.g. v1.9.2 of  is using and has reviewed.\
Finally, my favorite features of the sumdb: it doesn’t require any key management on the part of module authors, and it works seamlessly with the decentralized nature of Go modules.The VCS is the source of truthMost projects are developed through some version control system (VCS) and then, in other ecosystems, uploaded to the package repository. This means there are two accounts that could be compromised, the VCS host and the package repository, the latter of which is used more rarely and more likely to be overlooked. It also means it’s easier to hide malicious code in the version uploaded to the repository, especially if the source is routinely modified as part of the upload, for example to minimize it.\
In Go, there is no such thing as a package repository account. The import path of a package embeds the information that needs in order to fetch its module directly from the VCS, where tags define versions.\
We do have the Go Module Mirror, but that’s only a proxy. Module authors don’t register an account and don’t upload versions to the proxy. The proxy uses the same logic that the  tool uses (in fact, the proxy runs ) to fetch and cache a version. Since the Checksum Database guarantees that there can be only one source tree for a given module version, everyone using the proxy will see the same result as everyone bypassing it and fetching directly from the VCS. (If the version is not available anymore in the VCS or if its contents changed, fetching directly will lead to an error, while fetching from the proxy might still work, improving availability and protecting the ecosystem from “left-pad” issues.)\
Running VCS tools on the client exposes a pretty large attack surface. That’s another place the Go Module Mirror helps: the  tool on the proxy runs inside a robust sandbox and is configured to support every VCS tool, while the default is to only support the two major VCS systems (git and Mercurial). Anyone using the proxy can still fetch code published using off-by-default VCS systems, but attackers can’t reach that code in most installations.Building code doesn’t execute itIt is an explicit security design goal of the Go toolchain that neither fetching nor building code will let that code execute, even if it is untrusted and malicious. This is different from most other ecosystems, many of which have first-class support for running code at package fetch time. These “post-install” hooks have been used in the past as the most convenient way to turn a compromised dependency into compromised developer machines, and to worm through module authors.\
To be fair, if you’re fetching some code it’s often to execute it shortly afterwards, either as part of tests on a developer machine or as part of a binary in production, so lacking post-install hooks is only going to slow down attackers. (There is no security boundary within a build: any package that contributes to a build can define an  function.) However, it can be a meaningful risk mitigation, since you might be executing a binary or testing a package that only uses a subset of the module’s dependencies. For example, if you build and execute  on macOS there is no way for a Windows-only dependency or a dependency of example.com/cmd/othertool to compromise your machine.\
In Go, modules that don’t contribute code to a specific build have no security impact on it.“A little copying is better than a little dependency”The final and maybe most important software supply chain risk mitigation in the Go ecosystem is the least technical one: Go has a culture of rejecting large dependency trees, and of preferring a bit of copying to adding a new dependency. It goes all the way back to one of the Go proverbs: “a little copying is better than a little dependency”. The label “zero dependencies” is proudly worn by high-quality reusable Go modules. If you find yourself in need of a library, you’re likely to find it will not cause you to take on a dependency on dozens of other modules by other authors and owners.\
That’s enabled also by the rich standard library and additional modules (the  ones), which provide commonly used high-level building blocks such as an HTTP stack, a TLS library, JSON encoding, etc.\
All together this means it’s possible to build rich, complex applications with just a handful of dependencies. No matter how good the tooling is, it can’t eliminate the risk involved in reusing code, so the strongest mitigation will always be a small dependency tree.\
This article is available on  under a CC BY 4.0 DEED license.]]></content:encoded></item><item><title>Graph RAG vs SQL RAG</title><link>https://towardsdatascience.com/graph-rag-vs-sql-rag/</link><author>Reinhard Sellmair</author><category>dev</category><category>ai</category><pubDate>Sat, 1 Nov 2025 14:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Evaluating RAGs on graph and SQL databases]]></content:encoded></item><item><title>CharlotteOS – An Experimental Modern Operating System</title><link>https://github.com/charlotte-os/Catten</link><author>ementally</author><category>hn</category><pubDate>Sat, 1 Nov 2025 13:12:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Archinstall 3.0.12 &amp; Pacman 7.1 Released For Arch Linux Users</title><link>https://www.phoronix.com/news/Archinstall-3.0.12-Pacman-7.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 13:05:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Kicking off November for Arch Linux users happen to be the releases of Pacman 7.1 as well as Archinstall 3.0.12...]]></content:encoded></item><item><title>There’s a Dinosaur ‘Mummy Zone.’ Here’s What Scientists Found There.</title><link>https://www.404media.co/theres-a-dinosaur-mummy-zone-heres-what-scientists-found-there/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/image3.jpg" length="" type=""/><pubDate>Sat, 1 Nov 2025 13:00:22 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to a special scary installment of the Abstract! Is Halloween over? Technically yes. But as lovers of spooky season will know, the fallout from Halloween—when dawn reveals the remains of the festivities—is an extension of the day itself. That is why I have assembled a parade of horrors for you this morning.First, ancient mummies from a watery grave. Then: : let’s get tangled up in one of the great mysteries of spider webs; watch out for the venomous snakes; and lastly, scientists literally ask where the bodies are buried. And now, Halloween is over but the darkness lingers on…Prepare to enter the “mummy zone”If you think human mummies are scary, wait until you meet dinosaur mummies. Paleontologists have discovered the mummified remains of two duck-billed dinosaurs that belong to the species (go Oilers!) , which lived 66 million years ago in what is now Wyoming. The immaculate preservation of the animals—a 2-year-old juvenile and young adult that was roughly 5 to 8 years old—exposed unprecedented corporeal details, such as intricate polygonal scales, spinal spikes, fleshy contours, skin wrinkles, and the first hooves ever identified in a dinosaur (or any reptile), making them the oldest hooves in the fossil record.“The late juvenile is the first subadult dinosaur mummy on record and the first large-bodied dinosaur preserving the fleshy midline over the trunk,” said researchers led by Paul Sereno of the University of Chicago. “The early adult is the first hadrosaurid to preserve the entire spike row from hips to tail tip and the first reptile preserving wedge-shaped pedal hooves.”These “end-Cretaceous  preserve the oldest hoof renderings for any tetrapod, the first record of hooves in a reptile” and “the first instance of a hooved tetrapod capable of bipedal locomotion,” the team added.For more than a century, paleontologists have discovered dinosaur mummies in what the team delightfully calls the “mummy zone” of the Lance Formation of east-central Wyoming. In addition to adding new specimens to this collection, Sereno and his colleagues have also shed light on the rare process of “clay templating” that produces this preservation of mummified flesh, skin, and other soft tissues. As the carcasses of these Edmontosaurs dried in the Cretaceous sun, they were suddenly immersed under a flash flood that left a thin biofilm over their skin, preserving the three-dimensional soft tissues in time. The team concluded that “the extraordinary preservation of the ‘mummy zone’ is due to rapid subsidence in a coastal setting subject to seasonal drought-flood cycles.”While it’s sad that these dinos died young, it’s a miracle that we can observe their flesh, skin, and hooves 66 million years later. Anyway, it’s not as if they had much to look forward to, since the die was already cast for the non-avian dinosaurs. In this case, the die is an apocalyptic space rock to which we humans owe our existence—and the reality of these cosmic gambles of fate is frankly scarier than any mummy, even a  mummy, ever could be.Deck the halls with stabilimentaHalloween revelers will be taking down their decorative spider-web today, but real spiders keep their web decor up all year long. Researchers have long struggled to make sense of the silky zigzag ornaments that some spiders weave, known as “stabilimenta,” which could serve as insect attractors, predator defense, thermal protection, or water collection. Nobody is really sure!Now, a team has suggested that stabilimenta might be vibrational amplifiers that help alert spiders to prey impacting the web. “No studies have yet investigated how web decorations affect vibration propagation in orb webs,” said researchers led by Gabriele Greco of the Swedish University of Agricultural Sciences. By studying wasp spiders in Sardinia, the team found that in certain cases, ”the presence of the stabilimentum enhances the spider’s ability to detect prey location…compared to webs without a stabilimentum.”“However, from a biological standpoint, the high variability in stabilimentum geometry suggests that the observed differences in elastic wave propagation are unlikely to have a consistent or significant functional role,” the team added. In other words, these web decorations still defy a one-size-fits-all explanation. My own hypothesis is that they are just the dorm room posters of the spider world.A new antivenom bites backThough I am a militant , even I can see how a clade of dangerous limbless weirdos ended up becoming so widely feared and maligned. Snakes kill tens of thousands of people per year, an ongoing tragedy that has galvanized researchers to develop antivenom “cocktails” that could treat snakebit emergencies across many species while also mitigating adverse immunological reactions.  A new study has now addressed this challenge “by immunizing an alpaca and a llama with the venoms of 18 different snakes, including mambas, cobras and rinkhals,” said researchers led by Shirin Ahmadi of the Technical University of Denmark. The cocktail “effectively prevented venom-induced lethality in vivo across 17 African elapid snake species” and “shows considerable promise for comprehensive, continent-wide protection against snakebites by all medically relevant African elapids,” the team added. While it will take much more research to prove it is safe and effective in humans, the antivenom is a significant step toward preventing snakebite deaths and injuries. It's also about the only cocktail that you hope you’ll never have the desire to order.The more (dead), the merrierWe’ll close on that most hallowed of Halloween traditions—a trip to the haunted cemetery.In an unprecedented study, researchers mapped out tens of thousands of ancient Chinese tombs spanning the past 4,000 years since the Xia Dynasty. The team used mapping software to analyze the age and location of the tombs to search for clues about the social and political impacts on burial locations.  “The number of ancient tombs varied significantly across historical periods” with the Qing dynasty (1644 - 1912) accounting for 47.012 percent of the national total, while the Sui dynasty (581 - 618) had only 0.134 percent, according to researchers led by Quanbao Ma of the Beijing University of Civil Engineering and Architecture.“From a temporal perspective, periods of frequent dynastic transitions and wars in Chinese history were often accompanied by significant population declines and migrations,” the team added. “However, during post-war recovery and periods of societal stability, population numbers typically rebounded and grew rapidly, which was also reflected in the increasing number of deceased individuals requiring burial.”In other words, contrary to Halloween lore, it can be a good sign to find a lot of dead bodies in one place because it’s often an indicator of more peaceful and stable times (dead bodies, after all, are the product of living ones). What might be more creepy is an absence of graves in your general vicinity. With that in mind, go visit your local ghosts; they are great company this time of year.Thanks for reading! See you next week.]]></content:encoded></item><item><title>Chips Need to Chill Out</title><link>https://spectrum.ieee.org/thermal-management-chips</link><author>Harry Goldstein</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1NzcyNS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxMjg5Nzk0M30.oEWlOSimqjs-0YhG6yHcN93FhULhj37OplNQHCtEdkQ/image.png?width=600" length="" type=""/><pubDate>Sat, 1 Nov 2025 13:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The semiconductor industry seeks radical cooling solutions]]></content:encoded></item><item><title>Samsung Building Facility With 50,000 Nvidia GPUs To Automate Chip Manufacturing</title><link>https://hardware.slashdot.org/story/25/10/31/2352207/samsung-building-facility-with-50000-nvidia-gpus-to-automate-chip-manufacturing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from CNBC: Korean semiconductor giant Samsung said Thursday that it plans to buy and deploy a cluster of 50,000 Nvidia graphics processing units to improve its chip manufacturing for mobile devices and robots. The 50,000 Nvidia GPUs will be used to create a facility Samsung is calling an "AI Megafactory." Samsung didn't provide details about when the facility would be built. It's the latest splashy partnership for Nvidia, whose chips remain essential for building and deploying advanced artificial intelligence. [...]
 
On Thursday, Nvidia representatives said they will work with Samsung to adapt the Korean company's chipmaking lithography platform to work with Nvidia's GPUs. That process will results in 20 times better performance for Samsung, the Nvidia representatives said. Samsung will also use Nvidia's simulation software called Omniverse. Known for its mobile phones, Samsung also said it would use the Nvidia chips to run its own AI models for its devices. In addition to being a partner and customer, Samsung is also a key supplier for Nvidia. Samsung makes the kind of high-performance memory Nvidia uses in large quantities, alongside its AI chips, called high bandwidth memory. Samsung said it will work with Nvidia to tweak its HBM4 memory for use in AI chips.]]></content:encoded></item><item><title>SQLite concurrency and why you should care about it</title><link>https://jellyfin.org/posts/SQLite-locking/</link><author>HunOL</author><category>hn</category><pubDate>Sat, 1 Nov 2025 12:59:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.SQLite is a file-based database engine running within your application and allows you to store data in a relational structure.
Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so.
Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.So an application that wants to use SQLite as its database needs to be the only one accessing it.
Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL).
The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file.
This allows multiple parallel writes to take place and get enqueued into the WAL.
When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly.
This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.A transaction is supposed to ensure two things.
Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction.
This is where it gets spicy and we come to the real reason why I am writing this blog post.
For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes.
This seems to be a not uncommon issue and there are many reports to be found on the issue.The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug.
From the reports this issue happens across all operating systems, drive speeds and with or without virtualization.
So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite.
During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal.
In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle.
While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error.
That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level.
EF Core supports a way of hooking into every command execution or transaction by creating Interceptors.
With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller.
The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary.
So, I decided on three locking strategies:As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override  in .Optimistic locking behavior​Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely.
This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.Jellyfin uses the  library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.Pessimistic locking behavior​Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed.
This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode.
While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.The future Smart locking behavior​In the future we might also consider combining both modes, to get the best of both worlds.Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here.
There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query.
Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.]]></content:encoded></item><item><title>Do you know that there is an HTML tables API?</title><link>https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/</link><author>begoon</author><category>hn</category><pubDate>Sat, 1 Nov 2025 12:58:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an old, forgotten  ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:let table 
let b  document.
let t  document.
b.t
table.rowri
  let r  t.ri
  row.li
    let c  r.i
    c. llet table = [
  ['one','two','three'],
  ['four','five','six']
];
let b = document.body;
let t = document.createElement('table');
b.appendChild(t);
table.forEach((row,ri) => {
  let r = t.insertRow(ri);
  row.forEach((l,i) => {
    let c = r.insertCell(i);
    c.innerText = l;  
  })
});You can then access each table cell with an index (with t being a reference to the table):console.t..console.log(t.rows[1].cells[1]);
// => <td>five</td>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:t.
t..
t...t.insertRow(-1);
t.rows[2].insertCell(0);
t.rows[2].cells[0].innerText = 'foo';There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.]]></content:encoded></item><item><title>The Python Coding Stack: And Now You Know Your ABC</title><link>https://www.thepythoncodingstack.com/p/and-now-you-know-your-abc-python-abstract-base-classes</link><author></author><category>dev</category><category>python</category><pubDate>Sat, 1 Nov 2025 12:52:51 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[I may have already mentioned in a previous post that I’ve rekindled an old passion: track and field athletics. I’m even writing about it in another publication:  (look out for the new series on sprint biomechanics, if you’re so inclined!)This post is inspired by a brief thought that crossed my mind when I considered writing software to assist my club—in the end, I chose not to. (But I already volunteer three hours a week coaching members of the youth team. So no, I don’t feel guilty.)Here’s the task. I want to create a class to represent a track and field event held in a competition. This entry allows you to enter the raw results as reported by the officials—the athlete’s bib number and the time they clocked in a race. It then computes the event’s full results.A good starting point is to define two classes,  and . I’ll focus on the  class in this post, so I’ll keep the  class fairly basic:All code blocks are available in text format at the end of this article • #1 • The code images used in this article are created using Snappify. [Affiliate link]There’s more we could add to make this class more complete. But I won’t. You can create a list of athletes that are competing in the track and field meeting:To make it easy and efficient to obtain the  object just from a bib number, you can create a  dictionary:The dictionary’s keys are the bib numbers as integers and the values are the  objects. Therefore, print(bib_to_athlete[259].name) displays .Now, let’s move to the  class:Let’s go through the data attributes in : is a string with the name of the event. You could use other data types, such as an , but I’ll keep this simple here and use a string. is a list containing  instances. These are the athletes participating in this particular event at the track and field meeting. contains each athlete’s performance. We have choices. The simplest option is to use a list of tuples, where each tuple contains the athlete and the performance. However, since we’re working in the object-oriented domain, perhaps we could create a  class, and  becomes a list of  instances.So, let’s create the  class at the top of the script:We’ll show an example of  and  instances soon. But first we need some methods. Let’s get back to the  class and define , which allows you to add a raw result in the format provided by the officials—the officials only provide the bib number and the performance, such as the time clocked by the athlete:You add checks for the scenarios when a bib number doesn’t match any athlete in the whole meeting or when the bib number doesn’t match any athlete in the specific event. The dictionary  method returns  by default if the key is not present in the dictionary.If the bib number is valid, you create a  instance and add it to .Once you add all the results from the officials, you’re ready to finalise the results:This method sorts the list of results based on the value of the  data attribute.Let’s try this class to see everything is as we expect:You create an  instance for the 100m sprint race. Then you add three individual results. Finally, you finalise the results to get the athletes and their positions in the race.Since  sorts based on the performace, the athlete with the fastest time takes the first position in the list, and so on. Here’s the output from the  loop displaying the athletes in  in order:Usain Bolt: 9.58
Carl Lewis: 9.86
Jesse Owens: 10.3Usain Bolt is first with the fastest time, followed by Carl Lewis, and Jesse Owens in third. Everything seems to be working well… But “It works” are the two most dangerous words in programming!It’s Time For The Long JumpThe long jump results come in next from the officials. You know how to input the data and finalise the results now:And here are the results displayed:Carl Lewis: 8.87
Mike Powell: 8.95But, but… Mike Powell’s 8.95m is better than Carl Lewis’s 8.87m. The order is wrong!Of course! In the 100m sprint the fastest time wins, and the fastest time is the lowest number. But in the long jump it’s the longest jump that wins, and that’s the largest number!The  method no longer works for the long jump or for other field events. For field events, the sorting needs to happen in reverse order.There are several ways to deal with this problem. There are always several ways to solve a problem in programming.The simplest is to add an  Boolean data attribute in  and then add an  statement in . And if this were the only difference between running and field events, this would indeed be a fine option.However, if there are more differences to account for, the extra fields and  statements scattered throughout the class make the class harder to maintain.So, let’s look at another option.Creating Separate Classes for Track Events and Field EventsYou could create two classes instead of one:  and . Each class will take care of getting the  method right and also deal with any other differences we may find between track events and field events.However, you don’t want to create two unrelated classes since these classes will have a lot in common. Sure, you can copy and paste the code that’s common between the two classes, but you don’t need me to tell you that’s not a good idea. Will you remember to make changes in both places when you decide to change the implementation later?You also want to make sure they have the same data attribute names for similar things. For example, you don’t want one class to have  and the other , say, or change the spelling by mistake. Now, I know you’ll pay attention when creating these methods to make sure this doesn’t happen, but why take the risk?And when you come back to your code in six months’ time (or a colleague starts working on the code) and you decide you need another class that follows the same principles, will you remember what methods you need to include? You can spend time studying your old classes, of course. But wouldn’t you like to make your life a bit simpler? Of course you would.However,  shouldn’t inherit from  since a track event  a field event. The same applies the other way around. These classes are siblings, so they can’t have a parent-child relationship.Instead, they could both inherit from a common parent. So, let’s keep the  class as a common parent for both  and .However, the only purpose of  is to serve as a starting point for the two child classes. You no longer want a user to create instances of  now that you have  and . They should only create instances of  or . How can you make this clear in your code and perhaps even prevent users from creating an instance of the parent class, ?Abstract Base Classes (ABCs)The answer is Abstract Base Classes, often shortened to ABCs. The ABC acronym gives the impression these are as easy as ABC—they’re not, but there’s no reason they need to be difficult, either. The title of this article is also a reference to a rhyme my children used to sing when they were toddlers learning their ABC!Let’s refresh your memory about different terms often used to describe the inheritance relationship between classes:You can refer to  and . The child class inherits from the parent class.Or you can refer to  and . The subclass inherits from the superclass. This is where the built-in  gets its name.Or you can refer to  and . The derived class inherits from the base class.Whatever terms you choose to use, they refer to the same things!So, an ABC is a base class, since other classes are derived from it. That deals with the BC in ABC. And it’s abstract because you’re never going to create a concrete instance of this class.You’re not meant to create an instance of an abstract base class and often you’ll be prevented from doing so. But you can use it to derive other classes.Let’s turn  into an abstract base class:The changes from the previous version are highlighted. Let’s go through each change:From the  module—and now you know what these letters stand for—you import  and . You’ll see both of these referred to and explained in the bullets below.When you define the  class, you include  as its base or parent class. When a class inherits from the  class, the class itself becomes an abstract base class. Anyone reading your code will immediately understand the role of this class.You add the  decorator before defining  and you also remove the contents of this method. Since you can’t leave the body of a function or method blank, you include the  statement as the method’s body. This method doesn’t do anything, but it’s there, and it’s marked as an abstract method. Let’s see what this means…Here’s what you’re effectively stating when you create this ABC with the  abstract method: Any class derived from the  ABC  include a  method.Let’s explore this by defining  and . At first, you’ll keep them simple:You define the two new classes  and . They inherit from , which is an abstract base class. You just include  as the body of each class for now. This means that these classes inherit everything from the parent class and have no changes or additions. They’re identical to the parent class.Note that you now use  and  to create instances for the 100m race and the long jump.However, you get an error when you try to create these instances:Traceback (most recent call last):
  File ... line 53, in <module>
    track_100m = TrackEvent(
        “100m Sprint”,
        [bib_to_athlete[259], bib_to_athlete[161], bib_to_athlete[362]]
    )
TypeError: Can’t instantiate abstract class TrackEvent without
    an implementation for abstract method ‘finalise_results’The abstract base class includes the method , which is marked as an abstract method. Python is expecting a concrete implementation of this method. Any class that inherits from the  ABC  have a concrete implementation of this method. Let’s fix this:In , you include the same code you had in the original  since this algorithm works well for track events where the smallest numbers (the fastest times) represent the best performances.However, you pass  to  in FieldEvent.finalise_results() since the largest numbers (longest distances) represent the best performances in this case.You can now try these new classes on the 100m race results and the long jump results you used earlier:You now use the new derived classes  and  in this code instead of . You also add two new printouts to separate the results.100m Sprint Results:
Usain Bolt: 9.58
Carl Lewis: 9.86
Jesse Owens: 10.3
​
Long Jump Results:
Mike Powell: 8.95
Carl Lewis: 8.87The 100m results show the fastest times (smallest values) as the best performances. The long jump results show the longer jump (larger value) as the best performance. All as it should be!Wind Readings and Breaking TiesBut there are more differences we need to account for. In some track and field events, the wind reading matters. In these events, if the wind reading is larger than 2.0 m/s, the results still stand but the performances cannot be used for official records.But does this affect track events or field events? So should you account for this in the  class or in the  class?It’s not so simple. Wind readings matter in some track events, but not all. And they also matter in some field events, but not all. So, you have a different subset of events to account for now. The 100m, 200m, 110m hurdles and 100m hurdles, which are all track events, are in the same category as the long jump and triple jump, which are field events.But before we find a solution for this, here’s something else to mess things up even more. What happens when there’s a tie—when two athletes have the same performance value? The tie-breaking rules also depend on the event. Let’s ignore the track events here, since depending on what timing system is used, it’s either the officials who decide or the higher precision times from the automatic timing systems.But what about the field events? In most of them, if there’s a tie, the next best performance is taken into account. However, the rules are different for the high jump and pole vault events where there’s a count back system used. Explaining the rules of track and field is not the purpose of this article, so I won’t!So that’s yet another subset to consider: tie breaks in the vertical jumps are different from tie breaks in the horizontal jumps and throws.How can we account for all these subsets of events?There are always many solutions to the same problem. You can extend the idea of using ABCs to cater for all options. But the Venn diagram of which event falls under which category is a bit complicated in this case.The 100m, 200m, 100m hurdles and 110m hurdles are all track events affected by wind readings. But the long jump and triple jump are also affected by wind readings but they’re field events. The discus and other throw events are field events—so the longest throw wins—but aren’t affected by high wind readings. And the long jump, triple jump, and the throws have a next-best jump/throw tie-breaking rule. But the pole vault and high jump are field events not affected by the wind but with different tie-breaking rules.Are you still with me? Confused? Can you think of an abstract base class structure to account for all these combinations. It’s not impossible, but you’ll need several layers in the hierarchy.Instead, I’ll explore a different route in a second article, which I’ll publish soon!The follow-up article will be part of The Club  The Python Coding Stack. These are the articles for paid subscribers. So, if you’d like to read about a different way——of merging all these requirements into our classes, The Club .Final Words for This Article • Ready for Part 2?Inheritance is a great tool. And abstract base classes enable you to organise your hierarchies by creating common base classes for concrete classes you may need to define.However, inheritance hierarchies can get quite complex. Since inheritance provides a tight coupling between classes, deep hierarchies can cause a bit of a headache.Still, abstract base classes provide a great tool to make code cleaner, more readable, and more robust. In the follow-up to this article (coming soon), we’ll look at another tool you can use along with inheritance to solve these complex relationships. So join  to carry on reading about the track and field classes and how mixins and composition can help manage the complexity.Code in this article uses Python 3.14The code images used in this article are created using Snappify.For more Python resources, you can also visitReal Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at.Further reading related to this article’s topic:class Athlete:
    def __init__(self, name, bib_number):
        self.name = name
        self.bib_number = bib_number
# ...

list_of_athletes = [
    Athlete(”Carl Lewis”, 259),
    Athlete(”Jesse Owens”, 161),
    Athlete(”Usain Bolt”, 362),
    Athlete(”Mike Powell”, 412),
    Athlete(”Florence Griffith-Joyner”, 263),
    Athlete(”Allyson Felix”, 298),
    Athlete(”David Rudisha”, 177),
    # ...  Add more athletes as needed
]
# ...

bib_to_athlete = {
    athlete.bib_number: athlete for athlete in list_of_athletes
}
# ...

class Event:
    def __init__(self, event_name, participants):
        self.event_name = event_name
        self.participants = participants
        self.results = []
class Result:
    def __init__(self, athlete, performance):
        self.athlete = athlete
        self.performance = performance

# ...
# ...

class Event:
    # ...
        
    def add_result(self, bib_number, performance):
        athlete = bib_to_athlete.get(bib_number)
        if not athlete or athlete not in self.participants:
            raise ValueError(f”Invalid bib number {bib_number}”)
        self.results.append(
            Result(athlete, performance)
        )
# ...

class Event:
    # ...
    
    def finalise_results(self):
        self.results.sort(key=lambda item: item.performance)
# ...

track_100m = Event(
    “100m Sprint”,
    [bib_to_athlete[259], bib_to_athlete[161], bib_to_athlete[362]]
)

track_100m.add_result(259, 9.86)
track_100m.add_result(161, 10.3)
track_100m.add_result(362, 9.58)
track_100m.finalise_results()

for result in track_100m.results:
    print(f”{result.athlete.name}: {result.performance}”)
# ...

field_long_jump = Event(
    “Long Jump”,
    [bib_to_athlete[412], bib_to_athlete[259]]
)

field_long_jump.add_result(412, 8.95)
field_long_jump.add_result(259, 8.87)
field_long_jump.finalise_results()

for result in field_long_jump.results:
    print(f”{result.athlete.name}: {result.performance}”)
from abc import ABC, abstractmethod

class Result:
    def __init__(self, athlete, performance):
        self.athlete = athlete
        self.performance = performance

class Athlete:
    def __init__(self, name, bib_number):
        self.name = name
        self.bib_number = bib_number

class Event(ABC):
    def __init__(self, event_name, participants):
        self.event_name = event_name
        self.participants = participants
        self.results = []

    def add_result(self, bib_number, performance):
        athlete = bib_to_athlete.get(bib_number)
        if not athlete or athlete not in self.participants:
            raise ValueError(f”Invalid bib number {bib_number}”)
        self.results.append(
            Result(athlete, performance)
        )
    
    @abstractmethod
    def finalise_results(self):
        pass

# ...
# ...

class TrackEvent(Event):
    pass

class FieldEvent(Event):
    pass

# ...

track_100m = TrackEvent(
    “100m Sprint”,
    [bib_to_athlete[259], bib_to_athlete[161], bib_to_athlete[362]]
)

# ...

field_long_jump = FieldEvent(
    “Long Jump”,
    [bib_to_athlete[412], bib_to_athlete[259]]
)

# ...
# ...

class TrackEvent(Event):
    def finalise_results(self):
        self.results.sort(key=lambda item: item.performance)

class FieldEvent(Event):
    def finalise_results(self):
        self.results.sort(
            key=lambda item: item.performance,
            reverse=True,
        )

# ...
# ...

track_100m = TrackEvent(
    “100m Sprint”,
    [bib_to_athlete[259], bib_to_athlete[161], bib_to_athlete[362]]
)

track_100m.add_result(259, 9.86)
track_100m.add_result(161, 10.3)
track_100m.add_result(362, 9.58)
track_100m.finalise_results()

print(”100m Sprint Results:”)
for result in track_100m.results:
    print(f”{result.athlete.name}: {result.performance}”)

field_long_jump = FieldEvent(
    “Long Jump”,
    [bib_to_athlete[412], bib_to_athlete[259]]
)

field_long_jump.add_result(412, 8.95)
field_long_jump.add_result(259, 8.87)
field_long_jump.finalise_results()

print(”\nLong Jump Results:”)
for result in field_long_jump.results:
    print(f”{result.athlete.name}: {result.performance}”)
For more Python resources, you can also visitReal Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at.]]></content:encoded></item><item><title>PCI Resizable BAR Improvements Heading To Linux 6.19</title><link>https://www.phoronix.com/news/PCI-ReBAR-Better-Linux-6.19</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 12:35:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Restructuring to the Linux kernel's PCI Resizable BAR "ReBAR" support is set to be submitted for the upcoming Linux 6.19 kernel cycle...]]></content:encoded></item><item><title>Engineering a Rust optimization quiz</title><link>https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 1 Nov 2025 11:08:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.]]></content:encoded></item><item><title>Linux 6.18 Kernel Happenings, Python 3.14, NTFSPLUS &amp; Other October Highlights</title><link>https://www.phoronix.com/news/October-2025-Highlights</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:36:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[During the month of October on Phoronix were 305 original news articles around Linux/open-source and another 21 featured Linux hardware reviews / multi-page featured benchmark articles. There was an exciting mix of software and hardware happenings over the past month. Here is a look back at what excited readers the most...]]></content:encoded></item><item><title>AMD Acknowledges RDSEED Failure On AMD Zen 5 With Software Fix Coming</title><link>https://www.phoronix.com/news/AMD-SB-7055-RDSEED-Zen-5</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:27:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In mid-October a Meta engineer uncovered an RDSEED architectural issue with AMD Zen 5 CPUs. A patch in turn was sent out to the Linux kernel mailing list to disable RDSEED usage on affected Zen 5 processors. AMD this week issued a security bulletin to acknowledge the issue and report that a microcode fix is coming...]]></content:encoded></item><item><title>KDE Plasma 6.6 To Support Intel&apos;s Adaptive Sharpness Feature</title><link>https://www.phoronix.com/news/Plasma-6.6-Adaptive-Sharpness</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:09:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma developers continue to be busy landing more fixes for the recently introduced Plasma 6.5 while also lining up more new features for Plasma 6.6...]]></content:encoded></item><item><title>Falling Panel Prices Lead To Global Solar Boom, Except For the US</title><link>https://hardware.slashdot.org/story/25/10/31/2340238/falling-panel-prices-lead-to-global-solar-boom-except-for-the-us?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader AmiMoJo shares a report from the Financial Times: Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses. But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. "We will not approve wind or farmer destroying [sic] Solar," he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.
 
Thousands of miles away on the other side of the Pacific Ocean, it is a different story. China has laid solar panels across an area the size of Chicago high up on the Tibetan Plateau, where the thin air helps more sunlight get through. The Talatan Solar Park is part of China's push to double its solar and wind generation capacity over the coming decade. "Green and low-carbon transition is the trend of our time," President Xi Jinping told delegates at a UN summit in New York last month. China's vast production of solar panels and batteries has also pushed down the prices of renewables hardware for everyone else, meaning it has "become very difficult to make any other choice in some places," according to Heymi Bahar, senior analyst at the International Energy Agency. [...]
 
More broadly, the US's focus on fossil fuels and pullback of support for clean energy further cedes influence over the future global energy system to China. The US is trying to tie its trading partners into fossil fuels, pressing the EU to buy $750 billion of American oil, natural gas, and nuclear technologies during his presidency as part of a trade deal, scuppering an initiative to begin decarbonizing world shipping and pressuring others to reduce their reliance on Chinese technology. But the collapsing cost of solar panels in particular has spoken for itself in many parts of the world. Experts caution that the US's attacks on renewables could cause lasting damage to its competitiveness against China, even if an administration more favorable to renewables were to follow Trump's.]]></content:encoded></item><item><title>Digest #186: Inside the AWS Outage, Docker Compose in Production, F1 Hacks and 86,000 npm Packages Attacks</title><link>https://www.devopsbulletin.com/p/digest-186-inside-the-aws-outage</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/2p_huDMN8XI" length="" type=""/><pubDate>Sat, 1 Nov 2025 09:32:04 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!A recent 14-hour AWS us-east-1 outage took down 140 services after a DNS race condition in DynamoDB spiraled out of control. Palo Alto’s Unit42 uncovered a cloud-based gift card fraud campaign, and researchers exploited bugs in the FIA portal to access F1 driver data. Meanwhile, npm faced another supply-chain attack, with over 86,000 malicious packages downloaded.Cloudflare detailed how it’s escaping the Linux networking stack, AWS quietly deprecated two dozen services, and Netflix revealed how Tudum supports 20M+ users using CQRS.On the hands-on side: Docker Compose in production, ArgoCD for multi-cluster deployments, detecting bad images in S3 with Rekognition, and TDD with Terraform. Plus, why for some workloads, Postgres can beat Kafka.Tools of the week: WhoDB (chat-based DB explorer), LME (CISA’s free SIEM), Grype (vulnerability scanner), Kanchi (Celery monitor), Bruin (data pipeline), and Nyno (multi-language workflow engine).All this and more in this week’s DevOps Bulletin, don’t miss out! Consider supporting it with a paid subscription. You’ll keep the free Friday issues  get extras like bonus deep-dives, templates, and the full archive.Ever feel like your cloud bill keeps growing, but you’re not sure  the money’s going? Start with an .Listing all your resources — EC2 instances, S3 buckets, Lambdas, and more — often reveals idle or forgotten assets quietly adding to your bill. You can script it yourself with the AWS CLI or use tools like AWS Config or CloudQuery for a more automated setup.If you want more hands-on tips like this, check out my latest book, “.A lightweight  - Postgres, MySQL, SQLite, MongoDB, Redis, MariaDB, Elastic Search, and Clickhouse with Chat interface. is a no-cost, open-source platform that centralizes log collection, enhances threat detection, and enables real-time alerting. is a vulnerability scanner for container images and filesystems. is a real-time Celery task monitoring (and management) system with an enjoyable user interface. is a data pipeline tool that brings together data ingestion, data transformation with SQL & Python, and data quality into a single framework. is an open-source multi-language workflow engine that lets you build, extend, and connect automation in the languages you already know. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email. ]]></content:encoded></item><item><title>You can&apos;t refuse to be scanned by ICE&apos;s facial recognition app, DHS document say</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>nh43215rgb</author><category>hn</category><pubDate>Sat, 1 Nov 2025 08:58:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.]]></content:encoded></item><item><title>Talk Python to Me: #526: Building Data Science with Foundation LLM Models</title><link>https://talkpython.fm/episodes/show/526/building-data-science-with-foundation-llm-models</link><author></author><category>dev</category><category>python</category><pubDate>Sat, 1 Nov 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Today, we’re talking about building real AI products with foundation models. Not toy demos, not vibes. We’ll get into the boring dashboards that save launches, evals that change your mind, and the shift from analyst to AI app builder. Our guide is Hugo Bowne-Anderson, educator, podcaster, and data scientist, who’s been in the trenches from scalable Python to LLM apps. If you care about shipping LLM features without burning the house down, stick around.<br/>
<br/>
<strong>Episode sponsors</strong><br/>
<br/>
<a href='https://talkpython.fm/ppm'>Posit</a><br>
<a href='https://talkpython.fm/nordstellar'>NordStellar</a><br>
<a href='https://talkpython.fm/training'>Talk Python Courses</a><br/>
<br/>
<h2 class="links-heading mb-4">Links from the show</h2>
<div><strong>Hugo Bowne-Anderson</strong>: <a href="https://x.com/hugobowne?featured_on=talkpython" target="_blank" >x.com</a><br/>
<strong>Vanishing Gradients Podcast</strong>: <a href="https://vanishinggradients.fireside.fm?featured_on=talkpython" target="_blank" >vanishinggradients.fireside.fm</a><br/>
<strong>Fundamentals of Dask: High Performance Data Science Course</strong>: <a href="https://training.talkpython.fm/courses/fundamentals-of-dask-getting-up-to-speed" target="_blank" >training.talkpython.fm</a><br/>
<strong>Building LLM Applications for Data Scientists and Software Engineers</strong>: <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=friendsoftalkpython" target="_blank" >maven.com</a><br/>
<strong>marimo: a next-generation Python notebook</strong>: <a href="https://marimo.io?featured_on=talkpython" target="_blank" >marimo.io</a><br/>
<strong>DevDocs (Offline aggregated docs)</strong>: <a href="https://devdocs.io?featured_on=talkpython" target="_blank" >devdocs.io</a><br/>
<strong>Elgato Stream Deck</strong>: <a href="https://www.elgato.com/us/en/p/stream-deck?featured_on=talkpython" target="_blank" >elgato.com</a><br/>
<strong>Sentry's Seer</strong>: <a href="https://talkpython.fm/seer" target="_blank" >talkpython.fm</a><br/>
<strong>The End of Programming as We Know It</strong>: <a href="https://www.oreilly.com/radar/the-end-of-programming-as-we-know-it/?featured_on=talkpython" target="_blank" >oreilly.com</a><br/>
<strong>LorikeetCX AI Concierge</strong>: <a href="https://www.lorikeetcx.ai?featured_on=talkpython" target="_blank" >lorikeetcx.ai</a><br/>
<strong>Text to SQL & AI Query Generator</strong>: <a href="https://www.text2sql.ai?featured_on=talkpython" target="_blank" >text2sql.ai</a><br/>
<strong>Inverse relationship enthusiasm for AI and traditional projects</strong>: <a href="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/04/LLM-SDLC_Fig1_edit3-1.png?featured_on=talkpython" target="_blank" >oreilly.com</a><br/>
<br/>
<strong>Watch this episode on YouTube</strong>: <a href="https://www.youtube.com/watch?v=_LFdKjsKdPE" target="_blank" >youtube.com</a><br/>
<strong>Episode #526 deep-dive</strong>: <a href="https://talkpython.fm/episodes/show/526/building-data-science-with-foundation-llm-models#takeaways-anchor" target="_blank" >talkpython.fm/526</a><br/>
<strong>Episode transcripts</strong>: <a href="https://talkpython.fm/episodes/transcript/526/building-data-science-with-foundation-llm-models" target="_blank" >talkpython.fm</a><br/>
<br/>
<strong>Theme Song: Developer Rap</strong><br/>
<strong>🥁 Served in a Flask 🎸</strong>: <a href="https://talkpython.fm/flasksong" target="_blank" >talkpython.fm/flasksong</a><br/>
<br/>
<strong>---==  Don't be a stranger  ==---</strong><br/>
<strong>YouTube</strong>: <a href="https://talkpython.fm/youtube" target="_blank" ><i class="fa-brands fa-youtube"></i> youtube.com/@talkpython</a><br/>
<br/>
<strong>Bluesky</strong>: <a href="https://bsky.app/profile/talkpython.fm" target="_blank" >@talkpython.fm</a><br/>
<strong>Mastodon</strong>: <a href="https://fosstodon.org/web/@talkpython" target="_blank" ><i class="fa-brands fa-mastodon"></i> @talkpython@fosstodon.org</a><br/>
<strong>X.com</strong>: <a href="https://x.com/talkpython" target="_blank" ><i class="fa-brands fa-twitter"></i> @talkpython</a><br/>
<br/>
<strong>Michael on Bluesky</strong>: <a href="https://bsky.app/profile/mkennedy.codes?featured_on=talkpython" target="_blank" >@mkennedy.codes</a><br/>
<strong>Michael on Mastodon</strong>: <a href="https://fosstodon.org/web/@mkennedy" target="_blank" ><i class="fa-brands fa-mastodon"></i> @mkennedy@fosstodon.org</a><br/>
<strong>Michael on X.com</strong>: <a href="https://x.com/mkennedy?featured_on=talkpython" target="_blank" ><i class="fa-brands fa-twitter"></i> @mkennedy</a><br/></div>]]></content:encoded></item><item><title>#526: Building Data Science with Foundation LLM Models</title><link>https://talkpython.fm/episodes/show/526/building-data-science-with-foundation-llm-models</link><author></author><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/526/building-data-science-with-foundation-llm-models.mp3" length="" type=""/><pubDate>Sat, 1 Nov 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>rkta</author><category>hn</category><pubDate>Sat, 1 Nov 2025 07:31:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Tryton News: Newsletter November 2025</title><link>https://discuss.tryton.org/t/newsletter-november-2025/8920</link><author></author><category>dev</category><category>python</category><pubDate>Sat, 1 Nov 2025 07:00:38 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Usually we would release the new Tryton version 7.8 in November, but this time we postpone the release. In the last month we focused on fixing bugs, improving the behaviour of things, speeding-up performance issues - building on the changes from our last release. We also added some new features which we would like to introduce to you in this newsletter.Sales, Purchases and ProjectsAccounting, Invoicing and PaymentsSystem Data and Configuration
Please update your systems to take care of a security related bug we found last month: 

  Changes for Implementers and Developers]]></content:encoded></item><item><title>SpaceX Set To Win $2 Billion Pentagon Satellite Deal</title><link>https://tech.slashdot.org/story/25/10/31/2347207/spacex-set-to-win-2-billion-pentagon-satellite-deal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to the Wall Street Journal, SpaceX is reportedly poised to secure a $2 billion Pentagon contract to develop hundreds of missile-tracking satellites for President Trump's ambitious Golden Dome defense system. The Independent reports: The planned "air moving target indicator" system in question could ultimately feature as many as 600 satellites once it is fully operational, The Wall Street Journal reports. Musk's company has also been linked to two more satellite ventures, which are concerned with relaying sensitive communications and tracing vehicles, respectively.
 
Golden Dome, inspired by Israel's "Iron Dome," was announced by Trump and Secretary of War Pete Hegseth at the White House in May and will amount to a complex system of satellites and weaponry capable of destroying incoming missiles before they hit American targets. The president promised it would be "fully operational" before he leaves office in January 2029, capable of intercepting rockets, "even if they are launched from space," with an overall price tag of $175 billion.]]></content:encoded></item><item><title>The TechBeat: From Cloud to Desk: 3 Signs the AI Revolution is Going Local (11/1/2025)</title><link>https://hackernoon.com/11-1-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 1 Nov 2025 06:10:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker-Antho [ 4 Min read ] 
 New research shatters AI security assumptions, showing that poisoning large models is easier than believed and requires a very small number of documents. Read More.By @socialdiscoverygroup [ 6 Min read ] 
 Discover how React 19's new hooks—useActionState, useFormStatus, and useOptimistic—simplify form handling with less boilerplate and cleaner code.  Read More.By @mayukhsuri [ 3 Min read ] 
 AWS outage on Oct 20, 2025, disrupted major apps worldwide. Learn what caused it, how it spread, and key lessons to build stronger cloud systems. Read More.By @filestack [ 6 Min read ] 
 Stop babysitting profile pictures. Learn how Filestack Workflows turn image uploads into scalable, async, and lightning-fast experiences. Read More.By @nownodes [ 4 Min read ] 
 Blast API ends operations in Oct 2025. Explore the best developer alternatives like NOWNodes and Alchemy for secure, scalable RPC migration. Read More.By @mend [ 4 Min read ] 
 Traditional testing breaks with AI. Learn how red teaming and AI-powered fuzzing uncover hidden weaknesses in large language models. Read More.By @knightbat2040 [ 5 Min read ] 
 What started as a simple script evolved into a full-fledged data engineering and NLP pipeline that can process a decade's worth of legal decisions in minutes. Read More.By @hackmarketing [ 7 Min read ] 
 Learn how Web3 projects can grow sustainably through education, trust, and human-centered marketing that builds real users and community. Read More.By @botbeat [ 8 Min read ] 
 A deep dive into the 30 companies that burned over one trillion OpenAI tokens—featuring Duolingo, OpenRouter, and Indeed as top power users of GPT tech. Read More.By @melvin-manni [ 5 Min read ] 
 Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  Read More.By @giovannicoletta [ 11 Min read ] 
 An interrogation of how physics concepts like black holes, entropy, and quantum theory mirror the rise and limits of artificial intelligence. Read More.By @ichebykin [ 5 Min read ] 
 Context engineering for coding agents is the best way to improve the model performance for code generation.  Read More.By @mcsee [ 3 Min read ] 
 Avoid Boolean variables, they lead to conditional logic and force you to write Ifs. Create polymorphic states instead Read More.By @sanjaybarot [ 23 Min read ] 
 Ransomware has gone cloud-native: no payloads, just API abuse. Learn the tactics—IAM takeovers, KMS locks, backup sabotage—and how to build resilience. Read More.By @ainativedev [ 4 Min read ] 
 GitHub Copilot evolves: cloud-based agents now handle PRs, iterate from feedback, and fit seamlessly into dev workflows. Read More.By @aifundingtracker [ 13 Min read ] 
 AI startups raised over $3.6 billion this week across infrastructure, wearable AI, enterprise automation, and fintech innovation. Read More.]]></content:encoded></item><item><title>The Numbers Show Xbox&apos;s Current Plan Isn&apos;t Working</title><link>https://games.slashdot.org/story/25/10/31/2332211/the-numbers-show-xboxs-current-plan-isnt-working?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Gizmodo: It's time for Xbox to eat some humble pie and perform some real soul-searching. Microsoft released its latest quarterly earnings report and proved the worst of our fears about its gaming brand. Not only are Xbox hardware sales down significantly, but the brand itself is barely treading water. Gamers are voicing their displeasure with their wallets, but Microsoft's top brass is still only thinking about the margins. Microsoft was more keen to promote the scale of its cloud and AI services revenue -- which was up 28% year over year -- than talk about its beleaguered gaming brand. The company's overall gaming revenue fell by 2% compared to the same time last year. This was precipitated by a "decline in Xbox hardware," which was down by 22% following a steady decline quarter after quarter. Its first-party games and its Game Pass subscription were doing better, though the overall growth was only up by 1%, and even that was driven by the "better-than-expected performance" of third-party games. You can give credit to titles like Clair Obscur: Expedition 33 for why Xbox isn't in an even deeper hole than it is now.
 
The tech giant has no expectation that its Xbox brand will start making more money anytime soon. In its earnings call with investors, Microsoft Chief Financial Officer Amy Hood said the company expects Xbox will continue to decline "in the low to mid-single digits" for the following quarter. That's mostly due to the lack of landmark first-party titles. Just this month, Xbox released Ninja Gaiden 4, The Outer Worlds 2, and Double Fine's The Keeper. Xbox also made a huge marketing push for its first handheld, made in partnership with Asus, the ROG Xbox Ally and Ally X. In any other year, this would be a big month for any gaming company. The dour outlook comes after months of bad news. After two subsequent price hikes, Xbox Series S and Series X consoles now cost between $100 to $150 more than they did at launch five years ago. Microsoft also pushed prices of its Game Pass Ultimate subscription tier from $20 to $30 per month. A full-year's subscription would now demand $360. In a separate article, Gizmodo reviews Microsoft's new ROG Xbox Ally X handheld, which "offers a better experience overall" than the "other small-scale Windows PC gaming devices released this year." However, "it's still nowhere close to what you truly want from a console."]]></content:encoded></item><item><title>The profitable startup</title><link>https://linear.app/now/the-profitable-startup</link><author>doppp</author><category>hn</category><pubDate>Sat, 1 Nov 2025 03:18:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>When AI And Secure Chat Meet, Users Deserve Strong Controls Over How They Interact</title><link>https://www.techdirt.com/2025/10/31/when-ai-and-secure-chat-meet-users-deserve-strong-controls-over-how-they-interact/</link><author>Thorin Klosowski</author><category>tech</category><pubDate>Sat, 1 Nov 2025 02:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Both Google and Apple are cramming new AI features into their phones and other devices, and neither company has offered clear ways to control which apps those AI systems can access. Recent issues around WhatsApp on both Android and iPhone demonstrate how these interactions can go sideways, risking revealing chat conversations beyond what you intend. Users deserve better controls and clearer documentation around what these AI features can access.After diving into how Google Gemini and Apple Intelligence (and in some cases Siri) currently work, we didn’t always find clear answers to questions about how data is stored, who has access, and what it can be used for.At a high level, when you compose a message with these tools, the companies can usually see the contents of those messages and receive at least a temporary copy of the text on their servers.When receiving messages, things get trickier. When you use an AI like Gemini or a feature like Apple Intelligence to summarize or read notifications, we believe companies should be doing that content processing on-device. But poor documentation and weak guardrails create issues that have lead us deep into documentation rabbit holes and still fail to clarify the privacy practices as clearly as we’d like.We’ll dig into the specifics below as well as potential solutions we’d like to see Apple, Google, and other device-makers implement, but first things first, here’s what you can do right now to control access:Control AI Access to Secure Chat on Android and iOSHere are some steps you can take to control access if you want nothing to do with the device-level AI features’ integration and don’t want to risk accidentally sharing the text of a message outside of the app you’re using.How to Check and Limit What Gemini Can AccessIf you’re using Gemini on your Android phone, it’s a good time to review your settings to ensure things are set up how you want. Here’s how to check each of the relevant settings:Disable Gemini App Activity: Gemini App Activity is a history Google stores of all your interactions with Gemini. It’s enabled by default. To disable it, open Gemini (depending on your phone model, you may or may not even have the Google Gemini app installed. If you don’t have it installed, you don’t really need to worry about any of this). Tap your  > then change the toggle to either “Turn off,” or “Turn off and delete activity” if you want to delete previous conversations. If the option reads “Turn on,” then Gemini Apps Activity is already turned off. Control app and notification access: You can control which apps Gemini can access by tapping your  > , then scrolling down and disabling the toggle next to any apps you do not want Gemini to access. If you do not want Gemini to potentially access the content that appears in notifications, open the Settings app and revoke notification access from the Google app.: Depending on your phone model, you might be able to delete the Gemini app and revert to using Google Assistant instead. You can do so by long-pressing the Gemini app and selecting the option to delete. How to Check and Limit what Apple Intelligence and Siri Can AccessSimilarly, there are a few things you can do to clamp down on what Apple Intelligence and Siri can do: Disable the “Use with Siri Requests” option: If you want to continue using Siri, but don’t want to accidentally use it to send messages through secure messaging apps, like WhatsApp, then you can disable that feature by opening  >  > , and disabling “Use with Siri Requests,” which turns off the ability to compose messages with Siri and send them through that app.Disable Apple Intelligence entirely: Apple Intelligence is an all-or-nothing setting on iPhones, so if you want to avoid any potential issues your only option is to turn it off completely. To do so, open  > Apple Intelligence & Siri, and disable “Apple Intelligence” (you will only see this option if your device supports Apple Intelligence, if it doesn’t, the menu will only be for “Siri”). You can also disable certain features, like “writing tools,” using Screen Time restrictions. Siri can’t be universally turned off in the same way, though you can turn off the options under “Talk to Siri” to make it so you can’t speak to it. For more information about cutting off AI access at different levels in other apps, this Consumer Reports article covers other platforms and services.Sending Messages Has Different Privacy Concerns than Receiving ThemLet’s start with a look at how Google and Apple integrate their AI systems into message composition, using WhatsApp as an example.Google Gemini and WhatsAppOn Android, you can optionally link WhatsApp and Gemini together so you can then initiate various actions for sending messages from the Gemini app, like “Call Mom on WhatsApp” or “Text Jason on WhatsApp that we need to cancel our secret meeting, but make it a haiku.” This feature raised red flags for users concerned about privacy.By default, everything you do in Gemini is stored in the “Gemini Apps Activity,” where messages are stored forever, subject to human review, and are used to train Google’s products. So, unless you change it, when you use Gemini to compose and send a message in WhatsApp then the message you composed is visible to Google.If you turn the activity off, interactions are still stored for 72 hours. Google’s documentation claims that even though messages are stored, those conversations aren’t reviewed or used to improve Google machine learning technologies, though that appears to be an internal policy choice with no technical limits preventing Google from accessing those messages.The simplicity of invoking Gemini to compose and send a message may lead to a false sense of privacy. Notably, other secure messaging apps, like Signal, do not offer this Gemini integration.For comparison’s sake, let’s see how this works with Apple devices.According to its privacy policy, when you dictate a message through Siri to send to WhatsApp (or anywhere else), the message, including metadata like the recipient phone number and other identifiers, is sent to Apple’s servers. This was confirmed by researchers to include the text of messages sent to WhatsApp. When you use Siri to compose a WhatsApp message, the message gets routed to both Apple and WhatsApp. Apple claims it does not store this transcript unless you’ve opted into “Improve Siri and Dictation.” WhatsApp defers to Apple’s support for data handling concerns. This is similar to how Google handles speech-to-text prompts.In response to that research, Apple said this was expected behavior with an app that uses SiriKit—the extension that allows third-party apps to integrate with Siri—like WhatsApp does.Both Siri and Apple Intelligence can sometimes run locally on-device, and other times need to rely on Apple-managed cloud servers to complete requests. Apple Intelligence can use the company’s Private Cloud Compute, but Siri doesn’t have a similar feature.The ambiguity around where data goes makes it overly difficult to decide on whether you are comfortable with the sort of privacy trade-off that using features like Siri or Apple Intelligence might entail.How Receiving Messages WorksSending encrypted messages is just one half of the privacy puzzle. What happens on the receiving end matters too. We could not find anything in Google’s Utilities documentation that clarifies what information is collected, stored, or sent to Google from these notifications. When we reached out to Google, the company responded that it “builds technical data protections that safeguard user data, uses data responsibly, and provides users with tools to control their Gemini experience.” Which means Google has no technical limitation around accessing the text from notifications if you’ve enabled the feature in the Utilities app. This could open up any notifications routed through the Utilities app to the Gemini app to be accessed internally or from third-parties. Google needs to publicly make its data handling explicit in its documentation.Apple is more clear about how it handles this sort of notification access.Siri can read and reply to messages with the “Announce Notifications” feature. With this enabled, Siri can read notifications out loud on select headphones or via CarPlay. In a press release, Apple states, “When a user talks or types to Siri, their request is processed on device whenever possible. For example, when a user asks Siri to read unread messages… the processing is done on the user’s device. The contents of the messages aren’t transmitted to Apple servers, because that isn’t necessary to fulfill the request.”Apple Intelligence can summarize notifications from any app that you’ve enabled notifications on. Apple is clear that these summaries are generated on your device, “when Apple Intelligence provides you with preview summaries of your emails, messages, and notifications, these summaries are generated by on-device models.” This means there should be no risk that the text of notifications from apps like WhatsApp or Signal get sent to Apple’s servers just to summarize them.New AI Features Must Come With Strong User ControlsAs more device-makers cram AI features into their devices, the more necessary it is for us to have clear and simple controls over what personal data these features can access on our devices. If users do not have control over when a text leaves a device for any sort of AI processing—whether that’s to a “private” cloud or not—it erodes our privacy and potentially threatens the foundations of end-to-end encrypted communications.Google, Apple, and other device makers should add a device-level AI permission, just like they do for other potentially invasive privacy features, like location sharing, to their phones. You should be able to tell the operating system’s AI to not access an app, even if that comes at the “cost” of missing out on some features. The setting should be straightforward and easy to understand in ways the Gemini an Apple Intelligence controls currently are not.Offer On-Device-Only ModesDevice-makers should offer an “on-device only” mode for those interested in using some features without having to try to figure out what happens on device or on the cloud. Samsung offers this, and both Google and Apple would benefit from a similar option.Both Google and Apple should improve their documentation about how these features interact with various apps. Apple doesn’t seem to clarify notification processing privacy anywhere outside of a press release, and we couldn’t find anything about Google’s Utilities privacy at all. We appreciate tools like Gemini Apps Activity as a way to audit what the company collects, but vague information like “Prompted a Communications query” is only useful if there’s an explanation somewhere about what that means.The current user options are not enough. It’s clear that the AI features device-makers add come with significant confusion about their privacy implications, and it’s time to push back and demand better controls. The privacy problems introduced alongside new AI features should be taken seriously, and remedies should be offered to both users and developers who want real, transparent safeguards over how a company accesses their private data and communications.]]></content:encoded></item><item><title>OpenAI Launches Aardvark To Detect and Patch Hidden Bugs In Code</title><link>https://it.slashdot.org/story/25/10/31/2314223/openai-launches-aardvark-to-detect-and-patch-hidden-bugs-in-code?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 02:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[OpenAI has introduced Aardvark, a GPT-5-powered autonomous agent that scans, reasons about, and patches code like a human security researcher. "By embedding itself directly into the development pipeline, Aardvark aims to turn security from a post-development concern into a continuous safeguard that evolves with the software itself," reports InfoWorld. From the report: What makes Aardvark unique, OpenAI noted, is its combination of reasoning, automation, and verification. Rather than simply highlighting potential vulnerabilities, the agent promises multi-stage analysis -- starting by mapping an entire repository and building a contextual threat model around it. From there, it continuously monitors new commits, checking whether each change introduces risk or violates existing security patterns.
 
Additionally, upon identifying a potential issue, Aardvark attempts to validate the exploitability of the finding in a sandboxed environment before flagging it. This validation step could prove transformative. Traditional static analysis tools often overwhelm developers with false alarms -- issues that may look risky but aren't truly exploitable. "The biggest advantage is that it will reduce false positives significantly," noted Jain. "It's helpful in open source codes and as part of the development pipeline."
 
Once a vulnerability is confirmed, Aardvark integrates with Codex to propose a patch, then re-analyzes the fix to ensure it doesn't introduce new problems. OpenAI claims that in benchmark tests, the system identified 92 percent of known and synthetically introduced vulnerabilities across test repositories, a promising indication that AI may soon shoulder part of the burden of modern code auditing.]]></content:encoded></item><item><title>FCC To Rescind Ruling That Said ISPs Are Required To Secure Their Networks</title><link>https://it.slashdot.org/story/25/10/31/237241/fcc-to-rescind-ruling-that-said-isps-are-required-to-secure-their-networks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 01:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The FCC plans to repeal a Biden-era ruling that required ISPs to secure their networks under the Communications Assistance for Law Enforcement Act, instead relying on voluntary cybersecurity commitments from telecom providers. FCC Chairman Brendan Carr said the ruling "exceeded the agency's authority and did not present an effective or agile response to the relevant cybersecurity threats." Carr said the vote scheduled for November 20 comes after "extensive FCC engagement with carriers" who have taken "substantial steps... to strengthen their cybersecurity defenses." Ars Technica reports: The FCC's January 2025 declaratory ruling came in response to attacks by China, including the Salt Typhoon infiltration of major telecom providers such as Verizon and AT&T. The Biden-era FCC found that the Communications Assistance for Law Enforcement Act (CALEA), a 1994 law, "affirmatively requires telecommunications carriers to secure their networks from unlawful access or interception of communications."
 
"The Commission has previously found that section 105 of CALEA creates an affirmative obligation for a telecommunications carrier to avoid the risk that suppliers of untrusted equipment will "illegally activate interceptions or other forms of surveillance within the carrier's switching premises without its knowledge,'" the January order said. "With this Declaratory Ruling, we clarify that telecommunications carriers' duties under section 105 of CALEA extend not only to the equipment they choose to use in their networks, but also to how they manage their networks." A draft of the order that will be voted on in November can be found here (PDF).]]></content:encoded></item><item><title>What is Bending Spoons? Everything to know about AOL’s acquirer</title><link>https://techcrunch.com/2025/10/31/what-is-bending-spoons-everything-to-know-about-aols-acquirer/</link><author>Anna Heim</author><category>tech</category><pubDate>Sat, 1 Nov 2025 01:11:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bending Spoons remains largely unknown, even as its portfolio of products has served more than a billion people. ]]></content:encoded></item><item><title>Bluesky Hits 40 Million Users, Introduces &apos;Dislikes&apos; Beta</title><link>https://tech.slashdot.org/story/25/10/31/231232/bluesky-hits-40-million-users-introduces-dislikes-beta?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bluesky has surpassed 40 million users and is launching a "dislikes" beta to improve its personalization algorithms and reduce toxic content. TechCrunch reports: With the "dislikes" beta rolling out soon, Bluesky will take into account the new signal to improve user personalization. As users "dislike" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.
 
The company explained the changes are designed to make Bluesky a place for more "fun, genuine, and respectful exchanges" -- an edict that follows a month of unrest on the platform as some users again criticized the platform over its moderation decisions. While Bluesky is designed as a decentralized network where users run their own moderation, some subset of Bluesky users want the platform itself to ban bad actors and controversial figures instead of leaving it up to the users to block them. Bluesky, however, wants to focus more on the tools it provides users to control their own experience.]]></content:encoded></item><item><title>Wine 10.18 Released With More WoW64 Mode Improvements</title><link>https://www.phoronix.com/news/Wine-10.18-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:35:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Wine 10.18 is now available for capping off the month of October and working toward the code freeze for Wine 11.0 beginning in early December...]]></content:encoded></item><item><title>How L.A. Scores “Vulnerability” of Unhoused People Is Changing: What You Need to Know</title><link>https://hackernoon.com/how-la-scores-vulnerability-of-unhoused-people-is-changing-what-you-need-to-know?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:23:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Welcome to The Markup, where we use investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
One year after a Markup investigation revealed racial bias in Los Angeles’s housing intake system for people experiencing homelessness, local politicians have pressed for reforms and the agency responsible for housing is taking steps to make its approach more equitable and effective.\
Shortly after our original investigation published, Los Angeles City Council Member Nithya Raman, who chairs the Housing and Homelessness committee, introduced a motion citing the article and calling on the Los Angeles Homeless Services Authority (LAHSA) to come up with a plan to reform its intake system. The legislation, approved unanimously, called specifically for greater fairness in the “vulnerability” scoring system that The Markup analyzed. Used by Los Angeles for the past decade, the system rated Black people as significantly less vulnerable than White people year after year, making them less likely to obtain subsidized permanent housing.\
Black people are hugely overrepresented among unhoused people in L.A., making up about 9 percent of Los Angeles County’s population but about 30 percent of the county’s people experiencing homelessness.\
“To see that the tool that we’re using to put people in line for housing was not actually housing unhoused Black Angelenos as quickly as we could was really surprising to me,” said Raman, who read the article in the Los Angeles Times, where it was co-published. Raman, who is currently running for re-election in District Four, in central LA, said the investigation “absolutely” spurred the council to act.\
LAHSA, given a deadline of April 2023 in the legislation, still has not provided a reform plan. A spokesperson for the agency didn’t directly respond to a request for comment about the plan.\
Raman said LAHSA has taken some steps in the past year to improve how it allocates housing. Among other changes, she said, the agency has started to prioritize some groups, including those already involved in housing programs and those who already have the documents required to move into a building, like an ID and social security number.\
Meanwhile, the agency also de-emphasized the score’s importance in placing people for permanent housing. People applying for housing are scored on a 17-point scale. Previously, the people with the highest scores were given the highest priority, but now any person who scores an eight or above can be prioritized, depending on the other factors being considered.\
Still, equity in the housing system remains a known problem. In November, researchers from the University of Southern California and the University of California Los Angeles, working in partnership with LAHSA, released a long-awaited study on racial bias in the system and ways to reform the scoring system, known as the VI-SPDAT.\
The study, which analyzed scores across race and ethnicity, tracked with The Markup’s findings from earlier in the year, concluding that the scoring tool is biased toward White people and that it’s ineffective overall. The study, in some respects, went even further. Using data on who ultimately faced an “adverse” event, like jail or death, the researchers concluded that tool was “not much more accurate than a random guess at predicting vulnerability.”\
The study suggested several ways the scoring system could become more accurate and equitable, some of which matched The Markup’s reporting. The scoring system asks intensely personal questions about a person’s life, including around issues like violence and substance abuse, and the report recommends rewording and reframing questions to make the survey less complex and more sensitive. A revised version of the system with new questions and scoring could substantially reduce bias, the researchers conclude.\
For example, the study suggests that the existing question about whether anyone has “forced you or tricked you to do things that you do not want to do” should be amended to stress that answering yes “will not result in punishment or any negative consequences.” Another question currently asks, “Are there any medications like painkillers that you don’t take the way the doctor prescribed or where you sell the medication?” The study suggested softening it to, “Do you have medication that you choose to sell instead of taking to help support yourself financially? Answering yes to this question will not result in punishment or negative consequences for you.” Several questions were suggested for removal entirely.\
In a written statement, LAHSA spokesperson Christopher Yee acknowledged that it’s long been clear that “the VI-SPDAT has shortcomings related to equity,” adding that the survey is “long, cumbersome, and not trauma-informed in the content of the questions or administration process.”\
Yee highlighted the study on recommended changes to the system and said the agency is “working with key partners and stakeholders to create a plan to implement and refine” a new iteration of the scoring system while it continues to use the old version.\
The agency, he noted, has already dropped a requirement to score people for interim housing entry or time-limited subsidy programs, but will still require scoring for permanent housing. LAHSA must use some sort of prioritization system to access certain federal housing funds under rules established by the U.S. Department of Housing and Urban development.\
The planned changes to the scoring system will first apply to screening for adults, and later the agency plans to explore changes to related tools for young people and families with children. The Markup found that racial disparities were even more stark for a variation of the VI-SPDAT used in Los Angeles for people under the age of 25.\
Yee’s statement did not provide a timeline for the revised tool’s launch, but in a FAQ released alongside the study LAHSA said service providers could expect more information early this year on changes to the intake process, known as the Coordinated Entry System.\
Raman, for her part, said she’s withholding judgment until data can show how those changes affect who is housed. But, she said, “there’s no question in my mind that CES needs reform.”]]></content:encoded></item><item><title>Austria&apos;s Ministry of Economy Has Migrated To a Nextcloud Platform In Shift Away From US Tech</title><link>https://yro.slashdot.org/story/25/10/31/2023230/austrias-ministry-of-economy-has-migrated-to-a-nextcloud-platform-in-shift-away-from-us-tech?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from ZDNet: Even before Azure had a global failure this week, Austria's Ministry of Economy had taken a decisive step toward digital sovereignty. The Ministry achieved this status by migrating 1,200 employees to a Nextcloud-based cloud and collaboration platform hosted on Austrian-based infrastructure. This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers.
 
European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."
 
Austria's move and motivation echo similar efforts in Germany, Denmark, and other EU states and agencies. The organizations include the German state of Schleswig-Holstein, which abandoned Exchange and Outlook for open-source programs. Other agencies that have taken the same path away from Microsoft include the Austrian military, Danish government organizations, and the French city of Lyon. All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance.]]></content:encoded></item><item><title>YouTube TV Loses ESPN, ABC and Other Disney Channels</title><link>https://entertainment.slashdot.org/story/25/10/31/2017209/youtube-tv-loses-espn-abc-and-other-disney-channels?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 23:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Disney's channels, including ESPN, ABC, FX, and NatGeo, have gone dark on YouTube TV after Google and Disney failed to renew their carriage agreement before the October 30 deadline, with each side blaming the other for using unfair negotiating tactics and price hikes. YouTube TV says it will issue a $20 credit to subscribers if the blackout continues while negotiations proceed. Engadget reports: "Last week Disney used the threat of a blackout on YouTube TV as a negotiating tactic to force deal terms that would raise prices on our customers," YouTube said in an announcement on its blog. "They're now following through on that threat, suspending their content on YouTube TV." YouTube added that Disney's decision harms its subscribers while benefiting its own live TV products, such as Hulu+Live TV and Fubo.
 
In a statement sent to the Los Angeles Times, however, Disney accused Google's YouTube TV of choosing to deny "subscribers the content they value most by refusing to pay fair rates for [its] channels, including ESPN and ABC." Disney also accused Google of using its market dominance to "eliminate competition and undercut the industry-standard terms" that other pay-TV distributors have agreed to pay for its content.]]></content:encoded></item><item><title>Show HN: Strange Attractors</title><link>https://blog.shashanktomar.com/posts/strange-attractors</link><author>shashanktomar</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 23:23:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.The Basics: Dynamical Systems and Chaos TheoryDynamical Systems are a mathematical way to understand how things . Imagine you have a system, which
could be anything from the movement of planets to the growth of a population. In this system, there are rules that
determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is
happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a
population (we are all too familiar with this one), and stock market.There are two primary things to understand about this system:: This is like a big collection of all the possible states the system can be in. Each state is like a
snapshot of the system at a specific time. This is also called the  or the .: These are the rules that takes one state of the system and moves it to the next state. It can be
represented as a function that transforms the system from now to later.For instance, when studying population growth, a phase-space (world-state) might consist of the current population size
and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population
dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate
the changes in population size over time.Another way of saying this is that the dynamical systems describe how things change over time, in a space of
possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,
study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in
relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.There are different ways of classifying dynamical systems, and one of the most interesting is the classification into
chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to
chaotic systems which exhibit randomness and unpredictability. is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional
deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop
of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,
pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow
deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,
might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of
the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex
systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information
to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially
observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a
completely different outcome, a phenomenon known as the . In this post, we will try to see the
butterfly effect in action but before that, let's talk about .To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems
are all about . During this change, the system moves through different possible states (remember the
phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,
towards which it is . It's like a magnet that pulls the system towards it.For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the
bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of
the reasons why different states get attracted towards attractors:: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay
there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This
loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is
what happens in the case of the pendulum.: In some regions of the phase space, the system's dynamics cause trajectories to converge. This
contraction effect means that nearby states will tend to come closer together over time, eventually being drawn
towards the attractor.Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These
nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These
complex attractors are categorised as . They are called "strange" due to their unique
characteristics.: Strange attractors often have a fractal-like structure, meaning they display intricate
patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial
conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known
as the "butterfly effect".Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting
non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by
deterministic rules.Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order.
Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the
butterfly effect.A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small
changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the
. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to
drastically different outcomes.In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.There is another way of observing the butterfly effect in this visualisation. Change the  from  to
 in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a
technique called . This method handles iterative updates of particle systems directly on the GPU,
minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One
stores the current state of particles and render them on the screen, while the other calculates the next state.Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,  and , to hold the current and
next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use
of GPU resources.Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to
each particle. Following is the attractor function which update the particle positions based on the attractor equation.Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's
equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously
inactive buffer becomes active, and vice versa.This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The  discussion can be found here.]]></content:encoded></item><item><title>S.A.R.C.A.S.M: Slightly Annoying Rubik&apos;s Cube Automatic Solving Machine</title><link>https://github.com/vindar/SARCASM</link><author>chris_overseas</author><category>hn</category><pubDate>Fri, 31 Oct 2025 23:03:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Most Anticipated BNB Launch of 2025: $BALZ Brings The Meme Migration Home</title><link>https://hackernoon.com/the-most-anticipated-bnb-launch-of-2025-$balz-brings-the-meme-migration-home?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:52:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Singapore, October 31st, 2025/Chainwire/--The Binance Smart Chain (BNB) network has seen renewed activity, and BALZ has emerged as one of its notable community movements, with over 40,000 active members before launch on X (@). Observers regard it as one of the more anticipated community-driven launches of the year, comparable to projects such as Aster and Four.meme.Raising over $2 million within days of opening, BALZ has positioned itself as a significant project developing on BNB, despite its informal branding and memetic culture. With more than 40,000 members prior to its anticipated token presale, the project has adopted an unconventional approach to community growth through guerrilla marketing and its "rug pull recovery protocol."Instead of allocating capital to influencer campaigns, the team integrated communities from Solana and Base, migrating them to BNB through its protocol. At the time of writing, more than 10,000 verified holders are in the process of migration.The Token Presale: Closing Tonight, October 31st at 23:59 PDTAt the center of BALZ is the Fair-As-F* Launch (FAF), a limited-time token presale closing on October 31 at 23:59 PDT. Within days of opening, BALZ raised over $2 million, drawing parallels to earlier community-led launches such as Shiba Inu and Floki in 2020.FAF is structured with a fixed price and specific time frame, allowing equal participation without insider advantages or automated trading. In a market that has frequently favored early access and automation, BALZ seeks to show that fairness can be built into its design.BNB Market Conditions and TimingThe timing aligns with a significant shift in the cryptocurrency market. On October 10, 2025, the sector experienced its largest liquidation event to date, with $19 billion eliminated within 48 hours as Bitcoin declined from $126,000 to $105,000. This event represented market deleveraging rather than capitulation.Open interest decreased from $48.7 billion to $45.1 billionFunding rates fell by 51 percentOverleveraged positions were clearedThe result is a market now characterized by conviction-based participants and institutional capital seeking new deployment opportunities.Market structure mirrors 2020-2021 exactly:Bitcoin ETFs pulled in $2.71 billion during October 6-10, BlackRock's IBIT holding $65.26 billion85% of institutional firms now allocate to digital assetsFed rate cuts hit 93% probability for next quarterBNB Smart Chain Shows Continued Growth3.62 million daily active addresses in October 2025Total Value Locked surged 217% to $17.1 billion70% of BNB meme traders are currently profitableCZ is back. He changed his X profile from "ex-@binance" to "@binance" in September 2025. BNB hit an all-time high of $1,311. Real infrastructure that actually supports growth. BNB is where smart money is rotating.BALZ is capturing this momentum at the exact moment Solana and Base communities are looking for an exit. Market observers note the project is one CZ tweet away from a billion-dollar market cap, similar to previous meme token cycles where single endorsements rapidly accelerated valuations into nine-figure territory.The presale window closes October 31st at 23:59 PDT.Follow: X: @ | Telegram: t.me/BALZ_Official is a meme coin launching on Binance Smart Chain with a mission: to build the safest, fastest trading platform and no-code launchpad in crypto. Led by a doxxed team and powered by 40,000+ active members.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Amazon To Block Piracy Apps On Fire TV</title><link>https://yro.slashdot.org/story/25/10/31/2012202/amazon-to-block-piracy-apps-on-fire-tv?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon will begin blocking sideloaded piracy apps on Fire TV devices by cross-checking them against a blacklist maintained by the Alliance for Creativity and Entertainment. The company will, however, continue to allow legitimate sideloading for developers. Heise reports: In response to an inquiry, Amazon explained that it has always worked to ban piracy from its app store. As part of an expanded program led by the ACE, it is now blocking apps that demonstrably provide access to pirated content, including those downloaded outside the app store. This builds on Amazon's ongoing efforts to support creators and protect customers, as piracy can also expose users to malware, viruses, and fraud.
 
[...] The sideloading option will remain available on Fire TV devices running Amazon's new operating system, Vega OS. However, it is generally limited to developers here. In this context, the company emphasized that, contrary to rumors, there are no plans to upgrade existing Fire TV devices with Fire OS as the operating system to Vega OS.]]></content:encoded></item><item><title>Aster’s Rocket Launch Surpasses $1B in Trading Volume, as Nubila Joins with Over 6 Million $NB</title><link>https://hackernoon.com/asters-rocket-launch-surpasses-$1b-in-trading-volume-as-nubila-joins-with-over-6-million-$nb?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:45:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[George Town, British Virgin Islands, October 31st, 2025/Chainwire/--, the decentralized trading platform, has generated strong momentum with its innovative product .In the first six days following the debut of Rocket Launch, Aster recorded approximately $122 million in spot trading volume and $933 million in perpetual trading volume. Within five days after APRO’s $AT token TGE, Aster captured over 90% of the market share in $AT perpetual trading, underscoring Rocket Launch’s significant contribution to overall market activity.Since its debut on October 24, Rocket Launch has meaningfully increased both user activity and engagement on the platform. On October 29, Aster announced a 500,000 $AT Loyalty Bonus distributed to early participants who traded within the first four days of the campaign.The platform also disclosed that the spot trading competition features a reward pool of no less than 1.5 million $AT, followed by a perpetual trading campaign with at least 1.5 million $AT in additional rewards, marking a continuation of strong user engagement across both markets.The first Rocket Launch event not only accelerated new user acquisition but also reactivated existing traders and token holders, significantly enhancing overall liquidity and engagement across the Aster ecosystem. This milestone demonstrates Rocket Launch’s strong driving force and long-term potential in shaping the growth of the Aster DeFi landscape.Next Rocket Launch: Nubila Debuts, Powering the Physical Oracle Layer for AI and Prediction MarketsAster announced that the next Rocket Launch will begin on October 31, 2025, at 12:00 UTC, featuring , a decentralized oracle network for AI and prediction markets. The seven-day campaign will include both spot and perpetual trading campaigns for Nubila ($NB).The event adopts a dual reward structure. The Spot campaign offers a $200,000 $ASTER prize pool alongside over 3 million $NB in rewards, while the Perpetual campaign features an exclusive pool exceeding 3 million $NB, aimed at fostering broader participation and sustained market activity.Continuing its long-term vision, Aster is redefining the evolution of token launches through Rocket Launch, transforming what used to be a single market event into a continuous, growth-oriented journey.Each Rocket Launch campaign is structured to create a self-reinforcing value loop. The reward pool combines $ASTER and the project’s native tokens. Project teams contribute both capital and tokens, while Aster allocates those funds to buy back $ASTER from the open market.The repurchased $ASTER, together with the project tokens, are then distributed as rewards to participants, ensuring that users benefit directly from both trading activity and ecosystem growth.“Aster’s Rocket Launch is more than a trading campaign; it’s an engine for on-chain innovation,” said Leonard, CEO of Aster. “Every participant becomes part of the ecosystem, contributing to the process of value creation for emerging projects.” is building the physical oracle layer for AI and prediction markets. Its decentralized sensor network captures real-world data and transforms it into verifiable intelligence for AI systems and smart contracts.Backed by BCG, Block Space Force, Quantum Holdings, VeChain, and IoTeX, Nubila has deployed 21,000+ devices across 122 countries and 16,000+ validator nodes, powering the next wave of AI agents and decentralized applications with real, trustworthy physical data. is a next-generation decentralized exchange offering both Perpetual and Spot trading, designed as a one-stop onchain venue for global crypto traders. It features MEV-free, one-click execution in 1001x Mode. Perpetual Mode adds 24/7 stock Perpetuals, Hidden Orders, and grid trading, available across BNB Chain, Ethereum, Solana, and Arbitrum.Its unique edge lies in the ability to use liquid-staking tokens (asBNB) or yield-generating stablecoins (USDF) as collateral, unlocking unparalleled capital efficiency. Backed by YZi Labs, Aster is building the future of DeFi: fast, flexible, and community-first.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Denmark Reportedly Withdraws &apos;Chat Control&apos; Proposal Following Controversy</title><link>https://yro.slashdot.org/story/25/10/31/205234/denmark-reportedly-withdraws-chat-control-proposal-following-controversy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Record: Denmark's justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms. Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM). After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.
 
Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections. "This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports. The current model allowing for voluntary scanning expires in April, Hummelgaard said. "Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children," he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."]]></content:encoded></item></channel></rss>