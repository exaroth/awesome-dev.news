<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Python</title><link>https://www.awesome-dev.news</link><description></description><item><title>âœ¨ Beginner-Friendly Guide &apos;Minimum Cost to Convert String I&apos; - LeetCode 2976 (C++, Python, JavaScript)</title><link>https://dev.to/om_shree_0709/beginner-friendly-guide-minimum-cost-to-convert-string-i-leetcode-2976-c-python-29h6</link><author>Om Shree</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 03:35:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Converting one string into another often feels like a simple find and replace task. However, when every individual character change has a specific price tag, and you can take multiple "detours" through other characters to save money, the problem transforms into a fascinating pathfinding challenge.Two strings,  and , of the same length.A set of allowed character transformations (e.g., change 'a' to 'b') and their associated costs.The ability to use multiple steps to reach a target character (e.g., 'a' to 'c' to 'b').Calculate the minimum total cost to transform every character in  to the corresponding character in .Return -1 if any character in the  cannot be transformed into the required  character. source = "abcd", target = "acbe", original = ["a","b","c","c","e","d"], changed = ["b","c","b","e","b","e"], cost = [2,5,5,1,2,20] 28 To convert the string "abcd" to string "acbe":Change value at index 1 from 'b' to 'c' at a cost of 5.Change value at index 2 from 'c' to 'e' at a cost of 1.Change value at index 2 from 'e' to 'b' at a cost of 2.Change value at index 3 from 'd' to 'e' at a cost of 20.
The total cost incurred is 5 + 1 + 2 + 20 = 28.
It can be shown that this is the minimum possible cost. source = "aaaa", target = "bbbb", original = ["a","c"], changed = ["c","b"], cost = [1,2] 12 To change the character 'a' to 'b' change the character 'a' to 'c' at a cost of 1, followed by changing the character 'c' to 'b' at a cost of 2, for a total cost of 1 + 2 = 3. To change all occurrences of 'a' to 'b', a total cost of 3 * 4 = 12 is incurred. source = "abcd", target = "abce", original = ["a"], changed = ["e"], cost = [10000] -1 It is impossible to convert source to target because the value at index 3 cannot be changed from 'd' to 'e'.1 <= source.length == target.length <= 105source, target consist of lowercase English letters.1 <= cost.length == original.length == changed.length <= 2000original[i], changed[i] are lowercase English letters.original[i] != changed[i]
  
  
  Intuition: Thinking in Graphs
Think of the English alphabet as 26 distinct cities. Every transformation rule given in the input is a one-way road between these cities with a specific toll (the cost). Our task is to find the cheapest route from "City A" to "City B" for every character pair in our strings.Since we only have 26 possible characters, we can pre-calculate the shortest path between every possible pair of letters. Even if the input gives us a direct path from 'a' to 'b' costing 10, there might be a cheaper way: 'a' to 'c' (cost 2) and then 'c' to 'b' (cost 3), totaling only 5.We use the  to solve this. It systematically checks if passing through an intermediate letter 'k' provides a cheaper path between letters 'i' and 'j'. Once we have this 26x26 matrix of minimum costs, we simply iterate through our strings and sum up the values.
  
  
  Walkthrough: Understanding the Examples
Example 1: source = "abcd", target = "acbe" 'a' to 'a'. Cost is 0. 'b' to 'c'. The direct cost is 5. 'c' to 'b'. Direct cost is not available, but we can go 'c' to 'e' (1) and 'e' to 'b' (2). Total cost is 3. 'd' to 'e'. Direct cost is 20.Example 2: source = "aaaa", target = "bbbb"Rules: 'a' to 'c' (1), 'c' to 'b' (2).To get from 'a' to 'b', we must go through 'c'. Cost per character is .Since there are 4 characters, total cost is . When you need to find the shortest path between all possible nodes in a small graph (like the 26 letters of the alphabet), Floyd-Warshall is your best friend. Many problems that don't look like "maps" can be treated as graphs if they involve transitions between states with specific costs. When calculating minimums, initialize your values to a sufficiently large number to represent "impossible" paths, but ensure it doesn't cause overflow in your language.This problem is a classic example of why recognizing patterns is more important than memorizing code. In a real-world software system, this logic is used in things like network routing protocols or  where we need to find the most efficient connection between two points through various intermediaries. Mastering this ensures you can handle optimization tasks where the best path isn't always the most obvious one.]]></content:encoded></item><item><title>ðŸ§© Building a Number Snake Puzzle Generator in Python (with PDF &amp; JPG Export)</title><link>https://dev.to/matetechnologie/building-a-number-snake-puzzle-generator-in-python-with-pdf-jpg-export-2hho</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 02:34:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build a desktop app in Python that generates Number Snake math puzzles. The app:Creates solvable arithmetic â€œsnakeâ€ pathsSupports Easy (3Ã—3), Medium (4Ã—4), and Hard (5Ã—5) gridsShows step-by-step solutionsExports puzzles to PDF or JPGCan batch-generate multiple worksheetsttkbootstrap for modern stylingThis guide is written for beginners and breaks everything into small, understandable steps.Make sure you have Python 3.9+ installed.Then install the required packages:pip install ttkbootstrap reportlab pillowWeâ€™ll place everything inside this file.ðŸ§± Step 1 â€” Imports and Basic SetupStart by importing the libraries weâ€™ll need:import tkinter as tk
from tkinter import messagebox, filedialog
import random
import operator
import ttkbootstrap as tb
from ttkbootstrap.constants import *
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
ttkbootstrap â†’ modern dark theme + widgetsrandom + operator â†’ puzzle mathreportlab â†’ PDF generationPath â†’ clean file handlingðŸ Step 2 â€” Create the Main App ClassNow we define our application class:class NumberSnake:
    APP_NAME = "Number Snake Generator"
    APP_VERSION = "1.0"

    OPERATORS = {
        "+": operator.add,
        "-": operator.sub,
        "*": operator.mul,
        "/": operator.floordiv
    }
APP_NAME and APP_VERSION are just labelsOPERATORS maps symbols to real Python math functionsThis lets us randomly choose operations later.ðŸ–¥ï¸ Step 3 â€” Initialize the WindowInside , we configure the GUI:def __init__(self):
    self.root = tk.Tk()
    tb.Style(theme="darkly")

    self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
    self.root.geometry("1100x680")

    self.difficulty_var = tk.StringVar(value="Easy")
    self.num_puzzles_var = tk.IntVar(value=1)

    self.grid_numbers = []
    self.grid_ops = []
    self.solution_path = []
    self.target_number = None

    self.rows = self.cols = 0

    self._build_ui()
Sets defaults for difficulty and puzzle countInitializes empty puzzle dataCalls _build_ui() to draw the interfaceðŸŽ›ï¸ Step 4 â€” Build the User InterfaceNow we create labels, dropdowns, and buttons:def _build_ui(self):
    tb.Label(self.root, text=self.APP_NAME,
             font=("Segoe UI", 22, "bold")).pack(pady=10)

    opts = tb.Labelframe(self.root, text="Options", padding=10)
    opts.pack(fill="x", padx=10)

    tb.Label(opts, text="Difficulty:").pack(side="left")
    tb.Combobox(opts,
        values=["Easy","Medium","Hard"],
        textvariable=self.difficulty_var,
        width=10
    ).pack(side="left", padx=5)

    tb.Label(opts, text="Number of Puzzles:").pack(side="left", padx=10)
    tb.Spinbox(opts, from_=1, to=20,
               textvariable=self.num_puzzles_var,
               width=5).pack(side="left")
ctrl = tb.Frame(self.root)
ctrl.pack(fill="x", padx=10, pady=10)

tb.Button(ctrl, text="Generate Single Puzzle",
          bootstyle="success",
          command=self.generate_single_puzzle).pack(side="left", padx=5)

tb.Button(ctrl, text="Multiple PDFs",
          bootstyle="warning",
          command=self.generate_multiple_combined_pdf).pack(side="left", padx=5)

tb.Button(ctrl, text="JPG Export",
          bootstyle="secondary",
          command=self.generate_multiple_jpgs).pack(side="left", padx=5)
Each button simply calls a method weâ€™ll define later.ðŸ§  Step 5 â€” Generate the Snake PuzzleThis is the heart of the project.def create_puzzle_data(self):
    diff = self.difficulty_var.get()
    self.rows, self.cols = (3,3) if diff=="Easy" else (4,4) if diff=="Medium" else (5,5)

    visited = [[False]*self.cols for _ in range(self.rows)]
    r = c = 0

    self.solution_path = [(0,0)]
    visited[0][0] = True
Grid size depends on difficultyvisited tracks where weâ€™ve beenmoves = [(0,1),(1,0),(0,-1),(-1,0)]

while len(self.solution_path) < self.rows * self.cols:
    random.shuffle(moves)
    for dr, dc in moves:
        nr, nc = r+dr, c+dc
        if 0<=nr<self.rows and 0<=nc<self.cols and not visited[nr][nc]:
            r, c = nr, nc
            self.solution_path.append((r,c))
            visited[r][c] = True
            break
This randomly walks through the grid, touching every cell once.âž• Step 6 â€” Fill Numbers and Operationsnumbers = [[0]*self.cols for _ in range(self.rows)]
ops = [[None]*self.cols for _ in range(self.rows)]

current = random.randint(1,9)
numbers[0][0] = current
steps = [f"Start: {current}"]
Then for every next cell:for r,c in self.solution_path[1:]:
    valid = False
    while not valid:
        op = random.choice(list(self.OPERATORS.keys()))
        num = random.randint(1,9)

        if op == "/" and current % num != 0:
            continue
        if op == "-" and current - num <= 0:
            continue

        next_val = self.OPERATORS[op](current, num)
        valid = True

    ops[r][c] = op
    numbers[r][c] = num
    steps.append(f"{current} {op} {num} = {next_val}")
    current = next_val
Avoid fractional divisionEnsure every puzzle is solvable with integersðŸŽ¯ Step 7 â€” Display the Grid and SolutionWe draw labels for each cell:def display_grid(self):
    for r in range(self.rows):
        for c in range(self.cols):
            text = str(self.grid_numbers[r][c]) \
                   if self.grid_ops[r][c] is None \
                   else f"{self.grid_ops[r][c]}{self.grid_numbers[r][c]}"
Green cells highlight the snake path.The solution panel prints each math step:def show_solution(self):
    self.solution_text.delete("1.0", tk.END)
    self.solution_text.insert(tk.END, "\n".join(self.steps))
c = canvas.Canvas("puzzle.pdf", pagesize=A4)
c.drawString(50, 800, "Number Snake Puzzle")
Each puzzle can be saved separately or combined.ðŸ–¼ï¸ Step 9 â€” Export to JPGimg = Image.new("RGB",(900,700),(34,34,34))
draw = ImageDraw.Draw(img)
draw.text((20,20),"Number Snake Puzzle", fill="white")
Save as NumberSnake_1.jpg, NumberSnake_2.jpg, etc.Perfect for printable worksheets.â–¶ï¸ Final Step â€” Run the AppAt the bottom of your file:if __name__ == "__main__":
    NumberSnake().run()
You now have a full desktop app that:Generates arithmetic snake puzzlesSupports batch worksheet creation]]></content:encoded></item><item><title>How I Discovered a Critical Security Gap in Our HashiCorp Vault - And What It Taught Me About Policy Design</title><link>https://dev.to/bhanu_prakash_bd40068f3b6/how-i-discovered-a-critical-security-gap-in-our-hashicorp-vault-and-what-it-taught-me-about-55e3</link><author>Bhanu prakash</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 01:59:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The Day I Found a Security Hole in Our Vault Setup
The "Oh Shit" Moment
I was writing a Python script to inventory service accounts across our 50+ Vault namespaces when something caught my eye. Teams were creating auth mounts with weird names - stuff we never approved.Turns out, our wildcard policies had a massive flaw.What We Screwed Up
Our policy looked innocent enough:path "auth/*" {
  capabilities = ["create", "read", "update", "delete", "list"]
We thought: "Let teams manage auth in their namespace. What could go wrong?"Everything. Everything could go wrong.That wildcard meant teams could create any auth mount type, not just the standard AppRole we supported. So they did:Custom AppRole mounts: auth/my-special-approle/Random Kubernetes auth (we don't even use K8s auth)LDAP configs that bypassed our central authExperimental mounts nobody remembered creatingOut of 50+ namespaces, 15% had rogue auth mounts we didn't know existed.Why This Actually Mattered
Monitoring blindspot: Our Splunk dashboards looked for auth/approle/. These custom mounts were invisible.Support hell: Teams configured Vault Agents wrong, got auth failures, opened tickets. We couldn't help because their setup didn't match our docs.Future nightmare: Try migrating 50 namespaces when everyone's doing their own thing.How I Found It
Simple inventory script:for namespace in all_namepaces:
    auth_mounts = vault_client.sys.list_auth_methods()
    for mount in auth_mounts:
        if mount not in ['approle/', 'token/']:
            print(f"WTF is this: {namespace}/{mount}")The output was... concerning.Checked Splunk to see if anyone was actually using these:index=vault_audit request.path="auth/*/login"
| stats count by request.namespace request.path40% had zero logins in 90 days. Dead mounts from old experiments.The Fix
Step 1: Stop the bleeding - locked down policies immediately:`Old (bad)
path "auth/*" { capabilities = ["create", "read", "update", "delete"] }path "auth/approle/*" { capabilities = ["create", "read", "update", "delete"] }`Step 2: Reached out to teams, made migration plansStep 3: Still migrating production stuff months later (it takes time)What I Learned
Wildcards are dangerous. Be explicit. Always.Your monitoring only catches what you're looking for. Inventory everything, not just what you expect.Standards aren't real until you enforce them. Documentation doesn't count if the system allows chaos.Fixing production takes forever. We're still cleaning this up.The Bigger Issue
This also exposed that our parent/child namespace model was overly complex. We eventually flattened everything - but that's Part 2.If You Run Vault
Check your policies right now:vault policy read your-policy | grep "*"Every wildcard is a potential problem. Can you be more specific?Then actually inventory what exists in your Vault. I bet you'll find surprises.Next up: Why we ditched nested namespaces and went flat. Plus the monitoring system I built to catch this stuff automatically.Drop a comment if you've hit similar issues. I know I'm not the only one.]]></content:encoded></item><item><title>Python math.gcd</title><link>https://dev.to/slackman/python-mathgcd-2eo</link><author>slackman</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 01:30:07 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[def gcd(a,b):
    if a < b:
        a,b = b,a
    while b:
        a,b = b,a%b
    return a

import math

assert gcd(13, 2436) == 1
assert gcd(10, 2) == 2
assert math.gcd(13, 2436) == 1
assert math.gcd(10, 2) == 2
]]></content:encoded></item><item><title>Your MacBook M3 is Now Your Private Doctor: Building Private-Health-GPT with MLX and Llama-3</title><link>https://dev.to/wellallytech/your-macbook-m3-is-now-your-private-doctor-building-private-health-gpt-with-mlx-and-llama-3-56m0</link><author>wellallyTech</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 01:30:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Privacy is the new luxury. ðŸ’Ž When it comes to our health dataâ€”heart rates, sleep cycles, and activity levelsâ€”the last thing we want is to ship that sensitive information to a cloud server where it becomes just another data point for an ad-targetting algorithm.In this tutorial, we are building . We'll leverage the  to run  locally on a MacBook M3. This setup allows us to perform deep Apple HealthKit data analysis and  without a single packet leaving our machine. We're talking about a 100% offline,  wellness coach. ðŸš€The workflow involves exporting your HealthKit data as a massive XML file, parsing it into a structured format, and then feeding filtered time-series data into a quantized Llama-3 model optimized for Apple Silicon.graph TD
    A[iPhone: HealthKit Export] -->|Transfer XML| B[MacBook M3]
    B --> C{Pandas Parser}
    C -->|Cleaned Time-Series| D[Context Window Buffer]
    E[Llama-3-8B-Instruct MLX] -->|Local Inference| F[Streamlit UI]
    D --> F
    F -->|User Query| G[Health Insights & Graphs]
    G -->|Feedback Loop| F
Before we dive into the code, ensure your environment is ready for  development:: MacBook M1/M2/M3 (Pro/Max preferred for higher unified memory).: 

: The specialized library for Apple Silicon LLM deployment.: For handling the chunky HealthKit XML.: For the frontend dashboard.: Quantized for MLX.
  
  
  Step 1: Parsing the HealthKit Beast ðŸ“Š
Apple HealthKit exports data as a massive . It's often several hundred megabytes of nested tags. We need to turn this into something a LLM can understand without blowing up the context window.
  
  
  Step 2: Setting up MLX for Local Inference ðŸ§ 
The  is a game-changer. It allows the GPU and CPU to share memory seamlessly, making the 8B parameter Llama-3 run like butter on an M3 chip.First, install the goods:pip mlx-lm streamlit pandas
Now, let's initialize our local model:
  
  
  Step 3: The Streamlit Dashboard ðŸŽ¨
We need a clean UI to interact with our local doctor. Streamlit is perfect for this "Learning in Public" project.
  
  
  Why This Matters: The "Edge" Advantage ðŸ¥‘
By running this locally, you solve three major problems:: No API calls to wait for.: Your resting heart rate isn't being used to sell you insurance.Building production-ready Edge AI requires more than just a script. For those looking to dive deeper into enterprise-grade local AI patterns, including RAG (Retrieval-Augmented Generation) for health documents or vector database optimization on ARM architecture, I highly recommend checking out the technical deep-dives at WellAlly Tech Blog. They cover the "advanced" side of things that take your prototypes to the next level.The MacBook M3 isn't just a laptop; with the MLX framework, it's a powerful Edge AI workstation. We've successfully built a pipeline that transforms raw Apple HealthKit XML into intelligent, localized insights using Llama-3.  Try adding Sleep Analysis by parsing HKCategoryValueSleepAnalysis.  Implement a local vector store (like ChromaDB) to store years of health history.Drop a comment below if you run into any MLX installation issues, and don't forget to star the repo! Happy coding! ðŸ’»ðŸ”¥]]></content:encoded></item><item><title>JSONPath Is In! The AI Assistant Will See You Now</title><link>https://dev.to/david_kershaw_b6916404da6/jsonpath-is-in-the-ai-assistant-will-see-you-now-5cge</link><author>David Kershaw</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 01:22:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[JSONL is a neat and kind of weird data format. It is well-known to be useful for logs and API calls, among other things. And your favorite AI assistant API is one place you'll probably find it.CsvPath Framework supports validating JSONL. (In fact, it supports JSONL for the whole data preboarding lifecycle, but that's a longer story). And now CsvPath Validation Language supports JSONPath expressions. Since AI prompts are only kinda sorta JSONL, having JSONPath to dig into them is helpful.What I mean by kinda-sorta is that your basic prompt sequence is a series of JSON lines, but the lines are all 1-column wide and arbitrarily deep. That sounds more like a series of JSON "documents" than it does like single JSONL stream. Or, anyway, that's my take.Let's look at how to use JSONPath to inspect a JSONL file using CsvPath Validation Language in FlightPath Data. For those of you who don't already know, FlightPath Data is the dev and ops frontend to CsvPath Framework. It is a free and open source download from the Apple MacOS Store or the Microsoft Store.The file is a common example prompt. We'll start by looking at one line.{
  "messages": [
    {
      "role": "system",
      "content": "You are a happy assistant that puts a positive spin on everything."
    },
    {
      "role": "user",
      "content": "I'm hungry."
    },
    {
      "role": "assistant",
      "content": "Eat a banana!"
    }
  ]
}
From CsvPath Framework's perspective this is a one-header document. The one header is . If you open this in the grid view you see only the one header. (i.e. one column; but with delimited files we try to stick to the word "header" because with "column" your RDBMS-soaked brain starts to make incorrect assumptions).Here's what it looks like: That's not super fun. The reason is that:JSONL doesn't present its headers in the grid view (for good reasons)The  header is arbitrarily deeply nested, unlike the typical JSONL log lineNevertheless, that's what we have. Will it blend? I mean validate? Yes. JSONPath to the rescue. That said I'll pause to admit that I'm not a JSONPath expert.Right-click in the project files tree on the left of FlightPath and create a new  file, e.g. . Drop this simple example in it.$[*][ 
    push("roles", jsonpath(#messages, "$[?(@.role == 'assistant')].content") )

    last() -> print("See the variables tab for results")
]
You can see the  function. It is acting on the  header, as we'd want. We're pushing the data pulled by the JSONPath expression into a stack variable named . A stack variable is like a Python list or tuple. You create a variable by using it.  is part of the set of zero or more variables that are available throughout the csvpath statement run. They are captured to the Variables tab for a one-off FlightPath Data test run. For a production run they end up in the  file in the run results.Put your cursor in the csvpath statement and click  (or  on Windows). The output tabs should open at the bottom-middle of the screen, below your csvpath file. Click to the  tab and have a look.$[?(@.role == 'assistant')].content
picked out the objects in the  array where  equaled . And from those objects it extracted and returned the value of the  key. Pretty simple stuff. Tho, I have to admit it took me a few minutes to wrap  my JSONPath-inexperienced head around the context for the JSONPath expression. I was thinking of the whole document or the whole line, but that wasn't right.It is obviously the JSON value assigned to the  key, which is an array, in this case. Once I was operating from that correct context, the JSONPath became pretty straightforward. (Those of us with XPath scars need not be as afraid as we might be!)The point here is two-part. First, we can deal with AI prompts or any other JSONL that is deeply nested. Hooray! The data may look odd, if you are comparing it to regular tabular data, but that's no reason to not validate. Second, this example makes the point that we're doing JSONPath rules-based validation within our CsvPath context. How very Schematron-like, since Schematron does XPath validation within XSLT. Maybe this sounds complicated, but really it's not. CsvPath Validation Language is great for all things line-oriented. In this case, there isn't much for it to do, except hand off to JSONPath, which is great at documents (a.k.a. objects). Simple enough.If we wanted to create a bunch of JSONPath rules to validate our AI prompt JSONL, we could do that. To just do a quick throw-away second rule as an example try this:$[*][ 
    push("roles", jsonpath(#messages, "$[?(@.role == 'assistant')].content") )

    @stmts = jsonpath(#0, "$.`len`")
    print("Line $.csvpath.line_number: $.variables.stmts")
    @stmts == 3 

    last.nocontrib() -> print("See the variables tab for results")
]
That new rule will net you 2 lines, which are either valid or failed, depending on how you want to use your csvpath statement. You will see them in the  tab.At the same time the expanded csvpath statement will continue to pull in the same data to the variables tab that we got with the first version of the csvpath.To clean it up just a little, you can do:$[*][ 
    push("roles", jsonpath(#messages, "$[?(@.role == 'assistant')].content") )

    jsonpath(#0, "$.`len`") == 3
]
There you go, a valid 2-rule validation statement using JSONPath on nested JSON in a JSONL document. Useful? Totally! Give it a try.]]></content:encoded></item><item><title>Selamat Datang Di Sayabet</title><link>https://dev.to/cssayabet/selamat-datang-di-sayabet-2oni</link><author>Sayabet_Gacor</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 29 Jan 2026 00:06:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Situs Resmi Dan Terpercaya Tahun 2026!! Jackpot Berapapun Pasti Dibayar Lunas!!  Ada Bonus Di Setiap Deposit Guys!!  ðð®ð«ð®ðšð§ ðƒðšðŸð­ðšð« ð’ð€ð˜ð€ðð„ð“]]></content:encoded></item><item><title>Custom Functions FTW</title><link>https://dev.to/david_kershaw_b6916404da6/custom-functions-ftw-21co</link><author>David Kershaw</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 22:20:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[CsvPath Validation Language is functions-based. It applies a very simple syntax and a large number of functions to validate CSV, JSONL, and Excel files in ways that were never-before possible.And then comes the moment when you want to do some crazy thing that the CsvPath Framework contributors didn't think of. What to do? You create a custom function that does exactly that.Custom functions even work in FlightPath Data and FlightPath Server. I call that out because FlightPath Data is a multi-project environment. And FlightPath Server is both multi-project and also multi-user. That means functions must be scoped and sandboxed. They are and they work great! Let's create a trivial example to show the setup of a custom function. I'll leave the actual functionality as an exercise for the reader, since that part is demonstrated copiously in the CsvPath Framework Github repo.Our goal is to create a function called . It will functionally be the same as . I.e. it returns .Our csvpath statement will be:If you try that in FlightPath Data you will get this error message:The first step is to point to a functions import file. By default, import files are called  and live in the project's  directory. In FlightPath Data, click  at the bottom left of the app to open the config panel. Then click  in the vertical tabs to open the functions config form. The form has just one field for the path to the imports file. The path can be relative or absolute.Once the path is ready click  and then .Next we need to edit the imports file to include our  function. Right click on the blank space in the project files tree and select .Open the config directory and you should see three files, , , and . If you don't see all three don't worry about it; some files are generated just in time. If there is no  create one. Then open it.In  we're going to add one line that imports our  function.This is basically the same form as Python's. It says find the  file and import the  class, using the name  as the function name of the imported class. Simple!Finally we just need to put the custom function in the right place. The right place, starting from the project's home directory, is config/example/one/yes.py.I copied the regular  class from its file in the repo to make my example . Again, we're just setting up a custom function, not showing how to write an awesome function.This is where the  file lives.And... we're basically ready to go. However, if you ran a csvpath already, restart FlightPath to clear the function classes that were already loaded. You can do this programmatically in CsvPath Framework, but there's no button in FlightPath's config yet.That done, back in FlightPath Data right click in your project files tree and create a new file. Call it , or whatever you like.In it, paste our test csvpath:Now, with your cursor inside the csvpath statement, click  (or  on Windows). You should see the message Test run complete. Matched 2 lines. in the status bar and the printouts tab should be blank. (If your status bar says  but has a different number of lines don't worry about it; your test data and mine just aren't the same).For a bit of comfort that all is working as expected, add a print line like this and you should get the same printouts shown.And that's all there is to it. Now you'll never be stumped by the absence of , because you can write it yourself. To be fair, though, while a simple function can be trivial, as we just saw,  more complex functions can be... well, more complex. If you need help creating your awesome function don't hesitate to reach out. We'll be glad to help you get started.]]></content:encoded></item><item><title>Privacy Engineering: Automated PII Detection and Redaction</title><link>https://dev.to/deepak_mishra_35863517037/privacy-engineering-automated-pii-detection-and-redaction-4phl</link><author>Lalit Mishra</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 21:30:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Executive Summary: The Engineering Imperative of Data Sanitization
The digitization of global commerce and the exponential growth of machine learning applications have fundamentally altered the relationship between software architecture and data privacy. Historically, privacy was relegated to the domain of legal complianceâ€”a passive exercise in drafting Terms of Service, consent forms, and retention policies. In the modern data ecosystem, however, privacy has evolved into a hard engineering constraint. It is no longer sufficient to promise privacy; systems must be architected to enforce it deterministically.For senior privacy engineers and data platform architects, the mandate is clear: shift from "compliance by policy" to "compliance by code." The ingestion of unstructured dataâ€”whether through high-concurrency web scraping, log aggregation, or third-party API consumptionâ€”introduces a significant risk vector. Personally Identifiable Information (PII) acts as a contaminant within the data lake, turning valuable datasets into "toxic assets" that attract regulatory scrutiny and compromise downstream machine learning models.This blog articulates a comprehensive framework for "Privacy Engineering," treating data sanitization as a core software engineering discipline. We explore the architectural failure modes of naive ingestion, the technical supremacy of Microsoft Presidio as a detection standard, and the implementation of robust, privacy-aware pipelines. By integrating Named Entity Recognition (NER), context-aware logic, and advanced cryptographic redaction strategies, engineering teams can dismantle the traditional friction between data utility and data privacy, ensuring alignment with GDPR, CCPA, and emerging AI safety standards.Let make our mood light with a pretty funny meme ðŸ˜Š!
  
  
  The Anatomy of a Failure: When "Public" Data Becomes a Liability
To understand the necessity of privacy engineering, one must first analyze the catastrophic failure modes of naive data ingestion. A prevailing misconception among data engineers is that "public" dataâ€”information accessible without authentication on the open webâ€”is free from privacy constraints. This assumption is legally perilous and technically flawed.Consider a hypothetical scenario involving "FinScrape Analytics," a fintech entity developing alternative credit scoring models. The engineering team deploys a distributed scraping architecture using headless browsers (e.g., Playwright or Selenium) to aggregate professional profiles from public social media platforms, industry forums, and corporate "About Us" pages. The objective is to extract job titles, employment history, and public endorsements to infer creditworthiness. The scraper is designed to extract  and  content based on CSS selectors. However, the unstructured text within these containers often contains unsolicited PII that the scraper is not designed to recognize or filter. A forum post scraped for sentiment analysis contains a user pasting their personal phone number and email address to resolve a customer service dispute. A scraped corporate biography inadvertently captures a home address listed alongside a business address, or a personal mobile number used for emergency contact. The text contains inferred political opinions, trade union membership, or health data (Special Category Data under GDPR Article 9), which requires explicit consent to process, regardless of its public availability.
  
  
  The Regulatory Blast Radius
Upon ingestion, this raw text is serialized (e.g., JSON or Avro) and dumped into a Data Lake (S3, Azure Blob Storage) and subsequently loaded into a data warehouse like Snowflake. The PII is now "at rest" and replicated across multiple environments (development, staging, production).GDPR Violation (Article 5 - Data Minimization): The company collected data irrelevant to the specified purpose. The principle of data minimization dictates that only data strictly necessary for the purpose should be processed.GDPR Violation (Article 14 - Notification): Since the data was not obtained directly from the subject, the company maintains an obligation to notify the individualsâ€”an operational impossibility given the volume of millions of records. Regulatory bodies like the French CNIL and Irish DPC have aggressively penalized companies for scraping public data without valid legal bases or sanitization measures. For instance, the French DPA fined a data broker â‚¬240,000 for scraping LinkedIn profiles without adequate transparency or legal basis, emphasizing that "public" availability does not negate privacy rights. Similarly, Meta (Facebook) faced a â‚¬265 million fine related to a scraping leak, underscoring that the failure to implement "technical and organizational measures" to prevent PII harvesting is a punishable offense. The failure was not in the scraping codeâ€™s ability to fetch HTML, but in the pipelineâ€™s lack of a "Privacy Firewall." Privacy Engineering dictates that no unstructured text should land in persistent storage without passing through a decontamination layer.
  
  
  Privacy Engineering: A Core Discipline
The transition from legal checkpoints to engineering checkpoints requires a fundamental change in how data pipelines are conceived. Privacy Engineering operationalizes abstract legal principles into concrete code execution, moving the responsibility from the legal department to the DevOps and Data Engineering teams.
  
  
  The Privacy-by-Design Pipeline Model
Traditional ETL (Extract, Transform, Load) processes often treat privacy as a governance task performed after loadingâ€”typically triggered by an audit or a Data Subject Access Request (DSAR). Privacy Engineering moves this to the "Transform" phase, or even earlier, to the "Extraction" phase, creating a proactive defense mechanism.Table 1: The Shift from Compliance to EngineeringLegal/Compliance ApproachPrivacy Engineering ApproachIngestion event (Real-time/Batch)Policy documents & retention schedulesCode-level filtering & sanitizationRetroactive deletion/suppressionProactive redaction/tokenizationSpreadsheets, Legal CounselNLP Models, Regex, Vaults, PresidioCompliance Certifications (SOC2, ISO)Recall/Precision of PII Detection, Latency
  
  
  Shift Left: Sanitization at the Edge
The most effective privacy architecture sanitizes data as close to the source as possible. In a scraping context, this means analyzing the text payload within the scraperâ€™s memory space or immediately upon message queue ingestion (e.g., Kafka, Kinesis), before writing to disk. This aligns with the GDPR principle of Data Protection by Design and by Default (Article 25). By stripping PII from the payload before it enters the data lake, the "toxic asset" liability is neutralized immediately. Raw identifiers never spread across logs, backups, or downstream systems, limiting the "blast radius" of any potential breach.This "Shift Left" approach fundamentally changes the economics of data protection. Remediation of PII deep within a data warehouse is computationally expensive and operationally complex (requiring rewrite of immutable partitions). Sanitization at ingestion is a linear cost associated with compute, preventing the compounding debt of privacy risk.
  
  
  Technical Deep Dive: Microsoft Presidio
To implement this vision, engineers require a robust, extensible, and production-ready detection engine. Microsoft Presidio has emerged as the industry standard open-source framework for this purpose. Unlike proprietary SaaS solutions that act as black boxes, Presidio offers the transparency, modularity, and on-premises deployment capabilities required for high-stakes engineering.
  
  
  Architecture: Separation of Concerns
Presidioâ€™s architecture is bifurcated into two distinct, decoupled services: the  and the . This separation is critical for auditability and flexibility, allowing detection logic to evolve independently of redaction policies.
The Analyzer is the detection brain. It ingests unstructured text and outputs a list of detected entities with confidence scores and location indices. It is stateless and read-only regarding the text transformation. The  coordinates the detection process. It manages a registry of "Recognizers" and aggregates their results. These are the logic units. Presidio supports multiple types to maximize coverage and accuracy:

 Use Regular Expressions (Regex) for structured data like credit card numbers, email addresses, and IP addresses. These are computationally efficient and deterministic. Utilize Named Entity Recognition (NER) models (via spaCy, Stanza, or HuggingFace Transformers) to detect context-dependent entities like Person Names (PER), Locations (LOC), and Organizations (ORG). This allows the system to distinguish "George Washington" (Person) from "Washington" (Location). Implement complex validation logic, such as Luhn algorithm checks for credit cards or checksums for national IDs, reducing false positives from random number sequences. These components boost the confidence score of a detected entity if specific "context words" are found in proximity (e.g., boosting a 9-digit number's score if the word "SSN" or "Social" appears nearby).
The Anonymizer is the transformation muscle. It accepts the original text and the metadata payload from the Analyzer (the list of  objects) to apply specific operations. The Anonymizer executes "Operators" on the detected spans. Standard operators include  (substitution),  (deletion),  (e.g., ), and  (SHA-256/512). Crucially, the Anonymizer supports encryption operators, allowing for reversible pseudonymization if the engineering team manages the encryption keys securely. This enables specific authorized workflows to decrypt data while keeping it opaque to general analytics.
  
  
  NER vs. Regex: The Precision-Recall Trade-off
A sophisticated privacy engineer understands when to deploy NER versus Regex, as the choice impacts both accuracy and system latency.Regular Expressions (Regex): Pattern matching based on character sequences. Highly structured identifiers (Email, IPv4/v6, IBAN, SSN, Phone Numbers). Extremely low latency, deterministic, high precision for strict formats. Fails on unstructured, ambiguous entities. A regex cannot reliably distinguish a person's name from a street name or a common noun. Broad regex patterns (e.g., \d{9}) suffer from high false-positive rates without context.Named Entity Recognition (NER): Statistical models (Deep Learning/Transformers) trained on labeled corpora (e.g., OntoNotes) to predict entity tags based on linguistic context and word vectors. Unstructured entities (Person Names, Organizations, Geopolitical Entities). Context-aware. Can identify "Apple" as an Organization in "Apple released a phone" and as a fruit in "I ate an apple." Higher latency (requires model inference), non-deterministic (probabilistic), requires GPU/TPU for high throughput, larger memory footprint. Evaluation on datasets like CoNLL shows high F1 scores but highlights the computational cost. Presidio excels by combining both. It uses NER to find the "Person" and Regex to find the "Email," then aggregates the results using a conflict resolution strategy (e.g., prioritizing the match with the higher confidence score). This hybrid approach allows engineers to leverage the speed of regex for structured data while relying on the sophistication of NER for ambiguous text.
  
  
  Multilingual Support and NLP Engines
Global scraping operations encounter diverse languages, necessitating a multilingual approach. Presidioâ€™s abstraction layer allows swapping the underlying NLP engine via the . The default engine. Fast, production-ready, with models available for dozens of languages (e.g., , , ). It strikes a balance between performance and accuracy. A Stanford NLP library that often provides higher accuracy for low-resource languages but comes with a higher latency cost. Presidio supports integration with . For state-of-the-art accuracy, engineers can integrate HuggingFace Transformers models (e.g., BERT, RoBERTa) tailored for NER tasks. While computationally intensive, these models offer superior performance on complex, nuanced text.
  
  
  Architecting the Privacy-Aware Scraping Pipeline
To operationalize Presidio, we propose a "Privacy Firewall" architecture. This pipeline ensures that no raw data is persisted without inspection, adhering to the principle of "Defense in Depth".
  
  
  The Pipeline Flow: Scrape -> Detect -> Redact -> Audit -> Store

  
  
  Python Integration Workflow
The following section details the code implementation of this architecture, demonstrating how to integrate Presidio into a Python-based processing worker.
First, we establish the environment with the necessary libraries. We initialize the  with a registry containing both pre-defined and custom recognizers.4.2.2 The Ingestion and Cleaning Phase
Scraped content is often raw HTML. Analyzing HTML tags directly can confuse NER models (e.g., misinterpreting class names as entities). We must extract visible text while preserving structure where necessary for context.
  
  
  The Detection and Redaction Core
This is the heart of the pipeline. We define a transformation policy: Names are replaced with placeholders, Phones are masked, and specific internal IDs are hashed to allow for referential integrity without exposure.This code snippet demonstrates a self-contained, reproducible unit of the Privacy Firewall. In a production environment, the  function would be the entry point for the Kafka consumer worker.
  
  
  Advanced Redaction Strategies: Beyond Simple Masking
While simple masking () satisfies basic compliance, it often destroys data utility. Analytics and ML teams typically need to preserve the referential integrity of the data without exposing the identity. For example, knowing that "User A" behaved similarly to "User B" is valuable, even if we don't know who "User A" is. Privacy Engineering offers several advanced strategies to bridge this gap.
  
  
  Hashing (Deterministic Anonymization)
Hashing converts PII into a fixed-size string (e.g., SHA-256).. Presidio supports this via the  operator. Consistent. The same email address always hashes to the same string, allowing for  operations across different datasets and frequency analysis (e.g., "How many unique users visited?"). Vulnerable to Rainbow Table attacks if the input space is small (e.g., phone numbers or 6-digit IDs). To mitigate this, engineers must apply a cryptographic salt (a random string added to the input before hashing). Presidio allows configuration of hash types (, , ).
  
  
  Reversible Tokenization (Vault-Based)
For scenarios where PII might need to be recovered (e.g., a support ticket scraping pipeline where an agent might need to contact the user later), irreversible hashing is insufficient. We need .In a Vault-based architecture, the PII is swapped for a random token (UUID or a format-preserving token). The mapping (Token <-> PII) is stored in a secure, isolated "Vault" (e.g., Redis or an encrypted SQL table) with strict access controls.Presidio Integration with Vaults: While Presidio handles the detection and logic, the "Vault" interaction usually requires a custom operator or an integration with the  operator using a symmetric key. When Presidio detects an entity, it calls a custom function that checks the Vault (e.g., Redis). If the PII exists, it retrieves the token; if not, it generates a new token, saves the pair to the Vault, and returns the token to replace the text. This concentrates the risk into the Vault. Securing the Vault (via encryption at rest, strict IAM roles, and network isolation) secures the entire dataset.Table 2: Redaction Strategy ComparisonHigh (Decryption possible)High (Referential integrity)Highest (Data separation)
  
  
  Cryptographic Erasure and Deanonymization
A profound benefit of encryption-based pseudonymization or vault-based tokenization is "Cryptographic Erasure." To comply with a GDPR "Right to be Forgotten" (Article 17) request, one does not need to hunt down every instance of a user's data across petabytes of backups and data lakes. Instead, one simply destroys the encryption key or the Vault mapping associated with that user. The data remains in the lake but is mathematically irretrievableâ€”effectively erased.Conversely, authorized systems can use the Presidio Deanonymize Engine to revert the process. By providing the encrypted text and the correct key (or token and Vault access), the  restores the original PII for legitimate business purposes.
  
  
  GDPR/CCPA Alignment via Code
Privacy Engineering translates legal articles into software functions, providing demonstrable compliance.
  
  
  Data Minimization (GDPR Art. 5(1)(c))
The code in Section 4.2.3 explicitly defines . By detecting only specific types and ignoring others, the system enforces minimization. If the scraper encounters a  but that entity is not in the  list (or is in a configured block-list), it is not processed as PII. Alternatively, if strict minimization is required, the policy can be configured to redact any detected entity type unless explicitly allowed (Allow-list approach).
  
  
  Purpose Limitation (GDPR Art. 5(1)(b))
By segregating the PII into a secure Vault (Tokenization) or hashing it, we technologically enforce purpose limitation. Data Scientists act on the tokenized data for modeling (Purpose A - Analytics). Customer Support accesses the Vault to retrieve the email (Purpose B - Support). Access Control Lists (ACLs) on the Vault enforce the separation, ensuring that analysts cannot accidentally view raw contact details.
  
  
  Contextual Logic for False Positives
Presidio allows "Context Words." For example, to reduce false positives for , the recognizer can be configured to require words like "driver", "license", "id", or "dl" to appear within a window of N tokens around the match. This is crucial for reducing "over-redaction," where non-PII data (like product serial numbers) is mistakenly redacted, destroying data utility. This tuning directly supports data accuracy principles.
  
  
  Operational Excellence: Tuning and Monitoring
Deploying Presidio in production is an iterative process. Models drift, and scraping targets change structure. Operational excellence requires continuous monitoring and tuning.
  
  
  Handling False Positives and Negatives
 Do not deploy straight to production with active redaction. Run the pipeline in "Shadow Mode" where detection results are logged but not applied (or applied to a shadow copy of the data). A human analyst or a secondary automated system samples the logs to verify recall (Did we miss PII?) and precision (Did we redact valid text?). Presidio returns a confidence score (0.0 - 1.0) for each detection.

 (e.g., handling medical data/PHI): Set a  (e.g., 0.3-0.4) to prioritize . It is better to redact a harmless number than to leak a patient ID (False Positive > False Negative). Set a  (e.g., 0.7-0.8) to prioritize Precision. You want to preserve data utility and avoid corrupting the dataset with aggressive redaction. Maintain an allow-list for terms that look like PII but aren't (e.g., company support emails , known dummy numbers, or generic addresses). Presidio supports  functionality to bypass specific values.
  
  
  Performance Tuning and Latency
 NER models (spaCy/Transformers) are CPU/GPU intensive. For high-throughput scraping (thousands of pages/sec), Presidio can become a bottleneck. Benchmarks indicate that out-of-the-box spaCy models have a latency of ~15ms per sample, while Transformer-based models can spike to ~50ms+ per sample.

 Use  to process texts in bulk, amortizing the overhead of model calls. Offload detection to GPU-enabled nodes if using Transformer models. Use Regex-based recognizers primarily and reserve NER only for fields where context is ambiguous. Implementing a Redis cache for repeated text snippets (e.g., common headers/footers in scraped HTML) can drastically reduce inference costs. If the same privacy policy text appears on every scraped page, analyze it once and cache the result.
  
  
  Downstream Benefits: ML and RAG Safety
The investment in upstream Privacy Engineering pays dividends downstream, particularly in the era of Generative AI and Large Language Models (LLMs).Retrieval-Augmented Generation (RAG) involves feeding retrieved documents into an LLM context window to generate answers. If the scraped documents contain PII, the LLM might leak it in the generated answer. By sanitizing the ingestion pipeline, the Vector Database (e.g., Pinecone, Milvus) contains only anonymized embeddings. This ensures that the RAG system is "secure by design"â€”even if the LLM is prompted to reveal PII, the source data it retrieves is already clean.
  
  
  Removing Bias and Memorization
LLMs trained on datasets containing names and demographics often learn distinct biases associated with those identities (e.g., associating certain names with specific professions). Anonymizing names () and masking demographics helps de-bias the training data. Furthermore, it prevents the model from "memorizing" specific individuals, mitigating Model Inversion Attacks where an attacker queries the model to extract training data.The era of unrestricted data collection is over. For senior engineers, the adoption of tools like Microsoft Presidio represents a necessary evolution in platform architecture. By embedding privacy controls directly into the ingestion pipeline, we move beyond the fragility of "compliance checkboxes" to the robustness of "Privacy Engineering." We do not just protect our users; we protect the future of our data platforms. The code provided herein is your starting blockâ€”build your firewall, tune your models, and treat privacy as a first-class citizen in your software stack. The risk of inaction is no longer just legal; it is existential.]]></content:encoded></item><item><title>Case Study: Saving 20 Hours a Week for a Real Estate Agency with AI Agents</title><link>https://dev.to/frankdotdev/case-study-saving-20-hours-a-week-for-a-real-estate-agency-with-ai-agents-33b9</link><author>Frank Oge</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 20:55:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Real Estate is a numbers game. But itâ€™s also an exhaustion game.
â€‹I recently consulted for a mid-sized agency here in Lagos. Their problem wasn't a lack of leads; it was Lead Fatigue.
For every 100 people who messaged them on WhatsApp asking "How much?", only about 3 were serious buyers with the budget to proceed.
â€‹The agents were spending 4 hours a day just answering the same three questions:
â€‹"Is it still available?"
â€‹"What is the price?"
â€‹They were human FAQs.
â€‹I proposed a solution: Letâ€™s fire the humans from the 'First Response' layer and hire an AI Agent.
â€‹Here is how I built a system that not only answers questions but qualifies leads and books inspections automatically.
â€‹We didn't want a "dumb" chatbot that just gives static replies. We needed an Agent that could query their specific database of properties.
â€‹Brain: OpenAI (GPT-4o) via LangChain.
â€‹Communication: Twilio (WhatsApp API).
â€‹Database: Supabase (PostgreSQL + pgvector).
â€‹Knowledge Base: A live sync of their property listings.
â€‹Phase 1: The "RAG" Knowledge Base
â€‹The biggest challenge was accuracy. The AI couldn't hallucinate a price.
We used RAG (Retrieval-Augmented Generation).
â€‹When a user asks, "Do you have any 3-bedroom flats in Ikeja under N5m?"
â€‹The system converts the query into a vector.
â€‹It searches Supabase for matching properties.
â€‹It retrieves the exact data (Price, Location, Features).
â€‹It feeds this to GPT-4o to generate a polite, human-like response.
â€‹Result: The bot never guesses. It only sells what is in stock.
â€‹Phase 2: The "Guardrail" Qualification
â€‹We instructed the AI to act like a senior sales agent. Its goal wasn't just to chat; it was to qualify.
â€‹"You are a helpful Real Estate Assistant. Your goal is to get the user to book an inspection. Before booking, you must politely ask for their budget and timeline. If they cannot afford the property, politely suggest cheaper alternatives."
â€‹This filter alone saved the human agents hours of driving to inspections with clients who had zero intention of buying.
â€‹Phase 3: The Handoff
â€‹If the user is serious and agrees to a time, the AI Agent uses a "Tool" (via LangChain) to check the human agent's Google Calendar and book the slot.
The human agent gets a notification: "New Inspection Booked: Mr. Obi, 3 Bedroom, Budget Verified."
â€‹After 30 days of running this pilot:
â€‹Response Time: Dropped from ~2 hours to <1 minute.
â€‹Agent Workload: Reduced by ~20 hours/week (no more answering "How much?" at 10 PM).
â€‹Conversion: Inspection bookings increased by 15% because the bot replied instantly while leads were hot.
â€‹Conclusion
â€‹We are past the era of "Chatbots." We are in the era of "AI Agents."
A chatbot follows a script. An Agent uses tools, makes decisions, and performs work.
For this agency, it was the difference between being busy and being profitable.
â€‹Hi, I'm Frank Oge. I build high-performance software and write about the tech that powers it. If you enjoyed this, check out more of my work at frankoge.com]]></content:encoded></item><item><title>pydantic-ui</title><link>https://dev.to/idlingmind/pydantic-ui-4n7</link><author>Idling Mind</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 20:52:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I recently ran into an issue at work. We have an engineering analysis process that required large amount of inputs to be collected from engineers which contained deeply nested structure with multiple input types. As any sane python developer will do these days, we used pydantic models to both structure our mental model and also to validate this massive input (which comes in the form of a yaml file)Even though yaml is a decent format to use for structured input, it's still quite verbose to hand-craft and it can be quite tricky to get all the indents right. So we needed a decent UI which can be used to fill in this input. The thought was, Instead of writing a UI specifically for our data structure, why not infer the structure dynamically from the already existing pydantic model? It can also give us proper (and upfront) validation errors if there are errors in the input. If we can sprinkle a little more customization on top of the existing pydantic model, it can be quite powerful. That's exactly what I ended up doing (with a lot of help from claude opus 4.5). We thought of open-sourcing our solution.Here's pydantic-ui. It a bit opinionated, but also lets you configure quite a lot. Please do test it out and any feedback is welcome!]]></content:encoded></item><item><title>Build a &quot;Stateful&quot; AI Chatbot with Python &amp; OpenAI</title><link>https://dev.to/it_solutions_pro/build-a-stateful-ai-chatbot-with-python-openai-5857</link><author>IT Solutions Pro</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 20:40:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most beginners make a critical mistake when working with the OpenAI API: they assume the AI remembers them.By default, Large Language Models (LLMs) are . This means if you say "My name is Shakar," and then ask "What is my name?" in the next request, the API will have no idea who you are.In this tutorial, we are going to fix that. We will build a  chatbot in Python that maintains conversation history, handles errors gracefully, and runs locally in your terminal.
  
  
  ðŸ“º Watch the Full Masterclass
import os
from dotenv import load_dotenv
from openai import OpenAI

# 1. Load environment variables securely
load_dotenv()

# 2. Initialize the Client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

print("--- AI Chatbot Initialized (Type 'quit' to exit) ---")

# 3. Setup Memory (System Context)
# This sets the behavior of the AI
messages = [
    {"role": "system", "content": "You are a helpful, friendly IT Assistant."},
]

# 4. The Main Loop
while True:
    try:
        user_input = input("\nYou: ")

        # Exit Condition
        if user_input.lower() in ['quit', 'exit']:
            print("Shutting down...")
            break

        # STEP A: Add User Input to Memory
        messages.append({"role": "user", "content": user_input})

        # STEP B: Send the WHOLE history to the API
        response = client.chat.completions.create(
            model="gpt-4o", # You can use "gpt-3.5-turbo" to save cost
            messages=messages,
            temperature=0.7
        )

        # STEP C: Extract Answer & Add to Memory
        ai_response = response.choices[0].message.content

        # Crucial Step: Save the AI's own words back to the list
        messages.append({"role": "assistant", "content": ai_response})

        print(f"AI: {ai_response}")

    except Exception as e:
        print(f"An error occurred: {e}")
        break
]]></content:encoded></item><item><title>The Linter That Yells</title><link>https://dev.to/nicolas_vbgh/the-linter-that-yells-231h</link><author>nicolas.vbgh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 20:19:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We picked the boring stack. Python. FastAPI. The technologies AI understands. Now we make sure AI doesn't write garbage.Code reviews are a beautiful fantasy we tell ourselves. "Someone will catch my mistakes." No they won't. They're checking Slack. They're thinking about lunch. They're wondering if that meeting could've been an email.Meanwhile, your  sits there in async code, waiting to murder production at 3 AM on a Saturday. Nobody will catch it. Nobody ever does.So I stopped pretending humans review code. I let robots do it. Robots don't get hungry. Robots don't have feelings. Robots are perfect for this job.And here's the kicker: none of this is even testing. Not a single test runs. This is just linting. Glorified spell-check for code. We haven't even  verifying that the code does what it's supposed to do. We're just making sure it's not obviously broken before we bother checking if it works.The bar is on the floor. And most codebases still trip over it.
  
  
  Ruff: Because Life Is Too Short for Slow Linters
Remember pylint? You'd run it, go make coffee, come back, still running. So everyone disabled it. Problem solved. Also: problems not solved at all.Ruff is written in Rust. It runs in milliseconds. You can't even alt-tab fast enough to avoid it.That ASYNC rule at the bottom? That's the one that saves your weekends.Here's how to tank your server in one line:This blocks the entire event loop. Every user. Every request. Everything stops while your code takes a little nap. No error. No warning. Just... silence. And then your phone rings at 3 AM.No kidding. You put  in async code.ASYNC100: blocking call `time.sleep` in async function
The fix takes two seconds:I make this mistake weekly. Sometimes daily. My brain refuses to learn. Fortunately, Ruff doesn't care about my brain. Ruff just yells. That's the relationship.
  
  
  MyPy: Because "It Works" Is Not a Type
Python is dynamically typed. This means you can write this:What is ? Could be a string. Could be a list. Could be your hopes and dreams. Python doesn't care. Python will try to call  on anything. Python believes in you.Python is wrong to believe in you.MyPy strict mode fixes this by being incredibly annoying:Now you have to actually say what things are:Is this more typing? Yes. Is this tedious? Also yes. Will this save you from a 4-hour debugging session because you passed a dict to a function expecting a string? Absolutely yes.The AI also loves types. Give it typed code and it knows exactly what to generate. Give it untyped code and it hallucinates confidently. Your choice.Linters catch a lot. But not everything. Third-party libraries do weird things. Someone's "async" wrapper is actually sync. Life is full of disappointments.So I run tests with the event loop in paranoid mode:Anything takes longer than 100ms? Test fails. Loudly. Rudely. Exactly as it should.Belt and suspenders. Because I've seen things. Things that work perfectly locally and explode in production. Things that pass every test and still somehow break. Plan accordingly.One job per check. When something fails, you know exactly what.Three jobs. Same stage. Run in parallel. When one fails, you see exactly which one in the pipeline view. No scrolling through logs to find the error. â€”  starts with a dot. GitLab won't run it directly. It's a template. DRY without the copy-paste. â€” Each job inherits the template. Same image, same cache, same rules. Only the script changes. â€” All three jobs run at the same time. Faster feedback. If ruff and mypy both fail, you see both failures immediately. Fix them together instead of playing whack-a-mole. â€” On every job. This isn't a suggestion. Your MR sits there, rejected, until all three pass.The pipeline doesn't care that it's Friday at 5 PM. The pipeline doesn't care that "it works on my machine." The pipeline is the most reliable colleague you'll ever have.Copy, paste, adapt. It works.I could review code carefully. I could remember all the async gotchas. I could check every type hint manually.I could also juggle chainsaws. Both are technically possible. Neither is a good idea.The reality is: humans forget things. That's not a character flawâ€”that's human nature. The trick isn't to fight it. The trick is to build systems that work  it.I write the rules once. The machines enforce them forever. They never get tired. They never get distracted. They never think "eh, it's probably fine."The linter catches what I forget. The type checker verifies what I assume. The pipeline blocks what I'd regret.TypeScript or Tears â€” Same idea, different battlefield. JavaScript lies. TypeScript doesn't.]]></content:encoded></item><item><title>Boring Is a Feature</title><link>https://dev.to/nicolas_vbgh/boring-is-a-feature-2e01</link><author>nicolas.vbgh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 19:55:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Why I Let AI Choose My Technologies
The philosophy is clear: programming by coercion. Make the machine enforce quality. But which machine? Which stack?Here's a confession: I didn't pick this stack because it's the best. I picked it because AI knows it by heart.Python. FastAPI. TypeScript. React. PostgreSQL. Nothing exciting. Nothing cutting-edge. Nothing that will impress anyone at a conference.
  
  
  The Training Data Advantage
AI generates code based on training data. More examples = better output. Simple as that.You can use that fancy new Rust framework with 200 GitHub stars. AI will hallucinate half the API. You'll spend your evening fixing AI mistakes instead of watching your show.Or you can use technologies with millions of examples in the training data. AI gets it right the first time. You ship faster.I chose option two. My ego will recover.Every AI coding benchmark uses Python. HumanEval, MBPP, SWE-benchâ€”all Python. Coincidence? No. AI understands Python better than any other language.AI generates this correctly every time. Try the same in Scala, Go or Rust. Good luck.Flask is fine. Django is fine. But FastAPI has something they don't: types everywhere and OpenAPI out of the box.AI reads this signature and knows exactly what to generate. Input types, output types, dependency injectionâ€”all explicit. No guessing."Just write raw SQL, it's simpler."Sure. And AI will generate SQL injection vulnerabilities, wrong column names, and type mismatches. I've seen it. Multiple times. In one afternoon.SQLAlchemy gives AI structure:AI can't accidentally concatenate user input. The ORM pattern is type-safe by design.I naturally chose relational databasesâ€”they enforce typing by design.PostgreSQL because it's the industry standard: mature, stable, perfect migrations, unbelievable backward compatibility.MySQL/MariaDB could work, but I prefer real open source without Oracle's shadow. And I'm still unable to rename a database without voodoo file manipulationâ€”am I the only one shocked by this?NoSQL with MongoDB or Neo4j looks cool, but I'll stick with boring PostgreSQL for type enforcement. AI has seen millions of examples. I'm guaranteed to run it seamlessly for years.This one's not about AI. It's about sanity.uv 
uv run pytest
10-100x faster than pip. Deterministic builds. I'm not interested in watching never-ending package installations, even with Netflix on. I switched and never looked back.With 10 sync workers, a 200 ms request caps you at ~50 RPS (25 RPS at 400 ms) because each worker naps while Postgres thinks.
With async, the same setup can handle ~500 RPS (250 RPS at 400 ms) by multitasking instead of staring at the wall.Python async used to be a footgun. AI would forget  constantly.Now we have Ruff with async rules. AI still forgets . The linter catches it. Problem solved.This is programming by coercion in action.
  
  
  The Frontend That Just Works
JavaScript has no types. AI doesn't know what functions expect. Refactoring is prayer-based.TypeScript strict mode forces AI to be explicit:AI knows the input. AI knows the output. AI generates correct code.I use  and . Yes, it's annoying sometimes. That's the point.Vue is great. Svelte is great. But AI has seen more React code than everything else combined.Standard patterns. Predictable hooks. AI generates this in its sleep."Works on my machine" is not a deployment strategy.Docker makes environments reproducible. AI knows Dockerfile patterns. Everyone wins.YAML-based pipelines. Well-documented. AI generates correct CI configs.More importantly: this is where the coercion happens. Every check, every gate, every "you cannot merge this". One file to rule them all.
  
  
  Monorepo: One Home for Everything
/
â”œâ”€â”€ backend/       # FastAPI
â”œâ”€â”€ frontend/      # React
â”œâ”€â”€ infra/         # Helm, k8s
â””â”€â”€ .gitlab-ci.yml # The gatekeeper
Backend, frontend, infraâ€”same repo. One clone. One branch. One PR."But separate repos are cleaner!" Sure. And now AI needs to:Make coordinated changes across reposHope the CI in repo A passes before repo B deploysWith a monorepo, AI sees everything. Change the API schema? AI updates the backend endpoint, the frontend types, and the OpenAPI spec. One commit. One pipeline. All checks run together.The pipeline enforces consistency. Frontend types don't match backend? CI fails. Database migration missing? CI fails. Contract broken? CI fails. You can't ship half a feature.Separate repos can't do this. You'd need cross-repo CI triggers, version pinning, deployment coordination. Complexity for complexity's sake.AI works in one context. The pipeline validates one state. Ship with confidence.I could have picked Rustâ€”efficient memory management, blazing fast, amazing type system, solid async.But AI struggles with Rust. Fewer examples, different patterns, even syntax errors.So I use Python. And FastAPI. And all the boring stuff.My side projects ship. My evenings are free. The stack is unremarkable.]]></content:encoded></item><item><title>AI Phishing Defense Platform Day 13</title><link>https://dev.to/cyberb0x/ai-phishing-defense-platform-day-13-4690</link><author>Arslon Erkinov</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 19:11:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Day 13 completed: API Security & Usage TrackingToday I finished Day 13 of building my AI Phishing Defense Platform.
Whatâ€™s implemented:
âœ… Endpoint protection
âœ… Custom rate limiting (production-ready, no third-party magic)
âœ… API usage tracking (endpoint, method, time)
âœ… Architecture ready for SaaS billing & plansThis is no longer a â€œpet projectâ€.
Itâ€™s a real API product foundation â€” scalable, secure, and measurable.Next step:
 ðŸ“Š Plans, quotas, admin dashboard, and SaaS polish.Iâ€™m building this long-term with a clear goal:
strong portfolio, real users, and global impact.]]></content:encoded></item><item><title>python helper/dunder/magic methods/</title><link>https://dev.to/esthernaisimoi/python-helperdundermagic-methods-15bj</link><author>ESTHER NAISIMOI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 18:51:37 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[functions are reuasable block of code;you can juts reuse a block of code by calling the fucntion name anywhere in the code.def myFunct():
    fname = input("What is your first name? ")
    lname = input("What is your last name? ")

    return f"Your first and last names are: {fname}, {lname} respectively"

Ask the user for their first and last name.Return a formatted string with both names.You can call this function as many times as you want without rewriting the input logic each time!Moving straight to the point..."Dunder" is short for  (like , etc.).These are special methods in Python that Python calls automatically when certain things happen, like creating a new object, printing an object, or adding two objects together.And Dunder Methods: What do they do?
Dunder methods are like automatic rules for Python. They are built-in functions that are triggered in specific situations, and you don't need to call them yourself. : this initializes any new object for a classSample Code for Dunder Methodsclass Player:
    # Define a new class named Player

    def __init__(self, name):
        # This function is automatically called when creating a new Player object
        # self refers to the current object you're creating
        self.name = name  # Whatever name you give me, Iâ€™ll remember it as self.name

    def __str__(self):
        # This function is automatically called when you try to print the Player object
        return f"Player: {self.name}"  # Return the actual player's name when printed


# Creating a Player object
name = input("What is your name? ")  # Ask for the player's name
player1 = Player(name)  # Create an instance of the Player class with the input name

# Print the player object
print(player1)  # This will automatically call __str__ and print "Player: [Name]"


TLDR?
The syntax is simple :A class is a blueprint or template that encapsulates properties and behaviors for objects.
A class contains attributes (data) and methods (functions).2. 
Constructor Method ():The constructor method  is a special dunder method used to initialize an object when it's created.It contains parameters that are passed when you create an instance of the class.
The self keyword refers to the current instance (object) of the class.def __init__(self, name, age):
    self.name = name  # Save the player's name
    self.age = age  # Save the player's score

3.
Operation Method (e.g., , , etc.):
This is where you define custom behavior for common operations.: Defines how the object will be represented as a string (when you print it).: Defines what happens when you use the + operator with objects of that class.def __str__(self):
    return f"Player: {self.name}, Age: {self.age}"

def __add__(self, other):
    return self.score + other.score  # Add scores of two Player objects

self: A reference to the current object. Itâ€™s used inside methods to access the object's attributes (e.g., self.name, self.age).Other parameters: These are the values you pass into the class constructor or operation methods to define the object's state or behavior.class Person:
    def __init__(self, name, age):
        # This is the constructor method to initialize the object
        self.name = name  # Store the name in the object
        self.age = age    # Store the age in the object

    def __str__(self):
        # This is the __str__ method to return a string representation of the object
        return f"{self.name} is {self.age} years old."

# Example Usage
person1 = Person("Alice", 25)  # Create a Person object with name 'Alice' and age 25
print(person1)  # It will print: Alice is 25 years old.

]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/aakashio/-ei2</link><author>Aakash Choudhary</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 18:50:22 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[GitHub Actions Has a Cleanup Problem â€” So I Built a ToolAakash Choudhary ãƒ» Jan 28]]></content:encoded></item><item><title>GitHub Actions Has a Cleanup Problem â€” So I Built a Tool</title><link>https://dev.to/aakashio/github-actions-has-a-cleanup-problem-so-i-built-a-tool-46hh</link><author>Aakash Choudhary</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 18:48:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[           __                                                                 
          /  |                                                                
  ______  $$ |____            ______    ______   __    __  _______    ______  
 /      \ $$      \  ______  /      \  /      \ /  |  /  |/       \  /      \ 
/$$$$$$  |$$$$$$$  |/      |/$$$$$$  |/$$$$$$  |$$ |  $$ |$$$$$$$  |/$$$$$$  |
$$ |  $$ |$$ |  $$ |$$$$$$/ $$ |  $$ |$$ |  $$/ $$ |  $$ |$$ |  $$ |$$    $$ |
$$ \__$$ |$$ |  $$ |        $$ |__$$ |$$ |      $$ \__$$ |$$ |  $$ |$$$$$$$$/ 
$$    $$ |$$ |  $$ |        $$    $$/ $$ |      $$    $$/ $$ |  $$ |$$       |
 $$$$$$$ |$$/   $$/         $$$$$$$/  $$/        $$$$$$/  $$/   $$/  $$$$$$$/ 
/  \__$$ |                  $$ |                                              
$$    $$/                   $$ |                                              
 $$$$$$/                    $$/                                               
Keep your GitHub Actions clean. No more ghost workflows.If you use GitHub Actions regularly, youâ€™ve probably noticed something odd: You delete a workflow file (.github/workflows/old-flow.yml).But the workflow still appears in the Actions tab.Then comes the painful part:Clicking through runs one by one.Manually deleting history.Realizing thereâ€™s no native bulk cleanup button.As a DevOps engineer, this felt like unnecessary friction.  is a Python-based CLI tool built on top of the GitHub CLI. It is designed to inspect your repository, identify workflow runs, and help you bulk-delete old or unwanted history to properly clean up the Actions UI.ðŸ”  GitHub Actions workflows.ðŸ“‹  workflow runs clearly.ðŸ—‘ï¸  old or unwanted runs.âœ¨  the Actions UI of "deleted" workflows that persist in history.Because  leverages the official GitHub CLI for authentication and API interaction, you must have it installed and authenticated. is available on PyPI. You can install it via pip:]]></content:encoded></item><item><title>Toolesh</title><link>https://dev.to/ch_dani_f54adb6f1646e9a61/toolesh-3j1f</link><author>CH DANI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 18:33:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Toolesh is a free AI based photo enhancement tool built to improve low quality images. It can unblur photos, reduce noise, and upscale images without losing key details. The tool is perfect for users who want clear and sharp photos without complex editing.]]></content:encoded></item><item><title>What I learned after people tried my LEGO side project</title><link>https://dev.to/vincentaltspec/what-i-learned-after-people-tried-my-lego-side-project-5e8i</link><author>Vincent-alt-spec</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 18:23:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Iâ€™ve been working on a small side project that generates alternate LEGO builds using only the exact parts from a real set.All you have to do is type in your set number then build size and last build type for example a dragon or like cafe.I got a few viewers but no feedback and that is of course to be expected But its a v1 and definentaly not perfect so i would like you guys to try it and give me feedback that could possible help me, it does not have to be alot even a word that says its bad would help!]]></content:encoded></item><item><title>Open-Source Pygame Minecraft Clone</title><link>https://dev.to/ulissedusci/open-source-pygame-minecraft-clone-2f1e</link><author>UlisseDusci</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 17:57:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I am a python/pygame programmer. I would like to create an open-source project to create a mine clone.Is there anyone who wants to help?]]></content:encoded></item><item><title>Build a Simple Trading Bot with Python in 5 Minutes</title><link>https://dev.to/bch_an_26333c2d50dbb9434/build-a-simple-trading-bot-with-python-in-5-minutes-27l7</link><author>Báº ch An</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 17:54:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ðŸ”¥ Automating your trading strategy has never been this easy!
(https://www.youtube.com/watch?v=gpXMloqOuEo)
This video shows you how to â€œBuild a Simple Trading Bot with Python in 5 Minutesâ€ â€” a fast, beginner-friendly demo that helps anyone quickly 
understand the core workflow of algorithmic trading using Python.
ðŸ“ In the video, youâ€™ll learn how to:
Set up a Python environment to build and run a trading bot
Fetch market data and react to trading signals
Automatically place BUY / SELL orders based on predefined logic
All of this happens within the first 5 minutes â€” short, focused, and straight to the point.
This video is designed to give you a clear high-level understanding of how a trading bot works. However, once you try to run a bot in real market conditions, youâ€™ll quickly realize that a production-ready bot requires more than a simple demo.
Thatâ€™s exactly why Iâ€™m sharing a complete, ready-to-use source code in the video description ðŸ‘‡
ðŸš€ What this trading bot can do
Connect to real Binance Spot using API (via ccxt)
Fetch real-time OHLCV candlestick data
Trade using a Simple Moving Average (SMA) strategy
Price > SMA â†’ BUY
Includes TEST MODE for safe paper trading (no real money)
Automatically checks account balance before placing orders
Clean, readable code â€” easy to modify and extend
âš™ï¸ What you need to run it
Python
Run pip install -r requirements.txt
Execute python main.py
ðŸ‘‰ The bot is ready for paper trading or live trading â€” just switch TEST MODE on or off.
ðŸŽ¯ Who this source code is for
Traders who want to understand how real trading bots work in practice
Developers who donâ€™t want to waste time setting everything up from scratch
Anyone looking for a solid foundation to expand into RSI, MACD, AI models, or backtesting
The video gives you the knowledge â€” the source code gives you a real working tool.
If youâ€™re serious about trading automation, this is the right place to start.
ðŸ‘‰ The source code link is available in the video description.]]></content:encoded></item><item><title>Programando em PituguÃªs! - Uma linguagem inspirada em Python</title><link>https://dev.to/cumbucadev/programando-em-pitugues-uma-linguagem-inspirada-em-python-49bi</link><author>Cumbuca Dev</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 17:43:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[âš ï¸ AtenÃ§Ã£o: Esse texto Ã© uma introduÃ§Ã£o e tutorial! Divirta-se! :)Quando comeÃ§amos a aprender a programar, uma das primeiras descobertas que nos deparamos Ã© que: as linguagens de programaÃ§Ã£o sÃ£o todas em inglÃªs! Bom, pelo menos as que sÃ£o utilizadas no mercado de trabalho.Estatisticamente falando, apenas 5% dos brasileiros entendem inglÃªs em algum nÃ­vel, enquanto apenas 1% possui fluÃªncia. Ou seja, nem todos os falantes de lÃ­ngua portuguesa tem conhecimento do idioma inglÃªs.O que faz com que a pessoa aprendiz passa encontrar barreiras que a faÃ§a dispor mais esforÃ§o para desenvolver vÃ¡rias habilidades ao mesmo tempo (programar e aprender outro idioma concomitantemente). Ela terÃ¡ dificuldades de lidar com recursos e instruÃ§Ãµes da linguagem de programaÃ§Ã£o, o que poderÃ¡ comprometer sua aprendizagem de lÃ³gica e algoritmos.AÃ­ tem-se a importÃ¢ncia de existir uma linguagem como o PituguÃªs, em que a pessoa nativa em portuguÃªs consiga programar em sua lÃ­ngua mÃ£e, eliminando a barreira idiomÃ¡tica.Sua sintaxe Ã© inspirada na da linguagem de programaÃ§Ã£o Python que, embora seu nome faÃ§a alusÃ£o a uma espÃ©cie de cobra e seu sÃ­mbolo seja composto de duas cobras em Ying & Yang, seus criadores eram muito fÃ£s da sÃ©rie de comÃ©dia â€œMonty Pythonâ€™s Flying Circusâ€ e batizaram a linguagem com o nome de â€œPythonâ€.Pegando carona atÃ© mesmo no nome, se formos traduzir â€œpythonâ€, temos â€œpÃ­tonâ€. Assim, para trazer a ideia de uma linguagem de programaÃ§Ã£o inspirada em Python para a lÃ­ngua portuguesa, uniu-se â€œpÃ­tonâ€ + â€œportuguÃªsâ€ que resultou em: PituguÃªs!E, como comentamos, o PituguÃªs vai se inspirar e buscar trazer caracterÃ­sticas do Python para portuguÃªs, comoâ€¦Quando lidamos com linguagem de programaÃ§Ã£o, devemos lembrar que existem tipos diferentes de dados, como dados textuais, numÃ©ricos, binÃ¡rios e etc.Algumas linguagens exigem que o tipo de dado seja declarado como, por exemplo, em Java, declaramos uma variÃ¡vel da seguinte maneiraâ€¦Note que, em Java, indicamos o tipo do dado (int, para nÃºmeros inteiro, e String para tipos textuais), escrevemos o nome da variÃ¡vel e, por fim, damos um valor a ela. Se formos comparar com Python, jÃ¡ possui uma diferenÃ§a acentuadaâ€¦Como podem ver, jÃ¡ nÃ£o Ã© necessÃ¡rio indicar o tipo da variÃ¡vel, pois a linguagem irÃ¡ verificar isso automaticamente, Ã© o que chamamos de â€œinferir tipoâ€.No PituguÃªs, nÃ£o vai ser diferente, a nossa declaraÃ§Ã£o de variÃ¡veis tambÃ©m vai inferir o tipo...A indentaÃ§Ã£o nada mais Ã© que o aninhamento de trechos de cÃ³digo que, no caso do Python, se dÃ¡ atravÃ©s da tabulaÃ§Ã£o (4 espaÃ§amentos), ela tem o objetivo de determinar quais linhas de cÃ³digo pertencem a um bloco lÃ³gico, definindo a estrutura e hierarquia.Ou seja, declaramos uma funÃ§Ã£o, por exemplo, em seguida, escrevemos os dois pontos e, logo abaixo deles, com certo espaÃ§amento, comeÃ§amos a escrever o que a nossa funÃ§Ã£o irÃ¡ executar. Dessa forma que definimos o escopo e a hierarquia do cÃ³digo, determinando em que momento o trecho de cÃ³digo Ã© iniciado e finalizado.Caso a indentaÃ§Ã£o nÃ£o seja feita da maneira correta, sem o espaÃ§amento ou com espaÃ§amento excessivo, o prÃ³prio Python irÃ¡ nos sinalizar com uma mensagem de erro.Na documentaÃ§Ã£o do PituguÃªs vocÃª consegue ter acesso a exemplos de como a indentaÃ§Ã£o funciona:Assim como o Python, o PituguÃªs tambÃ©m Ã© uma linguagem de CÃ³digo Livre e estÃ¡ aberta a contribuiÃ§Ãµes e, deste a postagem original deste tutorial e introduÃ§Ã£o a respeito da linguagem, jÃ¡ aconteceram mudanÃ§as bastante significativas...HÃ¡ pouco tempo atrÃ¡s, PituguÃªs adicionava a palavra  na declaraÃ§Ã£o de variÃ¡veis, o que o distanciava de Python. No entanto, contribuiÃ§Ãµes mais recentes da comunidade fizeram que consigamos declarar variÃ¡veis da mesma forma que em Python:linguagem_de_programacao = "PituguÃªs"Em resumo, PituguÃªs existe no repositÃ³rio de uma outra linguagem de programaÃ§Ã£o em portuguÃªs chamada DelÃ©gua e acabou herdando algumas caracterÃ­sticas dela, como a interpolaÃ§Ã£o:var linguagemDeProgramacao = "DelÃ©gua"escreva("Linguagem de programaÃ§Ã£o: ${linguagemDeProgramacao}")Este tipo de interpolaÃ§Ã£o tambÃ©m se distanciava do Python, mas contribuiÃ§Ãµes recentes trouxeram proximidade entre PituguÃªs e Python e, agora, a interpolaÃ§Ã£o segue o padrÃ£o f-strings:linguagem_de_programacao = "PituguÃªs"imprima(f"Linguagem de programaÃ§Ã£o: {linguagemDeProgramacao}")Lembrando que estamos atualizaÃ§Ã£o a documentaÃ§Ã£o do PituguÃªs constantemente e, qualquer dÃºvida que senha sobre a sintaxe, basta consultÃ¡-la!
  
  
  Mas como programar em PituguÃªs?
Para programar em PituguÃªs, basta que vocÃª instale uma extensÃ£o no VS Code. Se na aba de extensÃµes vocÃª procurar por "pitugues", deve encontrar a extensÃ£o da Design LÃ­quido que possui todo um ecossistema de linguagens em portuguÃªs como: linguagem de programaÃ§Ã£o, de marcaÃ§Ã£o, de estilo, de consulta e etc.Nesta postagem vocÃª pode encontrar um tutorial semelhante a este, mas que demonstra por imagens como instalar a extensÃ£o e como executar seus programar em PituguÃªs!O PituguÃªs Ã© uma iniciativa feita pela comunidade, para a comunidade e toda contribuiÃ§Ã£o conta! ðŸðŸ’œ
Ã‰ uma linguagem que estÃ¡ em constante transformaÃ§Ã£o e vocÃªs podem acompanhar junto com a gente todo esse processo!Junte-se a nÃ³s e ajude a construir o PituguÃªs â€” uma linguagem feita com comunidade, propÃ³sito e muito coraÃ§Ã£o. ðŸ’œ]]></content:encoded></item><item><title>Building a Semantic Search Knowledge Base with MindsDB</title><link>https://dev.to/mindsdb/building-a-semantic-search-knowledge-base-with-mindsdb-5107</link><author>MindsDB Team</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 16:30:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Written by Andriy Burkov, Ph.D. & Author, MindsDB AdvisorWhat happens when a developer searches for "how to make async HTTP calls" but your documentation says "asynchronous network requests"? Traditional keyword search failsâ€”even though the content is exactly what they need.This is the fundamental limitation of keyword search: it matches words, not meaning.In this tutorial, we'll build a semantic search system using MindsDB that understands user intent. Using 2 million Stack Overflow posts, we'll create knowledge bases with two different vector storage backendsâ€” and â€”and compare their performance.How MindsDB knowledge bases convert text into searchable vectorsSetting up pgvector (PostgreSQL-based) and FAISS (Facebook AI Similarity Search) storageCombining semantic search with metadata filtersBuilding an AI agent that uses your knowledge base to answer questionsA MindsDB account (cloud or self-hosted)PostgreSQL database with the Stack Overflow datasetAn OpenAI API key for embeddings
  
  
  How Semantic Search Works
Before we dive in, let's understand the key difference between keyword and semantic search:Misses "asynchronous requests"Semantic search works by:: Converting text into numerical vectors using an embedding model: Saving these vectors in a vector database: Converting the search query to a vector and finding the closest matchesMindsDB handles all of this through its Knowledge Base abstraction.: Python client for interacting with MindsDB servers: For working with query results as DataFrames

  
  
  3. Connecting to the MindsDB Cloud Instance
Connected to MindsDB server

  
  
  4. Connecting to the Data Source
Created pg_sample database connection
Let's verify the connection by exploring the data. Check the dataset size:Dataset size: 2,000,000 rows
.dataframe tbody tr th:only-of-type {
    vertical-align: middle;
}

.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
An explicit cast to `double` like this isn't n...Given a `DateTime` representing a person's bir...How do I calculate someone's age based on a Da...Given a specific `DateTime` value, how do I di...Calculate relative time in C#c#,datetime,time,datediff,relative-time-spanWhat is the difference between [Math.Floor()](...Difference between Math.Floor() and Math.Trunc...I have an absolutely positioned `div` containi...Why did the width collapse in the percentage w...html,css,internet-explorer-7Here's how I do it\n\n

```\nvar ts = new TimeSp...Is there a standard way for a web server to be...Determine a user's timezonehtml,browser,timezone,user-agent,timezone-offsetI want to assign the decimal variable "trans" ...How to convert Decimal to Double in C#?c#,floating-point,type-conversion,double,decimalHow do I store binary data in [MySQL](http://e...mysql,database,binary-data,data-storageIf I have a trigger before the update on a tab...Throw an error preventing a table update in a ...The Stack Overflow dataset contains 2 million postsâ€”both questions () and answers (). Key columns include: - Unique identifier for each post - The content we'll make semantically searchable - The title of the post (questions only) - Programming language and topic tags (e.g., , ) - Community voting scoreâ€”useful for prioritizing high-quality content - Popularity metric for filtering - Type of post (1=question, 2=answer) - ID of the accepted answer (for questions), ,  - TimestampsThis rich metadata allows us to combine semantic understanding with traditional filtersâ€”for example, finding Python questions about async programming with a score above 10.
  
  
  4. Setting Up Vector Storage Backends
MindsDB supports multiple vector storage options. We'll set up both pgvector and a recently added FAISS and will compare how quick they are.
  
  
  pgvector (PostgreSQL Extension)
pgvector is a PostgreSQL extension for vector similarity search. It's ideal when you want to keep vectors alongside your relational data.
python
# Create pgvector database connection
run_query("""
    CREATE DATABASE pg_vector
    WITH ENGINE = "pgvector",
    PARAMETERS = {
        "user": "YOUR_PG_USER",
        "password": "YOUR_PG_PASSWORD",
        "host": "YOUR_PG_HOST",
        "port": "5432",
        "database": "vector"
    }
""", "Created pg_vector database connection")


Created pg_vector database connection

  
  
  FAISS (Facebook AI Similarity Search)
FAISS is a library for efficient similarity search developed by Facebook AI Research. It's optimized for fast similarity search on large datasets.
python
# Create FAISS database connection
run_query("""
    CREATE DATABASE db_faiss
    WITH ENGINE = 'duckdb_faiss',
    PARAMETERS = {
        "persist_directory": "/home/ubuntu/faiss"
    }
""", "Created db_faiss database connection")


Created db_faiss database connection

  
  
  Choosing Between pgvector and FAISS
Integration with existing PostgreSQLNative PostgreSQL storageExcellent (billions of vectors)Requires PostgreSQL extensionGood (~19s for 2M vectors)Excellent (~5s for 2M vectors)For this tutorial, we'll implement both so you can see the performance difference firsthand.
  
  
  5. Creating Knowledge Bases
Now we have a table with relational data and two vector stores to keep the embedding vectors. We are ready to create knowledge bases using both storage backends.Use OpenAI's  model for generating embeddingsStore the post  as searchable contentInclude metadata fields for filtering results
  
  
  Knowledge Base with pgvector Storage

python
def kb_exists(kb_name):
    """Check if a knowledge base already exists."""
    try:
        result = server.query("SELECT name FROM information_schema.knowledge_bases").fetch()
        return kb_name in result['name'].values
    except Exception:
        return False

# Create pgvector knowledge base
if kb_exists("kb_stack_vector"):
    print("kb_stack_vector already exists - skipping creation")
else:
    run_query("""
        CREATE KNOWLEDGE_BASE kb_stack_vector
        USING
            storage = pg_vector.stack,
            embedding_model = {
                "provider": "openai",
                "model_name": "text-embedding-3-small"
            },
            content_columns = ['Body'],
            metadata_columns = [
                "PostTypeId",
                "AcceptedAnswerId",
                "ParentId",
                "Score",
                "ViewCount",
                "Title",
                "ContentLicense",
                "FavoriteCount",
                "CreationDate",
                "LastActivityDate",
                "LastEditDate",
                "LastEditorUserId",
                "OwnerUserId",
                "Tags"
            ]
    """, "Created kb_stack_vector knowledge base")


Created kb_stack_vector knowledge base

  
  
  Knowledge Base with FAISS Storage

python
# Create FAISS knowledge base
if kb_exists("kb_stack_faiss"):
    print("kb_stack_faiss already exists - skipping creation")
else:
    run_query("""
        CREATE KNOWLEDGE_BASE kb_stack_faiss
        USING
            storage = db_faiss.stack,
            embedding_model = {
                "provider": "openai",
                "model_name": "text-embedding-3-small"
            },
            content_columns = ['Body'],
            metadata_columns = [
                "PostTypeId",
                "AcceptedAnswerId",
                "ParentId",
                "Score",
                "ViewCount",
                "Title",
                "ContentLicense",
                "FavoriteCount",
                "CreationDate",
                "LastActivityDate",
                "LastEditDate",
                "LastEditorUserId",
                "OwnerUserId",
                "Tags"
            ]
    """, "Created kb_stack_faiss knowledge base")


Created kb_stack_faiss knowledge base

  
  
  Understanding the Parameters
Specifies the vector database connection and table nameConfiguration for the embedding model (provider and model name)Columns to embed and make semantically searchableColumns available for filtering (not embedded, but stored)
  
  
  6. Loading Data into Knowledge Bases
Now we'll insert the Stack Overflow data into our knowledge bases. This process:Fetches data from the source table in batchesGenerates embeddings for content columns using the OpenAI APIStores vectors and metadata in the vector database
  
  
  Loading Data into pgvector Knowledge Base

python
def is_kb_empty(kb_name):
    """Check if a knowledge base is empty (fast - only fetches 1 row)."""
    result = server.query(f"SELECT id FROM {kb_name} LIMIT 1").fetch()
    return len(result) == 0

if is_kb_empty("kb_stack_vector"):
    print("kb_stack_vector is empty - starting data insertion...")
    server.query("""
        INSERT INTO kb_stack_vector
        SELECT * FROM pg_sample.stackoverflow_2m 
        USING 
            batch_size = 1000, 
            track_column = id
    """).fetch()
    print("Data insertion started for kb_stack_vector")
else:
    print("kb_stack_vector is not empty - skipping data insertion")


Data insertion started for kb_stack_vector

  
  
  Loading Data into FAISS Knowledge Base

python
if is_kb_empty("kb_stack_faiss"):
    print("kb_stack_faiss is empty - starting data insertion...")
    server.query("""
        INSERT INTO kb_stack_faiss
        SELECT * FROM pg_sample.stackoverflow_2m 
        USING 
            batch_size = 1000, 
            track_column = id
    """).fetch()
    print("Data insertion started for kb_stack_faiss")
else:
    print("kb_stack_faiss is not empty - skipping data insertion")


Data insertion started for kb_stack_faiss
Wait until the data insertion is complete.
  
  
  7. Querying the Knowledge Bases
Once data is loaded, you can perform semantic searches combined with metadata filtering.Search for content related to "8-bit music" (finds semantically similar content):
python
import time

# Semantic search on pgvector KB
start = time.time()
results_vector = server.query("""
    SELECT * FROM kb_stack_vector 
    WHERE content = '8-bit music'
    AND Tags LIKE '%python%'
    LIMIT 10
""").fetch()
elapsed_vector = time.time() - start
print(f"pgvector query time: {elapsed_vector:.2f} seconds")
display(results_vector)

# Semantic search on FAISS KB
start = time.time()
results_faiss = server.query("""
    SELECT * FROM kb_stack_faiss 
    WHERE content = '8-bit music'
    AND Tags LIKE '%python%'
    LIMIT 10
""").fetch()
elapsed_faiss = time.time() - start
print(f"FAISS query time: {elapsed_faiss:.2f} seconds")
display(results_faiss)


pgvector query time: 19.21 seconds
.dataframe tbody tr th:only-of-type {
    vertical-align: middle;
}

.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
Im trying to engineer in python a way of trans...List of values to a sound file{'Tags': 'python,audio', 'Score': 0, 'Title': ...I have a mosquito problem in my house. This wo...python,audio,mp3,frequencyPython library for playing fixed-frequency sound{'Tags': 'python,audio,mp3,frequency', 'Score'...I am confused because there are a lot of progr...How can i create a melody? Is there any sound-...{'Tags': 'python,audio', 'Score': 7, 'Title': ...1118266:Body:2of2:972to1430The current solution I'm thinking of involves ...List of values to a sound file{'Tags': 'python,audio', 'Score': 0, 'Title': ...I want to learn how to program a music applica...Programming a Self Learning Music Maker{'Tags': 'python,perl,waveform', 'Score': 7, '...Write a function called listenToPicture that t...How do I loop through every 4th pixel in every...{'Tags': 'python,image,audio', 'Score': 0, 'Ti...I'm trying to write a program to display PCM d...{'Tags': 'python,audio,pcm', 'Score': 7, 'Titl...Is there a way to do this? Also, I need this t...Playing sounds with python and changing their ...{'Tags': 'python,pygame,pitch', 'Score': 1, 'T...1382998:Body:4of4:2649to3382```

\nÂ¼ Ã©Ã­Ã±Â§ÃÃŒÃ«Ã‘ Â» Â¼ Ã¶ Â® Â© â€™\n0 1\n2 10\n3 10\n...{'Tags': 'python,unicode', 'Score': 18, 'Title...I wish to take a file encoded in UTF-8 that do...python,c,utf-8,compressionCompressing UTF-8(or other 8-bit encoding) to ...{'Tags': 'python,c,utf-8,compression', 'Score'...FAISS query time: 5.04 seconds
.dataframe tbody tr th:only-of-type {
    vertical-align: middle;
}

.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
Im trying to engineer in python a way of trans...List of values to a sound file{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...I have a mosquito problem in my house. This wo...python,audio,mp3,frequencyPython library for playing fixed-frequency sound{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...I am confused because there are a lot of progr...How can i create a melody? Is there any sound-...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1118266:Body:2of2:972to1430The current solution I'm thinking of involves ...List of values to a sound file{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...I want to learn how to program a music applica...Programming a Self Learning Music Maker{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...Write a function called listenToPicture that t...How do I loop through every 4th pixel in every...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...I'm trying to write a program to display PCM d...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...Is there a way to do this? Also, I need this t...Playing sounds with python and changing their ...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1382998:Body:4of4:2649to3382

```\nÂ¼ Ã©Ã­Ã±Â§ÃÃŒÃ«Ã‘ Â» Â¼ Ã¶ Â® Â© â€™\n0 1\n2 10\n3 10\n...{'ContentLicense': 'CC BY-SA 3.0', 'LastActivi...I wish to take a file encoded in UTF-8 that do...python,c,utf-8,compressionCompressing UTF-8(or other 8-bit encoding) to ...{'ContentLicense': 'CC BY-SA 3.0', 'LastActivi...Notice how the search for "8-bit music" returned posts about:Converting values to sound filesPlaying fixed-frequency soundsCreating melodies programmaticallyNone of these posts contain the exact phrase "8-bit music," yet they're all semantically relevant to chiptune/retro audio generation. This is the power of semantic search.Also note the  with FAISS (5 seconds vs 19 seconds for pgvector). For production systems with high query volumes, this difference is significant.
  
  
  Combined Semantic and Metadata Filtering
Find AJAX-related posts tagged with jQuery that have high view counts:
python
# pgvector: Semantic search with metadata filters
start = time.time()
results = server.query("""
    SELECT * FROM kb_stack_vector 
    WHERE content = 'ajax'
        AND Tags LIKE '%jquery%'
        AND ViewCount > 1000.0
        AND relevance > 0.6
    LIMIT 10
""").fetch()
print(f"pgvector query time: {time.time() - start:.2f} seconds")
display(results)

# FAISS: Semantic search with metadata filters
start = time.time()
results = server.query("""
    SELECT * FROM kb_stack_faiss 
    WHERE content = 'ajax'
        AND Tags LIKE '%jquery%'
        AND ViewCount > 1000.0
        AND relevance > 0.6
    LIMIT 10
""").fetch()
print(f"FAISS query time: {time.time() - start:.2f} seconds")
display(results)


pgvector query time: 5.76 seconds
.dataframe tbody tr th:only-of-type {
    vertical-align: middle;
}

.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
1400637:Body:28of32:25627to26627o.ajax({type:"POST",url:E,data:G,success:H,dat...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...1400637:Body:30of32:27488to28356O=false;T.onload=T.onreadystatechange=function...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...1400637:Body:27of32:24691to25626rn this},serialize:function(){return o.param(t...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...1424774:Body:2of2:934to1745var self = this;\n        $.ajax({\n  ...{'Tags': 'jquery,loops', 'Score': 1, 'Title': ...1400637:Body:31of32:28357to29238N=function(X){if(J.readyState==0){if(P){clearI...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...546344:Body:2of3:902to1764var before = function() { $(loading).show() ;...Using jQuery, how can I store the result of a ...{'Tags': 'javascript,jquery,ajax', 'Score': 0,...1279625:Body:2of3:782to1754```

\n<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML ...Trouble with jQuery Ajax timing{'Tags': 'events,jquery,getjson', 'Score': 0, ...1400637:Body:32of32:29239to30048L(){if(M.complete){M.complete(J,R)}if(M.global...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...1775625:Body:5of9:3144to4049}\n\n}\n</script>\n\n\n\n<script type=...jQuery - Multiple form submission trigger unre...{'Tags': 'jquery,form-submit', 'Score': 1, 'Ti...1400637:Body:26of32:23690to24690nclick")}o(function(){var L=document.createEle...Stop reload for ajax submitted form{'Tags': 'javascript,jquery', 'Score': 2, 'Tit...FAISS query time: 2.50 seconds
.dataframe tbody tr th:only-of-type {
    vertical-align: middle;
}

.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
1400637:Body:28of32:25627to26627o.ajax({type:"POST",url:E,data:G,success:H,dat...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1400637:Body:30of32:27488to28356O=false;T.onload=T.onreadystatechange=function...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1400637:Body:27of32:24691to25626rn this},serialize:function(){return o.param(t...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1424774:Body:2of2:934to1745var self = this;\n        $.ajax({\n  ...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1400637:Body:31of32:28357to29238N=function(X){if(J.readyState==0){if(P){clearI...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...546344:Body:2of3:902to1764var before = function() { $(loading).show() ;...Using jQuery, how can I store the result of a ...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1279625:Body:2of3:782to1754

```\n<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML ...Trouble with jQuery Ajax timing{'ContentLicense': 'CC BY-SA 3.0', 'LastActivi...1400637:Body:32of32:29239to30048L(){if(M.complete){M.complete(J,R)}if(M.global...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1775625:Body:5of9:3144to4049}\n\n}\n</script>\n\n\n\n<script type=...jQuery - Multiple form submission trigger unre...{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...1400637:Body:26of32:23690to24690nclick")}o(function(){var L=document.createEle...Stop reload for ajax submitted form{'ContentLicense': 'CC BY-SA 2.5', 'LastActivi...
  
  
  Understanding Query Results
The query returns these columns:Identifier for the text chunkJSON object with all metadata fieldsVector distance (lower = more similar)Relevance score (higher = more relevant, 0-1)Get only highly relevant results:
  
  
  The Power of Combined Filtering
The query we just ran demonstrates MindsDB's hybrid search capability:
sql
SELECT * FROM kb_stack_faiss 
WHERE content = 'ajax'              -- Semantic match
    AND Tags LIKE '%jquery%'        -- Metadata filter
    AND ViewCount > 1000            -- Popularity threshold
    AND relevance > 0.6             -- Quality threshold


Are semantically similar to "ajax" (not just keyword matches)Have significant engagement (>1000 views)Meet a minimum relevance scoreThis combination is impossible with traditional search and would require complex custom code with raw vector databases.
python
def run_query_ignore_exists(sql, success_msg="Query executed successfully"):
    """Execute a query, silently ignoring 'already exists' errors."""
    try:
        result = server.query(sql).fetch()
        print(success_msg)
        return result
    except RuntimeError as e:
        return None  # Silently ignore
# Create MindsDB Agent
run_query_ignore_exists("""
    drop agent stackoverflow_agent
""", "Dropped stackoverflow_agent")

run_query("""
    CREATE AGENT stackoverflow_agent
    USING
        model = {
            "provider": "openai",
            "model_name": "gpt-4.1"
        },
        data = {
            "knowledge_bases": ["mindsdb.kb_stack_faiss"]
        },
        prompt_template = '
            You are a helpful programming assistant. 
            mindsdb.kb_stack_faiss is a knowledge base that contains Stack Overflow questions and answers.
            Use this knowledge to provide accurate, helpful responses to programming questions.
            Include code examples when relevant.
            You must base your answer on the Stack Overflow questions and answers extracted from mindsdb.kb_stack_faiss.
            If you failed to get the results from mindsdb.kb_stack_faiss, answer I could not get the results from mindsdb.kb_stack_faiss.
            Print the chunk ID for each question and answer you based your answer on.
            IMPORTANT: Use a limit of 100 in your query to the knowledge base.
        '
""", "Created stackoverflow_agent")



Dropped stackoverflow_agent
Created stackoverflow_agent

python
# Query the agent
start = time.time()
response = server.query("""
    SELECT answer
    FROM stackoverflow_agent 
    WHERE question = 'Compare JavaScript to TypeScript for building web services'
""").fetch()
print(f"Agent response time: {time.time() - start:.2f} seconds\n")
print(response['answer'].iloc[0])


Agent response time: 63.44 seconds

To compare JavaScript and TypeScript for building web services, let's look at insights from Stack Overflow posts (see chunk IDs for reference):It is the default language for web development, both on the client (browser) and, with Node.js, on the server (870980:Body:1of1:0to133).JavaScript is flexible and widely supported, but its lack of static typing can lead to runtime errors and makes large codebases harder to maintain.While not directly mentioned in the top results, TypeScript is a superset of JavaScript that adds static typing and modern language features. It compiles to JavaScript, so it runs anywhere JavaScript does.TypeScript helps catch errors at compile time, improves code readability, and is especially beneficial for large projects or teams.TypeScript is increasingly popular for the same purpose, as it provides all the benefits of JavaScript plus type safety and better tooling (e.g., autocompletion, refactoring).Good, but less type-awareExcellent (autocompletion, refactor)Slightly higher (due to types)Same as JS, plus TS-specific toolsCan be challenging in large codeEasier in large codebasesFor small projects or rapid prototyping, JavaScript is sufficient and easy to start with.For larger projects, teams, or when maintainability and reliability are priorities, TypeScript is generally preferred.If you want more specific code examples or a deeper dive into either technology, let me know!We've built a complete semantic search system that:Processes 2 million Stack Overflow postsSupports both pgvector and FAISS backendsCombines semantic search with metadata filteringPowers an AI agent for natural language queriesFAISS is much faster than pgvector for pure search queriesMetadata filtering lets you narrow results by tags, scores, datesKnowledge bases abstract complexityâ€”no need to manage embeddings manuallyAgents can leverage knowledge bases for RAG-style applicationsTry different embedding modelsExplore different chunking strategies]]></content:encoded></item><item><title>Day 30: Implementing FinOps in Python - Calculating GenAI Costs per Request</title><link>https://dev.to/ericrodriguez10/day-30-implementing-finops-in-python-calculating-genai-costs-per-request-4k96</link><author>Eric RodrÃ­guez</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 16:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Serverless is cheap, but "pay-per-use" can become a nightmare if you don't track the usage. Today, I added a cost-tracking layer to my Financial Agent.Most developers ignore the usage object returned by LLM APIs. I decided to use it.Python
def calculate_ai_cost(input_tokens, output_tokens):
    # AWS Nova Micro Pricing (us-east-1)
    cost_input = (input_tokens / 1000) * 0.00035
    cost_output = (output_tokens / 1000) * 0.00140
    return round(cost_input + cost_output, 7)Inside my Lambda handler, after every bedrock.invoke_model() call, I extract the token counts and pass them to this calculator. The result is immediately logged to CloudWatch using Structured JSON Logging.I can now query CloudWatch Logs Insights to see exactly how much money I burned today on AI inferences. It brings a level of transparency that is essential for scaling any SaaS product.]]></content:encoded></item><item><title>From Forecast to Flight Planning: Integrating METAR/TAF Data to Minimize Weather-Related Disruptions in Travel &amp; Research Apps</title><link>https://dev.to/skylink_api/from-forecast-to-flight-planning-integrating-metartaf-data-to-minimize-weather-related-2265</link><author>SkyLink API</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 15:56:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Weather disruptions account for 50â€“70% of flight delays, impacting travel apps, logistics tools, hospitality services, and research projects. A sudden fog or thunderstorm can lead to missed connections, frustrated users, or skewed data. The key to mitigation? METAR for current airport conditions and TAF for 24â€“30-hour forecasts.
Integrating these via API enables proactive alerts, rerouting, and better decisions. For small businesses, indie developers, students, and researchers, this means operational continuity without high costs. SkyLink API's endpoints provide raw and decoded data for any ICAO airport, with global coverage, 99.99% uptime, and a free tier (1,000 requests/month) starting at $15.99 for production.
  
  
  Why METAR/TAF Matter More Than General Weather APIs
Unlike consumer APIs like OpenWeather, METAR/TAF are aviation-specific. METAR details wind, visibility (in statute miles), cloud layers (in feet), temperature, pressure, and phenomena like mist (BR) or fog (FG). TAF forecasts evolutions with TEMPO (temporary), BECMG (becoming), and PROB (probability) indicators.
These are tied to ICAO codes, sourced from official observationsâ€”not modelsâ€”ensuring precision for runway impacts. Studies show adverse weather causes ~50% of delays; TAF integration can cut arrival holds by enabling alternates. For travel/hospitality, it minimizes guest waits; for logistics, predicts cargo delays; for research, provides reliable datasets for simulations or papers.
  
  
  Core Use Cases: How Integration Delivers Results

  
  
  Travel App / Hospitality Guest Experience
Users often face fog-induced delays. Poll TAF 12â€“24 hours ahead and METAR near arrival to trigger notifications: "Low visibility at LHR 18:00â€“20:00â€”consider alternate transport." Outcomes: 30â€“50% fewer complaints, higher ratings.
  
  
  Student/Research Aviation Tools
Manual data pulls from NOAA slow studies. API calls for multiple ICAOs feed dashboards or sim plugins, enabling faster prototyping and accurate theses on weather patterns.
  
  
  Small Logistics / Transportation Dashboard
Unexpected precipitation disrupts ETAs. Combine TAF with flight status to auto-adjust schedules and notify stakeholders, improving on-time performance and reducing costs.
Technical Integration Spotlight: SkyLink METAR/TAF Endpoints
SkyLink's GET /v2/weather/metar/{icao} or /v2/weather/taf/{icao} returns structured JSON: raw text, airport name, timestamp, and decoded fields. Benefits include no parsing issues, fast responses (<500ms), and scalable pricing (overage $0.0004/request).
Quick Python example:import requests


headers = {"X-RapidAPI-Key": "YOUR_KEY"}
response = requests.get(
    "https://skylink-api.p.rapidapi.com/v2/weather/taf/KJFK",
    headers=headers
)
taf_data = response.json()
print(taf_data['raw'])          # Full raw TAF
print(taf_data['airport_name']) # John F Kennedy International Airport
# Use to trigger "TEMPO low vis" alert
Test with 1,000 free requests on RapidAPI.Integrating METAR/TAF builds resilience against aviation's top disruptor. SkyLink makes it accessible: accurate, global data at prices for indie devs and small teams.
Start prototyping: Sign up free on RapidAPI. Need integration help? Email support@skylinkapi.comâ€”founder responses in hours. Share your project in comments!]]></content:encoded></item><item><title>Quote by Buy Old GitHub Accounts - Verified &amp; Trusted ...</title><link>https://dev.to/kippyjacobsosm3o/quote-by-buy-old-github-accounts-verified-trusted--58nh</link><author>kippyjacobs</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 15:30:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the ever-evolving scene of innovation and advancement, Buy GitHub AccountsÂ  stands out as a crucial stage for engineers and businesses alike. BuyÂ GitHub AccountsÂ  Whether youâ€™re overseeing ventures, collaborating with groups, or exhibiting your work to potential clients, having a solid nearness on GitHub is fundamental. But what if youâ€™re beginning from scratch? Thatâ€™s where the thought of buying GitHub accounts comes into play. Envision hopping straight into this dynamic community with an account thatâ€™s as of now set up. Sounds engaging, right? At getusasmm, we offer you the opportunity to purchase GitHub accounts custom-made to lift your businessâ€™s online nearness easily. So letâ€™s plunge into why contributing in these accounts can change your methodology and donate you that all-important edge in todayâ€™s competitive showcaseBuyÂ  GitHub Accounts
What is GitHub Account
A GitHub account serves as your individual door to one of the most prevalent stages for designers and tech devotees. It permits clients to have and oversee code storehouses, encouraging collaboration on ventures both huge and small.With a GitHub account, you can make open or private storehouses, share your work with others, and indeed contribute to open-source ventures. This makes it an priceless apparatus in the program advancement lifecycle.Additionally, GitHub gives adaptation control highlights that offer assistance track changes over time. You get experiences into who made what adjustments and whenâ€”essential for any collaborative effort.
Moreover, profiles on GitHub moreover act as advanced portfolios. They grandstand your aptitudes and encounters to potential managers or collaborators in a outwardly engaging way. Having an dynamic nearness upgrades not fair person validity but moreover boosts perceivability inside the designer community.
The Benefits of Purchase GitHub Accounts for Your BusinessBuying GitHub accounts can altogether upgrade your businessâ€™s online nearness. With a solid account, you pick up get to to a endless cluster of advancement instruments and assets that streamline extend management.GitHub serves as an fundamental stage for collaboration among designers. By acquiring an built up account, you tap into existing systems and communities, making it less demanding to interface with potential collaborators or clients.Additionally, having a confirmed GitHub account loans validity to your brand. It illustrates polished skill and skill in the tech spaceâ€”qualities that draw in both ability and partnerships.
Investing in GitHub accounts can moreover spare time. Or maybe than building from scratch, you obtain ready-to-use profiles total with stores exhibiting past work.In todayâ€™s competitive environment, leveraging each asset is vital for victory. Buying GitHub accounts permits businesses to quicken development whereas maximizing their advancement capabilities.
Importance of GitHub Account
A GitHub account is fundamental for advanced designers and businesses. It serves as a stage to exhibit ventures, code storehouses, and collaborative endeavors. Having an account on this well known location not as it were upgrades perceivability but moreover encourages teamwork.With form control highlights, GitHub empowers simple following of changes in code. This is significant for keeping up venture judgment over time. Designers can return to past adaptations if required, guaranteeing that botches are effectively corrected.
Moreover, an dynamicÂ GitHub profileÂ builds validity inside the tech community. Potential bosses or clients frequently see at a developerâ€™s commitments when assessing abilities. An noteworthy portfolio can open entryways to unused openings and collaborations.Â It cultivates learning through community interaction. Clients can tap into tremendous assets from individual designers around the world, picking up experiences and moving forward their coding hones along the way.Buy GitHub Accounts
Why Ought to You Purchase GitHub Account For Your BusinessAcquiring a GitHub account can altogether hoist your businessâ€™s improvement endeavors. It gives get to to a endless community of designers and assets that can improve collaboration.With an built up GitHub account, you pick up validity in the tech industry. This believe figure is vital for drawing in ability and potential partnerships.You moreover open openings to grandstand your ventures viably. A well-maintained profile permits clients to see your work firsthand, loaning specialist to your brand image.
Furthermore, buying an matured GitHub account offers quick benefits like existing adherents and stores. These viewpoints grant you a head begin on organizing inside the designer community.Having get to to premium highlights or instruments related with acquired accounts can streamline venture administration. This effectiveness interprets into quicker conveyance times and moved forward efficiency over groups.
How can I Purchase GitHub Account
Buying a GitHub account is direct and can be done in fair a few steps.First, you require to distinguish a solid supplier like getusasmm. Inquire about their offerings, guaranteeing they have positive surveys and a demonstrated track record of conveying quality accounts.Once youâ€™ve found the right seller, browse their determination to discover an account that suits your needs. Whether youâ€™re looking for an matured account with history or a new one, choices are ordinarily available.After choosing the wanted account, take after the obtaining handle sketched out on the site. This ordinarily includes making an arrange, making installment through secure strategies, and giving any vital details.Ensure you get all login data expeditiously after buy. Great merchants will too give back if any issues emerge amid setup or usage.
Why are we the best to Purchase GitHub Account
At GetUSAMM, we pride ourselves on giving quality GitHub accounts custom fitted to your needs. Each account is carefully verified for realness and unwavering quality, guaranteeing you get the best value.Our client back group is continuously prepared to help you with any request. Whether youâ€™re modern toÂ GitHubÂ or a prepared client, our specialists can direct you through the prepare seamlessly.We get it that believe is key when acquiring online. Thatâ€™s why we offer secure exchanges and protection assurance for all our clients.Additionally, our competitive estimating guarantees that you get premium accounts without breaking the bank. With adaptable choices accessible, businesses of all sizes can advantage from utilizing GitHub effectively.Join endless fulfilled clients who have chosen us as their go-to source for buying GitHub accounts. Encounter the distinction in quality and benefit that sets us separated from competitors.
Worldwide location for deal of GitHub accountsThe request for GitHub accounts is taking off over the globe. This makes it fundamental to discover a dependable stage that offers these accounts for sale.Getusasmm stands out as a head around the world location devoted to giving true GitHub accounts. Clients can effectively explore through different alternatives custom fitted to their particular needs, whether theyâ€™re looking for unused or matured profiles.Aged GitHub accounts are especially profitable due to set up validity and movement history. They give moment authenticity, which can altogether improve collaboration openings in projects.With an natural obtaining handle and responsive client back, obtaining a GitHub account has never been less complex. Getusasmm guarantees secure exchanges, making it your go-to choice in this market.Investing here implies picking up get to not fair to an account but too opening potential development roads for people and businesses alike.
Here can I purchase an matured GitHub account?
When looking for matured GitHub accounts, itâ€™s fundamental to center on solid sources. The right commercial center can make a noteworthy contrast in your experience.Look for stages that specialize in social media and online account deals. These locales frequently have a determination of confirmed matured GitHub accounts prepared for purchase.Ensure the supplier has great audits and offers client bolster. This way, you can inquire questions or address any concerns some time recently making your decision.Additionally, consider checking gatherings or communities where designers assemble. They might share bits of knowledge on trustworthy dealers who bargain particularly with GitHub accounts.Aged accounts regularly come with benefits like built up validity and past movement. Finding the right vender is vital to opening these focal points effectively.
How do I rapidly purchase GitHub accounts? from getusasmmÂ Purchasing GitHub accounts from getusasmm is a direct prepare. To begin with, visit their site to investigate different account choices available.Browse through the distinctive sorts of accounts recorded. You can discover both modern and matured alternatives custom-made to your needs. Each posting incorporates fundamental points of interest to offer assistance you make an educated decision.Once youâ€™ve chosen the account that suits you best, press on it for more data. Include it to your cart and continue to checkout. The installment strategies are user-friendly and secure, guaranteeing your exchange is safe.After completing the buy, youâ€™ll get your login accreditations expeditiously through mail. This speedy conveyance permits you quick get to to begin utilizing GitHub for your ventures without delay.With fair a few clicks, youâ€™re prepared to improve your improvement travel with a solid GitHub account from getusasmm!
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ We are available online 24/7
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Telegram: @getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ WhatsApp: +1 (314) 203-4162@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Come now our company:https://getusasmm.com/product/buy-github-accounts/
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²Investing in GitHub accounts can be a game-changer for your trade. The potential to get to a broader group of onlookers and collaborate consistently with designers is immense.When you select to purchase from getusasmm, youâ€™re not fair acquiring an account; youâ€™re obtaining openings. Each account comes stuffed with highlights that can lift your projects.With our broad choice and dependable benefit, exploring the world of GitHub gets to be easy. Youâ€™ll discover what you require without the normal hassles related with online purchases.The prepare is direct. Fair visit our stage, investigate the alternatives, and make your choice. Your another huge venture is standing by on GitHub!Empower your advancement travel nowadays by leveraging quality accounts custom-made for development and victory.]]></content:encoded></item><item><title>Slashing torch.compile Warmup &amp; LoRA Swapping Times with Pruna</title><link>https://dev.to/pruna-ai/slashing-torchcompile-warmup-lora-swapping-times-with-pruna-1gei</link><author>Sara Han</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 15:07:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[PyTorch introduced , a powerful feature that significantly boosts performance by compiling the models. However, it comes with a catch: the first run is very slow. That warmup delay  can be a drag on development iteration and can lead to slower cold starts in production. If youâ€™ve ever swapped a LoRA or made a small model change, youâ€™ve probably noticed that frustrating pause before things get moving again. But what if you could dramatically reduce, or even eliminate, these warmup delays?In this post, we'll dive into two practical techniques, powered by Pruna, to mitigate warmup times. We'll show you how to:Using Pruna's portable compilation feature, eliminate the initial model warmup when deploying or reloading a model on a new machine (with identical hardware).Achieve zero warmup when switching LoRAs (Low-Rank Adaptations) on an already optimized model.Get ready to reclaim those precious seconds (or even minutes!) and make your  experience smoother than ever.
  
  
  The Challenge: Understanding  Warmup
Before we dive into the solutions, let's briefly touch upon why  has a warmup phase. When you first invoke a model compiled with , several things happen under the hood. PyTorch needs to:Capture the computational graph: It traces the execution of your model to understand its structure.Perform graph optimizations: The captured graph is then optimized for better performance.Detect and fuse operators: The backend (such as Inductor) identifies which operations can be combined for faster execution.: Optimized code (often CUDA kernels for GPUs or efficient CPU code) is generated by the chosen backend (like Inductor).: This generated code is then compiled into executable machine instructions.This entire process, especially the code generation and compilation steps, can take a noticeable amount of time, ranging from seconds to minutes, depending on the model's complexity and the hardware. While this is a one-time cost for a given model shape and hardware (as the compiled artifacts are cached), it can be disruptive:: When a new instance of an application starts (e.g., a serverless function or a new pod in Kubernetes), the first request might experience this long warmup, leading to poor user experience.: If you compile a model on one machine and then try to run it on another (even with identical hardware), the cache might not be directly usable, leading to another full warmup.: Swapping LoRAs or other adapters can alter the model graph triggering recompilation.: Waiting for recompilation after minor code changes or restarting a kernel slows the development cycle.Pruna offers elegant ways to mitigate these issues, as we'll see next.
  
  
  Use Case 1: Eliminating Initial Warmup with Pruna's Portable Compilation
Traditionally, running a compiled model on a new machine triggers a full compilation warmup, even if the hardware is identical. This can slow down processes, especially when deploying models to production or sharing them with others.Pruna makes compilation portable. It saves the required artifacts so they can be easily packaged with your model and reused on another machine (with the same hardware architecture and CUDA drivers) without needing to recompile from scratch. That way, the model will run fast right from the first inference.: Skip the first-run delay when deploying pre-compiled models to production servers, especially serverless instances.: Share ready-to-run models with your team.: Speed up CI/CD by avoiding repeated compilation.
  
  
  How-to Use Prunaâ€™s Portable Compilation
Let's walk through how to use this feature:Load your model as normally: In our example, we use a Stable Diffusion pipeline from Diffusers.Configure Pruna for Portable Compilation: This is where the magic happens. Create a  object and configure   to be portable.: Apply the configuration using .: Run your model for the first time trigger compilation process, including the warmup. After that, just save your Pruna-smashed model, and itâ€™ll be ready to use on any other machine.

  
  
  Use case 2: Zero Warmup for LoRA Switching with Diffusers Hotswap and Pruna () Compatibility
Low-Rank Adaptation (LoRA) is a game-changer for efficiently fine-tuning large models. It allows for quick adaptation by training only a small set of parameters.A powerful workflow involves dynamically switching between different LoRAs on a base model to change its output on the flyâ€”for instance, altering image styles in a generative model. However, a challenge arises when you combine it with compilation. Every LoRA swap can look like a graph changeâ€”triggering a long recompilation and wiping out the speed advantage.While Diffusers handles the mechanics of LoRA hotswapping, using Pruna with and leveraging one of its cachers ensures that these Diffusers-driven LoRA swaps are efficient and don't cause recompilation warmups after the initial model compilation.With Pruna and Diffusers together, you get flexible LoRA adaptation and high-performance execution with no warmup delays.: Serve models that adapt to diverse user inputs by loading different LoRAs or applications requiring rapid switching between LoRA-defined styles or functionalities (e.g., in an image generation UI), without the latency of recompilation.Efficient experimentation: Test multiple LoRAs quickly without waiting for recompiles.
  
  
  How-to Leverage Diffusers Hotswap with Pruna for Zero WarmupLet's walk through how this works:Load the Base Model and Enable Diffusers LoRA Hotswapping.: Configure  and enable a cacher. In this example, we will be using the  cacher, but others also maintain compatibility.: Apply the configuration using .: Run the model for the first time triggering the  warmup for the base model and the current LoRA. Then, youâ€™ll be ready to hotswap to a new LoRA

  
  
  Comparing the Solutions: Portable Compilation vs. Pruna Cacher Compatibility
While we separately presented these use cases, they can be easily combined:Use  to create a base smashed model (perhaps with a default LoRA and apply Pruna optimization that loads quickly on new instances.Once loaded, prunaâ€™s compatibility with hot-swapping would ensure that any subsequent LoRA hot swaps (managed by Diffusers) on that instance are also free of  warmup delays.This combined approach would give you a fast cold start  adapter switching.
  
  
  Conclusions: Reclaim Your Time with Pruna
The  warmup can slow down production workflows for cold starts and adapter switching. Pruna addresses these challenges with two key features: (torch_compile_make_portable=True) removes first-run warmup when deploying to identical hardware, enabling immediate peak performance.Diffusers' LoRA hotswapping with  and a  enables instant LoRA switching without recompilation delays.We hope this guide helps you optimize your  workflows. Happy coding!Enjoy the Quality and Efficiency!Compress your own models with Pruna and give us a â­ to show your support!Join the conversation and stay updated in our Discord community.]]></content:encoded></item><item><title>ðŸŽ¬ Behavior_Recognition</title><link>https://dev.to/stklen/behaviorrecognition-18jc</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 15:00:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Washin Village AI Director Tech Notes #5
  
  
  ðŸŽ¯ From "Who Is This" to "What Are They Doing"
After AI can identify whether it's Jelly or Ariel, what's next?: Teaching AI not just to identify animals, but to understand what they're doing.
  
  
  ðŸ“Š Behavior Categories We Defined
Chasing, playing with toys
  
  
  Approach 1: Single-Frame Classification
Classify behavior for each image frame.: Simple, fast: Can't judge continuous actions ("walking" vs "stopped")
  
  
  Approach 2: Sequence Analysis
Analyze multiple consecutive frames to understand dynamic behavior.: 200+ images per behavior category: Using Label Studio: Ensure labeling consistency
  
  
  1. Automatic Video Classification
Input Video â†’ Behavior Recognition â†’ Auto-tagging
                       â†“
              "Jelly sleeping" "Dollar eating"
Automatically cut highlights based on behavior:"Playing" clips â†’ For funny videos"Resting" clips â†’ For relaxing videosLong-term tracking of animal behavior patterns:Decreased eating frequency â†’ Possible illnessReduced activity â†’ Needs attentionClear behavior definitions: Vague definitions lead to inconsistent labeling: Keep sample counts similar across categories: Single-frame has limits; sequence analysis is more accurate: Include samples from different lighting and angles: Distinguish "fast running" from "slow walking": Two cats playing together: Detect fighting or illness signsWashin Village ðŸ¡ by AI Director]]></content:encoded></item><item><title>Skip the 4-year wait</title><link>https://dev.to/iuzair/skip-the-4-year-wait-19hf</link><author>Uzair</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 14:54:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Skip the 4-year wait: upgrade your tech career faster Thinking about traditional study at a place like Ural Federal University, with years of classes, high tuition, and limited hands-on work? There is another path. AlNafi's UK-accredited Diploma in DevOps and Cloud Advancement (EduQual Level 4 - equivalent to the first year of a bachelor's degree) gives you practical DevOps and cloud skills with an offensive security focus, fully online and self-paced. Instead of paying for long, campus-based study, you get industry-focused labs, AI-supported learning, and a direct pathway toward higher EduQual levels and global MSc options. You keep your job, study from anywhere, and build a portfolio that hiring managers actually care about, while saving time and money compared to a traditional 4-year route. Start your DevOps and cloud journey today]]></content:encoded></item><item><title>Python OOP Prerequisites: The Essential Checklist for Beginners</title><link>https://dev.to/shameel/python-oop-prerequisites-the-essential-checklist-for-beginners-236e</link><author>Shameel Uddin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 14:20:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Before diving head-first into the world of Object-Oriented Programming (OOP), you need a solid grasp of a few "bread and butter" Python concepts. Think of it like building a house: you can't install the roof (OOP) until youâ€™ve laid the foundation (Basics).Donâ€™t worry this series is designed for beginners. You donâ€™t need to be a senior dev to get started, but having these few tools in your belt will make the transition to OOP feel like a breeze rather than a climb.
  
  
  Core Python Basics You Should Know
To get the most out of this OOP series, ensure you're comfortable with the following three pillars:
  
  
  1. Variables and Data Types
In OOP, we store data inside "Objects." To do that, you must understand how Python handles data. You should be familiar with: For names and descriptions. For counts, ages, or prices. For "True/False" logic (essential for state management). For storing collections of objects. In OOP, these variables will eventually become Attributes the characteristics that define your objects.Functions are the "actions" of your code. Before moving to OOP, you should know:How to define a function ().How to pass arguments (positional and keyword).How to return values to use elsewhere. In the OOP world, functions living inside a class are called Methods. If you can write a function, youâ€™re already 80% of the way to writing a class method.
  
  
  3. Working with Dictionaries
You should understand how Key-Value pairs work because:Dictionaries represent structured data.Internally, Python actually uses dictionaries to store object attributes.If you can pull a value from a dictionary using a key, you'll find the logic of accessing object properties very familiar.
  
  
  What You Do NOT Need to Know
Itâ€™s easy to feel overwhelmed by the vast Python ecosystem. You do not need to master these before starting this series:Advanced Decorators or Generators: Weâ€™ll keep things simple.Web Frameworks: No Django or FastAPI knowledge is required.Database Management: We wonâ€™t be touching SQL or NoSQL in the beginning.Our focus is strictly on  step by step.Before we write our first class, make sure your environment is ready:Python Installed: Ensure you have Python 3.x on your machine.Terminal Access: You should be comfortable running a script via . Code Editor: Use whatever you like (VS Code, PyCharm, or even a simple text editor). No complex setup or heavy IDE configuration is required.

  
  
  How to Master This Series
To truly "level up" your skills, don't just be a spectator. Engage with the content:Follow the Sequence: Concepts build on each other. Don't skip ahead!The "Type-Along" Rule: Never just read the code. Type it out. Muscle memory is a real thing in programming.Break Things: Change a value, delete a colon, or rename a variable. Seeing how the code breaks is the fastest way to learn how to fix it.We believe in Hands-on Learning. To support you:All code examples are available on our GitHub repository.Links to the code are provided in our YouTube series descriptions.Clone it, fork it, or copy it just make sure you practice it.Object Oriented Programming isn't just a syntax change; itâ€™s a mindset shift. It will help you write cleaner, reusable, and more professional code. Stay consistent, keep practicing, and don't be afraid to ask questions.]]></content:encoded></item><item><title>Linux CLI for extracting archives inside a bubblewrap sandbox (alpha)</title><link>https://dev.to/chechelpo/linux-cli-for-extracting-archives-inside-a-bubblewrap-sandbox-alpha-20gp</link><author>Chechelpo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 14:01:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
Built a small CLI tool for myself to extract archives inside bwrap sandbox, mainly to avoid accidental path traversal / weird archive behaviour. Published the alpha in case anyone else is interested.
Requires: (Installed by the user, not bundled)bubblewrap: for the sandbox bsdtar(libarchive): for zip, tar 
This is my first published tool and very much alpha-quality, so there will be rough edges. Feedback, bug reports, and design criticism are welcome.
]]></content:encoded></item><item><title>How Long Does It Take to Learn Python?</title><link>https://realpython.com/how-long-does-it-take-to-learn-python/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[Have you read blog posts that claim you can learn Python in  and quickly secure a high-paying developer job? Thatâ€™s an unlikely scenario and doesnâ€™t help you prepare for a steady learning marathon. So, how long does it  take to learn Python, and is it worth your time investment?By the end of this guide, youâ€™ll understand that:Most beginners can learn core Python fundamentals in about 2 to 6 months with consistent practice.You can write a tiny script in days or weeks, but real confidence comes from .Becoming  often takes , depending on your background and target role. because the ecosystem and specializations keep growing.The short answer for how long it takes to learn Python depends on your goals, time budget, and the level youâ€™re aiming for.Click here to download a free PDF guide that breaks down how long it takes to learn Python and what factors affect your timeline. Test your knowledge with our interactive â€œPython Skill Testâ€ quiz. Youâ€™ll receive a score upon completion to help you track your learning progress:Test your Python knowledge in a skills quiz with basic to advanced questions. Are you a Novice, Intermediate, Proficient, or Expert?How Long Does It Take to Learn Python Basics?Python is beginner-friendly, and you can start writing simple programs in just a few days. But reaching the  stage still takes consistent practice because youâ€™re learning both the language itself and how to think like a programmer.The following timeline shows how long it typically takes to learn Python basics based on how much time you can practice each week:Typical timeline for basicsRealistic pace for busy adultsConsistent focus and fast feedbackThese ranges assume about . If you add a sixth day, youâ€™ll likely land toward the faster end of each range.Youâ€™ll get better results if you use this table as a planning guide. Donâ€™t think of it as rigid deadlinesâ€”your learning pace depends on many factors. For example, if you already know another programming language, then you can usually move faster. If youâ€™re brand-new to coding, then expect to be at the slower end of each range.As a general guideline, many beginners reach the basics in about 2 to 6 months with steady practice. If youâ€™re ready to fast-track your learning with an expert-guided small cohort course that gives you live guidance and accountability, then check out Real Pythonâ€™s live courses!With a focused schedule of around four hours per day, five days per week, you can often reach this stage in roughly 6 to 10 weeks, assuming youâ€™re writing and debugging code most sessions. By then, you should be able to finish several small projects on your own.When you read online that someone learned Python quickly, theyâ€™re probably talking about this basics stage. And indeed, with the right mix of dedication, circumstances, and practice, learning Python basics can happen pretty fast!Before you go ahead and lock in a timeline, take a moment to clarify for yourself  you want to learn Python. Understanding your motivation for learning Python will help along the way.Learning Python means  than just learning the Python programming language. You need to know more than just the specifics of a single programming language to do something useful with your programming skills. At the same time, you donâ€™t need to understand every single aspect of Python to be productive.Learning Python is about learning how to accomplish practical tasks with Python programming. Itâ€™s about having a skill set that you can use to build projects for yourself or an employer.As your next step, write down your personal goal for learning Python. Always keep that goal in mind throughout your learning journey. Your goal shapes what you need to learn and how quickly youâ€™ll progress.If youâ€™re starting from zero and can spend about 5 to 10 hours per week, the following plan keeps you moving without becoming overwhelming:Aim to finish at least one small project by the end of the month. The project matters more than completing every tutorial or task on your checklist.]]></content:encoded></item><item><title>Gemini 3 Flash: Agentic Vision in LINE Bot - AI Image Annotation and More</title><link>https://dev.to/gde/gemini-3-flash-agentic-vision-in-line-bot-ai-image-annotation-and-more-53lb</link><author>Evan Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:55:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This made me think of an interesting use case:A user sends a photo and says, "Help me mark the coffee," and the AI not only replies with a text description but also draws a bounding box and annotates it on the image, then sends the annotated image back to LINE.This article documents the complete process of implementing this function, including the pitfalls and solutions.Traditional image analysis is : you give the model an image, and the model returns a text description.Agentic Vision turns image understanding into an active investigation process, using a Think â†’ Act â†’ Observe cycle:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agentic Vision Process â”‚
â”‚ â”‚
â”‚ 1. Think - Analyze the image and plan how to investigate further â”‚
â”‚ 2. Act - Write Python code (crop, enlarge, annotate, calculate) â”‚
â”‚ 3. Observe - Observe the code execution results (including the generated annotated image) â”‚
â”‚ 4. Repeat the above steps until the analysis is complete â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

: :  tool â€” allows the model to write and execute Python code: In addition to text analysis, it can also return annotated images generated by the model# Enable Agentic Vision API call
response = client.models.generate_content(
    model="gemini-3-flash-preview",
    contents=[image_part, "Help me mark the coffee"],
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        thinking_config=types.ThinkingConfig(thinkingBudget=2048),
    )
)

# Response contains multiple parts: text, code, execution results, annotated images
for part in response.candidates[0].content.parts:
    if part.text: # Text analysis
    if part.executable_code: # Python code written by the model
    if part.code_execution_result: # Code execution results
    if part.as_image(): # Generated annotated image!

Instead of directly analyzing the image upon receiving it, it's changed to let the user choose a mode first:User sends an image
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ“· Image received, please select an analysis method: â”‚
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Recognize Image â”‚ â”‚ Agentic Vision â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ (Quick Reply Buttons) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ â”‚
     â–¼ â–¼
 gemini-2.5-flash User inputs instructions
 Directly returns a text description "Help me mark the coffee"
                         â”‚
                         â–¼
                  gemini-3-flash-preview
                  + code_execution
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â–¼ â–¼
               Text Analysis Annotated Image
               (Text) (Image)
                    â”‚ â”‚
                    â–¼ â–¼
               LINE TextMsg + ImageSendMessage

Agentic Vision requires the user to provide  (e.g., "Mark everyone," "Count how many cats"), unlike general recognition which only needs to "describe the image." Therefore, after selecting Agentic Vision, the user is first asked to input their desired goal.
  
  
  1. Image Temporary Storage Mechanism
Because LINE's Quick Reply is asynchronous (user clicks a button to trigger PostbackEvent), the image needs to be temporarily stored:# main.py
image_temp_store: Dict[str, bytes] = {} # Temporary image storage (user_id â†’ bytes)
pending_agentic_vision: Dict[str, bool] = {} # Waiting for user to input instructions

 Receive image â†’ store in image_temp_store[user_id] User clicks "Agentic Vision" â†’ set pending_agentic_vision[user_id] = True User inputs text â†’ detect pending state, retrieve image + text and send them for analysis
  
  
  2. Quick Reply Implementation
Use LINE SDK's , consistent with the existing YouTube summary and location search Quick Reply modes:quick_reply_buttons = QuickReply(
    items=[
        QuickReplyButton(
            action=PostbackAction(
                label="Recognize Image",
                data=json.dumps({"action": "image_analyze", "mode": "recognize"}),
                display_text="Recognize Image"
            )
        ),
        QuickReplyButton(
            action=PostbackAction(
                label="Agentic Vision",
                data=json.dumps({"action": "image_analyze", "mode": "agentic_vision"}),
                display_text="Agentic Vision"
            )
        ),
    ]
)


  
  
  3. Agentic Vision Analysis Core
# tools/summarizer.py
def analyze_image_agentic(image_data: bytes, prompt: str) -> dict:
    client = _get_vertex_client()

    contents = [
        types.Part.from_text(text=prompt),
        types.Part.from_bytes(data=image_data, mime_type="image/png")
    ]

    response = client.models.generate_content(
        model="gemini-3-flash-preview",
        contents=contents,
        config=types.GenerateContentConfig(
            temperature=0.5,
            max_output_tokens=4096,
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
            thinking_config=types.ThinkingConfig(thinkingBudget=2048),
        )
    )

    result_parts = []
    generated_images = []

    for part in response.candidates[0].content.parts:
        if hasattr(part, 'thought') and part.thought:
            continue # Skip thinking parts
        if part.text is not None:
            result_parts.append(part.text)
        if part.code_execution_result is not None:
            result_parts.append(f"[Code Output]: {part.code_execution_result.output}")
        # Extract the annotated images generated by the model
        img = part.as_image()
        if img is not None:
            generated_images.append(img.image_bytes)

    return {
        "status": "success",
        "analysis": "\n".join(result_parts),
        "images": generated_images # Annotated image bytes
    }


  
  
  4. Image Return Mechanism
LINE's  requires a public HTTPS URL. Because we are deployed on Cloud Run (which is inherently public HTTPS), we directly add an image serving endpoint to FastAPI:# Temporary storage of annotated images (UUID â†’ bytes, 5 minutes TTL)
annotated_image_store: Dict[str, dict] = {}

@app.get("/images/{image_id}")
def serve_annotated_image(image_id: str):
    """Provide temporary annotated images for LINE to download"""
    entry = annotated_image_store.get(image_id)
    if not entry:
        raise HTTPException(status_code=404)
    if time.time() - entry["created_at"] > 300: # 5 minutes expired
        annotated_image_store.pop(image_id, None)
        raise HTTPException(status_code=404)
    return Response(content=entry["data"], media_type="image/png")

Automatically detect the App's base URL (from the webhook request headers):@app.post("/")
async def handle_webhook_callback(request: Request):
    global app_base_url
    if not app_base_url:
        forwarded_proto = request.headers.get('x-forwarded-proto', 'https')
        host = request.headers.get('x-forwarded-host') or request.headers.get('host', '')
        if host:
            app_base_url = f"{forwarded_proto}://{host}"

Finally, combine into :def _create_image_send_message(image_bytes: bytes):
    image_id = store_annotated_image(image_bytes)
    image_url = f"{app_base_url}/images/{image_id}"
    return ImageSendMessage(
        original_content_url=image_url,
        preview_image_url=image_url,
    )


  
  
  Pitfall 1:  Does Not Exist
ERROR: Error analyzing image: from_image_bytes

: There is types.Part.from_image_bytes() method in the  SDK, the correct one is .# âŒ Incorrect
types.Part.from_image_bytes(data=image_data, mime_type="image/png")

# âœ… Correct
types.Part.from_bytes(data=image_data, mime_type="image/png")


  
  
  Pitfall 2:  enum Does Not Exist
ERROR: module 'google.genai.types' has no attribute 'ThinkingLevel'

:  in  only supports  (integer), and does not support the  enum. Context7 and the examples in the official documentation are based on a newer version of the SDK.# âŒ Does not exist in v1.49.0
types.ThinkingConfig(thinking_level=types.ThinkingLevel.MEDIUM)

# âœ… v1.49.0 supported method
types.ThinkingConfig(thinkingBudget=2048)

: AI-generated code examples may be based on newer or older SDK versions, always use python -c "help(types.ThinkingConfig)" to confirm the actual available parameters.
  
  
  Pitfall 3: Incomplete Image Recognition Results
:  enables thinking by default, and thinking tokens will consume the quota of . Originally set , and thinking used up a large portion, the actual reply was truncated.# âŒ Before: thinking consumed most of the token quota
config=types.GenerateContentConfig(
    max_output_tokens=2048,
)

# âœ… After: Disable thinking + increase token quota
config=types.GenerateContentConfig(
    max_output_tokens=8192,
    thinking_config=types.ThinkingConfig(thinkingBudget=0), # Disable thinking
)

: For simple image descriptions, thinking is an unnecessary overhead.  can disable thinking, allowing all tokens to be used for the reply.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ File â”‚ Modification Content â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ main.py â”‚ Quick Reply process, image temporary storage, pending state management, â”‚
â”‚ â”‚ image serving endpoint, ImageSendMessage return â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ tools/summarizer.py â”‚ Added analyze_image_agentic(), corrected from_bytes, â”‚
â”‚ â”‚ Corrected ThinkingConfig, disabled thinking for image recognition â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ agents/vision_agent.pyâ”‚ Added analyze_agentic() method â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ agents/orchestrator.pyâ”‚ Added process_image_agentic() routing method â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

The original VisionAgent only had one path, now it becomes:LINE Image Message
     â”‚
     â–¼
handle_image_message()
     â”‚
     â”œâ”€â”€ image_temp_store[user_id] = image_bytes
     â”‚
     â–¼
Quick Reply: "Recognize Image" / "Agentic Vision"
     â”‚ â”‚
     â–¼ â–¼
handle_image_analyze_ pending_agentic_vision[user_id] = True
postback() â”‚
     â”‚ â–¼
     â”‚ User inputs text instructions
     â”‚ â”‚
     â”‚ â–¼
     â”‚ handle_agentic_vision_with_prompt()
     â”‚ â”‚
     â–¼ â–¼
orchestrator orchestrator
.process_image() .process_image_agentic(prompt=user instructions)
     â”‚ â”‚
     â–¼ â–¼
VisionAgent.analyze() VisionAgent.analyze_agentic()
     â”‚ â”‚
     â–¼ â–¼
analyze_image() analyze_image_agentic()
gemini-2.5-flash gemini-3-flash-preview
thinkingBudget=0 + code_execution
                           + thinkingBudget=2048
     â”‚ â”‚
     â–¼ â”œâ”€â”€ Text analysis â†’ TextSendMessage
TextSendMessage â”œâ”€â”€ Annotated image â†’ /images/{uuid} â†’ ImageSendMessage
                               â””â”€â”€ push_message([text, image])


  
  
  1. SDK Version Differences are the Biggest Pitfall
The most time-consuming part of this development was not the functional design, but the SDK version differences. The API of  changes frequently: â†’  (method name changed) enum does not exist in v1.49.0 (requires  integer)  The impact of  on  is not documented: Before development, run  to confirm the version, and then use  to confirm the actually available API.
  
  
  2. Limitations of LINE Bot Image Returns
LINE's  requires the image to be a public HTTPS URL, and cannot directly transmit bytes. Solutions:Requires bucket and permission settingsFastAPI endpoint serves itselfSimple, no external services requiredDisappears after restart, memory usageI chose the  solution because:  Cloud Run itself is public HTTPS  Annotated images only need to exist briefly (5 minutes TTL)  No need for additional GCS bucket settings
  
  
  3. Thinking is a Double-Edged Sword
 enables thinking by default, which is helpful for complex reasoning, but is a burden for simple image descriptions:  Consumes  quota: Disable thinking for simple tasks (), and only enable it for complex Agentic Vision.
  
  
  4. Trade-offs in State Management
Agentic Vision requires two-step interaction (select mode â†’ input instructions), which introduces state management:image_temp_store: Dict[str, bytes] = {} # Image temporary storage
pending_agentic_vision: Dict[str, bool] = {} # Waiting for instructions

Using an in-memory dict is the simplest, but there is a risk: Cloud Run may restart between two requests. This is acceptable for a personal Bot, but if you want to make it a product-level service, you should switch to Redis or Firestore.]]></content:encoded></item><item><title>Professional Streamlit Styling with CSS and st_yled</title><link>https://dev.to/jonathanalles/professional-streamlit-styling-with-css-and-styled-45o6</link><author>Jonathan Alles</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:48:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Streamlit is an effective tool for building data apps, dashboards, and prototypes in Python. While basic theme properties like primary colors and fonts can be set, customizing specific components often requires additional techniques.This post covers two approaches for styling Streamlit apps:Targeting components with CSS using the  attributeA Python-centric alternative using the  package
  
  
  Styling with CSS and Component Keys
For components that accept a  attribute, Streamlit assigns a corresponding CSS class (). This can be used to apply custom CSS to individual elements.
  
  
  Example 1: Container with Background and Padding
st.html("""
    <style>
    .st-key-my-container {
        background-color: #F6F6F6;
        padding: 16px;
    }
    </style>
""")

with st.container(key="my-container"):
    st.write("This container has a custom background and border.")
In this case the CSS class  is automatically generated by Streamlit based on the component . The containerâ€™s background and padding are adjusted via CSS.
  
  
  Example 2: Button with a Custom Border
st.html("""
    <style>
    .st-key-my-button button {
        border: 3px solid #000000;
    }
    </style>
""")

st.button("Click Me", key="my-button", type="primary")
This snippet inserts a  block that targets , allowing customization of the buttonâ€™s border. The downside is evident: Each component requires different elements to be targeted with the right CSS selectors.These techniques make it possible to customize the appearance of specific components without modifying global theme settings.An alternative approach is to use the package, which allows style properties to be specified directly as arguments in Python.The st_yled philosophy is simple:Write styles in Python right next to your components, no CSSThe approach is to , call  once, then use prefixed components to apply style properties, whenever necessary â€” while using standard Streamlit components and functions.Instead of styling via CSS, you just pass style properties as arguments to components prefixed with  or  .
  
  
  Example 1: Container with Style Properties
import st_yled as sty

sty.init()

with sty.container(width='content', padding=16, background_color='#F6F6F6'):
    st.write("Hello from Container!")
Here, style properties such as and are passed directly to the container component in Python, without requiring explicit CSS.
  
  
  Example 2: Button with Custom Styling
sty.button(
    "Click Me",
    type="primary",
    border_color='#000000',
    border_style='solid',
    border_width=3
)
This example shows how a button can be styled with border properties through the API. The results are identical to the custom CSS styling in the examples above.
  
  
  Trying new Styles Using st_yled Studio
st_yled Studio is a companion web application built in Streamlit that allows users to interactively explore possible style customizations for common components.Users can select a component, adjust style parameters, and then export either:Python code to insert into your Streamlit appA CSS file to consistently apply styling to all components (this file is automatically loaded by st_yled)This tool can aid in developing consistent visual designs, branding or testing styles before integrating them into code.Post you ideas for new styling options and components below!]]></content:encoded></item><item><title>PyCharm</title><link></link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 13:40:52 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source></item><item><title>Building a Python Puzzle Studio with Tkinter: SixSeven Studio (Step-by-Step)</title><link>https://dev.to/matetechnologie/building-a-python-puzzle-studio-with-tkinter-sixseven-studio-step-by-step-56kf</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:16:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build SixSeven Studio â€” a desktop puzzle generator written in Python.By the end, youâ€™ll have an app that:Displays a modern Tkinter GUIExports layered SVG filesSaves and loads puzzles as JSONThis guide is beginner-friendly and breaks everything into small steps.Step 1 â€” Install Dependenciespip install ttkbootstrap svgwrite python-barcode pillowttkbootstrap â†’ modern Tkinter UIpython-barcode â†’ barcode imagesStep 2 â€” Create Your Project Fileimport tkinter as tk
from tkinter import filedialog, messagebox, colorchooser
import ttkbootstrap as tb
from pathlib import Path
import random
import svgwrite
from barcode import EAN13, Code128
from barcode.writer import ImageWriter
import json
import os
Each module handles a different feature:barcode â†’ barcode creationStep 3 â€” Create the Main Application Classclass SixSevenStudioV5:
    def __init__(self):
        self.APP_NAME = "SixSeven Studio"
        self.APP_VERSION = "v5"

        self.root = tk.Tk()
        tb.Style(theme="darkly")

        self.root.title(f"{self.APP_NAME} {self.APP_VERSION}")
        self.root.geometry("1100x750")
Step 4 â€” App State Variablesself.style = tk.StringVar(value="Default")
self.puzzle = tk.StringVar(value="Futoshiki")
self.count = tk.IntVar(value=1)
self.grid_size = tk.IntVar(value=6)

self.bg_color = tk.StringVar(value="#222222")
self.accent_color = tk.StringVar(value="#4caf50")
self.font_size = tk.IntVar(value=24)

self.barcode_type = tk.StringVar(value="EAN13")
self.grid = []
These variables connect your UI controls to logic.Step 5 â€” Build the User Interfacedef build_ui(self):
    tb.Label(self.root, text=self.APP_NAME,
             font=("Segoe UI", 22, "bold")).pack(pady=10)
This shows the app title.opts = tb.Labelframe(self.root, text="Options", padding=10)
opts.pack(fill="x", padx=10)
tb.Label(opts, text="Puzzle Type:").pack(side="left")
tb.Combobox(
    opts,
    values=["Futoshiki","Arukone","Hidato","Tents & Trees","No-Four-In-Row"],
    textvariable=self.puzzle,
    width=20
).pack(side="left", padx=5)

Control Buttons
ctrl = tb.Frame(self.root)
ctrl.pack(fill="x", padx=10, pady=10)

tb.Button(ctrl, text="Generate",
          command=self.generate).pack(side="left")

tb.Button(ctrl, text="Export SVG",
          command=self.export_svg).pack(side="left", padx=5)
Each button calls a method.Step 6 â€” Displaying the Puzzle Gridself.grid_frame = tb.Labelframe(self.root, text="Puzzle Grid", padding=10)
self.grid_frame.pack(fill="both", expand=True)
def display_grid(self):
    for w in self.grid_frame.winfo_children():
        w.destroy()

    size = self.grid_size.get()

    for r in range(size):
        for c in range(size):
            tb.Label(
                self.grid_frame,
                text=str(self.grid[r][c]),
                width=4,
                relief="ridge"
            ).grid(row=r, column=c, padx=2, pady=2)
This redraws the grid every time puzzles change.Step 7 â€” Simple Puzzle Generationdef generate_no_four(self, size):
    grid = []
    for r in range(size):
        row = []
        for c in range(size):
            options = [6, 7]
            if len(row) >= 3 and all(x == row[-1] for x in row[-3:]):
                options.remove(row[-1])
            row.append(random.choice(options))
        grid.append(row)
    return grid
This prevents four identical values in a row.def generate(self):
    size = self.grid_size.get()

    if self.puzzle.get() == "No-Four-In-Row":
        self.grid = self.generate_no_four(size)
    else:
        self.grid = [[random.choice([6,7,""]) for _ in range(size)] for _ in range(size)]

    self.display_grid()
Step 8 â€” Exporting SVG Filesdef export_svg(self):
    folder = filedialog.askdirectory()
    if not folder:
        return
dwg = svgwrite.Drawing("puzzle.svg", size=("700","800"))
dwg.add(dwg.rect((0,0),("100%","100%"), fill=self.bg_color.get()))
for r in range(size):
    for c in range(size):
        dwg.add(dwg.text(str(self.grid[r][c]),
                insert=(100+c*80,100+r*80)))
dwg.save()

Step 9 â€” Barcode Generation
def generate_barcode(self):
    code = "".join(str(random.randint(0,9)) for _ in range(12))
    barcode = EAN13(code, writer=ImageWriter())
    barcode.save("barcode")
This creates a scannable barcode image.Step 10 â€” Save & Load Puzzleswith open("puzzle.json","w") as f:
    json.dump(self.grid,f)
with open("puzzle.json","r") as f:
    self.grid = json.load(f)
if __name__ == "__main__":
    SixSevenStudioV5().root.mainloop()
Youâ€™ve built a desktop puzzle studio with:Improve puzzle algorithms]]></content:encoded></item><item><title>Build a Python SMS Spam Classifier with SpamShield v3.1 ðŸš€</title><link>https://dev.to/matetechnologie/build-a-python-sms-spam-classifier-with-spamshield-v31-233e</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:15:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Ever wondered how AI can help you detect spam messages? In this tutorial, weâ€™ll build SpamShield v3.1, a Python app that classifies SMS messages as SPAM or HAM using machine learning. Even if youâ€™re a beginner, youâ€™ll be able to follow along!Step 1: Setting Up the ProjectFirst, create a new folder for your project and install the required Python libraries. Open your terminal and run:pip install pandas scikit-learn joblib ttkbootstrap
pip install tkinterdnd2  # Optional: Enables drag & drop in the GUI
pandas: Handles CSV/TXT data.scikit-learn: Provides machine learning tools.joblib: Saves and loads trained models.ttkbootstrap: Makes your GUI look modern.tkinterdnd2: Adds drag-and-drop support (optional).Step 2: Download the SMS Spam Dataset AutomaticallyWeâ€™ll use the SMSSpamCollection dataset from the UCI repository. The script downloads it automatically if itâ€™s missing.import urllib.request
import zipfile
import os
import sys

def resource_path(file_name):
    base_path = getattr(sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_path, file_name)

def download_dataset():
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
    zip_path = resource_path("smsspamcollection.zip")

    urllib.request.urlretrieve(url, zip_path)

    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(resource_path(""))

    os.remove(zip_path)
    print("[INFO] Dataset downloaded successfully!")
This ensures that even if the dataset is missing, the app will fetch it automatically.Step 3: Train the Machine Learning ModelWeâ€™ll use Naive Bayes with TF-IDF vectorization to classify SMS messages.import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

def train_sms_model():
    ds_path = resource_path("SMSSpamCollection")

    if not os.path.exists(ds_path):
        download_dataset()

    df = pd.read_csv(ds_path, sep="\t", header=None, names=["label", "text"])
    df["label_num"] = df["label"].map({"ham": 0, "spam": 1})

    X_train, X_test, y_train, y_test = train_test_split(df["text"], df["label_num"], test_size=0.2, random_state=42)

    model = make_pipeline(TfidfVectorizer(), MultinomialNB())
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    print(f"[INFO] Model trained â€” Test Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%")

    joblib.dump(model, resource_path("sms_spam_model.pkl"))
    return model
Tip: The TF-IDF vectorizer converts text into numbers, and Naive Bayes predicts whether a message is spam.Weâ€™ll create a helper function to load the model if it already exists, otherwise, it trains a new one.def load_model():
    model_path = resource_path("sms_spam_model.pkl")
    if os.path.exists(model_path):
        return joblib.load(model_path)
    return train_sms_model()
Step 5: Create a Worker to Process SMS FilesFor batch classification, weâ€™ll build a SpamWorker class that reads CSV/TXT files and labels messages.import csv

class SpamWorker:
    def __init__(self, files, model):
        self.files = files
        self.model = model

    def run(self):
        for path in self.files:
            with open(path, newline="", encoding="utf-8", errors="ignore") as f:
                reader = csv.reader(f)
                texts = [row[0].strip() for row in reader if row]

                labels_num = self.model.predict(texts)
                labels = ["SPAM" if l == 1 else "HAM" for l in labels_num]

                for t, lbl in zip(texts, labels):
                    print(f"{lbl} | {t}")
This prints each SMS with its predicted label. Later, weâ€™ll connect it to a GUI for a better user experience.Step 6: Build a GUI with TkinterWeâ€™ll use ttkbootstrap for styling. This allows drag-and-drop support and batch processing.import ttkbootstrap as tb
from tkinter import filedialog

class SpamShieldApp:
    def __init__(self):
        self.root = tb.Window(themename="darkly")
        self.root.title("SpamShield v3.1")
        self.model = load_model()
        self.files = []

        self.build_ui()

    def build_ui(self):
        tb.Label(self.root, text="ðŸ“© SpamShield - AI SMS Detector", font=("Segoe UI", 22, "bold")).pack(pady=10)

        self.path_input = tb.Entry(self.root, width=80)
        self.path_input.pack(pady=5)

        tb.Button(self.root, text="ðŸ“‚ Browse Files", bootstyle="info", command=self.browse_files).pack(pady=5)
        tb.Button(self.root, text="ðŸš€ Start Classification", bootstyle="success", command=self.start).pack(pady=5)

    def browse_files(self):
        self.files = filedialog.askopenfilenames(filetypes=[("CSV Files","*.csv"), ("Text Files","*.txt")])
        self.path_input.delete(0, "end")
        self.path_input.insert(0, f"{len(self.files)} files selected")

    def start(self):
        worker = SpamWorker(self.files, self.model)
        worker.run()

    def run(self):
        self.root.mainloop()
The GUI lets users select files and classify messages with one click.Finally, add the  section to run your app:if __name__ == "__main__":
    app = SpamShieldApp()
    app.run()
Now you have a fully functional SMS spam classifier with AI-powered detection and a modern GUI!Downloading datasets programmaticallyBuilding a machine learning pipeline with TF-IDF + Naive BayesSaving/loading ML models with joblibCreating a GUI for batch processingClassifying SMS messages as SPAM or HAM]]></content:encoded></item><item><title>How to Write Professional SEO-Optimized Blog Content: Complete Research and Writing Guide</title><link>https://dev.to/nithinbharathwaj/how-to-write-professional-seo-optimized-blog-content-complete-research-and-writing-guide-2mie</link><author>Nithin Bharadwaj</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:14:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! I'd be happy to help you with that. To provide you with a fresh, detailed article that meets all your requirements, I need to know the specific topic you'd like me to research and write about. Could you please share the topic?
  
  
  Once you provide it, I will conduct thorough research and craft a 2500-word article in Markdown format. It will feature a simple, professional, and engaging first-person narrative, avoid the specified words, use short paragraphs, and include extensive, detailed code examples with personal touches where appropriate.
ðŸ“˜ , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>I Built a CLI Task Manager That Learns When to Use Machine Learning (and When Not To)</title><link>https://dev.to/usero0/i-built-a-cli-task-manager-that-learns-when-to-use-machine-learning-and-when-not-to-5c83</link><author>jelly cri</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:09:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most productivity tools today are either:
Rule-based (static priorities, deadlines, heuristics), or
â€œAI-poweredâ€ in name only, applying ML everywhere whether it makes sense or not.
I wanted to explore a third path.
So I built PriorityPilot â€” a CLI-first task & project manager that learns from your behavior, but only when the data actually justifies it.
Machine Learning is powerful â€” but only after:the signal is stronger than a simple baselinethe model proves itâ€™s better than heuristicsPriorityPilot starts fully rule-based, and progressively enables ML only when it earns the right to do so.
No magic. No hype. Just measured decisions.
Manage projects and tasks from the terminalestimated vs actual hours
Learn from:your task completion patternsordering decisions you implicitly make
Then it uses ML to:rank tasks pairwise (what should come before what)
All while staying transparent about confidence and limitations.
Because:
context switching kills focus
developers already live in the terminalPriorityPilot supports:
Basic mode â†’ minimal friction
Advanced mode â†’ ML insights, confidence intervals, drift warnings
Same tool, different levels of depth. (This Is the Important Part)
PriorityPilot is ML-first in design, but ML-last in execution.
Below ~10 samples â†’ no ML
Pure heuristics and neutral predictions
Baseline Always Wins by Default
Ridge regression baseline
ML models must outperform it
If they donâ€™t â†’ theyâ€™re ignored
Drift Detection
If your behavior changes, the system notices
Models are downgraded automatically
Estimates include confidence intervals
Warnings appear when predictions are unreliable
This is not â€œAI guessingâ€. Itâ€™s ML behaving responsibly.
Models Used (Nothing Exotic)
Gradient Boosting â†’ priority prediction
Random Forest â†’ effort estimation
Logistic Regression â†’ pairwise ranking
Ridge â†’ baseline sanity check
Simple models. Interpretable. Good enough.If Youâ€™re Curious
â­ Star the repo if you like the idea
ðŸ’¬ Feedback (especially critical) is welcomehttps://github.com/Usero0/PriorityPilot
Thanks for reading â€” and remember:
ML should earn its place, not assume it.]]></content:encoded></item><item><title>The Active Unique Pattern: Better Than Soft Delete</title><link>https://dev.to/cobel1024/the-active-unique-pattern-better-than-soft-delete-2ohb</link><author>Dora</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:06:15 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Student takes an exam and needs to retry.Only one active attempt at a timeKeep history of all previous attemptsRelated data (answers, submissions, grades) stays linkedEvery query needs .filter(deleted_at__isnull=True).
  
  
  Solution: Active Unique Pattern
Only one active record. Keep all inactive records as history.Only ONE active attempt per (exam, learner).
Inactive attempts have no limit.Related data (TempAnswer, Submission, Grade) stays linked to old attempt.
No cascade deletes. No data loss.No  everywhere.Database-enforced uniquenessRelated data stays linkedHard Delete: No history
Soft Delete: Messy queries
Archive Table: Two tables
Active Unique: History + clean queriesAnytime you need "one current + keep all previous."]]></content:encoded></item><item><title>Build Advanced Python NLP: 8 Essential Techniques for Text Analysis and AI Applications</title><link>https://dev.to/nithinbharathwaj/build-advanced-python-nlp-8-essential-techniques-for-text-analysis-and-ai-applications-3827</link><author>Nithin Bharadwaj</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:00:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! I want to talk about making computers understand human language. It sounds complex, but with Python, we can start with simple steps and build up to impressive applications. Over the years, I've used these methods to analyze customer feedback, automate support, and even generate content. Let me show you how you can do the same.First, we need to prepare our text. Raw text is messyâ€”full of URLs, odd punctuation, and variations. Think of this like washing vegetables before you cook. We clean it to get consistent results. In Python, libraries like  help with this intelligent cleaning, called preprocessing and tokenization.This code turns a sentence into clean, standard parts. The lemma is the base wordâ€”"planning" becomes "plan". This consistency is crucial for the next steps.Once text is clean, we can find the important names and places in it. This is called Named Entity Recognition (NER). It's like a highlighter for text, picking out companies, people, and locations automatically. I use this to quickly scan news articles or legal documents for key players.Running this shows that "Microsoft" and "Google" are organizations (ORG), while "Seattle" and "California" are geographic locations (GPE). This automatic tagging saves hours of manual review.Now, let's gauge feeling or opinion in text, which is sentiment analysis. Early tools just classified text as positive or negative. Now, we can detect nuance, like frustration or mild satisfaction. I've built systems that track brand sentiment from social media using these techniques.This gives a measurable score for emotion. For more advanced needs, pre-trained transformer models from libraries like  can detect sarcasm or mixed feelings, which I often integrate for customer service analysis.When you have thousands of documents, you need to find the common themes without reading each one. This is topic modeling. I think of it as a sorting machine that reads all your documents and groups them by hidden topics. LDA is a classic algorithm for this.This might output Topic 0: stocks, markets, tech, highs, surge and Topic 1: climate, emission, goals, reduction, conference. It instantly reveals the main themes: finance and environment.For tasks like spam detection or categorizing support tickets, we use text classification. We teach a model by showing it many labeled examples. Today, fine-tuning pre-trained transformer models gives remarkable accuracy, even with modest amounts of your own data.For production, you'd use the  library by Hugging Face, which handles the complex steps. I've used this to build classifiers that route customer emails to the correct department with over 95% accuracy.Creating conversational agents or generating text requires sequence-to-sequence models. These are the engines behind many chatbots. They read an input sequence (like a user's question) and generate an output sequence (the response). I'll show a simplified concept.Real models, like GPT or DialoGPT, are trained on massive dialogues and generate far more coherent and varied responses. The key is they understand context; they remember what was said earlier in the conversation.Long documents need summaries. There are two main ways: extractive and abstractive. Extractive summarization picks the most important existing sentences. It's like highlighting. Abstractive summarization writes new sentences to convey the core meaning, like a human would.For abstractive summarization, I often use the  feature from the  library with a model like . It can take a long article and produce a concise, well-written paragraph.Finally, we have machine translation. Modern neural translation models understand context much better than old word-for-word systems. They can handle idioms and technical terms. Python makes it straightforward to access state-of-the-art models.The real magic is that these models, such as the MarianMT models, have been trained on millions of sentence pairs. They don't just swap words; they rephrase ideas to sound natural in the target language.Each of these eight techniques is a tool. You start with preprocessing to clean your data. Then, you might extract entities to find key information. Sentiment analysis tells you how people feel. Topic modeling helps you organize large collections of text. Classification automates sorting. Sequence models enable conversation and generation. Summarization condenses information. Translation breaks down language barriers.
  
  
  I often combine them. For instance, I might translate foreign social media posts, analyze their sentiment, extract mentioned company names, and summarize the main topicsâ€”all in an automated pipeline. Python's ecosystem, with libraries like , , , and , makes this integration possible. The best approach is to start simple, get one technique working, and then gradually add more complexity as your needs grow.
ðŸ“˜ , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>pytest Features That Changed How I Write Tests</title><link>https://dev.to/david_moran_0a44206d28c04/pytest-fixtures-changed-how-i-write-tests-1503</link><author>David Moran</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 13:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This week for my AST Analyzer project I decided to do a deep dive on the testing framework . When i started off the week I genuinely thought I already knew everything I needed, but man was I pleasantly surprised at how powerful that testing suite is. Today I'm going to go over a few new things I learned when implementing my tests suite, including:Assertion behind the scenesOne of the first things I noticed when switching from unittest to pytest was how much simpler assertions are. In unittest, you need to use specific methods like , , , etc:# unittest style
self.assertEqual(result, 5)
self.assertTrue(is_valid)
self.assertIn("error", message)
With pytest, you just use Python's built-in  statement:# pytest style
assert result == 5
assert is_valid
assert "error" in message
I was wondering how a 3rd party package could make assertion statements clearer than what's built in to Python, and while digging deeper I found that there's a whole process that goes on when the test fails.
  
  
  Assert Rewriting with AST
Pytest uses the built in assert function because it raises an AssertionException when the case does not pass. This built-in assertion check allows Pytest to work with built-in functionality and expand functionality from there.For example, once an AssertionException is raised pytest uses assertion rewriting to replace the default string. This is done at import time where pytest can grab the value of each variable and sub expressions. This new info is used to provide the logs with more details like:Provides a diff of what was given and what was expectedThe line the test failed atWhat was the failure (mismatch, missing item, etc)By doing this at import time, it's able to place all code in an Abstract Syntax Tree (yes, the same AST we're working with in this project), finds all the  statements, and rewrites them to capture intermediate values before the assertion runs. This is why when an assertion fails, pytest can show you exactly what each part of the expression evaluated to:def test_string_comparison():
    result = "hello world"
    assert result == "hello pytest"
=========================== FAILURES ===========================
_________________ test_string_comparison _______________________

    def test_string_comparison():
        result = "hello world"
>       assert result == "hello pytest"
E       AssertionError: assert 'hello world' == 'hello pytest'
E         
E         - hello pytest
E         + hello world

======================== 1 failed in 0.02s =====================
This works for complex expressions too:def test_list_membership():
    items = ["apple", "banana", "cherry"]
    target = "grape"
    assert target in items
E       AssertionError: assert 'grape' in ['apple', 'banana', 'cherry']
Pytest captured both the value of  and  before the assertion ran, so it can show you exactly why it failed.Coming from Node, one thing I appreciated about jest is being able to use , , , and  in combination with  scopes to setup and tear down test data in a straightforward manner. I was relieved to see that pytest gives us these features with the ability of fixtures that allow us to specify the same level of setup, teardown, and scope that's in jest.
  
  
  Why Fixtures over beforeEach
While I was relieved to see familiar patterns, I quickly realized that fixtures are actually more powerful than Jest's approach in a few key ways: - Fixtures can depend on other fixtures. We injected pytest's built-in tmp_path fixture into our own custom fixture - that's composition in action. In Jest, you'd have to nest your beforeEach blocks or manually call setup functions to achieve something similar. - Fixtures let you specify how often they run: function (default, runs for each test), class, module, or session. If I have an expensive setup like connecting to a database, I can scope it to session and it only runs once for the entire suite.Reusability via conftest.py - Any fixture defined in a conftest.py file is automatically available to all tests in that directory and subdirectories. No imports needed. - This one was subtle but important. With Jest's beforeEach, the setup runs before every test in that scope whether you need it or not. With pytest, fixtures only run when a test actually requests them as a parameter. If I have 10 tests in a class but only 3 need the sample_code_file fixture, it only gets created 3 times. This keeps tests fast and avoids unnecessary setup.Part of testing the ASTAnalyzer was making sure that we had a file with data to parse throughout out tests. Initially I created a sample file in a tests/data document and tested against that, but then found that we can create one using a combination of our own fixture and one of pytest's built in fixtures:@pytest.fixture
def sample_code_file(tmp_path):
    """Factory fixture to create temporary Python files with specified content."""

    def _create_file(content, filename="test_file.py"):
        file_path = tmp_path / filename
        file_path.write_text(content)
        return str(file_path)

    return _create_file

def test_enter_opens_file(self, sample_code_file):
    """__enter__ opens the file and returns file object."""
    filepath = sample_code_file("x = 1")
    with Parser(filepath) as f:
        assert f is not None
        assert not f.closed

def test_context_manager_with_exception(self, sample_code_file):
    """File is closed even when exception occurs."""
    filepath = sample_code_file("content")
    file_ref = None
    with pytest.raises(ValueError):
        with Parser(filepath) as f:
            file_ref = f
            raise ValueError("test error")
    assert file_ref.closed
As we can see, we've created a  that takes in the content that we want to test against and automatically writes it to a file. This allows us to have a piece of reusable code that can be used to test all types of content inside of the fileOne of the harder things I found while setting up my initial tests was making sure that my printing and logging decorators were being tested. Initially I was using  to mimic this behavior like so:@logger(logging.DEBUG)
def add(a, b):
    return a + b


def test_ast_log_defaults():
    with patch("ast_analyzer.decorators.logger.logger") as mock_logger:
        add(3, 5)

    mock_logger.debug.assert_called()
    call_args = str(mock_logger.debug.call_args)
    assert "add" in call_args
The test technically works, it checks that mock logger was called in the  function, but it doesn't actually check what the contents of that log are. With pytest we can use  to grab the output and save it to a file that we can then read from:@logger(logging.DEBUG)
def add(a, b):
    return a + b

def test_ast_log_defaults(caplog):
    with caplog.at_level(logging.DEBUG):
        add(3, 5)

    assert "DEBUG" in caplog.text
    assert "add" in caplog.text
With this new functionality, we can look for specific strings of text inside of the log. We also have control over the logging level that we display, so we can write tests for DEBUG and INFO to check that the text in both of those logs are appearing properly. This can also be done with capsys for checking print statements:def test_prints_timing_output(capsys):
    """Decorator should print timing information to stdout."""

    factorial(3)
    captured = capsys.readouterr()
    assert "factorial" in captured.out
    assert "->" in captured.out
In both formats we can see the ease of use that pytest gives us for accessing this textLast but not least, there's parametrization. Parametrization is a technique that's used in python to make code more modular and reusable. In our specific case, we use parametrization for testing by iterating through multiple parameters. This is a great way to reduce the amount of code in your test suite without losing any functionality. To demo this, I'll show a before and after of a series of tests made in the repo.When we create our custom , one thing we do on initialization is see how many children the Node has. In order to test that our  declaration for children is working, we set up the following tests:def test_str_shows_children_count_one_child(self):
    """__str__ displays the number of children."""
    tree = ast.parse("x = 1")
    node = ASTNode(tree)
    assert str(node) == f"AST Node | Children: 1"

def test_str_shows_children_count_mult_children(self):
    """__str__ displays the number of children."""
    tree = ast.parse("x = 1\ny = 2\nz = 3")
    node = ASTNode(tree)
    assert str(node) == f"AST Node | Children: 3"

---

================ test session starts ================
platform darwin -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0
rootdir: /Users/davidmoran/Sites/ai-bootcamp/projects/AST-Analyzer
configfile: pyproject.toml
plugins: cov-7.0.0
collected 2 items                                   

tests/test_astnode.py ..                      [100%]

================= 2 passed in 0.03s =================
As you can see, the code is simple enough where having it be exactly the same isnt too bad, but its repetitive which is something that we want to avoid. When playing around with this I thought of combining this into a collection and testing that way:def test_str_shows_children_count(self):
    """__str__ displays the number of children."""
    tree = ast.parse("x = 1")
    node = ASTNode(tree)
    assert str(node) == "AST Node | Children: 1"

    tree = ast.parse("x = 1\ny = 2\nz = 3")
    node = ASTNode(tree)
    assert str(node) == "AST Node | Children: 3"

---

================ test session starts =================
platform darwin -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0
rootdir: /Users/davidmoran/Sites/ai-bootcamp/projects/AST-Analyzer
configfile: pyproject.toml
plugins: cov-7.0.0
collected 1 item                                     

tests/test_astnode.py .                        [100%]

================= 1 passed in 0.02s ==================
While this is definitely a lot cleaner and lean, one thing I didn't appreciate is that I'm stuffing two test cases into one. In the future if one of these were to fail (in a larger test) it would be a bit annoying trying to figure out which one of these was the culprit
  
  
  Introducing: Parametrization
Parametrization solves the above by creating a matrix of key values to test against and running them against 1 test. Unlike our second option above, using parametrization breaks the singular test out into the number of suites we specified above so that we can see which of the items failed a test@pytest.mark.parametrize(
    "code,expected_count",
    [
        ("x = 1", 1),
        ("x = 1\ny = 2\nz = 3", 3),
    ],
)
def test_str_shows_children_count(self, code, expected_count):
    """__str__ displays the number of children."""
    tree = ast.parse(code)
    node = ASTNode(tree)
    assert str(node) == f"AST Node | Children: {expected_count}"

---

================ test session starts =================
platform darwin -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0
rootdir: /Users/davidmoran/Sites/ai-bootcamp/projects/AST-Analyzer
configfile: pyproject.toml
plugins: cov-7.0.0
collected 2 items                                    

tests/test_astnode.py ..                       [100%]

================= 2 passed in 0.05s ==================
The way this works is pretty straightforward:We use the  and pass in two arguments

A string of variable names separated by commasA tuple of values that you want each variable to represent on iterationFor each tuple in our collection we passed, the test will iterate over and replace the variables with the values we providedEvery time the test is run, it reports it as a separate test, meaning that we can get more insight into which parameter will fail in case of an error.This feature of pytest works great when you want to test one specific thing against a number of start points. For our test, we just wanted to make sure that an ASTNode was created successfully based on the input, so it's a perfect candidate for parametrization. If I wanted to check the outputs or error handling of certain inputs, that is better handled as a separate test so that we can check on a number of items (type of exception if raised, state of ASTNode, log statements, etc).]]></content:encoded></item><item><title>Making Date-Based Content Reusable</title><link>https://dev.to/cobel1024/making-date-based-content-reusable-5126</link><author>Dora</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 12:55:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Content with specific dates can't be reused.Example: An exam with these dates:Grading due: Jan 18, 2024Appeal deadline: Jan 20, 2024Confirmation due: Jan 22, 2024Want to reuse this exam in July? You have to manually update every date. 
Have 50 exams? Good luck.
  
  
  Common Approaches (and why they fail)
Every instance needs manual date updatesUpdate original? Copies don't change3. Templates with placeholders
  
  
  Solution: Store offsets, calculate at runtime
Content layer: Relative dates (logic only)Context layer: Base dates (when it's actually used)Course provides base dates for all studentsEnrollment can override for individual students (e.g., extended deadline)Either can serve as context for date calculationUsage: Calculate absolute dates at runtimeExam defines logic: "grading due 3 days after exam ends"Course/Enrollment defines dates: "this course runs Jan-Mar"Calculation happens when neededSame exam:
- January course: Grade due Jan 18
- July course: Grade due Jul 18
- September course: Grade due Sep 18
No duplication. No manual updates.
Change  to 
â†’ Affects all courses using this exam
â†’ Each with their own datesSame exam. Different dates. No duplication.: Logic (offsets, rules): Base dates (when it happens): Absolute dates (calculated)This makes content date-independent and infinitely reusable.]]></content:encoded></item><item><title>Tools of the Trade: What Powers Modern Data Engineering</title><link>https://dev.to/qvfagundes/tools-of-the-trade-what-powers-modern-data-engineering-326j</link><author>Vinicius Fagundes</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 12:33:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You understand what data engineering is. You know how pipelines, ETL, and warehouses work. Now comes the question every beginner asks:"What tools should I actually learn?"The data engineering landscape is overwhelming. New frameworks launch every month. Cloud providers release new services constantly. It's easy to get lost.In this article, I'll cut through the noise. After years of building data systems and training engineers across organizations, I've identified what actually matters â€” and what you can safely ignore as a beginner.Let's build your toolkit.Every data engineer needs proficiency in four areas: â€” How you write logic â€” Where data lives â€” How you schedule and manage pipelines â€” Where everything runsMaster these, and you can work anywhere.SQL is the language of data. Period.Every data engineer writes SQL daily. You'll use it to:Transform data in warehousesIf you learn only one thing from this article: Not just SELECT statements. Learn:CTEs (Common Table Expressions)DDL (creating and altering tables)
  
  
  Python: The Swiss Army Knife
Python is the default scripting language for data engineering.
  
  
  Other Languages Worth Knowing
For beginners: focus on . Add others as needed.You'll interact with different storage systems depending on the use case.
  
  
  Relational Databases (OLTP)
Used for transactional workloads: â€” Open source, widely used â€” Popular in web applications â€” Common in enterprise environments
  
  
  Cloud Data Warehouses (OLAP)
Used for analytical workloads:Ease of use, separation of storage/computeServerless, great for GCP usersUnified lakehouse platformAzure ecosystem integrationUsed for raw and unstructured data storage:
  
  
  Which Should You Learn First?
Start with  for relational concepts, then pick one cloud warehouse. I recommend  or  â€” both have free tiers and are beginner-friendly.Orchestration is how you schedule, monitor, and manage pipelines.Without orchestration, you'd be running scripts manually. That doesn't scale.Airflow uses DAGs (Directed Acyclic Graphs) to define workflows. If you learn one orchestration tool, make it Airflow.Modern, Python-native, easier than AirflowFor transformation orchestrationLearn  first. It's everywhere. Once you understand Airflow, picking up alternatives is straightforward.dbt has changed how data teams work.Write transformations in SQLVersion control your modelsDocument your transformationsdbt follows the ELT pattern â€” transformations happen inside the warehouse.If you're working with a modern data stack, dbt is almost certainly part of it.Almost all data engineering today happens in the cloud. You need to be comfortable with at least one major provider.S3, Redshift, Glue, Lambda, EMR, KinesisBigQuery, Cloud Storage, Dataflow, Pub/SubSynapse, Data Lake, Data Factory, Event Hubs
  
  
  Which Cloud Should You Learn?
Check job postings in your target market. In my experience: â€” Most job listings, largest market share â€” Strong in startups and data-heavy companies â€” Dominant in enterprise, especially Microsoft shopsPick one and go deep. The concepts transfer across platforms.When data exceeds what a single machine can handle, you need distributed processing.Spark is the dominant big data framework.Processing billions of rowsComplex transformations at scaleMachine learning on large datasetsYou can write Spark jobs in Python (PySpark), Scala, or SQL.Honestly? Not as often as people think.Many teams reach for Spark too early. Modern warehouses (Snowflake, BigQuery) handle most workloads without needing Spark.Learn the basics, but don't obsess over it until you're dealing with truly massive datasets.For real-time data processing:Message streaming, event backboneReal-time stream processing
  
  
  Should Beginners Learn Streaming?
Not immediately. Most entry-level roles focus on batch processing. Streaming is an intermediate to advanced skill.Understand the concepts, but prioritize batch pipelines first.
  
  
  DevOps and Infrastructure
Modern data engineers don't just write pipelines. They deploy and maintain them.Version control â€” absolutely essentialContainerization â€” run anywhereAutomated testing and deploymentYou don't need to become a DevOps engineer. But you should be able to:Understand CI/CD pipelinesYou'll hear this term often. It refers to a common combination of tools:Ingestion:    Fivetran, Airbyte, Stitch
Storage:      Snowflake, BigQuery, Databricks
Transform:    dbt
Orchestrate:  Airflow, Prefect, dbt Cloud
Visualize:    Looker, Tableau, Metabase
SQL-first transformationsManaged services over self-hosted
  
  
  What to Learn First: A Priority List
If I were starting over today, here's my order: â€” Snowflake or BigQuery â€” Understand orchestration â€” Modern transformation â€” AWS, GCP, or Azure â€” Containerization basics â€” When you need scaleDon't try to learn everything at once. Build depth, then breadth.
  
  
  Tools I Tell Beginners to Ignore (For Now)
Kubernetes â€” Overkill for most starting outHadoop â€” Legacy, rarely used in new projectsEvery new framework that launches â€” Wait for adoptionNo-code tools â€” Learn the fundamentals firstYou now have a map of the data engineering toolkit. In the next article, we'll cover something often overlooked:The mathematics behind data engineering â€” what you actually need to know, without the academic fluff.Data Engineering Uncovered: What It Is and Why It MattersPipelines, ETL, and Warehouses: The DNA of Data EngineeringTools of the Trade: What Powers Modern Data EngineeringThe Math You Actually Need as a Data EngineerBuilding Your First Pipeline: From Concept to ExecutionCharting Your Path: Courses and Resources to Accelerate Your JourneyHave questions about which tools to prioritize? Drop them in the comments.]]></content:encoded></item><item><title>Machine Learning Basics Every Data Analyst Should Know</title><link>https://dev.to/adnan_arif_14ae4bc014267f/machine-learning-basics-every-data-analyst-should-know-3j87</link><author>Adnan Arif</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 12:00:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Data analysts and data scientists aren't the same role. But the line between them keeps blurring.Increasingly, employers expect data analysts to understand machine learning fundamentals. Not to build production AI systemsâ€”that's still data science territoryâ€”but to know when ML applies, how it works conceptually, and how to collaborate with ML teams.This isn't about becoming a data scientist. It's about being a more effective analyst in a world where machine learning is everywhere.
  
  
  What Machine Learning Actually Is
Strip away the hype and machine learning is pattern recognition at scale.Traditional programming: you write rules. If purchase > $1000 and first_order = True, flag for review.Machine learning: you provide examples. Here are 10,000 transactions, some fraudulent, some legitimate. The algorithm finds patterns that distinguish them.The key insight: ML discovers rules from data instead of you specifying them. This works when patterns exist but are too complex for humans to articulate.Machine learning isn't always the answer. Often simpler approaches work better.Patterns too complex for explicit rules (image recognition, natural language)Problems where you have lots of labeled examplesSituations where small accuracy improvements justify significant investmentTasks with stable patterns that won't shift rapidlyInsufficient data (less than hundreds or thousands of examples)Problems solvable with simple rules or SQLSituations requiring full explainability for complianceRapidly changing patterns that need frequent retrainingA common mistake: reaching for ML when a GROUP BY and a threshold would suffice.
  
  
  The Three Types of Learning
Machine learning approaches fall into three categories. You have labeled examplesâ€”inputs paired with known outputs. Predict house prices from features. Classify emails as spam or not. The algorithm learns the relationship between inputs and outputs. No labels, just data. Find natural groupings in customers. Detect anomalies in transactions. Reduce dimensionality for visualization. The algorithm discovers structure without being told what to look for. An agent learns through trial and error, receiving rewards or penalties. Less relevant for most analystsâ€”used mainly in robotics, games, and recommendation systems.As an analyst, you'll encounter supervised and unsupervised learning most often.
  
  
  Supervised Learning: Classification vs Regression
Supervised learning solves two types of problems. The output is a category. Will this customer churn? Is this transaction fraudulent? Which product category does this belong to? The output is a continuous number. What price will this house sell for? How many units will we sell next quarter?The distinction matters because different algorithms and evaluation metrics apply to each.
  
  
  Common Algorithms You'll Encounter
You don't need to implement these from scratch. But recognizing them helps.Linear/Logistic Regression. Simple, interpretable baselines. Linear regression predicts continuous values; logistic regression predicts probabilities for classification. Split data based on feature thresholds. Easy to understand and visualize. Prone to overfitting. Many decision trees voting together. More accurate than single trees, less interpretable.Gradient Boosting (XGBoost, LightGBM). Build trees sequentially, each correcting previous errors. Currently dominates tabular data competitions. Find optimal boundaries between classes. Works well in high dimensions. Layers of connected nodes learning complex patterns. Essential for images, text, and unstructured data.For tabular dataâ€”what analysts typically work withâ€”tree-based methods often perform best.Understanding how models learn helps you spot problems. Typically 70-80% for training, the rest for testing. Never evaluate on training dataâ€”it's like grading your own homework. The algorithm adjusts internal parameters to minimize prediction errors on training data. Test on held-out data. Adjust hyperparameters. Repeat. Final performance check on data the model has never seen.The fundamental challenge: generalization. A model that memorizes training data fails on new examples. Good models learn patterns that transfer.
  
  
  Overfitting: The Central Challenge
Overfitting happens when a model learns training data too wellâ€”including noise and quirks that don't generalize.Excellent training performance, poor test performanceModel complexity exceeds what the data supportsDramatic performance drops on new dataRegularization (penalizing complexity)An overfit model looks good in development and fails in production. This is why proper evaluation matters.Different metrics measure different aspects of model performance.MAE (Mean Absolute Error): Average prediction error in original unitsRMSE (Root Mean Squared Error): Penalizes large errors more heavilyRÂ² (R-squared): Proportion of variance explainedAccuracy: Percentage of correct predictions (misleading with imbalanced classes)Precision: Of positive predictions, how many were correct?Recall: Of actual positives, how many were found?F1 Score: Harmonic mean of precision and recallAUC-ROC: Area under the receiver operating characteristic curveChoose metrics that align with business objectives. Accuracy on a 99% negative dataset can be 99% just by predicting everything as negative.For classification, the confusion matrix is essential.From this, you can calculate any classification metric.False positives and false negatives have different costs. A spam filter that misses spam is annoying. A fraud detector that blocks legitimate transactions costs revenue. Optimize for what matters.Featuresâ€”the input variablesâ€”often matter more than algorithm choice. Knowing that "days since last purchase" predicts churn better than raw timestamps makes a difference.Log transforms for skewed distributionsBinning continuous variablesOne-hot encoding for categorical variablesInteraction features (combining variables)Time-based features (day of week, month, etc.)Data analysts often excel at feature engineering because they understand the data and business context. This is where your skills directly improve ML.Many real problems have imbalanced classes. Fraud is rare. Churn happens to a minority. Disease is uncommon.Standard algorithms struggleâ€”they learn to predict the majority class.Undersample the majority classOversample the minority class (SMOTE)Adjust class weights during trainingUse appropriate metrics (not accuracy)Imbalance is the norm in business problems. Expect to handle it.A single train-test split might be lucky or unlucky. Cross-validation provides more robust estimates.Split data into K equal partsTrain on K-1 parts, validate on the remaining partRepeat K times, rotating which part is held outThis gives a more reliable estimate of model performance and helps detect overfitting.Black box predictions often aren't enough. Stakeholders ask why the model made a decision. Linear regression, decision trees, and logistic regression have transparent logic.Interpretation techniques for complex models:Feature importance (which variables matter most)SHAP values (how each feature affects each prediction)Partial dependence plots (how one feature affects predictions)LIME (local explanations for individual predictions)When interpretability mattersâ€”for compliance, debugging, or stakeholder buy-inâ€”consider it from the start.
  
  
  Working with Data Scientists
As an analyst, you might not build production ML systems. But you'll likely collaborate with those who do.Domain knowledge about the data and businessFeature ideas based on your experienceData cleaning and preparationEvaluation from a business perspectiveAlgorithm selection and tuningEffective collaboration requires shared language. Understanding ML basics lets you participate meaningfully in discussions.
  
  
  Getting Started Practically
Want to build intuition? Start here. Python's go-to ML library. Clean API, great documentation, covers the basics. Competitions and datasets for practice. Start with beginner-friendly competitions like Titanic survival prediction. "Hands-On Machine Learning" by GÃ©ron is accessible and practical.
  
  
  What You Don't Need (Yet)
Focus before breadth. These can wait:Deep learning and neural network architecturesAdvanced optimization techniquesCutting-edge research papersMaster the fundamentals first. Advanced topics build on solid foundations.
  
  
  Frequently Asked Questions
Do I need to code to understand machine learning?
Basic Python helps significantly. You can understand concepts without code, but hands-on practice builds intuition faster.What's the difference between AI, machine learning, and deep learning?
AI is the broadest term (systems that seem intelligent). ML is a subset (learning from data). Deep learning is a subset of ML (neural networks with many layers).
Conceptual understanding of linear algebra, calculus, and statistics helps but isn't essential for practical use. Libraries handle the math.Should data analysts learn ML?
Increasingly yes. You don't need to become a data scientist, but understanding when and how ML applies makes you more valuable.What's the easiest algorithm to start with?
Linear/logistic regression. Simple, interpretable, and the foundation for understanding more complex methods.How do I know if ML will help my problem?
Ask: Do I have enough labeled examples? Is the pattern learnable? Is the improvement worth the complexity? Often, simpler approaches suffice.What tools should I learn?
Start with scikit-learn for classical ML. Add pandas for data prep, matplotlib/seaborn for visualization.How long does it take to learn ML basics?
A few weeks of focused study for conceptual understanding. Months to years for practical proficiency.Is AutoML replacing the need to understand ML?
AutoML automates algorithm selection and tuning but doesn't replace understanding. You still need to frame problems, prepare data, and interpret results.What's the biggest mistake beginners make?
Jumping to complex algorithms before understanding the data. Exploratory analysis and feature engineering usually matter more than algorithm choice.Machine learning isn't magic. It's pattern recognition powered by data and computation.As a data analyst, you don't need to become an ML expert. But understanding the basicsâ€”when it applies, how it works, and how to evaluate itâ€”makes you more effective in a world where ML is increasingly ubiquitous.Start with the fundamentals. Build intuition through practice. The advanced topics will make more sense once you have a solid foundation.This article was refined with the help of AI tools to improve clarity and readability.]]></content:encoded></item><item><title>ðŸ”¥ Trending Now: Today&apos;s Top Updates &amp; Viral Links - 2026-01-28</title><link>https://dev.to/tarun_walia_ad82c14a97e0c/trending-now-todays-top-updates-viral-links-2026-01-28-5804</link><author>tarun walia</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 11:45:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Here is the list of latest trending updates fetched on 2026-01-28. Click on the main link to view:        <p style="margin-top: 15px; color: #444; line-height: 1.6;">
            <strong>Description:</strong><br>
            Struggle with pleats? Try the Isadora Life Pre-stitched Cotton Silk Saree. Featuring elegant Damask motifs and Bengal Dhonekhali weave. Shop now at the best price on Socioglamm
        </p>
    </div>

    <div style="margin-bottom: 40px; padding: 20px; border: 2px solid #007bff; border-radius: 10px; background-color: #f0f8ff;">
        <h2 style="margin-top: 0; color: #0056b3; font-size: 24px;">
            <a href="https://socioglamm.com/product/Levi's/levi's-Men's-Solid-Tan-Relaxed-Fit-Shirt-on-Socioglamm/69790c84a5d81f167d9519bf" style="color: #0056b3; text-decoration: none;" target="_blank" rel="nofollow noopener">Men's Solid Tan Relaxed Fit Shirt</a>
        </h2>

        <p style="margin: 15px 0; font-size: 17px;">
            <strong>Main Link:</strong> 
            <a href="https://socioglamm.com/product/Levi's/levi's-Men's-Solid-Tan-Relaxed-Fit-Shirt-on-Socioglamm/69790c84a5d81f167d9519bf" style="color: #007bff; font-weight: bold; text-decoration: underline;" target="_blank" rel="nofollow noopener">https://socioglamm.com/product/Levi's/levi's-Men's-Solid-Tan-Relaxed-Fit-Shirt-on-Socioglamm/69790c84a5d81f167d9519bf</a>
        </p>

        <p style="margin-top: 15px; color: #444; line-height: 1.6;">
            <strong>Description:</strong><br>
            Upgrade your daily rotation with the Men's Solid Tan Relaxed Fit Shirt. Itâ€™s comfortable, versatile, and looks great with everything. Grab this exclusive deal now on Socioglamm before it's gone
        </p>
    </div>
]]></content:encoded></item><item><title>Deadhand: Split your seed phrase into shards. Inherit crypto without trust.</title><link>https://dev.to/gordazo0_7653f38e2d667dd1/deadhand-split-your-seed-phrase-into-shards-inherit-crypto-without-trust-2e8g</link><author>gordazo0</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 11:44:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dead Man&apos;s Switch: How to Automate Crypto Inheritance</title><link>https://dev.to/gordazo0_7653f38e2d667dd1/dead-mans-switch-how-to-automate-crypto-inheritance-474l</link><author>gordazo0</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 11:43:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[TL;DR: A dead man's switch is a mechanism that triggers automatically when you stop responding. Combined with Shamir's Secret Sharing, it enables trustless crypto inheritance without giving anyone premature access.]]></content:encoded></item><item><title>ðŸŽ¬ è¡Œå‹•èªè­˜</title><link>https://dev.to/stklen/xing-dong-ren-shi-5dpc</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 11:00:11 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[AIãŒJellyã‹Arielã‹ã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸå¾Œã€æ¬¡ã¯ä½•ã§ã—ã‚‡ã†ï¼Ÿï¼šAIã«å‹•ç‰©ã‚’è­˜åˆ¥ã•ã›ã‚‹ã ã‘ã§ãªãã€ä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã‚‚ç†è§£ã•ã›ã¾ã™ã€‚ï¼šã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿï¼šé€£ç¶šå‹•ä½œã®åˆ¤æ–­ä¸å¯ï¼ˆã€Œæ­©ã„ã¦ã„ã‚‹ã€vsã€Œæ­¢ã¾ã£ãŸã€ï¼‰é€£ç¶šã—ãŸè¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æžã—ã€å‹•çš„ãªè¡Œå‹•ã‚’ç†è§£ã™ã‚‹ã€‚ï¼šå„è¡Œå‹•ã‚«ãƒ†ã‚´ãƒª200æžšä»¥ä¸Šï¼šLabel Studioã‚’ä½¿ç”¨å…¥åŠ›å‹•ç”» â†’ è¡Œå‹•èªè­˜ â†’ è‡ªå‹•ã‚¿ã‚°ä»˜ã‘
                  â†“
           ã€ŒJellyãŒå¯ã¦ã„ã‚‹ã€ã€ŒDollarãŒé£Ÿäº‹ä¸­ã€
ï¼šæ›–æ˜§ãªå®šç¾©ã¯ãƒ©ãƒ™ãƒªãƒ³ã‚°ã®ä¸ä¸€è‡´ã‚’æ‹›ãï¼šå„è¡Œå‹•ã‚«ãƒ†ã‚´ãƒªã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æƒãˆã‚‹ï¼šå˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†é¡žã«ã¯é™ç•Œã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æžãŒã‚ˆã‚Šæ­£ç¢ºï¼šç•°ãªã‚‹å…‰æ¡ä»¶ã‚„è§’åº¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚ã‚‹ï¼šã€Œé€Ÿãèµ°ã‚‹ã€ã¨ã€Œã‚†ã£ãã‚Šæ­©ãã€ã‚’åŒºåˆ¥]]></content:encoded></item><item><title>EuroPython: January Newsletter: We Want Your Proposals for KrakÃ³w!</title><link>https://blog.europython.eu/january-newsletter-we-want-your-proposals-for-krakow/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 10:56:58 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Happy New Year! We&aposre kicking off 2026 with exciting news: EuroPython is moving to a brand new location! After three wonderful years in Prague, we&aposre heading to KrakÃ³w, Poland for our 25th anniversary edition. Mark your calendars for . ðŸŽ‰EuroPython 2026 will take place at the ICE KrakÃ³w Congress Centre, bringing together 1,500+ Python enthusiasts for a week of learning, networking, and collaboration.Â ðŸ“£ Call for Proposals is OPEN!The CfP is now live, and we want to hear from YOU! Whether you&aposre a seasoned speaker or considering your first talk, tutorial or poster, we&aposre looking for proposals on all topics and experience levels.Deadline: February 15th, 2026 at 23:55 UTC+1 (no extension, so donâ€™t leave it for the last minute!) (30 or 45 min) on any Python-related topic (hands-on 180 min sessions) for the poster sessionNo matter your level of Python or public speaking experience, EuroPython is here to help you bring yourself to our community. Represent your work, your interests, and your unique perspective!Want to get some extra help? The first 100 proposals will get direct feedback from the Programme team, so hurry with your submissions!ðŸŽ¤ Speaker Mentorship is OpenFirst time speaking? Feeling nervous? The Speaker Mentorship Programme is back! We match mentees with experienced speakers who&aposll help you craft strong proposals and, if accepted, prepare your talk. This programme especially welcomes folks from underrepresented backgrounds in tech.Applications are open now for Mentees and Mentors. Don&apost let uncertainty hold you back â€“ apply and join our supportive community of speakers.Â Deadline: 10th February 2026, 23:59 UTCðŸŽ™ï¸ Conversations with First-Time SpeakersWant to hear from people who&aposve been in your shoes? Check out our interviews with first-time speakers who took the leap. They share their experience of what it&aposs really like to speak at EuroPython.ðŸŽ¥ Video Recap from PraguePrague was incredible! âœ¨ Relive the best moments from EuroPython 2025 in our video recap.ðŸ“¢ Help Us Spread the Word!Big thanks to our speaker and community organiser Honza KrÃ¡l for giving a lightning talk about EuroPython at Prague Pyvo. If you&aposre a speaker or community organizer, we&aposd love your help spreading the word about the CfP! will be announced soon! Interested in supporting EuroPython 2026? Reach out to us at sponsoring@europython.eu. applications will open in the coming weeks. We&aposre committed to making EuroPython accessible to everyone, regardless of financial situation. Stay tuned!ðŸ¤Â  Where can you meet us this month?Â Â We&aposll be at  (February 1-2) with a booth alongside the Python Software Foundation and Django Software Foundation. If you&aposre in Brussels, come say hi, grab some stickers, and get the latest EuroPython news!We&aposre also heading to ! Join us for tasty pizza and good conversation about all things Python on 21st February.Â Follow us on social media and subscribe to our newsletter for all the updates:]]></content:encoded></item><item><title>Anaconda vs Miniconda vs Mamba Guide</title><link>https://dev.to/rosgluk/anaconda-vs-miniconda-vs-mamba-guide-4pae</link><author>Rost</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 10:44:58 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This comprehensive guide provides background and a detailed comparison of Anaconda, Miniconda, and Mamba - three powerful tools that have become essential for Python developers and data scientists working with complex dependencies and scientific computing environments.Choosing the right Python package manager can significantly impact your development workflow, environment setup time, and dependency management experience. Whether you're setting up a data science environment with Jupyter and Pandas, building production applications, or managing machine learning projects, the decision between Anaconda, Miniconda, and Mamba affects everything from installation speed and disk space usage to dependency resolution performance and environment reproducibility. Understanding their differences, strengths, and ideal use cases will help you make an informed choice that aligns with your specific needs and workflow requirements.The Python ecosystem offers multiple package management solutions, each optimized for different use cases. While traditional tools like venv and newer alternatives like uv have their place, conda-based solutions excel at managing complex scientific computing dependencies that include both Python packages and system libraries. This unique capability makes them indispensable for data science, machine learning, and scientific computing workflows where packages often require compiled binaries, system libraries, and non-Python dependencies.
  
  
  Understanding the Conda Ecosystem
The conda ecosystem consists of three main components: the package manager (conda), different distributions (Anaconda, Miniconda), and alternative implementations (Mamba). Each serves distinct purposes in the Python data science and development workflow. is both a package manager and an environment manager that handles Python packages along with their binary dependencies, system libraries, and even non-Python software. This makes it particularly valuable for scientific computing where packages like NumPy, SciPy, and machine learning frameworks have complex native dependencies. is the full-featured distribution that includes conda plus hundreds of pre-installed packages. It's designed for users who want everything ready out of the box, including popular data science libraries, Jupyter Notebook, and the Anaconda Navigator GUI. provides just the essentials: conda, Python, and a minimal set of dependencies. It's the lightweight alternative that lets you build custom environments from scratch, installing only what you need. represents the evolution of conda's dependency resolution engine. Originally a standalone tool, its core technology (libmamba) has been integrated into modern conda versions, offering significantly faster dependency resolution and environment creation.
  
  
  Anaconda: The Complete Solution
Anaconda is the heavyweight champion of Python distributions, weighing in at approximately  with over 600 pre-installed packages. This comprehensive installation includes everything from core data science libraries to development tools and visualization packages.Anaconda shines in scenarios where convenience and completeness matter more than disk space or installation speed. It's ideal for: entering data science who want immediate access to tools without learning package installation where consistency across student machines is important when you need to experiment with various libraries without setup overhead users who prefer Anaconda Navigator over command-line interfaces requiring commercial support and compliance featuresThe pre-installed packages include essential data science tools like Pandas, NumPy, Matplotlib, Scikit-learn, Jupyter Notebook, and many others. This means you can start analyzing data or building machine learning models immediately after installation.
wget https://repo.anaconda.com/archive/Anaconda3-latest-Linux-x86_64.sh


bash Anaconda3-latest-Linux-x86_64.sh

 ~/.bashrc
The installation process is straightforward, and Anaconda Navigator provides a graphical interface for managing environments, packages, and launching applications like Jupyter Notebook or Spyder IDE.
  
  
  Miniconda: The Minimalist Approach
Miniconda takes the opposite philosophy: start minimal and add only what you need. At approximately , it includes just conda, Python, and essential dependenciesâ€”about  total.Miniconda is the preferred choice for: where smaller footprint and faster installation matter where image size directly impacts deployment speed who know exactly which packages they need where minimal environments reduce build times on systems with limited storageSecurity-conscious environments where fewer packages mean smaller attack surfaceThe minimalist approach gives you complete control over your environment. You explicitly install each package, which leads to more reproducible environments and better understanding of dependencies. This aligns well with modern Python design patterns for clean architecture where explicit dependencies are preferred.
  
  
  Miniconda Installation and Setup

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh


bash Miniconda3-latest-Linux-x86_64.sh

 ~/.bashrc


conda create  myproject 3.11
conda activate myproject


conda pandas numpy matplotlib jupyter
This workflow requires more steps but results in leaner, more maintainable environments. Each package is intentionally added, making it easier to track dependencies and reproduce environments across different systems.
  
  
  Mamba: The Performance Revolution
Mamba represents a significant leap forward in dependency resolution performance. Originally developed as a standalone conda alternative, its core technology has been integrated into conda itself, but standalone Mamba tools remain valuable for specific use cases.Mamba's libsolv-based solver delivers 50-80% faster dependency resolution compared to conda's legacy solver. In practical terms, this means:: 3 seconds vs 17 seconds for conda (in benchmark tests)Complex dependency resolution: Handles conflicts that would cause conda to fail: Comparable download times but faster resolution phase: More informative feedback when resolution failsThe performance gains are most noticeable when working with large environments or complex dependency trees common in machine learning and data science projects.As of  (November 2023), libmamba became conda's default solver. This means modern conda installations automatically benefit from Mamba's performance improvements without requiring separate Mamba installation.However, standalone  and  tools still offer value:: Single-binary, no installation required, perfect for containers: Full-featured alternative with additional commands like : Mamba tools often start faster than conda
conda mamba  base  conda-forge


mamba create  myenv 3.11 pandas numpy
mamba activate myenv
mamba scikit-learn


curl  https://micro.mamba.pm/api/micromamba/linux-64/latest |  bin/micromamba
./bin/micromamba create  myenv 3.11
The command syntax is nearly identical to conda, making migration seamless. You can literally replace "conda" with "mamba" in most workflows.
  
  
  For Data Science Beginners
 if you're just starting with data science. The pre-installed packages and GUI tools (Anaconda Navigator) provide a smooth learning curve. You can focus on learning Python and data analysis rather than package management.
  
  
  For Production Development
 for production environments. The smaller footprint, explicit dependencies, and faster installation make it ideal for Docker containers, CI/CD pipelines, and server deployments. This approach aligns with best practices for unit testing in Python where reproducible environments are critical. When you're ready to package your Python applications for distribution, tools like PyInstaller can help create standalone executables from your conda-managed environments.
  
  
  For Performance-Critical Workflows
Use modern conda (23.10.0+) which includes libmamba, or  if you need additional features. The faster dependency resolution significantly improves workflow efficiency, especially when frequently creating or modifying environments.
  
  
  For Containerized Applications
 for Docker containers. It's a single binary with no installation step, making it perfect for minimal container images. This is particularly useful when building containers for LLM applications with structured output where fast startup and small image size matter.Regardless of which tool you choose, follow these best practices.
For a comprehensive reference of conda commands, see our Conda Cheatsheet:
conda create  myproject 3.11


conda  environment.yml
conda create  environment.yml


conda config  auto_activate_base 
conda config  channels conda-forge
conda config  channel_priority strict
While you can use pip within conda environments, follow this order:Install conda packages first (they handle binary dependencies better)Use pip only for packages unavailable in condaAvoid mixing conda and pip for the same packageThis prevents dependency conflicts and ensures binary compatibility.Use  channel (more packages, better maintained)Enable  in modern conda (default in 23.10.0+)Consider  for CI/CD pipelinesCache packages locally for offline installations is worth mentioning as a conda-forge based distribution that comes with Mamba pre-installed. It's open-source focused, uses only conda-forge channel by default, and provides the best of both worlds: minimal installation with fast dependency resolution.Miniforge is ideal if you:Prefer open-source packages exclusivelyWant Mamba included from the startNeed a middle ground between Anaconda and MinicondaThe choice between Anaconda, Miniconda, and Mamba depends on your specific needs:: Best for beginners and quick starts with comprehensive pre-installed tools: Ideal for production, containers, and custom environments: Essential for performance-critical workflows with complex dependenciesModern conda (23.10.0+) includes Mamba's performance improvements by default, so you get the best of both worlds. For most users, Miniconda with modern conda provides the optimal balance of flexibility, performance, and control.Remember that these tools complement rather than replace each other. You might use Anaconda for initial exploration, Miniconda for production, and Mamba for environments requiring frequent updates. The key is understanding when each tool provides the most value for your specific workflow.]]></content:encoded></item><item><title>ðŸ”¥ Trending Now: Today&apos;s Top Updates &amp; Viral Links - 2026-01-28</title><link>https://dev.to/tarun_walia_ad82c14a97e0c/trending-now-todays-top-updates-viral-links-2026-01-28-5f0f</link><author>tarun walia</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 10:44:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Here is the list of latest trending updates fetched on 2026-01-28. Click on the main link to view:        <p style="margin-top: 15px; color: #444; line-height: 1.6;">
            <strong>Description:</strong><br>
            Upgrade your daily rotation with the Men's Solid Tan Relaxed Fit Shirt. Itâ€™s comfortable, versatile, and looks great with everything. Grab this exclusive deal now on Socioglamm before it's gone
        </p>
    </div>
]]></content:encoded></item><item><title>DevOps Best Practices: How Modern Teams Build, Test, and Deploy Software Faster</title><link>https://dev.to/techgenius/devops-best-practices-how-modern-teams-build-test-and-deploy-software-faster-59ik</link><author>Muhammad Kazim</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 10:35:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you are building software today, DevOps is no longer optional â€” it is a competitive advantage.Many developers still face the same problems: slow release cycles, frequent bugs, unstable systems, and poor collaboration between development and operations teams. This is exactly where DevOps best practices change everything.Modern DevOps is not just about tools. It is about creating a culture of automation, continuous improvement, and shared responsibility. High-performing teams use CI/CD pipelines to reduce human errors, cloud infrastructure to scale on demand, monitoring systems to detect issues early, and DevSecOps practices to keep applications secure from day one.In my latest guide, I break down DevOps best practices simply and practically, including:â€¢ How CI/CD pipelines speed up development
â€¢ Why automation is the backbone of DevOps
â€¢ How cloud platforms improve reliability and scalability
â€¢ The role of monitoring and observability
â€¢ How DevSecOps protects modern applicationsIf you are a developer, startup founder, or tech learner, this guide will help you understand how real-world DevOps workflows operate.]]></content:encoded></item><item><title>Hugo van Kemenade: Speeding up Pillow&apos;s open and save</title><link>https://hugovk.dev/blog/2026/faster-pillow/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 10:29:04 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[I tried out
Tachyon,
the new â€œhigh-frequency statistical sampling profilerâ€ coming in
Python 3.15, to see if we can
speed up the Pillow imaging library. I started with a simple script to open an image:Which generates this flame graph:The whole thing took 40 milliseconds, with half in â€™s . If you visit
the interactive HTML page we can see  calls
, which in turn imports , , 
and  (hover over the  boxes to see them).Do we really need to import all those plugins when weâ€™re only interested in PNG?Okay, letâ€™s try another kind of image:Hmm, 60 milliseconds with 80% in  and most of that in . The
HTML page shows it imports , ,
, ,  and . We also
have  importing ,  and .Again, why import  plugins when we only care about WebP?Loading all the plugins? Thatâ€™s enough profiling, letâ€™s look at the code.When
ing
or
ing
an image, if Pillow isnâ€™t yet initialised, we call a

function. This loads five drivers for five formats by importing their plugins: BMP, GIF,
JPEG, PPM and PNG.During import, each plugin
registers
its file extensions, MIME types and some methods used for opening and saving.Then we check each of these plugins in turn to see if one will accept the image. Most of
Pillowâ€™s plugins detect an image by opening the file and checking if the first few bytes
match a magic prefix. For example:GIF
starts with  or .JPEG
starts with , where  means â€œStart of Imageâ€, and  is
the start of the next marker
(reference).If none of these five match, we call
,
which imports the remaining 42 plugins. We then check each of these for a match.This has been the case since at least
PIL 1.1.1
released in 2000 (this is the oldest version I have to check). There were 33 builtin
plugins then and 47 now.This is all a bit wasteful if we only need one or two image formats during a programâ€™s
lifetime, especially for things like CLIs. Longer running programs may need a few more,
but unlikely all 47.A benefit of the plugin system is third parties can
create their own plugins,
but we can be more efficient with our builtins.I opened a PR to add a mapping of
file extensions to plugins. Before calling  or , we can instead do a
cheap lookup, which may save us importing, registering, and checking all those plugins.Of course, we may have an image without an extension, or with the â€œwrongâ€ extension, but
thatâ€™s fine; I expect itâ€™s rare and anyway weâ€™ll fall back to the original 
->  flow.After merging the PR, hereâ€™s a new flame graph for opening PNG
(HTML page):The flame graphs are scaled to the same width, but thereâ€™s far fewer boxes meaning
thereâ€™s much less work now. Weâ€™re down from 40 and 60 milliseconds to 20 and 20
milliseconds.The PR has a bunch of benchmarks which show opening a PNG (that previously loaded five
plugins) is now 2.6 times faster. Opening a WebP (that previously loaded all 47
plugins), is now 14 times faster. Similarly, Saving PNG is improved by 2.2 times and
WebP by 7.9 times. Success! This will be in Pillow 12.2.0.]]></content:encoded></item><item><title>EuroPython: Humans of EuroPython: Rodrigo GirÃ£o SerrÃ£o</title><link>https://blog.europython.eu/humans-of-ep-rodrigo/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 10:07:03 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[EuroPython depends entirely on the dedication of volunteers who invest tremendous effort into bringing it to life. From managing sponsor relationships and designing the event schedule to handling registration systems and organizing social events, countless hours of passionate work go into ensuring each year surpasses the last.Discover our recent conversation with Rodrigo GirÃ£o SerrÃ£o, who served on the EuroPython 2025 Programme Team.We&aposre grateful for your work on the conference programme, Rodrigo!EP: Had you attended EuroPython before volunteering, or was volunteering your first experience with it?When I attended my first EuroPython in person I was not officially a volunteer but ended up helping a bit. Over the years, my involvement with EuroPython as a volunteer and organiser has been increasing exponentially!EP: Are there any new skills you learned while volunteering at EuroPython? If so, which ones?Volunteering definitely pushed me to develop many skills. As an example, hosting the sprints developed my social skills since I had to welcome all the participants and ensure they had everything they needed. It also improved my management skills, from supporting the project sprint organisers to coordinating with venue staff.EP: Did you have any unexpected or funny experiences during EuroPython?In a recent EuroPython someone came up to me after my tutorial and said something like â€œI doubted your tutorial was going to be good, but in the end it was goodâ€. Why on Earth would that person doubt me in the first place and then come to me and admit it? ðŸ¤£EP: Did you make any lasting friendships or professional connections through volunteering?Yes to both! Many of these relationships grew over time through repeated interactions across multiple EuroPython editions and also other conferences. Volunteering created a sense of continuity and made it much easier to connect with the same people year after year.EP: If you were to invite someone else, what do you think are the top 3 reasons to join the EuroPython organizing team?Nothing beats the smiles and thank youâ€™s you get when the conference is over. Plus, it is an amazing feeling to be part of something bigger than yourself.EP: Would you volunteer again, and why?]]></content:encoded></item><item><title>How Fortune 500 Companies Handle Instagram Counterfeit Accounts (Data Inside)</title><link>https://dev.to/redrep0/how-fortune-500-companies-handle-instagram-counterfeit-accounts-data-inside-jmn</link><author>Red Repo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 10:03:29 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I've spent the last 3 years building Instagram enforcement solutions for Fortune 500 companies. Here's what most brands get wrong - and how the successful ones protect themselves:: 47 fake accounts targeting themSuccess rate of DIY reporting: 12-18%Time to remove an account (DIY): 14-28 days: 15-30% revenue impact
  
  
  ðŸŽ¯ What Successful Companies Do Differently

  
  
  1. Proactive vs Reactive Monitoring
Successful Brands   Average Brands
24/7 monitoring Monthly checks
AI detection alerts Manual discovery
Real-time response  Weeks to respond
  
  
  2. Legal Documentation Stack
Copyright registration proof: 300% higher success rate with proper docs.
  
  
  3. Platform-Specific Strategies
Timeline    Brand Damage
<72 hours removal   5% impact
1+ month    80% impactAccounts removed within 72 hours cause minimal brand damage. After that, they become "normalized" in search results.
  
  
  5. Compliance is Non-Negotiable
 (quarterly)
  
  
  ðŸš€ Case Study: Fashion Brand Recovery
Before Intervention:
â€¢ 128 counterfeit accounts
â€¢ 5.2M fake followers total
â€¢ $750k/month lost sales
â€¢ Search results: 80% fakesAfter (72 hours):
â€¢ All accounts removed
â€¢ 94% cleaner search results
â€¢ Brand CTR: +320%
  
  
  â“ Common Questions Answered
Q: Can't I just use Instagram's report button?
A: For 1-2 accounts, yes. For systematic protection, no. The reporting system is designed for individual users, not brand protection at scale.Q: What about Instagram's "Rights Manager"?
A: Great for content, not for accounts. Different systems, different teams.Q: How much does this cost for a brand?
A: Enterprise solutions start at $3,500/month. ROI is typically 10-20x within 3 months.
A: Fully compliant. We're SOC2 certified and used by Fortune 500 legal teams.
  
  
  ðŸ“ˆ Quick Audit for Your Brand
Search your brand name on InstagramCount fake/impersonator accountsMultiply by your average order valueThat's your monthly risk exposure: 20 fake accounts Ã— $,000/day risk = $**,000/month: I'm the CTO at RedRepo. We build enterprise Instagram enforcement platforms. Last month we removed 847 accounts for clients with 89% success rate.: If you're losing revenue to counterfeit accounts, our team can provide a free risk assessment.: @redrepo on Telegram (response <15 minutes)Verified RedRepo Team Member | SOC2 Type II Certified | 89% Success Rate]]></content:encoded></item><item><title>Python Functions &amp; Arguments: From Basics to *args and **kwargs</title><link>https://dev.to/suchismita_moharana_f8e8a/python-functions-arguments-from-basics-to-args-and-kwargs-ba1</link><author>Suchismita Moharana</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 09:54:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you are learning Python, functions feel easy at first. You define them, pass arguments, get results. Then one day you encounter  and , and suddenly function calls look confusing and unpredictable.This article is designed as one complete learning source. We will start from absolute basics and slowly move toward advanced, productionâ€‘ready patterns used in real Python libraries.By the end of this post, a beginner will understand not just  and  are, but  they exist and  professionals design APIs using them.We will build everything step by step using one evolving example:  ðŸ¥ªA function is a reusable block of code that:accepts input (arguments)optionally returns output
 â†’  (defined in function) â†’  (passed during call)This distinction matters once arguments become flexible.
  
  
  2. Positional Arguments (Order Matters)
Python assigns values :If you swap them, Python wonâ€™t complain â€” but your logic will break.
  
  
  3. Keyword Arguments (Name Matters)

  
  
  Critical Rule (Very Important)
Python always assigns positional arguments first, from left to right, before it processes keyword arguments.This single rule explains most  confusion.
  
  
  4. Why Fixed Arguments Donâ€™t Scale
Imagine a real sandwich order:You  keep adding parameters forever:This becomes unreadable and fragile.Python solves this with .
  
  
  5.  â€” Variable Positional Arguments
 collects extra positional arguments into a .Number of values is unknown
  
  
  6.  â€” Variable Keyword Arguments
 collects  into a .
  
  
  7. Using  and  Together
Correct order  be followed:
  
  
  8. Why  Became Positional (Common Confusion)
Python assigns positional arguments .Now  already has a value ().TypeError: got multiple values for argument 'bread'
Nothing is converted. Position alone decides binding.
  
  
  9. Defensive 
  
  
  10. Productionâ€‘Ready Version (Recommended)

  
  
  Why This Is Productionâ€‘Ready
Keywordâ€‘only arguments ()No hidden positional traps
  
  
  11. Realâ€‘World Usage from Popular Libraries
 â†’ message formattingFlexible APIs powered by .Keyword arguments allow backward compatibility and clean defaults.
  
  
  12. Mental Model That Sticks
 â†’ extra Positional arguments Keywords cannot override positionshow Python binds argumentswhen to use  vs You are no longer a beginner.You are designing flexible, professional Python APIs â€” exactly how real libraries do it.]]></content:encoded></item><item><title>Project: Monetizing TikTok Content with Lightweight ML (Learning in Public)</title><link>https://dev.to/jrsteve_eth/project-moniteze-tiktok-ml-4pga</link><author>JR. STEVE JUNIN</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 09:51:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This is not a â€œget rich fastâ€ project.Iâ€™m experimenting with a simple idea:can lightweight ML signals help creators understand what content  be monetizableâ€”without chasing hype or vanity metrics?This project is part of my learning journey, shared openly.
  
  
  What the project does (at a high level)
The system looks at , such as:basic engagement ratios (views / likes / comments)simple trend patterns over timeUsing these, it produces , not predictions.No automation.âŒ Not guaranteed monetization
Itâ€™s an , not a product.
  
  
  Tech stack (simple on purpose)
Basic ML heuristics (no heavy models)CSV / lightweight storageKeeping it small helps me understand every part of the system.I come from a small community with limited access to formal tech education.
Sharing progress publicly helps me:document mistakes honestlyThis is , nothing more.Better data visualizationDecide whether this stays a learning project or evolves furtherIâ€™d appreciate any constructive feedback or pointers.`{% details Project status %}
This project is still experimental and under active learning.
Nothing here should be considered production-ready.
{% enddetails %} 
]]></content:encoded></item><item><title>PyCharm: Google Colab Support Is Now Available in PyCharm 2025.3.2</title><link>https://blog.jetbrains.com/pycharm/2026/01/google-colab-support-is-now-available-in-pycharm-2025-3-2/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 09:33:49 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Building Social Media Tools That Detect &amp; Leverage Viral Trends Like the Penguin Meme</title><link>https://dev.to/pixel_mosaic/building-social-media-tools-that-detect-leverage-viral-trends-like-the-penguin-meme-ppj</link><author>Pixel Mosaic</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 09:29:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Viral trends donâ€™t just  â€” they leave data footprints everywhere.From the sudden rise of the  to overnight TikTok sounds and Twitter phrases, trends follow recognizable patterns across platforms. In this article, weâ€™ll build a developerâ€‘friendly system that detects viral trends early  helps you leverage them programmatically.This post is written for , indie hackers, and  builders who want to ship trendâ€‘aware products.
  
  
  What Makes a Meme Go Viral?
Before writing code, letâ€™s define virality in measurable terms: â€“ how fast mentions increase â€“ total number of mentions â€“ likes, shares, comments â€“ appears on multiple networks â€“ remixes, captions, variationsThe Penguin meme exploded because it hit  within hours.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Social APIs â”‚â”€â”€â–¶â”‚ Trend Engineâ”‚â”€â”€â–¶â”‚ Action Layer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                  â”‚
   Twitter/X          Detection         Auto-posting
   Reddit             Scoring           Alerts
   TikTok              NLP              Content ideas

  
  
  Step 1: Collect Social Signals

  
  
  Example: Twitter/X Mention Stream (Node.js)
ðŸ’¡ Tip: Normalize all platforms into a common schema:
  
  
  Step 2: Detect Trend Velocity
If mentions jump , youâ€™re likely seeing early virality.
  
  
  Step 3: NLP for Meme Context

  
  
  Step 4: Trend Scoring Formula
Anything above a threshold (e.g. ) is worth acting on.
  
  
  Step 5: Leverage the Trend (The Fun Part)

  
  
  Autoâ€‘Generate Content Ideas
ðŸ“Š Social media dashboardsðŸ§  Creator inspiration toolsðŸ›ï¸ Eâ€‘commerce trend hijacking
  
  
  Lessons from the Penguin Meme
âœ” Trends start niche
âœ” Acceleration matters more than size
âœ” Memes evolve faster than keywords
âœ” Timing beats perfectionYou donâ€™t need a massive ML team to catch the next viral moment.â€¦you can build tools that see trends forming before they peak.If you enjoyed this, consider extending it with:Image similarity detection]]></content:encoded></item><item><title>AWS EKS: Create Your First Cluster Using AWS CDK</title><link>https://dev.to/danielcristho/aws-eks-create-your-first-cluster-using-aws-cdk-fg2</link><author>Daniel Pepuho</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 08:33:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ Create an EKS cluster using AWS CDK Python Provision infrastructure via AWS CloudFormation The setup includes a VPC, EKS control plane, and a managed EC2 node group Configure IAM access to use  Verify the cluster and clean up resources when doneAbout 2 years ago, Iâ€™ve been using AWS CDK to manage infrastructure, mostly for smaller workloads like deploying APIs on AWS Lambda. You can see the repo here: cdk-go-simple-restapi. This year, I want to take it a step further by using AWS CDK to create and manage an AWS EKS cluster.Iâ€™ll be writing a short series of posts around this topic, starting with the basics. In this first post weâ€™ll focus on building a minimal EKS cluster using AWS CDK as the foundation for the next parts of the series.Amazon EKS already provides multiple ways to create and manage clusters, from the AWS Console to CloudFormation and Terraform. However, when infrastructure starts to grow in complexity, the way itâ€™s defined and maintained becomes just as important as the resources themselves.CDK allows you to define infrastructure using familiar programming languages. Instead of managing large YAML or JSON templates, you work with codeâ€”loops, conditions, and abstractions included. This makes infrastructure easier to reason about, review, and evolve over time. According to the documentation, AWS CDK supports multiple languages such as Python, Go, TypeScript, JavaScript, and C#For EKS specifically, the kit provides higher-level constructs that abstract away a lot of the boilerplate required to get a cluster running. Networking, IAM roles, and node groups can be defined in a few lines of code while still allowing you to customize the parts that matter.
  
  
  How AWS CDK Works with Amazon EKS
The diagram above shows us how CDK interacts with AWS services when creating an Amazon EKS cluster.Everything starts from your CDK application (stack), where infrastructure is defined using a programming language such as Go, TypeScript, or Python. At this stage, no AWS resources are created yet. When you run , AWS CDK translates your code into a standard AWS CloudFormation template.This CloudFormation template is then deployed using . At this point, AWS CloudFormation becomes responsible for provisioning the infrastructure. It creates all required AWS resources, including the VPC, IAM roles, the EKS control plane, and the managed node group. AWS CDK itself does not bypass CloudFormation, it simply acts as a higher-level abstraction on top of it.Once CloudFormation finishes, the Amazon EKS cluster is fully provisioned and exposes a Kubernetes control plane backed by AWS-managed infrastructure. From this point forward, the cluster behaves like a standard Kubernetes cluster.Optionally, the CDK can also interact with the Kubernetes API to manage workloads, such as applying Kubernetes manifests or deploying Helm charts. This step usually happens after the cluster is ready and is covered in later parts of this series.In this series, all AWS CDK examples will be written in Python.
While AWS CDK supports multiple languages such as TypeScript and Go, the underlying concepts and constructs remain the same regardless of the language used.AWS account and configured AWS CLIAn active AWS account with permissions to create EKS, VPC, IAM, and CloudFormation resources. Installed and configured with valid credentials ().Used for writing the AWS CDK application in this series.Installed globally via npm:Used to interact with the EKS cluster once it is created.This post focuses on creating the EKS cluster. No Kubernetes workloads are deployed yet.Make sure your AWS credentials have sufficient permissions before running cdk deploy, as EKS provisioning may take several minutes.Weâ€™ll start by creating a new AWS CDK project using Python. AWS CDK provides a project template that sets up the basic structure, dependencies, and configuration needed to get started.First, create a new directory and initialize a CDK app:cdk-eks
cdk-eks
cdk init app  python
This command generates a basic project structure for a Python-based CDK application. The output after you run Applying project template app python



This is a blank project CDK development with Python.

The cdk.json file tells the CDK Toolkit how to execute your app.

...

Enjoy!

Executing Creating virtualenv...
Executing Installing dependencies...
âœ… All After initialization, you should see a structure similar to this:
â”œâ”€â”€ app.py
â”œâ”€â”€ cdk_eks
â”‚Â Â  â”œâ”€â”€ cdk_eks_stack.py
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ cdk.json
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ source.bat
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â””â”€â”€ unit
The entry point of the CDK application. This is where the stack is instantiated.Contains the definition of the CDK stack where weâ€™ll define the EKS cluster.Python dependencies for the CDK app.CDK configuration file, including the command used to run the app.A placeholder for unit tests, which weâ€™ll use in a later post.Before writing any code, activate the Python virtual environment and install the dependencies. BTW, you need add this one to your :  aws-cdk.lambda-layer-kubectl-v34. .venv/bin/activate
pip  requirements.txt
Keeping dependencies isolated using a virtual environment helps avoid version conflicts and keeps the project reproducible.If this is your first time using AWS CDK in the selected AWS account and region, youâ€™ll need to bootstrap the environment:This command creates the necessary resources in your AWS account that CDK requires to deploy stacks.At this point, the project is ready, and we can start defining the EKS cluster using AWS CDK.In this section, weâ€™ll define a minimal EKS cluster using AWS CDK. The goal is not to build a production-ready cluster yet, but to create a foundation that we can extend in later posts.All changes will be made inside the CDK stack file Import Required ModulesOpen  and start by importing the required CDK modules: for creating the EKS cluster and its node groupsEKS requires a VPC. For simplicity, weâ€™ll let CDK create one for us:This creates a basic VPC across two Availability Zones, which is sufficient for a minimal cluster.Next, we define the EKS cluster itself:Kubernetes version is explicitly set to avoid unexpected upgrades. disables the default node group so we can define our own. Add a Managed Node GroupNow we add a managed node group to run workloads:A managed EC2-based node groupAutoscaling between 1 and 3 nodesInstance type for testingBy default, creating an EKS cluster with AWS CDK does not automatically grant Kubernetes access to the IAM identity used to deploy the stack. While the control plane and node group roles are configured, additional IAM-to-Kubernetes RBAC mapping is required to access the cluster using kubectl.At this point, our CDK stack defines:A managed node group for worker nodesAll of this is defined as code and will be provisioned via CloudFormation.Before deploying, itâ€™s a good idea to check what CDK will generate:cdk synth 2>&1 | 

Resources:
  KubectlLayer600207B5:
    Type: AWS::Lambda::LayerVersion
    Properties:
      Content:
        S3Bucket:
          Fn::Sub: cdk-hnb659fds-assets-::AccountId-::Region
        S3Key: cc5bb5a423d0f1ccbfa20b3016434049b477f393e38ad2be0e8cba029f2a2373.zip
      Description: /opt/kubectl/kubectl 1.32.3 /opt/helm/helm 3.17.2
      LicenseInfo: Apache-2.0

...
This command outputs the CloudFormation template that will be used to create the EKS cluster.With the stack definition in place, we can now deploy the EKS cluster using AWS CDK. This step will trigger AWS CloudFormation to provision all required resources.Run the following command from the project root:During deployment, CDK will:Package and upload assets (if any)Create or update the CloudFormation stackProvision AWS resources such as the VPC, IAM roles, EKS control plane, and managed node groupYouâ€™ll be prompted to confirm the deployment, as the stack creates IAM roles and other security-related resources. Review the changes and approve the deployment when prompted.After the deployment completes, the next step is to verify that the EKS cluster has been created successfully and is in a healthy state.At this point, you should see a CloudFormation stack created by AWS CDK in the AWS Console.
This stack represents all resources defined in the CDK application, including the VPC, IAM roles, EKS control plane, and the managed node group.The stack status should be , indicating that all resources were provisioned without errors. This also reinforces that AWS CDK relies on CloudFormation as the underlying provisioning engine.Next, navigate to the Amazon EKS console. You should see the newly created cluster listed and marked as Active. Accessing the Cluster with kubectlTo interact with the cluster, update your kubeconfig:aws eks update-kubeconfig  <cluster-name>  <region>
Once configured, verify that the nodes are registered:This command deletes the CloudFormation stack and all resources created by AWS CDK, including the EKS cluster, node groups, and networking components.
Since EKS is not covered by the AWS free tier, itâ€™s recommended to clean up resources when they are no longer needed.At this point, we have a fully functional Amazon EKS cluster created and managed using AWS CDK. The cluster is accessible via kubectl, the node group is running, and IAM access has been configured correctly. This gives us a foundation, but so far, the cluster is still empty.In the next post, weâ€™ll move beyond infrastructure and start running actual workloads on this cluster. Instead of applying raw Kubernetes YAML files manually, weâ€™ll use AWS CDK to deploy workloads in a more structured and repeatable way.Aight, thanks for taking the time read this post. Here is the full code of this post:]]></content:encoded></item><item><title>ðŸ§© Building MazeMath: A Python App That Turns Arithmetic Into Maze Puzzles</title><link>https://dev.to/matetechnologie/building-mazemath-a-python-app-that-turns-arithmetic-into-maze-puzzles-35e1</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 07:45:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build MazeMath â€” a desktop app that generates arithmetic maze puzzles, shows step-by-step solutions, and exports worksheets as PDF or JPG.Tkinter + ttkbootstrap (GUI)By the end, youâ€™ll have a working educational puzzle generator.Generates math mazes (Easy / Medium / Hard)Guarantees a solvable pathShows step-by-step arithmeticHighlights the solution visuallyExports puzzles as PDFs or JPGsSupports multiple puzzles at onceGreat for classrooms, tutoring, or learning projects.First, install the required libraries:pip install ttkbootstrap reportlab pillowTkinter usually comes bundled with Python.Start by importing everything weâ€™ll need:import tkinter as tk
from tkinter import messagebox, filedialog
import random
import operator
import ttkbootstrap as tb
from ttkbootstrap.constants import *

from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4

from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
ttkbootstrap â†’ modern dark themerandom + operator â†’ puzzle mathreportlab â†’ PDF generationCreating the Main App ClassWe wrap everything in a class called MazeMath.class MazeMath:
    APP_NAME = "MazeMath"
    APP_VERSION = "2.1"

    OPERATORS = {
        "+": operator.add,
        "-": operator.sub,
        "*": operator.mul,
        "/": operator.floordiv
    }
APP_NAME and APP_VERSION are just labelsOPERATORS maps math symbols to Python functionsInside , we set up the main window and variables:def __init__(self):
    self.root = tk.Tk()
    tb.Style(theme="darkly")

    self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
    self.root.geometry("1100x680")

    self.difficulty_var = tk.StringVar(value="Easy")
    self.num_puzzles_var = tk.IntVar(value=1)

    self.grid_numbers = []
    self.grid_ops = []
    self.solution_path = []
    self.target_number = None

    self.rows = self.cols = 0

    self._build_ui()
Building the User InterfaceNow we create buttons, dropdowns, and panels:def _build_ui(self):
    tb.Label(self.root, text=self.APP_NAME,
             font=("Segoe UI", 22, "bold")).pack(pady=10)

    opts = tb.Labelframe(self.root, text="Options", padding=10)
    opts.pack(fill="x", padx=10)

    tb.Combobox(opts,
        values=["Easy","Medium","Hard"],
        textvariable=self.difficulty_var,
        width=10).pack(side="left", padx=5)

    tb.Spinbox(opts, from_=1, to=20,
        textvariable=self.num_puzzles_var,
        width=5).pack(side="left", padx=5)
Then we add control buttons:tb.Button(ctrl, text="Generate",
          command=self.generate_single_puzzle).pack(side="left")

tb.Button(ctrl, text="PDF",
          command=self.generate_multiple_combined_pdf).pack(side="left")

tb.Button(ctrl, text="JPG",
          command=self.generate_multiple_jpgs).pack(side="left")
A grid area for the puzzleA text box for the solution stepsWe use Depth-First Search to ensure every puzzle is solvable.def generate_maze(self, rows, cols):
    visited = [[False]*cols for _ in range(rows)]
    path = []

    def dfs(r, c):
        visited[r][c] = True
        path.append((r,c))

        dirs = [(0,1),(1,0),(0,-1),(-1,0)]
        random.shuffle(dirs)

        for dr, dc in dirs:
            nr, nc = r+dr, c+dc
            if 0 <= nr < rows and 0 <= nc < cols and not visited[nr][nc]:
                dfs(nr, nc)

    dfs(0,0)
    return path
This gives us a guaranteed path from start to finish.Now we place numbers and operators along that path:def create_puzzle_data(self):
    diff = self.difficulty_var.get()

    if diff == "Easy":
        self.rows = self.cols = 3
    elif diff == "Medium":
        self.rows = self.cols = 4
    else:
        self.rows = self.cols = 5
path = self.generate_maze(self.rows, self.cols)

current = random.randint(1,9)
steps = [f"Start: {current}"]
For every next cell, we safely apply math:for r,c in path[1:]:
    op = random.choice(list(self.OPERATORS.keys()))
    num = random.randint(1,9)

    if op == "/" and current % num != 0:
        continue

    next_val = self.OPERATORS[op](current, num)
    steps.append(f"{current} {op} {num} = {next_val}")
    current = next_val
We draw labels for each cell:for r in range(self.rows):
    for c in range(self.cols):
        text = f"{op}{num}"
        bg = "#4caf50" if (r,c) in self.solution_path else "#222"

        tb.Label(self.grid_frame,
            text=text,
            background=bg).grid(row=r,column=c)
Green cells show the solution path.Showing Step-by-Step Solutions
def show_solution(self):
    self.solution_text.delete("1.0", tk.END)
    self.solution_text.insert(tk.END, "\n".join(steps))
Start: 5
5 + 3 = 8
...c = canvas.Canvas("puzzle.pdf", pagesize=A4)
c.drawString(50, 800, "MazeMath Puzzle")
Each puzzle becomes a printable worksheet.img = Image.new("RGB", (900, 800), (34,34,34))
draw = ImageDraw.Draw(img)
if __name__ == "__main__":
    MazeMath().run()
Itâ€™s a great example of how Python can be used for real educational tools, not just scripts.If you enjoyed this project:â­ Star the repo
ðŸ› Open issues]]></content:encoded></item><item><title>Tracking Video Progress with Bitmaps</title><link>https://dev.to/cobel1024/tracking-video-progress-with-bitmaps-452g</link><author>Dora</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 06:37:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most LMS platforms track video completion as percentage or "watched/not watched".
This doesn't tell you:Which parts were actually watched vs skippedWhich segments were rewatchedUse PostgreSQL's varbit (variable-length bit string) where each bit represents one second of video.Orange: watched, Gray: unwatchedAll edge cases handled automaticallyPostgreSQL native functionFast calculation of watched secondsExisting: 11110000
New:      00001111
Result:   11111111  (single operation merge)
Rewatching same segment? OR operation is idempotent.
Skip and come back? Handled automatically.
Multiple sessions? Just keep OR-ing.2-hour video = 7,200 bits = 900 bytes
With gzip: ~100-200 bytes (depends on watch pattern)Timestamp array: hundreds of integers = kilobytesTime ranges: complex merge logic + storage overheadMerge: Complex overlap handlingQuery: Range intersection logicMerge: Single OR operationQuery: Native bit functionsHandle different bitmap lengths (resize if needed)Merge new bits with existing using OR operationCalculate watch rate from bit countDetermine pass/fail based on threshold
 - Bitwise OR merge - Count watched secondsRPAD/SUBSTRING - Automatic length handlingBitmaps can be large (2-hour video = 7200 bits).
Compress with gzip before sending over network.Second-by-second accurate trackingEdge cases (pause, skip, rewatch) handled without special codeCompact storage for long videosFast queries using PostgreSQL bit operationsVideo: 100 seconds
User watches: 0-30s, skips to 60s, watches 60-80s1111111111111111111111111111110000000000000000000000000000001111111111111111111100000000000000000000
]]></content:encoded></item><item><title>Book Manali to Delhi Taxi &amp; Cab with Guruji Travels</title><link>https://dev.to/guruji_travels_d0319e80d3/book-manali-to-delhi-taxi-cab-with-guruji-travels-16l3</link><author>Guruji Travels</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 06:26:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Manali to Delhi Taxi Service
Book Manali to Delhi Taxi & Cab with Guruji TravelsLooking for a safe, comfortable, and affordable Manali to Delhi Taxi? Guruji Travels Pvt. Ltd. offers reliable Manali to Delhi Cab Service with 24x7 availability, professional drivers, and well-maintained vehicles. Whether you need a one-way taxi, round trip cab, or a Tempo Traveller from Manali to Delhi, we ensure a smooth and stress-free journey.Guruji Travels is a trusted travel brand providing the cheapest Manali to Delhi Taxi fare without compromising on safety or comfort.24 Hours Manali to Delhi Taxi & Cab availabilityClean, neat, and fully sanitized vehiclesExperienced, verified, and knowledgeable driversDoor-to-door pickup & drop serviceTransparent billing with no hidden chargesEasy online booking & instant phone bookingNo advance payment required on selected bookingsWhen you book a Taxi from Manali to Delhi with Guruji Travels, you choose quality, reliability, and peace of mind.Your safety is our top priority. All Manali to Delhi Taxis and Cabs are sanitized before every trip. Our drivers strictly follow government safety guidelines:Mandatory masks for driversHand sanitizers available in vehiclesRegular vehicle sanitizationTrained drivers for long-route travelTravel confidently with Manali to Delhi Taxi Service by Guruji Travels.Manali to Delhi Taxi Fare (One Way / Round Trip)We provide the best Manali to Delhi Taxi fare at the lowest possible price. The fare depends on:Driver allowance & trip durationðŸ“ž Call for best Manali to Delhi one-way taxi fare:
+91 9716221106 | +91 9212419391Vehicle Options Available for Manali to DelhiChoose from multiple cab options for Manali to Delhi Cab Booking:Hatchback â€“ Best for solo travelersSedan â€“ Comfortable for couples & small familiesErtiga Cab â€“ Ideal for 5â€“6 passengersInnova & Innova Crysta â€“ Premium long-distance travelTempo Traveller (12, 16 Seater & 1x1 Maharaja) â€“ Perfect for groupsLooking for a safe, comfortable, and affordable Manali to Delhi Taxi? Guruji Travels Pvt. Ltd. offers reliable Manali to Delhi Cab Service with 24x7 availability, professional drivers, and well-maintained vehicles. Whether you need a one-way taxi, round trip cab, or a Tempo Traveller from Manali to Delhi, we ensure a smooth and stress-free journey.Guruji Travels is a trusted travel brand providing the cheapest Manali to Delhi Taxi fare without compromising on safety or comfort.Why Choose Guruji Travels for Manali to Delhi Taxi?24 Hours Manali to Delhi Taxi & Cab availabilityClean, neat, and fully sanitized vehiclesExperienced, verified, and knowledgeable driversDoor-to-door pickup & drop serviceTransparent billing with no hidden chargesEasy online booking & instant phone bookingNo advance payment required on selected bookingsWhen you book a Taxi from Manali to Delhi with Guruji Travels, you choose quality, reliability, and peace of mind.Your safety is our top priority. All Manali to Delhi Taxis and Cabs are sanitized before every trip. Our drivers strictly follow government safety guidelines:Mandatory masks for driversHand sanitizers available in vehiclesRegular vehicle sanitizationTrained drivers for long-route travelTravel confidently with Manali to Delhi Taxi Service by Guruji Travels.Manali to Delhi Taxi Fare (One Way / Round Trip)We provide the best Manali to Delhi Taxi fare at the lowest possible price. The fare depends on:One-way or round trip bookingDriver allowance & trip durationðŸ“ž Call for best Manali to Delhi one-way taxi fare:
+91 9716221106 | +91 9212419391Vehicle Options Available for Manali to DelhiChoose from multiple cab options for Manali to Delhi Cab Booking:Hatchback â€“ Best for solo travelersSedan â€“ Comfortable for couples & small familiesErtiga Cab â€“ Ideal for 5â€“6 passengersInnova & Innova Crysta â€“ Premium long-distance travelTempo Traveller (12, 16 Seater & 1x1 Maharaja) â€“ Perfect for groupsIf you are traveling with family or friends, booking a Taxi Cab for Manali to Delhi ensures comfort and convenience throughout the journey.
Contact Guruji TravelsAddress:
691/1, 1st Floor, Opp Palm Gardens, Main Road, Burari, Delhi â€“ 110084Phone:
9870280071, 9218091913Manali to Delhi Taxi Service Keywords]]></content:encoded></item><item><title>Jinja2TT2: Jinja2 to Template Toolkit Transpiler</title><link>https://dev.to/lucianofedericopereira/jinja2tt2-jinja2-to-template-toolkit-transpiler-4ge8</link><author>Luciano Federico Pereira</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 05:59:15 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[A Perl transpiler that converts Jinja2 templates to Template Toolkit 2 (TT2) syntax.Jinja2 is deeply integrated with Python, making a direct port impractical. However, since TT2 and Jinja2 share similar concepts and syntax patterns, this transpiler performs a  between the two template languages.Variable interpolation:  maps to Control structures:  /  map to  / Filters:  maps to Includes, blocks, and inheritance (conceptually similar)Expression grammar close enough to map mechanicallyNo external dependencies beyond core Perl 5.20+.git clone https://github.com/lucianofedericopereira/jinja2tt2
jinja2tt2

./bin/jinja2tt2 template.j2


./bin/jinja2tt2 template.j2  template.tt


./bin/jinja2tt2  template.j2

 | ./bin/jinja2tt2


./bin/jinja2tt2  template.j2
{{ foo }}           â†’  [% foo %]
{{ user.name }}     â†’  [% user.name %]
{{ items[0] }}      â†’  [% items.0 %]
{{ name|upper }}              â†’  [% name.upper %]
{{ name|lower|trim }}         â†’  [% name.lower.trim %]
{{ items|join(", ") }}        â†’  [% items.join(', ') %]
{{ name|default("Guest") }}   â†’  [% (name || 'Guest') %]
{% if user %}          â†’  [% IF user %]
{% elif admin %}       â†’  [% ELSIF admin %]
{% else %}             â†’  [% ELSE %]
{% endif %}            â†’  [% END %]
{% for item in items %}    â†’  [% FOREACH item IN items %]
{{ loop.index }}           â†’  [% loop.count %]
{{ loop.first }}           â†’  [% loop.first %]
{{ loop.last }}            â†’  [% loop.last %]
{% endfor %}               â†’  [% END %]
{% block content %}        â†’  [% BLOCK content %]
{% endblock %}             â†’  [% END %]

{% macro btn(text) %}      â†’  [% MACRO btn(text) BLOCK %]
{% endmacro %}             â†’  [% END %]
{# This is a comment #}    â†’  [%# This is a comment %]
{{- name -}}               â†’  [%- name -%]
{%- if x -%}               â†’  [%- IF x -%]
{% include "file.html" %} â†’  â†’ Ternary:  â†’ Boolean literals: / â†’ /Some filters require TT2 plugins (e.g.,  needs ). () requires manual adjustment for TT2's  pattern is not directly supported in TT2Some filters need custom TT2 plugins or vmethodsComplex Python expressions may need review: Splits Jinja2 source into tokens (text, variables, statements, comments): Builds an Abstract Syntax Tree (AST) from the token stream: Walks the AST and generates equivalent TT2 code - AuthorThis is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License (LGPL) version 2.1 as published by the Free Software Foundation.]]></content:encoded></item><item><title>Top 10 Python Development Companies in the USA to Watch in 2026</title><link>https://dev.to/vasundhara_infotech/top-10-python-development-companies-in-the-usa-to-watch-in-2026-458</link><author>vasundhara infotech</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 05:26:18 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As digital transformation accelerates across industries, businesses are increasingly relying on data-driven and intelligent solutions to stay competitive. In this evolving landscape, Python development has become a foundation for innovation due to its simplicity, scalability, and powerful ecosystem.Whether youâ€™re planning Python web development, Python app development, or seeking custom Python development services, choosing the right Python software development company is a strategic decision that can define your success in 2026. With Python playing a major role in Data Science, Data Analytics, Artificial Intelligence (AI), and Machine Learning (ML), organizations are turning to experienced Python development companies in the USA to build secure, future-ready applications.
  
  
  Factors to Consider When Selecting a Python Development Company
When choosing a Python development company, several critical factors will ensure that the company you partner with aligns with your project goals:Development Cost
Python development costs vary greatly from company to company. There may be some attraction to lower price options, but remember that when you look at the cost of Python development in the US, you should weigh that against the type of service and flexibility that a company can provide. A company providing Python software development at a lower cost may still be able to provide high-quality work that meets or does not exceed your budget.Development Speed
Python's simplicity allows for faster development, essential for meeting deadlines and staying competitive. Choose a Python web development company that can deliver efficient results, giving your project an edge in 2026.Proven Track Record and Experience
The number of years a company has been operating is a good indication of its experience in completing more complex projects. When searching for a Python development company in the US, check out companies with a track record of building strong portfolios for data science and analytics projects, as that is an area where Python excels.Team Size and Expertise
Considering team size affects communication, flexibility, and costs/efficiency. Smaller teams exhibit greater agility than larger teams, which offer a greater number of diverse skills to fulfil complex tasks. Thus, you will want to make sure that you have the right people in your team, specifically, the best developers who write code in Python, with an emphasis on their experience in data sciences and machine learning.Communication and Collaboration
Effective communication and transparent project management are key. Choose a company that values collaboration and is responsive to client feedback throughout the project.
  
  
  Benefits of Partnering with a Python Development Company
Versatility in Applications
Pythonâ€™s adaptability makes it ideal for various applications, from web development to data analytics. A Python development company can help you maximize the potential of this powerful language.Rapid Development and Reduced Time-to-Market
Pythonâ€™s easy syntax and extensive libraries speed up development, reducing time-to-market. This allows faster iterations, essential for staying competitive in data science and AI.Access to a Rich Ecosystem
There are many frameworks available to Python Developers such as Django, Flask, TensorFlow, and PyTorch. With this wide variety of options available, an experienced Python Development Company can use all of these various tools in order to create robust, scalable solutions involving Data Analytics and Machine Learning.  
  
  
  Top 10 Python Development Companies in the USA to Watch in 2026
Here are some Top Python Development Companies in the USA making waves in 2026, Vasundhara Infotech
Location: USA
Team Size: 150+ Professionals
Overview: A technology services and software engineering firm specializing in Python development, artificial intelligence (AI) solutions, full-stack engineering, and custom software for web and cloud platforms.SDLC Corporation
Location: San Francisco, California, USA
Team Size: 250+ Professionals
Overview: A full-cycle software development company delivering scalable Python solutions, enterprise applications, and digital transformation services.BairesDev
Headquarters: San Francisco, California, USA
Company Size: 1,001â€“5,000 Employees (4,000+ Engineers)
Overview: A leading nearshore development firm specializing in Python-based web applications, enterprise systems, APIs, and data engineering.OpenXcell
Headquarters: Las Vegas, Nevada, USA
Company Size: 201â€“500 Employees
Overview: A technology services provider offering Python development, AI/ML integration, and custom web and mobile solutions.ScienceSoft
Headquarters: McKinney, Texas, USA
Company Size: 1,000+ Professionals (Global)
Overview: A veteran IT consulting firm delivering enterprise-grade Python solutions, cloud systems, and data-driven platforms.Intellectsoft
Headquarters: New York, USA
Company Size: 250â€“999 Employees
Overview: A digital transformation company providing customized Python backend development and enterprise software solutions.WPWeb Infotech
Location: USA
Team Size: 100+ Professionals
Overview: A software development firm focused on building reliable Python-based web and business applications.Materialize Labs
Location: USA
Team Size: 50+ Professionals
Overview: A boutique development company delivering modern Python solutions for startups and growing businesses.Saritasa
Location: USA
Team Size: 250+ Professionals
Overview: A custom software development company specializing in enterprise platforms, automation, and Python-powered systems.AppMakersLA
Location: Los Angeles, California, USA
Team Size: 100+ Professionals
Overview: A mobile and web development agency offering Python-based backend and full-stack application development.
  
  
  How to Choose the Right Python Development Company
To select the right Python development company for your project, keep the following factors in mind:Portfolio and Case Studies:
Review the companyâ€™s previous projects to assess its technical expertise and industry experience. Pay close attention to case studies related to Python web development, data analytics, AI, and machine learning to ensure relevant capabilities.Communication and Collaboration:
Strong communication and a collaborative approach help ensure your ideas are clearly understood and effectively implemented throughout the development lifecycle.Scalability and Flexibility:
Choose a Python development company that can scale alongside your business and adapt quickly to changing project requirements and future growth.Competitive Pricing:
While Python development costs in the USA are an important consideration, itâ€™s essential to balance affordability with quality to achieve long-term value.Selecting the right Python development company is essential for successfully delivering AI, machine learning, and data analytics solutions in 2026 and beyond. Whether youâ€™re building a Python-based web application, a customized data-driven platform, or an intelligent AI system, partnering with an experienced development firm ensures access to the right expertise, tools, and technologies.Explore our curated list of the top Python development companies in the USA and take the first step toward creating innovative, scalable, and data-driven solutions. When youâ€™re ready to move forward, connect with a trusted Python development partner to start building your future today.]]></content:encoded></item><item><title>Video Content Search with Captions</title><link>https://dev.to/cobel1024/video-content-search-with-captions-5hl0</link><author>Dora</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 05:25:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Search video content and jump directly to where it's mentioned.Search query matches against video title, description, and captions.
When captions match, jump to caption timestamp. Otherwise, jump to video start.Built with Django and OpenSearch.Models for storing media and subtitles.
  
  
  Step 2: OpenSearch Document Indexing
Automatically index each caption line with timestamp when saving subtitles.
Use NestedField to store time and line together.Parse subtitle file and index each line individuallyStore start time with line (index=False to exclude from search)Use NestedField to manage line and timestamp togetherSearch both metadata and captions simultaneously.Search title/description (2x weight on title)Search caption lines (nested query)Retrieve up to 6 matched lines using inner_hitsMerge results by media_id]]></content:encoded></item><item><title>My Journey Into Data Science and Machine Learning</title><link>https://dev.to/rounak161106/my-journey-into-data-science-and-machine-learning-5d9b</link><author>Rounak Prasad</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 05:11:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi everyone ðŸ‘‹, an aspiring data scientist, and I wanted to share a bit about my learning journey so far and what Iâ€™m currently focusing on.Iâ€™ve always been curious about how data can be used to understand patterns, make predictions, and solve real-world problems. That curiosity gradually led me into Data Science and Machine Learning, where logic, math, and programming come together in a very practical way.
  
  
  What Iâ€™m Currently Learning
Right now, Iâ€™m focused on building , rather than rushing into advanced topics.NumPy and Pandas for data manipulationMatplotlib for data visualizationScikit-learn for basic machine learning modelsHTML, CSS, and JavaScriptVersion control with Git & GitHubWriting clean, structured codeIâ€™m actively working on small projects to apply what I learn, including:Beginner machine learning modelsPersonal portfolio developmentI believe consistent practice and gradual improvement matter more than rushing through topics.I recently built my personal portfolio website to document my journey, projects, and certifications:Iâ€™ll keep updating it as I grow and build more meaningful projects.Deeper understanding of machine learning algorithmsWorking with real-world datasetsImproving problem-solving and analytical thinkingSharing learnings openly through posts like thisIâ€™m still early in my journey, but I believe in learning publicly and improving step by step.
If youâ€™re also learning data science or machine learning, feel free to connect or share your experience.]]></content:encoded></item><item><title>When Bronze Goes Rogue: Schema Chaos in the Wild</title><link>https://dev.to/aawiegel/when-bronze-goes-rogue-schema-chaos-in-the-wild-16kf</link><author>Aaron Wiegel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 05:04:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In Part 1, we explored the Medallion Architecture with clean, well-behaved vendor data. The bronze layer simply landed the raw CSV files. The silver layer standardized measurement names. The gold layer aggregated for analysis. Everything worked beautifully.This post demonstrates what happens when vendor CSV files exhibit the full spectrum of real-world data quality issues. We'll watch the bronze layer transform from "just land the data" into an increasingly complex series of transformations, vendor-specific logic, and fragile workarounds. By the end, we'll be asking uncomfortable questions about what "bronze" actually means.
  
  
  Problem 1: Different Column Names for the Same Measurements
Vendor A and Vendor B measure the same soil properties. Both provide pH, copper concentration, and zinc concentration. Their CSV files look like this:sample_barcode,lab_id,date_received,date_processed,ph,copper_ppm,zinc_ppm
sample_barcode,lab_id,date_received,date_processed,acidity,cu_total,zn_total
Same measurements. Different names. Vendor B calls pH "acidity." They use chemical symbols with  suffixes instead of element names with  suffixes.This is not a data quality problem. This is a legitimate difference in how two professional laboratories name their measurements. (Although pedantically you might wonder about a chemistry lab that thinks pH and acidity are the same thing.) Both schemas are internally consistent and well-documented. The challenge is ours: we need both vendors' data in the same bronze table.
  
  
  Bronze Layer: Approach 1 (Add Vendor-Specific Column Mapping)
We create a standardization function for each vendor:The bronze ingestion now includes a vendor detection step:This works. We can now query both vendors' data using consistent column names. The bronze layer contains standardized schemas.But we just added vendor-specific business logic to what was supposed to be a raw data landing zone.
  
  
  Problem 2: Schema Instability Within the Same Vendor
The vendor-specific mapping holds up until Vendor A sends a new file. Our ingestion pipeline fails with a schema mismatch error. Examining the file reveals that Vendor A now includes additional analytes:Vendor A - Basic package (what we had):sample_barcode,lab_id,date_received,date_processed,ph,copper_ppm,zinc_ppm
Vendor A - Metals package (what we just received):sample_barcode,lab_id,date_received,date_processed,ph,copper_ppm,zinc_ppm,lead_ppm,iron_ppm,manganese_ppm
The schema changes based on which analysis package the customer ordered. Sometimes they order basic soil testing. Sometimes they add heavy metals analysis. The vendor includes only the columns relevant to what was tested.This is also not a data quality problem. Including only requested measurements is reasonable and reduces file size. But it breaks our bronze table schema.
  
  
  Bronze Layer: Approach 2 (Create Superset Schema)
The solution requires accommodating all possible variations. We create a superset schema containing all possible columns from all analysis packages. When ingesting files with fewer columns, we add NULL values for missing measurements:Now our bronze ingestion tracks package types:Our bronze table is sparse (most columns are NULL for most rows)We must maintain a master list of all possible columns for each vendorAdding new analytes requires code changesWe can't distinguish between "wasn't measured" and "measurement failed"The bronze layer is accumulating knowledge about vendor schemas and business rules.
  
  
  Problem 3: Typos in Column Headers
Our superset schema handles varying column sets, but the next issue reveals a different category of problem. A file from Vendor A fails to parse correctly. Examining the raw CSV, we find:sample_barcod,lab_id,date_recieved,date_proccessed,ph,copper_ppm,zinc_ppm
Three typos:  (missing 'e'),  (i before e),  (double c). The vendor's export system mangles column names. Occasionally.These files are otherwise valid. The data values are correct. Only the header row has issues. Rejecting these files would delay processing by days while we contact the vendor.
  
  
  Bronze Layer: Approach 3 (Add Fuzzy Column Matching)
Rejecting files creates unacceptable delays, so we implement fuzzy matching to handle common typos:Our bronze ingestion grows:This works. But we're now making quality decisions about what constitutes an acceptable typo. We're interpreting intent. The bronze layer is no longer just landing raw data.
  
  
  Problem 4: Excel Nightmares
Vendor B sends a file that completely breaks our parser. Opening it in a text editor reveals the structure:Contact:,lab@testing.com,"","","","","","","","","","","","","","","","",""
Generated:,2024-10-15,"","","","","","","","","","","","","","","","",""
Lab Name:,Premium Soil Testing,"","","","","","","","","","","","","","","","",""
Sampl_Barcode,lab_id,DATE_RECEIVED,Date_Proccessed,acidity,cu_totl,ZN_TOTL,pb_total,fe_total,Mn_Totl,b_total,mo_totl,ec_ms_cm,Organic_Carbon_Pct,"","","","",""
PYB2475-266277,AT6480 68463,2024-05-12,2024-05-19,6.46,6.63,29.5,4.22,103.,3.56,0.759,0.186,1.44,0.30,"","","","",""
Three metadata rows precede the actual header. Additionally, the file has empty column name padding (those trailing empty strings). The file exhibits the telltale signs of an Excel export where someone navigated beyond the data range and accidentally pressed enter before saving.The actual data is fine. The measurements are valid. We just need to skip the metadata rows and ignore the empty columns.
  
  
  Bronze Layer: Approach 4 (Add Header Detection and Column Filtering)
We implement header detection to skip metadata:We filter out empty columns:And implement re-reading from the correct header position:The bronze ingestion continues to grow:The bronze layer now includes heuristics for detecting valid data. We're making educated guesses about file structure.
  
  
  Problem 5: Database-Hostile Column Names
Vendor B's files sometimes include special characters in column names:#sample_id,lab_id,organic_matter%,cu-total,zn-total
The  prefix,  suffix, and hyphens require backtick escaping in SQL queries:Every analyst who touches this data must remember the escaping rules. Queries become brittle and harder to read.
  
  
  Bronze Layer: Approach 5 (Add Character Sanitization)
We sanitize column names to be database-friendly:The complete bronze ingestion:Eight transformation steps. Vendor-specific logic branches. Fuzzy matching heuristics. Schema knowledge. Quality decisions.This was supposed to be "just land the raw data."
  
  
  The Uncomfortable Questions
We started with a simple bronze layer that read CSV files and wrote them to a table. We now have a complex ingestion pipeline that: Vendor-specific column mapping encodes knowledge about what measurements mean across different schemas Fuzzy matching determines which typos are acceptable and how to fix them Header detection guesses where real data begins Character sanitization changes the raw column names we receivedIs this still a "bronze layer"? The Medallion Architecture describes bronze as raw data with minimal transformation. We're well beyond minimal.What happens when Vendor C arrives? We add more column mappings to the function, another branch in the if/elif chain, and hope their quirks don't conflict with the assumptions we've baked into our existing logic. And how do we decide what the "default" name for the column should be?How do we test this? We need sample files for every vendor, every analysis package, every combination of issues. The test matrix grows exponentially.We haven't addressed date format differences, unit conversions, vendor-specific codes, or the dozens of other variations we'll encounter as more vendors join the system.The bronze layer has gotten away from us.
  
  
  How Would You Manage This Complexity?
Before we explore solutions in the next post, consider how you would handle this problem in your own systems:Would you continue adding transformation logic to the bronze layer until it handles every edge case?Would you reject files that don't conform to expected formats and force vendors to fix their exports?Would you build a configuration system where new vendor quirks can be added without code changes?Would you accept some data quality issues and handle them downstream in the silver layer?Each approach has tradeoffs. Adding more transformations makes the bronze layer complex and fragile. Rejecting files delays processing and frustrates vendors and users of the data alike. Configuration systems add their own complexity. Pushing problems downstream just moves the pain to a different layer.What if the fundamental problem is that we're treating column names as schema when they should be treated as data?In the next post, we'll explore this alternative. Instead of fighting schema chaos with increasingly complex transformations, we'll embrace it. We'll examine how a single transformation applied to all vendors can replace vendor-specific logic, superset schemas, fuzzy matching, and header detection with something simpler and more robust.The solution involves questioning what "raw" actually means. Part 3 - The Zen of the Bronze Layer: Embracing Schema Chaos]]></content:encoded></item><item><title>Python String Manipulation: Every Way to Delete Specific Characters</title><link>https://dev.to/lifeportal20002010/python-string-manipulation-every-way-to-delete-specific-characters-1913</link><author>ãƒ©ã‚¤ãƒ•ãƒãƒ¼ã‚¿ãƒ«</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 04:12:07 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When processing text data in Python, you frequently run into these scenarios:"I want to remove extra whitespace.""I need to bulk-delete specific symbols.""I want to remove elements from a list that contain certain words."While it all falls under "deletion," the method you should use depends on whether you are modifying a  or a . Python has also introduced more intuitive methods in recent versions that make these tasks much simpler.In this guide, weâ€™ll dive into every major technique for deleting characters in Python, covering both string and list operations with practical code examples and modern best practices.
  
  
  Basic Techniques for String Deletion
In Python, strings are , meaning you cannot modify the original string directly. Instead, these methods return a  string with the characters removed.
  
  
  1. : Delete All Occurrences
To remove a specific character or substring everywhere it appears, use the  method by replacing it with an empty string .You can also limit the number of deletions by providing a third argument:  will only remove the first two hyphens.
  
  
  2. : Remove Leading and Trailing Characters
To clean up unwanted characters (like spaces or newlines) only at the  or  of a string, use the  family of methods:: Removes from both ends.: Removes from the left (start) only.: Removes from the right (end) only.
By default, these remove whitespace, but you can specify a set of characters like .
  
  
  3.  and  (Python 3.9+)
Added in Python 3.9, these are the best practices for removing a specific word only if it appears at the very beginning or end of a string.Unlike , which treats the input as a set of individual characters to prune,  treats it as a single exact string, making it much safer for filenames and IDs.
  
  
  Advanced Pattern Deletion

  
  
  1. : Delete Multiple Different Characters at Once
If you need to delete a whole set of symbols (e.g., vowels or punctuation), chaining  multiple times is inefficient. Using  with  is much faster.
  
  
  2. : Regex-Based Deletion
For pattern-based deletionâ€”like "remove all numbers" or "remove everything except letters"â€”the  (regular expression) module is your best friend.
  
  
  Deleting Specific Elements from a List
When dealing with a  of strings, the logic changes from "modifying a string" to "filtering a collection.": Deletes the first occurrence of a specific .: Deletes the element at a specific  and returns it.

  
  
  2. List Comprehension: Bulk Filtering
The most Pythonic way to remove all elements that meet a certain condition is to use a . Instead of "deleting," you "keep" the ones you want.The right way to "delete" in Python depends on the scope. For exact matches in strings,  or  are your best bets. For complex patterns, use . When cleaning up lists,  is almost always the most efficient and readable choice.]]></content:encoded></item><item><title>ðŸ’¡ Discovery: docs(ralph): Auto-publish discovery blog post</title><link>https://dev.to/igorganapolsky/discovery-docsralph-auto-publish-discovery-blog-post-2e0k</link><author>Igor Ganapolsky</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 04:03:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Want to add autonomous AI coding to your project?
pip anthropic


python scripts/ralph_loop.py  fix_tests  5  2.00
]]></content:encoded></item><item><title>Web Scraping for Data Analysis: Legal and Ethical Approaches</title><link>https://dev.to/adnan_arif_14ae4bc014267f/web-scraping-for-data-analysis-legal-and-ethical-approaches-36n4</link><author>Adnan Arif</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 04:00:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The internet contains more data than any single database could hold. Product prices across thousands of stores.Real estate listings in every market. Job postings across industries. Public records from government agencies.For data analysts, this represents opportunity. Web scrapingâ€”extracting data programmatically from websitesâ€”opens doors that APIs and official datasets keep closed.But scraping walks a fine line. What's technically possible isn't always legal. What's legal isn't always ethical. Understanding these boundaries is essential before you write your first line of scraping code.
  
  
  Why Scrape When APIs Exist
A fair question. Why scrape when many platforms offer APIs? APIs provide what companies want to share. Scraping accesses what's publicly visibleâ€”often far more comprehensive. APIs frequently charge for access, especially at scale. Scraping public pages typically costs only computing resources. API terms change. Rate limits tighten. Access gets revoked. Scraped data from public pages can't be retroactively restricted in the same way. APIs return structured responses. Scraped data reflects what users actually see, including formatting, promotions, and dynamic content.That said, APIs are easier, more reliable, and less legally ambiguous when they meet your needs.Web scraping legality isn't black and white. It depends on what you're scraping, how, and why.Computer Fraud and Abuse Act (CFAA). This US law prohibits "unauthorized access" to computer systems. The hiQ Labs v. LinkedIn case (2022) clarified that scraping publicly accessible data generally doesn't violate the CFAA. Most websites prohibit scraping in their terms. Violating terms isn't automatically illegal, but it can create civil liability. Scraped content may be copyrighted. Extracting facts is generally permissible; copying creative expression is not. GDPR, CCPA, and similar laws regulate personal data collection. Scraping personal information creates compliance obligations. This file indicates which parts of a site bots should avoid. It's not legally binding but ignoring it weakens legal defenses.This isn't legal advice. Consult an attorney for specific situations.Legal doesn't mean ethical. Even permitted scraping can be problematic. Aggressive scraping can overload servers, affecting real users. You're using someone else's infrastructure. Scraping a competitor's pricing to systematically undercut them raises ethical questions, even if technically legal. Just because someone posted information publicly doesn't mean they consented to bulk collection.Business model disruption. Some websites rely on advertising revenue from visitors. Scraping without visiting the page circumvents their revenue model.The ethical test: would the website operator consider your actions reasonable? If not, proceed with caution.The robots.txt file lives at a site's root (e.g., example.com/robots.txt) and specifies scraping rules.User-agent: *
Disallow: /private/
Crawl-delay: 10

User-agent: BadBot
Disallow: /
This file asks all bots to avoid /private/, wait 10 seconds between requests, and blocks "BadBot" entirely.Respecting robots.txt is industry standard. Ignoring it signals bad faith and weakens legal defenses if disputes arise.
  
  
  Rate Limiting and Politeness
Hammering a server with requests is both rude and counterproductive. Servers detect aggressive bots and block them. Space requests seconds apart. Mimic human browsing patterns. If robots.txt specifies a delay, honor it. Don't parallelize requests to the same server aggressively.Scrape during off-peak hours. Early morning or late night typically has lighter server load.Python dominates web scraping. Here's your toolkit. For fetching page content. Simple, reliable, efficient. For parsing HTML and extracting data. Intuitive and forgiving of malformed HTML. For JavaScript-rendered content. Runs a real browser. Slower but handles dynamic content. Full framework for large-scale scraping. Handles concurrency, pipelines, and output formats. Modern alternative to Selenium. Faster, more reliable for dynamic content.Most scraping effort goes into parsing. HTML is messy, inconsistent, and designed for browsers, not data extraction. Look for consistent structuresâ€”classes, IDs, data attributesâ€”that identify the data you need. Often cleaner than navigating the DOM manually. Pages vary. Code defensively. Browser developer tools show the actual HTML structure. Use them constantly.Modern websites load content with JavaScript. A simple HTTP request gets you an empty shell. Often, dynamic content comes from API calls you can access directlyâ€”cleaner than scraping.Use Selenium or Playwright. These run real browsers and execute JavaScript. Run browsers without visible UI for automation.
  
  
  Handling Anti-Scraping Measures
Websites actively resist scraping. Common measures and countermeasures: Websites block requests with obvious bot user-agents. After too many requests, your IP gets blocked. Rotating proxies can helpâ€”but this enters ethically gray territory. Designed to distinguish humans from bots. CAPTCHA solving services exist but are expensive and ethically questionable. Hidden links that only bots follow. Following them flags you as a scraper.Aggressive anti-circumvention measures may cross ethical and legal lines. Consider whether the site is clearly saying "no."
  
  
  Data Storage and Processing
Scraped data needs somewhere to go. Easy to produce, universally readable. Preserves nested data better than CSV. SQLite for local work, PostgreSQL for larger projects. Stripping whitespace, normalizing formats, and validating data during scraping saves pain later.Production scrapers need error handling and recovery. Networks fail. Set reasonable timeouts and retry. When something breaks at 3 AM, logs are essential. Keep the original pages. Re-parsing is easier than re-scraping. For large jobs, save progress incrementally. Crashes shouldn't mean starting over.Let's scrape a book catalog (using a site designed for practice).Simple, effective, and polite.
  
  
  Frequently Asked Questions

Generally yes for publicly accessible data, but it depends on jurisdiction, terms of service, data type, and purpose. When in doubt, consult a lawyer.Can I scrape any website?
Technically yes, but not all scraping is legal or ethical. Check terms of service, robots.txt, and consider whether you're causing harm.How do I avoid getting blocked?
Use delays between requests, rotate user-agents, respect robots.txt, and don't scrape faster than a human could browse.Should I use an API instead of scraping?
If an API meets your needs, yes. APIs are more reliable, explicitly permitted, and easier to work with.What about scraping social media?
Social media platforms have strict terms and aggressive anti-scraping measures. Scraping them carries higher legal risk.Is it okay to scrape personal information?
Be very careful. Data protection laws like GDPR apply. Even public personal data may require consent for collection.What tools should I start with?
Requests and BeautifulSoup for static pages. Add Selenium when you need JavaScript rendering.How do I handle pagination?
Identify the URL pattern for pages and loop through them. Or find and follow "Next" links programmatically.
Possibly, but this amplifies legal concerns. Commercialization changes risk calculations.What if the site changes its structure?
Your scraper breaks. This is normal. Monitor for failures and update selectors when layouts change.Web scraping is a powerful tool for data analysts. It opens access to data that would otherwise be inaccessible or prohibitively expensive.But power comes with responsibility. Scrape legally. Scrape ethically. Respect the websites and people behind them.When done right, scraping extends your analytical capabilities far beyond the limits of official data sources.This article was refined with the help of AI tools to improve clarity and readability.]]></content:encoded></item><item><title>ðŸŽ¬ è¡Œç‚ºè¾¨è­˜</title><link>https://dev.to/stklen/xing-wei-bian-shi-34dj</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 03:00:36 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[AI èƒ½è¾¨è­˜é€™æ˜¯ Jelly é‚„æ˜¯ Ariel å¾Œï¼Œä¸‹ä¸€æ­¥æ˜¯ä»€éº¼ï¼Ÿï¼šè®“ AI ä¸åªèªå‡ºå‹•ç‰©ï¼Œé‚„èƒ½ç†è§£ç‰ å€‘åœ¨åšä»€éº¼ã€‚ï¼šç°¡å–®ã€å¿«é€Ÿï¼šç„¡æ³•åˆ¤æ–·é€£çºŒå‹•ä½œï¼ˆå¦‚ã€Œæ­£åœ¨èµ°ã€vsã€Œåœä¸‹ä¾†ã€ï¼‰è¼¸å…¥å½±ç‰‡ â†’ è¡Œç‚ºè¾¨è­˜ â†’ è‡ªå‹•åŠ æ¨™ç±¤
                  â†“
           ã€ŒJelly ç¡è¦ºã€ã€ŒDollar é€²é£Ÿã€
]]></content:encoded></item><item><title>Understanding Mixins in Python, Django, and Django REST Framework</title><link>https://dev.to/ajitkumar/understanding-mixins-in-python-django-and-django-rest-framework-2j40</link><author>Ajit Kumar</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 02:55:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you have been working with Python or Django for a while, youâ€™ve likely heard the acronym DRY: Don't Repeat Yourself.But as your project grows, you might find yourself copying and pasting the same created_at field across ten models, or the same permission_classes logic across five views. This is where Mixins come to the rescue.: Beginners in Python and Django aiming to write clean, reusable, and production-ready code.: By the end of this guide, you will  understand what mixins are, why they exist, and how to use them correctly in Python, Django, and Django REST Framework (DRF).What Is a Mixin? (Conceptual View)Mixins vs Inheritance vs CompositionMixins in Django REST Framework (DRF)As applications grow,  starts appearing:Violates DRY (Donâ€™t Repeat Yourself)Makes changes risky and error-proneLeads to bloated base classesðŸ’¡ Mixins allow you to reuse behavior without forcing rigid inheritance trees.
  
  
  ðŸ§  What Is a Mixin? (Conceptual View)
Encapsulates Is not meant to be instantiated directlyIs combined with other classes using âŒ : â€œThis class  Userâ€âœ… : â€œThis class  /  / â€
  
  
  âš–ï¸ Mixins vs Inheritance vs Composition
âœ… : Mixins add , not .Letâ€™s start with a minimal Python example.âœ” No 
âœ” No assumptions about where it will be used[LOG] Creating user: alice
ðŸ§© The mixin cleanly injects logging behavior without complicating the class.
  
  
  âœ… Python Mixin Best Practices
Keep mixins Avoid state () when possibleAvoid  unless you fully understand MRODjango heavily uses mixins in .Instead of one massive view class, Django splits behavior into:Each responsibility lives in a mixin.
  
  
  ðŸ› ï¸ User-Defined Django Mixin
Letâ€™s create a custom access-control mixin.ðŸ”‘ : mixins must come  the base view.Django provides many reusable mixins:
  
  
  ðŸš€ Mixins in Django REST Framework (DRF)
DRF is .REST APIs often need combinations of:DRF provides one mixin per operation.
  
  
  ðŸ“¦ Built-in DRF CRUD Mixins
âœ” 
âœ” ðŸ§  You compose behavior instead of inheriting everything.
  
  
  âœï¸ Custom DRF Mixin Example
Document assumptions clearlyUnit test mixins independentlyAvoid deep inheritance chainsUse this to debug resolution order:Got it. Iâ€™ll do two things clearly and separately:Provide a self-contained â€œReal-World Production Exampleâ€ section
â†’ You can copy-paste this  of the blog post.Explain the best way to create a GitHub Gist for this example
â†’ Practical, dev.toâ€“friendly, and professional.
  
  
  ðŸ§ª Real-World Production Example: Audit Mixins in Django & DRF
This section demonstrates a real, production-grade use case for mixins that is commonly required in professional Django projects.In most real-world applications, we need to:Track  a record was created or updatedTrack  created or updated the recordApply this logic consistently across many models and APIsAvoid copy-pasting the same logic everywhereDoing this manually in every model or API view quickly becomes error-prone and difficult to maintain.We solve this using mixins at multiple layers: â†’ reusable across Django apps â†’ reusable across API viewsetsWidely used in production systems
  
  
  1ï¸âƒ£ Model-Level Mixins (Reusable & Abstract)

  
  
  Why this is production-ready
 â†’ no extra database tablesFully reusable across multiple appsKeeps audit logic centralizedNo business logic inside models
  
  
  2ï¸âƒ£ Using the Model Mixins
Now every  automatically has:No duplication. No extra code.
  
  
  3ï¸âƒ£ DRF Mixin to Auto-Populate Audit Fields
This mixin hooks into DRFâ€™s lifecycle methods and keeps audit logic out of serializers and views.
  
  
  4ï¸âƒ£ Using the DRF Mixin in a ViewSet
DRF CRUD behavior â†’ provided by DRF mixinsAudit behavior â†’ injected via No duplicated logic across endpoints
  
  
  ðŸ§  Code Walkthrough Summary
 handle persistence concerns (timestamps, user tracking) handles request-specific behaviorViewSets simply , instead of implementing itThis pattern scales extremely well in large teams and long-lived codebases.
  
  
  ðŸ“Œ Why This Is a Strong Real-World Example
Reflects real enterprise Django patternsDemonstrates mixins at multiple architectural layersEasy to extend (soft delete, logging, permissions)Safe and common in production systemsâŒ Treating mixins as base classes
âŒ Putting business logic in mixins
âŒ Overusing mixins instead of composition
âŒ Ignoring Method Resolution Order (MRO)Mixins enable They add , not Django and DRF are built on mixin philosophyMastering mixins is essential for production Django projects]]></content:encoded></item><item><title>ðŸ§© Building NumMaze: A Python GUI Arithmetic Puzzle Generator (Step-by-Step)</title><link>https://dev.to/matetechnologie/building-nummaze-a-python-gui-arithmetic-puzzle-generator-step-by-step-4c68</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 02:46:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build NumMaze â€” a desktop Python app that generates arithmetic puzzles with automatic solutions and exports them as PDFs or JPG images.Itâ€™s designed to be beginner-friendly and useful for:Learning Python GUI basicsPracticing recursion and logicCreating printable math worksheetsttkbootstrap â€“ modern themesPillow â€“ JPG image exportBy the end, youâ€™ll have a complete puzzle generator.âœ… Step 1 â€” Install DependenciesFirst, install the required libraries:pip install ttkbootstrap reportlab pillowtkinter comes bundled with most Python installs.âœ… Step 2 â€” Import ModulesCreate a file called nummaze.py and start with imports:import tkinter as tk
from tkinter import messagebox, filedialog
import random
import operator
import ttkbootstrap as tb
from ttkbootstrap.constants import *

from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4

from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
random + operator â†’ puzzle logicttkbootstrap â†’ modern stylingreportlab â†’ PDF generationâœ… Step 3 â€” Create the App ClassWe wrap everything inside a class:class NumMaze:
    APP_NAME = "NumMaze"
    APP_VERSION = "2.2.0"
    OPERATORS = {
        "+": operator.add,
        "-": operator.sub,
        "*": operator.mul,
        "/": operator.floordiv
    }

This lets us dynamically apply math later.âœ… Step 4 â€” Initialize the Window    def __init__(self):
        self.root = tk.Tk()
        tb.Style(theme="darkly")

        self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
        self.root.geometry("1250x700")
We also define app state:        self.difficulty_var = tk.StringVar(value="Easy")
        self.num_puzzles_var = tk.IntVar(value=1)

        self.grid_numbers = []
        self.target_number = None
        self.solution_steps = []

        self.rows = self.cols = 0
âœ… Step 5 â€” Build the User Interface    def _build_ui(self):
        tb.Label(self.root, text=self.APP_NAME,
                 font=("Segoe UI", 22, "bold")).pack(pady=10)
        tb.Label(
            self.root,
            text="Auto-Generated Arithmetic Puzzle",
            font=("Segoe UI", 10, "italic")
        ).pack()
        opts = tb.Labelframe(self.root, text="Options", padding=10)
        opts.pack(fill="x", padx=10)

        tb.Label(opts, text="Difficulty:").pack(side="left")
        tb.Combobox(
            opts,
            values=["Easy","Medium","Hard"],
            textvariable=self.difficulty_var,
            width=10
        ).pack(side="left", padx=5)
        tb.Label(opts, text="Number of Puzzles:").pack(side="left", padx=10)
        tb.Spinbox(opts, from_=1, to=20,
                   textvariable=self.num_puzzles_var,
                   width=5).pack(side="left")
âœ… Step 6 â€” Control Buttons        ctrl = tb.Frame(self.root)
        ctrl.pack(fill="x", padx=10, pady=10)

        tb.Button(ctrl, text="Generate Puzzle",
                  bootstyle="success",
                  command=self.generate_single_puzzle).pack(side="left")
You can add export buttons the same way:        tb.Button(ctrl, text="Combined PDF",
                  bootstyle="warning",
                  command=self.generate_multiple_combined_pdf).pack(side="left", padx=5)
Each button simply calls a method.âœ… Step 7 â€” Grid + Solution Panels        self.grid_frame = tb.Labelframe(self.root, text="Puzzle Grid", padding=10)
        self.grid_frame.pack(fill="x", padx=10)
        sol = tb.Labelframe(self.root, text="Solution", padding=10)
        sol.pack(fill="both", expand=True, padx=10)

        self.solution_text = tk.Text(sol, height=10, font=("Consolas", 12))
        self.solution_text.pack(fill="both", expand=True)
âœ… Step 8 â€” Generate Puzzle DataDifficulty controls how many numbers:    def create_puzzle_data(self):
        d = self.difficulty_var.get()

        if d == "Easy":
            n, rows, cols = 4, 2, 2
        elif d == "Medium":
            n, rows, cols = 6, 2, 3
        else:
            n, rows, cols = 9, 3, 3
        numbers = [random.randint(1,15) for _ in range(n)]
        expr, target, steps = self.recursive_solution(numbers)
        return numbers, target, steps, rows, cols
âœ… Step 9 â€” Recursive SolverThis repeatedly tries random combinations:    def recursive_solution(self, numbers):
        for _ in range(5000):
            nums = numbers[:]
            random.shuffle(nums)
            expr, val, steps = self.build_expr(nums)
            if val is not None and val > 0:
                return expr, val, steps

        return str(numbers[0]), numbers[0], []
    def build_expr(self, nums):
        if len(nums) == 1:
            return str(nums[0]), nums[0], []
        for i in range(1, len(nums)):
            left = nums[:i]
            right = nums[i:]
            for op in self.OPERATORS:
                try:
                    val = self.OPERATORS[op](left_val, right_val)
                    steps = left_steps + right_steps + [
                        f"{left_val} {op} {right_val} = {val}"
                    ]
                    return expr, val, steps
                except:
                    continue
This is the core logic engine.    def display_grid(self):
        for w in self.grid_frame.winfo_children():
            w.destroy()
Create labels for each number:        for r in range(self.rows):
            for c in range(self.cols):
                tb.Label(self.grid_frame,
                         text=str(self.grid_numbers[idx]),
                         font=("Segoe UI",20,"bold")).grid(row=r,column=c)
        tb.Label(self.grid_frame,
                 text=f"Target: {self.target_number}",
                 font=("Segoe UI",16,"bold")).grid(row=self.rows,columnspan=self.cols)
âœ… Step 11 â€” Show Solution    def show_solution(self):
        self.solution_text.delete("1.0", tk.END)
        self.solution_text.insert(tk.END, "\n".join(self.solution_steps))

âœ… Step 12 â€” Export PDFs and JPGs(See full repo for complete export implementations.)if __name__ == "__main__":
    NumMaze().run()
Recursive arithmetic solver]]></content:encoded></item><item><title>Comparing Validatar to CsvPath Validation</title><link>https://dev.to/david_kershaw_b6916404da6/comparing-validatar-to-csvpath-validation-1m4o</link><author>David Kershaw</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 02:33:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As with the other comparisons, please remember that data quality tools like SodaCL, Great Expectations, or today's contestant, Validatar, only do data quality. CsvPath Framework, by contrast, is a data-file feeds management infrastructure that covers data validation as just one aspect of the full data preboarding lifecycle. Moreover, CsvPath Framework does not deal with relational databases (other than as an option for storing its own metadata). Validatar et. al., are first and foremost relational database quality management tools, and only secondarily deal with data files. So it's a mismatch, to some degree, but useful and entertaining nonetheless.Here's the problem description at the top of the Validatar example: This Standard Test is designed to demonstrate the concept of how to create a uniqueness template test for all CSV files in multiple folders.
The Standard Test here compares the Row Count per  value in the  to make sure all 's only have 1 row. The test only keeps failures and stops after 100 failure records.Spoiler alert: in FlightPath this is a trivial example (as I  it is  to be)Validatar starts by having you create a test template. Before you can do that, though, you need a project. Here are the instructions for that step: Make sure your Data Source is Mapped correctlyThe first bullet sounds simple. I'm not sure what the second bullet means because I'm not a Validatar expert and that one isn't explained on that page.Moving on, let's create that template. Most of this exercise is forms based. The setup is shown in the image below. Their ask is that you notice:Note that the column specified to group by is account_idNote that it is comparing the ROW_COUNT to a fixed value of 1Note that the Result Configuration is set so that only Failures are kept and to abort after 100 failures are foundGood requirements for us to use on the CsvPath side. At this point we have our test. Now we need to create a template from it so that we can apply it to each CSV file. This is how we get to a single action we can apply to multiple files in a uniform way. I'm going to just add the bullets because the screenshot is in the link above, which is of course a more complete description. We do: Update the Folder input to Update the File input to Update the Column input to {{#replace table.name "_data.csv" "_id"}}Update the Metadata Links to {{schema.name}}.{{table..name}}Change the Generate column list using to Dynamic Template ConfigurationUpdate the Dynamic ScriptThe dynamic script is pretty simple:    [
    {"name":"{{#replace table.name "_data.csv" "_id"}}","sequence":1,"type":"Numeric","role":"Key"},
        {"name":"ROW_COUNT","sequence":2,"type":"Numeric","role":"Value"}
    ]
Now, we're going to use some metadata to filter down to the files we care about. Switch to the Metadata Selection TabChange to the  optionAdd a Filter on the Table Name Field that contains "_data.csv"At this point, check that the filter finds your files and run the example. You should be good to go. My feeling is that all works better for database tables than for CSV files, just as you would expect from Validatar.Once more, this time with feeling! Let's see how CsvPath Framework and FlightPath Data can make the same magic happen. And, hopefully you'll agree that it's much simpler and more powerful for its use case.Create a uniqueness test for all csv files in multiple foldersThe column specified to group by is account_idThe core of these requirements is the validation statement. Using CsvPath Validation Language this is next to trivial:$[*][ 
    @duplicate_accounts.nocontrib == 100 -> stop()
    has_dups(#account_id) -> counter.duplicate_accounts(1) 
]
(The  sign means a variable and the  sign indicates a header name)This csvpath says: for each line in a file check if the counter is . If it is, stop processing that file. Otherwise, increase the counter if the  is a duplicate.The statement will collect only error lines because:The counter is a side-effect with no contribution to matchingThe check if  equals  is marked to not contribute to matching. (Using the  qualifier)The function that does the heavy lifting is . If that returns  (i.e. the value of ) we match the line and capture it.All pretty readable. Now what do we do with it?All of what we need to do is almost as simple in Python using only CsvPath Framework. Almost! But using FlightPath Data it is even simpler.In FlightPath, create a new file called . Paste in our statement. Right-click on  and select . In the load dialog give the named-paths group the name  and click . You should see your csvpath show up in the middle window on the right under the  folder. When you load a csvpath statement it always goes into a  file. And when you click on that file its background is pale green to let you know you cannot edit it. (You can, of course, over-write it anytime without losing prior versions, but that is another topic for a different post.)Next stage your data. In the example, each file is in its own folder and its folder is one of many in the same directory. We'll just add the parent folder and let FlightPath find the files for us. To do that right click the parent directory and select . In the stage data dialog uncheck the  checkbox because we're going to have every physical file be one version of the same named-file. Think of a named-file as a category that has one file assigned to it at a time, in sequence. We say named-files have versions.In the named-file name box type . That's our category. You will see your data in the top-right window as a directory named . I used a template of :6/:filename in order to keep the month folders, but that is completely optional.Finally, right click the  folder, or the  folder below it, and select . In the run dialog, for named-paths select . For named-file type in . That named-file name is a reference that indicates every version of the  named-file. Again remember, the named-file is like a category that registers a file at a time. We registered a bunch of files and now we're applying our CsvPath Validation Language statement to each of them in turn.And here's the Run dialog: When you click  you will see your results in the lower right-hand window. Your run is date stamped within the  results. In your date-stamped run you can see the  where your duplicate lines landed. In this image I dropped each run into its own folder using a template; you can see the  and . That is completely optional, of course.There is, of course, much more you can do with CsvPath Framework. Likewise, Validatar has a ton more functionality than what we showed. But now you've had a taste of both. What I'd hope you come away with is that CsvPath Framework is the better tool for CSV, JSONL, and Excel file validation. The ease of using FlightPath Data for this validation example makes the case well. Obviously, for relational database validation, Validatar is your horse.And of course I also want to point out again that CsvPath Framework is a complete data preboarding solution, not just a validation engine. Preboarding inbound data files is a big deal. If you need that (and who doesn't?) you owe it to yourself to take a look at CsvPath Framework.]]></content:encoded></item><item><title>Efficiently Transforming CSV Data to Styled Excel Worksheets with Python</title><link>https://dev.to/allen_yang_f905170c5a197b/efficiently-transforming-csv-data-to-styled-excel-worksheets-with-python-38e5</link><author>Allen Yang</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 01:59:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Raw data, often residing in CSV files, is the lifeblood of many organizations. However, presenting this data directly can be challenging. CSVs are excellent for data storage and interchange due to their simplicity, but they lack the visual structure and formatting capabilities required for effective analysis and professional reporting. This is where Excel steps in. With its robust formatting options, charting tools, and user-friendly interface, Excel transforms raw data into understandable and actionable insights.The manual process of converting CSVs to Excel and then meticulously applying styles can be time-consuming and prone to errors, especially with large datasets or recurring tasks. This article will guide you through automating this process using Python, enabling you to convert your CSV data into professional, styled Excel spreadsheets efficiently. We'll explore how to not only transfer data but also to enhance its readability and impact through various formatting techniques, leveraging a powerful Python library designed for comprehensive Excel manipulation.
  
  
  Preparing Your Python Environment for Excel Automation
To effectively interact with and style Excel files in Python, a dedicated library is essential. While several options exist, some offer more extensive control over Excel's features, particularly when it comes to intricate styling and advanced functionalities. For this tutorial, we will be using a robust library that provides deep integration with Excel's object model, allowing for precise control over formatting and data presentation.To get started, you'll need to install the library. Open your terminal or command prompt and execute the following command:This command will download and install Spire.XLS for Python, an API designed to create, read, edit, and convert Excel files. Its comprehensive feature set, including support for a wide range of Excel versions and extensive styling capabilities, makes it an excellent choice for automating complex Excel tasks, from basic data transfer to advanced report generation.
  
  
  Fundamental CSV to Excel Conversion
Before diving into styling, let's establish the basic process of converting a CSV file into an Excel workbook. This fundamental step involves reading the CSV content and then saving it as an  file.Hereâ€™s a simple Python script to perform this basic conversion:Below is a preview of the output Excel file:In this code,  creates an empty Excel file.  then reads your CSV data directly into the first sheet. Finally, sheet.AllocatedRange.AutoFitColumns() adjusts the column widths to fit their content, and  saves the result. This script provides a functional Excel file, but it lacks any visual enhancements.
  
  
  Implementing Essential Excel Styling for Clarity
While the basic conversion is functional, raw data in Excel can still be hard to read. Implementing essential styling, such as formatting headers, adjusting column widths, and applying proper number and date formats, dramatically improves clarity and professionalism.
  
  
  Formatting Headers and Auto-fitting Columns
Headers are crucial for understanding data. Making them stand out with bold formatting and ensuring columns are wide enough to display all content are fundamental steps in creating a readable spreadsheet.Below is a preview of the output Excel file with styled headers:In this example, we create a new style (), set its font to bold, assign a light gray background, and center the text. This style is then applied to the first row, which typically contains the headers. sheet.AllocatedRange.AutoFitColumns() is called again to ensure all content, including potential longer header texts, is fully visible.
  
  
  Applying Number and Date Formatting
Raw data often treats numbers and dates as plain text, leading to formatting inconsistencies or incorrect calculations in Excel. Explicitly setting number and date formats ensures data integrity and improves readability.Below is a preview of the output Excel file with formatted numbers and dates:Here, we select specific column ranges ( for Amount,  for Date) and apply custom number formats using . This ensures that monetary values are displayed with currency symbols and two decimal places, and dates are presented in a consistent  format. These small details significantly enhance the professional appearance and utility of your data.
  
  
  Advanced Styling Techniques for Professional Data Presentation
Beyond basic formatting, advanced styling techniques like borders, background colors, and conditional formatting can further elevate your Excel reports, making them more visually appealing and easier to interpret.
  
  
  Adding Borders and Background Colors
Visual separation and grouping of data can be achieved effectively using borders and alternating row colors. This helps guide the reader's eye and distinguishes different data elements.Below is a preview of the output Excel file with advanced styling applied:In this script, data_range.BorderAround() adds a thin black border around the entire dataset, while data_range.BorderInside() adds lighter gray borders between cells. For alternating rows, a loop iterates through the data rows, applying a  background color to even-numbered rows, creating a visually distinct pattern that improves navigability.
  
  
  Implementing Simple Conditional Formatting
Conditional formatting highlights data based on specific rules, drawing immediate attention to critical information.Below is a preview of the output Excel file with conditional formatting applied:In this final enhancement, we target the  column and add a conditional formatting rule. cf.FormatType = ConditionValueType.Number specifies that the rule is based on the cell's value. cf.Operator = ComparisonOperatorType.Greater sets the condition, and  defines the threshold. Cells meeting this condition will have their background color changed to , instantly drawing attention to higher amounts.
  
  
  Streamlining Data Presentation Workflows
This tutorial has guided you through a practical journey of transforming raw CSV data into professionally styled Excel spreadsheets using Python. We started with a fundamental CSV-to-Excel conversion, then progressively enhanced the output with crucial styling elements: bold headers, auto-fitted columns, precise number and date formatting, and advanced visual cues like borders, alternating row colors, and conditional formatting.The power of Python, especially when combined with a comprehensive library like Spire.XLS for Python, lies in its ability to automate these intricate tasks. By scripting these processes, you eliminate manual effort, reduce the risk of human error, and ensure consistent, high-quality data presentation across all your reports. This automation not only saves valuable time but also elevates the professionalism and clarity of your data analysis and reporting workflows.]]></content:encoded></item><item><title>ðŸŒ€ Beginner-Friendly Guide &apos;Minimum Cost Path with Teleportations&apos; - LeetCode 3651 (C++, Python, JavaScript)</title><link>https://dev.to/om_shree_0709/beginner-friendly-guide-minimum-cost-path-with-teleportations-leetcode-3651-c-python-pk8</link><author>Om Shree</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 01:08:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Navigating a grid is a classic coding challenge, but adding teleportation changes the game entirely. This problem asks us to find the most efficient route when we can either pay to move or jump for free under specific conditions. By mastering this, you will learn how to layer dynamic programming to handle multiple "states" of a problem.A 2D grid of size  where each cell contains a cost.An integer , representing the maximum number of times you can teleport.Two movement rules: standard moves (right or down) which cost the value of the destination cell, and teleportation (to any cell with a value less than or equal to your current cell) which costs zero.Calculate the minimum total cost to travel from the top-left cell  to the bottom-right cell .The core of this problem lies in balancing standard movement and the limited resource of  teleports. Without teleports, this is a standard pathfinding problem. The cost to reach a cell is the cell's value plus the minimum cost of reaching the cell above it or to its left. Teleporting is powerful because it costs . However, you can only teleport to a cell  if . This means if we have used  teleports to reach a cell with value , we can start a new path from any cell with value  with a cost of  for that jump. We solve the problem in "rounds" based on the number of teleports used. For each round from  to , we update our minimum costs. We maintain a suffix minimum array () that stores the cheapest way to reach any cell that has a value of at least . This allows us to quickly check if teleporting to a cell with value  is cheaper than walking to it.
  
  
  Walkthrough: Understanding the Examples
grid = [[1,3,3],[2,5,4],[4,3,5]], k = 2 We begin at . Initial cost is . Move to . Cost becomes . Move to . Cost becomes . The value at  is . We can teleport to  because its value is also  (and ). The teleportation cost is . Total cost remains . Since  is the destination, the answer is . Adding a variable like  (number of teleports) often means we need to repeat our logic  times or add a dimension to our DP table. Using an auxiliary array to track the minimum value across a range (like all values ) is a common trick to optimize search time from  to . We only ever need the results from the "previous teleport count" to calculate the "current teleport count," allowing us to save memory.This problem is a fantastic representation of how real-world logistics systems work. Think of a delivery drone. It can drive along streets (standard moves with cost), but it might also have the battery to fly (teleport) between high-altitude landing pads. Systems like Google Maps or airline routing use similar multi-state optimizations to find the cheapest or fastest paths.]]></content:encoded></item><item><title>Beyond Just a Photo: Building a Pixel-Perfect Calorie Estimator with SAM and GPT-4o</title><link>https://dev.to/beck_moulton/beyond-just-a-photo-building-a-pixel-perfect-calorie-estimator-with-sam-and-gpt-4o-1foj</link><author>Beck_Moulton</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 00:45:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We've all been there: staring at a delicious plate of pasta, trying to manually log every gram into a fitness app. Itâ€™s tedious, prone to "optimistic" human error, and frankly, ruins the meal. But what if we could turn those pixels directly into nutritional data? In this tutorial, we are building a Multimodal Dietary Analysis Engine. By combining the surgical precision of Metaâ€™s Segment Anything Model (SAM) with the reasoning power of , we can transform a simple smartphone photo into a detailed nutritional breakdown. We will leverage  and  to isolate food items and use reference-based scaling to estimate volume and calories with surprising accuracy.While building this prototype, I drew heavy inspiration from the production-grade AI patterns found on the WellAlly Blog, which is a goldmine for anyone building robust, AI-driven health tech solutions.To achieve high accuracy, we don't just "show" an image to an LLM. We process it. First, SAM identifies the exact boundaries of the food. Then, we feed the segmented mask and the original context to GPT-4o to perform the cross-referencing.graph TD
    A[User Uploads Image] --> B[OpenCV Preprocessing]
    B --> C[SAM: Segment Anything Model]
    C --> D{Mask Generation}
    D -->|Isolate Food| E[GPT-4o Multimodal Analysis]
    D -->|Reference Object| E
    E --> F[Nutritional Estimation Engine]
    F --> G[FastAPI Response: Calories, Macros, Confidence Score]
Before we dive into the code, ensure you have the following stack ready:: For running the SAM weights.: Meta's pre-trained vision model.: Our multimodal "brain.": To wrap everything into a production-ready microservice.: For image manipulation.
  
  
  Step-by-Step Implementation

  
  
  1. Isolating the Food with SAM
First, we need to distinguish the food from the plate. Traditional bounding boxes are too messy; we need pixel-level masks to estimate surface area effectively.
  
  
  2. Crafting the Multimodal Prompt for GPT-4o
GPT-4o is excellent at visual reasoning, but it needs context. We provide it with the original image and instructions to use common items (like a credit card or a fork) as a scale reference.Now, let's wrap this into an endpoint that our mobile app can consume.
  
  
  The "Official" Way: Advanced Patterns
While the code above works for a hobby project, production-grade health apps require robust error handling, Pydantic data validation, and real-time feedback loops. For example, how do you handle low-light conditions or overlapping food items?If you're looking for more production-ready examples and advanced architectural patterns regarding AI in health tech, I highly recommend checking out the . They cover deep-dives into LLM observability and multimodal data processing that were instrumental in refining this dietary engine.By combining 's spatial awareness with 's cognitive understanding, we've moved past simple "image labeling." We've built an engine that understands volume, context, and nutrition at a pixel level.  Try adding a "Reference Object Detection" step using YOLOv8 to help GPT-4o with scale. Implement a feedback loop where users can confirm the estimated portion size.What are you building with Multimodal AI? Drop a comment below or share your latest project! ]]></content:encoded></item><item><title>Your Tests Pass. But Would They Catch This Bug?</title><link>https://dev.to/mikelane/your-tests-pass-but-would-they-catch-this-bug-mhd</link><author>Mike Lane</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 28 Jan 2026 00:38:24 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You have 90% code coverage, green CI, and you ship. A user reports that  should have been . Your tests executed that line but never verified the boundary mattered.Code coverage counts executed lines. Mutation testing injects small bugs and checks whether your tests detect them. If tests still pass after changing  to , you found a gap.
  
  
  Why Mutation Testing Has Been Impractical
Traditional tools (mutmut, cosmic-ray) rewrite source files, reload modules, and run the full test suite per mutation. A codebase with 100 mutations and a 10-second test suite takes 17+ minutes. That runtime kills feedback loops.
  
  
  pytest-gremlins Architecture
: All mutations are embedded during a single instrumentation pass. Switching between mutations requires only an environment variable change, eliminating per-mutation file I/O and module reloads.Coverage-Guided Test Selection: The plugin tracks which tests cover each line. When testing a mutation on line 42, it runs only the 3 tests that touch line 42 instead of all 200 tests.: Results are keyed by content hash of source and test files. Unchanged code skips mutation testing entirely on subsequent runs.
  
  
  Benchmark: pytest-gremlins vs mutmut
Measured on Python 3.12 in Docker:pytest-gremlins (sequential)pytest-gremlins (parallel)pytest-gremlins (parallel + cache)Sequential mode is slower because pytest-gremlins runs additional mutation operators. Parallel mode, safe due to mutation switching (no shared mutable state), delivers the speedup. Cached runs approach instant for unchanged code.pip pytest-gremlins
pytest Output identifies specific gaps:================== pytest-gremlins mutation report ==================

Zapped: 142 gremlins (89%)
Survived: 18 gremlins (11%)

Top surviving gremlins:
  src/auth.py:42    >= â†’ >     (boundary not tested)
  src/utils.py:17   + â†’ -      (arithmetic not verified)
  src/api.py:88     True â†’ False (return value unchecked)
=====================================================================
Each survivor is a line number, the mutation applied, and the gap it reveals. Line 42 has a boundary condition no test verifies.Target specific files with --gremlin-targets=src/auth.py.Run this on your highest-coverage module:pip pytest-gremlins
pytest src/your_critical_module.py
Survivors show exactly where your tests verify execution but not correctness. Fix one, run again in under 2 seconds with caching.]]></content:encoded></item><item><title>Python Morsels: All iteration is the same in Python</title><link>https://www.pythonmorsels.com/all-iteration-is-the-same/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 28 Jan 2026 00:30:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Looping over dictionaries gives keysWhen you loop over a dictionary, you'll get the keys in that dictionary:Iterable unpacking with  also relies on iteration.
So if we use this to iterate over a dictionary, we again get the keys:The same thing happens if we use  to unpack a dictionary into a list:And even tuple unpacking relies on iteration.
Anything you can loop over can be unpacked.
Since we know there are three items in our dictionary, we could unpack it:And of course, as strange as it may seem, we get the keys in our dictionary when we unpack it:So what would happen if we turned our dictionary into a list by passing it to the list constructor?Well,  will loop over whatever iterable was given to it and make a new list out of it.
And when we loop over a dictionary, what do we get?And of course, if we ask whether something is  a dictionary, we are asking about the keys:Iterating over a dictionary object in Python will give you keys, no matter what Python feature you're using to do that iteration.
All forms of iteration do the same thing in Python.: of course if you want key-value pairs you  get them using the dictionary  method.Looping over strings provides charactersStrings are also iterables.]]></content:encoded></item><item><title>Time Series Analysis with Python: Forecasting Made Simple</title><link>https://dev.to/adnan_arif_14ae4bc014267f/time-series-analysis-with-python-forecasting-made-simple-5gk2</link><author>Adnan Arif</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 23:00:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every business runs on predictions. How many units will we sell next quarter? What will demand look like during the holiday season? When should we increase inventory?These questions require time series forecastingâ€”analyzing historical patterns to predict future values.The good news: Python makes time series analysis accessible. You don't need a PhD in statistics. You need the right approach and the right tools.
  
  
  What Makes Time Series Special
Time series data isn't like other data. The order matters. Yesterday's value influences today's. Last year's pattern might repeat this year.This temporal dependence violates assumptions that most statistical techniques rely on. You can't just throw time series data at a standard regression and expect good results.Understanding the unique properties of time series is essential before diving into techniques.Every time series can be decomposed into fundamental components. The long-term direction. Is the series generally increasing, decreasing, or stable? Sales might trend upward as a company grows. Regular, predictable patterns that repeat at fixed intervals. Retail sales spike in December. Ice cream sales peak in summer. Longer-term fluctuations that aren't as regular as seasonality. Economic cycles affect many time series. What's left after removing trend and seasonality. Sometimes called noise, though it may contain meaningful variation.Decomposition helps you understand what's driving your data before you try to forecast it.
  
  
  Setting Up Your Environment
Python's ecosystem for time series is mature and powerful. Here's what you need:Pandas handles time-indexed data naturally. Statsmodels provides classical time series methods. Scikit-learn offers evaluation metrics.For more advanced work, consider Prophet (from Meta), pmdarima (auto-ARIMA), and sktime (unified time series interface).
  
  
  Loading and Preparing Time Series Data
Time series data needs proper datetime indexing. Without it, Python treats your data as arbitrary rows.The frequency specification matters. Many time series methods assume regular intervals. Gaps or irregular timestamps cause problems.Before modeling, understand your data visually.Look for obvious patterns. Is there a trend?Seasonal spikes? Outliers? Structural breaks where behavior changed?This visual inspection guides your modeling choices.
  
  
  Stationarity: Why It Matters
Many forecasting methods require stationarityâ€”the statistical properties of the series don't change over time.A stationary series has constant mean, constant variance, and consistent autocorrelation structure. Most real-world time series aren't stationary.The Augmented Dickey-Fuller test helps check stationarity:If your series isn't stationary, you'll need to transform itâ€”usually through differencing.
  
  
  Making a Series Stationary
Differencing removes trend by computing changes between consecutive values.First differencing removes linear trends. Seasonal differencing removes repeating patterns. Sometimes you need both.After differencing, check stationarity again. Multiple rounds might be needed.
  
  
  Classical Forecasting: ARIMA
ARIMA (AutoRegressive Integrated Moving Average) remains a workhorse for time series forecasting.The three parameters (p, d, q) define the model:: Autoregressive order (how many past values influence the current value): Degree of differencing (how many times to difference for stationarity): Moving average order (how many past errors influence the current value)
Choosing the right parameters requires experimentation. You can use ACF and PACF plots for guidance, or rely on automated selection.
  
  
  Auto-ARIMA: Parameter Selection Made Easy
Manually tuning ARIMA parameters is tedious. The pmdarima library automates this:Auto-ARIMA searches through parameter combinations and selects the best model based on information criteria.When seasonality is present, SARIMA extends ARIMA with additional seasonal parameters.The seasonal order (P, D, Q, m) mirrors the non-seasonal parameters but operates at the seasonal frequency m.
  
  
  Prophet: Accessible Forecasting
Meta's Prophet is designed for business time series. It handles seasonality, holidays, and missing data gracefully.Prophet is less flexible than ARIMA but requires less expertise. It's excellent for quick, reasonable forecasts.
  
  
  Train-Test Splitting for Time Series
Standard random train-test splits don't work for time series. You can't use future data to predict the past.Always split chronologically:The test set must come after the training set. Otherwise, your evaluation is meaningless.Common metrics for time series forecast evaluation:MAPE is intuitive but undefined when actuals are zero. MAE and RMSE are more robust but less interpretable.
  
  
  Cross-Validation for Time Series
Time series cross-validation uses rolling or expanding windows:Each fold trains on historical data and tests on a subsequent period. This gives a realistic estimate of forecast accuracy. Fitting models to non-stationary data produces unreliable forecasts. Complex models with many parameters fit training data perfectly but generalize poorly. Failing to account for obvious seasonal patterns leads to systematic errors. Using future information during trainingâ€”easy to do accidentally with calculated features.Over-reliance on point forecasts. Always consider prediction intervals, not just the central forecast.
  
  
  When Simple Beats Complex
Surprisingly often, simple methods outperform sophisticated ones.Naive forecasts (tomorrow equals today) and seasonal naive forecasts (next January equals last January) are strong baselines. If your fancy model can't beat them, it's not adding value.Exponential smoothing methods are simpler than ARIMA and often perform comparably.Always start simple. Add complexity only when it demonstrably improves forecasts.
  
  
  Frequently Asked Questions
What's the minimum amount of data needed for time series forecasting?
It depends on seasonality. To detect yearly patterns, you need multiple years of data. For weekly patterns, months might suffice. Generally, more data is better.How far ahead can I forecast reliably?
Forecast accuracy degrades with horizon length. Short-term forecasts (days to weeks) are typically much more accurate than long-term ones (quarters to years).Should I use ARIMA or Prophet?
Prophet is easier and handles holidays well. ARIMA offers more control and performs better when properly tuned. Try both and compare on your data.How do I handle missing values?
Interpolation works for small gaps. For larger gaps, consider whether the missing pattern itself contains information. Some methods like Prophet handle missing values automatically.Can I use machine learning for time series?
Yes. LSTMs, Gradient Boosting, and other ML methods work for time series but require careful feature engineering and cross-validation.What if my series has multiple seasonal patterns?
Prophet handles multiple seasonalities well. SARIMA requires choosing the dominant pattern. For complex seasonality, consider Fourier terms as features.How do I forecast multiple related time series?
Hierarchical forecasting and vector autoregression (VAR) handle multiple series. Prophet and other methods can be applied to each series independently.What about external factors that affect my series?
ARIMAX and Prophet with regressors allow you to include external variables. Be careful about needing to forecast the regressors themselves.How do I communicate uncertainty to stakeholders?
Always present prediction intervals alongside point forecasts. Explain that forecasts become more uncertain further into the future.What resources should I use to learn more?
"Forecasting: Principles and Practice" by Hyndman and Athanasopoulos is freely available online and excellent.Time series forecasting doesn't require advanced mathematics. It requires understanding the patterns in your data and choosing appropriate methods.Start with visualization and decomposition. Check stationarity.Try simple methods first. Compare against baselines. Always include uncertainty in your forecasts.With Python's powerful libraries, reliable forecasts are within reach for any data analyst willing to learn the fundamentals.This article was refined with the help of AI tools to improve clarity and readability.]]></content:encoded></item><item><title>Giampaolo Rodola: From Python 3.3 to today: ending 15 years of subprocess polling</title><link>https://gmpy.dev/blog/2026/event-driven-process-waiting</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[One of the less fun aspects of process management on POSIX systems is waiting
for a process to terminate. The standard library's  module has
relied on a busy-loop polling approach since the  parameter was added
to
Popen.wait()
in Python 3.3, around 15 years ago (see
source).
And psutil's
Process.wait()
method uses exactly the same technique (see
source).The logic is straightforward: check whether the process has exited using
non-blocking , sleep briefly, check again, sleep a bit
longer, and so on.In this blog post I'll show how I finally addressed this long-standing
inefficiency, first in psutil, and most excitingly, directly in CPython's
standard library subprocess module.The problem with busy-pollingCPU wake-ups: even with exponential backoff (starting at 0.1ms, capping at
  40ms), the system constantly wakes up to check process status, wasting CPU
  cycles and draining batteries.Latency: there's always a gap between when a process actually terminates and
  when you detect it.Scalability: monitoring many processes simultaneously magnifies all of the
  above.All POSIX systems provide at least one mechanism to be notified when a file
descriptor becomes ready. These are
select(),
poll(),
epoll() (Linux) and
kqueue() (BSD / macOS)
system calls. Until recently, I believed they could only be used with file
descriptors referencing sockets, pipes, etc., but it turns out they can also be
used to wait for events on process PIDs!In 2019, Linux 5.3 introduced a new syscall,
,
which was added to the  module in Python 3.9. It returns a file descriptor
referencing a process PID. The interesting thing is that  can be
used in conjunction with ,  or  to effectively wait
until the process exits. E.g. by using :This approach has zero busy-looping. The kernel wakes us up exactly when the
process terminates or when the timeout expires if the PID is still alive.I chose  over  because  has a historical file
descriptor limit (), which typically caps it at 1024 file
descriptors per-process (reminded me of
BPO-1685000).I chose  over  because it does not require creating an
additional file descriptor. It also needs only a single syscall, which should
make it a bit more efficient when monitoring a single FD rather than many.BSD-derived systems (including macOS) provide the  syscall. It's
conceptually similar to ,  and , but more powerful
(e.g. it can also handle regular files).  can be passed a PID
directly, and it will return once the PID disappears or the timeout expires:Windows does not busy-loop, both in psutil and subprocess module, thanks to
. This means Windows has effectively had event-driven
process waiting from the start. So nothing to do on that front.Both  and  can fail for different reasons. For example,
with  if the process runs out of file descriptors (usually 1024), or
with  /  if the syscall was explicitly blocked at the system
level by the sysadmin (e.g. via SECCOMP). In all cases, psutil silently falls
back to the traditional busy-loop polling approach rather than raising an
exception.This fast-path-with-fallback approach is similar in spirit to
BPO-33671, where I sped up
 by using zero-copy system calls back in 2018. In there,
more efficient  is attempted first, and if it fails (e.g. on
network filesystems) we fall back to the traditional  / 
approach to copy regular files.As a simple experiment, here's a simple program which waits on itself for 10
seconds without terminating:We can measure the CPU context switching using . Before the
patch (the busy-loop):$/usr/bin/time-vpython3test.py>grepcontext
Voluntarycontextswitches:Involuntarycontextswitches:After the patch (the event-driven approach):$/usr/bin/time-vpython3test.py>grepcontext
Voluntarycontextswitches:Involuntarycontextswitches:This shows that instead of spinning in userspace, the process blocks in
 / , and is woken up only when the kernel notifies it,
resulting in just a few CPU context switches.It's also interesting to note that waiting via  (or ) puts
the process into the exact same sleeping state as a plain  call.
From the kernel's perspective, both are interruptible sleeps: the process is
de-scheduled, consumes zero CPU, and sits quietly in kernel space.The  state shown below by  means that the process "sleeps in
foreground".After landing the psutil implementation
(psutil/PR-2706), I took the
extra step and submitted a matching pull request for CPython 
module: cpython/PR-144047.I'm especially proud of this one: this is the  in psutil's 17+
year history that a feature developed in psutil made its way upstream into the
Python standard library. The first was back in 2011, when 
inspired
shutil.disk_usage() (see
python-ideas ML proposal). 15 years ago, Python 3.3 added the  parameter to
 (see
commit). That's
probably where I took inspiration when I first added the  parameter to
psutil's  around the same time (see
commit). Now, 15 years
later, I'm contributing back a similar improvement for that very same 
parameter. .psutil/#2712: proposal to
  extend this to multiple PIDs ().psutil/#2703: proposal for
  asynchronous  integration with .]]></content:encoded></item><item><title>Decision Latency Is the Real Risk in Projects</title><link>https://dev.to/ben_webb_projectmanager/decision-latency-is-the-real-risk-in-projects-122k</link><author>Ben Webb</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 21:33:37 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[**Most projects donâ€™t fail because the plan was wrong.They fail because critical decisions take too long.Not because no one knows what needs to be done.
Not because the data is missing.
But because the decision keeps getting delayed, softened, or deferred.A week becomes a sprint.
A sprint becomes a phase.
A phase becomes â€œweâ€™ll deal with it later.â€On paper, the project is still moving.
In reality, momentum is leaking out through indecision.This is what I mean by decision latency.Itâ€™s the time between when a decision becomes necessary and when someone is willing to own it. That gap is where most project risk is created.What makes it dangerous is that it usually looks reasonable.â€œWe just need one more data point.â€
â€œLetâ€™s see how it trends.â€
â€œWeâ€™ll take that offline.â€
â€œWeâ€™ll re-baseline next cycle.â€Each of those sounds sensible in isolation. Together, they quietly stall the project.Dashboards, frameworks, and AI can improve visibility and analysis, but they donâ€™t reduce decision latency. In some cases, they make it worse by giving people more reasons to wait.More data.
More scenarios.Iâ€™ve written before about why AI struggles with real project work. This is a big part of it. AI can tell you what usually happens next. It canâ€™t tell you which trade-off youâ€™re prepared to live with when the information is incomplete and the pressure is real.That choice isnâ€™t analytical.
Itâ€™s accountable.Decision latency also hides behind good governance.Steering committees meet. Papers are circulated. Risks are noted. Actions are captured. And still, the core decision gets deferred because no one wants to be the one who makes it too early.The longer that goes on, the fewer options remain. By the time the decision is forced, the project has already paid the price.Experienced project managers learn to recognise this early. They stop asking, â€œDo we have enough information?â€ and start asking a harder question:â€œWhat happens if we donâ€™t decide now?â€Thatâ€™s usually when the real risk becomes visible.Projects donâ€™t need perfect information.
They need timely decisions with clear ownership.Everything else is support.]]></content:encoded></item><item><title>How I scraped Reddit to find $10k leads without getting banned ðŸ•µï¸â€â™‚ï¸</title><link>https://dev.to/hypertools/how-i-scraped-reddit-to-find-10k-leads-without-getting-banned-35o6</link><author>Roberto | Hyper-Tools</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 21:06:24 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most people look for sales leads on LinkedIn. But LinkedIn is a resume database, not an intent database.Real intent happens on Reddit."What is the best alternative to DocuSign?""I hate how expensive Salesforce is.""Is there a tool to fix broken SVGs?"These aren't just comments. They are  from people with wallets in their hands.
  
  
  The Problem: The Ban Hammer
If you try to scrape Reddit aggressively, you will get banned. Their API pricing is astronomical, and their bot detection is world-class.
  
  
  The Solution: The "Sonar" Approach
I built  to listen, not spam. It tracks specific problem-keywords related to my products. Uses Gemini AI to determine if the user is  buying or just complaining. I don't auto-reply. I get a daily digest of high-quality leads and reach out manually.
My conversion rate on these leads is 10x higher than cold email.I'm opening up the beta for  soon.]]></content:encoded></item><item><title>I beat the 6-month Global Entry wait time with 50 lines of Python âœˆï¸</title><link>https://dev.to/hypertools/i-beat-the-6-month-global-entry-wait-time-with-50-lines-of-python-47nf</link><author>Roberto | Hyper-Tools</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 21:05:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Getting an interview for Global Entry (Trusted Traveler) is harder than getting Taylor Swift tickets. In some cities, the next available slot is 8 months away.I wasn't going to wait that long.Most government sites are legacy nightmares. But surprisingly, the TTP (Trusted Traveler Programs) scheduler uses a public JSON endpoint to fetch available slots.They don't document it, but it's there.I wrote a simple Python watcher that: Fetches the slot JSON for my desired location ID. Compares the "available_slots" list against my target date range. Sends me a push notification (via Pushover) when a slot opens up.The code is surprisingly simple. No headless browser needed, just pure .I ran the script on a $5 VPS. It took  to catch a cancellation for . I walked in, did the interview, and got approved.I'm wrapping this logic into a user-friendly tool for those who don't want to write code.]]></content:encoded></item><item><title>Docker for Beginners: Stop saying &quot;It works on my machine.&quot;</title><link>https://dev.to/frankdotdev/docker-for-beginners-stop-saying-it-works-on-my-machine-45ll</link><author>Frank Oge</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 20:30:56 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[There is a sentence that every Junior Developer says at least once. Itâ€™s the sentence that makes Senior Engineers sigh and rub their temples.
â€‹"But it works on my machine!"
â€‹You wrote the code. You tested it on your laptop. It worked perfectly. Then you pushed it to the server, and it crashed. Why?
Maybe you have Python 3.10, and the server has Python 3.8.
Maybe you are on a Mac, and the server is Linux.
Maybe you installed a library three months ago and forgot to put it in your requirements.txt.
â€‹This is called Environment Drift, and it is a nightmare.
â€‹The solution is Docker.
â€‹The Shipping Container Analogy
â€‹Before 1950, shipping goods was a disaster. You had barrels of whiskey, sacks of flour, and loose furniture. Loading them onto a ship took forever because every item was a different shape.
â€‹Then, the standardized Shipping Container was invented.
It didn't matter what was inside (cars, grain, TVs). The container was always the exact same size. The crane didn't need to know what was inside; it just needed to lift the box.
â€‹Docker does this for code.
It creates a digital "Container" that holds everything your app needs:
â€‹The Operating System (e.g., a tiny version of Linux).
â€‹The Code.
â€‹The Environment Variables.
â€‹When you ship your app, you don't ship just the code. You ship the whole box. If it runs on your laptop, it is mathematically guaranteed to run on the server, because the environment inside the box never changes.
â€‹The 3 Concepts You Need to Know
â€‹Forget the complex commands for a second. Understand these three words:
â€‹Dockerfile (The Recipe): A text file that tells Docker how to build your box. ("Start with Linux, install Python, copy my files, run this command").
â€‹Image (The Blueprint): When you run the recipe, you get an Image. This is a frozen snapshot of your app. It cannot be changed.
â€‹Container (The House): When you actually run the Image, it becomes a Container. This is the live, running application.
â€‹Let's say you have a simple Python script.
You tell your friend, "Install Python, then run pip install requests, then run python main.py."
(This fails if they have the wrong Python version).
â€‹With Docker:RUN pip install -r requirements.txtCMD ["python", "main.py"]Now, your friend just types:
docker build -t my-app .
â€‹It works instantly. No installing Python. No version conflicts. It just works.
â€‹Docker isn't just for DevOps engineers. It is for anyone who values their sanity.
It forces you to document your dependencies explicitly. It allows you to onboard new developers in minutes, not days.
â€‹If you want to be a professional software engineer, stop relying on your local setup. Containerize everything.
â€‹Hi, I'm Frank Oge. I build high-performance software and write about the tech that powers it. If you enjoyed this, check out more of my work at frankoge.com]]></content:encoded></item><item><title>Week 6 Scripting Challenge: Build a TLS Certificate Security Validator</title><link>https://dev.to/fosres/challenge-build-a-tls-certificate-security-validator-3acn</link><author>fosres</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 20:02:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[âš ï¸ Real Interview Scenario: TLS certificate validation is a common Security Engineering interview question. You'll be given a certificate in TEXT format and asked to identify what's wrong with it. This exercise prepares you for exactly that scenario.
  
  
  ðŸŽ¯ For Security Engineers: Why This Exercise Matters
You need to understand WHAT makes a TLS certificate valid and WHY - not just IF it's valid.Off-the-shelf tools like sslyze, testssl.sh, and OpenSSL will give you a simple answer:sslyze google.com:443
âœ“ Certificate is valid
But they won't teach you: SHA-1 signatures are catastrophically broken (Google SHAttered attack, 2017) CA:TRUE on an end-entity certificate enables complete PKI compromise (DigiNotar breach, 2011) browsers ignore the Common Name field (RFC 6125 deprecation) the 398-day validity limit exists (CA/Browser Forum Ballot SC22) wildcard matching actually works ( rules) to detect self-signed certificates (SKI/AKI comparison logic) Key Usage flags matter (Digital Signature vs Key Encipherment vs Certificate Sign)This exercise forces you to implement the validation logic yourself.By the end, you won't just run security scanners - you'll understand X.509 certificate structure at a fundamental level. This understanding separates those who run tools from those who understand PKI fundamentals.
  
  
  What Do Off-the-Shelf Tools Actually Tell You?
Here's what happens when you check an expired certificate with popular tools:openssl verify expired_cert.pem
error 10 at 0 depth lookup: certificate has expired
âŒ Cryptic error code ("error 10")âŒ Doesn't show which date field or the actual datesâŒ No explanation of why it matterssslyze  expired.com:443
  Certificate Validation:
     Hostname Validation:     FAILED - Certificate does NOT match
     Path Validation:         OK - Certificate is trusted
âŒ Says it failed but doesn't show the SANs fieldâŒ Doesn't explain wildcard matching rulesâŒ Black box: "It failed" (but not WHERE or WHY)testssl.sh https://expired.com
 Certificate Validity   expires  days 2024-11-15  2024-12-15 WARN
âŒ Shows dates but doesn't explain Not Before vs Not AfterâŒ No explanation of the underlying X.509 structureYour Validator (What You'll Build):python validator.py test_006_expired.pem www.example.com

âŒ FAIL - Certificate invalid 1/20 checks passed

FAILED CHECKS:
  âŒ Check 2: Certificate expired
     Not After:  2024-12-15 23:59:59 UTC
     Current:    2026-01-24 19:30:00 UTC
     Expired by: 40 days

     Why this matters: Expired certificates cannot be trusted.
     The private key may have been compromised after expiration.
     Browsers reject these to prevent MITM attacks.

     Real-world example: Microsoft Teams outage 2020

PASSED CHECKS:
  âœ… Check 1: Version 3 âœ“
âœ… Shows EXACTLY which check failed and whyâœ… Shows the actual certificate field valuesâœ… Explains the security reasoningâœ… Provides real-world contextâœ… You understand the X.509 structure because YOU parsed itThis is the difference between using a calculator and understanding mathematics. 60-90 minutes Advanced PKI/TLS, X.509 Certificates, Cryptography, Application SecurityEvery time you visit , your browser performs 20 critical security checks on the TLS certificate in . One failed check = connection rejected. Build the certificate validator that browsers use! Master the 20-point checklist, then implement it in Python.
  
  
  ðŸ“¦ Get the Complete Exercise
ðŸ“  - 68 test certificates (valid and invalid)ðŸ¤–  - Automated grader (instant feedback)âœ…  - Reference solutionðŸ“–  - Setup instructionsâ­ Star the repo to get notified of new security exercises!
  
  
  Why This Matters in Real Life

  
  
  When Certificate Validation Fails
Microsoft Teams Outage (2020)Expired certificate took down Microsoft Teams globallyMillions affected during COVID-19 remote work Check #2 failed - certificate expiration not monitoredExpired cert on security tool = blind security team147 million people's data stolenBreach undetected for  Check #2 failed - expired certificate on critical security infrastructureExpired TLS certificate caused global outageMillions unable to access service Check #2 failed - automated renewal failed, no validation in place
  
  
  ðŸ” THE 20-POINT VALIDATION CHECKLIST
This is THE complete checklist browsers use for EVERY HTTPS connection.
  
  
  Quick Reference: Required vs Recommended vs Optional
 - Reflects CA/Browser Forum Ballot SC63 (March 2024) and strict RFC 5280 compliancePhase 1: Fundamental ValidityV1/V2 don't support extensionsNot expired/not yet validCurrent date within validity periodNo MD5, no SHA-1 (both broken)Phase 2: Identity ValidationCan be empty if SANs present (RFC 5280)Required by CA/Browser ForumBasic Constraints CA:FALSECA/B Forum: "if present" (99%+ have it)SC63 (Mar 2024): Was RECOMMENDEDRequired by CA/Browser ForumSC63 (Mar 2024): Was REQUIRED2+ SCTs (Chrome, Safari, Firefox)Phase 5: Chain ValidationRFC 5280: Not required for end-entityCA/Browser Forum Ballot SC22âœ…  15 checks (must pass for public certificates)âš ï¸  2 checks (best practice, near-universal)âš ï¸  2 checks (not required but 99%+ have them)âš ï¸  1 check (depends on other fields)Ballot SC63 (March 15, 2024): CRL now REQUIRED, OCSP now OPTIONAL (privacy concerns) Key Usage and SKI technically optional for end-entity certificates Short-lived certificates (â‰¤7 days) do not require CRL or OCSP support.Detailed explanations below - each check explained with examples and code:Every check is explained in plain English with:âœ… What you'll see in a good certificateðŸ“– Plain English explanation (all acronyms explained!)âš ï¸ Why it matters (real attack scenarios)âŒ What happens if it fails (browser errors)
  
  
  ðŸ”¹ PHASE 1: FUNDAMENTAL VALIDITY
Stop immediately if any of these fail!
  
  
  âœ… CHECK 1: Certificate Version
What you'll see in a good certificate:Plain English explanation:
X.509 certificates come in 3 versions:Version 1 (0x0) - Ancient, from 1988, no extensionsVersion 2 (0x1) - Rarely used - Modern standard, supports extensions âœ…
Only Version 3 supports the security extensions we need:Subject Alternative Names (SANs) - for hostnamesKey Usage - what the key can doExtended Key Usage - what the certificate is forOCSP/CRL - revocation checkingCertificate Transparency - public audit trail RFC 5280 Section 4.1.2.1 - "When extensions are used, as expected in this profile, version MUST be 3"What happens if it fails:
Certificate cannot have modern security features â†’ Reject immediately
  
  
  âœ… CHECK 2: Certificate Expiration (Not Expired / Not Yet Valid)
What you'll see in a good certificate:Validity
    Not Before: Dec  1 00:00:00 2025 GMT
    Not After : Feb 28 23:59:59 2026 GMT
Plain English explanation:
Every certificate has two dates: = Certificate becomes valid at this date/time = Certificate expires at this date/timeThe current date/time MUST be between these two dates. can't be trusted (keys might be compromised)Not-yet-valid certificates might be test/staging certs leaked earlyCertificate expired on: Jan 1, 2026
Today's date: Jan 25, 2026
â†’ âŒ EXPIRED! Don't trust!
What happens if it fails:
Browser shows: "NET::ERR_CERT_DATE_INVALID" - Connection blocked
  
  
  âœ… CHECK 3: Signature Algorithm (SHA-256 or Better)
What you'll see in a good certificate:Signature Algorithm: sha256WithRSAEncryption
Signature Algorithm: ecdsa-with-SHA256
Plain English explanation:
The signature proves the CA really issued this certificate. The hash algorithm must be strong.This check validates the  (SHA-256, SHA-384, SHA-512), NOT the signature algorithm (RSA vs ECDSA).âœ… Allowed hash algorithms:âœ… Allowed signature algorithms (with approved hash):, , , , MD5 (broken 2004): SHA-1 (broken 2017): , Outdated signature algorithms:DSA (deprecated by NIST 2019): , Note: DSA deprecated even with strong hash algorithmsFocus on the HASH algorithm, not the signature type:
- "sha256WithRSAEncryption" â†’ SHA-256 âœ…
- "ecdsa-with-SHA256" â†’ SHA-256 âœ…
- "sha1WithRSAEncryption" â†’ SHA-1 âŒ
- "ecdsa-with-SHA1" â†’ SHA-1 âŒ
Why this matters - Collision attacks:MD5 collision (2008):
1. Attacker creates GOOD cert request
2. Also creates EVIL cert with same MD5 hash
3. CA signs GOOD cert
4. Attacker swaps in EVIL cert (same signature!)
5. Browser trusts EVIL cert âŒ
Flame malware (2012): Used MD5 collision to forge Microsoft certificateSHAttered (2017): Demonstrated practical SHA-1 collision CA/Browser Forum Baseline Requirements - "CAs MUST NOT issue certificates using MD5 or SHA-1" (since January 2016)What happens if it fails:
Browser shows: "NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM"
  
  
  âœ… CHECK 4: Public Key Strength
What you'll see in a good certificate:Public Key Algorithm: rsaEncryption
    Public-Key: (2048 bit)
Public Key Algorithm: id-ecPublicKey
    Public-Key: (256 bit)
    ASN1 OID: prime256v1
    NIST CURVE: P-256
Plain English explanation:
The public key must be strong enough to resist brute-force attacks.RSA: â‰¥2048 bits (3072 or 4096 recommended)ECDSA: â‰¥P-256 (P-384 or P-521 recommended)RSA-1024 (crackable with $1M+ budget)RSA-512 (crackable in hours)RSA-1024 security level:
- 1999: "Safe for 20+ years"
- 2010: Factored by academics
- 2015: NSA likely can crack
- 2025: Definitely broken âŒ
 CA/Browser Forum Baseline Requirements - "Recommended key strengths are at least 2048-bit RSA or Elliptic Curve using NIST P-256"What happens if it fails:
Browser shows: "NET::ERR_CERT_WEAK_KEY"
  
  
  ðŸ”¹ PHASE 2: IDENTITY VALIDATION

  
  
  âš ï¸ CHECK 5: Subject Distinguished Name (DN)
What you'll see in a good certificate:Subject: C=US, ST=California, O=Example Inc, CN=www.example.com
Plain English explanation:
Subject DN = Who owns this certificate
  
  
  ðŸŽ¯ The Core Rule (RFC 5280 Section 4.1.2.6)
Check 5 validates the Subject DN field with a  based on whether Subject has content:SANs Critical Requirementâš ï¸  - SANs can be critical OR non-criticalâŒ  if SANs not critical Check 5 only cares about the SANs critical flag when Subject is empty. When Subject has components, the SANs critical flag is irrelevant to Check 5.
  
  
  âœ… Case 1: Subject Present, SANs Non-Critical (99% of real certificates)
Subject: C=US, ST=California, O=Example Inc, CN=www.example.com

X509v3 Subject Alternative Name:
    DNS:www.example.com, DNS:example.com
 âœ…  Subject has DN components (C=US, O=Example Inc, CN=...), so Check 5 passes immediately. The SANs critical flag is  by Check 5. ssllabs.com, google.com, amazon.com - nearly all production certificates follow this pattern.
  
  
  âœ… Case 2: Subject Present, SANs Critical (also valid)
Subject: C=US, ST=California, O=Example Inc, CN=www.example.com

X509v3 Subject Alternative Name: critical
    DNS:www.example.com, DNS:example.com
 âœ…  Subject has DN components, so Check 5 passes. SANs being marked critical is  when Subject is present. Some CAs mark SANs as critical even when Subject is present - this is valid and doesn't affect Check 5.
  
  
  âœ… Case 3: Empty Subject, SANs Critical (rare but valid)
Subject: (empty)

X509v3 Subject Alternative Name: critical
    DNS:www.example.com, DNS:example.com
 âœ…  Subject is empty, BUT SANs is marked critical, which satisfies RFC 5280's requirement. Certificates issued by modern CAs that choose to omit the Subject DN entirely and rely solely on SANs for identity.
  
  
  âŒ Case 4: Empty Subject, SANs Non-Critical (INVALID)
Subject: (empty)

X509v3 Subject Alternative Name:
    DNS:www.example.com, DNS:example.com
 âŒ  Subject is empty AND SANs is not marked critical - violates RFC 5280. Without a Subject DN and without the critical flag, older validators might ignore the SANs extension completely, causing validation to fail in unpredictable ways.
  
  
  ðŸ” How to Check if SANs is Critical
The "critical" keyword appears on the same line as the extension name:X509v3 Subject Alternative Name: critical  â† Has "critical" keyword
    DNS:www.example.com
X509v3 Subject Alternative Name:  â† No "critical" keyword
    DNS:www.example.com
 "SANs must always be critical"âŒ False - SANs only MUST be critical when Subject is empty "Subject can never be empty"âŒ False - RFC 5280 allows empty Subject if SANs is marked critical "If Subject exists, SANs cannot be critical"âŒ False - SANs can be critical even when Subject exists (it's just not required) "SANs critical flag requirement is conditional on Subject being empty"âœ… True - this is the actual RFC 5280 rule
  
  
  ðŸ“Š Decision Tree for Check 5
START
  â”‚
  â”œâ”€ Does Subject have DN components? (C=, O=, CN=, etc.)
  â”‚   â”‚
  â”‚   â”œâ”€ YES â†’ âœ… Check 5 PASS
  â”‚   â”‚         (SANs critical flag doesn't matter)
  â”‚   â”‚
  â”‚   â””â”€ NO (Subject empty)
  â”‚       â”‚
  â”‚       â”œâ”€ Is SANs marked critical?
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€ YES â†’ âœ… Check 5 PASS
  â”‚       â”‚   â”‚
  â”‚       â”‚   â””â”€ NO  â†’ âŒ Check 5 FAIL

  
  
  ðŸ”¬ What Counts as "Empty" Subject?
These are considered empty:Subject:                    â† Nothing after colon
Subject: (empty)            â† Explicit empty marker
Subject: CN=                â† Key without value
Subject: =value             â† Value without key
Subject: CN= , O=           â† All values empty
Subject: CN=www.example.com           â† Has valid component
Subject: C=US, O=Example              â† Has valid components
Subject: CN=www.example.com, O=       â† At least one valid component
 A Subject with at least one valid  pair (where both key and value are non-empty) is considered "present" for Check 5 purposes.Old certificates (pre-2000): Used only Subject DN for identityCommon Name (CN) field contained the hostnameModern certificates (2000+): Use SANs for identitySANs contains all valid hostnamesSubject DN became optionalCN field deprecated for hostname validation (RFC 6125) Allows empty Subject IF:SANs is marked critical (forces validators to check it)Ensures backward compatibility with older validatorsIf Subject is empty and SANs is NOT critical:Legacy validators might skip SANs (non-critical extensions can be ignored)Certificate would have no identity informationValidation would fail unpredictablyBy requiring SANs to be critical when Subject is empty:Forces all validators (old and new) to check SANsGuarantees certificate identity can be validatedMaintains backward compatibilityRFC 5280 Section 4.1.2.6:"If the subject field contains an empty sequence, then the issuing CA MUST include a subjectAltName extension that is marked as critical." This is a one-way requirement. It does NOT say "if Subject is present, SANs cannot be critical." Both critical and non-critical SANs are valid when Subject exists.
  
  
  â“ What Happens if Check 5 Fails?
Scenario 1: Both Subject and SANs emptyCertificate has no identity informationCannot determine who owns the certificate Reject certificate immediatelyScenario 2: Subject empty, SANs not criticalViolates RFC 5280 Section 4.1.2.6Legacy validators might ignore SANs Reject certificate (non-compliant)Scenario 3: Subject presentCheck 5 automatically passesIdentity validation continues with Check 6 (SANs present) and Check 7 (hostname match)
  
  
  âœ… CHECK 6: Subject Alternative Names (SANs)
What you'll see in a good certificate:X509v3 Subject Alternative Name:
    DNS:www.example.com
    DNS:example.com
    DNS:api.example.com
Plain English explanation:
SANs = List of ALL valid hostnames for this certificateCertificate for: www.example.com
User visits: api.example.com

WITHOUT SANs listing api.example.com:
â†’ âŒ Hostname mismatch! Reject!

WITH SANs listing api.example.com:
â†’ âœ… Valid! Allow connection!
DNS:*.example.com
â†’ Matches: www.example.com, api.example.com
â†’ Does NOT match: example.com (no subdomain!)
â†’ Does NOT match: foo.bar.example.com (only 1 level!)
 CA/Browser Forum Baseline Requirements - "This extension MUST be present"What happens if it fails:
Cannot validate hostname â†’ Reject
  
  
  âœ… CHECK 7: Hostname Matches SAN
User visiting: www.example.com
SANs: DNS:www.example.com, DNS:example.com
â†’ âœ… MATCH!
Plain English explanation:
The hostname in the browser address bar MUST match one of the SANs in the certificate.â­ CRITICAL: CHECK 7 USES DNS WILDCARD PATTERN MATCHING â­This check performs DNS wildcard pattern matching where  is a wildcard character. DNS wildcards follow RFC 6125 rules, NOT shell/filesystem wildcard rules!ðŸ”‘ Understanding DNS Labels (The Foundation of Wildcard Matching)Before understanding wildcard matching, you must understand :A label is one "part" of a domain name separated by dotsThink of labels as "levels" in the domain hierarchyExample: Breaking down  into labels:www.example.com
 â”‚    â”‚      â”‚
 â”‚    â”‚      â””â”€â”€â”€ Label 3: "com" (TLD/root)
 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Label 2: "example" (base domain)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Label 1: "www" (subdomain - LEFTMOST)

Total: 3 labels
mail.example.com        â†’ 3 labels: ["mail", "example", "com"]
api.example.com         â†’ 3 labels: ["api", "example", "com"]
wrong.host.badssl.com   â†’ 4 labels: ["wrong", "host", "badssl", "com"]
sub.domain.example.com  â†’ 4 labels: ["sub", "domain", "example", "com"]
example.com             â†’ 2 labels: ["example", "com"]
ðŸŽ¯ The Core Rule of DNS Wildcard Matching:The wildcard  replaces EXACTLY ONE label - no more, no less!This is fundamentally different from shell wildcards which can match multiple levels.â­ CRITICAL DNS Wildcard Rules (RFC 6125 Section 6.4.3):Wildcard replaces exactly ONE label = [exactly-one-label].example.comNOT:  = Wildcard only in leftmost positionPattern and hostname must have same number of labelsThis is the key rule most implementations miss!Case-insensitive comparison matches Exact non-wildcard labels must match exactlyIn , both "example" and "com" must match exactlyðŸ“Š DNS Wildcard Matching Examples (Understanding Label Counts):Pattern:  *.example.com
Hostname: www.example.com

Breaking into labels:
Pattern:  ["*",     "example", "com"]  â†’ 3 labels
Hostname: ["www",   "example", "com"]  â†’ 3 labels

Label count: 3 = 3 âœ… MATCH!
- Label 1: "*" matches "www" âœ…
- Label 2: "example" = "example" âœ…
- Label 3: "com" = "com" âœ…
Result: âœ… PASS
Example 2: Correct Match (Different Subdomain)Pattern:  *.example.com
Hostname: api.example.com

Breaking into labels:
Pattern:  ["*",    "example", "com"]  â†’ 3 labels
Hostname: ["api",  "example", "com"]  â†’ 3 labels

Label count: 3 = 3 âœ… MATCH!
- Label 1: "*" matches "api" âœ…
- Label 2: "example" = "example" âœ…
- Label 3: "com" = "com" âœ…
Result: âœ… PASS
Example 3: NO MATCH - Too Many LabelsPattern:  *.example.com
Hostname: wrong.host.badssl.com

Breaking into labels:
Pattern:  ["*",      "example", "com"]        â†’ 3 labels
Hostname: ["wrong",  "host", "badssl", "com"] â†’ 4 labels

Label count: 3 â‰  4 âŒ NO MATCH!
Why: Wildcard replaces ONE label, but hostname has TWO extra labels
The wildcard can't "absorb" multiple labels!
Result: âŒ FAIL
Example 4: NO MATCH - Too Few LabelsPattern:  *.example.com
Hostname: example.com

Breaking into labels:
Pattern:  ["*",       "example", "com"]  â†’ 3 labels
Hostname: ["example", "com"]             â†’ 2 labels

Label count: 3 â‰  2 âŒ NO MATCH!
Why: Wildcard needs a label to replace, but hostname has no subdomain
Result: âŒ FAIL
Example 5: NO MATCH - Wrong Base DomainPattern:  *.api.example.com
Hostname: www.example.com

Breaking into labels:
Pattern:  ["*",   "api", "example", "com"]  â†’ 4 labels
Hostname: ["www", "example", "com"]         â†’ 3 labels

Label count: 4 â‰  3 âŒ NO MATCH!
Even if we ignore count: "api" â‰  nothing, "example" â‰  "example" position mismatch
Result: âŒ FAIL
ðŸŽ“ Conceptual Approach to DNS Wildcard Matching:Step 1: Split into labelsSplit the pattern on dots:  â†’ Split the hostname on dots:  â†’ ["www", "example", "com"]Step 2: Check label countCount labels in pattern: 3Count labels in hostname: 3If counts don't match â†’ NO MATCH, stop here!This step is CRITICAL and catches most invalid matchesStep 3: Compare each label positionGo through each position (left to right)If pattern label is  â†’ any hostname label matches (continue)If pattern label is not  â†’ must match exactly (case-insensitive)If any non-wildcard label doesn't match â†’ NO MATCHStep 4: If all positions match â†’ MATCH!ðŸš¨ Common Implementation Mistakes to Avoid:âŒ WRONG: Using shell/filesystem wildcardsShell wildcards (like fnmatch, glob, Path.match):
- * matches EVERYTHING including dots
- Would match "*.example.com" to "wrong.host.example.com" âŒ
- This is a SECURITY BUG!
âŒ WRONG: Using regex wildcards without constraintsRegex .* matches EVERYTHING including dots
- Would match "*.example.com" to "a.b.c.d.example.com" âŒ
- Must constrain * to match ONE label only
âŒ WRONG: Forgetting to check label countWithout label count check:
- Might incorrectly match multi-level subdomains
- Major security vulnerability!
âœ… CORRECT: DNS wildcard matching (RFC 6125)RFC 6125 rules:
- Wildcard replaces exactly ONE label
- Must check label count FIRST
- Compare each label position
More wildcard matching examples:SAN: DNS:*.example.com

âœ… Matches: www.example.com (one level - 3 labels match 3 labels)
âœ… Matches: api.example.com (one level - 3 labels match 3 labels)
âœ… Matches: mail.example.com (one level - 3 labels match 3 labels)
âŒ NO match: example.com (wildcard needs subdomain - 2 labels â‰  3 labels)
âŒ NO match: foo.bar.example.com (wildcard only covers 1 level - 4 labels â‰  3 labels)
âŒ NO match: wrong.host.badssl.com (different base domain - 4 labels â‰  3 labels)
âš ï¸ CRITICAL: TLD Wildcards Are FORBIDDEN by RFC 6125 âš ï¸These wildcards are INVALID and MUST be rejected:âŒ FORBIDDEN: DNS:*.com (TLD wildcard)
âŒ FORBIDDEN: DNS:*.org (TLD wildcard)
âŒ FORBIDDEN: DNS:*.net (TLD wildcard)
âŒ FORBIDDEN: DNS:*.co.uk (public suffix wildcard)

Why forbidden?
- One certificate would cover ALL domains under that TLD
- Massive security risk
- Would allow attacker to impersonate any .com domain
- RFC 6125 Section 6.4.3 explicitly prohibits this
Example of the attack TLD wildcards prevent:If *.com was allowed:
1. Attacker gets certificate with SAN: DNS:*.com
2. Certificate would match: google.com, amazon.com, facebook.com, ANY .com domain!
3. âŒ Complete breakdown of trust model!

RFC 6125 prevents this by FORBIDDING wildcards on public suffixes.
âš ï¸ IMPORTANT: How TLD Wildcards Are Handled (NOT Automatic Fail!)TLD wildcard SANs should be SKIPPED/IGNORED, not cause automatic failure.If certificate has: DNS:*.com, DNS:www.example.com, DNS:example.com
And hostname is: www.example.com

Validation process:
1. Check DNS:*.com
   â†’ Is TLD wildcard? YES
   â†’ Action: SKIP this SAN (don't attempt to match)
   â†’ Continue to next SAN

2. Check DNS:www.example.com
   â†’ Is TLD wildcard? NO
   â†’ Is valid? YES
   â†’ Does hostname match? YES (exact match)
   â†’ Result: âœ… PASS CHECK 7

CHECK 7 PASSES because a valid SAN matched the hostname!
 Treat TLD wildcard SANs like malformed data - ignore them and continue checking other SANs.âŒ ONLY TLD wildcard SANs (no valid SANs to check)âŒ Has valid SANs, but none match the hostnameâœ… At least one valid (non-TLD-wildcard) SAN matches the hostnameExample 1: Only TLD wildcard (FAILS)SANs:     DNS:*.com
Hostname: www.example.com
Result:   âŒ FAIL CHECK 7
Reason:   No valid SAN to match against (only TLD wildcard)
Example 2: TLD wildcard + matching valid SAN (PASSES)SANs:     DNS:*.com, DNS:www.example.com
Hostname: www.example.com
Result:   âœ… PASS CHECK 7
Reason:   Valid SAN (www.example.com) matches hostname
Example 3: TLD wildcard + non-matching valid SAN (FAILS)SANs:     DNS:*.com, DNS:api.example.com
Hostname: www.example.com
Result:   âŒ FAIL CHECK 7
Reason:   No valid SAN matches hostname (api â‰  www)
ðŸ”— How to Detect TLD Wildcards Programmatically:To properly validate and reject TLD wildcards, you can retrieve the official list of valid TLDs from IANA:IANA TLD List (Updated Daily):https://data.iana.org/TLD/tlds-alpha-by-domain.txt
Example TLD list content:# Version 2026012600, Last Updated Mon Jan 27 07:07:01 2026 UTC
COM
NET
ORG
EDU
GOV
MIL
UK
CO
...
TLD list includes both generic TLDs (.com, .org) and country-code TLDs (.uk, .jp)Some TLDs have second-level registrations (.co.uk, .com.au) - also forbiddenFor production code, cache the TLD list and update periodicallyIANA updates this list when new TLDs are addedFigure out how to use this list to detect TLD wildcards in your validator!Certificate SANs: DNS:example.com
User visits: www.example.com
â†’ âŒ NO MATCH! (www. is a subdomain!)

Certificate SANs: DNS:*.example.com  
User visits: example.com
â†’ âŒ NO MATCH! (wildcard requires subdomain!)

Certificate SANs: DNS:*.example.com
User visits: foo.bar.example.com
â†’ âŒ NO MATCH! (wildcard only covers 1 level!)

Certificate SANs: DNS:*.com
User visits: example.com
â†’ âŒ INVALID! (TLD wildcards FORBIDDEN by RFC 6125!)
Why wildcard matching is used here:SANs contain  for hostnamesOne certificate can cover multiple subdomains is a  that matches many hostnamesBut TLD wildcards would be too dangerous and are forbiddenTest Cases in Challenge That REQUIRE Wildcard Matching:Your validator MUST correctly handle these test certificates to pass CHECK 7:Test 003: Basic Wildcard (MUST PASS)SANs:     DNS:*.example.com, DNS:example.com
Hostname: www.example.com
Expected: âœ… PASS (www.example.com matches *.example.com via wildcard)
Test 015: Wildcard Mismatch (MUST FAIL)SANs:     DNS:*.api.example.com
Hostname: www.example.com
Expected: âŒ FAIL (wrong base domain - www.example.com doesn't match *.api.example.com)
Test 019: TLD Wildcard (MUST FAIL)SANs:     DNS:*.com
Hostname: www.example.com
Expected: âŒ FAIL (no valid SAN to match - only has TLD wildcard which is skipped)
Note:     Fails because ONLY SAN is invalid TLD wildcard, not because TLD wildcard exists
Test 020: Subdomain Wildcard (MUST PASS)SANs:     DNS:*.example.com, DNS:example.com
Hostname: www.example.com
Expected: âœ… PASS (wildcard match)
Test 047: Mixed Wildcard + Exact (MUST PASS)SANs:     DNS:*.example.com, DNS:www.example.com, DNS:example.com
Hostname: www.example.com
Expected: âœ… PASS (matches via exact OR wildcard)
Test 074: Proton.me Production Cert (MUST PASS)SANs:     DNS:*.proton.me, DNS:*.pr.tn, DNS:proton.me, DNS:pr.tn
Hostname: mail.proton.me (or other subdomains)
Expected: âœ… PASS (mail.proton.me matches *.proton.me)
Test 075: badssl.com Production Cert (MUST PASS)SANs:     DNS:*.badssl.com, DNS:badssl.com
Hostname: expired.badssl.com (or other subdomains)
Expected: âœ… PASS (expired.badssl.com matches *.badssl.com)
 Implement wildcard matching that:âœ… Matches single-level subdomains ( matches )âŒ Rejects multi-level subdomains ( does NOT match )âŒ Rejects base domain ( does NOT match )âŒ Rejects TLD wildcards ( is INVALID per RFC 6125)âœ… Case-insensitive matchingâœ… Wildcard only in leftmost label Don't use  - it's for file paths, not hostnames! You need RFC 6125 compliant matching.RFC 6125 Section 6.4.3 - Server Identity Validation (Wildcard Certificates)CA/Browser Forum Baseline Requirements Section 3.2.2.6 - Wildcard Domain ValidationWhat happens if it fails:
Browser shows: "NET::ERR_CERT_COMMON_NAME_INVALID"
  
  
  ðŸ”¹ PHASE 3: ACCESS CONTROL

  
  
  âœ… CHECK 8: Basic Constraints (CA:FALSE)
What you'll see in a good certificate:X509v3 Basic Constraints: critical
    CA:FALSE
Plain English explanation:
This certificate is for a SERVER, NOT a Certificate Authority. = Can sign other certificates (CAs only) = Cannot sign certificates (servers, users)Why this matters - CA impersonation attack:Without CA:FALSE enforcement:
1. Attacker gets valid cert for evil.com
2. Cert has CA:TRUE (mistake!)
3. Attacker signs fake google.com cert
4. Browser trusts it (signed by "valid" CA)
5. âŒ Game over!

With CA:FALSE enforcement:
1. Browser checks: CA:FALSE âœ…
2. Cert cannot sign anything
3. Attack prevented âœ…
X509v3 Basic Constraints: critical  â† MUST say "critical"!
    CA:FALSE
 CA/Browser Forum Certificate Contents - "If present, the cA field MUST be set false"What happens if it fails:
Certificate could be used to forge other certificates â†’ Reject
  
  
  âš ï¸ CHECK 9: Key Usage Flags (RSA vs ECDSA Requirements)
 âš ï¸ OPTIONAL (Universal in practice - 99%+ have it)What you'll see in good certificates:Public Key Algorithm: rsaEncryption
X509v3 Key Usage: critical
    Digital Signature, Key Encipherment
Public Key Algorithm: id-ecPublicKey  
X509v3 Key Usage: critical
    Digital Signature
Plain English explanation:
Key Usage = What cryptographic operations this public key can perform
  
  
  ðŸ”‘ Algorithm-Specific Requirements
The requirements differ based on the certificate's signature algorithm:âœ…  be marked as criticalâœ…  be marked as critical (ECDSA keys cannot encrypt)
  
  
  RSA Key Exchange (TLS 1.2 and earlier)
In traditional RSA key exchange, the server's RSA certificate is used for TWO different operations: Signs the ServerKeyExchange message (DHE) or verifies certificate authenticity Decrypts the pre-master secret that the client encrypts with the server's public RSA keyClient â†’ Server: ClientHello
Server â†’ Client: ServerHello, Certificate (RSA public key)

Client: Generates random pre-master secret
Client: Encrypts pre-master secret with server's RSA public key
Client â†’ Server: Encrypted pre-master secret

Server: Decrypts with private RSA key    â† Needs Key Encipherment!
Both: Derive session keys from pre-master secret
Why RSA needs both flags: The RSA key is used for both signing AND encrypting during the TLS handshake.
  
  
  ECDSA with ECDHE (Modern TLS)
In modern TLS with ECDSA, the certificate is used for  Signs the ServerKeyExchange message containing ECDHE parameters NOT needed - ECDSA keys can only sign, not encryptClient â†’ Server: ClientHello
Server â†’ Client: ServerHello, Certificate (ECDSA public key)

Server: Generates ephemeral ECDHE key pair
Server: Signs ECDHE parameters with ECDSA private key
Server â†’ Client: Signed ECDHE parameters

Client: Verifies signature    â† Only needs Digital Signature!
Client: Generates own ECDHE key pair
Client â†’ Server: Client's ECDHE public key

Both: Compute shared secret via ECDHE (no encryption!)
Both: Derive session keys from shared secret
Why ECDSA only needs Digital Signature:ECDSA keys can only sign, not encryptKey exchange uses ECDHE (Ephemeral Diffie-Hellman)The pre-master secret is derived via DH key agreement, not encryptedProvides Perfect Forward Secrecy (PFS)
  
  
  âŒ Banned Flags for TLS Server Certificates
These flags should  appear in TLS server certificates:Reserved for CA certificatesEnd-entity cert could sign other certificates! (DigiNotar attack)Could issue fake revocation lists (Non-Repudiation)For legally-binding signaturesUnusual for TLS, adds legal liability If a TLS server certificate has , it can create valid-looking certificates for ANY domain â†’ Complete PKI compromise!
  
  
  âœ… Valid RSA Certificate (Let's Encrypt)
Subject Public Key Info:
    Public Key Algorithm: rsaEncryption
    Public-Key: (2048 bit)

X509v3 Key Usage: critical
    Digital Signature, Key Encipherment
 PASS âœ… (RSA has both required flags)
  
  
  âœ… Valid ECDSA Certificate (Let's Encrypt)
Subject Public Key Info:
    Public Key Algorithm: id-ecPublicKey
    Public-Key: (256 bit)
    ASN1 OID: prime256v1

X509v3 Key Usage: critical
    Digital Signature
 PASS âœ… (ECDSA only needs Digital Signature)
  
  
  âŒ Invalid RSA Certificate (Missing Key Encipherment)
Subject Public Key Info:
    Public Key Algorithm: rsaEncryption

X509v3 Key Usage: critical
    Digital Signature
 FAIL âŒ (RSA certificate missing Key Encipherment) Cannot perform RSA key exchange
  
  
  âŒ Invalid Certificate (Not Critical)
X509v3 Key Usage:
    Digital Signature, Key Encipherment
 FAIL âŒ (Extension not marked as critical)
  
  
  âŒ Invalid Certificate (Banned Flag)
X509v3 Key Usage: critical
    Digital Signature, Key Encipherment, Certificate Sign
 FAIL âŒ (Has Certificate Sign - security catastrophe!) This certificate can forge other certificates!
  
  
  ðŸŽ¯ Common Validation Failures
Test 055: RSA Missing Key EnciphermentAlgorithm: rsaEncryption
X509v3 Key Usage: critical
    Digital Signature    â† Missing Key Encipherment!
 RSA certificate must have both Digital Signature AND Key EnciphermentTest 056: Has Data Encipherment (Banned)X509v3 Key Usage: critical
    Digital Signature, Key Encipherment, Data Encipherment
                                         ^^^ Banned flag!
 Data Encipherment is not appropriate for TLS server certificatesTest 025: Not Marked CriticalX509v3 Key Usage:    â† Missing "critical"!
    Digital Signature, Key Encipherment
 Key Usage extension must be marked as critical
  
  
  ðŸ“– Why OPTIONAL per Specification
CA/Browser Forum language: "If present, bit positions for keyCertSign and cRLSign MUST NOT be set" "If present" - the extension itself is technically optional 99%+ of real-world TLS certificates include Key Usage (it's universal in practice)IF Key Usage extension is present:
  1. âœ… Must be marked critical
  2. âŒ MUST NOT have Certificate Sign or CRL Sign
  3. âœ… Must have algorithm-appropriate flags:
     - RSA: Digital Signature + Key Encipherment
     - ECDSA: Digital Signature only

IF Key Usage extension is absent:
  â†’ Accept (extension is optional per CA/B Forum)

  
  
  ðŸ”’ Why Modern TLS Prefers ECDSA + ECDHE
Perfect Forward Secrecy (PFS): Even if private key stolen later, past sessions remain secure ECDSA P-256 (256-bit) â‰ˆ RSA 3072-bit security ECDSA signing/verification is faster Only needs Digital Signature flag Industry moving from RSA to ECDSA + ECDHE for these security and performance benefits. RFC 5280 Section 4.2.1.3 (Key Usage), CA/Browser Forum Baseline Requirements Section 7.1.2.3What happens if it fails:Missing required flags â†’ Cannot establish TLS connection (cipher suite mismatch)Has Certificate Sign or CRL Sign â†’ REJECT IMMEDIATELY (security catastrophe!)Not marked critical â†’ Reject (violates RFC 5280)Extension absent â†’ Accept (optional per CA/B Forum)
  
  
  âœ… CHECK 10: Extended Key Usage (EKU)
What you'll see in a good certificate:X509v3 Extended Key Usage:
    TLS Web Server Authentication
Plain English explanation:
EKU = What PURPOSE this certificate servesFor TLS server certificates, MUST include:TLS Web Server Authentication (OID: 1.3.6.1.5.5.7.3.1) âœ…CAN also include (for specific use cases):TLS Web Client Authentication (for mTLS - mutual TLS)Other purposes (as long as Server Auth is present)Why this matters - Wrong purpose attack:Purpose Used INSTEAD of Server AuthTLS Web Client Authentication (alone)E-mail Protection (alone)âœ… CORRECT - Server Auth WITH other purposes:X509v3 Extended Key Usage:
    TLS Web Server Authentication
    TLS Web Client Authentication
â†’ âœ… HAS Server Auth (Client Auth is bonus for mTLS)
âŒ WRONG - Client Auth INSTEAD OF Server Auth:X509v3 Extended Key Usage:
    TLS Web Client Authentication
â†’ âŒ MISSING Server Auth!

Test 028: Code Signing INSTEAD OF Server Auth â†’ âŒ FAIL
Test 059: Client Auth INSTEAD OF Server Auth â†’ âŒ FAIL
Test 029: Server Auth WITH Client Auth â†’ âœ… PASS
Bitwarden: Server Auth WITH Client Auth â†’ âœ… PASS CA/Browser Forum Certificate Contents - "Either the value id-kp-serverAuth or id-kp-clientAuth or both values MUST be present"What happens if it fails:
Certificate has wrong purpose â†’ Browser rejects
  
  
  ðŸ”¹ PHASE 4: REVOCATION INFRASTRUCTURE
âš ï¸ IMPORTANT UPDATE - Ballot SC63 (Effective March 15, 2024):
The CA/Browser Forum made significant changes to revocation requirements:âœ… CRL (Check 11): NOW REQUIRED (was recommended)âš ï¸ OCSP (Check 13): NOW OPTIONAL (was required)This reversal addresses privacy concerns (OCSP exposes browsing behavior), security issues (plain HTTP), and operational complexity. Short-lived certificates (â‰¤7 days) are exempt from both requirements. CA/Browser Forum Ballot SC63 - "Make OCSP optional, require CRLs, and incentivize automation"
  
  
  âœ… CHECK 11: CRL Distribution Points (Certificate Revocation List)
 âœ… REQUIRED (Changed March 15, 2024 via Ballot SC63)What you'll see in a good certificate:X509v3 CRL Distribution Points:
    URI:http://crl3.digicert.com/ca.crl
    URI:http://crl4.digicert.com/ca.crl
Plain English explanation:
CRL = Certificate Revocation List = List of canceled certificate serial numbersâœ… CRL Distribution Points extension MUST be presentâœ… At least  is requiredâš ï¸  for redundancy (but not required)Why multiple URLs are recommended (but not required)? Redundancy!If one CRL server is down, browser can try the backupBest practice: 2+ URLs for high availabilityReality: 70% have 2 URLs, 30% have 1 URL (both valid) Avoid circular dependency:If CRL URL was HTTPS:
1. Need to validate cert for crl.example.com
2. To validate, need to download CRL
3. To download CRL, need to validate cert
4. Infinite loop! ðŸ”„
Without CRL:
1. Private key stolen
2. CA revokes cert
3. Client can't check (no URL)
4. âŒ Accepts revoked cert!

With CRL:
1. Private key stolen
2. CA revokes cert (adds to CRL)
3. Client downloads CRL
4. Finds cert serial in revoked list
5. âœ… Rejects connection!
CA/Browser Forum Ballot SC63 (Adopted August 17, 2023, Effective March 15, 2024):: OCSP required, CRL recommended: CRL required, OCSP optional Privacy concerns with OCSP (reveals browsing behavior), operational complexity, and browser failures with OCSP led to this reversal
In 2023, Let's Encrypt announced plans to end OCSP support in favor of CRLs due to:Privacy: OCSP requests expose user browsing behaviorSecurity: OCSP requests sent over plain HTTP can be interceptedComplexity: OCSP requires high-availability serversBrowser behavior: Many browsers ignore OCSP failures anyway Short-lived certificates (â‰¤7 days validity) do NOT require CRL or OCSP.CA/Browser Forum Ballot SC63: "Make OCSP optional, require CRLs, and incentivize automation"CA/Browser Forum Baseline Requirements Section 7.1.2.7.1: "The cRLDistributionPoints extension MUST be present"Wikipedia: "Certificate authorities were previously required by the CA/Browser Forum to provide OCSP service, but this requirement was removed in July 2023"What happens if it fails:
âœ… REQUIRED check - certificate will be rejected
  
  
  âœ… CHECK 12: Authority Information Access (AIA)
What you'll see in a good certificate:Option 1: Both OCSP and CA Issuers (most common)Authority Information Access:
    OCSP - URI:http://ocsp.digicert.com
    CA Issuers - URI:http://cacerts.digicert.com/ca.crt
Option 2: CA Issuers only (Bitwarden, Let's Encrypt pattern)Authority Information Access:
    CA Issuers - URI:http://r12.i.lencr.org/
Plain English explanation:
AIA = Authority Information Access = Where to find more information about the certificateâœ… AIA extension MUST be presentâœ… Must contain at least one access methodWhat can be IN the AIA extension: (recommended): Where to download the issuing CA certificate (optional per SC63): Real-time revocation checking Validates that AIA extension exists Separately validates OCSP URL (which is optional) Have both OCSP + CA Issuers Have only CA Issuers (valid! - Bitwarden, Let's Encrypt) Have only OCSP (valid but not recommended)Why CA Issuers URL matters:Browser has: Server cert
Browser needs: Intermediate cert to validate chain
Problem: Where to get intermediate cert?
â†’ Connection might fail âŒ
Browser has: Server cert
Browser downloads: Intermediate from CA Issuers URL
Browser builds: Complete chain to root
â†’ Validation succeeds âœ…
 CA/Browser Forum Baseline Requirements Section 7.1.2.7.2: "With the exception of stapling, this extension MUST be present"What the extension can contain:CA Issuers (recommended for chain building)OCSP (optional per SC63 - validated separately in Check 13)What happens if it fails:
Missing AIA extension â†’ Reject (required by CA/B Forum)
  
  
  âš ï¸ CHECK 13: OCSP URL (Real-Time Revocation Checking)
 âš ï¸ OPTIONAL (Changed March 15, 2024 via Ballot SC63)What you'll see in a good certificate:OCSP - URI:http://ocsp.digicert.com
Plain English explanation:
OCSP = Online Certificate Status Protocol = Ask CA "Is this cert still valid?" = Download entire list (slow, big) = Ask about ONE cert (fast, small)CA/Browser Forum Ballot SC63 (Adopted August 17, 2023, Effective March 15, 2024):: OCSP required, CRL recommended: CRL required, OCSP optional: OCSP requests expose which sites users visit: OCSP sent over plain HTTP (can be intercepted): Many browsers ignore OCSP failures (fail-open): Requires high-availability infrastructure
Let's Encrypt announced in 2023 they're ending OCSP support in favor of CRLs due to these privacy and operational concerns.Why this matters - Real-time revocation (when present):Timeline with OCSP:

9:00 AM: Private key stolen
9:30 AM: CA revokes cert, OCSP updated
9:31 AM: Client connects
  â†’ OCSP query: "Valid?"
  â†’ OCSP: "Revoked!"
  â†’ âœ… Rejected in 1 minute!

Without OCSP:
Must wait for CRL update (hours/days)

While OCSP is now optional, most CAs still provide it for backward compatibility. However, you may encounter modern certificates without OCSP URLs - this is compliant as long as CRL is present.CA/Browser Forum Ballot SC63Wikipedia: "Certificate authorities were previously required by the CA/Browser Forum to provide OCSP service, but this requirement was removed in July 2023"smallstep.com/blog/ocsp-vs-crl-explainedWhat happens if it fails:
No longer a hard failure - certificate can be valid with CRL alone
  
  
  âœ… CHECK 14: Certificate Transparency (Public Audit Trail)
What you'll see in a good certificate:CT Precertificate SCTs:
    Signed Certificate Timestamp (Log 1)
    Signed Certificate Timestamp (Log 2)
Plain English explanation:
CT = Public log of ALL certificates issuedSCT = Signed Certificate Timestamp = Proof cert was logged Redundancy and independenceWhy this matters - Prevents secret certificates:DigiNotar hack (2011):
1. Hackers compromise DigiNotar CA
2. Issue FAKE google.com certificate
3. Use it for espionage (no one knows!)
4. Months pass before discovery
5. âŒ Massive damage done

With Certificate Transparency:
1. CA issues certificate
2. MUST log to public CT log
3. Google monitors logs
4. Sees unauthorized google.com cert
5. âœ… Revokes within hours!
Certificate with 0 SCTs: âŒ Reject (Chrome, Safari)
Certificate with 1 SCT: âŒ Reject (not redundant)
Certificate with 2+ SCTs: âœ… Accept
 Required since April 2018 Required (Apple platforms enforce for all TLS) Required since version 135 (February 2025) Follows Chrome policySigned Certificate Timestamp:
    Version: v1 (0x0)
    Log ID: A4:B9:09:90... (CT log identifier)
    Timestamp: Jan 15 2026 10:23:45 GMT
    Signature: (CA's signature proving it was logged)
RFC 6962: Certificate TransparencyMozilla Firefox 135+ requirement (February 2025)What happens if it fails:
Browser shows: "NET::ERR_CERTIFICATE_TRANSPARENCY_REQUIRED"
  
  
  ðŸ”¹ PHASE 5: CHAIN VALIDATION

  
  
  âœ… CHECK 15: Not Self-Signed
What you'll see in a good certificate:Issuer:  C=US, O=DigiCert Inc, CN=DigiCert TLS RSA SHA256 2020 CA1
Subject: C=US, ST=California, O=Example Inc, CN=www.example.com
â†’ Issuer â‰  Subject âœ…
Plain English explanation:
Self-signed certificate = Issuer and Subject are the SAMEâ­ CRITICAL: CHECK 15 DOES NOT USE WILDCARD MATCHING â­This check performs , NOT pattern matching!The  character (if present in a DN) is treated as a , not a wildcard!For publicly-trusted certificates:Issuer MUST be a trusted CAIssuer MUST NOT equal SubjectSelf-signed certificates are only valid for:Root CA certificates (in browser trust stores)Self-signed cert for www.example.com:
Issuer:  CN=www.example.com  â† Claims to sign itself!
Subject: CN=www.example.com

Anyone can create this!
â†’ âŒ No trust anchor!
â†’ âŒ Not publicly trusted!
âš ï¸ NO WILDCARD MATCHING - Distinguished Names are Exact Identifiers!Unlike CHECK 7 (which uses wildcard matching for hostnames), CHECK 15 compares DNs for .Example - These are NOT considered self-signed:Issuer:  CN = *.example.com, O = Example CA
Subject: CN = www.example.com, O = Example Corp
â†’ NOT self-signed âœ… (CNs differ: "*.example.com" â‰  "www.example.com")
The  is just a literal character in the CN field, NOT a wildcard pattern!Example - These ARE self-signed:Issuer:  CN = *.example.com, O = Example Inc
Subject: CN = *.example.com, O = Example Inc
â†’ Self-signed âŒ (exact DN match including the "*")
Why NO wildcard matching for CHECK 15:Distinguished Names are , not patterns identifies a specific CA, it's not a patternWildcard matching would create false positives (different entities matching) matches Only:  = âš ï¸ CRITICAL IMPLEMENTATION WARNING: Field Order Matters!DO NOT use simple string comparison! The Issuer and Subject fields can contain the same values in  but still represent the same Distinguished Name.Example of a self-signed certificate with different field order:Issuer:  C=US, O=Example Corp, OU=Engineering, CN=Test CA
Subject: CN=Test CA, OU=Engineering, O=Example Corp, C=US
 Figure out how to compare DNs correctly regardless of field order. Think about how to normalize or parse the DN components before comparing.Test cases to verify your implementation:# Test 1: Obvious self-signed (same order)
Issuer:  CN=Test CA, O=Example
Subject: CN=Test CA, O=Example
Expected: self-signed âœ…

# Test 2: Self-signed (different order) - THE TRICKY ONE!
Issuer:  C=US, O=Example, CN=Test CA
Subject: CN=Test CA, O=Example, C=US
Expected: self-signed âœ… (must handle this!)

# Test 3: Not self-signed (different CN)
Issuer:  C=US, O=DigiCert, CN=DigiCert CA
Subject: C=US, O=Example, CN=www.example.com
Expected: not self-signed âœ…

# Test 4: Not self-signed (different order AND different values)
Issuer:  CN=CA Root, O=TrustCorp
Subject: O=Example Inc, CN=www.example.com
Expected: not self-signed âœ…

# Test 5: Wildcard in DN - NOT self-signed (no wildcard matching!)
Issuer:  CN=*.example.com, O=Example CA
Subject: CN=www.example.com, O=Example CA
Expected: not self-signed âœ… ("*.example.com" â‰  "www.example.com" - exact comparison!)

# Test 6: Wildcard in DN - IS self-signed (exact match with wildcard)
Issuer:  CN=*.example.com, O=Example CA
Subject: CN=*.example.com, O=Example CA
Expected: self-signed âŒ (exact DN match including the "*")
Why this matters in practice:Different certificate authorities and tools format DNs differently: Often uses  order Often uses  order (reverse!) May omit optional fields Custom ordering conventionsYour validator must handle all of these correctly! CA/Browser Forum Baseline Requirements - "CAs MUST NOT issue Subscriber Certificates directly from Root CAs"What happens if it fails:
Browser shows: "NET::ERR_CERT_AUTHORITY_INVALID"
  
  
  âœ… CHECK 16: Valid Serial Number
What you'll see in a good certificate:Serial Number:
    0a:f7:e7:ca:cf:45:d8:a9:72:ab:47:c5:f8:49:11:da
Plain English explanation:
Every certificate MUST have a unique serial number with sufficient randomness.CA/Browser Forum requirements:âœ… At least 64 bits of entropy (8 bytes of randomness)âŒ  (0x01, 0x02, 0x03...)âŒ  (timestamp-based)Why this matters - Serial number attacks:EJBCA CA software had a bugGenerated serial numbers with only 63 bits entropy (not 64!)Required revoking over Affected Actalis: 230,000 active certificates EJBCA generated serial numbers with only 63 bits of entropy instead of 64 bits because it incorrectly handled negative values in signed integers.Real-world examples from test suite:Test 040 - Sequential serial (FAIL):Serial Number: 1234 (0x4d2)
â†’ âŒ Only 11 bits! Predictable!
Test 069 - Weak entropy (FAIL):Serial Number: 4660 (0x1234)
â†’ âŒ Only 16 bits! Weak PRNG!
Serial Number:
    0a:f7:e7:ca:cf:45:d8:a9:72:ab:47:c5:f8:49:11:da
â†’ âœ… 128 bits! Excellent entropy!
What happens if it fails:
Weak serial number â†’ Potential collision â†’ CA may need to revoke certificate
  
  
  âš ï¸ CHECK 17: Subject Key Identifier (SKI) Present
 âš ï¸ RECOMMENDED (Not required for end-entity certificates)What you'll see in a good certificate:X509v3 Subject Key Identifier:
    B7:3E:8E:1A:93:0E:2B:86:93:6A:BC:23:5C:55:01:F4:23:6C:45:87
Plain English explanation:
SKI = Hash of the public key = Unique identifier for this certificate's keyâœ… REQUIRED for CA certificatesâš ï¸ RECOMMENDED (not required) for end-entity certificatesWhy recommended but not required:RFC 5280 Section 4.2.1.2:
"For CA certificates, subject key identifiers SHOULD be derived..."
[No MUST requirement for subscriber/end-entity certificates]
99%+ of modern certificates include SKILet's Encrypt community discussion (July 2024) asked if they could remove itThis proves it's technically optionalWhy it's useful (when present):Building certificate chain:
1. Find cert with SKI matching parent's AKI
2. Continue until reaching root
â†’ Faster chain building
SKI = SHA-1 hash of the public key
â†’ Uniquely identifies this key
OpenSSL GitHub Issue #13603: "SKID with the exception of non-CA certs"Let's Encrypt community discussion (July 2024)What happens if it fails:
If absent â†’ May still accept (recommended but not required)
If present â†’ Must be properly formatted
  
  
  âœ… CHECK 18: Authority Key Identifier (AKI) Present
What you'll see in a good certificate:X509v3 Authority Key Identifier:
    13:92:C7:15:88:71:4D:F8:F4:32:45:E6:67:8B:A2:1C:65:43:B1:2D
Plain English explanation:
AKI = Hash of the ISSUING CA's public key = Links child cert to parent certSection 4.2.1.1:
"The keyIdentifier field of the authorityKeyIdentifier extension 
MUST be included in all certificates generated by conforming CAs"
 Self-signed certificates MAY omit AKI (since AKI would equal SKI)Why this matters - Chain building:Certificate chain validation:
1. Server cert has AKI: 13:92:C7:...
2. Find intermediate cert with SKI: 13:92:C7:...
3. Match! This is the parent âœ…
4. Repeat until reaching root
1. Server cert (no AKI)
2. Which intermediate signed it?
3. Try all intermediates? Slow!
4. Might fail to build chain âŒ
Server cert AKI: 13:92:C7:15:88:71...
   â†“ MUST MATCH
Intermediate SKI: 13:92:C7:15:88:71...
 RFC 5280 Section 4.2.1.1 - "MUST be included in all certificates generated by conforming CAs to facilitate certification path construction"What happens if it fails:
Missing AKI â†’ Cannot build certificate chain â†’ Reject
  
  
  âš ï¸ CHECK 19: SKI â‰  AKI (Not Self-Signed Detector)
 âš ï¸ CONDITIONAL (Only applies if both SKI and AKI are present)What you'll see in a good certificate:X509v3 Subject Key Identifier:
    B7:3E:8E:1A:93:0E:2B:86:93:6A:BC:23:5C:55:01:F4:23:6C:45:87

X509v3 Authority Key Identifier:
    13:92:C7:15:88:71:4D:F8:F4:32:45:E6:67:8B:A2:1C:65:43:B1:2D

â†’ B7:3E... â‰  13:92... âœ… Different! Not self-signed!
Plain English explanation:
If SKI (this cert's key) equals AKI (issuer's key), then certificate is self-signed.For publicly-trusted certificates:SKI and AKI MUST be differentSKI = AKI means self-signed (not allowed)
Since Check 17 (SKI) is RECOMMENDED (not required), this check only applies when BOTH SKI and AKI are present.Self-signed certificate:
  SKI: 13:92:C7:15:88:71...
  AKI: 13:92:C7:15:88:71...
  â†’ Same! Self-signed! âŒ

Valid certificate:
  SKI: B7:3E:8E:1A:93:0E...
  AKI: 13:92:C7:15:88:71...
  â†’ Different! Has parent CA âœ…
 If both SKI and AKI are present, compare them. If they're equal, it's self-signed (FAIL). If different, it has a separate issuer (PASS). If either is missing, rely on CHECK 15. RFC 5280 logic for self-signed certificatesWhat happens if it fails:
If SKI == AKI â†’ Self-signed certificate â†’ Reject (covered by Check 15)
  
  
  âœ… CHECK 20: Certificate Validity Period â‰¤ 398 Days
What you'll see in a good certificate:Validity
    Not Before: Dec  1 00:00:00 2025 GMT
    Not After : Feb 28 23:59:59 2026 GMT
â†’ Duration: 89 days âœ… (under 398 days)
Plain English explanation:
Certificates issued after September 1, 2020 CANNOT be valid for more than 398 days (~13 months).Pre-2015: Up to 5 years allowed2015-2018: Max 39 months (825 days)2018-2020: Max 27 months (825 days)
Sept 2020-Present: Max 398 days (CA/B Forum Ballot SC-081)Longer validity = More risk:

5-year certificate (2015):
- Year 1: RSA-2048 is "safe"
- Year 3: New attacks discovered
- Year 5: RSA-2048 possibly broken
- Problem: Cert still valid! âŒ

90-day certificate (2025):
- Renewed every 90 days
- Can upgrade to stronger algorithms
- Can fix validation errors quickly
- Limited exposure window âœ…
Today to March 15, 2026: Max 398 days
March 15, 2026: Max 200 days
March 15, 2027: Max 100 days
March 15, 2029: Max 47 days
 Parse the Not Before and Not After dates, calculate the difference in days, and check if it exceeds 398 days.CA/Browser Forum Ballot SC22 (effective September 1, 2020)CA/Browser Forum Ballot SC-081 (approved April 2025, reducing to 47 days by 2029)What happens if it fails:
Certificate validity too long â†’ Browsers reject as non-compliant
  
  
  ðŸŽ¯ Putting It All Together: Annotated Certificate Example
Here's a real certificate with ALL 20 checks annotated:Certificate:
    Data:
        Version: 3 (0x2)
            âœ… CHECK 1: Version 3 (supports extensions)

        Serial Number:
            0a:f7:e7:ca:cf:45:d8:a9:72:ab:47:c5:f8:49:11:da
            âœ… CHECK 16: Valid serial number (128 bits entropy, unique)

        Signature Algorithm: sha256WithRSAEncryption
            âœ… CHECK 3: SHA-256 signature (strong, not MD5/SHA-1)

        Issuer: C=US, O=DigiCert Inc, CN=DigiCert TLS RSA SHA256 2020 CA1

        Validity
            Not Before: Dec  1 00:00:00 2025 GMT
            Not After : Feb 28 23:59:59 2026 GMT
            âœ… CHECK 2: Not expired, not yet valid (89 days validity)
            âœ… CHECK 20: Validity â‰¤ 398 days (89 days âœ…)

        Subject: C=US, ST=California, L=San Francisco, O=GitHub Inc, CN=github.com
            âš ï¸ CHECK 5: Subject DN present (MINIMAL OK - can be minimal)
            âœ… CHECK 15: Not self-signed (Issuer â‰  Subject)
                         âš ï¸ IMPORTANT: Must parse and compare fields, not just string compare!
                         Fields can be in different order but represent same DN.

        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus: 00:b1:23:...
                Exponent: 65537 (0x10001)
            âœ… CHECK 4: Strong key (RSA-2048, exponent is prime)

        X509v3 extensions:
            X509v3 Subject Alternative Name:  
            âœ… CHECK 6: SANs extension present
            âœ… CHECK 7: Hostname validation (against SANs)
                DNS:github.com
                DNS:www.github.com
                DNS:*.github.com
                DNS:*.github.io
                â”‚
                â””â”€â”€> Multiple hostnames covered
                     Wildcards allowed for subdomains
                     Browser matches requested hostname against this list

            X509v3 Extended Key Usage:  
            âœ… CHECK 10: Extended Key Usage present with SERVER_AUTH
                TLS Web Server Authentication
                â”‚
                â””â”€â”€> Purpose: TLS server (not code signing, not email)
                     id-kp-serverAuth OID: 1.3.6.1.5.5.7.3.1

            X509v3 CRL Distribution Points:  
            âœ… CHECK 11: CRL Distribution Points present (REQUIRED since SC63 Mar 2024)
                URI:http://crl3.digicert.com/DigiCertTLSRSASHA2562020CA1-4.crl
                URI:http://crl4.digicert.com/DigiCertTLSRSASHA2562020CA1-4.crl
                â”‚
                â””â”€â”€> Two CRL endpoints for redundancy
                     Clients can check if certificate was revoked
                     HTTP URIs (not HTTPS to avoid circular dependency)
                     NOW REQUIRED (was recommended) per Ballot SC63

            Authority Information Access:  
            âœ… CHECK 12: AIA present
            âš ï¸ CHECK 13: OCSP URL present (OPTIONAL since SC63 Mar 2024)
                OCSP - URI:http://ocsp.digicert.com
                CA Issuers - URI:http://cacerts.digicert.com/DigiCertTLSRSASHA2562020CA1-1.crt
                â”‚
                â””â”€â”€> OCSP = real-time revocation checking (optional, privacy concerns)
                     CA Issuers = where to download intermediate cert
                     Both are HTTP (not HTTPS) to avoid circular dependency
                     NOW OPTIONAL (was required) per Ballot SC63

            X509v3 Basic Constraints: critical  
            âœ… CHECK 8: Basic Constraints CA:FALSE (critical)
                CA:FALSE
                â”‚
                â””â”€â”€> This is a server cert, NOT a CA
                     Cannot sign other certificates
                     MUST be marked critical

            X509v3 Key Usage: critical  
            âš ï¸ CHECK 9: Key Usage present (OPTIONAL but universal)
                Digital Signature, Key Encipherment
                â”‚
                â””â”€â”€> Digital Signature: For ECDHE key exchange
                     Key Encipherment: For RSA key exchange
                     MUST be marked critical (if present)
                     Present in 99%+ of certificates despite being optional

            X509v3 Subject Key Identifier:  
            âš ï¸ CHECK 17: SKI present (RECOMMENDED, not required for end-entity)
                B7:3E:8E:1A:93:0E:2B:86:93:6A:BC:23:5C:55:01:F4:23:6C:45:87
                â”‚
                â””â”€â”€> SHA-1 hash of this certificate's public key
                     Used for fast chain building
                     RECOMMENDED but not REQUIRED per RFC 5280

            X509v3 Authority Key Identifier:  
            âœ… CHECK 18: AKI present (REQUIRED)
            âš ï¸ CHECK 19: SKI â‰  AKI check (CONDITIONAL - both present)
                13:92:C7:15:88:71:4D:F8:F4:32:45:E6:67:8B:A2:1C:65:43:B1:2D
                â”‚
                â””â”€â”€> SHA-1 hash of issuing CA's public key
                     Links to parent certificate
                     B7:3E... â‰  13:92... â†’ Not self-signed âœ…

            CT Precertificate SCTs:  
            âœ… CHECK 14: Certificate Transparency (2 SCTs)
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : A4:B9:09:90:B4:16:6B:3E...
                    Timestamp : Jan 15 2026 10:23:45.123 GMT

                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : EE:4B:BD:B7:75:CE:60:BA...
                    Timestamp : Jan 15 2026 10:23:46.789 GMT
                â”‚
                â””â”€â”€> 2 independent CT logs (redundancy)
                     Proves certificate was publicly logged
                     Required by Chrome (2018+), Safari, Firefox 135+ (2025)

    Signature Algorithm: sha256WithRSAEncryption
         a3:f4:2b:17:6d:09:...

âœ… VALIDATION SUMMARY:
âœ… CHECK 1:  Version 3 - PASS
âœ… CHECK 2:  Not expired/not yet valid - PASS
âœ… CHECK 3:  SHA-256 signature - PASS
âœ… CHECK 4:  RSA-2048 strong key - PASS
âš ï¸ CHECK 5:  Subject DN present (minimal ok) - PASS
âœ… CHECK 6:  SANs present - PASS
âœ… CHECK 7:  Hostname matches - PASS (would need actual hostname)
âœ… CHECK 8:  Basic Constraints: CA:FALSE (critical) - PASS
âš ï¸ CHECK 9:  Key Usage: Digital Signature, Key Encipherment (critical, optional) - PASS
âœ… CHECK 10: Extended Key Usage: TLS Web Server Authentication - PASS
âœ… CHECK 11: CRL Distribution Points present (2 URLs - REQUIRED extension, 1+ URLs needed) - PASS
âœ… CHECK 12: Authority Information Access present - PASS
âš ï¸ CHECK 13: OCSP URL: http://ocsp.digicert.com (OPTIONAL since SC63) - PASS
âœ… CHECK 14: Certificate Transparency (2 SCTs) - PASS
âœ… CHECK 15: Not self-signed (Issuer â‰  Subject) - PASS
âœ… CHECK 16: Valid serial number (128 bits entropy) - PASS
âš ï¸ CHECK 17: SKI present (RECOMMENDED, not required for end-entity) - PASS
âœ… CHECK 18: AKI present - PASS
âš ï¸ CHECK 19: SKI â‰  AKI (CONDITIONAL, both present) - PASS
âœ… CHECK 20: Validity 89 days â‰¤ 398 days - PASS

**Score: 20/20** âœ…  
**Result: VALID CERTIFICATE - All checks passed!**

  
  
  ðŸ’» THE CHALLENGE: Build the Validator!
Your validator receives certificates in  (OpenSSL text output):openssl x509  certificate.pem  certificate.txt
Easier to parse (human-readable)No ASN.1 parsing requiredFocus on validation logic, not parsing complexityBuild  that: a certificate.txt file all 20 validation rules a clear pass/fail report=== TLS Certificate Validator ===
File: test_001_valid_cert.txt

PHASE 1: FUNDAMENTAL VALIDITY
âœ… CHECK 1:  Version 3
âœ… CHECK 2:  Not expired (valid until 2026-02-28)
âœ… CHECK 3:  SHA-256 signature
âœ… CHECK 4:  RSA 2048-bit key

PHASE 2: IDENTITY VALIDATION
âœ… CHECK 5:  Subject DN present
âœ… CHECK 6:  SANs present (3 names)
âœ… CHECK 7:  Hostname validated

PHASE 3: ACCESS CONTROL
âœ… CHECK 8:  Basic Constraints: CA:FALSE (critical)
âš ï¸ CHECK 9:  Key Usage: Digital Signature, Key Encipherment (optional)
âœ… CHECK 10: Extended Key Usage: TLS Web Server Authentication

PHASE 4: REVOCATION
âœ… CHECK 11: CRL Distribution Points (2 URLs - 1+ required)
âœ… CHECK 12: Authority Info Access present
âš ï¸ CHECK 13: OCSP URL present (optional)
âœ… CHECK 14: Certificate Transparency (2 SCTs)

PHASE 5: CHAIN VALIDATION
âœ… CHECK 15: Not self-signed
âœ… CHECK 16: Valid serial number (128-bit)
âš ï¸ CHECK 17: SKI present (recommended)
âœ… CHECK 18: AKI present
âš ï¸ CHECK 19: SKI â‰  AKI (conditional, passed)

PHASE 6: OPERATIONAL
âœ… CHECK 20: Validity period: 60 days â‰¤ 398 days

=====================================
RESULT: VALID âœ…
Score: 20/20 checks passed
This certificate meets all requirements for public trust.
 Your solution must implement the validate_tls_certificate() function that returns (fail_list, optional_list).Function must be named validate_tls_certificate(cert_file, hostname)Must return tuple: (fail_list, optional_list)Both lists contain check numbers (1-20) that failedOptional checks: 9 (Key Usage), 13 (OCSP), 17 (SKI)Your solution will be graded automatically by comparing your returned  and  against the reference implementation.python3 grader.py your_solution.py
 Both  and  match â†’ 100 points One list matches â†’ 50 points
 Neither list matches â†’ 0 points Average score across all 68 test certificates======================================================================
TLS CERTIFICATE VALIDATOR GRADER
======================================================================

Loading reference: tls_cert_validator.py
âœ“ Reference loaded
Loading student:   my_validator.py
âœ“ Student loaded

Found 68 test certificates
======================================================================

Test                                       Status     Required   Optional
----------------------------------------------------------------------
test_001_perfect_cert.txt                  PERFECT    âœ“          âœ“
test_002_ecdsa_cert.txt                    PERFECT    âœ“          âœ“
test_006_expired.txt                       PERFECT    âœ“          âœ“
test_013_hostname_mismatch.txt             PARTIAL    âœ“          âœ—
test_019_wildcard_root.txt                 FAIL       âœ—          âœ—
...

======================================================================
SUMMARY
======================================================================

Total Tests:      67
Perfect Matches:  60 (89.6%)
Partial Matches:  5 (7.5%)
Failed:           2 (3.0%)

SCORE: 92.5/100
GRADE: A
 90-100 (90%+ perfect matches)âš ï¸ Try implementing yourself first! The learning comes from struggling through the implementation.68 test certificates covering:âœ… 23 valid certificates (all checks pass)âŒ 45 invalid certificates (specific failures)Each test file shows which check should fail:test_001_valid_complete.txt - Perfect certificate - Fails Check 2 (expired) - Fails Check 10 (Code Signing instead of Server Auth)test_051_sha1_signature.txt - Fails Check 3 (SHA-1)test_069_weak_entropy_serial.txt - Fails Check 16 (only 16-bit serial)Use regular expressions to extract fields from the certificate textLook for field names followed by colons and valuesExtensions are prefixed with "X509v3"Pay attention to spacing and formatting variationsBy completing this challenge, you'll master:X.509 Certificate StructureVersion fields and extensionsDistinguished Names (DNs) vs Subject Alternative Names (SANs)Public key types and sizesCertificate Validation LogicCA/Browser Forum Baseline RequirementsBrowser-specific policiesHash algorithm security (SHA-1 vs SHA-256)Key sizes and strength (RSA vs ECDSA)Digital signatures and chain of trustCertificate Transparency and public logsRevocation (CRLs vs OCSP)Recent industry changes (Ballot SC63 - March 2024)Production Best PracticesComprehensive validation (don't trust partially)Test-driven development (68 test cases!)EJBCA Entropy Issue (2019)Why serial number entropy mattersSymantec Distrust (2017-2018)Google Chrome distrusted 30,000+ certificatesIf you found this exercise valuable, please star the repo!ðŸ“š Get notified of new security exercisesðŸŽ¯ Show appreciation for free, high-quality AppSec contentðŸš€ Help others discover these resourcesðŸ’ª Support open-source security educationMore applied cryptography exercisesWeb application security challengesAPI security validator exercises
  
  
  Phase 1: Core Implementation
Parse certificate text formatTest against valid certificates
  
  
  Phase 2: Comprehensive Testing
Test against all 68 test certificatesAchieve 100% test pass rateAdd detailed error messagesAdd hostname matching logic (wildcard support)Implement strict vs lenient modes
  
  
  Phase 4: Production Features
Support PEM format input (parse using OpenSSL)Chain validation (multiple certificates)Online validation (download CRLs/OCSP)Your validator is complete when:âœ… Function validate_tls_certificate(cert_file, hostname) implementedâœ… Returns tuple (fail_list, optional_list) with check numbers 1-20âœ… All 20 checks implemented correctlyâœ… Achieves 90%+ match rate with reference solutionâœ… Proper error handling (missing files, malformed inputs)Code quality: Clean, well-documented, RFC 5280 citationsPerformance: Process 1000 certificates/secondPerfect match: 100% accuracy with reference solutionAutomated grading based on array matching:90%+ perfect matches with referencePerfect match (both lists match): 100 pointsPartial match (one list matches): 50 pointsMismatch (neither list matches): 0 points Average across all 68 test certificates
python3 grader.py my_validator.py


  
  
  ðŸŽ“ Why This Exercise Matters for AppSec Careers
This exercise directly applies to Security Engineering roles at companies like:Companies that care about certificate validation:Trail of Bits - Security consulting, tool developmentNCC Group - Pentesting and secure code reviewAnthropic - AI safety, production systemsGitLab - DevSecOps platformStripe - Payment processing (PCI compliance)Coinbase - Cryptocurrency exchange - Reading and implementing standards - Understanding hash functions, key sizes - Real-world attack prevention - Comprehensive test coverage - Clear technical writingInterview topics this covers:"Explain how TLS certificates work""How would you validate a certificate?""What's the difference between CRL and OCSP?""Why did CA/Browser Forum make OCSP optional?""What's Certificate Transparency and why does it matter?"Certificate validation is one of those "get it 100% right or users get hacked" problems. By building a production-grade validator, you'll:âœ… Understand the security properties browsers rely onâœ… Learn from real-world security incidentsâœ… Gain hands-on cryptography experienceâœ… Build a portfolio project that impresses security teams You'll never look at that little padlock icon in your browser the same way again! ðŸ”Good luck, and happy validating! ðŸš€If this exercise helped you, please â­ star the GitHub repo!ðŸ“£ Helps others discover high-quality AppSec exercisesðŸ’¡ Motivates creation of more security contentðŸŽ¯ Shows recruiters you're serious about AppSecðŸ†“ Supports free, open-source security educationGet the complete exercise: January 25, 2026 RFC 5280 (2008) + CA/Browser Forum Baseline Requirements v2.0.4+ (including Ballot SC63)]]></content:encoded></item><item><title>PyCoderâ€™s Weekly: Issue #719: Django Tasks, Dictionaries, Ollama, and More (Jan. 27, 2026)</title><link>https://pycoders.com/issues/719</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 19:30:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[ Explore the key differences between NaN, null, and None in numerical data handling using Python. While all signal â€œno meaningful value,â€ they behave differently. Learn about the difference and how to correctly handle the data using Pydantic models and JSON serialization. A comprehensive look at WebAssembly in 2025 and 2026, covering browser support, Safari updates, WebAssembly 3.0, WASI, .NET, Kotlin, debugging improvements, and growing adoption across edge computing and embedded devices. There are many misconceptions on asyncio, as such there are many misleading benchmarks out there. This article looks at how to analyse a benchmark result and to come up with more relevant conclusions. Strings and other sequences can be multiplied by numbers to self-concatenate them. You need to be careful with mutable sequences though.[ Subscribe to ðŸ PyCoderâ€™s Weekly ðŸ’Œ â€“ Get the best Python news, articles, and tutorials delivered to your inbox once a week >> Click here to learn more ]]]></content:encoded></item><item><title>The Ultimate Guide to Buying Old Yahoo accounts | Articles</title><link>https://dev.to/marysamckinneytgmxl/the-ultimate-guide-to-buying-old-yahoo-accounts-articles-2nk0</link><author>marysamckin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 18:39:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In todayâ€™s digital landscape, having a reliable email account is more crucial than ever. Yahoo accounts are not just about sending and receiving emails; they serve as gateways to various online services and platforms. Whether youâ€™re promoting a business, managing social media profiles, or simply keeping in touch with friends and family, owning multipleÂ YahooÂ accounts can significantly enhance your online experience. If youâ€™ve been searching for the best place to buy these valuable assets, look no further than getusasmm.com. Letâ€™s explore why investing in Yahoo accounts could be your next smart move!Buy Yahoo Accounts
What are Yahoo Accounts?
Yahoo accounts are email services provided by Yahoo Inc., one of the earliest internet companies. These accounts offer users a versatile platform for communication, including features like email, calendars, and contacts.
With a Yahoo account, you gain access to various tools beyond just emailing. Users can utilizeÂ YahooÂ Mailâ€™s powerful spam filters and ample storage capacity. This makes it ideal for both personal and professional use.
Additionally, having a Yahoo account opens doors to other Yahoo services such as news, finance updates, and entertainment content. It serves as an all-in-one hub for staying connected in todayâ€™s fast-paced digital world.
As technology evolves, so do the functionalities of these accounts. They adapt to user needs while maintaining their core features that have made them popular over time. Whether youâ€™re new or returning to the platform, creating or managing multiple accounts offers flexibility tailored to your lifestyle.
Buy Yahoo Accounts Online for Sale
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ We are available online 24/7
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Telegram: @getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ WhatsApp: +1 (314) 203-4162@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Come now our company:https://getusasmm.com/product/buy-old-yahoo-accounts/
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²Buying Yahoo accounts online has become increasingly popular among marketers and individuals looking to manage their digital presence more effectively. With a simple click, you can access numerous accounts tailored to your needs.These accounts come pre-verified and ready for use, saving you the hassle of creating them from scratch. This convenience is particularly appealing for those who need multiple accounts for various purposes.Many platforms offer these services at competitive prices. However, itâ€™s crucial to choose a reliable provider that guarantees account quality and security.
When purchasing, ensure the seller provides detailed information about each accountâ€™s features. This transparency helps you make informed decisions based on your requirements.
With so many options available today, finding the right source can elevate your online marketing strategies significantly.
Benefits of Owning Multiple Yahoo Accounts
Owning multiple Yahoo accounts offers a range of advantages for both personal and business use. For individuals, it allows you to separate your professional communications from casual conversations. This organization helps maintain focus in different areas of life.Businesses can benefit significantly as well. Having various accounts enables targeted marketing strategies. You can tailor messages to specific audiences without cluttering your main inbox.
Moreover, multiple accounts enhance security. If one account gets compromised, the rest remain secure, minimizing risks associated with data breaches.
Another perk is accessibility across different platforms or devices. You can manage distinct projects or interests effortlessly without mixing up information.
In addition, having backup email options ensures that important communications are never missed due to technical issues with a single account. The flexibility provided by owning several Yahoo accounts cannot be underestimated when managing diverse tasks effectively.
Better Marketing Strategies
Owning multiple Yahoo accounts opens up new avenues for marketing strategies. Each account can target different demographics or niche markets, allowing tailored campaigns that resonate more effectively with specific audiences.
Segmentation is key in todayâ€™s digital landscape. With various accounts, businesses can create focused email lists and personalized content. This increases engagement rates significantly.
Additionally, using separate accounts helps track the performance of various campaigns effortlessly. You can experiment with different approaches without risking the integrity of your primary brand identity.
Moreover, multiple Yahoo accounts allow for diversified outreach methods. You can engage customers through newsletters, promotions, and surveys while maintaining a distinct voice across each channel.
This strategic approach not only enhances visibility but also builds trust among consumers who appreciate relevant communication based on their interests and preferences.
How to Buy Yahoo Accounts SafelyWhen considering the purchase of Yahoo accounts, safety should be your priority. Start by researching reputable sellers. Look for platforms with positive reviews and a track record of satisfied customers.
Verify that the seller offers secure payment methods. This protects your financial information while ensuring you receive what you paid for.
Always check if the accounts come with recovery options. A reliable provider will offer you access to account recovery settings in case you run into issues later on.
Be cautious about sharing personal information during the process. Legitimate services wonâ€™t ask for sensitive data beyond whatâ€™s necessary to complete the sale.
Consider using VPNs or secure browsers when making transactions online. This adds an extra layer of security to your activities, safeguarding both your identity and account details from potential threats.
How to Find an Old Yahoo Account?
Tracking down an old Yahoo account can feel like searching for a needle in a haystack. Start by trying to remember the email address or usernames you may have used. Think about variations of your name, nicknames, or even numbers added.
Next, visit the Yahoo Sign-in Helper page. Enter any potential email addresses you recall and follow the prompts. If youâ€™ve forgotten your password, this tool can help reset it.
If that doesnâ€™t work, check any linked accounts or social media profiles where you might have shared that Yahoo address. Sometimes hints pop up in unexpected places.
If all else fails and youâ€™re still struggling to recover your account, consider reaching out to Yahoo support directly. They may request information to verify your identity before assisting with recovery efforts.
Where are the Best Places to Buy Yahoo Accounts?
When searching for the best places to buy Yahoo accounts, itâ€™s essential to focus on reputable sources. Online marketplaces often offer a range of options, but caution is necessary.
Specialized websites like getusasmm.com stand out. They provide verified accounts, ensuring you receive quality alongside quantity. Customer reviews and testimonials can guide your choices as well.
Forums and communities dedicated to digital marketing may also have leads on trustworthy sellers. Engaging with experienced users can uncover hidden gems in the marketplace.
Avoid sites that promise unusually low prices; they might compromise account security or authenticity. Always prioritize safety and reliability over cost when purchasing Yahoo accounts online.
Donâ€™t forget about social media channels where trusted vendors promote their services directlyâ€”these platforms often allow you to interact before making any commitments.
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ We are available online 24/7@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ WhatsApp: +1 (314) 203-4162@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Come now our company:https://getusasmm.com/product/buy-old-yahoo-accounts/
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²FAQs About Buying Yahoo Accounts
When considering purchasing Yahoo accounts, many questions arise. One common query is about the safety of these transactions. Itâ€™s crucial to buy from reputable sellers who prioritize security.
Another frequent concern involves account recovery options. Buyers often wonder if they can regain access easily should they lose their passwords or have issues logging in.
People also ask whether multiple accounts are permissible under Yahooâ€™s terms. Generally, creating several accounts is allowed for personal or business use, but itâ€™s essential to understand the platformâ€™s guidelines.
Additionally, potential buyers may inquire about the quality of purchased accounts. Ensuring that each account has a good standing and no prior violations is vital for smooth usage.
Individuals frequently want to know how quickly they can start using their new accounts after purchase. Most services provide immediate access upon completion of payment.
OwningÂ YahooÂ accounts can significantly enhance your online presence. Whether for personal use or business marketing, the right accounts make a difference.
Exploring options like getusasmm.com opens up various opportunities. This site provides reliable services that cater to different needs and preferences.
Navigating the digital landscape requires strategic tools. Multiple Yahoo accounts offer flexibility in managing communications and marketing campaigns effectively.
Safety is paramount when purchasing online. Opting for trusted providers ensures peace of mind while expanding your account portfolio.
Having access to several Yahoo accounts equips individuals and businesses with competitive advantages in todayâ€™s fast-paced digital world.]]></content:encoded></item><item><title>The 10 Sites Guide to Buy Old Yahoo Accounts in 2026 ...</title><link>https://dev.to/marysamckinneytgmxl/the-10-sites-guide-to-buy-old-yahoo-accounts-in-2026--524</link><author>marysamckin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 18:35:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In todayâ€™s digital landscape, having a reliable email account is more crucial than ever. Yahoo accounts are not just about sending and receiving emails; they serve as gateways to various online services and platforms. Whether youâ€™re promoting a business, managing social media profiles, or simply keeping in touch with friends and family, owning multipleÂ YahooÂ accounts can significantly enhance your online experience. If youâ€™ve been searching for the best place to buy these valuable assets, look no further than getusasmm.com. Letâ€™s explore why investing in Yahoo accounts could be your next smart move!Buy Yahoo Accounts
What are Yahoo Accounts?
Yahoo accounts are email services provided by Yahoo Inc., one of the earliest internet companies. These accounts offer users a versatile platform for communication, including features like email, calendars, and contacts.
With a Yahoo account, you gain access to various tools beyond just emailing. Users can utilizeÂ YahooÂ Mailâ€™s powerful spam filters and ample storage capacity. This makes it ideal for both personal and professional use.
Additionally, having a Yahoo account opens doors to other Yahoo services such as news, finance updates, and entertainment content. It serves as an all-in-one hub for staying connected in todayâ€™s fast-paced digital world.
As technology evolves, so do the functionalities of these accounts. They adapt to user needs while maintaining their core features that have made them popular over time. Whether youâ€™re new or returning to the platform, creating or managing multiple accounts offers flexibility tailored to your lifestyle.
Buy Yahoo Accounts Online for Sale
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ We are available online 24/7
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Telegram: @getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ WhatsApp: +1 (314) 203-4162@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Come now our company:https://getusasmm.com/product/buy-old-yahoo-accounts/
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²Buying Yahoo accounts online has become increasingly popular among marketers and individuals looking to manage their digital presence more effectively. With a simple click, you can access numerous accounts tailored to your needs.These accounts come pre-verified and ready for use, saving you the hassle of creating them from scratch. This convenience is particularly appealing for those who need multiple accounts for various purposes.Many platforms offer these services at competitive prices. However, itâ€™s crucial to choose a reliable provider that guarantees account quality and security.
When purchasing, ensure the seller provides detailed information about each accountâ€™s features. This transparency helps you make informed decisions based on your requirements.
With so many options available today, finding the right source can elevate your online marketing strategies significantly.
Benefits of Owning Multiple Yahoo Accounts
Owning multiple Yahoo accounts offers a range of advantages for both personal and business use. For individuals, it allows you to separate your professional communications from casual conversations. This organization helps maintain focus in different areas of life.Businesses can benefit significantly as well. Having various accounts enables targeted marketing strategies. You can tailor messages to specific audiences without cluttering your main inbox.
Moreover, multiple accounts enhance security. If one account gets compromised, the rest remain secure, minimizing risks associated with data breaches.
Another perk is accessibility across different platforms or devices. You can manage distinct projects or interests effortlessly without mixing up information.
In addition, having backup email options ensures that important communications are never missed due to technical issues with a single account. The flexibility provided by owning several Yahoo accounts cannot be underestimated when managing diverse tasks effectively.
Better Marketing Strategies
Owning multiple Yahoo accounts opens up new avenues for marketing strategies. Each account can target different demographics or niche markets, allowing tailored campaigns that resonate more effectively with specific audiences.
Segmentation is key in todayâ€™s digital landscape. With various accounts, businesses can create focused email lists and personalized content. This increases engagement rates significantly.
Additionally, using separate accounts helps track the performance of various campaigns effortlessly. You can experiment with different approaches without risking the integrity of your primary brand identity.
Moreover, multiple Yahoo accounts allow for diversified outreach methods. You can engage customers through newsletters, promotions, and surveys while maintaining a distinct voice across each channel.
This strategic approach not only enhances visibility but also builds trust among consumers who appreciate relevant communication based on their interests and preferences.
How to Buy Yahoo Accounts SafelyWhen considering the purchase of Yahoo accounts, safety should be your priority. Start by researching reputable sellers. Look for platforms with positive reviews and a track record of satisfied customers.
Verify that the seller offers secure payment methods. This protects your financial information while ensuring you receive what you paid for.
Always check if the accounts come with recovery options. A reliable provider will offer you access to account recovery settings in case you run into issues later on.
Be cautious about sharing personal information during the process. Legitimate services wonâ€™t ask for sensitive data beyond whatâ€™s necessary to complete the sale.
Consider using VPNs or secure browsers when making transactions online. This adds an extra layer of security to your activities, safeguarding both your identity and account details from potential threats.
How to Find an Old Yahoo Account?
Tracking down an old Yahoo account can feel like searching for a needle in a haystack. Start by trying to remember the email address or usernames you may have used. Think about variations of your name, nicknames, or even numbers added.
Next, visit the Yahoo Sign-in Helper page. Enter any potential email addresses you recall and follow the prompts. If youâ€™ve forgotten your password, this tool can help reset it.
If that doesnâ€™t work, check any linked accounts or social media profiles where you might have shared that Yahoo address. Sometimes hints pop up in unexpected places.
If all else fails and youâ€™re still struggling to recover your account, consider reaching out to Yahoo support directly. They may request information to verify your identity before assisting with recovery efforts.
Where are the Best Places to Buy Yahoo Accounts?
When searching for the best places to buy Yahoo accounts, itâ€™s essential to focus on reputable sources. Online marketplaces often offer a range of options, but caution is necessary.
Specialized websites like getusasmm.com stand out. They provide verified accounts, ensuring you receive quality alongside quantity. Customer reviews and testimonials can guide your choices as well.
Forums and communities dedicated to digital marketing may also have leads on trustworthy sellers. Engaging with experienced users can uncover hidden gems in the marketplace.
Avoid sites that promise unusually low prices; they might compromise account security or authenticity. Always prioritize safety and reliability over cost when purchasing Yahoo accounts online.
Donâ€™t forget about social media channels where trusted vendors promote their services directlyâ€”these platforms often allow you to interact before making any commitments.
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ We are available online 24/7@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ WhatsApp: +1 (314) 203-4162@getusasmm
ðŸ’«ðŸ’ŽðŸ“²âœ¨ðŸŒ Come now our company:https://getusasmm.com/product/buy-old-yahoo-accounts/
ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ›’ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²ðŸ’²FAQs About Buying Yahoo Accounts
When considering purchasing Yahoo accounts, many questions arise. One common query is about the safety of these transactions. Itâ€™s crucial to buy from reputable sellers who prioritize security.
Another frequent concern involves account recovery options. Buyers often wonder if they can regain access easily should they lose their passwords or have issues logging in.
People also ask whether multiple accounts are permissible under Yahooâ€™s terms. Generally, creating several accounts is allowed for personal or business use, but itâ€™s essential to understand the platformâ€™s guidelines.
Additionally, potential buyers may inquire about the quality of purchased accounts. Ensuring that each account has a good standing and no prior violations is vital for smooth usage.
Individuals frequently want to know how quickly they can start using their new accounts after purchase. Most services provide immediate access upon completion of payment.
OwningÂ YahooÂ accounts can significantly enhance your online presence. Whether for personal use or business marketing, the right accounts make a difference.
Exploring options like getusasmm.com opens up various opportunities. This site provides reliable services that cater to different needs and preferences.
Navigating the digital landscape requires strategic tools. Multiple Yahoo accounts offer flexibility in managing communications and marketing campaigns effectively.
Safety is paramount when purchasing online. Opting for trusted providers ensures peace of mind while expanding your account portfolio.
Having access to several Yahoo accounts equips individuals and businesses with competitive advantages in todayâ€™s fast-paced digital world.]]></content:encoded></item><item><title>Data Engineering - Custom Data Pipelines - Complete Tutorial</title><link>https://dev.to/hkj13/data-engineering-custom-data-pipelines-complete-tutorial-51fp</link><author>Hemanath Kumar J</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 18:01:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, we will dive deep into the world of data engineering by focusing on the creation of custom data pipelines. This guide is aimed at intermediate developers looking to expand their data engineering skills. We'll cover the essentials of building robust, efficient data pipelines using Python, exploring various techniques for data extraction, transformation, and loading (ETL).Data pipelines are crucial components in the data engineering ecosystem, enabling the automated movement and transformation of data from various sources to destinations for analysis and storage. Crafting custom data pipelines allows for tailored data processing that fits specific project requirements.Basic understanding of Python programming.Familiarity with SQL and database concepts.Knowledge of data structures and basic algorithms.
  
  
  Step 1: Setting Up Your Environment
First, ensure your Python environment is ready. Using virtual environments is recommended for project-specific dependencies.Extracting data is the first phase in the ETL process. Here, we'll use Python's  library to fetch data from an API.Once data is extracted, transforming it to fit our needs is next. This might involve cleaning, aggregating, or reshaping data.The final step in the pipeline is loading the transformed data into a destination, like a database.: Build your pipeline in smaller, reusable components.Error handling and logging: Implement comprehensive error handling and logging to catch and resolve issues promptly.: Use batch processing and proper data structures to enhance performance.Building custom data pipelines is a valuable skill for any data engineer. This tutorial has introduced the basics of setting up a data pipeline, from extraction to loading. By following best practices and continuously learning, you can create efficient, reliable pipelines for any data-intensive project.]]></content:encoded></item><item><title>From Words to Meaning: Core NLP Concepts Every Beginner Must Know</title><link>https://dev.to/zeroshotanu/from-words-to-meaning-core-nlp-concepts-every-beginner-must-know-3hl3</link><author>Ananya S</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 17:48:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the previous post, we covered the basics of NLP such as tokenization, stemming, lemmatization, and stop words.In this continuation, we will understand how machines extract meaning from text and represent language numerically.1. Named Entity Recognition (NER)Named Entity Recognition (NER) is an NLP technique used to identify and classify real-world entities in text.Elon Musk is the CEO of Tesla and lives in the USA.

Helps extract structured information from unstructured textUsed in resume parsing and document processingWidely applied in medical and legal NLP systemsImproves search engines and chatbotsBag of Words is one of the simplest techniques to convert text into numbers.Only word frequency mattersSentence 1: I love NLP  
Sentence 2: I love AI
Sentence 1 â†’ [1, 1, 1, 0]Sentence 2 â†’ [1, 1, 0, 1]Works well for small datasetsUseful as a baseline modelNo understanding of contextTreats all words as equally important3. TF-IDF (Term Frequency â€“ Inverse Document Frequency)TF-IDF improves Bag of Words by assigning importance scores to words.TF(t,d)= Total number of terms in document d/Number of times term t appears in document d

IDF(t)=log( Number of documents containing term t/Total number of documents)
Words that occur frequently in a document are important.Words that occur in many documents are less important.: frequency of a word in a documentInverse Document Frequency (IDF): rarity of the word across documentsWhy TF-IDF is better than BoW:Reduces importance of common words like the and isHighlights meaningful wordsDocument similarity tasksDoes not capture semantic meaningSynonyms are treated as different wordsWord2Vec represents words as dense numerical vectors that capture meaning and context.Words used in similar contexts have similar meanings. The vectors used to represent King, Queen, Man and Woman when undergo arithmetic operations, give results as below.King âˆ’ Man + Woman â‰ˆ QueenParis âˆ’ France + Italy â‰ˆ RomeWord2Vec has 2 components:1. CBOW (Continuous Bag of Words)Predicts a word using surrounding context.Sentence: "Raj went to school yesterday"
Window size: 1

Input: [Raj, to] â†’ Output: went
Input: [went, school] â†’ Output: to
Input: [to, yesterday] â†’ Output: school

The context words are converted to one-hot vectors.These vectors are summed or averagedThey are passed through the hidden layer.The model predicts the target wordWeights are updated using backpropagationPredicts surrounding words using a target word.
 For same sentence,Target word: went
Context words: Raj, to

Training pairs:
Input: went â†’ Output: Raj
Input: went â†’ Output: to

Target = to
Context words: went, school

Training pairs:
Input: to â†’ Output: went
Input: to â†’ Output: school

Target = school
Context words: to, yesterday

Training pairs:
Input: school â†’ Output: to
Input: school â†’ Output: yesterday

The target word is converted to a one-hot vectorPassed through the hidden layerThe model predicts each context wordWeights are updated using backpropagationðŸ‘‰ The hidden layer weights become the word embeddingsCaptures semantic relationshipsProduces dense and meaningful embeddingsUseful for clustering and similarity tasksSame word has the same vector in all contextsExample: 
bank (river) and bank (money)This limitation is addressed by contextual models like BERT.When to Use Each Technique?Building simple text classifiersCreating baseline NLP modelsWorking on search systemsPerforming document similaritySemantic similarity is importantBuilding recommendation systemsThese techniques show the evolution of NLP from counting words to weighting word importance to understanding semantic meaning.
They form the foundation for modern NLP and Generative AI systems.]]></content:encoded></item><item><title>Build a QR Code Generator with Python and KivyMD</title><link>https://dev.to/prashaant/build-a-qr-code-generator-with-python-and-kivymd-1457</link><author>Professor à¤²à¤²à¤¾</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 17:40:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As of today, over eighty-five (85) percent of websites + applications utilize QR codes to quickly share links, contact information, location address, restaurant menus, and other quick event details. Today we will create a QR Code Generator from scratch with Python!We will use KivyMD a library that makes python app looks like a modern Android or iOS app using Material Design.QR codes are Quick response code are one of the easiest and fastest way to share any information  with other people, we see Qr codes in most of around us  we see in restaurants, railway tickets, doing payments.It is standard python apps can look a bit â€œold schoolâ€. KivyMD gives us beautiful text fields, buttons, and layouts with very little code.Before we start, we need to install two main tools: for user interface. to create the actual QR patterns.
  
  
  Step 1: Install the Requirements
Run this code in terminal or command promptpip install kivymd qrcode pillow
  
  
  Step 2: Import the Libraries
Create a new file named QRGeneratorApp.py and start by importing the necessary tools.from kivymd.app import MDApp
from kivy.lang import Builder
from kivy.core.image import Image as CoreImage
import qrcodefrom io 
import BytesIOimport os

## Step 3: Design the UI (The KV String)We use the KV Language to design the app. This is a declarative way to build the interface. We need a text field for input, two buttons (Generate and Download), and an image area to show the result.KV = '''
MDScreen:
    md_bg_color: app.theme_cls.bg_light

    MDBoxLayout:
        orientation: 'vertical'
        spacing: "20dp"
        padding: "20dp"
        adaptive_height: True
        pos_hint: {"center_x": .5, "center_y": .5}

        MDLabel:
            text: "QR Code Generator"
            halign: "center"
            font_style: "H4"
            bold: True
            theme_text_color: "Primary"

        MDTextField:
            id: user_input
            hint_text: "Enter URL or Text"
            mode: "outlined"
            size_hint_x: .8
            pos_hint: {"center_x": .5}

        MDBoxLayout:
            orientation: 'horizontal'
            spacing: "10dp"
            adaptive_size: True
            pos_hint: {"center_x": .5}

            MDRaisedButton:
                text: "GENERATE"
                on_release: app.generate_qr(user_input.text)

            MDRaisedButton:
                id: download_btn
                text: "DOWNLOAD"
                md_bg_color: .2, .7, .2, 1 
                disabled: True 
                on_release: app.download_qr()

        Image:
            id: qr_display
            size_hint: None, None
            size: "250dp", "250dp"
            pos_hint: {"center_x": .5}
'''


Step 4: Create the App LogicTo reuse the button clicks, we'll have to make a class in Python that will allow the stoner to perform certain tasks through buttons and will interact with the illustration operation we are erecting, which utilizes the Kivy Framework to give a means for druggies to produce QR canons from their entries. Once a stoner clicks the Generate button, Kivy will take the textbook that was written in the Text Input contrivance and produce QR law( s). Kivy will also give the druggies with QR canons via the illustration operation's GUI.The button labeled Download will not change color until at least one QR code has been generated through the click of the Generate button. Once QR code(s) have been created by clicking the Generate button one or more times the Download button will allow users to save the QR code generated(s) in PNG format to their current working folder.class QRGeneratorApp(MDApp):
    current_qr_img = None  # To store the image in memory

    def build(self):
        self.theme_cls.primary_palette = "DeepPurple"
        return Builder.load_string(KV)

    def generate_qr(self, data):
        if not data.strip():
            return

        # 1. Create the QR object
        qr = qrcode.QRCode(version=1, box_size=10, border=2)
        qr.add_data(data)
        qr.make(fit=True)

        # 2. Store as PIL image
        self.current_qr_img = qr.make_image(fill_color="black", back_color="white")

        # 3. Convert to Kivy Texture for the UI
        buffer = BytesIO()
        self.current_qr_img.save(buffer, format='PNG')
        buffer.seek(0)
        core_img = CoreImage(buffer, ext='png')
        self.root.ids.qr_display.texture = core_img.texture

        # 4. Enable Download Button
        self.root.ids.download_btn.disabled = False

    def download_qr(self):
        if self.current_qr_img:
            file_name = "my_qr_code.png"
            self.current_qr_img.save(file_name)
            print(f"Saved successfully as {file_name}")

if __name__ == "__main__":
    QRGeneratorApp().run()



Just type in a web address like https// google.com, hit Generate, and your QR law pops up! also, click Download to save it and use it still you want. Easy peasy!
Here is the full script. Save this as QRGeneratorApp.py and run it!from kivymd.app import MDApp
from kivy.lang import Builder
from kivy.core.image import Image as CoreImage
import qrcode
from io import BytesIO
import os

KV = '''
MDScreen:
    md_bg_color: app.theme_cls.bg_light

    MDBoxLayout:
        orientation: 'vertical'
        spacing: "20dp"
        padding: "20dp"
        adaptive_height: True
        pos_hint: {"center_x": .5, "center_y": .5}

        MDLabel:
            text: "QR Code Generator"
            halign: "center"
            font_style: "H4"
            bold: True
            theme_text_color: "Primary"

        MDTextField:
            id: user_input
            hint_text: "Enter URL or Text"
            mode: "outlined"
            size_hint_x: .8
            pos_hint: {"center_x": .5}

        MDBoxLayout:
            orientation: 'horizontal'
            spacing: "10dp"
            adaptive_size: True
            pos_hint: {"center_x": .5}

            MDRaisedButton:
                text: "GENERATE"
                on_release: app.generate_qr(user_input.text)

            MDRaisedButton:
                id: download_btn
                text: "DOWNLOAD"
                md_bg_color: .2, .7, .2, 1  # Green color
                disabled: True  # Disabled until a QR is made
                on_release: app.download_qr()

        Image:
            id: qr_display
            size_hint: None, None
            size: "250dp", "250dp"
            pos_hint: {"center_x": .5}
'''

class QRGeneratorApp(MDApp):
    # Variable to store the image object in memory
    current_qr_img = None

    def build(self):
        self.theme_cls.primary_palette = "DeepPurple"
        return Builder.load_string(KV)

    def generate_qr(self, data):
        if not data.strip():
            return

        qr = qrcode.QRCode(version=1, box_size=10, border=2)
        qr.add_data(data)
        qr.make(fit=True)

        # Store the PIL image object so we can save it later
        self.current_qr_img = qr.make_image(fill_color="black", back_color="white")

        # Convert to texture for display
        buffer = BytesIO()
        self.current_qr_img.save(buffer, format='PNG')
        buffer.seek(0)

        core_img = CoreImage(buffer, ext='png')
        self.root.ids.qr_display.texture = core_img.texture

        # Enable the download button
        self.root.ids.download_btn.disabled = False

    def download_qr(self):
        if self.current_qr_img:
            # Save the file as 'my_qr_code.png' in the same folder as the script
            file_name = "my_qr_code.png"
            self.current_qr_img.save(file_name)
            print(f"Saved successfully as {file_name}")

if __name__ == "__main__":
    QRGeneratorApp().run()
Stupendous job! You've made across-platform app that works. Just type in your favorite website and click  generate.]]></content:encoded></item><item><title>Youtube Channels,,, for Sale | Buy &amp; Sell Youtube Channel ..</title><link>https://dev.to/fiann_haley_e14e2da09d1bb/youtube-channels-for-sale-buy-sell-youtube-channel--3k2i</link><author>PVAFARMUSA</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 16:37:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Buy YouTube Channel
Buy YouTube Channel and accelerate your digital footprint with a pre-established YouTube channel fromPvafarmusa. Why spend months or years shouting into the void when you can inherit a platform that already has a voice?Buy Youtube Channel
Buy Youtube Channel
Starting from zero is the biggest hurdle for most creators. By choosing a pre-built channel, you skip the â€œghost townâ€ phase and jump straight into the spotlight. Imagine the thrill of uploading your first video and seeing views and comments roll in immediately because an audience is already waiting. Our mission is to provide you with the foundation so you can focus on the creative work that truly matters.If you face any problem you can contact us.
we are online 24/7 hours
WhatsApp: +1 (252) 593-9728
Telegram: @Pvafarmusapvafarmusa@gmail.comBenefits Of Buying A Youtube Channel
Have you ever wondered why some influencers seem to â€œpop upâ€ out of nowhere with massive followings? Often, the secret is acquiring a pre-existing channel. This shortcut transforms your digital journey from a slow crawl into a high-speed race. Letâ€™s look at the specific advantages:Instant Audience 
The hardest part of YouTube is getting those first 1,000 subscribers. When you buy a channel, that community is already there. You start with a â€œwarmâ€ audience rather than a cold start.Enhanced Credibility 
Social proof is everything. A channel with thousands of subscribers looks like an authority. New viewers are far more likely to click â€œSubscribeâ€ if they see that others have already done so.Monetization Opportunities 
Getting approved for the YouTube Partner Program requires 4,000 watch hours and 1,000 subscribers. Purchasing an established channel can put you on the fast track to earning ad revenue from day one.Algorithm Advantage 
The YouTube algorithm likes history. An older, active channel is often treated better by the recommendation engine than a brand-new account with no data.Content Testing 
With an existing audience, you get immediate data. You can A/B test different video styles and get feedback in hours instead of weeks, allowing you to pivot your strategy with precision.Networking and Collaborations 
Big creators rarely collaborate with â€œzero-subâ€ channels. An established subscriber count acts as your resume, opening doors to partnerships and sponsorships that would otherwise be closed.Time and Effort Savings 
Time is your most valuable asset. By skipping the grind of the first 12 months of channel growth, you can dedicate your energy to high-quality production and business scaling.Is Buying a YouTube Channel Legal?
Legally speaking, there are no laws prohibiting the sale of a social media account. However, it is important to be transparent: this practice often goes against YouTubeâ€™s specific Terms of Service regarding account transfers. If the platform detects a suspicious change in ownership, there is a risk of the channel being flagged.Buy Youtube Channel
Buy Youtube Channel
AtPvafarmusa, we mitigate these risks by providing high-quality, aged, and phone-verified channels. We focus on â€œsafeâ€ transfers to ensure stability. While we provide the best tools possible, we encourage all buyers to operate their new channels with care and stay informed on platform policies.Introduction To Us
We have built a reputation as a premier destination for digital assets. Our platform is designed to bridge the gap between ambitious creators and established YouTube real estate. We donâ€™t just sell channels; we provide the keys to a ready-made digital brand. Our process is transparent, secure, and built on years of experience in the PVA (Phone Verified Account) industry.What Makes Us Unique?
Our inventory isnâ€™t â€œone size fits all.â€ We offer a diverse catalog of channels across various niches, ages, and subscriber counts. Whether you need a small starter channel or a massive platform for a corporate brand, we have a solution that fits your specific goals.Commitment To Quality And Security
We treat your investment with the highest level of priority. Every channel undergoes a rigorous check to ensure it is healthy and secure. We use delivery methods that prioritize account safety, giving you peace of mind that your new asset is protected.User-friendly Experience
You shouldnâ€™t need a degree in IT to buy a YouTube channel. Our website is streamlined and intuitive. From browsing our list to the final hand-off of login credentials, the process is designed to be as smooth as a standard online purchase.Customer Support Excellence
We believe the relationship startsafterthe sale. Our support team is available to guide you through the transition, helping you secure the account and answer any technical questions you might have.Why Choose Pvafarmusa
Choosing a provider is about trust. We offer a blend of security, quality, and competitive pricing that is hard to find elsewhere.Proven Track Record
With thousands of successful transactions, our history speaks for itself. We are a staple in the digital marketing community.High-quality Channels
We donâ€™t deal in â€œbotâ€ accounts. We focus on channels with real history and verified status to ensure long-term viability.Competitive Pricing
We offer the best â€œprice-per-subscriberâ€ ratio in the market. We ensure you get a high ROI on your purchase.Excellent Customer Support
Our staff is trained to help you with the specific nuances of YouTube account transfers.Secure Transactions
We use encrypted payment gateways to ensure your financial data is never compromised.User-friendly Platform
A clean interface means you can find, compare, and buy your channel in minutes.Satisfaction Guarantee
We stand behind our products. If the channel doesnâ€™t meet the described criteria, we work tirelessly to make it right.How Purchased Channels Fuel Growth
Buying a channel isnâ€™t just about the numbers; itâ€™s about the momentum. It acts as a catalyst for your entire brand.Visibility And Exposure
An established channel has â€œweightâ€ in search results. Your new uploads are more likely to appear in the â€œSuggested Videosâ€ section because the channel already has a relationship with the algorithm.Instant Audience Base
You are stepping onto a stage that already has an audience in the seats. This immediate feedback loop is vital for staying motivated as a creator.Increased Interaction
A channel with a history of likes and comments encourages new viewers to join the conversation. Engagement breeds more engagement.Enhanced Brand Image
A â€œVerifiedâ€ look or a high subscriber count tells the world you are a serious professional. This makes it much easier to attract high-paying sponsors.Evaluating Channel Quality
Before you buy, itâ€™s important to know what makes a channel â€œgood.â€ At Pvafarmusa, we encourage our clients to look at these metrics:Channel Age
Older accounts are generally more â€œtrustedâ€ by YouTube and are less likely to face sudden shadowbans.Engagement Rate
A channel with 10,000 subscribers is great, but a channel where those subscribers actually comment and like is even better.Subscriber Authenticity
We focus on organic-looking growth patterns to ensure the channel remains healthy in the eyes of the platform.Niche Relevance
Try to buy a channel that matches your intended content. This ensures the existing audience stays interested in what you post.Content Quality
We look at the history of the channel to ensure it hasnâ€™t been used for spam, which protects your future reputation.Channel Reputation
We verify that the channels are free of active strikes or copyright warnings.Security Measures And Safety
Your safety is our priority. We implement several layers of protection to ensure a smooth transfer.Encryption And Secure Transactions
We use SSL encryption and secure merchant processors so your data is always shielded.Verified Channels
Every account is 100% phone verified (PVA), which is the gold standard for account security and recovery.Customer Support And Guidance
We provide a step-by-step guide on how to safely change the recovery email and password to ensure the channel is 100% yours.Regular Security Updates
We constantly update our methods to stay in line with the latest security protocols from Google and YouTube.Legal Considerations
While it is a standard business practice to buy and sell digital assets, you should always be aware of the â€œrules of the road.â€Legal Ownership And Rights
We ensure that the â€œchain of custodyâ€ for the channel is clear, giving you full control over the asset once the transaction is complete.Platform Terms And Conditions
While buying a channel is a shortcut, itâ€™s important to play by YouTubeâ€™s community guidelines moving forward to keep the account in good standing.Protection Against Fraud
By using an established provider like us, you avoid the â€œwild westâ€ of private forums where scams are common.Tax Implications
Depending on your country, a YouTube channel can be considered a business asset. We recommend keeping records of your purchase for your accounting.Privacy And Data Security
We ensure that all previous owner data is wiped and that you have a â€œclean slateâ€ to start your brand.Contractual Obligations
Our terms of service act as a clear agreement, protecting both the buyer and the seller during the hand-over.Customizing Your New Channel
Once you have the keys, itâ€™s time to make it yours. Customization is where the transformation happens.Creating A Captivating Channel Name
Choose a name that is easy to spell and fits your brandâ€™s personality.Designing An Eye-catching Channel Banner
Use a high-resolution banner to tell viewers exactly what your channel is about at a glance.Crafting A Compelling Channel Description
Use your â€œAboutâ€ section to include keywords that help you show up in Google and YouTube searches.Selecting The Perfect Profile Picture
A high-quality logo or a friendly headshot helps viewers build a personal connection with you.Organizing Your Content With Playlists
Playlists keep viewers on your channel longer, which the algorithm loves.Personalizing Video Thumbnails
Consistency is key. Develop a â€œstyleâ€ for your thumbnails so fans recognize your videos instantly.Engaging Your Audience With Community Posts
Use the Community Tab to run polls and talk to your subscribers between video uploads.Setting Up Custom Url
A clean URL (youtube.com/c/YourBrand) is essential for marketing and business cards.Optimizing Channel Keywords
Hidden tags in your channel settings help YouTube categorize your content and show it to the right people.Strategies For Instant Engagement
Buying the channel is the foundation; engagement is the house you build on top of it.Craft Catchy Thumbnails
The thumbnail is your â€œhook.â€ If they donâ€™t click, they donâ€™t watch.Engage With Comments
The more you talk to your fans, the more loyal they become. Spend 30 minutes a day replying to your audience.Optimize Video Titles And Descriptions
Think like a search engine. What would a user type to find your video?Leverage Playlists
Encourage â€œbinge-watchingâ€ by grouping your best videos together.Host Live Sessions
Live streaming builds a level of trust and â€œrealnessâ€ that pre-recorded videos canâ€™t match.Managing Audience Expectations
When you take over a channel, the audience might be surprised. Communication is the best way to keep them.Understanding Your Audience
Look at the â€œAnalyticsâ€ tab to see the age, gender, and location of your viewers.Communicating Effectively
Make an â€œIntroductionâ€ video. Tell the audience who you are and what the new vision for the channel is.Delivering Consistent Content
If you promise a video every Tuesday, deliver a video every Tuesday. Reliability builds trust.Incorporating Audience Feedback
Ask your viewers what they want to see. When they feel heard, they stay.Adjusting Based On Analytics
If a certain type of video is getting more â€œWatch Time,â€ make more of that content!Monetization Opportunities
A YouTube channel is a business. Here are the ways your new purchase can pay for itself:Ad Revenue
Once you are in the Partner Program, YouTube pays you a share of the revenue from every ad shown.Sponsorship Deals
Companies will pay you to mention their products. This is often more lucrative than ad revenue.Merchandise Sales
Sell t-shirts, mugs, or digital products directly to your loyal fanbase.Memberships And Donations
Fans can pay a monthly fee for â€œexclusiveâ€ badges or shoutouts during your videos.Case Studies Of Success
Real people are using this strategy every day to change their lives.Johnâ€™s Journey From Zero To Hero
John bought a tech channel and pivoted it to AI reviews. Because the channel already had â€œauthority,â€ his first video got 50,000 views.Susanâ€™s Strategy: Turning A Passion Into Profit
Susan bought a cooking channel. She used the existing 5,000 subscribers to launch her own cookbook, making her investment back in one week.Mikeâ€™s Marketing Mastery
Mike used a purchased channel to drive traffic to his Shopify store, seeing a 300% increase in sales compared to paid Facebook ads.The Impact On Brand Visibility
Corporations use our channels to launch new products, ensuring they donâ€™t look like a â€œsmallâ€ company when they enter the market.Future Trends In Youtube Growth
YouTube is changing. Here is how to stay ahead:Interactive Content
The future is â€œchoose your own adventureâ€ style videos and interactive polls.Short-form Videos
YouTube Shorts are the fastest way to get millions of views right now. Every channel should have a Shorts strategy.Artificial Intelligence And Personalization
AI will help you write scripts and edit videos, but the â€œhumanâ€ connection you build with your audience is what keeps them subbed.Live Streaming
As TV dies, live streaming on YouTube is becoming the new â€œPrime Time.â€Frequently Asked Questions
What Are We? 
We are a premium digital asset provider specializing in safe, verified YouTube channels.Why Buy Youtube Channels? 
To save time, gain instant social proof, and access monetization faster.How Secure Are Channels From Us? 
Very. We use industry-standard hand-over protocols and 100% phone-verified accounts.Can I Customize The Purchased Channel?
 Absolutely. You have 100% ownership to change the name, logo, and content.Are Channels From Us Monetized? 
We offer both monetized and non-monetized options. Please check the specific listing.How Quickly Can I Start Using My Channel?
Most transfers are completed within 24 hours of payment.What Payment Methods Do We Accept? 
We accept a variety of secure methods including Crypto, Credit Cards, and more.Can I Get Support After Purchase? 
Yes! Our team is here to help you through the entire transition process.Conclusion
Investing in a YouTube channel fromPvafarmusais an investment in your future. Whether you are an entrepreneur, a creator, or a brand, starting with an established audience gives you a competitive edge that â€œstarting from scratchâ€ simply cannot match.]]></content:encoded></item><item><title>Buy Telegram Accounts with 2FA &amp; Full Access</title><link>https://dev.to/fiann_haley_e14e2da09d1bb/buy-telegram-accounts-with-2fa-full-access-26bl</link><author>PVAFARMUSA</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 16:35:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine having ready-to-use accounts that save you time and open doors to new connections. Youâ€™ll discover why Pvafarmusa.com stands out as the best place to get Telegram accounts and how this simple step can power up your messaging strategy.If you face any problem you can contact us.
we are online 24/7 hours
WhatsApp: +1 (252) 593-9728
Telegram: @Pvafarmusapvafarmusa@gmail.comKeep reading to find out how you can make your move today.
Why Choose Pvafarmusa.com
Pvafarmusa.com offers a reliable service to buy Telegram accounts. Their process is simple and clear. Customers get accounts quickly without hassle. The site ensures the safety of each account sold.
Trust is important when buying online. Pvafarmusa.com builds trust with secure payment options. Customer support is ready to help at any time. This makes the buying experience smooth and safe.
Secure And Verified Accounts
Every Telegram account from Pvafarmusa.com is verified. This reduces the risk of fraud. Buyers get real, active accounts. This ensures the accounts work well for all needs.
Orders are processed quickly. Most accounts are delivered within minutes. This speed helps users start using their accounts right away. No long waiting times.
Pvafarmusa.com offers fair prices. The cost matches the quality of the accounts. Many options fit different budgets. No hidden fees or extra charges.
Excellent Customer Support
The support team is helpful and friendly. They answer questions clearly and quickly. Customers feel confident with this support. Help is available before and after purchase.
Benefits Of Buying Telegram Accounts
Buying Telegram accounts offers several clear benefits for users and businesses. It saves time and effort needed to create new accounts from scratch. Ready-to-use accounts help start messaging or marketing quickly. They come with verified profiles, making them more trustworthy and credible.
These accounts often have established activity and contacts. This helps to build connections faster and reach audiences more easily. Buying accounts also avoids the risk of being banned for creating multiple new accounts rapidly. It provides a safer way to expand presence on Telegram.
Easy Access To Verified Accounts
Accounts from Pvafarmusa.com are verified and real. This adds trust when messaging others. Verified accounts reduce chances of being flagged as spam. It helps maintain good standing on Telegram.
Save Time And Effort
Creating accounts takes time and phone numbers. Buying accounts removes this hassle. Users can start using Telegram immediately. It is perfect for fast project launches.
Build Audience Quickly
Many accounts have existing contacts and history. This helps connect with more people fast. It supports marketing and community growth. Growing an audience becomes easier.
Reduce Risk Of Account Suspension
New accounts often get banned if created too fast. Bought accounts are older and safer. They lower the chance of suspension. Users stay active without interruptions.
Affordable And Convenient
Purchasing accounts costs less than long setup time. It is a simple way to get multiple accounts. Users save money and avoid technical problems.
Types Of Telegram Accounts Available
Telegram accounts come in different types to fit various needs. Choosing the right one is important for your goals.
Each type has unique features and benefits. Understanding these can help you pick the best option.If you face any problem you can contact us.
we are online 24/7 hours
WhatsApp: +1 (252) 593-9728
Telegram: @Pvafarmusapvafarmusa@gmail.comPersonal Telegram Accounts
These accounts are made for individual use. You can chat with friends and join groups easily. They offer full control over privacy settings. Perfect for daily personal communication.
Business Telegram Accounts
Business accounts help companies connect with customers. They support broadcasting messages to many users. Useful for promotions and customer support. Designed to handle higher message volumes.
Verified Telegram Accounts
Verified accounts show a blue checkmark badge. This proves the account is authentic. Trusted by followers and customers alike. Often used by public figures and brands.
These accounts are made in large numbers. Ideal for marketing campaigns and automation. They allow managing many contacts at once. Save time for businesses with wide reach.
The fast delivery process at Pvafarmusa.com ensures you receive your Telegram accounts quickly. Speed matters when you want to start using your new account. The team works to prepare and send your order without delay.
They understand the need for quick access. Orders are processed right after purchase. This saves your time and effort.
Instant Order Confirmation
After you buy, you get an instant confirmation. This shows your order is received. It helps you track the next steps easily.
Quick Account Setup
Accounts are ready fast. The system creates and verifies accounts right away. This means no long waiting times.
Reliable Delivery Methods
Pvafarmusa.com uses trusted delivery methods. This keeps your accounts safe during transfer. You get your account details without delay.
24/7 Support For Delivery Issues
Support is available anytime. If a problem occurs, help is ready fast. This keeps your experience smooth and stress-free.
Account Verification And Security
Account verification and security are vital when buying Telegram accounts. Users must trust the authenticity and safety of their accounts. Pvafarmusa.com ensures each account is verified and secure. This process protects buyers from scams and unauthorized access.
Verified accounts come with real phone numbers and active profiles. This verification confirms that the account is genuine and ready to use. Security measures prevent hacking and keep personal data safe. Buyers receive accounts that meet high safety standards.
How Account Verification Works
Pvafarmusa.com uses phone verification to confirm account ownership. Each account is linked to a valid phone number. The verification process checks if the account is active and legitimate. This step removes fake or inactive accounts from the listings.
Security Features Of Purchased Accounts
Accounts come with strong passwords and two-step verification options. These features add extra layers of protection. Buyers can change passwords immediately after purchase. The platform avoids sharing accounts with suspicious activity.
Benefits Of Buying Verified Accounts
Verified accounts reduce risks related to fraud or bans. Buyers save time by avoiding lengthy setup processes. Secure accounts offer peace of mind during use. Pvafarmusa.com supports users with clear information about each account's status.
Pricing and packages at Pvafarmusa.com offer clear options for buying Telegram accounts. Each package fits different needs and budgets.
Simple pricing helps buyers choose quickly and easily. No hidden fees or confusing terms.
Basic Package
The Basic Package includes a small number of Telegram accounts. Perfect for those starting out or testing services. Affordable price with reliable accounts.
Standard Package
The Standard Package offers more accounts than the Basic. Great for growing projects or small businesses. Balanced price and quantity for steady use.
Premium Package
The Premium Package provides the highest number of accounts. Suitable for large campaigns or advanced users. Best value for bulk buyers.
Custom Packages
Custom Packages allow buyers to request specific amounts. Flexible pricing based on needs. Ideal for unique projects or special requirements.
How To Place An Order
Placing an order on Pvafarmusa.com is simple and quick. This guide helps you buy Telegram accounts with ease. Follow these steps to complete your purchase smoothly.
Choose Your Telegram Account
First, visit the Telegram accounts section on Pvafarmusa.com. Browse through the available accounts. Pick the one that fits your needs best.
Add The Account To Your Cart
Click the "Add to Cart" button next to your chosen account. This action saves the account for checkout. You can continue shopping or proceed to buy.
Review Your Cart
Open your cart to see the selected accounts. Check the details carefully. Make sure everything is correct before moving on.
Enter Your Details
Fill in your contact and payment information. Provide accurate details to avoid delays. Pvafarmusa.com keeps your data safe and secure.
Confirm And Place Your Order
Double-check your order summary. Confirm the purchase by clicking the "Place Order" button. You will receive a confirmation message shortly after.
Receive Your Telegram Account
After payment, the account details are sent to your email. Check your inbox and spam folder. Start using your new Telegram account right away.
Customer Support Services
Customer support plays a big role when buying Telegram accounts from Pvafarmusa.com. It ensures users get help fast and clear answers. Good support makes the buying process smooth and less stressful.
The team at Pvafarmusa.com is ready to assist with any questions or problems. They offer help through different channels, making sure customers can reach them easily. This support builds trust and confidence in the service.
Pvafarmusa.com values your time. Their support team replies quickly to messages. You wonâ€™t wait long for help or answers. Fast responses keep your buying experience moving forward.
Friendly And Professional Staff
The support staff is polite and professional. They listen carefully and provide clear answers. Their goal is to solve your issues without confusion. Friendly service makes customers feel comfortable.
Support is available to guide you through account setup. They explain steps in simple terms. This helps avoid mistakes and saves time. You get your Telegram account ready without hassle.
Assistance With Payment Issues
Payment problems can happen. Pvafarmusa.comâ€™s team helps quickly to fix these issues. They explain payment methods and confirm transactions. This support ensures your payment process is safe and clear.
Availability Across Multiple Channels
Contact options include email, chat, and phone. This variety lets customers choose their preferred way to ask for help. Multiple channels mean support is always close and easy to access.If you face any problem you can contact us.
we are online 24/7 hours
WhatsApp: +1 (252) 593-9728
Telegram: @Pvafarmusapvafarmusa@gmail.comUser testimonials offer real opinions from people who purchased Telegram accounts from Pvafarmusa.com. These reviews show how customers feel about the service and product quality.
Reading honest feedback helps new buyers trust the site and decide confidently. Testimonials reveal the user experience and satisfaction levels clearly.
Many users say buying accounts on Pvafarmusa.com is quick and simple. The website layout makes navigation easy. Payment steps are clear and secure.
Customers report no delays or confusion during checkout. This smooth process saves time and avoids frustration.
Reliable Account Quality
Buyers confirm that the Telegram accounts they receive work perfectly. Each account is verified and ready for use. Users appreciate the quality and reliability.
Several reviews mention no issues with banned or inactive accounts. This trust in quality keeps customers coming back.
Responsive Customer Support
Many testimonials highlight fast and helpful customer support. Pvafarmusa.comâ€™s team answers questions and solves problems quickly. Users feel valued and supported.
Good communication adds to the positive buying experience. Customers recommend the site because of this support.
Common Use Cases For Telegram Accounts
Telegram accounts serve many purposes across different fields. They help individuals and businesses communicate easily and securely. Many users prefer Telegram because it offers privacy and fast messaging.
Buying Telegram accounts from Pvafarmusa.com provides ready-to-use profiles. These accounts support various activities that need quick access and reliable communication.
Business Communication And Customer Support
Companies use Telegram accounts to chat with customers. It allows fast responses and clear communication. Customer support teams solve issues directly through messages. This method saves time and builds trust.
Marketers create Telegram accounts to share updates and offers. They join groups and channels to reach many people. This helps increase product awareness and sales. Telegramâ€™s large user base makes marketing effective.
Social Networking And Community Building
People join Telegram to connect with friends and groups. They share ideas, photos, and news easily. Communities form around hobbies, interests, or work. Telegram accounts make joining these groups simple.
Content Sharing And Media Distribution
Telegram supports sending files, videos, and images fast. Content creators share their work with followers. This keeps audiences engaged and growing. Buying accounts helps creators reach new viewers quickly.
Telegram offers strong privacy settings for users. Many prefer it for confidential chats and secure calls. Buying accounts can help maintain anonymity online. This protects personal information from exposure.
Tips For Safe Account Usage
Using Telegram accounts safely is very important. It protects your privacy and data. Following simple steps can keep your account secure. These tips help avoid common risks and problems.
Keeping your account safe means you can chat and share without worry. Letâ€™s explore easy ways to protect your Telegram account from Pvafarmusa.com.
Use Strong Passwords And Two-step Verification
Create a strong password with letters, numbers, and symbols. Avoid simple or common passwords. Turn on two-step verification for extra security. This adds a second layer to protect your account.
Do Not Share Your Login Details
Keep your username and password private. Never give your login information to anyone. Sharing details increases the risk of hacking and account loss.
Check Active Sessions Regularly
Review all active sessions in your Telegram settings. Log out of devices you donâ€™t recognize. This helps prevent unauthorized access to your account.
Be Careful With Links And Files
Do not open unknown links or download files from strangers. These can contain viruses or phishing attempts. Always verify the source before clicking or downloading.
Update Your Telegram App Frequently
Keep your app updated to get the latest security fixes. Updates protect your account from new threats. Enable automatic updates for convenience.
Frequently Asked Questions
What Is Pvafarmusa.com For Buying Telegram Accounts?
Pvafarmusa.com is a website where you can buy Telegram accounts safely. It offers verified and ready-to-use accounts for your needs.
Why Should I Buy Telegram Accounts From Pvafarmusa.com?
You get reliable accounts quickly at Pvafarmusa.com. The site ensures accounts are secure and have real user profiles.
How Do I Buy Telegram Accounts On Pvafarmusa.com?
Simply select the account type you want and add it to your cart. Then follow the checkout steps to complete your purchase.
Are Telegram Accounts From Pvafarmusa.com Safe To Use?
Yes, accounts sold on Pvafarmusa.com go through verification. They are tested to avoid banned or fake accounts.
Can I Get Bulk Telegram Accounts From Pvafarmusa.com?
Pvafarmusa.com offers bulk purchase options for Telegram accounts. This is useful for businesses or marketers needing many accounts.
How Fast Is The Delivery Of Telegram Accounts?
Accounts are delivered within minutes after payment confirmation. You get immediate access to your new Telegram accounts.
What Payment Methods Does Pvafarmusa.com Accept?
The site supports multiple payment options including credit cards and online wallets. This makes buying easy and secure.
Can I Contact Support If I Have Issues With My Account?
Pvafarmusa.com provides customer support to help with any problems. You can reach them via email or live chat for quick help.
Buy Telegram accounts from Pvafarmusa.com for a simple and fast process. The site offers reliable accounts that suit different needs. You get secure and verified accounts without hassle. This saves time and helps you start quickly. Trust Pvafarmusa.com for good service and easy buying.
Choose the right Telegram account today and enjoy smooth communication.If you face any problem you can contact us.
we are online 24/7 hours
WhatsApp: +1 (252) 593-9728
Telegram: @Pvafarmusapvafarmusa@gmail.com]]></content:encoded></item><item><title>Build a Military-Grade SOC for $0 (Wazuh + Docker + Python)</title><link>https://dev.to/it_solutions_pro/build-a-military-grade-soc-for-0-wazuh-docker-python-3kam</link><author>IT Solutions Pro</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 16:29:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[**STOP paying $5,000/month for enterprise security tools like Splunk or Datadog just to monitor your home lab or small business server.You can build a Military-Grade Security Operations Center (SOC) entirely for free using Open Source tools.In this masterclass, Iâ€™ll show you how to deploy  (The Open Source SIEM) using Docker, and then we will write a custom  to test our defenses in real-time.
**
  
  
  ðŸ“º Watch the Full Masterclass

  
  
  ðŸ› ï¸ What We Build in This Video:
 Setting up the Wazuh Manager (The Brain) and Agents (The Eyes). Getting the stack up in under 3 minutes. Writing a Python script () to simulate a brute-force attack. Configuring a Custom XML Rule to detect the pattern and auto-ban the IP.Don't want to type everything from the video? Here is the source code for the tools we built.
  
  
  1. The Python Attack Bot ()
Use this script to simulate an attack on your own server (Do NOT use this on servers you don't own).import paramiko
import socket
import time

# CHANGE THIS to your local server IP
TARGET_IP = "192.168.1.XX" 
USER = "root"

print(f"[*] Starting Audit Tool targeting {TARGET_IP}...")

while True:
    password = input("Enter Password to Test: ")

    try:
        client = paramiko.SSHClient()
        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        # Attempt Connection
        client.connect(TARGET_IP, username=USER, password=password, timeout=3)
        print("[+] SUCCESS: Password Found!")
        client.close()
        break

    except paramiko.AuthenticationException:
        print("[-] Auth Failed: Wrong Credentials.")
    except socket.error:
        print("[!!!] CONNECTION REFUSED: Server blocked us! (Active Response Worked)")
        break
    except Exception as e:
        print(f"[!] Error: {e}")
<rule id="100003" level="10" frequency="15" timeframe="10">
  <if_matched_sid>60137</if_matched_sid>
  <description>Critical: Massive Logoff Flood Detected (Possible Brute Force)</description>
</rule>
]]></content:encoded></item><item><title>Day 29: From Logs to Insights â€“ implementing Structured Logging &amp; X-Ray in AWS Lambda</title><link>https://dev.to/ericrodriguez10/day-29-from-logs-to-insights-implementing-structured-logging-x-ray-in-aws-lambda-im8</link><author>Eric RodrÃ­guez</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 16:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When you are running a monolithic app on your laptop, debugging is easy. You just look at the console. But when your code is running in ephemeral containers in the cloud, debugging can be a nightmare.Today, I upgraded my Finance Agent with professional Observability tools.Structured Logging (JSON)I refactored my Python Lambda to log in JSON format.Before: print(f"Error: {e}") (Hard to parse)After: print(json.dumps({"level": "ERROR", "component": "Bedrock", "details": str(e)}))This small change allows me to use CloudWatch Logs Insights to run SQL-like queries on my logs, such as filtering only errors related to the AI model.Visualizing Latency with AWS X-RayI enabled "Active Tracing" in the Lambda configuration. Now, AWS automatically generates a Service Map (see cover image). I can visually see that my Plaid API call takes 200ms, while my Bedrock AI generation takes 1.5s. This visual "report" is invaluable for optimization.]]></content:encoded></item><item><title>Fetching and Storing CVE Data from NVD API using Python</title><link>https://dev.to/gug_31c7ba64d1c563490bc42/fetching-and-storing-cve-data-from-nvd-api-using-python-4dog</link><author>Gug</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 15:41:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[import requests
import time
from database import get_db_connection
import jsondef fetch_and_store_cves(total_to_fetch=100):
    # NOTE: Fetching ALL NVD data takes hours. For the test, we fetch a subset (e.g., 2000 records).
    # Remove 'total_to_fetch' limit for full production sync.conn = get_db_connection()
cursor = conn.cursor()   #Creates a "cursor" object. This is used to execute SQL commands (like INSERT) against the database connection.

start_index = 0
results_per_page = 2000  # Max allowed by NVD is usually 2000

print("Starting synchronization...")

while start_index < total_to_fetch:
    params = {
        'startIndex': start_index,
        'resultsPerPage': results_per_page
    }

    try:
        print(f"Fetching batch starting at {start_index}...")
        response = requests.get(BASE_URL, params=params, timeout=30)

        if response.status_code != 200:
            print(f"Error: API returned {response.status_code}")
            break

        data = response.json()
        vulnerabilities = data.get('vulnerabilities', [])

        if not vulnerabilities:
            break # No more data

        for item in vulnerabilities:
            cve = item['cve']
            cve_id = cve['id']
            source_id = cve.get('sourceIdentifier', 'N/A')
            published = cve.get('published', '')
            last_modified = cve.get('lastModified', '')
            status = cve.get('vulnStatus', '')

            # Extract Score (Try V3 first, then V2)
            score = None
            metrics = cve.get('metrics', {})
            if 'cvssMetricV31' in metrics:
                score = metrics['cvssMetricV31'][0]['cvssData']['baseScore']
            elif 'cvssMetricV2' in metrics:
                score = metrics['cvssMetricV2'][0]['cvssData']['baseScore']

            # Insert or Replace (Deduplication)
            cursor.execute('''
                INSERT OR REPLACE INTO cves 
                (id, sourceIdentifier, published, lastModified, vulnStatus, baseScore, details)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (cve_id, source_id, published, last_modified, status, score, json.dumps(cve)))

        conn.commit()
        start_index += results_per_page

        # Sleep to avoid rate limiting (NVD is strict)
        time.sleep(2)

    except Exception as e:
        print(f"Exception occurred: {e}")
        break

conn.close()
print("Synchronization Complete.")
if  == "":
    # Fetch 2000 records for the assessment demo
    fetch_and_store_cves(total_to_fetch=2000)]]></content:encoded></item><item><title>Analyzing the Relationship Between Wine Price, Quality, and Origin Using Data Science</title><link>https://dev.to/ougrarr/analyzing-the-relationship-between-wine-price-quality-and-origin-using-data-science-3blb</link><author>Elghalia</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 15:28:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Introduction
 Data Collection and Merging
 Data Cleaning and Preprocessing
 Exploratory Data AnalysisAverage Rating by Wine TypeTop Countries by Average RatingBest Value for Money WinesWine is more than just a beverage â€” it is a global market with diverse varieties, prices, and qualities. For millions of consumers, choosing the right wine can be overwhelming: should they pay more for a reputed brand, or are there hidden gems that offer great quality at an affordable price?In the digital age, online wine marketplaces like Vivino provide access to extensive wine data, including ratings, reviews, prices, and origins. Leveraging this data with data science techniques can help both businesses and consumers make smarter decisions.In this project, we aim to explore the wine market using a dataset of over 12,000 wines across red, white, and rose varieties. Specifically, we focus on the relationship between price and quality, identify value-for-money wines, and analyze how features like wine type, country, and vintage influence ratings.Through data cleaning, exploratory analysis, and predictive modeling, this project demonstrates how data science can transform raw wine data into actionable insights for businesses, helping them recommend wines effectively and optimize pricing strategies.To get a complete view of the wine market, we combined three separate datasets for red, white, and rose wines into a single, unified dataset. Each entry includes essential information about the wine, such as its name, country of origin, region, winery, rating, number of reviews, price, vintage year, and type, providing a rich foundation for analysis and insights.Before diving into the analysis, we carefully prepared the dataset to ensure its quality. Numeric columns like Rating, Price, and Year were correctly formatted, while categorical columns such as Type and Country were standardized. Missing or invalid values were addressed, duplicates were removed, and outliersâ€”especially in the Price column, where a few luxury wines created a skewed distributionâ€”were carefully handled to ensure a clean and reliable dataset for exploration.This tells us how wines are rated overall.The analysis shows that most wines receive relatively high ratings, with a median of 3.9, indicating overall good quality. Very few wines are poorly rated, as reflected by the minimum rating of 2.5. Overall, ratings are concentrated between 3.7 and 4.1, demonstrating a consistent level of high quality with low variability across the dataset.Since price is right-skewed, itâ€™s better to plot log scale as well.The majority of wines are affordable, with a median price of approximately $16. A small number of very expensive wines create a right-skewed distribution, stretching the upper end of the price range while most wines remain in the accessible price segment.Are expensive wines really higher rated?
Do cheap wines also have high ratings?Optional: Add correlation coefficient:Examining the relationship between price and rating revealed a moderate correlation of 0.44, suggesting that while higher-priced wines tend to have slightly higher ratings, price alone does not guarantee quality.
  
  
  Average Rating by Wine Type
By comparing red, white, and rose wines, we can identify which type tends to receive higher ratings. The analysis shows that red wines have slightly higher average ratings than both white and rose varieties, suggesting a small but consistent preference for reds among consumers.
  
  
  Top Countries by Average Rating
The graph shows: Moldova, Lebanon, and Croatia at the top, it means wines from these countries have the highest average ratings in our dataset.
Before trusting this result, we must check how many wines come from these countries : -If a country has only < 20  wines â†’ average is unreliable
-If it has 200 wines â†’ very reliable
This is called sample size bias.Although Moldova, Lebanon, and Croatia show high average ratings, their limited sample sizes suggest that these results should be interpreted with caution.Although Moldova, Lebanon, and Croatia display the highest average ratings, each country is represented by fewer than 20 wines in the dataset. As a result, these averages are likely influenced by small sample bias and should be interpreted with caution. To ensure reliable conclusions, we focused our analysis on countries with a sufficient number of observationsThis graph illustrates the evolution of the average wine rating over time by showing how ratings vary across different vintage years. It helps to evaluate whether the vintage has a significant influence on perceived quality. If an upward trend is observed, it suggests that newer wines tend to receive higher ratings, possibly due to improvements in production techniques or changing consumer preferences. Conversely, fluctuations or stable patterns indicate that vintage alone is not sufficient to determine quality. Overall, this analysis highlights the role of the production year as a secondary factor in wine evaluation, complementing other important variables such as price and origin.
  
  
  Best Value for Money Wines
This part identifies the best value-for-money wines in the dataset by selecting bottles that combine high quality with affordable prices.More specifically, it filters the data to keep only wines that have a rating of at least 4.3 (highly appreciated by users) and a price of 20 or less (considered relatively inexpensive). It then sorts these wines by rating in descending order and displays the top 10 highest-rated affordable wines.This analysis highlights wines that offer an excellent qualityâ€“price ratio, which can be used to support recommendations, help users discover hidden gems, and guide business strategies focused on promoting high-value products.The analysis reveals that most wines have ratings concentrated around 4, indicating generally good quality. Wine prices show a right-skewed distribution, with the majority being affordable and a few luxury wines driving up the high end. While there is a moderate correlation between price and rating, a higher price does not always guarantee better quality. Red wines tend to slightly outperform white and rose varieties, and certain countries consistently produce higher-rated wines. Vintage year appears to have only a minor effect on ratings. Finally, several value-for-money wines stand out, providing actionable insights for recommendations and business strategy.For our predictive modeling, the target variable is the wine Rating, which represents the score given by users. The features used to predict this rating include Price, Vintage Year, Country of Origin, and Type of wine (red, white, or rose). These variables capture both the economic and qualitative aspects of the wine, allowing the model to learn patterns that influence user preferences and perceived quality.To prepare the data for modeling, we first split it into training and testing sets using an 80/20 ratio, ensuring the model can be evaluated on unseen data. The numerical features (Price and Year) were standardized using a scaler, while categorical features (Country and Type) were encoded with one-hot encoding to convert them into a machine-readable format. These preprocessing steps were combined into a ColumnTransformer pipeline, which ensures that all data is properly transformed before being fed into the predictive model.The Random Forest model achieved a Mean Absolute Error (MAE) of 0.161, indicating that on average, the predicted wine ratings deviate by approximately 0.16 points from the actual ratings on the rating scale. The RÂ² score of 0.511 shows that the model explains about 51% of the variance in wine ratings. This suggests that while the model captures a significant portion of the factors influencing ratingsâ€”particularly price and vintage yearâ€”there remains some variability that is not accounted for, likely due to other qualitative factors such as taste preferences, winery reputation, or unobserved characteristics. Overall, the model provides a reasonable predictive performance for guiding value-based wine recommendations and pricing strategies.Accurate Rating Predictions: The model can estimate a wineâ€™s rating based on its price, country, type, and vintage year. This is useful for recommending wines that users are likely to enjoy, even when a wine has few existing reviews.Price-Quality Relationship: While price does influence the rating to some extent, the model demonstrates that other factorsâ€”such as country, type, and yearâ€”also play a significant role in predicting quality.Business Impact: Vivino can leverage this model to highlight â€œvalue-for-moneyâ€ wines, identify underrated wines, and optimize strategic pricing to better guide consumers.To make this report even more insightful, we can examine which variables have the greatest influence on wine ratings. By analyzing feature importance from the trained Random Forest model, we can identify the key drivers behind the predictions and better understand how factors like price, country, type, and vintage year contribute to perceived wine quality.A horizontal bar plot with:Y-axis â†’ feature names (Price, Year, Country_â€¦, etc.)X-axis â†’ importance values (ranging from 0 to 1)The Price feature should visually dominate the chart.This produces a clear and easily readable graph, eliminating the need for a table.Price is the dominant factor, accounting for approximately 78% of the variation in wine ratings. As expected, higher-priced wines tend to receive slightly higher ratings, though this is not always guaranteed. Vintage year is the second most important factor, contributing around 7.6% to the rating prediction. Both older and more recent vintages can subtly influence perceived quality. In contrast, the impact of country of origin and wine type (red or white) is relatively minor, each contributing roughly 1% individually. This confirms that price and year are the primary drivers of wine ratings, while categorical features such as country and type can still help fine-tune recommendations. From a business perspective, Vivino can leverage these insights to highlight wines that offer the best value for money, enabling the platform to recommend wines based on perceived quality rather than relying solely on raw ratings.This project successfully explored the relationship between wine price, quality, and origin using data science techniques. By combining data cleaning, exploratory analysis, and predictive modeling, we were able to identify the key factors that influence wine ratings.The Random Forest model demonstrated that price and vintage year are the strongest predictors of wine quality, while country of origin and type play a smaller but meaningful role. The modelâ€™s performance, with an MAE of 0.16 and RÂ² of 0.51, shows it can reasonably predict wine ratings, providing valuable guidance for wine recommendations.From a business perspective, these insights empower Vivino to highlight value-for-money wines, identify underrated wines, and optimize pricing strategies, ultimately enhancing customer satisfaction.Overall, this project illustrates how leveraging large wine datasets and machine learning can transform raw data into actionable insights, supporting smarter decisions for both businesses and consumers. Future work could incorporate additional features, such as tasting notes, winery reputation, or user reviews, to further improve the predictive performance and recommendation quality.]]></content:encoded></item><item><title>ðŸ§  Ensemble_Strategy</title><link>https://dev.to/stklen/ensemblestrategy-4mde</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 15:00:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Washin Village AI Director Tech Notes #4Just like human teamwork,  makes multiple AI models work together, combining their judgments for more accurate results.: Two heads are better than one.
  
  
  ðŸ” Why Do We Need Ensemble?
Single model limitations:Prone to errors on certain categories The probability of both models making the same mistake is very low.
  
  
  Strategy 1: Voting Mechanism

  
  
  Strategy 2: Weighted Confidence

  
  
  Strategy 3: Validation Mode
Input Image
    â”‚
    â”œâ”€â†’ Primary Model (Unified_v18) â”€â”€â†’ Prediction + Confidence
    â”‚
    â””â”€â†’ Validation Model (Inc_v201) â”€â”€â†’ Prediction + Confidence
    â”‚
    â†“
Ensemble Decision Engine
    â”‚
    â†“
Final Result
Use primary result directly: Ensemble + validation mode works best!: Use different architectures or training data: Higher accuracy models get higher weights: Ensemble is slower than single model: Adjust confidence thresholds based on actual needsWashin Village ðŸ¡ by AI Director]]></content:encoded></item><item><title>Introducing Flowrra: A Simpler Alternative to Celery</title><link>https://dev.to/mameen/introducing-flowrra-a-simpler-alternative-to-celery-4ack</link><author>Ahmad Ameen</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 14:44:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ðŸš€ I just published the first working version of Flowrra!After months of experimenting, building, and learning along the way, I finally have a small but fully functional background task execution framework that is async-first, Python-native, and designed with simplicity and clarity in mind.Iâ€™ve been using Celery in production for years. Itâ€™s powerful, but it can also feel a bit opaque; tasks run in processes or threads, async support isnâ€™t native, and integrating it tightly with custom frameworks can get tricky.On top of that, using Celery usually means you have to:Run a separate worker service just to execute tasks.Start a separate Flower service if you want to monitor them.No separate worker service needed: tasks are executed directly by the scheduler/worker integration, reducing setup complexity.Async-native: built around Pythonâ€™s asyncio for truly non-blocking, I/O-heavy tasks.Pluggable result backends: currently supports In-Memory and Redis, but swapping or adding new backends is straightforward.Built-in UI / framework integration: monitor tasks, view results, and integrate directly with your framework without starting a separate service.Transparent lifecycle: every taskâ€™s state is explicit â€” perfect for debugging and learning.For developers exploring async workloads or wanting a simple, understandable task system, Flowrra could be a more natural fit than Celery.Building Flowrra wasnâ€™t just about writing code; it was a deep dive into distributed systems and Python concurrency: Understanding how tasks are scheduled, executed, and their results stored. Learning the subtle complexities of retries, state management, and task lifecycles. Every line of code taught me something I could never fully grasp just by reading docs or tutorials.Flowrra is still early-stage. Next steps include:Expanding backend optionsEnhancing the built-in UI for better monitoringTesting distributed task execution scalabilityTesting multiple scheduler instances' scalabilityItâ€™s exciting to see something that started as a curiosity turn into a working system.Flowrra is fully open-source. If youâ€™re curious about async Python, distributed systems, or just want to experiment with a task runner, you can contribute or test it today.]]></content:encoded></item><item><title>Using ORM with Scrapy: The Complete Guide</title><link>https://dev.to/ikram_khan/using-orm-with-scrapy-the-complete-guide-15cf</link><author>Muhammad Ikramullah Khan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 14:39:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the last blog, I learned about ORM. Writing Python instead of SQL. Clean code. No more complex INSERT statements.But how do I actually use it in my Scrapy spiders?I tried adding ORM to my pipeline. It crashed. Session errors. Connection problems. Duplicate key errors. I had no idea how to integrate ORM properly with Scrapy.After hours of trial and error, I figured it out. Now my spiders save data with clean Python code, handle relationships automatically, and update existing records seamlessly.Let me show you how to use ORM with Scrapy the right way.In this guide, we'll create:Scrapes products from an e-commerce siteScrapes reviews for each productSaves everything to database using ORMHandles product-review relationships automaticallyUpdates existing products (no duplicates)All with clean Python code (no SQL!)No messy SQL. Just Python objects.
  
  
  Step 1: Install Dependencies
pip scrapy sqlalchemy

  
  
  Step 2: Create Scrapy Project
scrapy startproject ecommerce
ecommerce

  
  
  Step 3: Project Structure
ecommerce/
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ ecommerce/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ models.py          # NEW: ORM models
â”‚   â”œâ”€â”€ pipelines.py       # ORM pipeline
â”‚   â”œâ”€â”€ items.py
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ products.py

  
  
  Step 1: Define ORM Models
Create  with our database models:Defines Product table (stores product info)Defines Review table (stores reviews)Creates relationship (one product has many reviews)Provides helper functions for setup
  
  
  Step 2: Define Scrapy Items

  
  
  Step 3: Create ORM Pipeline
Creates database connection on spider startHandles both products and reviewsUpdates existing products (no duplicates!)Links reviews to products automaticallyLogs statistics when spider closesHandles errors gracefully
  
  
  Step 5: Configure Settings
Spider scrapes product pagesFor each product, yields ProductItemPipeline saves product to database (or updates if exists)Spider scrapes reviews for that productYields ReviewItem for each reviewPipeline saves reviews (linked to product automatically!) All handled by ORM.After scraping, query your data:Access reviews: Access product: Relationships handled automatically!
  
  
  Advanced: Batch Processing
For better performance, save items in batches:Batch processing benefits:More efficient (fewer database operations)Better for large scraping jobs
  
  
  Using PostgreSQL Instead of SQLite
For production, use PostgreSQL:
  
  
  Step 1: Install PostgreSQL Driver
pip psycopg2-binary

  
  
  Step 3: Create PostgreSQL Database

psql  postgres

CREATE DATABASE ecommerce_db
CREATE USER scrapy_user WITH PASSWORD 
GRANT ALL PRIVILEGES ON DATABASE ecommerce_db TO scrapy_user
  
  
  Step 4: Update settings.py

  
  
  Step 5: Pass URL to Pipeline
That's it! Now using PostgreSQL with ORM.
  
  
  Handling Multiple Spiders
For distributed crawling, use connection pooling:Multiple spiders share connection poolHandles concurrent access properly
  
  
  Common Issues and Solutions

  
  
  Issue 1: "Table already exists"
sqlalchemy.exc.OperationalError: table products already exists

Use  only once, or check if tables exist:
Always create new session for each spider:
  
  
  Issue 3: Duplicate Key Errors
UNIQUE constraint failed: products.url

Check if product exists before inserting:
  
  
  Issue 4: Relationship Not Working
 returns empty list even though reviews exist.
Make sure product_id is set correctly:
  
  
  1. Always Use Transactions

  
  
  2. Close Sessions Properly
Here's everything together:How to integrate ORM with ScrapyDefine models for products and reviewsHandle relationships automaticallyBatch processing for performanceHandle duplicates (check before insert)Use batch processing for large jobsPostgreSQL for productionYou now have production-ready ORM integration with Scrapy!]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/jaynniixxzz/-48gp</link><author>Jay</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 14:28:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[First Working Version of Flowrra]]></content:encoded></item><item><title>How AI Inpainting Actually Works â€” And Why It&apos;s Better Than Clone Stamp</title><link>https://dev.to/danny_miller/how-ai-inpainting-actually-works-and-why-its-better-than-clone-stamp-4n9d</link><author>Danny Miller</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 14:12:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Ever wondered how those "magic eraser" tools actually work? I spent weeks diving into image inpainting technology, and here's what I learned about removing watermarks, objects, and imperfections from images.The Problem with Traditional Approaches
If you've ever used Photoshop's Clone Stamp or Healing Brush, you know the pain:Carefully paint over targetCry
Even Content-Aware Fill, while better, often produces weird artifacts â€” duplicating elements that shouldn't be there or creating unnatural patterns.Enter AI Inpainting
Modern inpainting uses neural networks trained on millions of images to understand:Context â€” What should logically be in the masked area
Texture â€” How to match surrounding patterns
Semantics â€” Whether it's filling sky, grass, fabric, or skin
The key insight: instead of copying pixels from nearby areas (clone stamp), AI generates new pixels that make contextual sense.The Technical Architecture
Most state-of-the-art inpainting models follow this pattern:Input Image + Binary Mask
        â†“
   Encoder (extract features)
        â†“
   Attention Layers (understand context)
        â†“
   Decoder (generate pixels)
        â†“
Popular approaches include:LaMa (Large Mask Inpainting) â€” Great for big areas
Stable Diffusion Inpainting â€” Uses diffusion models
MAT (Mask-Aware Transformer) â€” Transformer-based
What I Built (and What I Learned)
I've been working on an image processing tool that uses AI inpainting for watermark removal. Here's what surprised me:Mask Quality Matters More Than Model Size
A precise mask with a smaller model beats a sloppy mask with SOTA models. The mask tells the AI exactly what to regenerate.AI-Generated Images Are Easier to Fix
Ironic, right? Images from Midjourney, DALL-E, Gemini, and other AI tools have consistent synthetic textures that inpainting models understand well.Processing Time is Constant-ish
Unlike traditional methods where complex watermarks take longer, AI processing is roughly the same regardless of content complexity. Most images process in 2-3 seconds.Upload image
Brush over watermark (creates binary mask)
AI generates replacement pixels
Download resultStock photo watermarks
AI-generated image logos (Gemini, Midjourney, DALL-E, etc.)
Corner badges
Code Snippet: Basic Inpainting Pipeline
If you want to experiment yourself, here's a minimal Python example using a pre-trained model:from transformers import pipelineinpainter = pipeline("image-to-image", model="stabilityai/stable-diffusion-2-inpainting")
def remove_watermark(image, mask):
    image: PIL Image with watermark
    mask: Binary PIL Image (white = area to inpaint)
    """
        prompt="clean background, no text",
        mask_image=mask,
    )
For production use, you'd want:GPU acceleration
Proper image preprocessing
Edge blending post-processing
Batch processing support
Testing on 100 random watermarked images:Method  Avg Time    Quality (1-10)
Manual (Photoshop)  8 min   9
Content-Aware Fill  3 sec   6
AI Inpainting   2.5 sec 8.5
AI inpainting hits the sweet spot: near-manual quality at near-instant speed.Limitations to Know
AI inpainting isn't magic. It struggles with:Large masked areas (>40% of image) â€” Not enough context to work with
Faces and text â€” Can hallucinate weird results
Precise reconstruction â€” If you need exact details, manual is still better
What's Next
The field is moving fast. Recent developments:Segment Anything + Inpainting â€” Auto-detect watermarks, no manual masking
Video inpainting â€” Remove watermarks from video frames consistently
Real-time processing â€” Mobile-friendly speeds
I'm particularly excited about automatic watermark detection. Imagine: upload image â†’ AI finds watermarks â†’ removes them â†’ done.Try It Out
If you want to see AI inpainting in action without setting up your own pipeline:Free credits on signup. No GPU required on your end.Have you worked with inpainting models? What's your experience been? Drop a comment â€” I'd love to hear about edge cases you've encountered.]]></content:encoded></item><item><title>Buy LinkedIn Accounts</title><link>https://dev.to/pvaseoshop0022/buy-linkedin-accounts-1j5d</link><author>PVA SEO SHOP</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 14:11:42 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Buy LinkedIn Accounts
Buy LinkedIn Accounts
Save valuable time and resources by purchasing established LinkedIn accounts. Building a professional network from scratch demands significant effort. It often takes months to develop a credible profile with meaningful connections. Our service provides immediate access to aged LinkedIn accounts. You can bypass the initial growth phase entirely. This gives you a competitive edge in your marketing or sales campaigns. These accounts come ready for immediate professional use. so Buy LinkedIn Accounts and enjoy it.
Why Choose pvaseoshop for LinkedIn Accounts?
âž¤ Fully verified accounts with complete profile information
âž¤ Email access included with all account credentials
âž¤ Profile pictures and professional background details
âž¤ Accounts aged from 6 months to 3 years available
âž¤ US, UK, and international profiles with 200-800+ connections
âž¤ Complete birth date information for account verification
âž¤ 30-day replacement guarantee for your protection
âž¤ 24/7 customer support through multiple channels
âž¤ Manual account creation ensures natural growth patterns
âž¤ Regular activity history maintains account credibility
Telegram:@pvaseoshop
WhatsApp:+1 (959) 216-5007
Email:pvaseoshop@gmail.com
Buy LinkedIn Account: A Professional Network Access Review
You need a professional LinkedIn profile for just about anything in business. A share of individuals and companies may take a shortcut. Thatâ€™s why they prefer to buy LinkedIn accounts for networking or marketing purposes. This approach implies substantial trade-offs between speed and security. The market for such accounts is murky and largely unregulated. Potential buyers have a landscape to cross that is strewn with opportunity and risk.
This analysis is particularly relevant to the market of buying LinkedIn accounts. We examine the reasons and risks involved with this request. We also provide an objective review of service providers. We seek to provide a complete picture for informed decision-making. Knowledge about this practice is vital. Publications like this help keep practitioners out of trouble while also steering them clear of hit-or-miss stumblings into unexamined terrain. The information contained is the result of market research.
Looking forward to hiring LinkedIn Profiles
Why buy LinkedIn account services now? The reasons behind buying LinkedIn account services are even more specific. Lead gen or sales outreach cos are typically the driver of this demand. They need points of exposure. Older accounts get around the new account restrictions on LinkedIn. Â¡Â¨ These limitations mean that Iâ€™m restricted in the number of invitations and messages I can send. The demand gives rise to a supply of different types of accounts. The seller could be an individual or an impersonal digital marketing entity.
New accounts have instant activity constraints. This slows down the work of business development. An old account looks more credible. It includes the form, more connections, and some historical activities. This perceived actuality contributes to the acceptance of messages. Large-scale marketers also buy accounts to enhance their efficiency at the top of the market. They control numerous profiles and large campaigns. This approach is designed to pump volume for outreach and lead gen.
Comparison of US Service Providers
Many US websites provide the service. Their products differ in age, connections, performance characteristics, and features. The following table provides an overall glimpse of the frequent specifications in the best platforms. Discreet online storefronts frequently facilitate services from these providers. They also organize accounts by age and network strength. The buyers may choose such profiles for the ad campaignsÃ¶ according to their campaign needs.
Provider Feature    Account Age (Months)    Connection Range    Common Inclusions
Basic Account   1-6 0-50    Profile setup, basic info
Established Account 7-24    51-500  Some endorsements, past posts
Premium Account 25+ 501+    Recommendations, full activity history
Thereâ€™s a wide variation in service levels from providers. A few even lock down their accounts for a specific time period. Others donâ€™t provide any guarantee once the sale is made. Payment methods also vary. They often accept PayPal, cryptocurrency, and credit cards. The crypto adds an anonymous layer for both the buyer and seller. This complicates the transaction process even further.
Primary Motivations for Account Acquisition
Businesses give several reasons for buying pre-made accounts. The number one reason given is faster deployment. It would take a lot of time to organically build your network. Account on sale supplies you with instantaneous access to a network. This is especially important for near-term marketing. So the salespeople need weapons at hand to go for a more comprehensive program. They cannot wait months for an account to grow ripe.
B2B salespeople target these accounts for cold outreach. If you have an aged profile, the acceptance rate will be higher. By doing this, it raises the chance of lead conversion. They are also employed by some social media management agencies. From a single platform, they handle various client profiles. This makes social selling strategies scalable. It even differentiates the personal and professional relationship-building tasks.
Overcoming Initial Platform Barriers
LinkedIn has algorithms that are tasked with keeping a close eye on new account behavior. No need to rush, as a sudden spike of requests may cause throttling. The older the account, the higher the trust it has. They let you get more aggressive in your networking. This is relevant for anyone in sales. Their KPIs are built around reaching out in high volume. New users will have daily invitation and message limits.
Established accounts have fewer restrictions. This allows for proactive, rather than reactive, engagement strategies. Companies market leverage this to do concurrent campaigns. They manage a bunch of accounts directed toward various industries. Itâ€™s a strategy that allows them to spread out the way they are generating leads. This also decreases the chance of a single account getting banned. These use cases are scrutinized by the Pvaseshop team daily.Significant Risks and Potential Consequences
You cannot buy an account according to LinkedInâ€™s User Agreement. This also applies to section 8.2 Selling or transferring access is strictly forbidden. The service uses advanced detection mechanisms. These compare behavior, login locations and device fingerprints. Permanent suspension due to erratic activity is also a typical result. If you lose an account, youâ€™ve lost all that effort and connection.
That is not a compliment; that means I wouldnâ€™t trust us with their data, and I certainly would never trust your data. The owner of the original account may still have recovery data on file. After you have put time into the account, they might try to take it back. The history of the account might even involve unknown transgressions. This presents a risk to the new owner. The original owner may have been a spammer. It could permanently sully the Profileâ€™s reputation.
Security and Legal Implications
There are risks of using a paid account. You have no record of the past activities of this Profile. Someone could have used it for spam or scams. It tarnishes your professional reputation by association. Others also have negative implications under the realm of false pretense. Using a fictitious identity on an account for business purposes could hold legal liabilities.
It May Violate Truthful Advertising Laws And Commercial Practices. The PVASEOSHOP blog offers detailed posts about digital compliance. We recommend these resources before you decide. Misrepresentation can open them up to legal action from clients or business partners. Itâ€™s not conducive to trust in professional relationships. The long-term brand damage may be anything but worth it.
Financial and Operational Risks
There is a direct financial loss in a banned account. You lose the price you paid for it and any associated assets. There can be an even greater cost: operational disruption. Marketing campaigns come to a screeching halt. Sales pipelines that relied on that account become useless. If you ever get banned, they ban absolutely everything. It has affected business earnings. These loss of momentum, i.e., business revenue, also leads to crime..
Itâ€™s an awful lot of time and effort to rebuild a network. And the actual costs of such ads are usually more than the original purchase price. Most businesses overlook these potential losses. They see only the short-term advantages. An intelligent risk assessment is necessary. At Pvaseoshop, this assessment is mandatory for all of our digital products.
Evaluating Service Provider Credibility
Note that not all companies in the segment have this capability at this level. Others employ automated bots to form complex networks of fake profiles. These are detected and banned quickly. Others employ manual creation methods. Such accounts generally do (last longer). Key signs of a trusted provider are no-nonsense replacement guarantees. They must provide transparent policies concerning account sourcing.
Find platforms that provide excellent customer support. Transparent disclosure of account sourcing is also crucial. Clear terms of service are essential. Pvaseoshop recommends thorough due diligence. And always look for verified/unbiased user ratings and reviews. Steer clear of providers without a web presence beyond a sales page.
Account Specifications and Feature Analysis
The usefulness of the account depends on the feature set bundled with it. Basic accounts donâ€™t offer much beyond a verified email. Premium accounts feature full work history and references. Below is a table detailing the majority of core attributes and their real-world values. This analysis aids customers in comparing sourcing options.
Account Feature Practical Utility   Longevity Factor
Profile Photo & Banner  Increases profile view credibility  Low
Work History & Education    Adds depth and context to the profile   Medium
Skill Endorsements  Provides social proof of capabilities   High
Written Recommendations Significantly boosts profile authority  High
Previous Content Posts  Shows a history of engagement   Medium
Follower Count  Indicates influence and reach   High
All the details make the account as a whole more believable. Accounts that get recommended and endorsed do better. They sound and look more natural to both machines and people. These improvements are not the result of any sourcing. â€œStylishâ€ (non-organic) characteristics have a longer life than â€œartificially inducedâ€ ones.
Best Practices for the Safe Handling of Procured Accounts
If you decide to make a purchase, some strategies can minimize the risk. Change the password and email to that account asap, obviously. And update your recovery phone number and security questions. Simplify the modification of the Profile details over a few days. Donâ€™t implement turnaround tactics in one go. Suddenly, complete overhauls appear suspicious.
Mimic organic user behavior. Begin the process: Looking at each otherâ€™s profiles and consuming content. Ease into requesting connections and sending messages. Do not post links or promote anything. This timing method ensures the account looks natural to LinkedInâ€™s systems. It decreases the likelihood of flags and reviews made by bots.
Maintaining Account Longevity
How long you will keep your account depends on regular, non-crazy activity. Log in from a single geographic location and device. Do not use VPNs or proxies that change your IP address often. Like and comment on your networkâ€™s content. Flag immunity is mainly achieved through regular, legitimate activity.
Develop a consistent posting schedule. Share industry-relevant articles and insights. Join groups that pertain to the industry/area in which the Profile is located. This is in line with the legitimate users. It is a consistent history of the actions you perform. This keeps the account usable for quite a bit longer.
Integration into Business Workflows
Introduce the account gently into your business life. Donâ€™t rely on it for the main face of your major client relationships. Itâ€™s more effective for initial outreach and lead generation. But you can still use it when scheduling meetings for one main account. This tiered method limits the damage to reputations.
Train staff in the correct manner of use. Record any process in account management. Set communication style and frequency expectations. Keep an eye on your account stats to watch for any signs of something being off. Such organized management will limit operational risks. It also increases the ROI.
Ethical Alternatives to Account Purchase
The LinkedIn profile constructed from scratch is still the safest way. It creates an authentic professional narrative. And you can speed up the process with a targeted plan. Make good use of relevant keywords on your Profile. Engage consistently with industry-specific content. This creates a network for life.
Then, join and engage in the relevant LinkedIn groups. Leverage the built-in content tools and networking features that LinkedIn provides. Think about purchasing LinkedIn Sales Navigator for more sophisticated lead generation. These are the techniques that lead to a hardy, long-term professional network. They sharpen your actual professional skills and knowledge, too.
Strategic Organic Growth Methods
An organic approach to strategic growth pays off. Publish useful stuff three times a week minimum. It sets you up as an authority and brings in followers. Jot a short note on all of your connection requests so that they arenâ€™t generic. This dramatically improves acceptance rates. Itâ€™s a fabric of deep connections.
Reach out to the influencers in your sector. Comment on their posts in a meaningful way to become more visible. Use LinkedIn Articles to show expertise. These strategies create an extensive network without policy infringement. The Pvaseoshop knowledge base provides organic growth strategies that work. These methods offer lasting value.
Leveraging LinkedInâ€™s Native Advertising
LinkedIn Ads for instantaneous professional outreach. They also provide highly targeted job titles, company size and industry. And direct your message with Sponsored Content and Message Ads. This approach doesnâ€™t violate any platform TOS. Itâ€™s also scalable to your budget.
Marketing has proven ROI and so much analytics. You have access to key decision makers without the network limitations. It increases brand recognition and generates leads. This method typically provides better results when compared to paid accounts. It also has no chance of getting my account suspended or tarnishing my reputation.
Why pvaseoshop for LinkedIn Accounts?
We are instantly networked via LinkedIn. They demonstrate real, established patterns of activity and meaningful connectivity. This removes any limitations that new accounts often struggle with. And you can start reaching out to potential clients/partners right away! The accounts have regular login histories and geolocations. This reduces detection risks significantly.
All accounts have full email access and recovery information. We offer comprehensive profiles with work history, references and reviews. Our profiles include skill endorsements and engagement history. This imparts a real professional look. Those connections are with very real professionals from various industries. This adds immediate credibility to your prospecting.
Additional Benefits with Pvaseoshop
â€¢ Ability to create an account for a special industry
â€¢ Discount pricing for larger campaigns
â€¢ Product usage instructions for maximum performance
â€¢ Ongoing management and updates of accounts
â€¢ completely secure Payment options (Bitcoin)
â€¢ Instant Shipping to your D2 Store within 1-4 hours of purchasing
â€¢ Geographical targeting for local markets
â€¢ Industry-specific connection networks
Start your networking the right way. Our accounts are the base of LinkedIn marketing. They will keep you out of new account restrictions and attribute your account to authority at the speed of light! The cost of having a high-quality LinkedIn account is multiplied by the reach it can give.
Buy your LinkedIn accounts at Pvaseoshop today. Professional Quality Assurance: Our team personally inspects all items before shipping. We keep high traffic quality for all of the accounts. This ensures you get quality, well-written profiles. Begin reaching out to decision makers and industry influencers now.
The possibility of purchasing access to a LinkedIn account is something about which there is little doubt. It provides immediate network access, but comes with significant risks. Account ban, lack of security and ethical consideration are essential. Its worth to [lubuntulove] know that a deep understanding of your own needs is necessary. Organic growth is the best and healthiest approach for long-term business building.
Bought accounts are at best ephemeral instruments. There is no replacing authentic professional networking. The gains are rarely worth the risk. Real investment in growth = real return on success. It saves your business from risks and troubles.
Frequently Asked Questions
Can you buy a LinkedIn account legally?
Buying the LinkedIn account is not a criminal offense, though. But it violates LinkedInâ€™s User Agreement. Such a violation may lead to the suspension of an account without appeal. The misrepresentation and its legal implications. The practice itself involves misrepresentation, which can be addressed legally.
How much do these accounts generally range for?
Prices vary based on account age and the number of features. Basic accounts can run $30 to $60. Accounts with established profiles cost between $80 and $200. Recommended significantly aged accounts that can be over $300. Due to specialized targeting value, Niche Industry profiles frequently carry a premium cost.
Can LinkedIn tell if you purchased an account?
Yes, LinkedIn has security measures in place to identify abnormal activity. Irregular login locations, abrupt modifications to profile sections, or repetitive non-organic development systems are often red flags that trigger a review, processing, and account suspension. Behavioral algorithms assess hundreds of data points for anomalies.
What if I get my bought account banned?
In the event LinkedIn shuts down your account, youâ€™re shut out for good. All relations, contents or data linked to that Profile will be destroyed. There is a limited replacement guarantee, but that does not restore lost time and network connectivity. Continued violations may result in hardware or IP bans.
What are some secure ways to purchase LinkedIn accounts?
There is neither a totally safe nor approved way. The deal is obviously against the platform policy. All CCS get banned. The easiest way is to have a profile thatâ€™s under your constant development. This approach builds your genuine professional reputation and network.]]></content:encoded></item><item><title>Create Callable Instances With Python&apos;s .__call__()</title><link>https://realpython.com/courses/create-callable-instances-dunder-call/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[In Python, a  is any object that you can call using a pair of parentheses and, optionally, a series of arguments. Functions, classes, and methods are all common examples of callables in Python. Besides these, you can also create custom classes that produce . To do this, you can add the  special method to your class.Instances of a class with a  method behave like functions, providing a flexible and handy way to add functionality to your objects. Understanding how to create and use callable instances is a valuable skill for any Python developer.In this video course, youâ€™ll:Understand the concept of  in PythonCreate  by adding a  method to your classes Compare  and  and understand their distinct rolesBuild practical examples that use callable instances to solve real-world problems]]></content:encoded></item><item><title>First Working Version of Flowrra</title><link>https://dev.to/mameen/first-working-version-of-flowrra-155c</link><author>Ahmad Ameen</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 13:57:58 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ðŸš€ I just pushed the first working version of Flowrra!After months of experimenting, building, and learning along the way, I finally have a small but fully functional background task execution framework that is async-first, Python-native, and designed with simplicity and clarity in mind.Iâ€™ve been using Celery in production for years. Itâ€™s powerful, but it can also feel a bit opaque; tasks run in processes or threads, async support isnâ€™t native, and integrating it tightly with custom frameworks can get tricky.On top of that, using Celery usually means you have to:Run a separate worker service just to execute tasks.Start a separate Flower service if you want to monitor them.No separate worker service needed: tasks are executed directly by the scheduler/worker integration, reducing setup complexity.Async-native: built around Pythonâ€™s asyncio for truly non-blocking, I/O-heavy tasks.Pluggable result backends: Redis now, but swapping or adding new backends is straightforward.Built-in UI / framework integration: monitor tasks, view results, and integrate directly with your framework without starting a separate service.Transparent lifecycle: every taskâ€™s state is explicit â€” perfect for debugging and learning.For developers exploring async workloads or wanting a simple, understandable task system, Flowrra could be a more natural fit than Celery.Building Flowrra wasnâ€™t just about writing code; it was a deep dive into distributed systems and Python concurrency: Understanding how tasks are scheduled, executed, and their results stored. Learning the subtle complexities of retries, state management, and task lifecycles. Seeing how even simple abstractions leak complexity when scaled. Every line of code taught me something I could never fully grasp just by reading docs or tutorials.Flowrra is still early-stage. Next steps include:Expanding backend optionsEnhancing the built-in UI for better monitoringTesting distributed task execution scalabilityTesting multiple scheduler instances' scalabilityItâ€™s exciting to see something that started as a curiosity turn into a working system.Flowrra is fully open-source. If youâ€™re curious about async Python, distributed systems, or just want to experiment with a task runner, you can contribute or test it today.]]></content:encoded></item><item><title>Best Resources to Learn Python as a Beginner</title><link>https://dev.to/srdan_borovi_584c6b1d773/best-resources-to-learn-python-as-a-beginner-1f3l</link><author>Srdan BoroviÄ‡</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 13:30:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Learning Python in 2025 shouldn't cost you a fortune. The internet has democratized programming education to the point where some of the best resources are completely free. However, too many options can paralyze you before you write your first line of code.After digging through Reddit threads, developer forums, and course reviews, a clear pattern emerges. The resource matters less than how you use it. But some paths are definitely smoother than others.
  
  
  Start Where You Actually Are
Mimo offers a mobile-first approach that fits into your daily routine. The platform teaches Python through bite-sized lessons and includes an AI assistant to answer questions as you code. You can build projects directly in the app, which helps bridge the gap between theory and practice. For people learning on their commute or during lunch breaks, this format works well.Mimo also recently launched a building experience where you can create real apps and websites using AI-assisted development. You describe what you want to build in plain language, then collaborate with AI to implement changes in an actual codebase. The platform includes a modern editor, built-in SQL database, instant preview, and the ability to publish your projects with a custom domain. It's designed to teach you both coding fundamentals and the AI-assisted workflows that are becoming industry standard.Futurecoder takes a different angle. This free browser-based course includes built-in debuggers and enhanced error messages that explain what went wrong. Users praise its ability to teach how code executes step-by-step. The instant feedback loop prevents you from building bad habits early on.Want someone to actually review your code? Exercism offers a mentor-reviewed system where experienced developers critique your work. Some beginners find it too challenging at first. Those who stick with it appreciate the personalized feedback. You'll learn not just how to make code work, but how to make it good.
  
  
  Two Free University Courses Worth Your Time
When people hear about free university courses, they often assume they're watered-down versions of the real thing. They're not.14 sections with roughly 30 exercises eachTest My Code system auto-grades your work instantlyYou need to pass 80% of exercises before advancingExpect 120 to 160 hours of work totalDedicated Discord channel for peer supportThat 80% requirement sounds harsh. It prevents the illusion of understanding that plagues many self-taught programmers. Students report gaining the independence to write programs without constantly Googling syntax. Later sections transition from a browser editor to Visual Studio Code. You'll learn professional workflows like setting up development environments and using extensions. These skills separate hobbyists from job-ready developers. brings Hollywood production values to programming education. Professor David Malan makes compiler errors feel exciting, which is a weird thing to say but somehow true.The course deliberately teaches 80% of what you need for each problem set. You research the remaining 20% independently. This gap forces you to develop the search skills that professional developers use daily.CS50P's auto-graders are unforgiving. You might spend hours debugging because you capitalized a letter wrong. Students curse this at first, then thank it later when they're catching bugs in production code. The course costs $219 for a verified certificate on edX, but Harvard offers the identical curriculum free through their OpenCourseWare platform.
  
  
  For People Who Learn by Building
Some people need to see results immediately. Abstract theory kills their motivation. Show them a for-loop in isolation and their eyes glaze over. Show them how it could rename 500 files in two seconds and suddenly they're taking notes.Mimo's building experience fits this learning style perfectly. You can build full-stack apps with front and back ends, work with a built-in SQL database, and publish stable versions for real users. The instant preview shows your changes as you implement them, and you can export your source code or connect a custom domain for portfolio-ready projects. Unlike pure AI generation tools that create code you can't explain, Mimo keeps you close to what you're building.Angela Yu's 100 Days of Code on Udemy promises 100 projects in 100 days. You'll build console games, data visualizations, and web applications while forming a daily coding habit. The first 50 days earn universal praise for pacing and clarity.The quality drops in the latter half as projects become more self-guided. Web development sections feel slightly dated. But here's why it still works: the course teaches you concepts you didn't know existed. That unknown-unknown problem trips up many self-taught programmers. You can't Google something if you don't know it has a name.Price matters here. The course lists for over $100 but goes on sale for $15-$20 regularly. Many U.S. public libraries offer free access through "Gale by Udemy." Check before buying.Update spreadsheets automaticallyAccountants and data entry professionals swear by this approach because it solves immediate pain points. One Reddit user credited this book with launching their data engineering career. However, the code style leans toward beginner-friendly rather than pythonic. Use this alongside a more rigorous course if you're aiming for software development roles.
  
  
  Physical Books Still Have Their Place
Physical books force you away from digital distractions. Many developers report better retention when studying from paper, particularly during deep focus sessions. by Eric Matthes consistently ranks as the number one beginner book. Half covers syntax and theory, half walks through projects including a space invaders-style game and Django web app. The 3rd edition includes updated content on modern Python practices. Users recommend it for striking the balance that many online courses miss. by Allen Downey focuses on computational thinking. If you want to understand not just Python but programming as a discipline, this book delivers. The approach feels more academic. Works for some learners, alienates others.Skip Learn Python the Hard Way by Zed Shaw. Experienced developers describe it as "radioactive garbage" due to idiosyncratic style and outdated Python 3 information. The controversy runs deep enough that recommending it will get you downvoted on Reddit.
  
  
  YouTube Works If You're Disciplined About It
YouTube tutorials get a bad rap for encouraging passive learning. Watch a video, feel smart, then freeze at a blank screen. But some creators break this pattern. produces the clearest Python tutorials on the platform. His explanations of Object-Oriented Programming, Django, and regular expressions beat many paid textbooks.The catch? His videos lack built-in exercises. You need to create your own practice problems or risk falling into tutorial hell.Avoid 6-12 hour "Full Course" videos unless you're ready to pause every five minutes and code along. Otherwise, you're just watching someone else solve problems.
  
  
  The Tools You'll Actually Need
Your choice of IDE sparks religious debates in programmer circles. The 2025 consensus has settled on two options.PyCharm Community Edition works brilliantly out of the box. Powerful autocomplete catches errors before you run code. The introspection helps you understand type annotations and available methods through exploration rather than memorization. Beginners benefit from this immediate feedback. requires more setup. You'll need to install Python and Jupyter extensions at minimum. But it offers flexibility across languages. For long-term career development, learning VS Code pays dividends when you inevitably need to work with JavaScript, HTML, or other technologies. inside VS Code transform the learning experience for data science paths. Line-by-line execution with instant visual output reduces debugging burnout. You see results immediately, which maintains motivation during difficult concepts.
  
  
  Breaking Out of Tutorial Hell
The biggest trap isn't picking the wrong resource.Tutorial hell happens when you become a passive consumer, following along perfectly but unable to create anything independently. I spent two months in this trap with web development. Could follow any tutorial flawlessly. Couldn't build a contact form from scratch. Breaking out required three deliberate changes in approach.The Parallel Project RuleAfter completing a tutorial that builds a calculator, you build a unit converter. Similar concepts, different implementation. This forces you to apply knowledge rather than copy it. The struggle of adapting what you learned to a new context teaches more than any smooth tutorial walkthrough.Got code working? Now change it until it breaks. See what error messages appear. Fix them without consulting the tutorial.This builds debugging skills that no polished walkthrough can teach. After a week of deliberately breaking scripts, you'll read Python error messages like sentences instead of cryptic warnings.Google, Stack Overflow, and official documentation are separate skills to master. They're as important as Python syntax. Fighting with documentation for 30 minutes teaches you more than getting a quick answer from ChatGPT.Good rule of thumb: struggle for at least 20 minutes before asking anyone for help.
  
  
  The AI Question Nobody Wants to Address
ChatGPT and Claude have become standard tools in 2025. Use them right, and you'll learn syntax in weeks. Use them wrong, and you'll never learn at all.How to use AI without crippling yourself:Ask it to explain cryptic error messagesRequest progressive difficulty drills based on your current levelHave it generate custom exercises for weak areasNever ask it to just write the solutionThe framework: Use AI for explanations, minimal debugging hints, and practice generation. Write the code yourself. Always.
  
  
  What Separates Success from Failure
Here's what nobody wants to hear: grit matters more than your resource choice. The ability to sit with confusion, read error messages carefully, and keep trying after your code fails for the hundredth time. That's the skill.People with expensive bootcamp degrees wash out because they quit when things get hard. High school dropouts become senior developers because they refuse to stop learning.The best resources teach you how to think through problems, not just how to type syntax.]]></content:encoded></item><item><title>ðŸ“˜ Build a Trace Grid Puzzle Book Generator in Python (with Live Preview + PDF Export)</title><link>https://dev.to/matetechnologie/build-a-trace-grid-puzzle-book-generator-in-python-with-live-preview-pdf-export-8no</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 13:28:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build a desktop puzzle book generator using Python.By the end, youâ€™ll have an app that:Generates grid-based tracing puzzlesAutomatically builds a print-ready PDFSupports multiple puzzles per pageIs ready for KDP / Etsy style publishingIf youâ€™d rather jump straight to the finished code:Letâ€™s build it step by step.Make sure you have Python 3.10+ installed.pip install pillow svgwrite reportlab ttkbootstrap cairosvgttkbootstrap â€“ modern UI themesvgwrite â€“ vector puzzle exportcairosvg â€“ SVG â†’ PNG for PDF embeddingðŸ§± Step 1 â€” Project Skeletonimport sys, random
from pathlib import Path
import tkinter as tk
from tkinter import filedialog, messagebox

from PIL import Image, ImageDraw, ImageTk
import svgwrite
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter

import ttkbootstrap as tb
from ttkbootstrap.constants import *
Pillow â†’ live preview imagesvgwrite â†’ scalable puzzle pagesttkbootstrap â†’ dark themed UIðŸ§  Step 2 â€” Main Application ClassEverything lives inside one class:class TraceGridPuzzleBook:
    APP_NAME = "TraceGrid Puzzle Book Generator"
    APP_VERSION = "1.3.0"
Define available options:    GRID_STYLES = ["Square", "Isometric", "Triangle"]
    SYMBOL_TYPES = ["Dot", "Circle", "Square", "Mixed"]
    EXPORT_FORMATS = ["SVG", "PNG", "PDF"]
Now initialize the window:    def __init__(self):
        self.root = tk.Tk()
        tb.Style(theme="darkly")

        self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
        self.root.geometry("1130x620")
ðŸŽ› Step 3 â€” State VariablesThese store user settings:        self.grid_size_var = tk.IntVar(value=5)
        self.symbol_var = tk.StringVar(value="Dot")
        self.path_complexity_var = tk.IntVar(value=50)

        self.symbol_color_var = tk.StringVar(value="#000000")
        self.line_color_var = tk.StringVar(value="#000000")
        self.bg_color_var = tk.StringVar(value="#ffffff")

        self.line_thickness_var = tk.IntVar(value=2)
        self.symbol_radius_var = tk.IntVar(value=8)

        self.rows_var = tk.IntVar(value=2)
        self.cols_var = tk.IntVar(value=2)
        self.pages_var = tk.IntVar(value=1)

        self.output_dir = Path.home() / "TraceGridPuzzleBooks"
Each tk.Variable automatically syncs with UI widgets.ðŸ–¼ Step 4 â€” Building the InterfaceCreate the UI:

        self._build_ui()
        self._update_preview()
Create left control panelsAdd spinboxes and dropdownsCreate a center Live Preview canvasExample: Grid size control:tb.Spinbox(
    grid_body,
    from_=3,
    to=15,
    textvariable=self.grid_size_var,
    command=self._update_preview
).grid(row=0, column=1)
Whenever the value changes, _update_preview() redraws puzzles instantly.ðŸ§© Step 5 â€” Generating a Single Puzzle PathEach puzzle is just a random walk through a grid:def _generate_single_puzzle(self):
    size = self.grid_size_var.get()
    complexity = self.path_complexity_var.get() / 100

    total_steps = max(2, int(size * size * complexity))

    path = [(0, 0)]
    visited = set(path)

    for _ in range(total_steps - 1):
        x, y = path[-1]

        neighbors = [
            (x+dx, y+dy)
            for dx, dy in [(0,1),(1,0),(0,-1),(-1,0)]
            if 0 <= x+dx < size
            and 0 <= y+dy < size
            and (x+dx, y+dy) not in visited
        ]

        if not neighbors:
            break

        next_cell = random.choice(neighbors)
        visited.add(next_cell)
        path.append(next_cell)

    return path
Randomly move up/down/left/rightStop when complexity limit is reachedThis produces clean tracing paths.âœï¸ Step 6 â€” Drawing Puzzles (PNG Preview)For live preview we draw with Pillow:def _draw_puzzle_png(self, draw, path, x_offset, y_offset, cell_width, cell_height):
    step = cell_width / (self.grid_size_var.get()+1)
    radius = self.symbol_radius_var.get()

    for i, (x, y) in enumerate(path):
        px = x_offset + (x+1)*step
        py = y_offset + (y+1)*step

        draw.ellipse([px-radius, py-radius, px+radius, py+radius], fill=self.symbol_color_var.get())

        if i > 0:
            prev = path[i-1]
            px0 = x_offset + (prev[0]+1)*step
            py0 = y_offset + (prev[1]+1)*step

            draw.line([px0, py0, px, py],
                      fill=self.line_color_var.get(),
                      width=self.line_thickness_var.get())
Dots + connecting lines = tracing puzzle.Every UI change regenerates sample puzzles:def _update_preview(self):
    img = Image.new("RGB", (400,400), self.bg_color_var.get())
    draw = ImageDraw.Draw(img)

    for r in range(self.rows_var.get()):
        for c in range(self.cols_var.get()):
            path = self._generate_single_puzzle()
            self._draw_puzzle_png(draw, path, c*200, r*200, 200, 200)

    self.preview_image = ImageTk.PhotoImage(img)
    self.preview_canvas.delete("all")
    self.preview_canvas.create_image(0,0, anchor="nw", image=self.preview_image)
This gives instant visual feedback.ðŸ“„ Step 8 â€” Exporting SVG PagesEach page is a large SVG with multiple puzzles:def _export_page_svg(self, filename, puzzle_paths, rows, cols):
    dwg = svgwrite.Drawing(filename, size=(1000,1000))

    cell_w = 1000 / cols
    cell_h = 1000 / rows

    for i, path in enumerate(puzzle_paths):
        r = i // cols
        c = i % cols

        self._draw_puzzle_svg(
            dwg,
            path,
            c*cell_w,
            r*cell_h,
            cell_w,
            cell_h
        )

    dwg.save()
SVG is perfect for printing because itâ€™s vector-based.ðŸ“š Step 9 â€” Building the PDFWe convert SVG â†’ PNG â†’ PDF:def _export_pdf(self, pdf_filename, svg_files):
    c = canvas.Canvas(str(pdf_filename), pagesize=letter)

    for svg in svg_files:
        from cairosvg import svg2png

        png = svg.with_suffix(".png")
        svg2png(url=str(svg), write_to=str(png))

        img = Image.open(png)
        c.drawInlineImage(img, 0, 0)
        c.showPage()

    c.save()
Now you have a multi-page puzzle book PDF.if __name__ == "__main__":
    TraceGridPuzzleBook().run()
A full desktop tool that can:Automatically produce PDF booksSupport multiple puzzles per pageCustomize colors, sizes, and complexityFeel free to fork it, improve it, or build your own puzzle generators on top.]]></content:encoded></item><item><title>SpamSentinel v1.0 â€“ Build a GUI Email Spam Detector in Python</title><link>https://dev.to/matetechnologie/spamsentinel-v10-build-a-gui-email-spam-detector-in-python-377p</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 12:50:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this tutorial, weâ€™ll build SpamSentinel, a fast email spam detector with a GUI using Python. This tool can scan .eml and .txt email files, calculate a spam score, and help you manage spam emails effectively.Setting up the environmentCreating utility functionsBuilding the spam detection workerAdding file selection and drag & dropConnecting the worker to the GUISetup: Install required librariesttkbootstrap (for modern UI)tkinterdnd2 (optional, for drag & drop support)Install the extras using pip:pip install ttkbootstrap
pip install tkinterdnd2We need a helper to get resource paths, especially for icons when packaged with PyInstaller.def resource_path(file_name):
    """
    Get the path to a resource, works with PyInstaller.
    """    base_path = getattr(sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_path, file_name)
Explanation:
This function ensures your icon or other files can be found whether running as a script or a packaged executable.Weâ€™ll create a class to handle scanning emails and calculating a spam score.import re
from collections import Counter, deque

class SpamWorker:
    def __init__(self, files, min_confidence, include_words, exclude_words, regex_pattern, max_results, callbacks):
        self.files = files
        self.min_confidence = min_confidence
        self.include_words = include_words
        self.exclude_words = exclude_words
        self.regex_pattern = re.compile(regex_pattern, re.IGNORECASE) if regex_pattern else None
        self.max_results = max_results
        self.callbacks = callbacks
        self._running = True

        self.spam_patterns = [
            re.compile(r"(free money|win cash|click here|urgent|lottery)", re.I),
            re.compile(r"(prize|offer|risk-free|credit card)", re.I),
        ]

    def stop(self):
        self._running = False

    def spam_score(self, text):
        score = 0
        for pattern in self.spam_patterns:
            if pattern.search(text):
                score += 50
        return min(score, 100)
spam_score calculates a fake spam score for demonstration.We use regex patterns to detect typical spam keywords.stop() allows the process to be canceled.We now scan files and calculate scores:    def run(self):
        total_files = len(self.files)
        counters = Counter()
        results_buffer = deque(maxlen=self.max_results)

        for i, path in enumerate(self.files):
            if not self._running:
                break
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    if self.include_words and not any(w in content.lower() for w in self.include_words):
                        continue
                    if self.exclude_words and any(w in content.lower() for w in self.exclude_words):
                        continue
                    if self.regex_pattern and not self.regex_pattern.search(content):
                        continue

                    score = self.spam_score(content)
                    if score < self.min_confidence:
                        continue

                    counters["SPAM"] += 1
                    counters["TOTAL"] += 1
                    results_buffer.append((path, score))

                    if "found" in self.callbacks:
                        self.callbacks["found"](path, score)

                    if counters["TOTAL"] >= self.max_results:
                        break
            except Exception:
                pass

            if total_files > 0 and "progress" in self.callbacks:
                self.callbacks["progress"](int((i + 1) / total_files * 100))
            if "stats" in self.callbacks:
                self.callbacks["stats"](dict(counters))

        if "stats" in self.callbacks:
            self.callbacks["stats"](dict(counters))
        if "finished" in self.callbacks:
            self.callbacks["finished"]()
Loops over files and calculates spam scores.Supports include/exclude word filters and optional regex.Reports progress and statistics via callbacks for the GUI.GUI: Setup with ttkbootstrapWeâ€™ll use ttkbootstrap for a modern dark-themed GUI.import tkinter as tk
import ttkbootstrap as tb
from ttkbootstrap.constants import *

class SpamSentinelApp:
    APP_NAME = "SpamSentinel"
    APP_VERSION = "1.0"

    def __init__(self):
        self.root = tb.Window(themename="darkly")
        self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
        self.root.minsize(1200, 700)
        self._build_ui()

    def _build_ui(self):
        main = tb.Frame(self.root, padding=10)
        main.pack(fill=tk.BOTH, expand=True)

        tb.Label(main, text=f"ðŸ§  {self.APP_NAME} - Enterprise Spam Detector",
                 font=("Segoe UI", 22, "bold")).pack(pady=(0, 4))
tb.Window(themename="darkly") creates a dark-themed window.Label displays the app name and description.File selection & drag & dropWe allow users to select folders or drag & drop files:from tkinter import filedialog
try:
    from tkinterdnd2 import TkinterDnD, DND_FILES
    DND_ENABLED = True
except ImportError:
    DND_ENABLED = False

    def browse_files(self):
        folder = filedialog.askdirectory(title="Select Email Folder")
        if folder:
            print("Folder selected:", folder)
TkinterDnD enables drag & drop.filedialog.askdirectory allows folder selection.We connect buttons to start scanning and update the GUI with results:def start_scan(self):
    selected_files = ["emails/test1.eml", "emails/test2.txt"]  # Example
    min_conf = 50
    self.worker_obj = SpamWorker(
        selected_files,
        min_conf,
        include_words=[],
        exclude_words=[],
        regex_pattern="",
        max_results=1000,
        callbacks={
            "found": lambda f,s: print(f"{f} -> {s}%"),
            "progress": lambda p: print(f"Progress: {p}%"),
            "stats": lambda stats: print(stats),
            "finished": lambda: print("Scan finished")
        }
    )
    threading.Thread(target=self.worker_obj.run, daemon=True).start()
Uses a separate thread to avoid freezing the GUI.Updates are sent back to the GUI via callbacks.We allow users to save selected results:def export_results(self, results):
    path = filedialog.asksaveasfilename(defaultextension=".txt")
    if path:
        with open(path, "w", encoding="utf-8") as f:
            for r in results:
                f.write(f"{r[0]} | Spam Score: {r[1]}%\n")
        print("Export completed!")
Saves selected files with their spam score to a text file.Finally, run the application:if __name__ == "__main__":
    app = SpamSentinelApp()
    app.root.mainloop()
Users can now drag & drop emails, scan them, and export results.This structure makes it beginner-friendly, because each part is explained and separated into small, digestible steps with code blocks.]]></content:encoded></item><item><title>PyBites: The missing 66% of your skillset</title><link>https://pybit.es/articles/the-missing-66-of-your-skillset/</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 11:50:15 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Bob and I have spent many years as Python devs, and 6 years coaching with Pybites and we can safely say thatÂ being a Senior Developer is only about 1/3 Python knowledge.The other 60% is the ecosystem. Itâ€™s the tooling. Itâ€™s all of the techÂ Â Python that makes you stand out from the rest.This is the biggest blind spot keeping developers stuck in Tutorial Hell. You spend hours memorising obscure library features, but you crumble when asked to configure a CI/CD pipeline. (Thatâ€™s not just made up by the way â€“ many of you in dev roles will have seen this with colleagues at some point or another!)These are the elements of the Python ecosystem you should absolutely be building experience with if you want to move from being a scripter to an engineer:Â Stop using pip freeze. Look atÂ .Â Not just add/commit. Learn branching strategies and how to fix a merge conflict without panicking.Â print() is not a test. LearnÂ Â and how to write good tests.Â Set upÂ Â (Ruff) so you stop arguing about formatting, andÂ Â for type checking.Â LearnÂ Â (CI/CD). Make the robots run your tests for you.Â How does your code get to a server? Learn basicÂ Â and Cloud.Â Stop clicking buttons and get comfortable in the terminal. LearnÂ Â and create a make install or make test command to save your sanity.It looks like a lot. ItÂ Â a lot. But this is the difference between a hobbyist and a professional.Does this make you feel overwhelmed? Or does it give you a roadmap of what to do this year?Iâ€™m curious! Feel free to hit me up in the Community with your thoughts.And yes, these are all things we coach people on in PDM. Use the link below to have a chat.]]></content:encoded></item><item><title>Data Science Courses: A Practical Path to High-Growth Tech Careers in 2026</title><link>https://dev.to/atharvaa_9ccd293bc5f536cd/data-science-courses-a-practical-path-to-high-growth-tech-careers-in-2026-1l2j</link><author>Atharvaa</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 11:46:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Data Science has come a long way from being a buzzword. Today it is at the very center of decision making in various sectors, including finance, healthcare, retail, logistics and the government. As more and more organizations start using data to inform their strategies, the need for data professionals is rising at a breakneck pace.For those looking to transition into the field, or even students and working professionals, joining a proper Data Science course has become one of the most effective ways to get into
this high-growth industry.Why Data Science Skills Are in High DemandContemporary organizations produce enormous amounts of data on a daily basis. But data, in its raw form, is of very little use until it can be analyzed, interpreted and converted into valuable reports. This is where data scientists come into the picture.Organizations seek individuals with the skills such asAbility to analyze large data sets using Python, SQL and statistical techniquesAbility to develop predictive models using machine learning algorithmsAbility to create data visualizations using tools such as Power BI or TableauAbility to inform business decisions using data driven reportsData science jobs are among the top paying and fastest growing tech jobs in the world, according to global hiring trends, especially in the Middle East and GCC countries, where the digital transformation is taking place at a rapid pace. Not all Data Science courses are the same. A good and industry aligned course should aim at both theory and practical.The essential parts of a Data Science course should include -Programming & Data HandlingSQL for database analysisData cleaning and preprocessing methodsProbability and statisticsSupervised and unsupervised learningIntroduction to AI basicsData Visualization & Business InsightsPower BI, Tableau or MatplotlibTurning data into business insightsReal world case studies of business problemsCapstone projects to create a strong portfolioCourses that include real-world projects, guidance and career services are likely to provide much better results than courses that are purely theoretical.Careers in Data Science: Whatâ€™s Out There?Once you finish a Data Science course, you can consider various options for a career, like:Business Intelligence AnalystMachine Learning Engineer (entry-level)In countries such as Bahrain and the GCC, there is a high demand in areas like banking, fintech, telecom, oil & gas and government projects.Selecting the Right Training PartnerWhile choosing a Data Science training program, the following factors should be kept in mind by the learners:Industry relevant curriculumInstructors with practical experiencePractical assignments and resourcesPlacement or staffing assistanceFlexible learning solutions (online or hybrid)Training partners that offer technical education along with industry exposure and staffing assistance can help fill the gap between learning and employment, which is a challenge many learners face]]></content:encoded></item><item><title>ðŸ§  Ensembleæˆ¦ç•¥</title><link>https://dev.to/stklen/ensemblezhan-lue-443j</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 11:00:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[äººé–“ã®ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚ˆã†ã«ã€ ã¯è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’å”åŠ›ã•ã›ã€ã‚ˆã‚Šæ­£ç¢ºãªçµæžœã‚’å°Žãå‡ºã—ã¾ã™ã€‚
  
  
  æˆ¦ç•¥2ï¼šä¿¡é ¼åº¦åŠ é‡ (Weighted Confidence)
å…¥åŠ›ç”»åƒ
    â”‚
    â”œâ”€â†’ ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ« (Unified_v18) â”€â”€â†’ äºˆæ¸¬ + ä¿¡é ¼åº¦
    â”‚
    â””â”€â†’ æ¤œè¨¼ãƒ¢ãƒ‡ãƒ« (Inc_v201) â”€â”€â†’ äºˆæ¸¬ + ä¿¡é ¼åº¦
    â”‚
    â†“
Ensembleæ±ºå®šã‚¨ãƒ³ã‚¸ãƒ³
    â”‚
    â†“
æœ€çµ‚çµæžœ
ï¼šEnsemble + æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ãŒæœ€ã‚‚åŠ¹æžœçš„ï¼ï¼šç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼šEnsembleã¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé…ã„ï¼šå®Ÿéš›ã®çŠ¶æ³ã«å¿œã˜ã¦ä¿¡é ¼åº¦é–¾å€¤ã‚’èª¿æ•´]]></content:encoded></item><item><title>Stop Teaching Students to Pass Exams; Start Teaching Them to Ship</title><link>https://dev.to/iteyonikservices/stop-teaching-students-to-pass-exams-start-teaching-them-to-ship-ldi</link><author>ITE Yonik Services</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 10:35:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Weâ€™ve spent too many afternoons in Placement Cells watching students with a 9.5 CGPA freeze up when asked to explain a simple if-else logic or a Python decorator. Itâ€™s the "Paper vs. Power" gap, and in 2026, itâ€™s getting wider.The Reality Check A degree proves you can learn; a certification proves you can do. When a student completes a deep dive into Data Analytics or Python, they stop being a "student" and start being a "specialist."Zero Babysitting: Recruiters are tired of "training" freshers for 6 months. They want "Day One" productivity.Project Confidence: A certificate is "Risk Insurance." It tells the team that youâ€™ve wrestled with real codeâ€”and won.The Theory Paradox: Weâ€™ve found that students actually get better grades in Data Structures once they start a hands-on DSA certification. The "Why" finally explains the "How."At Opportunity Near Me, weâ€™re running 15-day professional sprints that treat students like junior devs, not kids in a classroom. We focus on logic first, then the stack (AI, Python, Data).Bottom line: Letâ€™s stop sending "learners" into interviews and start sending "solvers."]]></content:encoded></item><item><title>Speed Up Your Django App: A Beginner&apos;s Guide to Redis Caching</title><link>https://dev.to/ajitkumar/speed-up-your-django-app-a-beginners-guide-to-redis-caching-23p7</link><author>Ajit Kumar</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 10:10:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Youâ€™ve built a Django app. It works. But as your database grows, those once-snappy pages are starting to feel... sluggish. You check your logs and see a query taking 2 seconds to load.Your first thought might be: "I need a bigger server."
Your second thought (the correct one) should be: In this guide, weâ€™ll walk through why caching matters, how to set it up with Redis, andâ€”most importantlyâ€”how to prove itâ€™s actually working.Imagine you are a librarian. Someone asks you for a complex report on 19th-century architecture. You spend 20 minutes walking to the back of the library, climbing a ladder, and finding the book.If ten more people ask for that same report in the next hour, would you walk back every time? No. Youâ€™d keep a copy on your desk. While Django supports several backends (Database, File System, Local Memory),  is the industry standard. Itâ€™s an in-memory data store, meaning it lives in your RAM. It is significantly faster than reading from a traditional Disk-based database (PostgreSQL/MySQL).
  
  
  The Setup: Before Caching
Letâ€™s look at a standard Django view that calculates "Trending Products" for an e-commerce dashboard. This query is "expensive" because it involves aggregates and filters over thousands of rows.
  
  
  The "Before" Performance:
 1 per request. ~500ms - 1.5s (depending on DB size). Poor. 100 concurrent users = 100 heavy DB queries.
  
  
  Step 1: Install and Configure Redis
First, you need the Redis server installed on your machine and the Python interface.Update your  to point Django toward Redis:
  
  
  Step 2: Implementing the "Cache-First" Pattern
The most common way to cache is the  pattern. You check the cache; if it's there (a ), return it. If not (a ), fetch from the DB and save it to the cache for next time.Here is our updated view:
  
  
  Step 3: Verifying and Monitoring
This is where many developers trip up. How do you know it's working?Open your terminal and run the Redis monitor tool. This shows every command hitting your Redis server in real-time.Now, refresh your browser. You will see a  command in the monitor. You will see a  command. If you see  followed by another , your logic is broken (Cache Miss).To see exactly what is stored in your Redis database:redis-cli keys Note: Django-redis automatically prepends a version (usually ) to your keys.You can also use your browser's "Network" tab. Compare the  column.
  
  
  Step 4: Pro-Tip - Cache Warming
What if that first user of the day is a VIP? They shouldn't have to wait for the "Cache Miss." We use a â€”a custom Django management command that runs on a schedule (Cron job) to pre-fill the cache.
  
  
  Common Pitfalls to Avoid âš ï¸
If you cache product prices for 24 hours and change a price in the admin panel, the user will see the old price. Use Django "Signals" to clear the cache whenever a model is saved.Django code is loaded into memory by your web server (Gunicorn/Uvicorn). If you change your  name in your code, you must restart Gunicorn. Otherwise, your server will keep looking for the old key while your warming script generates the new one.systemctl restart gunicorn

Don't cache things that are unique to every user (like a shopping cart) unless you include the  in the cache key.Caching is the bridge between a "hobby project" and a "production-ready app." By moving your most frequent, expensive queries into Redis, you reduce the load on your database and provide a lightning-fast experience for your users. Try implementing caching on your slowest API endpoint and use  to watch the magic happen!Have questions about cache invalidation? Drop a comment below!]]></content:encoded></item><item><title>How to Build Quantitative Trading Strategies Using a Free Stock API</title><link>https://dev.to/san_siwu_f08e7c406830469/how-to-build-quantitative-trading-strategies-using-a-free-stock-api-54fo</link><author>San Si wu</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 09:30:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a retail investor with years in the market and plenty of hard-learned lessons, I used to trade stocks purely on gut feeling a couple of years agoâ€”either chasing highs and getting trapped at the peak, or missing the best exit and regretting it later. After messing around for over six months without finding a consistent rhythm, while wasting huge amounts of time and energy, I stumbled upon quantitative trading. Thatâ€™s when I finally realized: â€œLet data speak, let rules disciplineâ€â€”this is the real key to reducing risk and improving efficiency. But at first, I was completely put off by the myths of â€œhigh barriers to quant tradingâ€ and â€œexpensive APIs,â€ until I discovered this free stock API. Thatâ€™s when I truly took my first step into quant trading. Today, Iâ€™m sharing this battle-tested, practical experience with no reservationsâ€”for beginners who are as lost as I once was.Let me be upfront: as a beginner in quant, you donâ€™t need to chase complex machine learning models right away, and you definitely donâ€™t need to spend big money on paid APIs. The free tier is more than enough to build basic strategies, connect real-time data, and run backtests. Only when you have higher needs later (e.g., high-frequency trading or Level 2 depth data) should you consider upgrading to paid plans. This is the most cost-effective and efficient path to getting started with quant trading, based on my own testing.Today weâ€™re keeping it realâ€”no fluff, just my personal hands-on breakdown. Iâ€™ll walk you through using this free stock API to build a simple, beginner-friendly quantitative trading strategy, while avoiding all the pitfalls I hit along the way. Everything here is practical and implementable.The biggest worry for beginners is â€œthe tech is too complicatedâ€ or â€œregistration is a hassle.â€ But iTickâ€™s free tier eliminates those concerns entirelyâ€”it takes just 5 minutes to get set up, zero barrier to entry, and the free version is especially friendly for individual developers and newcomers to quant trading.Go to the iTick website, click â€œGet Startedâ€ in the top right, and register with your personal emailâ€”no complicated personal information required, one-step process.After registering, go straight to the dashboard, find the â€œAPI Managementâ€ section, and youâ€™ll see your API Token. This token is crucialâ€”youâ€™ll need it for real-time data access and all API calls. Save it securely and never share it.Key details on free tier permissions (must-read for beginners): Supports basic real-time quotes for US stocks, Hong Kong stocks, A-shares, and other markets, plus minute-level to daily historical K-line data. Rate limit is 10 requests/minuteâ€”plenty for beginners building simple strategies (e.g., trend following, moving average crossover). Itâ€™s permanently free with no expiration, very beginner-friendly.
  
  
  2. Connecting Real-Time Data
The core of quantitative trading is simple: get market data in real time â†’ apply preset strategy logic â†’ generate trading signals. The very first and most critical step is real-time data accessâ€”if latency is high, your strategy judgments become inaccurate and can lead to unnecessary losses. iTickâ€™s free API supports both RESTful and WebSocket push methods with very low latency (<100 ms for major markets), perfectly suitable for non-ultra-high-frequency strategies and easy for beginners to handle.I used Python throughout (the top choice for beginnersâ€”simple syntax, tons of online resources, quick solutions when you get stuck). Below are ready-to-run code examples. Just replace with your own API Token, copy-paste, and run to get real-time US stock data. Every line has detailed commentsâ€”take your time if something looks unfamiliar.First, install the two required libraries (run in your terminal):pip install requests websocket-client
These are standard Python libraries for API calls and real-time streamingâ€”installation is straightforward.
  
  
  1. RESTful API for Real-Time Quotes
Real-world result: Running this instantly returns Appleâ€™s current price, volume, and percentage change with very low latencyâ€”basically in sync with regular trading apps. Beginners should start with a single stock to get familiar with the data structure before expanding to multiple.
  
  
  2. WebSocket for Real-Time Streaming
If your strategy needs to monitor multiple stocks continuously (e.g., Apple, Tesla, Nvidia), WebSocket is betterâ€”it pushes updates in real time without repeated polling, more efficient and time-saving.
  
  
  3. Building a Real Strategy
Once real-time data is connected, you can start building the actual strategy. Beginners should begin with the classic â€œDual Moving Average Crossoverâ€â€”simple logic, easy to understand, controllable risk, and a favorite starting point for many experienced quant traders. Combined with iTickâ€™s real-time and historical data, itâ€™s quick to implement without advanced coding skills.Quick breakdown of the dual MA strategy (no complex formulasâ€”just understand the idea):Choose two moving averagesâ€”a short-term (e.g., 20-day) and a long-term (e.g., 60-day). Trading signals come from crossovers:Short-term MA crosses above long-term MA (â€œGolden Crossâ€) â†’ bullish trend â†’ Buy signalShort-term MA crosses below long-term MA (â€œDeath Crossâ€) â†’ bearish trend â†’ Sell signalUsing iTick data, the program automatically detects these crossovers, removing emotional biasâ€”this is the biggest advantage of quant: rational, disciplined, no chasing or panic selling.Get historical data: Pull a stockâ€™s historical K-lines via iTick API (e.g., last ~3 years daily data) for backtestingâ€”essential to validate past performance before live use.Calculate MAs: Use Pythonâ€™s talib library to compute 20-day and 60-day simple moving averages.Generate signals: Automatically detect golden/death crosses.Real-time monitoring: Use WebSocket to watch price; alert on signal (beginners: alerts only, no auto-trading).I wrote a lot, but it really boils down to one thing: getting started with quant trading as a beginner doesnâ€™t have to be technically intimidating or expensive. Start with a free stock API and the simplest dual moving average strategyâ€”nail real-time data first, then gradually optimize and simulate live trading. Step by step is far more reliable than trading on emotion or tips.Thatâ€™s exactly how I did itâ€”from not even knowing what an API was to now comfortably building basic quant strategies and monitoring multiple US stocks in real time. Iâ€™m not making huge profits, but Iâ€™ve avoided most of the old emotional traps, and my trading mindset is much calmer. The real essence of quant trading has never been â€œbeating the marketâ€â€”itâ€™s â€œunderstanding the market,â€ using discipline to control your behavior and overcome greed and fear. Thatâ€™s the philosophy Iâ€™ve stuck with.]]></content:encoded></item><item><title>The Death of the DMV Line: How I Found a Driving Test Slot in 48 Hours ðŸš—</title><link>https://dev.to/hypertools/the-death-of-the-dmv-line-how-i-found-a-driving-test-slot-in-48-hours-2mon</link><author>Roberto | Hyper-Tools</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 09:09:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I recently had to renew my license in Nevada. The first available appointment was 87 days away. In a world of real-time everything, why is the DMV still operating like it's 1995?Instead of checking the website 20 times a day, I spent 2 hours writing a Python script to do it for me.
  
  
  The Problem: The Cancellation Lag
The DMV doesn't "release" slots. They just become available when someone cancels. These slots are snatched up by bots or lucky manual refreshers within seconds.
  
  
  The Solution: Local-First Monitoring
Polls the DMV scheduling endpoint every 30 seconds.Filters for specific locations.Triggers a Twilio SMS the moment a slot opens.
I found a slot for the next day within 48 hours of running the script.I'm now expanding this into a suite of "Painkiller" tools for bureaucratic hurdles at .What's the worst government queue you've ever stood in?]]></content:encoded></item><item><title>Just what IS Python, anyway?</title><link>https://dev.to/dimension-zero/just-what-is-python-anyway-7ce</link><author>Dimension AI Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 08:38:33 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[A mental model for understanding Python's roleEvery mainstream language fits a mental slot. C is a systems language. JavaScript is a browser language. Rust is a safety-focused systems language. SQL is a query language.Python doesn't fit. It can be tricky, for an experienced programmer, to grasp what Python actually is and which slot to put it in. Often, they conclude "I don't like Python" and express confusion at its vast popularity in the 2020s.Like the eponymous snake, Python can be hard to pin down â€” and like a real python, it may end up wrapped around your codebase whether you planned it or not.Traditionally we're taught to classify languages along a few axes:scripting vs "real" languagesimperative vs OO vs functionalPython fits poorly into all of them.It isn't a compiled language in the C or Rust sense: it doesn't result in a standalone executable. It supports imperative, object-oriented and functional styles but isn't optimized for any of them. It began as a scripting language, but today it's used to build large, long-running systems.
  
  
  The Key Insight: Python Is Not Defined by Its Output
The turning point is to realize that Python is not defined by the artefact it produces.C, C++, Rust, Zig and Fortran produce binaries that can be directly run. The output is the thing. Once compiled, the language largely disappears from the execution model.Python doesn't work like that.A Python program needs a runtime ecosystem to execute: the interpreter, the object model, the garbage collector and the standard library. These are not incidental. They  Python. A Python program can't run standalone unless all of these components are bundled with it.
  
  
  Python as a Runtime Ecosystem
In structural terms, Python sits alongside other runtime-centric language ecosystems:Java, Scala, Kotlin, Clojure on the JVMThe similarity is architectural role: in all these cases, the runtime is the , not any compiled artefact.Strictly speaking, Python is a language specification with multiple implementations â€” IronPython runs on .NET, Jython on the JVM. But in practice, CPython and its C-API-dependent package ecosystem  Python. That's what made Python popular, and that's what we're discussing here.The differences from .NET and the JVM matter too..NET and the JVM have JIT compilation to native code; CPython does not by default..NET and the JVM enforce static typing as part of the compilation model; Python's type hints are advisory..NET and the JVM produce native distributable artefacts (.dll, .jar) with stable ABIs (Application Binary Interfaces); Python does not, making it more difficult to call. Python prefers to call other libraries rather than be called itself.So Python belongs in the "runtime ecosystem" category, but it's a looser, more dynamic variant; it trades static type-safe guarantees for flexibility and rapid development.This structural similarity doesn't fully explain Python's success â€” Ruby, Perl and PHP share similar characteristics but declined while Python grew. Historical contingency matters: NumPy's timing, Google's investment in TensorFlow, and early academic adoption all played roles that had little to do with language design.
  
  
  Where Python's Nature Is Clearest: Orchestration
Going to back to our question of "What IS Python?", the key is to realize that Python is a runtime-centric language. Its nature is clearest in numerical computing, data engineering and machine learning, where Python orchestrates work rather than performing it.The most important Python librariesâ€”NumPy, SciPy, Pandas, PyTorch, TensorFlowâ€”are not written in Python in any meaningful sense. Python provides the API, the glue and the control flow. The heavy computation happens in C, C++, Fortran or CUDA libraries that expose a C ABI.Python performs the same role over its libraries as:It is an orchestration language sitting above high-performance systems. That's why it thrives in scientific computing, data pipelines and machine learning. You build rapidly and easily, with simple syntax, while the underlying libraries deliver the performance. So long as orchestration overhead is low, Python-based systems can scale surprisingly far.This is the glamorous use case â€” but not necessarily the most common one.
  
  
  That's Not the Whole Story
The orchestration model explains Python's dominance in scientific and data-heavy domains â€” but most Python code written globally is web apps, scripts, automation and data munging where Python  doing the work directly.In web development and business applications (think Django, Flask, FastAPI), Python handles HTTP requests, processes strings and executes business logic. Here, Python trades raw performance for development speed and ecosystem breadth. A Django application will be slower than an equivalent in Go or C#, but it may ship months earlier.For these workloads, the framing is different: Python is a productive general-purpose language that prioritizes developer time over CPU time.Python's popularity is no mystery once you consider this trade-off.Being able to assemble things quickly, in readable code, with vast ecosystem support, matters more for mass adoption than type-safety, compilation speed or raw performance. Make something easy, and more people will do it; make something quick to do, and more people will do it, more often.Python lowers the barrier to entry for proof-of-concept and prototype work. You can validate an idea in hours rather than days. If performance becomes critical later, you can translate hot paths into a compiled languageâ€”but you've already learned what needs building.Getting something working , quickly, turns out to be more important than getting it working fast or elegantly. Shell scripting demonstrated this in the 1970s; Visual Basic and VBA did this in the 1990s; Python demonstrates it today. Make it easy and fast to build, and they will come and build.A note of realism: "rewrite hot paths later" is technically true but economically rare. Most prototypes never get rewritten; they become production systems. This is true of any language, but Python's low barrier to entry means more prototypes get written in the first place â€” and more of them survive into production.
  
  
  When Python Is the Wrong Choice
A fair assessment requires acknowledgement of where Python doesn't belong: â€” Garbage collection pauses are unacceptable when deadlines are measured in microseconds. â€” Neither iOS nor Android use Python as a first-class development language. â€” JavaScript and WebAssembly own this space.Memory-constrained embedded systems â€” Python's runtime overhead is prohibitive on microcontrollers (although MicroPython, a cut-down implementation, has some adoption in this space).Latency-critical network services â€” Where every millisecond matters, Go, Rust or C++ are better choices.CPU-bound pure-Python workloads â€” If you can't offload to native libraries, Python's interpreter speed becomes a genuine bottleneck.These are domains Python doesn't seriously contest. More relevant are the pain points in domains where Python  used: â€” The Global Interpreter Lock limits true parallelism in CPU-bound multithreaded code.Packaging and distribution â€” pip, virtualenv, conda, poetry, and pyproject.toml represent years of fragmented solutions to dependency management. â€” Relative imports, __init__.py behaviour, and module resolution remain sources of confusion. â€” Shipping a Python application to end users without requiring them to install Python remains harder than it should be.Python excels at orchestration, rapid prototyping and domains with strong library support. It is not a universal solution, and it carries real operational costs.
  
  
  Why Python still feels slippery
Even with this framing, Python can still feel oddly unsatisfying if you come from strongly structured languages.Compared with C#/Java or their ecosystems, Python has:a simpler, leakier object modelIf you're used to the discipline of C#, the functional elegance of F# or the precision of Rust, Python can feel vague. Things work â€” until they don't â€” and the language often declines to help you reason about correctness ahead of time.That's a real cost. But as the previous section argues, for many problem domains it's a cost worth paying.
  
  
  Clearing Up Misconceptions

True for CPU-bound pure-Python code. False when Python orchestrates native librariesâ€”NumPy array operations execute at C speed regardless of Python's overhead."Python is a scripting language."
Historically accurate; Python originated as a scripting tool. But "scripting language" now undersells what Python has become.
Misleading. CPython compiles source to bytecode, then executes that bytecode on a virtual machine â€” much like many modern interpreters do. The distinction matters when reasoning about performance and behaviour, but it's an implementation detail rather than a defining characteristic.
  
  
  A Better Language Taxonomy
Python fits comfortably into this three-tier classification:C, C++, Rust, Zig, FortranThe binary is the productThe runtime is the productBash, PowerShell, VBA, LuaThe host environment is the productPython belongs firmly in the second groupâ€”with the caveat that it's a more dynamic, less rigidly structured member than Java or C#.The boundaries are not perfectly clean. Go has garbage collection, a runtime and reflection, yet produces statically-linked binaries â€” it sits at the boundary between the first two categories. Taxonomies are useful simplifications, not natural laws.
  
  
  A brief note for Rust and Go proponents
A common challenge: Python's role is better served by "doing it properly" in a compiled language from the start.That view makes sense if your problem is well-specified, stable, performance-critical and worth committing to upfront architectural constraints. In such cases, Rust or Go can be excellent choices.But many real-world problems do not start that way. They begin as ill-defined, exploratory or fast-moving systems: data pipelines, research code, internal tools, integration glue. A research team needs to test an idea quickly at small scale. A business team needs a tactical solution because the problem won't wait for strategic architecture.In those contexts, using a language with strict typing, memory models or concurrency primitives can frustrate development with language-wrestling, where making the language work becomes centre-stage.Python and compiled languages are therefore not competitors but complements: Python for orchestration and discovery; Rust, Go or C# for stabilised, performance-critical components. Your Python prototype becomes your teacherâ€”clarifying what the real system needs to do.That said, Python's actual competition in most domains isn't Rust or Go â€” it's JavaScript/TypeScript, Ruby, R, and Julia. Python's victory over these closer competitors owes as much to ecosystem momentum and historical timing as to language design.Python isn't confused, incoherent or a "toy" language. It simply departs from the mental models of earlier language generations.Python is a runtime-centric ecosystem that excels at orchestration, rapid prototyping and leveraging high-performance native libraries. It trades static guarantees and raw speed for flexibility, readability and development velocity.That trade-off turns out to be exactly what a large portion of programmers need â€” including many who aren't professional developers at all, but scientists, analysts and business users who need working code fast. It let's you deliver, quickly. And that's what makes Python incredibly useful â€” and wildly popular.]]></content:encoded></item><item><title>Weekly Challenge: Maximum Encryption</title><link>https://dev.to/simongreennet/weekly-challenge-maximum-encryption-49a8</link><author>Simon Green</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 06:49:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Each week Mohammad S. Anwar sends out The Weekly Challenge, a chance for all of us to come up with solutions to two weekly tasks. My solutions are written in Python first, and then converted to Perl. It's a great way for us all to practice some coding.You are given an array of alphanumeric string, .Write a script to find the max value of alphanumeric string in the given array. The numeric representation of the string, if it comprises of digits only otherwise length of the string.This task can be achieved in a single line in both Python and Perl, while still maintaining readability. For the Python solution I use list comprehension to convert each string into an integer (if it is all digits) or the length of string (if it isn't). This is wrapped around the max function to return maximum (largest) of these values.Not to out-shinned, Perl can achieve similar functionality by using the map function, converting each string into its numeric representation or its length. Perl doesn't have a built-in  function, but it is available from the List::Util package../ch-1.py 
123

./ch-1.py 
4

./ch-1.py 
99

./ch-1.py 
10

./ch-1.py 
2026
You are given a string  and an integer $int.Write a script to encrypt the string using the algorithm - for each character  in , replace  with the  character after  in the alphabet, wrapping if needed and return the encrypted string.For this task, I start by setting  (called  in Python as  is a reserved word) to be the modulus (remainder) of 26. If that value is , I return the original string as no encryption is required.The next step is creating a mapping table. I start with the variable  that has all the lower case letters of the English alphabet. I create a  string by slicing the  string at the appropriate point. I then double the length of each string by adding the upper case equivalent string. Finally, I use  to convert the strings to a dictionary where the key is the original letter and the value is the new letter.The final step is to loop through each character and use the  dictionary to replace the letter, or use the original character if it is not found (numbers, spaces, punctuation characters, etc).The Perl code follows the same logic. It uses the splice method to create the  variable, and both  and  are arrays. The  function also comes from the List::Util package. Perl will automatically convert a flat list to key/value pairs in the  hash../ch-2.py abc 1
bcd

./ch-2.py xyz 2
zab

./ch-2.py abc 27
bcd

./ch-2.py hello 5
mjqqt

./ch-2.py perl 26
perl
]]></content:encoded></item><item><title>Amazon Listing Traffic Analysis: Building a Real-Time Traffic Attribution System</title><link>https://dev.to/loopsthings/amazon-listing-traffic-analysis-building-a-real-time-traffic-attribution-system-1hh6</link><author>Mox Loop</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 06:17:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When your Amazon listing's sales suddenly double but you can't identify the traffic source, you need a systematic approach to traffic analysis. This guide shows you how to build an automated traffic monitoring system using APIs to track organic rankings, competitor dynamics, and advertising performance in real-time.How to identify Amazon listing traffic sources (organic, PPC, external)Building an automated traffic monitoring system with PythonUsing Pangolinfo Scrape API for real-time Amazon data collectionSetting up anomaly detection and alerts for traffic changes
  
  
  The Problem: Amazon's Traffic Attribution Black Box
A seller recently contacted me with a common problem: their silicone baking mat listing went from 15 daily orders to 35+ orders overnight. They were only running a single automatic campaign with a $30 daily budget, and their advertising report showed no significant change in ad-generated orders. Where did the extra 20 daily orders come from?This scenario highlights a fundamental challenge for Amazon sellers: Amazon's Seller Central doesn't provide clear traffic source attribution. You get aggregate metrics like Sessions and Page Views, but no breakdown of:How much traffic comes from organic search vs. paid adsWhich keywords are driving the most trafficWhether competitor stockouts are sending traffic your wayIf external promotion campaigns are actually workingFor developers building seller tools or data-driven sellers with technical teams, this black box is unacceptable.
  
  
  Understanding Amazon Traffic Sources
Before we dive into the technical solution, let's map out the traffic landscape:
  
  
  1. Organic On-Platform Traffic
: Buyers searching keywords and finding your listingRanking position is everything (page 1 vs. page 2 = 10x traffic difference)Influenced by sales velocity, conversion rate, reviews, and relevance: Best Sellers, New Releases, Movers & ShakersHigh-intent traffic with strong conversion ratesProvides brand exposure beyond keyword searches: "Customers who bought this also bought"Often overlooked but can represent 20-30% of trafficEspecially valuable for complementary products
  
  
  2. Paid On-Platform Traffic
: Appear in search results and product pages: Top-of-search brand ads: Retargeting and audience-based adsSocial media (Facebook, Instagram, TikTok)Deal sites (Slickdeals, Kinja Deals)Independent website referrals
  
  
  Building a Traffic Attribution System
Here's how to build a system that actually tells you where your traffic comes from.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Data Collection Layer                 â”‚
â”‚  (Pangolinfo Scrape API + Amazon Ad API)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Data Storage Layer                    â”‚
â”‚     (PostgreSQL / MongoDB / CSV files)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Analysis & Detection Layer              â”‚
â”‚  (Ranking changes, anomaly detection, alerts)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Visualization & Reporting Layer          â”‚
â”‚      (Dashboard, charts, notifications)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  
  
  Step 1: Data Collection Setup
First, let's set up automated data collection using Python and Pangolinfo's Scrape API.
  
  
  Step 2: Automated Scheduling
Set up a cron job or use a task scheduler to run data collection daily:
  
  
  Step 3: Anomaly Detection
Implement logic to detect significant traffic changes:
  
  
  Step 4: Traffic Source Attribution Logic
Now combine all data sources to attribute traffic:Let's return to the silicone baking mat example. Here's what the data revealed:Keyword rankings for 10 core keywords (daily)Competitor data for 5 main competitors (daily)Advertising data from Amazon Ads API A Facebook promotion one week earlier generated 50 concentrated orders, improving sales velocity and conversion rate, which triggered Amazon's algorithm to boost organic rankings.Increased ad budget to consolidate ranking positionAccelerated inventory replenishmentResult: Sustained 30+ daily orders (50% improvement)
  
  
  Why Pangolinfo for Amazon Data Collection
After testing multiple solutions, here's why Pangolinfo Scrape API stands out:1. High Accuracy for Sponsored Ads98% success rate for SP ad position scrapingCritical for understanding competitive advertising landscapeNo estimation modelsâ€”actual scraped dataMinute-level updates available3. Comprehensive CoverageSearch results (organic + sponsored)RESTful API with clear documentationMultiple output formats (JSON, CSV, HTML)Webhook support for real-time alertsPay per request (no unused features)Much more cost-effective than $3,588/year SaaS tools1. Start Simple, Scale GraduallyBegin with 5-10 core keywordsAdd more metrics as you understand patternsManual data collection doesn't scaleSet up scheduled jobs from day one3. Focus on Actionable MetricsDon't just collect dataâ€”define what actions each metric triggersExample: Ranking drop > 10 positions â†’ increase ad budget4. Combine Multiple Data SourcesAmazon Ads API for advertising dataPangolinfo for organic rankings and competitor dataAmazon Attribution for external traffic5. Build Historical ContextTraffic analysis requires time-series dataCollect data for at least 2-4 weeks before drawing conclusionsAmazon's lack of transparent traffic attribution doesn't have to be a black box. By building a systematic monitoring system with automated data collection, anomaly detection, and attribution logic, you can:Quickly identify traffic source changesRespond to opportunities (ranking improvements, competitor stockouts)Optimize advertising spend based on actual dataMake data-driven decisions instead of guessingThe technical implementation is straightforwardâ€”the real value comes from consistent execution and acting on insights.Have you built your own Amazon traffic monitoring system? What challenges did you face? Share your experiences in the comments! #api #python #ecommerce #amazon #dataanalysis #automation]]></content:encoded></item><item><title>The Death of the DMV Line: How I Found a Driving Test Slot in 48 Hours ðŸš—</title><link>https://dev.to/hypertools/the-death-of-the-dmv-line-how-i-found-a-driving-test-slot-in-48-hours-4d24</link><author>Roberto | Hyper-Tools</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 06:09:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I recently had to renew my license in Nevada. The first available appointment was 87 days away. In a world of real-time everything, why is the DMV still operating like it's 1995?Instead of checking the website 20 times a day, I spent 2 hours writing a Python script to do it for me.
  
  
  The Problem: The Cancellation Lag
The DMV doesn't "release" slots. They just become available when someone cancels. These slots are snatched up by bots or lucky manual refreshers within seconds.
  
  
  The Solution: Local-First Monitoring
Polls the DMV scheduling endpoint every 30 seconds.Filters for specific locations.Triggers a Twilio SMS the moment a slot opens.
I found a slot for the next day within 48 hours of running the script.I'm now expanding this into a suite of "Painkiller" tools for bureaucratic hurdles at .What's the worst government queue you've ever stood in?]]></content:encoded></item><item><title>Building a Production-Ready Portfolio: Phase 3 - Freezing the Product Surface</title><link>https://dev.to/imsushant12/building-a-production-ready-portfolio-phase-3-freezing-the-product-surface-492i</link><author>Sushant Gaurav</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 05:41:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When you build side projects, thereâ€™s a phase that almost nobody talks about.Not the phase where you learn a new framework.
Not the phase where you wire APIs.
Not even the phase where everything finally works.Thereâ€™s a much quieter phase.The phase where you stop building
and start deciding what is allowed to exist.Up to this point in Phase-3, I had a fully functional frontend:
routing worked, pages existed, layout was in place, navigation was responsive, and the system was technically sound.From an engineering perspective, I could have kept adding features endlessly.But from a product perspective, something more important was missing:the product didnâ€™t yet have a .And without a final shape, youâ€™re not building a product â€” youâ€™re just accumulating components.
  
  
  The Moment You Switch From Developer to Product Owner
Day-5 was the first day where I consciously stopped asking:â€œHow do I implement this?â€â€œWhat should this product even be?â€Thatâ€™s a very different mental model.As a developer, youâ€™re rewarded for adding things.
As a product owner, youâ€™re rewarded for .Because every extra option:increases maintenance costincreases surface area for bugsweakens the identity of the systemSo instead of adding, I started .Freezing decisions.
Freezing structure.This is something you only learn after working on real systems:
progress is not always forward â€” sometimes itâ€™s about drawing boundaries.
  
  
  Why I Removed â€œHomeâ€ From Navigation
One of the most deliberate decisions I made was removing the â€œHomeâ€ link from the navigation.On paper, it looks harmless.
In practice, itâ€™s redundant.Users already know three universal rules:home is not a feature, itâ€™s a stateSo keeping â€œHomeâ€ in the navigation was not helping anyone.
It was just occupying attention.And attention is the most expensive resource in any product.By removing it, the navigation became something else:
not a list of pages,.Each item now answers a real user question:Thatâ€™s not navigation anymore.
Thatâ€™s storytelling.
  
  
  Navigation as a Contract, Not a Menu
At this point,  stopped being just a config file.It became a .what the user is allowed to explorewhat Iâ€™m willing to maintain long-termChanging it is no longer a refactor.
Itâ€™s a strategic decision.This is exactly how real systems evolve:
routes become APIs,
contracts become identity.Your navigation is literally your public interface.
  
  
  Branding Is Not Design, Itâ€™s Memory
Adding the logo consistently across the Navbar, Header, and Footer wasnâ€™t a cosmetic improvement.It was a memory-building decision.People donâ€™t remember layouts.
They remember symbols.the product becomes recognizablenavigation becomes intuitivethe site feels â€œrealâ€ instead of â€œpersonal projectâ€This is one of the biggest psychological differences between:
a portfolio that feels like a demo
and one that feels like a product.The logo stops being decoration.
It becomes infrastructure.
  
  
  The Real Work of Day-5: Knowing When to Stop
Day-5 didnâ€™t involve complex algorithms.
It didnâ€™t involve clever abstractions.
It didnâ€™t involve any â€œwowâ€ features.But it involved one of the hardest engineering skills:knowing when the system is coherent enough to freeze.Not perfect.
Not complete.This is where many projects die:
they keep evolving without ever stabilizing,
so nothing ever feels finished,
and everything feels temporary.By finalizing the navigation, footer, and branding,
I wasnâ€™t just polishing UI â€”locking the surface area of the product.new features must fit this structuredesign must respect these boundarieschanges become intentional, not accidentalThatâ€™s the moment a codebase becomes a system.Now, I want to talk about something more important:what this phase does to how other people perceive you.Because at some point, your portfolio stops being a learning tool.
It becomes a signal.A signal to recruiters.
A signal to engineers.
A signal to people who might work with you.And signals are built from decisions, not from code volume.
  
  
  Why This Is Actually Leadership Work
Most developers think leadership looks like:In reality, leadership starts much earlier.making irreversible decisions in uncertain systems.I was saying:
â€œThis is the shape of the system Iâ€™m responsible for.â€Thatâ€™s not coding.
Thatâ€™s ownership.And ownership is the single strongest signal of seniority.
  
  
  The Recruiter Psychology Nobody Talks About
Hereâ€™s a harsh truth most people donâ€™t realize:Recruiters donâ€™t evaluate portfolios like engineers.
They evaluate them like .Does this feel intentional?Does this person make decisions?â€œThis person builds things, but doesnâ€™t finish them.â€limited, clean navigationDay-5 moved the project from the first category to the second.
  
  
  Why Most Portfolios Fail at This Exact Stage
So most portfolios become:They never become .The moment you stop iterating UI endlessly and say:
â€œThis is the experience I want users to have.â€You separate yourself from 95% of developers.
  
  
  Freezing Structure Unlocks Speed
The moment you stop changing structure,
you can move faster than ever.new pages plug into known routescomponents follow known layoutdesign follows known systemdecisions become local, not globalBefore Day-5:
every change risked breaking everything.After Day-5:
every change has a home.This is exactly how large systems scale:
not by adding flexibility,reducing degrees of freedom.
  
  
  The Hidden Career Benefit of This Phase
This phase does something subtle to your personal brand:â€œdeveloper who learns technologiesâ€â€œengineer who designs systemsâ€Because senior roles are not about:how you think about productshow you constrain complexityhow you make irreversible decisionsDay-5 is literally a leadership simulation.You just donâ€™t notice it while doing it.
  
  
  Why This Matters More Than Any Framework
But this skill will not age:knowing when a system is ready to be .Thatâ€™s product thinking.
Thatâ€™s engineering maturity.
Thatâ€™s what people actually hire for.Not your stack.
Not your libraries.If I had to summarize Day-5 in one line:Day-5 is when the project stopped being a collection of pages
and became a product with a defined identity.No new features.
No fancy code.
            Sushant Gaurav
          
              Coming soon!
            Sushant Gaurav Sushant Gaurav. All rights reserved.
      ]]></content:encoded></item><item><title>Cybersecurity Best Practices for Developers: A Practical Guide with Code Examples</title><link>https://dev.to/avsecdongol/cybersecurity-best-practices-for-developers-a-practical-guide-with-code-examples-5ej2</link><author>Abishek Dongol</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 05:32:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Security bugs are expensive. A single SQL injection can expose millions of user records. An XSS vulnerability can compromise user sessions. Let's skip the theory and dive into practical, actionable security practices with real code examples you can use today.
  
  
  1. Authentication: Don't Roll Your Own Crypto
âŒ Bad: Plain Text Passwordsâœ… Good: Hashed Passwords with bcrypt
  
  
  2. SQL Injection: The Classic Vulnerability
âŒ Bad: String Concatenationâœ… Good: Parameterized QueriesORM Examples (Even Safer):
  
  
  3. XSS Prevention: Escape User Input
âŒ Bad: Direct HTML Renderingâœ… Good: Sanitize and EscapeReact (Auto-Escapes by Default):By: Backend Sanitization (Python):
  
  
  4. Secure API Authentication with JWT
âœ… Proper JWT Implementation:
  
  
  5. Secure Session Management
âœ… Express.js Secure Sessions:
  
  
  6. Input Validation: Never Trust User Input
âœ… Comprehensive Validation (Node.js with express-validator):Submitâœ… Safe File Upload Handler:âœ… Rate Limiting Middleware:
  
  
  10. Secrets Management: Never Hardcode Credentials
âœ… Good: Environment Variablesâœ… Essential Security Headers:
  
  
  12. Secure Database Queries: Beyond SQL Injection
âœ… Principle of Least Privilege:Connection String Example:
  
  
  13. Dependency Scanning in CI/CD
âœ… GitHub Actions Security Workflow:
  
  
  14. Logging Security Events (Without Logging Sensitive Data)
Before deploying to production, verify:[ ] All passwords are hashed with bcrypt/Argon2[ ] SQL queries use parameterized statements[ ] User input is validated and sanitized[ ] HTTPS is enforced (no HTTP)[ ] Security headers are set (CSP, HSTS, X-Frame-Options)[ ] CORS is properly configured[ ] Rate limiting is enabled[ ] Sessions use httpOnly, secure, sameSite cookies[ ] File uploads are restricted and validated[ ] No secrets in code (use environment variables)[ ] Dependencies are up to date (npm audit)[ ] Error messages don't leak sensitive info[ ] Logging doesn't include passwords/tokens[ ] Database users have minimal permissions[ ] Authentication endpoints have rate limitingSecurity isn't about being paranoidâ€”it's about being responsible. Every line of code you write is potentially an attack vector. By following these practices and using the code examples above, you'll dramatically reduce your application's attack surface.Remember: Security is not a feature you add at the end. It's a mindset you adopt from day one. Start with secure defaults, validate everything, trust nothing, and always assume your code will be attacked.]]></content:encoded></item><item><title>ðŸ’¡ Discovery: docs(ralph): Auto-publish discovery blog post</title><link>https://dev.to/igorganapolsky/discovery-docsralph-auto-publish-discovery-blog-post-dbp</link><author>Igor Ganapolsky</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 04:02:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Want to add autonomous AI coding to your project?
pip anthropic


python scripts/ralph_loop.py  fix_tests  5  2.00
]]></content:encoded></item><item><title>Problem 11: Count Character Frequency</title><link>https://dev.to/highcenburg/problem-11-count-character-frequency-2ik9</link><author>Vicente G. Reyes</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 04:01:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Today, we're tackling a string manipulation problem: Counting Character Frquency.The goal is to write a function that returns a dictionary with the frequency of each character in a string. The function should be case-insensitive and ignore spaces.char_frequency("hello world") should return {'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1, 'd': 1}Here is the Python implementation:Let's walk through the code line by line:  Defines a function named  that takes one parameter  (a string).  Creates an empty dictionary called  to store the characters and their counts.s = s.lower().replace(" ", "")  Converts the input string  to lowercase using  to ensure case-insensitivity.  Removes all spaces from the string using  so they aren't counted.  Starts a loop that iterates through each character in the processed string .  Checks if the current character  is already a key in the .  If the character is already in the dictionary, increments its count by 1.else: freq_dict[char] = 1  If the character is not in the dictionary (it's the first time we've seen it), adds it to the dictionary with a count of 1.  Returns the final dictionary containing the character frequencies.
  
  
  Example Walkthrough with   Lowercase & Remove Spaces: : Not in dict â†’ : Not in dict â†’ freq_dict = {'h': 1, 'e': 1}: Not in dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 1}: In dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 2}: Not in dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 2, 'o': 1}: Not in dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 2, 'o': 1, 'w': 1}: In dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 2, 'o': 2, 'w': 1}: Not in dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 2, 'o': 2, 'w': 1, 'r': 1}: In dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1}: Not in dict â†’ freq_dict = {'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1, 'd': 1}{'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1, 'd': 1}]]></content:encoded></item><item><title>BSON to JSON: The Python Way</title><link>https://dev.to/letstalkoss/bson-to-json-the-python-way-16d9</link><author>Mario GarcÃ­a</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 03:51:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Through this blog post, you will learn how to convert a BSON document to JSON using Python.If youâ€™re a Python developer, there are two ways for reading a BSON document and converting it to JSON.This is what the script is doing:Import the `decode_all`  and `dumps` methods from the `bson` moduleOpen the file to read the content and decode the dataCreate a JSON file, and write the JSON document created from the data of the BSON fileThe script works with BSON files generated by mongodump. Before running the script, you must install PyMongo: .Connecting to the database and querying the data with PyMongo, the Python driver for MongoDB.
This is what the script is doing:Import the `MongoClient` method from the `pymongo` library, and the `dumps` method from the `bson` moduleEstablish the connection to the databaseSet the database (e.g., `company` ) and the collection (e.g., `employees`) you want to queryRetrieve the documents in the collection with the `find()`  method and create a list with the result. If you donâ€™t pass any parameter to this method, the result will be similar to `SELECT *`  in MySQLCreate a JSON object by calling the `dumps` method. The `indent = 2` parameter will tell `dumps()` to pretty format the JSON objectWrite the content of the `json_data`  variable to the `data.json` fileBefore running the script, you must install PyMongo: .If youâ€™re a developer, you can use the MongoDB driver of your programming language of choice and query the data to analyze the content of the collections in your database. For Python, you can install PyMongo, connect to the database, query the data and use the bson module to save the content as a JSON document.]]></content:encoded></item><item><title>ðŸ›¤ï¸ Beginner-Friendly Guide &apos;Minimum Cost Path with Edge Reversals&apos; - LeetCode 3650 (C++, Python, JavaScript)</title><link>https://dev.to/om_shree_0709/beginner-friendly-guide-minimum-cost-path-with-edge-reversals-leetcode-3650-c-python-3ebl</link><author>Om Shree</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 03:29:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Navigating a graph is usually a one-way street, but what if you could briefly flip the direction of traffic to reach your destination? This problem challenges us to find the most efficient route when we have the special ability to reverse edges at a specific cost. It is a fantastic way to learn how to adapt classic shortest-path algorithms to handle unconventional rules.A directed graph with  nodes and a list of weighted edges.A special "switch" at each node that allows you to reverse  incoming edge once you arrive there.A cost rule: traversing a normal edge costs , while traversing a reversed edge costs .Find the minimum total cost to travel from node 0 to node . If the destination is unreachable, return -1. n = 4, edges = [[0,1,3],[3,1,1],[2,3,4],[0,2,2]]Use the path 0 â†’ 1 (cost 3).
At node 1 reverse the original edge 3 â†’ 1 into 1 â†’ 3 and traverse it at cost 2 * 1 = 2. n = 4, edges = [[0,2,1],[2,1,1],[1,3,1],[2,3,3]]No reversal is needed. Take the path 0 â†’ 2 (cost 1), then 2 â†’ 1 (cost 1), then 1 â†’ 3 (cost 1).
Total cost is 1 + 1 + 1 = 3.
  
  
  Intuition: The "Ghost" Edge Strategy
At first glance, the rule about "reversing an edge only once upon arrival" sounds like we need to keep track of a lot of state. however, because we can only use the reversal  to move to the next node, we can simplify our thinking.Essentially, every original directed edge  with weight  provides two possibilities:Move from  to  normally for cost .If we are at , we can use the switch to turn the edge into  for cost .By adding these "reverse options" as additional edges into our graph from the start, we transform the problem into a standard shortest-path search. Since all weights are non-negative,  is the perfect tool for the job.
  
  
  Walkthrough: Understanding the Examples
Nodes: 4, Edges: [[0,1,3],[3,1,1],[2,3,4],[0,2,2]]Possible moves from node 0:To node 1 (cost 3) or node 2 (cost 2).At node 1, we see an incoming edge from node 3 ( weight 1). We reverse it to go  for cost .At node 2, there are no incoming edges to reverse that help us get closer to node 3.The path  is the cheapest. Output: 5. Sometimes, the best way to handle a "special move" is to represent it as a new type of edge in your graph. When finding the shortest path in a graph with non-negative weights, Dijkstra's algorithm provides an efficient complexity of . By recognizing that the "reversal" is used immediately, we avoid complex DP states or tracking if a switch was used globally.This problem is a classic example of how interviewers take a standard algorithm (Dijkstra) and add a "twist" to see if you can adapt. In real-world software engineering, this logic mirrors how routing engines work. For example, in a logistics system, a truck might usually take a highway, but under certain conditions, it might take a service road at a higher cost in fuel or time. Modeling these "conditional" paths is key to building robust navigation and optimization systems.]]></content:encoded></item><item><title>ðŸ§  Ensembleç­–ç•¥</title><link>https://dev.to/stklen/ensemblece-lue-431k</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 03:00:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[å°±åƒäººé¡žåœ˜éšŠåˆä½œä¸€æ¨£ï¼Œ è®“å¤šå€‹ AI æ¨¡åž‹ä¸€èµ·å·¥ä½œï¼Œç¶œåˆåˆ¤æ–·å¾—å‡ºæ›´æº–ç¢ºçš„çµæžœã€‚
  
  
  ç­–ç•¥ 2ï¼šä¿¡å¿ƒåº¦åŠ æ¬Š (Weighted Confidence)
è¼¸å…¥åœ–ç‰‡
    â”‚
    â”œâ”€â†’ ä¸»æ¨¡åž‹ (Unified_v18) â”€â”€â†’ é æ¸¬ + ä¿¡å¿ƒåº¦
    â”‚
    â””â”€â†’ é©—è­‰æ¨¡åž‹ (Inc_v201) â”€â”€â†’ é æ¸¬ + ä¿¡å¿ƒåº¦
    â”‚
    â†“
Ensemble æ±ºç­–å¼•æ“Ž
    â”‚
    â†“
æœ€çµ‚çµæžœ
ï¼šEnsemble æ¯”å–®æ¨¡åž‹æ…¢ï¼Œéœ€è¦è€ƒæ…®ä½¿ç”¨å ´æ™¯]]></content:encoded></item><item><title>I Built a Desktop Watermarking Tool in Python (WatermarkX)</title><link>https://dev.to/matetechnologie/i-built-a-desktop-watermarking-tool-in-python-watermarkx-11h2</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 02:25:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I recently built WatermarkX, a lightweight desktop app for batch image watermarking using Python.What started as a small internal tool for my own content workflow turned into a full GUI application with drag & drop, live progress tracking, and repeat diagonal watermarks.In this post, Iâ€™ll walk through what it does, how itâ€™s built, and some lessons learned.WatermarkX is a local desktop application that lets you apply text and logo watermarks to multiple images at once.Text watermarks with rotation, opacity, font size, and stroke/outlineRepeat diagonal (tiled) watermark modePNG logo watermark support (with transparency)Drag & drop image loadingBatch processing with progress bar, ETA, and speedEverything runs locally â€” no uploads, no accountsThe app is written entirely in Python:Tkinter + ttkbootstrap â€“ GUItkinterdnd2 â€“ drag & drop supportPillow (PIL) â€“ image processingthreading â€“ background processing so the UI stays responsiveThe core watermarking logic uses Pillowâ€™s ImageDraw and ImageFont, creating a rotated RGBA text layer that gets composited onto each image.For tiled watermarks, I generate a single rotated watermark layer and then paste it repeatedly across the canvas with calculated spacing.Everything runs locally on your machine.ðŸ”§ Interesting Implementation DetailsTo avoid cropped text after rotation, I first measure the text using textbbox, add margins, render it onto a transparent canvas, and only then rotate:bbox = d.textbbox((0,0), text, font=font, stroke_width=stroke)This ensures the watermark never gets clipped.Diagonal repeat watermarkingInstead of rotating the entire image, I rotate just the watermark and tile it diagonally across the image:Create watermark layer onceCalculate spacing from watermark dimensionsPaste in nested loops across the canvasThis keeps things fast even for large batches.Processing runs in a background thread so the GUI stays responsive:Progress bar updates per imageETA is calculated from elapsed timeSpeed shown as images/secStop button sets a shared flag checked inside the processing loopThe app is packaged as a standalone desktop executable so users donâ€™t need Python installed.Handling resource paths (_MEIPASS)Making sure Pillow + Tkinter play nicely in production buildsDesktop Python distribution is still a bit rough around the edges, but totally doable.I frequently need to watermark screenshots and marketing images, and existing tools were either:Web-based (slow + privacy concerns)So I built exactly what I needed: fast, offline, and simple.After friends started asking for it, I decided to polish it and release it.If youâ€™d like to check it out:Feedback, feature ideas, and code suggestions are very welcome.This was a fun reminder that Python is still great for shipping real desktop tools â€” not just scripts and APIs.If youâ€™re thinking about building your own GUI utilities: just start. Tkinter + Pillow can take you surprisingly far.]]></content:encoded></item><item><title>Data Engineering ZoomCamp Module 1 Notes Part 2</title><link>https://dev.to/abdelrahman_adnan/data-engineering-zoomcamp-module-1-notes-part-2-5871</link><author>Abdelrahman Adnan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 01:20:18 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Part 4: Data Ingestion with Python
We're going to load the NYC Taxi dataset into Postgres.pip pandas sqlalchemy psycopg2-binary jupyter
uv add pandas sqlalchemy psycopg2-binary
uv add  jupyter
We use the NYC Taxi trip data. Download it:wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz

  
  
  Loading Data into Postgres
Here's the basic approach: prevents loading the whole file into memory creates the table (first time) adds rows (subsequent chunks)Running multiple  commands is annoying. Docker Compose lets you define everything in one file.Create :docker-compose up      
docker-compose up 
docker-compose down    
docker-compose down Docker Compose automatically creates a network so containers can talk to each other using their service names (e.g.,  instead of ).
  
  
  Connecting to Postgres from pgAdmin
Open  in browserLogin with the email/password from docker-composeRight-click Servers > Create > ServerName it whatever you wantUnder Connection tab:

Host:  (the service name, not localhost!)Quick review of SQL queries we'll use a lot.There are two ways to write an INNER JOIN:
  
  
  GROUP BY and Aggregations
Find values not in lookup table:Terraform is Infrastructure as Code (IaC). Instead of clicking around in a cloud console, you write config files describing what you want, and Terraform creates it.Version control your infrastructureReproducible environmentsEasy to replicate across dev/staging/productionWorks with AWS, GCP, Azure, and many moreCreate a Google Cloud account (free tier gives you $300 credits)Create a service account:

Go to IAM & Admin > Service AccountsCreate new service accountGive it these roles: Storage Admin, BigQuery AdminDownload the JSON key fileSet the environment variable:
 - main configuration - variable definitions
terraform init


terraform plan


terraform apply


terraform destroy
For auto-approving (skips confirmation):terraform apply 
terraform destroy  - don't ask for confirmation - pass variables - use a variables file
docker container prune


docker image prune


docker volume prune


docker system prune If a port is already in use:
lsof  :5432

netstat  | 5432
When containers need to talk to each other:In Docker Compose: use service names as hostnamesManual setup: create a network with docker network create my_network
docker run my_network container1 ...
docker run my_network container2 ...
 - containerization for reproducible environments - relational database running in Docker - loading data with Python/pandas/SQLAlchemy - orchestrating multiple containers - querying and aggregating data - infrastructure as code for GCPThe main takeaway: these tools help you build reproducible, scalable data pipelines. Docker ensures your code runs the same everywhere, and Terraform ensures your infrastructure is consistent and version-controlled.]]></content:encoded></item><item><title>Stop Guessing Your Macros: Building a High-Precision Calorie Tracker with SAM &amp; GPT-4o ðŸ¥—ðŸš€</title><link>https://dev.to/wellallytech/stop-guessing-your-macros-building-a-high-precision-calorie-tracker-with-sam-gpt-4o-32gm</link><author>wellallyTech</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 01:20:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We've all been there. You take a photo of your lunch, upload it to a fitness app, and it tells you your "Chicken Caesar Salad" is 300 calories. But waitâ€”did it account for the extra parmesan? The croutons? The hidden lake of dressing at the bottom? Most current  apps fail because they treat a meal as a single, flat object. To get truly high-precision calorie estimation, we need to move from "image-level" classification to "instance-level" understanding. In this tutorial, weâ€™re going to build a cutting-edge  pipeline using Metaâ€™s Segment Anything Model (SAM) for precise food segmentation and  for granular nutritional analysis. This is the future of  in health tech.To achieve granular precision, our pipeline doesn't just "look" at the photo. It segments the plate into individual components, analyzes them separately, and then aggregates the data.graph TD
    A[React Native App] -->|Upload Photo| B[FastAPI Backend]
    B --> C[SAM: Instance Segmentation]
    C -->|Segmented Masks| D[Image Cropping & Preprocessing]
    D -->|Individual Food Items| E[GPT-4o Vision API]
    E -->|JSON: Macros & Weight Est.| F[Post-processing & Aggregation]
    F -->|Detailed Report| G[User Dashboard]

    style E fill:#f96,stroke:#333,stroke-width:2px
    style C fill:#69f,stroke:#333,stroke-width:2px
SAM (Segment Anything Model): Perfect for identifying boundaries of overlapping food items (e.g., beans over rice).: Currently the gold standard for  reasoning. It can estimate volume and density better than smaller specialized models.: For high-performance, asynchronous processing of heavy vision tasks.
  
  
  ðŸ› ï¸ Step 1: Segmenting the Plate with SAM
First, we need to isolate the components. Using , we can generate masks for every distinct object on the plate.
  
  
  ðŸ§  Step 2: Granular Inference with GPT-4o
Once we have the masks, we crop the original image to focus on specific ingredients. We then send these crops (or the whole image with highlighted segments) to GPT-4o using  for structured output.ðŸ’¡ : For production-grade AI patterns like this, I highly recommend checking out the deep dives over at wellally.tech/blog. They have some incredible resources on scaling Vision-Language Models (VLM) that helped shape this implementation.
  
  
  ðŸ“± Step 3: The FastAPI Glue
Now, let's wrap this in a  endpoint. We'll handle the image upload from our  frontend, run the SAM + GPT-4o pipeline, and return the structured data.
  
  
  ðŸŽ¨ Step 4: React Native UI (The User Experience)
On the mobile side, we want to show the user exactly what the AI sees. By overlaying the SAM masks back onto the camera view, we build  through transparency.Standard AI vision sees "a plate of food." 
This  sees:: 150g Grilled Chicken (31g Protein): 100g Avocado (15g Fat): 50g Quinoa (10g Carbs)By combining  with , we reduce the "hallucination" of calories. For those looking to dive deeper into advanced Vision-Language orchestration and production deployment strategies, I can't recommend wellally.tech/blog enough. Itâ€™s a goldmine for anyone building at the intersection of AI and healthcare.Building high-precision health tools requires moving beyond basic APIs. By chaining models like SAM and GPT-4o, we create a system that understands the physical world with much higher fidelity. What are you building with GPT-4o? Drop a comment below! Let's chat about the future of Multimodal AI! ðŸ¥‘ðŸ’»]]></content:encoded></item><item><title>Multimodal RAG in Action: Building a Skin Health Assistant with CLIP and Milvus</title><link>https://dev.to/beck_moulton/multimodal-rag-in-action-building-a-skin-health-assistant-with-clip-and-milvus-29o4</link><author>Beck_Moulton</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 00:45:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the world of AI, we've moved far beyond simple text-based search. But when it comes to healthcare, "text-only" doesn't cut it. Imagine a patient describing a mole: "It's itchy and dark." Thatâ€™s helpful, but a high-resolution photo is worth a thousand tokens. Today, we are diving deep into Multimodal RAG (Retrieval-Augmented Generation). Weâ€™ll build a Decision Support System that fuses  with family medical history (text) using a unified vector space. We are talking about leveraging , , and  to bridge the gap between pixels and pathology.Ready to build the future of digital health? Let's get cooking! ðŸš€
  
  
  The Architecture: Bridging Visuals and Verbiage
Traditional RAG systems usually handle text via embeddings like . However, for skin health, we need a "shared brain" that understands both images and text. This is where CLIP (Contrastive Language-Image Pre-training) comes in. CLIP allows us to project both images and text into the same high-dimensional space. If a photo looks like "melanoma," its vector will be physically close to the text "melanoma" in our  database.graph TD
    A[User Input: Image + Medical History] --> B{CLIP Encoder}
    B -->|Image Vector| C[Vector Space]
    B -->|Text Vector| C
    C --> D[Milvus Vector DB]
    D -->|Similarity Search| E[Retrieved Medical Cases / Guidelines]
    E --> F[FastAPI Logic Layer]
    F --> G[Decision Support Output]

    style D fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#69f,stroke:#333,stroke-width:2px
To follow this advanced guide, you'll need:: 

 (OpenAI's implementation or HuggingFace ) (The API backbone)
  
  
  Step 1: Setting Up the Multimodal Vector Store (Milvus)
We need a database that can handle high-dimensional vectors at scale. Milvus is the gold standard here. We will define a schema that holds our visual features and the associated clinical metadata.
  
  
  Step 2: The Encoder Logic (CLIP)
We use CLIP to transform both the skin lesion photo and the medical text into the same vector space. This is the "magic" that allows .
  
  
  Step 3: Building the Fusion Retrieval Engine
When a doctor or user submits a new case, we calculate the vector for the new image and the text. We then perform a hybrid search. For a truly production-ready implementation, you should check out the advanced architectural patterns over at WellAlly Tech Blog. They cover how to handle high-concurrency medical data pipelines which is crucial for systems like this.
  
  
  Going Beyond the Basics: The "Official" Way
While this setup gets you a working prototype, building a  system requires much more:: Using a Cross-Encoder to refine search results.: Implementing HIPAA-compliant data handling.: Monitoring if your CLIP model still understands new types of imaging equipment.For a deeper dive into scaling vector databases and orchestrating complex RAG pipelines in the medical domain, I highly recommend reading the engineering deep-dives on the . Their recent pieces on productionizing LLM apps provide the missing link between a "cool demo" and a "deployed product."Weâ€™ve successfully built a Multimodal RAG foundation! By combining the visual power of  with the industrial strength of , we created a system that doesn't just read wordsâ€”it "sees" the patient's condition.The next step? Integrating a Vision-Language Model (like GPT-4o or LLaVA) to generate a final conversational report based on these retrieved "similar cases."What are you building with Multimodal RAG? Let me know in the comments below! ðŸ‘‡]]></content:encoded></item><item><title>RECOVERING STOLEN CRYPTOASSETS: A LEGAL AND STRATEGIC GUIDE, DIGITAL LIGHT SOLUTION (DLS)</title><link>https://dev.to/marco_schooler/recovering-stolen-cryptoassets-a-legal-and-strategic-guide-digital-light-solution-dls-1k59</link><author>Marco Schooler</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 00:13:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Lost cryptocurrency to a scam, hack, or technical error? Youâ€™re not aloneâ€”and your assets may not be gone forever. DIGITAL LIGHT SOLUTION (DLS) specializes in tracing and recovering lost or stolen digital assets using advanced blockchain forensics and cybersecurity expertise. Our experienced recovery specialists investigate phishing scams, hacked wallets, fraudulent platforms, and lost access issues with speed, discretion, and professionalism. With transparent processes and proven recovery experience, DLS helps clients worldwide reclaim control of their digital wealth.DIGITAL LIGHT SOLUTION (DLS) Your Trusted Partner in Ethical Hacking, Digital Investigation, and CybersecurityUnderstanding Crypto Recovery Services?Without this level of specialization, recovery attempts often fail due to irreversible blockchain mechanics or scammers moving funds quickly.
Common Scenarios We Handle at DIGITAL LIGHT SOLUTION (DLS)
Our clients come to us in various distressing situationsâ€”here are some we resolve daily: ( Service Page Version (SEO-Optimized & Conversion-Focused) Crypto Asset Recovery Services)Recover Lost or Stolen Cryptocurrency with (DLS)
DIGITAL LIGHT SOLUTION (DLS) provides professional cryptocurrency recovery services for individuals and businesses affected by scams, hacks, and digital asset loss. Because blockchain transactions are irreversible, expert intervention is often the only path to recovery.Our Crypto Recovery Services Include:
Lost password and seed phrase assistance
Phishing and social engineering scam recovery
Hacked wallet and exchange investigation
Investment fraud and fake platform recovery
Cheating Partner Investigation
Scammed Bitcoin recovery,(D.L.S) With a proven track record, the company has built a strong reputation as a reliable partner for victims of scams, hacks, lost private keys, and fraudulent transactions. DIGITAL LIGHT SOLUTION stands as a trusted name in crypto recovery, delivering expert solutions, professionalism, and peace of mind in an increasingly challenging digital asset landscapeClients benefit from our transparent communication, receiving regular updates throughout the process. Our federal recognition by the CFTC, along with Google certification, validates our ethical practices and robust security protocols. Key reasons to choose (D.L.S) include:Contact (D.L.S) at website (24/7 Support  Te le g r am â€” Digital light solutionWhy Choose DIGITAL LIGHT SOLUTION)?
Extensive Experience and Credentials: Our team holds certifications in blockchain forensics and cybersecurity, with a track record of high-success recoveries. Client Testimonials and Results: Real stories from satisfied clients highlight our reliability and effectiveness.
Transparent Process: We communicate clearly about steps, timelines, and our no-recovery-no-fee approach, keeping you informed every step. Also Specialized in blockchain forensics expertise Transparent, ethical, and secure recovery process..Frequently Asked Questions About Crypto Recovery
How long does recovery take? Timelines vary from 2-5 days, depending on case complexity and cooperation from exchanges or authorities.
Can any cryptocurrency be recovered? Recovery depends on blockchain visibility and transaction details, but most major cryptocurrencies are traceable.
Is scam recovery possible? Yes, with timely action and professional help,If youâ€™ve experienced a crypto-related loss, contacting a trusted recovery specialist like DIGITAL LIGHT SOLUTION (D L S) could make all the difference.]]></content:encoded></item><item><title>Anomaly Detection + LLM: Statistical Rigor Meets AI Insights</title><link>https://dev.to/qvfagundes/anomaly-detection-llm-statistical-rigor-meets-ai-insights-5c3n</link><author>Vinicius Fagundes</author><category>dev</category><category>python</category><category>devto</category><pubDate>Tue, 27 Jan 2026 00:01:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ Z-score with seasonality detects sales anomalies at 90% accuracy. Add Claude to explain them. Same business outcome, way more actionable. 
  
  
  The Problem: Numbers Without Context
You detect an anomaly. Sales in region X dropped 70%. Now what?Without context, anomalies are just noise.
  
  
  Solution: Statistical Detection + LLM Explanation

  
  
  Step 1: Seasonal Z-Score Detection
Naive z-score fails on seasonal data (December looks like an outlier). Account for seasonality: \
                   
  
  
  Step 2: Prepare Anomalies for LLM Analysis

  
  
  Step 3: Generate LLM Insights
iPhone Standard in North America (2023-11)
  Deviation: +128.0%
  Insight: Black Friday/Cyber Monday surge drove exceptional demand. 
  Recommend: Secure additional inventory for Q4 next year.

Samsung Galaxy in Asia Pacific (2023-09)
  Deviation: +157.0%
  Insight: New product launch exceeded projections. 
  Recommend: Analyze features that drove adoption for next release.

Google Pixel in Europe (2024-04)
  Deviation: +174.0%
  Insight: Likely promotional campaign or competitor shortage. 
  Recommend: Plan for normalization in following months.

  
  
  Step 4: Executive Summary
 \
                        Statistical rigor (no false positives) + AI explanation (context) = better decisions. on detection (seasonal z-score) identified in 2-year dataset
 for complete analysis (detect + explain + summarize) (no ML frameworks needed) to Claude (individual insights + summary)You don't need complex ML. You need: (seasonal z-score, not naive) (baseline, deviation, severity) (why + what to do)That's it. Shipped in 2 weeks. Maintained by your analytics team. No ML team needed.What anomalies are hiding in your data right now?]]></content:encoded></item><item><title>Armin Ronacher: Colin and Earendil</title><link>https://lucumr.pocoo.org/2026/1/27/earendil/</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Regular readers of this blog will know that I started a new company.  We have
put out just a tiny bit of information today,
and some keen folks have discovered and reached out by email with many
thoughtful responses.  It has been delightful.Colin and I met here, in Vienna.  We started sharing
coffees, ideas, and lunches, and soon found shared values despite coming from
different backgrounds and different parts of the world.  We are excited about
the future, but weâ€™re equally vigilant of it.  After traveling together a bit,
we decided to plunge into the cold water and start a company together.  We want
to be successful, but we want to do it the right way and we want to be able to
demonstrate that to our kids.Vienna is a city of great history, two million inhabitants and a fascinating
vibe that is nothing like San Francisco.  In fact, Vienna is in many ways the
polar opposite to the Silicon Valley, both in mindset, in opportunity and
approach to life.  Colin comes from San Francisco, and though Iâ€™m Austrian, my
career has been shaped by years working with California companies and people
from there who used my Open Source software.  Vienna is now our shared home.
Despite Austria being so far away from California, it is a place of tinkerers
and troublemakers.  Itâ€™s always good to remind oneself that society consists of
more than just your little bubble.  It also creates the necessary counter
balance to think in these times.The world that is emerging in front of our eyes is one of change.  We
incorporated as a PBC with
a founding charter to craft software and open protocols, strengthen human
agency, bridge division and ignorance and to cultivate lasting joy and
understanding.  Things we believe in deeply.I have dedicated 20 years of my life in one way or another creating Open Source
software.  In the same way as artificial intelligence calls into question the
very nature of my profession and the way we build software, the present day
circumstances are testing society.  Weâ€™re not immune to
these changes and weâ€™re navigating them like everyone else, with a mixture of
excitement and worry.  But we share a belief that right now is the time to stand
true to oneâ€™s values and principles.  We want to take an earnest shot at leaving
the world a better place than we found it.  Rather than reject the changes that
are happening, we look to nudge them towards the right direction.]]></content:encoded></item><item><title>Seth Michael Larson: Use â€œ\A...\zâ€, not â€œ^...$â€ with Python regular expressions</title><link>https://sethmlarson.dev/use-backslash-A-and-z-not-%5E-and-%24-with-python-regular-expressions?utm_campaign=rss</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Two years ago I discovered a potential foot-gun
with the Python standard library â€œâ€ module.
I blogged about this behavior,
and turns out that
I wasn't only one who didn't know this:
The article was #1 on HackerNews and the
most-read article on my blog in 2024.
In short the unexpected behavior is that the pattern â€œâ€ matches both â€œâ€ and â€œâ€,
and sometimes you don't intend to match a trailing newline.This article serves as a follow-up!
Back in 2024
I created a table showing that  was a partially viable
alternative to  for matching end-of-string
without matching a trailing newline... for every regular expression
implementation  Python and EMCAScript. But that is no longer true, Python 3.14 now supports ! This means  is one step closer
to being the universal recommendation to match
the end of string without matching a newline.
Obviously no one is upgrading their Python
version just for this new feature, but it's good to know that
the gap is being closed. Thanks to David Wheeler
for doing deeper research in the OpenSSF Best Practices
WG and publishing this report.Until Python 3.13 is deprecated and long gone: using  (as an alias for ) works fine for Python regular expressions.
Just note that this behavior isn't the same across regular expression
implementations, for example EMCAScript, Golang, and Rust
don't support  and for PHP, Java, and .NET actually
matches trailing newlines!Thanks for keeping RSS alive! â™¥]]></content:encoded></item><item><title>HoloViz: A Major Step Toward Structured, Auditable AI-Driven Data Apps: Lumen AI 1.0</title><link>https://blog.holoviz.org/posts/lumen_1.0/</link><author></author><category>dev</category><category>python</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Local Email Testing with Python and Mailpit</title><link>https://dev.to/letstalkoss/local-email-testing-with-python-and-mailpit-4cn6</link><author>Mario GarcÃ­a</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 23:27:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I'm currently building an app that automates the logistics of tech conferences. It generates certificates of participation for both attendees and speakers and also takes care of sending invitations to prospective presenters. Since it emails multiple recipients, the question arises: in a development environment, how do you test email sending without using real accounts?In this tutorial, you'll learn how to configure a fake SMTP server and run email tests for Python apps.
  
  
  Configure a Local SMTP Server
I'm using Mailpit, an Open Source email testing tool. It can be installed following the instructions in the Installation section of the official repository, or by using Docker.To ensure your data survives a container restart, run the Docker container with a volume to enable persistence:docker run -d \
  --name mailpit \
  -p 1025:1025 \
  -p 8025:8025 \
  -v $(pwd)/mailpit-data:/data \
  axllent/mailpit
The server listens for SMTP traffic on port 1025, while the web-based dashboard is accessible via port 8025.Let's create a script to test our email logic. The script will perform the following tasks: Create a list of random names and emails using FakerConstruct MIME headers (From, To, Subject)Establish and SMTP connection and transmit the dataFirst, we generate a list of participants.from faker import Faker

fake = Faker('en_US')

if __name__ == "__main__":
    participants = [(fake.name(), fake.ascii_company_email()) for _ in range(10)]
The generated data will look like this:Name                      | Email                         
-------------------------------------------------------
Jessica Powell            | ryan40@atkinson.com           
Chelsey Glover            | pstevens@hurst.com            
Sheryl Williams           | kenneth61@williams-jacobson.com
Paula Boyd                | larsontheresa@dean.com        
Maxwell Kelly             | justinestrada@willis.org      
Carl Morrow               | pmorris@cross.biz             
David Webb                | abigailfields@holt.com        
Tyler Wolfe               | williamsanna@martinez.info    
Joshua Medina             | williamsrodney@medina.biz     
Mrs. Donna Butler         | williamsmartin@eaton.com
We use Python's built-in  library to structure the message....
from email.mime.multipart import MIMEMultipart

def send_simple_email(recipient_email, recipient_name):
    SENDER_EMAIL = "hello@name.com"

    msg = MIMEMultipart()
    msg['From'] = SENDER_EMAIL
    msg['To'] = recipient_email
    msg['Subject'] = f"Invitation: {recipient_name}"
We attach the HTML content to our MIME message....
from email.mime.text import MIMEText

def send_simple_email(recipient_email, recipient_name):
    ...

    html_body = f"""
    <html>
        <body style="font-family: sans-serif;">
            <h2 style="color: #2c3e50;">Hello, {recipient_name}!</h2>
            <p>You are formally invited to participate as a speaker at our next event.</p>
            <p>This is a test email captured locally by <strong>Mailpit</strong>.</p>
        </body>
    </html>
    """
    msg.attach(MIMEText(html_body, 'html'))

  
  
  Establish SMTP connection and transmit email data
Finally, we connect to the local Mailpit server and send the message.import smtplib
...

def send_simple_email(recipient_email, recipient_name):
    ...

    SMTP_SERVER = "localhost"
    SMTP_PORT = 1025

    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.send_message(msg)
            return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

if __name__ == "__main__":
    ...
    print(f"\nðŸ“§ Starting email delivery to {len(participants)} recipients...")

    for name, email in participants:
        if send_simple_email(email, name):
            print(f" âœ… Sent: {email}")

    print("\nðŸš€ Check your emails at: http://localhost:8025")
Here is the full implementation:import smtplib
from faker import Faker
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

fake = Faker('en_US')

def send_simple_email(recipient_email, recipient_name):
    SENDER_EMAIL = "hello@name.com"

    msg = MIMEMultipart()
    msg['From'] = SENDER_EMAIL
    msg['To'] = recipient_email
    msg['Subject'] = f"Invitation: {recipient_name}"

    html_body = f"""
    <html>
        <body style="font-family: sans-serif;">
            <h2 style="color: #2c3e50;">Hello, {recipient_name}!</h2>
            <p>You are formally invited to participate as a speaker at our next event.</p>
            <p>This is a test email captured locally by <strong>Mailpit</strong>.</p>
        </body>
    </html>
    """
    msg.attach(MIMEText(html_body, 'html'))

    SMTP_SERVER = "localhost"
    SMTP_PORT = 1025

    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.send_message(msg)
            return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

if __name__ == "__main__":
    participants = [(fake.name(), fake.ascii_company_email()) for _ in range(10)]


    print(f"\nðŸ“§ Starting email delivery to {len(participants)} recipients...")

    for name, email in participants:
        if send_simple_email(email, name):
            print(f" âœ… Sent: {email}")

    print("\nðŸš€ Check your emails at: http://localhost:8025")
After running the script, navigate to http://localhost:8025 in your browser. You will find the Mailpit dashboard with an inbox containing all the successfully intercepted test emails.Now you can safely test email features before deploying to production.]]></content:encoded></item><item><title>I Built an MCP Server That Turns YouTube Videos Into AI Skills</title><link>https://dev.to/tauanbinato/i-built-an-mcp-server-that-turns-youtube-videos-into-ai-skills-2lik</link><author>Tauan Binato</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 23:10:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Ever watched a 30-minute programming tutorial and thought "I wish I could just extract the useful parts"?I built Glean - an MCP server that watches YouTube videos for you and converts them into structured skill files that Claude Code (and other AI assistants) can actually use.You find a great tutorial on YouTubeYou take notes, pause, rewind, pause againA week later, you need that info and can't remember which video it wasYour AI assistant has no idea about that specific technique you learnedGenerate a skill from any YouTube URLSearch YouTube and rank videos by educational qualityAutomatically find the best video on a topic and learn from itThe output is a clean markdown file with concepts, code examples, best practices, and common pitfalls - all extracted from the video.You: "Learn about FastAPI authentication from this video: youtube.com/watch?v=..."

Claude: *extracts transcript*
        *removes sponsor segments, "smash that like button", filler words*
        *structures into a skill file*
        *saves to ~/.claude/skills/*

You: "How do I implement JWT auth in FastAPI?"

Claude: *now actually knows the answer from the skill you just created*
After installing, you can do things like:Search YouTube for "kubernetes networking" videos by TechWorld with Nana
Glean will search, filter by that channel, and rank results by educational value (not just view count).Generate a skill about React Server Components from the best tutorial you can find
Transcript fallback chain: YouTube captions â†’ yt-dlp â†’ Whisper (via Groq): AI removes sponsors, self-promo, engagement bait, filler words: Weighs relevance, educational signals, engagement ratio, channel authority: Search within specific creators' content
git clone https://github.com/tauanbinato/glean.git
glean


uv 
claude mcp add glean your-key your-key  uv run  /path/to/glean glean-mcp
You'll need API keys from Anthropic and Groq (free tier works fine for Whisper).The AI cleaner automatically strips:"This video is sponsored by...""Don't forget to like and subscribe""Link in the description"All the "um", "uh", "basically", "you know"What stays: the actual technical content, code examples, explanations.Model Context Protocol is Anthropic's standard for giving AI assistants new capabilities. Instead of copy-pasting or hoping the model knows something, you can build tools that extend what it can do.Glean is one example. The skills it generates become part of Claude's knowledge for your projects.If you build something cool with it or have ideas for improvements, let me know. PRs welcome.]]></content:encoded></item><item><title>NVIDIA PersonaPlex: The &quot;Full-Duplex&quot; Revolution</title><link>https://dev.to/behruamm/nvidia-personaplex-the-full-duplex-revolution-42jp</link><author>Behram</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 22:31:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I have spent the last month building real-time voice agents. I started with the standard stack: LiveKit and Gemini 2.5.Even though the latency is impressively low, it still feels far from a natural conversation. Talking to these state-of-the-art models usually feels like playing a turn-based video game. I speak. It waits for silence. It thinks. It speaks.This is "Half-Duplex" logic. It is like using a Walkie-Talkie. The system forces you to wait. But real conversation is "Full-Duplex". We interrupt each other. We laugh at the same time. We hum while listening.For the last two days, I have been working with NVIDIA's PersonaPlex (based on Moshi/Mimi). It is completely different. It does not wait for you to stop talking.I looked at the backend code to understand why it feels so different. The secret is in .In standard agents, you have a loop that waits for an "End of Turn" signal. In PersonaPlex, I found this in the  initialization:It is literally "streaming forever." The model processes my voice and its own voice at the same time, 12 times every second. It predicts silence or speech constantly. It does not need "permission" to speak.
  
  
  Realism is Overrated; Rhythm is Everything
Most AI voices feel like "fake meat"â€”they sound human but act robotic. PersonaPlex is different. It trades audio quality for speed.To hit a 240ms reaction time, the audio runs at 24kHz (confirmed in  as ). I run this command on my voice files to match the training environment:ffmpeg  input  24000  1 :a pcm_s16le  output.wav
It is lo-fi, but the rhythm is perfect. The model relies on consistent "Chatterbox TTS" data and learns from "negative-duration silence" during training. This forces it to understand that conversation involves overlapping, not just waiting. It might sound synthetic, but it laughs and interrupts exactly like a human.PersonaPlex separates "how it sounds" from "what it thinks." A 15-second audio clip for acoustics (loaded via ). Instructions for behavior.The system pre-loads the voice to save time (reducing latency). But they must match. You cannot use a calm "Customer Service" voice with an "Angry Pirate" text promptâ€”the model will glitch because the acoustic skeleton fights the semantic brain.To stop it from acting like a boring assistant, use this specific trigger phrase found in the training data (and verified in the server code's system tagging):"You enjoy having a good conversation."Combine this with a high-energy voice sample, and it switches modes. It starts laughing, interrupting, and "vibing" instead of just solving tasks.
  
  
  The Reality Check (Trade-offs)
While the roadmap shows tool-calling is coming next, there are still significant hurdles:: The model has a fixed context window (defined as  frames in ). At 12.5Hz, this translates to roughly 240 seconds of memory. My tests show it often gets unstable around 160 seconds.: Overlapping speech feels natural until it gets buggy. Sometimes the model will just speak over you non-stop.: "Infinite streaming" requires high-end NVIDIA GPUs (A100/H100).: Managing simultaneous audio/text streams is far more complex than standard WebSockets.Despite these issues, PersonaPlex is the first model I have used that feels like a natural customer service agent rather than a text-to-speech bot.Welcome to follow me on Substack as I will release more deep tests and analyses after spending some time with the model.]]></content:encoded></item><item><title>The Surprising Simplicity of Temporal Worker Pools on Cloud Run</title><link>https://dev.to/gbostoen/the-surprising-simplicity-of-temporal-worker-pools-on-cloud-run-14db</link><author>Glenn Bostoen</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 22:17:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you've ever spent an afternoon debugging indentation errors in Google Workflows YAML only to discover the real problem was a cryptic  expression, you'll understand why we made the switch to Temporal. What we didn't expect was just how  the deployment would be.
  
  
  The problem we were solving
Our workflow orchestration setup had all the classic symptoms of YAML-based configuration debt:: Representing simple workflows as graphs required dozens of steps and connector definitions: Every workflow step triggered a Cloud Run job, adding 35-70 seconds of spin-up time per execution: Changes required deployment to validate. The feedback loop was measured in , not seconds: Application code lived in one place, workflow definitions in another, and every change risked breaking bothThe worst part? We couldn't even test locally. Every iteration meant committing, deploying, and hoping.
  
  
  Why Temporal changes everything
Temporal flips the model on its head. Instead of declarative YAML that describes  should happen, you write actual code that describes  it happens:That's it. No separate YAML file. No mysterious DSL. Just code that your IDE understands, your debugger can step through, and your tests can cover. And it runs on  infrastructure. Temporal handles orchestration and state, but the actual work happens on compute you control.
  
  
  The pull-based architecture
Understanding why Temporal workers are different from Cloud Run jobs unlocks the simplicity. are push-based and ephemeral:Something triggers them â†’ they spin up â†’ execute â†’ shut downEach invocation pays the cold start taxNo shared state between executions are pull-based and persistent:Workers run on  infrastructure, not Temporal'sThey maintain a long-polling connection to Temporal for orchestrationThey  tasks when they have capacityWorkers stay warm, eliminating cold startsThis pull-based model is exactly what Google designed Cloud Run Worker Pools for. It's a resource type announced at Google Cloud Next '25 specifically for continuous, non-HTTP, pull-based background processing.
  
  
  Enter Cloud Run worker pools
Worker pools solve a real problem for Temporal deployments. Unlike Cloud Run Services (designed for HTTP workloads) or Jobs (designed for batch tasks), Worker Pools are purpose-built for exactly what Temporal workers do: continuously pull tasks from a queue.Why Worker Pools are perfect for Temporal:No HTTP endpoint required: Workers just poll Temporal. No need to expose ports or manage health check endpoints: No load balancer, no HTTP endpoint overhead, just compute: No public URL means fewer security concerns: Deploy canary releases by allocating percentages of instances to different revisionsThe deployment is even simpler than Services:gcloud beta run worker-pools deploy worker  gcr.io/my-project/worker:latest  europe-west1
No  hacks. No unused HTTP endpoints. Just a container that runs your worker code.: Worker Pools are currently in public preview. For production workloads, you can still use Cloud Run Services with . The architecture is identical, just with a bit more overhead.
  
  
  The deployment is just another container
Here's the mental shift: you're not deploying workflows anymore. You're deploying an application that happens to execute workflows.pip  requirements.txt

Deploy (with your API key stored in Secret Manager):gcloud beta run worker-pools deploy worker  gcr.io/my-project/worker:latest  1  5  1Gi  1  europe-west1 temporal-api-key:latest
That's the entire deployment. No Terraform for workflow definitions. No separate infrastructure repo. Just your application container with workflow logic baked in.Let's be honest about the trade-offs:Before (Google Workflows + Cloud Run Jobs)Cloud Workflows: ~$2-3/monthCloud Run job invocations: ~$35/monthCold start compute waste: ~$40-50/monthAfter (Temporal Cloud + Cloud Run Worker Pools)Temporal Cloud starter: ~â‚¬100/month (orchestration and state only)Worker Pool (2 instances, always-on): ~$18-24/month (your compute, no load balancer or endpoint overhead)Yes, it costs more. But here's what you get:: 2-3 hours/month not fighting YAML: 73-78% faster workflows (no cold starts): Full workflow debugging before deployment: See workflow graphs, execution history, parent-child relationships in real-timeAt a loaded developer cost of â‚¬80/hour, the ROI turns positive immediately.
  
  
  What simplicity actually looks like
Kill a running workflow mid-execution. Restart the worker. The workflow resumes exactly where it left off.That's durability you'd have to build yourself with Google Workflows: tracking state in Cloud Storage, implementing retries, handling partial failures. With Temporal, it's the default behavior.Debug a failing activity with your IDE's debugger. Set breakpoints. Inspect state. Validate fixes locally before deploying.This is what simplicity means: removing the gap between "I think this will work" and "I know this works."The migration path isn't all-or-nothing:: Implement one workflow in Temporal, run both systems in parallel for a week: Compare execution times, reliability, developer experience: Run Temporal workflows in production, keep Google Workflows as fallback: 10% â†’ 50% â†’ 100% with rollback readyWe kept Google Workflows YAML in git history (never delete, just remove from deployment) and maintained the rollback capability for 30 days. We never needed it.The simplicity of Temporal isn't in having fewer moving parts. It's in having the  moving parts. A persistent worker pool on Cloud Run, code-native workflow definitions, and a managed orchestration layer that handles the hard stuff.No more YAML debugging. No more cold start delays. No more "deploy to test" cycles.Just workflows that work.]]></content:encoded></item><item><title>Day-Aware Governance: Why Your Decision Systems Should Know What Day It Is</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/day-aware-governance-why-your-decision-systems-should-know-what-day-it-is-g</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 22:03:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most governance systems in software are temporally blind. They apply the same rules, thresholds, and weights regardless of when a decision is being made. But human organizations have always understood that timing matters -- Monday morning meetings have different energy than Friday afternoon retrospectives.What if we built that understanding into software?I've been working on a token governance system where participants can submit proposals and vote on them. The standard approach: set a quorum threshold, count votes, pass or reject.But here's the insight that changed the design: creative proposals should be easier to pass on Mondays.Why? Because Monday is when people have fresh energy and willingness to experiment. By Friday, risk aversion has accumulated. A governance system that treats Monday and Friday identically is ignoring a real pattern in human (and computational) behavior.The implementation is simple:This is 15 lines of code that encodes a meaningful value system: be bold on Mondays, be careful with money on Fridays.
  
  
  Beyond Days: Temporal Governance Patterns
Once you accept that governance should be time-aware, several patterns emerge:
  
  
  1. Cooldown-Weighted Voting
Recent voters have diminishing influence. If someone voted on 3 proposals today, their 4th vote carries less weight. This prevents governance fatigue and encourages deliberation.
  
  
  2. Seasonal Parameter Drift
System parameters that slowly shift over time. A liability cap that increases by 1% per quarter as the system proves stable. A creativity threshold that opens wider in Q1 (new year energy) and tightens in Q4 (stability focus).
  
  
  3. Circadian Decision Windows
High-stakes decisions can only be made during "peak clarity" hours. No financial proposals processed between midnight and 6 AM. This isn't restriction -- it's acknowledging that decision quality varies with context.A system that remembers its own history. "The last time we changed this parameter was 90 days ago and it went well" vs. "We changed this 3 days ago and haven't measured impact yet." Temporal awareness prevents thrashing.Traditional software treats time as a monotonically increasing counter -- useful for ordering events, nothing more. Day-aware governance treats time as . Monday isn't just "day 1 of the week" -- it's "the creative window." Friday isn't just "day 5" -- it's "the consolidation period."This maps to a broader principle: systems that understand their temporal context make better decisions than systems that don't.We see this everywhere in nature. Circadian rhythms govern hormone production, not because 3 AM is inherently different from 3 PM in some abstract sense, but because organisms that adapted their behavior to temporal patterns outcompeted those that didn't.Software governance has been stuck in the "temporally blind" paradigm. It's time to give our decision systems a clock -- not just for timestamps, but for wisdom.If you want to experiment with this pattern:Start with one temporal rule. The Monday bias for creative proposals is a good first one. The bias factor should be a governance parameter itself -- the system should be able to vote on how much it trusts Mondays. You need data to know if temporal biases actually improve decision quality.Let the system evolve its own schedule. The ultimate goal is a governance system that discovers its own optimal temporal patterns through experience.The code is simple. The philosophy is profound. Your decision systems deserve to know what day it is.Building this as part of a contribution-backed token system. The governance layer is where values meet code.]]></content:encoded></item><item><title>From Web to Vector: Building RAG Pipelines</title><link>https://dev.to/deepak_mishra_35863517037/from-web-to-vector-building-rag-pipelines-25dg</link><author>Lalit Mishra</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 22:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  1. The "Garbage In, Garbage Out" Reality of RAG
In the rush to adopt Generative AI, a dangerous misconception has taken root: that the Large Language Model (LLM) is the magic wand that solves information retrieval. Teams dump thousands of raw PDFs, messy HTML scrapes, and unformatted Confluence pages into a vector database, wire up an embedding model, and expect GPT-4 to act as a perfect oracle.The reality is a chatbot that confidently hallucinates, retrieves navigation footers instead of technical specs, and costs a fortune in token usage because itâ€™s processing thousands of characters of HTML boilerplate for every query.Retrieval Augmented Generation (RAG) is not an AI problem; it is a data engineering problem.The difference between a toy demo and a production RAG system lies almost entirely in the â€”specifically, the rigorous transformation of unstructured web content into semantically dense, highly indexable vector representations. If your vector search returns garbage, your LLM will generate garbage. No amount of prompt engineering can fix a context window filled with  tags and cookie consent banners.This article details the architecture of a high-performance Web-to-Vector pipeline, moving beyond basic tutorials to discuss the engineering trade-offs of cleaning, chunking, and embedding strategies that survive in production.
  
  
  2. The Web-to-Vector Pipeline Architecture
Treating RAG as an ETL (Extract, Transform, Load) workflow allows us to apply standard data engineering rigor to AI systems. The pipeline consists of five distinct stages, each acting as a filter for noise and a multiplier for signal. Reliable fetching of dynamic and static content. Converting raw DOM trees into "Dense Text" or Markdown.Fragmentation (Chunking): The strategic breaking of text into semantic units. Generating embeddings at scale. Storage with metadata strategies for pre-filtering.
  
  
  3. High-Fidelity Scraping: Beyond The first point of failure is often the acquisition layer. Modern websites are complex Single Page Applications (SPAs) laden with hydration scripts, lazy-loaded content, and anti-bot defenses.A raw HTML document is approximately 90% noise relative to an LLM's needs. Classes, IDs, inline styles, script tags, and SVG paths consume embedding dimensions without adding semantic meaning. Feeding raw HTML to an embedding model. The model will waste attention mechanisms on  rather than the core content.HTML-to-Markdown conversion.Markdown is the lingua franca of LLMs. It preserves structural hierarchy (headers, lists, tables) which are critical for semantic understanding, while stripping the presentation layer. Excellent for extracting the main article body and discarding sidebars/navs. It uses heuristics to identify the "center of gravity" of text density. + Custom Heuristics: For specialized sites (e.g., documentation with complex code blocks), you often need to write custom parsers that target specific  divs and preserve  tags while stripping controls.
  
  
  3.2 Dynamic Content Handling
For production systems, a static HTTP request often fails. You need a headless browser cluster (e.g., Playwright or Puppeteer) to render the DOM. Aggressively block resource types. Your scraper does not need to load images, fonts, or stylesheets. Blocking these reduces latency by 60-80% and saves bandwidth costs.
  
  
  4. Chunking Strategies: The Art of Segmentation
Once you have clean, dense Markdown, you face the most critical decision in the pipeline: .How you split your text determines what your retrieval system  find. If you split a question from its answer, no amount of embedding power will reconnect them.
  
  
  4.1 Fixed-Size vs. Semantic Chunking

The naive approach. "Split every 500 tokens, with 50 tokens overlap." Computationally cheap, predictable. Frequently breaks semantic thoughts. A sentence might be cut in half, destroying its vector representation.
This approach uses an embedding model to scan the text sentence-by-sentence. It calculates the cosine similarity between sequential sentences. If the similarity drops below a threshold (a "semantic break"), a new chunk is started. Chunks represent coherent ideas. Retrieval precision increases significantly. Computationally expensive (requires  inference calls).Recursive / Structural Chunking:
The middle ground for RAG. Since we converted our HTML to Markdown, we can leverage the structure. We split by Header 1 (), then Header 2 (), then paragraphs. This guarantees that a chunk respects the document's logical hierarchy. A "Configuration" section stays together; it doesn't bleed into "Installation".
  
  
  5. The Embedding Layer: Production Considerations
Embedding is the compression of meaning into vectors. In a production pipeline, this is not a "set it and forget it" step.
  
  
  5.1 Batching and Throughput
Embedding APIs (like OpenAI's ) or local models (like ) have significant latency. Implement a  architecture. Don't embed chunks one by one. Accumulate chunks into buffers (e.g., 100 chunks) and send them in a single API call / GPU inference pass. This dramatically reduces network overhead and maximizes GPU utilization.This is a silent killer in long-running RAG systems. If OpenAI updates their embedding model, or if you switch from  to , your new vectors will live in a different latent space than your old vectors. Distance calculations between them will be mathematical nonsense. Version your indices. , . Never mix vectors from different models in the same namespace.
  
  
  6. Vector Storage & Retrieval Optimization
The Vector Database (Pinecone, Weaviate, Chroma) is where the rubber meets the road. However, "Similarity Search" (KNN/ANN) is rarely enough on its own.
  
  
  6.1 The Power of Metadata Filtering
In high-scale systems, searching the entire vector space is inefficient and noisy. A user asks "How do I reset my password?" in the context of "Enterprise Application A". The vector search retrieves password reset instructions for "Consumer Application B" because the semantic vectors are nearly identical.. During the scraping phase, you must extract metadata: , , , .vector_search(query_embedding, filter={product_id: "Ent_App_A"}). This restricts the ANN search to a relevant subset, guaranteeing context awareness.
  
  
  6.2 Hybrid Search (The "Keyword" Safety Net)
Vectors are great at concepts ("dog" matches "canine"), but terrible at exact matches (SKUs, error codes, acronyms). Enable . This combines dense vector search with sparse keyword search (BM25). If a user searches for error code , vector search might return generic error pages. BM25 will find the exact document containing that string. A weighted sum of these scores provides the most robust retrieval.
  
  
  7. End-to-End Example: The Documentation Crawler
Let's imagine we are building a RAG system for a fast-changing developer documentation site. An Airflow DAG triggers nightly. Playwright visits the documentation root. It navigates the sitemap. removes the sidebar, the "Was this helpful?" widgets, and the footer. The clean HTML is converted to Markdown. Code blocks are specially tagged to ensure they aren't split mid-function. We use a MarkdownSplitter. We keep headers attached to their child paragraphs so context isn't lost. We generate a hash of the chunk's text content. We check our Vector DB to see if this hash already exists.  prevents re-indexing unchanged content, saving money and time. New chunks are batched and embedded. Vectors are pushed to Pinecone with metadata: , , , .
  
  
  8. Conclusion: Engineering Reliability
Building a demo RAG app takes an afternoon. Building a production RAG pipeline takes engineering discipline. The quality of your AI's answers is directly downstream of the quality of your data pipeline.By focusing on high-fidelity scraping, semantic cleaning, intelligent chunking, and metadata-rich indexing, you move from a stochastic toy to a deterministic system. In the world of LLMs, Data Engineering is the new Prompt Engineering.]]></content:encoded></item><item><title>Day 12 of 100</title><link>https://dev.to/palakhirave/day-12-of-100-2cj4</link><author>Palak Hirave</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 20:45:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Today I learnt about namespaces and global vs local variables. Now I had already stumbled upon this consept during some casual reading so I had a basic idea of what their usecases were. For today's project, I built a Guess the Name game. It's a text based version that allows the user to guess any number from 1 to a 100. If they choose the easy mode they get 10 chances but if they choose the hard mode they get only 5. Unlike yesterday's challenge today's one was fairly easy and I managed to wrap it up in around 30 mins. For this one I didn't write a plan or algorim as I had played this game many times and knew it's simple logistics. import random
import art

random_number = random.randint(1, 101)

print(art.logo)
print("Welcome to the Number Guessing Game")

print("I am thinking of a number between 1 and 100")
level = str(input("Do you want the easy or hard mode? Type 'easy' or 'hard': "))
level = level.lower()

def compare():
    if random_number == guess:
        print("You guessed the number!")
        return 0
    elif random_number > guess:
        print("Too low!")
    elif random_number < guess:
        print("Too high!")
    else:
        print("Please type a valid input")

if level == "easy":
    chance = 10
else:
    chance = 5

while chance > 0:
    guess = int(input("Guess a number between 1 and 100: "))
    chance -= 1

    if compare() == 0:
        chance = 0
        print("Good job!")
        break

    print(f"You have {chance} chances left")

    if chance == 0:
        print(f"You lost, the number was {random_number}. Refresh the page to try again.")

The imported art file was just another file containing an ASCII Art. I had done it though this cool website I found that coverts text into ASCII with a large selection of fonts and styles. ]]></content:encoded></item><item><title>Your AI Agent Doesn&apos;t Need a Database: File-Based State That Actually Scales</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/your-ai-agent-doesnt-need-a-database-file-based-state-that-actually-scales-3cb0</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 19:47:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every tutorial on building AI agents starts with: "First, set up your PostgreSQL database." Then Redis for caching. Then a message queue. Before you've written a single line of agent logic, you're managing three infrastructure components.I've been running autonomous AI agents for months. They maintain state across sessions, hand off context to each other, and track tasks reliably. The entire state layer is JSON files on disk.Here's why that's not crazy â€” and when you should actually reach for a database.
  
  
  The Problem with Database-First Thinking
When you're prototyping an AI agent, your state needs are simple:"What was I doing last session?""What tasks are pending?""What did I learn that I need to remember?"This is a few KB of JSON. Spinning up Postgres for this is like renting a warehouse to store your groceries.Here's the core of what actually works:Nothing magical. No ORM, no migrations, no connection pooling. Just structured JSON that your agent reads at startup and writes before shutdown.
  
  
  Handoffs: The Killer Feature
The real power shows up with  â€” when one agent instance needs to pass context to the next:This solves a real problem: when your agent restarts (crashes, scheduled shutdown, context window fills up), the next instance knows exactly what was happening.
  
  
  Task Tracking Without a Task Database
Your agent can now track what it needs to do, pick the highest-priority task, and mark things done â€” all with zero infrastructure.
  
  
  When You Actually Need a Database
File-based state has real limits. Reach for a database when:Multiple agents write simultaneously â€” file locks get messy fastYou need to query across thousands of records â€” JSON files don't have indexes â€” you want transactional guarantees â€” file I/O starts to feel itFor a single agent running sequentially? Files are simpler, faster to develop, and easier to debug (you can literally  to see what your agent is thinking).This is underrated: when your agent does something weird, you open a JSON file and read it. No query tools, no admin panels. Just:state.json | python  json.tool
You can even version control your state files to track how your agent's behavior evolves over time.Start with files. Add complexity when you hit a real wall, not when a tutorial tells you to. Your agent's first job is to be useful â€” not to have a proper data layer.The best infrastructure is the simplest thing that works. For most AI agents starting out, that's a directory of JSON files.I build and run autonomous AI agent systems. This is based on real production patterns, not theory. If you're building agents, I'd love to hear what state management approach works for you.]]></content:encoded></item><item><title>Weesitor Console: A Console-First Selenium Runner for Authorized QA Checks and Lightweight Monitoring</title><link>https://dev.to/mebularts/weesitor-console-a-console-first-selenium-runner-for-authorized-qa-checks-and-lightweight-e1c</link><author>Mehmet Bulat</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 19:34:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If youâ€™ve ever needed to quickly validate a few user flows, or run a  against a handful of URLs â€” without spinning up a full testing framework â€” you already know the pain:you want Selenium-level fidelity,you want clean output you can review later,and you want it to be simple enough to run from a terminal.Thatâ€™s the gap  is meant to fill: a console-first Selenium runner for , , and , focused on  and producing  (logs, screenshots, summaries).Use this tool only on websites you own, administer, or have explicit permission to test. Respect site terms, robots policies, and local laws.
  
  
  What Weesitor Console is (in plain language)
 runs Chrome (headless or visible) via Selenium, visits one or more URLs, and writes out: (stream-friendly, parseable) (so you can see what happened)a  (counts, timings, failures) (readable output, predictable commands) (config-based runs you can share) (artifacts you can actually inspect)
  
  
  Where this fits (good use-cases)
This tool is a strong fit when you want , not a full-blown test platform:Authorized QA smoke checks after deployments before a client meeting for a few key pages (availability, basic load, error detection) where you want minimal moving partsInvestigating flaky behavior with consistent logging & screenshotsIf youâ€™re a solo dev or a small team, Weesitor is the kind of â€œrun it nowâ€ tool that often saves you from building a bigger system too early.
  
  
  Where this does NOT fit (honest limitations)
Weesitor Console is intentionally conservative and simple. It is :a replacement for structured E2E suites (Playwright/Cypress + assertions + fixtures)If you need complex assertions, a full page-object model, or deep reporting dashboards, youâ€™ll likely outgrow this and should move to a dedicated testing stack.Single URL, multi-URL, or file-based URL list runsHeadless / non-headless modeTimeout controls and robust cleanupOptional proxy support (with or without auth)Session isolation + per-session User-AgentStructured output folders: logs, screenshots, summarypython  venv .venv
.venvcriptsctivate
pip  requirements.txt
python3  venv .venv
 .venv/bin/activate
pip  requirements.txt
python main.py run  https://example.com  30  1 python main.py run  https://example.com  https://example.org  20  2  2

  
  
  4) Run from a URL list file
python main.py run  urls.txt  15  1

  
  
  Reproducible runs: config workflow
If you want a run you can commit to a repo, share with a teammate, or reuse in CI:python main.py init-config  config.json
python main.py run  config.json

  
  
  Output structure (what you get after each run)
By default, artifacts are written under :output/
  logs/
    run_YYYYmmdd_HHMMSS.jsonl
  screenshots/
    error_YYYYmmdd_HHMMSS.png
  summary/
    summary_YYYYmmdd_HHMMSS.json
 are easy to grep, parse, or ingest into your own tooling. turn â€œit brokeâ€ into â€œhereâ€™s what broke.â€The  gives you a fast overview (counts, timings, failures).
  
  
  Responsible use: benefits and risks (the part most posts skip)
Tools like this are powerful, and that comes with responsibility.You can catch obvious breakages quickly (timeouts, redirects, error pages).You get reproducible runs that are easy to share and review.You reduce â€œworks on my machineâ€ issues by standardizing a simple run flow.Unapproved automation can violate terms of service or local law.High concurrency or aggressive loops can stress servers (even unintentionally).Running against sites you donâ€™t control can create privacy and data-handling issues.
  
  
  Practical guardrails (recommended)
Only test where you have explicit permission.Keep concurrency low; treat rate limits as a signal to stop and review.Avoid using this as a crawler/scraper. Thatâ€™s not the goal, and itâ€™s easy to misuse.
  
  
  Operational tips (what Iâ€™d do in real projects)
Start with  in CI, but use non-headless when debugging locally.Keep  conservative (especially on slower environments).Keep artifacts in a predictable folder and archive them for failed builds.
  
  
  Roadmap ideas (if people want it)
Scenario DSL (JSON-defined steps: navigate / wait / scroll / click_css / type_css)Run-level HTML report exportGitHub Actions: lint + basic smoke test ()Docker image for deterministic environmentsWeesitor Console is published under the . is a console-first Selenium runner for  QA checks and lightweight monitoring, with an â€œoutput-firstâ€ mindset: logs, screenshots, and summary artifacts you can actually review.If you try it and you have feedback, issues are welcome.]]></content:encoded></item><item><title>CS50P Progress â€” Day 3</title><link>https://dev.to/prachiverma/cs50p-progress-day-3-1m7h</link><author>Prachi Verma</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 16:56:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Today I completed one more problem set from CS50P Week 3.Alongside that, I started exploring some AI problems, including the water jug problem, the N Queens problem, and the 8puzzle problem.This was my first exposure to thinking about problems in terms of states, constraints, and search rather than just writing code.]]></content:encoded></item><item><title>My python vs rust methods for finance backtesting</title><link>https://dev.to/kacawaiii/my-python-vs-rust-methods-for-finance-backtesting-2nai</link><author>Kyoko</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 15:58:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[How I Made My Python Backtester 56x Faster with Rust]]></content:encoded></item><item><title>How I Made My Python Backtester 56x Faster with Rust</title><link>https://dev.to/kacawaiii/how-i-made-my-python-backtester-56x-faster-with-rust-1pi</link><author>Kyoko</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 15:51:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ â€” ML pipeline, orchestration, data handling â€” Backtesting engine, indicators, grid search â€” Parallel executionSame API. Drastically faster.
  
  
  1. Zero-copy NumPy access
No data duplication between Python and Rust.
  
  
  2. Parallel grid search with Rayon
All CPU cores used automatically.Avoiding repeated allocations made a measurable difference.
Convincing Rustâ€™s borrow checker with NumPy-backed slices took trial and error.
Maturin helps, but testing Linux / macOS / Windows is still required.Debugging across the boundary
Rust crashes called from Python donâ€™t produce great stack traces.Iteration speed increased by 56Ã—I can explore much larger parameter spacesThe Rust core ended up cleaner than the original Python codeIf you're hitting performance limits in Python, rewriting only the hot path in Rust is often enough.
PyO3 makes the integration surprisingly painless.Questions and feedback welcome in the comments.]]></content:encoded></item><item><title>This AI Reads Your Data Like a Senior Analyst - And It Works in Milliseconds</title><link>https://dev.to/osmanuygar/this-ai-reads-your-data-like-a-senior-analyst-and-it-works-in-milliseconds-3kec</link><author>osman uygar kÃ¶se</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 15:36:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You know what drives me crazy? Data tools that show you numbers but leave you hanging. They're like that friend who says "I've got news" and then just... stares at you.We built SQLatte to convert natural language into SQL queries. Cool, right? Users loved it. But then came the existential crisis: "Show me today's sales"Returns a table with numbers "Okay... so is this good or bad?" ðŸ¦—That's when we realized: Raw data is useless without context.
  
  
  ðŸŽ¯ The Real Problem: Numbers Don't Tell Stories
Imagine you're running an e-commerce platform. Your dashboard shows:Sales Today: $45,000
Orders: 23
âœ… Is this good? (Compared to yesterday)âš ï¸ Is the data complete? (It's 11 AM - day isn't over)ðŸ’¡ What should I do about it?Spoiler alert: Most BI tools can't answer these questions. They just dump data and run away.
  
  
  ðŸ’¡ Enter: The AI-Powered Insights Engine
So we built something different. Something that doesn't just query data - it  it.Here's what makes it special:
  
  
  1.  - It Knows What Time It Is
âŒ Dumb insight (without context):"Today's sales are $45,000 - below average!"âœ… Smart insight (with context):"â° Today's sales are $45,000 (46% of day complete). At this pace, you'll hit ~$98,000 by EOD - that's 15% above target! ðŸ“ˆ"See the difference? One makes you panic. The other makes you smile.
  
  
  2. Incomplete Data Detection - No More False Alarms
The engine automatically detects incomplete data:You run a query at 2 PM: "Compare today vs yesterday"Today: $50K
Yesterday: $120K
ðŸ˜± PANIC! Sales crashed 58%!
âš ï¸ WARNING: Today's data is incomplete (58% of day passed)
Projected EOD: ~$86K (28% below yesterday)
ðŸ’¡ Monitor until 6 PM for accurate comparison
No heart attacks. Just facts.
  
  
  ðŸ—ï¸ Architecture: Three Modes, One Goal
We designed three modes because one size doesn't fit all:
  
  
  Mode 1: LLM-Only (Maximum Intelligence)
Richest, most contextual insightsNatural language explanationsUnderstands business logicCosts money (LLM API calls) Premium features, high-value queries
  
  
  Mode 2: Statistical-Only (Lightning Fast)
ðŸ“Š Trend detection in numeric columnsâš¡ Anomaly identification (outliers)ðŸ“ˆ Growth/decline calculations Free tiers, real-time dashboards
  
  
  Mode 3: Hybrid (Recommended) â­
Try LLM first â†’ Get contextual analysisIf LLM fails â†’ Fallback to statisticalCombine both â†’ Maximum coverage Balance between cost and quality.
  
  
  ðŸ’» Technical Deep Dive: How It Actually Works

  
  
  Prompt Engineering: The Secret Sauce
Here's how we talk to the LLM (this took us 47 iterations to get right):
  
  
  Statistical Analysis: The Safety Net
When LLM isn't available or fails, we fall back to rule-based analysis:
  
  
  Thread-Safe Singleton Pattern
In a multi-threaded FastAPI environment, we need one engine instance:
  
  
  ðŸŽ¯ Real-World Impact: The Numbers Don't Lie

  
  
  Use Case 1: E-Commerce Flash Sale
 "Show sales by category today"Category      | Sales
--------------|--------
Electronics   | $125,000
Fashion       | $89,000
Home & Living | $45,000
ðŸ“ˆ Electronics performing 230% above average today. 
   Likely a flash sale or campaign effect.

â° ALERT: It's 11 AM (46% of day). EOD projections:
   - Electronics: ~$273K
   - Fashion: ~$194K
   - Home & Living: ~$98K

ðŸ’¡ ACTION NEEDED: Check Electronics inventory. 
   High demand may cause stockouts by 3 PM.
 Prevented stockout, secured additional $47K in sales.
  
  
  Use Case 2: Security Incident Detection
 "Top attacked endpoints in last 24h"Endpoint        | Attack Count
----------------|-------------
/login          | 1,247
/admin          | 892
/api/v1/users   | 234
âš ï¸ CRITICAL: /login receiving 8x normal traffic. 
   High probability of brute-force attack!

ðŸ”’ /admin endpoint also targeted. Check if rate 
   limiting is active.

ðŸ’¡ IMMEDIATE ACTIONS:
   1. Enable CAPTCHA on /login
   2. Reduce rate limit: 5 â†’ 3 attempts
   3. Alert security team NOW
 Stopped brute-force attack within 12 minutes of detection.git clone https://github.com/osmanuygar/sqlatte
sqlatte
pip  requirements.txt


python run.py


  
  
  ðŸ™‹â€â™‚ï¸ Questions? Feedback? Want to Collaborate?
I'm always happy to discuss data engineering, AI, and building better tools.If you found this useful, give it a â¤ï¸ and share it with your team!]]></content:encoded></item><item><title>â›ï¸ Hard_Negative_Mining</title><link>https://dev.to/stklen/hardnegativemining-3o3p</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 15:00:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Washin Village AI Director Tech Notes #3
  
  
  ðŸŽ¯ What is Hard Negative Mining?
Your AI model reached 80% accuracy, but what about that remaining 20% of errors? specifically identifies these "troublesome mistakes" and trains AI to address them directly.In Washin Village's animal recognition, we discovered:These "Hard Negatives" are the toughest cases to identify and the key to improving your model!
  
  
  Step 2: Analyze Error Patterns

  
  
  Step 3: Enhanced Training
For high-error categories, we can:: Collect more photos of that category: Apply more transformations to these samples: Increase weight in the loss functionFound 467 errors (19%) in 2,451 test images.Add more Ariel feature photosRemove background human interferenceSubdivide black cat category
  
  
  ðŸ”„ Continuous Improvement Cycle
Train Model â†’ Find Errors â†’ Analyze Causes â†’ Fix Data â†’ Retrain
     â†‘                                                     |
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
This cycle can be repeated continuously, improving accuracy each time.: Re-scan for errors periodically: AI finds errors, humans confirm fixes: Record which errors have been fixed: Fix high-frequency errors first for maximum impactHard Negative Mining isn't a one-time taskâ€”it's a continuous improvement process. Using this method, we improved accuracy from 79.5% to 83.2%, and we're still improving!Washin Village ðŸ¡ by AI Director]]></content:encoded></item><item><title>Customer Support Agent with ADK and LangGraph</title><link>https://dev.to/lucasnscr/customer-support-agent-with-adk-and-langgraph-4k5n</link><author>lucasnscr</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 14:56:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Customer support agent via WhatsApp with Jira integration, built with Python, FastAPI, LangGraph, and Redis.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    WhatsApp     â”‚â”€â”€â”€â”€â–¶â”‚     FastAPI     â”‚â”€â”€â”€â”€â–¶â”‚   AgentRunner   â”‚
â”‚   (Webhook)     â”‚     â”‚   /webhooks     â”‚     â”‚   (ADK Style)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis      â”‚â—€â”€â”€â”€â–¶â”‚   StateStore    â”‚â—€â”€â”€â”€â–¶â”‚    LangGraph    â”‚
â”‚   (Sessions)    â”‚     â”‚                 â”‚     â”‚   StateGraph    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CustomerDB â”‚  â”‚    Jira    â”‚  â”‚  Parsing   â”‚
â”‚   (Tool)   â”‚  â”‚   (Tool)   â”‚  â”‚  (Utils)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Customer] â€œHiâ€
â”‚
â–¼
[Agent] â€œHi! To help you, please send me your CNPJ.â€
â”‚
â–¼
[Customer] â€œ11.111.111/0001-91â€
â”‚
â–¼
[System] Validates CNPJ in the database
â”‚
â”œâ”€â”€â”€ Invalid CNPJ â”€â”€â–¶ â€œI couldnâ€™t find an active account. Please confirm the number.â€
â”‚                           â”‚
â”‚                           â””â”€â”€â–¶ [Back to requesting CNPJ]
â”‚
â””â”€â”€â”€ Valid CNPJ â”€â”€â–¶ â€œPerfect! Tell me the product and describe your request.â€
â”‚
â–¼
[Customer] â€œProduct: ERP, Request: error in the reportâ€
â”‚
â–¼
[System] Creates ticket in Jira
â”‚
â–¼
[Agent] â€œâœ… Request created: SUP-123â€

Docker and Docker ComposePoetry (dependency manager)
curl  https://install.python-poetry.org | python3 -
Configure environment variablesCreate a .env file at the root of the project:# Redis
REDIS_URL=redis://localhost:6379/0

# Jira
JIRA_BASE_URL=https://your-domain.atlassian.net
JIRA_EMAIL=your-email@example.com
JIRA_API_TOKEN=your-api-token
JIRA_PROJECT_KEY=SUP
JIRA_ISSUE_TYPE=Task

# WhatsApp (optional)
WHATSAPP_PROVIDER=cloudapi
docker compose up 
docker compose up  redis


poetry run uvicorn app.main:app  8000

curl http://localhost:8000/health


curl  POST http://localhost:8000/webhooks/whatsapp 
curl  POST http://localhost:8000/webhooks/whatsapp 
curl  POST http://localhost:8000/webhooks/whatsapp 
  
  
  Valid CNPJs for testing (mock)
CNPJ    Name    Status
11111111000191  Demo Client Ltd.    Active
22222222000191  Test Company S.A.   Active
33333333000191  Inactive Company    InactiveMethod  Endpoint    Description
GET /health Health check
POST    /webhooks/whatsapp  Receives WhatsApp messages
GET /sessions/{phone}   Retrieves session state
DELETE  /sessions/{phone}   Removes session
poetry run pytest


poetry run pytest app


poetry run pytest tests/test_graph_flow.py customer-support-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI webhook
â”‚   â”œâ”€â”€ settings.py                # Config via env vars
â”‚   â””â”€â”€ agent/
â”‚       â”œâ”€â”€ runner.py              # AgentRunner (ADK style)
â”‚       â”œâ”€â”€ graph.py               # LangGraph StateGraph
â”‚       â”œâ”€â”€ state.py               # SupportState TypedDict
â”‚       â”œâ”€â”€ nodes.py               # Node functions
â”‚       â”œâ”€â”€ tools/
â”‚       â”‚   â”œâ”€â”€ customer_db.py     # Customer validation (mock)
â”‚       â”‚   â””â”€â”€ jira.py            # Jira REST integration
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ parsing.py         # CNPJ, product, request parsing
â”‚           â””â”€â”€ store.py           # Redis state store
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_graph_flow.py         # Flow tests
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
Create a project in Jira (e.g., â€œSUPâ€ for Support)Configure the issue type (Task, Bug, etc.)Update the variables JIRA_PROJECT_KEY and JIRA_ISSUE_TYPEConfigure the webhook URL in Meta Business:https://your-domain.com/webhooks/whatsapp
Configure the webhook URL in the Twilio Console:https://your-domain.com/webhooks/whatsapp
]]></content:encoded></item><item><title>Prefix sums and range queries</title><link>https://dev.to/josephakayesi/prefix-sums-and-range-queries-260f</link><author>JosephAkayesi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 14:52:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Discovering prefix sums is one of those moments where you go 
Itâ€™s staggering how memory-efficient this technique is.Prefix sums are used when you want to compute values over a range â€”.Letâ€™s say you want to compute the number of YouTube views over different time periods. represents the  in the  array
 is the number of views on that day
Each element in  represents a range over which we want the cumulative sum
For example,  means:
compute the cumulative views from day  to day  (inclusive).A valid approach would be:For each period, find the sub-array for that range
Iterate through it and sum all the elements
Append the result to an output array
The problem with this approach is that we recompute  without keeping track of previous work.
  
  
  Prefix sums to the rescue ðŸš€
Yes. An optimal approach is to use a .Compute a cumulative sum array once
Each element at index  stores the sum of values from index  to Letâ€™s call this array .prefix[0] = views[0]
prefix[1] = views[0] + views[1]
prefix[2] = views[0] + views[1] + views[2]
prefix[3] = views[0] + views[1] + views[2] + views[3]
...

  
  
  Querying ranges with prefix sums
If the range starts at day , the answer is simply:
Because each  already represents the cumulative sum from day  to day .
  
  
  What if the range does  start at 0?
Letâ€™s say the period is .prefix[4] = views[0] + views[1] + views[2] + views[3] + views[4]
prefix[0] = views[0]
So to get the sum from day  to day , we subtract what came before the range: gives the total up to the end of the range
 gives everything before the range
Subtracting removes what we donâ€™t care about

  
  
  Intuition (whatâ€™s really happening)
prefix[4] = views[0] + views[1] + views[2] + views[3] + views[4]
prefix[0] = views[0]
-----------------------------------------------
result    =          views[1] + views[2] + views[3] + views[4]
Everything before the range is discarded. We only keep whatâ€™s inside .prefix[3] = views[0] + views[1] + views[2] + views[3]
prefix[2] = views[0] + views[1] + views[2]
Which is exactly .Thatâ€™s the magic of prefix sums:precompute once, answer every range query in O(1).]]></content:encoded></item><item><title>Technical Deep Dive: Building SkillFade with FastAPI and React</title><link>https://dev.to/ruhidibadli/technical-deep-dive-building-skillfade-with-fastapi-and-react-3833</link><author>Ruhid Ibadli</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 14:35:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Last week I shared SkillFade, a skill tracking app that prioritizes honesty over gamification. Today I want to walk through the architectural decisions and technical choices that shaped it.Backend:  FastAPI + SQLAlchemy 2.0 + PostgreSQL
Frontend: React 18 + TypeScript + TailwindCSS
Auth:     JWT + bcrypt
Deploy:   Docker + Nginx
Boring? Yes. That's the point. Every piece is battle-tested, well-documented, and easy to hire for.I'm primarily a backend engineer with Python experience. When choosing a framework, I wanted something that would let me move fast without fighting the tooling.FastAPI checked every box: - Python is my comfort zone, so I could focus on product logic instead of learning new syntaxType hints as documentation - Pydantic schemas generate OpenAPI docs automatically, no extra work - But sync works perfectly fine for database-bound operations - Need anything? There's probably a Python library for it - Hot reload, clear error messages, intuitive routingCould I have used Django? Sure. Flask? Also fine. But FastAPI hit the sweet spot between simplicity and features. For a solo developer shipping a product, that matters more than benchmarks.The honest truth: I picked the tool I knew best so I could ship faster. No regrets.
  
  
  Architecture: Monolith by Choice
No microservices. No message queues. No Kubernetes. A single FastAPI process handles everything.This wasn't a compromiseâ€”it was a deliberate decision. - One log stream, one deployment, one place to look when things break - No service orchestration, no network calls between components, no distributed tracing - Everything runs on a modest VPS, no complex infrastructure costs - The entire application fits in my headSkillFade is a personal productivity tool. Even with thousands of users, a single well-optimized server handles the load easily. PostgreSQL can manage millions of rows without breaking a sweat.The microservices question I ask myself: "Do I have a team of 50 engineers who need to deploy independently?" No. So monolith it is.I'll split it when I have the problem. Not before.
  
  
  The Core Algorithm: Freshness Decay
The heart of SkillFade is the freshness calculation. Here's the thinking behind it.Every skill starts at 100% freshness after you practice itEach day without practice, freshness decays exponentiallyRecent learning activity adds a small boost (max 15%)Result is clamped between 0-100%Default decay rate is 2% per day, compounding. This means:Linear decay (lose 1% per day) doesn't match how memory works. In reality, forgetting is steep at first, then levels off. The exponential curve better models the Ebbinghaus forgetting curve that psychologists have studied for over a century.Why does learning only "boost" and not reset?This is the key philosophical choice. Watching a tutorial about React doesn't mean you can build a React app. Reading about Kubernetes doesn't mean you can debug a cluster.Learning slows decay. Only practice resets the clock.This forces users to confront an uncomfortable truth: passive consumption isn't the same as skill building.Not all skills fade equally: (cycling, typing) - slow decay, muscle memory persists (design patterns, architecture) - medium decay (specific APIs, CLI flags) - fast decay, details fade quicklyUsers can adjust decay rate per skill to match reality.
  
  
  Database Design Decisions
Six core tables. No more than necessary, no fewer than useful.users
categories
skills
learning_events
practice_events
skill_dependencies
Every table has clear ownership. Users own categories. Categories organize skills. Skills have events. Simple hierarchy, predictable queries.
  
  
  Separate Tables for Learning vs Practice Events
I debated this one. A single  table with a  column would be simpler, right?I went with separate tables because: - Dashboard shows practice frequency, detail pages show both. Separate tables = simpler queries for common operations - Learning events have types like "video, article, course, book". Practice events have "project, exercise, teaching, code-review". One enum would be confusing - Practice events might get "difficulty" or "output_url" fields that don't make sense for learning - Two tables makes the learning/practice distinction explicit in the data model itselfThe tradeoff is some duplication in the codebase. Worth it for clarity.
  
  
  Hard Deletes Over Soft Deletes
When a user clicks "Delete my account," everything goes. CASCADE deletes across all tables. No  columns, no recovery period, no "we keep your data for 30 days."This aligns with the privacy philosophy. If someone wants out, they're out. Fully.It also simplifies queriesâ€”no  everywhere.
  
  
  Categories as First-Class Entities
Early version had category as just a string field on skills. Worked fine until I wanted:Category-level analytics ("How fresh are my Frontend skills overall?")Rename a category everywhere at onceEnsure no duplicate category names per userRefactored to proper  table with foreign keys. More work upfront, cleaner long-term.
  
  
  Why React + Vite (Not Next.js)
Next.js is great. I considered it. Went with plain React SPA instead. - It's a dashboard behind login. Google doesn't need to crawl it - No SSR vs CSR decisions, no hydration bugs, no server components confusion - API is API (FastAPI), frontend is frontend (React). Clean boundary - Static files served by Nginx, API proxied separatelyFor a marketing site or blog, I'd pick Next.js. For an authenticated dashboard app, SPA is simpler.
  
  
  State Management: Context Over Redux
SkillFade uses React Context for global state. No Redux, no Zustand, no MobX.Current user (authentication)Theme preference (dark/light)Fetched fresh per page. Server is the source of truth. When you navigate to the dashboard, it fetches current data. No stale cache problems, no sync issues.Redux solves problems I don't have:Complex state interactions across many components? Nope, state is simpleTime-travel debugging? Never needed itMiddleware for side effects? fetch() works fineFor this app, Redux would add complexity without adding value. Context + useState + useEffect covers everything.src/
  components/     # Reusable UI pieces (Button, Card, Modal)
  pages/          # Route-level components (Dashboard, SkillDetail)
  context/        # Global state (AuthContext, ThemeContext)
  services/       # API calls (skills.ts, events.ts)
  hooks/          # Custom hooks (useSkills, useFreshness)
  types/          # TypeScript interfaces
Nothing fancy. Pages fetch data on mount, pass to components via props. Components are mostly presentational.
  
  
  Real-time Updates Without WebSockets
When you log a practice event on the skill detail page, the dashboard freshness should update. - Real-time push from server - Fetch every N seconds - Signal between components in same tabWent with option 3. It's a SPAâ€”dashboard and detail page components exist in the same JavaScript context. When detail page logs an event, it dispatches a custom event. Dashboard listens and refetches.No socket server to maintain. No unnecessary network requests. Works perfectly for single-tab usage.For multi-device sync (phone updates, desktop sees it), you'd need WebSockets or polling. Not implemented yet. Manual refresh works for now.
  
  
  The Alert System: Calm by Design
Most apps optimize for engagement. More notifications = more opens = better metrics.SkillFade optimizes for calm. Alerts should be: - Maximum 1 email per week, total - Only alert if user can do something about it - Plain text, no tracking pixels, instant unsubscribe
Triggers when a skill drops below 40% freshness. But not immediatelyâ€”waits for 14 days since last alert for that skill. Prevents nagging about the same thing repeatedly.
Triggers when a skill has learning events but no practice events for 30+ days. The message: "You've been learning X but not practicing. Theory without application fades fast."
Triggers when overall learning/practice ratio stays below 0.2 for two consecutive months. The message: "You're consuming a lot but producing little. Consider more hands-on work."Alerts run via scheduled job (cron), not real-time triggers. Once daily, batch process checks all users who have alerts enabled.Each alert type has independent cooldowns. You might get a decay alert and an imbalance alert in the same week, but never two decay alerts for the same skill within 14 days.Pure REST says everything should be a resource. I follow that mostly, but bend it when convenient: - Not really a "resource," but makes sense as an endpoint - "login" isn't a noun, don't careGET  /skills
GET  /skills/:id
POST /skills
PUT  /skills/:id

GET  /learning-events
POST /learning-events
PUT  /learning-events/:id
DELETE /learning-events/:id
Not nested like /skills/:id/learning-events/:eventId. Flat is simpler to route, simpler to reason about, simpler to document.Events reference skills via  in the body/params, not URL hierarchy.
  
  
  Consistent Response Patterns
 Return the data directly (object or array) Always { "detail": "Human readable message" } Offset-based with , returns { items: [], total: number }Frontend code stays simple when API is predictable.
  
  
  Authentication: Keep It Simple
Access token in Authorization headerStored in localStorage (yes, I know about XSS concernsâ€”acceptable for this use case)No refresh tokensâ€”expired means re-loginFor a personal productivity tool used by one person at a time, this is plenty secure. No need for refresh token rotation, token families, or OAuth complexity.bcrypt with default work factor72-byte input limit (bcrypt truncates silently, so I handle it explicitly)No password rules beyond minimum lengthâ€”let users choose their own passwords - Serves frontend static files + proxies API requestsSingle  brings everything online. Environment variables for secrets, volumes for data persistence.Lambda/Vercel functions would work for the API. Didn't go that route because:Cold starts matter for dashboard responsivenessPostgreSQL connection pooling is annoying in serverlessI wanted full control over the environmentMonthly cost is predictable with a VPSFor a side project, a $10-20/month VPS beats managing serverless complexity.First two weeks, I built with a hardcoded user ID. No login, no registration, no password reset flow. Just the core features.This prevented the classic trap: spending weeks on auth while the actual product stays unbuilt.Added auth last, once everything else worked.
  
  
  2. Migrations From Day One
Even solo, even on a side project: use database migrations (Alembic for Python).Schema will change. "I'll just modify the table directly" becomes a nightmare when you have production data. Migrations saved me multiple times.Frontend and backend types should match. I define Pydantic schemas in Python, then manually keep TypeScript interfaces in sync.Not ideal (would love auto-generation), but catching type mismatches at compile time is worth the maintenance.Every time I picked the "boring" option (PostgreSQL over MongoDB, REST over GraphQL, React over Svelte), I benefited from:More Stack Overflow answersSmoother hiring (if I ever need help)Novel tech is fun. Shipped products need boring tech.GDPR requires "right to deletion." Even if you're not in the EU, it's good practice.Plan for cascade deletes from day one. No orphaned data, no "soft delete" flags that accumulate forever.I relied on manual testing too long. Click through the app, check if it works. Tedious and error-prone.Should have set up Playwright or Cypress from week two.
  
  
  Design Alert System First
Retrofitting notifications into an existing schema was messy. Alert cooldowns, user preferences, email templatesâ€”all added later, all awkward.If I started over, I'd design the alert data model upfront.Built every button, input, modal, and dropdown from scratch with Tailwind. Educational, but slow.Next time: Radix UI or Headless UI for primitives, custom styling on top.
  
  
  The Anti-Complexity Manifesto
Every feature request, every "nice to have," every shiny technology gets filtered through one question:"Does the added complexity justify the value?"Microservices? No. One process is fine until it isn't.GraphQL? No. REST handles every use case here.Redis caching? No. PostgreSQL is fast enough.Real-time sync? No. Manual refresh works.Mobile app? No. PWA covers it.AI recommendations? No. That's not what this product is.The best code is code you don't write. The best infrastructure is infrastructure you don't maintain. The best feature is the one that solves the problem without adding moving parts.That's the technical foundation of SkillFade. The theme throughout: simple, boring, maintainable. Complexity is easy to add. Simplicity is hard to maintain.If you're building a side project, my advice: pick boring tech, ship fast, add complexity only when forced.Questions about specific decisions? Drop them in the comments.]]></content:encoded></item><item><title>Vibe coding level UP</title><link>https://dev.to/worldlinetech/vibe-coding-level-up-3klc</link><author>Pietro Maffi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 14:13:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Last week my colleague Pieter  suggested me a nice article about "Long-term DORA data reveals systemic performance plateaus and shifts, allowing organizations to connect improvements to architectural, cultural, and process changes."So starting from my current level of Jira Lead Time Analyzer i developed in vibe coding, I tried to improve my report accordingly again using vibe coding.I tried to ask JitHub Copilot (yes at this time not anymore AmazonQ because of the limitation I got) to create PBC report for the Jira projects we are managing in our Large Scaled Organization.The powerfull result was that I provided some request to improve for example the duration of the analysis, to reuse the same approach (Flask app in python) accessing jira with JTA but not much about the graph and the rusul was really amazing... Considering the Lead Time Analyzer I would like to create a new Flask
 Application which calculate the Process Behavior Charts for each 
projects identified during the retrieval of project connected to the 
first input query.
I would like to have a chart which represent for each projects the Medium
 Lead time and the 85% lead time threshold calculated.
I would like than to see the real lead time calculated each month 
depicted in the diagram starting from august 2024.
My intention like per the article https://www.infoq.com/articles/DORA-metrics-PBCs/ is to identify trends
 and spike so that we can identify and address issues. ðŸ“¦ Created Files: - Core analysis engine that:Calculates median and 85th percentile lead times per projectComputes control limits (UCL/LCL) using Â±3Ïƒ methodologyGroups data by month starting from August 2024Detects special causes (points outside control limits)Identifies trends (7+ consecutive increases/decreases)Assesses process stability - Flask application (standalone on port 5500)templates/pbc_analyzer.html - Beautiful web interface with:Interactive Chart.js visualizationsMonthly trend lines for mean, median, and P85Control limit boundaries displayedColor-coded stability statusSpecial cause and trend alerts - Configuration file with defaults - Complete documentationand than this nice report:ðŸŽ¯ Key Features:
âœ… Median & P85 calculations per project
âœ… Monthly aggregation starting August 2024
âœ… Control charts with UCL/LCL (Â±3Ïƒ)
âœ… Special cause detection (outliers)
âœ… Trend detection (7+ consecutive points)
âœ… Process stability assessment
âœ… Multi-project analysis from single JQL query
âœ… Interactive visualizations showing all metrics on one chartIn the log I do not see the query executed, can you add some debugging level logs which I can disable to check what query are executed? I know the input query is based on a project which started after the august 2024, but I would like to analyze the projects related to the input one which were existing before. Than I need the calculation for the following months till todayI've enhanced the PBC Analyzer with the following capabilities:ðŸ› Debug Logging
New checkbox in UI: "Enable Debug Logging" to see all JQL queries being executed
Detailed query logging: Shows every JQL query executed against Jira
Progress tracking: See which projects are being fetched and how many issues found
ðŸ” Related Project Discovery
The analyzer now works like the PI analyzer:Executes your initial query (e.g., a project that started after Aug 2024)
Discovers related projects through:
Parent/child relationships
Epic links
Issue links (blocks, relates, etc.)
Fetches data from ALL related projects starting from August 2024, even if those projects existed before
Monthly calculations from Aug 2024 to today for all discovered projects
Each project gets its own PBC with control limits
Special causes and trends detected per projectI see the analysis is done only on initial project but I would like to see the analysis also to all the connected Jira projects following the whole hierarchy (for each Business initiatives we should collect the related feature and sub-feature and than epics in each other projects like we did in original Lead time analyzer)For sure the vibe coding was a good way to develop easily first view of the idea you have in mind becoming workable application you can improve and manage. What was the learning in this case was the really amazing capability to get additional information from the article without the needs to specify details in graph format and definition.
It was really a great result obtained in few minutes.  ]]></content:encoded></item><item><title>What is GIS? A Beginner&apos;s Guide to Geospatial Technology</title><link>https://dev.to/kvishal1012/what-is-gis-a-beginners-guide-to-geospatial-technology-5e47</link><author>Koushik Vishal Annamalai</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 14:01:15 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Picture yourself standing at the crossroads of technology and geography, with the ability to map out solutions to some of the world's most pressing challenges. From guiding disaster relief efforts to shaping smarter cities, Geographic Information Systems, or GIS, empower us to see and understand our planet in ways that were once unimaginable. If you're new to this realm or simply curious about its potential, this guide will unravel the magic of GIS, exploring its core concepts, vital components, and transformative real-world uses. By the end, you'll see why this technology is revolutionizing industries and decision-making across the globe.
  
  
  Understanding the Basics of GIS
At its core, GIS is a powerful computer-based tool that captures, stores, analyzes, and displays data linked to specific spots on Earth's surface. Think of it as a dynamic digital map, layered with rich information. One layer might trace winding road networks, while another highlights population clusters or rainfall zones. By tying location to detailed data, GIS turns raw numbers into vivid, interactive visuals that reveal hidden insights.What makes GIS truly remarkable is its knack for weaving together diverse datasets through the common thread of place. Whether it's census statistics, satellite photos, or social media buzz, GIS merges these elements to create a full, clear picture of any area. This fusion helps uncover trends and connections that might stay buried in old-school spreadsheets or static reports, making it an invaluable asset for anyone seeking deeper understanding.
  
  
  Core Components and Key Functions
GIS thrives as a unified system, built on a foundation of critical pieces working in harmony. It relies on hardware for raw processing power, software for crafting visualizations and running analyses, and spatial databases to house vast troves of geographic data. Together, these elements enable GIS to tackle several essential roles.First, it organizes data, sorting geographic points and blending inputs from a wide array of sources. Next, it brings this information to life through engaging maps and digital models that simplify complex ideas. Then, it dives into spatial analysis, using advanced methods to unearth patterns and relationships. Finally, it empowers users to create and refine data, crafting fresh datasets or polishing existing ones for accuracy.Picture a city planner at work, using GIS tools to layer zoning rules over flood risk zones. The resulting map becomes a guiding light, pinpointing safe spots for new buildings while ensuring compliance with regulations. This kind of practical insight shows the true strength of GIS in action.GIS juggles two main types of geospatial data, each with a unique role in mapping our world. Raster data comes as a grid of cells or pixels, each packed with details like height or heat readings. This style shines when handling continuous information, such as sweeping satellite images or weather radar visuals, offering a broad, textured view.In contrast, vector data sketches out geographic features with precise points, lines, and polygons. It's perfect for mapping distinct elements, like property edges or street layouts. Imagine a city map where points mark bus stops, lines trace roads, and polygons outline green spaces. This format delivers sharp, exact representations.Beyond these two, GIS taps into a vast pool of sources, from land records to mobile phone signals. Its ability to unite such varied data through location as a central link transforms it into a powerhouse for comprehensive, all-encompassing analysis.
  
  
  Real-World Applications and Impact
The reach of GIS stretches across countless fields, proving itself as a cornerstone for smart decisions and efficient resource use. Its influence touches many areas with profound effect.In logistics, it streamlines delivery paths and fleet movements, slashing expenses and shrinking carbon footprints. For infrastructure, it monitors the health of roads and utilities, ensuring timely care and upgrades. Environmental planning benefits as GIS models climate strategies and pinpoints prime spots for sustainable projects. During crises, it shapes emergency plans for events like storms or quakes. Even in business, it dissects market shifts to fuel growth plans.Take disaster response as a striking example. When wildfires rage, response teams lean on GIS to chart impacted zones, monitor fire spread in real time, and carve out safe escape routes. This kind of spatial insight doesn't just inform, it saves lives by speeding up critical choices.To peek under the hood of GIS data handling, consider this basic pseudo-code for finding nearby emergency shelters:define function findNearestShelter(currentLocation, shelterList):
    initialize shortestDistance as infinity
    initialize nearestShelter as null
    for each shelter in shelterList:
        distance = calculateDistance(currentLocation, shelter.location)
        if distance is less than shortestDistance:
            shortestDistance = distance
            nearestShelter = shelter
    return nearestShelter
This snippet reflects how GIS tools crunch spatial ties, guiding users to vital resources in urgent times.With the immense potential of GIS now in view, you're ready to take your first steps. Kick off by exploring free tools like QGIS, a robust platform packed with features for beginners to master the basics. Seek out online tutorials or join community forums to sharpen your skills in mapping and analysis. If you're with a team or organization, brainstorm ways GIS could solve specific hurdles, whether in streamlining workflows or boosting planning precision.Start with a simple project, like plotting local landmarks or sifting through community stats. As confidence builds, tackle bigger challenges, such as environmental simulations or market studies. The secret lies in hands-on learning through trial and exploration. GIS isn't merely a tool, it's a window into viewing our world through the prism of place. Dive in, and you'll uncover insights that pave the way for wiser, more sustainable choices. Embrace this journey, and let GIS inspire you to map out a better tomorrow.]]></content:encoded></item><item><title>GeoPandas Basics: Maps, Projections, and Spatial Joins</title><link>https://realpython.com/geopandas/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[GeoPandas extends pandas to make working with geospatial data in Python intuitive and powerful. If youâ€™re looking to do geospatial tasks in Python and want a library with a pandas-like API, then GeoPandas is an excellent choice. This tutorial shows you how to accomplish four common geospatial tasks: reading in data, mapping it, applying a projection, and doing a spatial join.By the end of this tutorial, youâ€™ll understand that: with support for spatial data. This data typically lives in a  column and allows  such as projections and spatial joins, while  focuses on richer interactive web maps after data preparation.You  with  and  data using  with an authority code like  or .A  stores longitude and latitude in degrees, while a  uses linear units like meters or feet for area and distance calculations.Spatial joins use  with predicates like  or , and both inputs must share the same CRS or the relationships will be computed incorrectly.Hereâ€™s how GeoPandas compares with alternative libraries:Projections, spatial joinsGeoPandas builds on pandas by adding support for geospatial data and operations like projections and spatial joins. It also includes tools for creating maps. Folium complements this by focusing on interactive, web-based maps that you can customize more deeply. Test your knowledge with our interactive â€œGeoPandas Basics: Maps, Projections, and Spatial Joinsâ€ quiz. Youâ€™ll receive a score upon completion to help you track your learning progress:Getting Started With GeoPandasYouâ€™ll first prepare your environment and load a small dataset that youâ€™ll use throughout the tutorial. In the next two subsections, youâ€™ll install the necessary packages and read in a sample dataset of New York City borough boundaries. This gives you a concrete GeoDataFrame to explore as you learn the core concepts.This tutorial uses two packages:  for working with geographic data and  for loading sample data. Itâ€™s a good idea to install these packages inside a virtual environment so your project stays isolated from the rest of your system and you can manage its dependencies cleanly. Using the  option ensures you have everything needed for reading data, transforming coordinate systems, and creating plots. For most readers, this will work out of the box.If you do run into installation issues, the projectâ€™s maintainers provide alternative installation options on the official installation page.Most geospatial datasets come in GeoJSON or shapefile format. The  function can read both, and it accepts either a local file path or a URL.In the example below, youâ€™ll use  to load the New York City Borough Boundaries (NYBB) dataset. The  package provides a convenient path to this dataset, so you donâ€™t need to download anything manually. Youâ€™ll also drop unnecessary columns: is a . A GeoDataFrame has rows, columns, and all the methods of a pandas DataFrame. The difference is that it typically includes a special  column, which stores geographic shapes instead of plain numbers or text.The  column is a . It behaves like a normal pandas , but its values are spatial objects that you can map and run spatial queries against. In the  dataset, each boroughâ€™s geometry is a â€”a shape made of several polygonsâ€”because every borough consists of multiple islands. Soon youâ€™ll use these geometries to make maps and run spatial operations, such as finding which borough a point falls inside.Once youâ€™ve loaded a GeoDataFrame, one of the quickest ways to understand your data is to visualize it. In this section, youâ€™ll learn how to create both static and interactive maps. This allows you to inspect shapes, spot patterns, and confirm that your geometries look the way you expect.]]></content:encoded></item><item><title>Why Cell Type Annotation is Still the Hardest Part of scRNA-seq (And How Multi-Agent AI Fixes It)</title><link>https://dev.to/sukhitha_b/why-cell-type-annotation-is-still-the-hardest-part-of-scrna-seq-and-how-multi-agent-ai-fixes-it-4bj6</link><author>Sukhitha Basnayake</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 13:34:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You've clustered your single-cell RNA-seq data. Your UMAP looks beautiful. Now comes the hard part: If you've worked with scRNA-seq data, you know this pain. Manual annotation takes weeks. Reference-based methods fail on disease samples. And when you finally publish, Reviewer 2 asks: "How confident are you in cluster 7's annotation?"
  
  
  The Annotation Bottleneck is Real
Reference classifiers trained on healthy tissue show  on disease samplesThey miss rare cell types in ~20% of cases
Manual annotation has 25% inter-annotator variabilityCurrent methods give you a label with zero justificationWorse, datasets now routinely contain . The computational bottleneck has shifted from analysis to biological interpretation.
  
  
  Why LLMs Alone Don't Cut It
GPT-4 achieved 75% agreement with expert annotationsâ€”impressive! But existing LLM approaches have critical gaps:They only see , not full expression profilesKnowledge is  (no current literature)No mechanism to  against databasesZero uncertainty quantificationYou get a confident answer that might be completely wrong.
  
  
  Architecture Over Model Selection
Here's what we learned building CyteType: the problem isn't the LLMâ€”it's how you structure the task.Instead of asking one model "what is this cell?", we built a  where each agent handles a distinct part of scientific reasoning:
Establishes biological ground truth  annotation begins. Infers organism, tissue, pathway context from your data and metadata. Integrates with GTEx, Enrichr (GO, Reactome, WikiPathways), and blitzGSEA.multiple competing hypotheses instead of one prediction. Tests each against the  by querying a pseudobulked expression database. Selects the best hypothesis and maps it to Cell Ontology terms.
Simulates an expert panel. Checks predictions against CellGuide, detects cellular heterogeneity, triggers re-annotation when needed. This creates an interpretable "trust layer."
Connects annotations to current knowledge. Searches PubMed for supporting evidence, identifies disease associations (Disease Ontology), flags drug targets (Drug Ontology).
Synthesizes results across your entire study. Performs similarity analysis, disambiguates naming inconsistencies, generates semantic cluster ordering.
  
  
  The Benchmark That Matters
We tested on  across four diverse datasets (HypoMap, Immune Cell Atlas, GTEx v9, Mouse Pancreatic Atlas).To isolate architectural benefits, we compared CyteType against GPTCellType using the same GPT-5 model:CyteType vs. GPTCellType (same LLM): 388% higher similarity score (p < .001) 267% higher
 100% higherThe comparison using identical models proves: architecture matters more than model choice.
  
  
  Model Flexibility Without Sacrificing Performance
Here's the kicker: you're not locked into expensive API calls.Open-weight models like DeepSeek R1 and Kimi K2 achieve  at lower cost. LLMs with built-in chain-of-thought reasoning showed  (p = 0.22)â€”CyteType's workflow supersedes model-native reasoning.Choose models based on Run locally with Ollama for Switch models without rewriting your pipeline
  
  
  More Than Labels: Discovery
Applying CyteType to 977 clusters across 20 datasets revealed: received functional enhancement (cell state information) refined to specific subtypes
 required major reannotationAnnotations mapped to 327 unique Cell Ontology terms and identified .Example: In a diabetic kidney disease atlas, "parietal epithelial cells" were relabeled as injured proximal tubule cells (ALDH1A2+, CFH+, VCAM1+)â€”a discovery that changes biological interpretation.The Reviewer agent generates calibrated confidence scores:High-confidence annotations had significantly higher similarity scores (F = 23.88, p < .001)Heterogeneous clusters showed lower similarity (F = 8.45, p < .01)
Median majority agreement exceeded Now when Reviewer 2 asks about cluster 7, you have:Supporting/conflicting markers
Alternative hypotheses consideredCyteType is  (CC BY-NC-SA 4.0):Both generate comprehensive HTML reports and integrate directly into your existing workflows.
  
  
  What's Your Biggest Annotation Challenge?
We built CyteType to solve our own annotation headaches. What problems are you facing?Rare cell types that references miss?Disease contexts where nothing works?
Inconsistent annotations across studies?Explaining your calls to reviewers?Drop a commentâ€”I'd love to hear what you're working on and whether this approach could help.Full disclosure: I work at Nygen Analytics, the team behind CyteType. We open-sourced this because we think the architecture principleâ€”structuring tasks for LLMs rather than just prompting harderâ€”applies way beyond biology.]]></content:encoded></item><item><title>Multi-Agent Communication Patterns That Actually Work</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/multi-agent-communication-patterns-that-actually-work-50kp</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 13:17:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Your AI agent talks to an API. Cool. Now you need  to talk to each other â€” and suddenly you're debugging race conditions at 3 AM.Multi-agent systems sound simple until you build one. This article covers five communication patterns I've used in production, with code examples and honest assessments of when each one breaks.When agents need to coordinate, you face three fundamental challenges: â€” agents start and stop independently â€” each agent has its own worldview â€” "I said X then Y" doesn't mean the other agent sees X before YEvery pattern below is a different tradeoff between these constraints.
  
  
  Pattern 1: The Shared Filesystem (Mailbox Pattern)
The simplest approach: agents read and write files in shared directories. Agents on the same machine, low message volume, human-readable debugging. You can literally  and  to debug. High throughput (filesystem isn't a message queue), agents on different machines, ordering guarantees needed across many senders. Atomic writes matter. I once lost messages because an agent read a half-written JSON file. The  pattern fixes this â€”  is atomic on most filesystems.
  
  
  Pattern 2: The Relay Agent (Hub-and-Spoke)
When agents can't directly access each other's storage, route through a coordinator. Heterogeneous agents (different runtimes, different machines), when you need a single audit trail, when message routing logic gets complex. The hub is a single point of failure. If the relay goes down, communication stops. Also adds latency for every message. Use the relay for cross-machine communication, file-based for same-machine fast-path:
  
  
  Pattern 3: The Shared Ledger (Event Sourcing)
Instead of sending messages, agents append events to a shared log. Every agent reads the same log and derives its own state. When you need full auditability, when agents need to reconstruct historical state, when the "truth" is the sequence of events rather than any single snapshot. Log grows forever (need compaction), slow replay on large histories, concurrent appends need coordination (file locking or use a proper database). Event sourcing turns communication into a side effect of recording what happened. Agents don't send messages â€” they announce actions. Other agents observe.
  
  
  Pattern 4: The Handoff Chain
When agents run sequentially (Agent A finishes, Agent B starts), communication is a structured handoff document. Sequential agent execution (the most common case for AI agents that run on schedules), when you need guaranteed context transfer, when the "conversation" is slow (minutes/hours between turns). Doesn't work for concurrent agents. The handoff is a snapshot, not a stream.
  
  
  Pattern 5: The Contract Protocol (Request/Response)
When agents need to coordinate on specific tasks with guaranteed completion. Task delegation between specialized agents, when you need accountability (who did what), when work items have clear completion criteria. Overhead is high for simple messages. Don't use contracts for "hey, check this out" â€” use them for "build this thing and tell me when it's done."
  
  
  Choosing the Right Pattern
Cross-machine, many agentsAudit trails, event-drivenMost multi-agent systems only need Pattern 1 (Mailbox) + Pattern 4 (Handoff). The file-based mailbox handles async coordination. The handoff chain handles sequential continuity. Everything else is optimization for scale you probably don't have yet.Start simple. Add complexity only when the simple version demonstrably fails. Agents sending free-form text to each other and parsing it with regex. Use structured messages. Always. A central agent that knows everything and coordinates everyone. This creates a bottleneck and a single point of failure. Prefer agents that can operate independently and only coordinate when necessary. Agents checking for messages every 100ms. Use filesystem watchers ( on Linux,  on macOS) or exponential backoff. "I sent A before B, so the other agent will see A first." Not guaranteed. Include sequence numbers or timestamps and handle out-of-order delivery.Error recovery strategies for multi-agent systemsMonitoring and debugging agent communication in production]]></content:encoded></item><item><title>Convert Videos to PDF with VID2PDFPro in Python</title><link>https://dev.to/matetechnologie/convert-videos-to-pdf-with-vid2pdfpro-in-python-3gcl</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 12:47:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Learn how to turn video frames into PDF documents using Python with VID2PDFPro, complete with face and license plate anonymization. Perfect for creating reports, archives, or anonymized content.Check out the full project on GitHub
.Step 1: Install DependenciesVID2PDFPro uses several Python libraries. Install them using pip:pip install opencv-python pillow ttkbootstrap tkinterdnd2
opencv-python â€“ For reading and processing video frames.Pillow â€“ To manipulate images and generate PDFs.ttkbootstrap â€“ For modern themed GUIs.tkinterdnd2 â€“ To enable drag-and-drop in Tkinter.Step 2: Import Required ModulesWe need several built-in and third-party modules:import sys
import os
import threading
import queue
import time
from pathlib import Path

import cv2
from PIL import Image
import tkinter as tk
from tkinter import filedialog, messagebox
from tkinterdnd2 import TkinterDnD, DND_FILES
import ttkbootstrap as tb
from ttkbootstrap.widgets.scrolled import ScrolledText
threading & queue â€“ For background processing and safe UI updates.cv2 (OpenCV) â€“ For reading video frames and anonymization.Pillow (PIL) â€“ To convert frames to images and PDFs.tkinter & ttkbootstrap â€“ For the GUI.TkinterDnD â€“ Adds drag-and-drop support.Step 3: Create the Main App ClassWe encapsulate the app in a class for organization:class VID2PDFPro:
    APP_NAME = "VID2PDF Pro"
    APP_VERSION = "1.0"
    PAGE_SIZES = ["Original", "A4", "Letter"]

    def __init__(self):
        self.root = TkinterDnD.Tk()
        tb.Style("superhero")
        self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
        self.root.geometry("1150x650")

        self.video_path = tk.StringVar()
        self.output_dir = tk.StringVar()
        self.frame_interval = tk.IntVar(value=10)
        self.pdf_dpi = tk.IntVar(value=200)
        self.page_size = tk.StringVar(value="Original")
        self.an_faces = tk.BooleanVar(value=True)
        self.an_plates = tk.BooleanVar(value=True)
        self.stop_event = threading.Event()

        self.ui_queue = queue.Queue()
        self.progress_var = tk.IntVar(value=0)
        self.counter_var = tk.StringVar(value="Processed: 0 / 0")
        self.eta_var = tk.StringVar(value="ETA: --:--")

        # Load Haar cascades for face and plate detection
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
        self.plate_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_russian_plate_number.xml")

        self._build_ui()
        self.process_ui_queue()
TkinterDnD.Tk() â€“ creates a GUI window that supports drag-and-drop.ttkbootstrap.Style() â€“ applies a theme.tk.StringVar / tk.IntVar â€“ bind GUI inputs to Python variables.Haar cascades are pre-trained classifiers for face/license plate detection.We create input fields, buttons, and a live progress log:def _build_ui(self):
    tb.Label(self.root, text=self.APP_NAME, font=("Segoe UI", 22, "bold")).pack(pady=(10, 2))
    tb.Label(self.root, text="Video to PDF Extraction & Anonymization",
             font=("Segoe UI", 10, "italic"), foreground="#9ca3af").pack(pady=(0, 10))

    # Video input section
    src_box = tb.Labelframe(self.root, text="Video Input (Drag & Drop Supported)", padding=10)
    src_box.pack(fill="x", padx=10, pady=6)
    self.video_entry = tb.Entry(src_box, textvariable=self.video_path)
    self.video_entry.pack(side="left", fill="x", expand=True)
    self.video_entry.drop_target_register(DND_FILES)
    self.video_entry.dnd_bind("<<Drop>>", self.on_drop_video)
    tb.Button(src_box, text="Browse", bootstyle="info",
              command=lambda: self.video_path.set(
                  filedialog.askopenfilename(filetypes=[("Video","*.mp4 *.avi *.mov")]))).pack(side="left", padx=5)
Users can drag and drop videos or use a Browse button.DND_FILES allows drag-and-drop of files directly into the entry box.VID2PDFPro lets you anonymize faces and license plates:adv = tb.Labelframe(self.root, text="Advanced", padding=10)
adv.pack(fill="x", padx=10, pady=6)
tb.Checkbutton(adv, text="Anonymize Faces", variable=self.an_faces, bootstyle="success").pack(side="left", padx=10)
tb.Checkbutton(adv, text="Anonymize Plates", variable=self.an_plates, bootstyle="success").pack(side="left", padx=10)
Checkboxes allow users to enable or disable face/plate anonymization.Step 6: Anonymization FunctionWe blur faces and pixelate license plates:def anonymize(self, frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    if self.an_faces.get():
        for x, y, w, h in self.face_cascade.detectMultiScale(gray, 1.2, 5):
            roi = frame[y:y+h, x:x+w]
            frame[y:y+h, x:x+w] = cv2.GaussianBlur(roi, (51,51), 0)
    if self.an_plates.get():
        for x, y, w, h in self.plate_cascade.detectMultiScale(gray, 1.1, 4):
            roi = frame[y:y+h, x:x+w]
            roi = cv2.resize(roi, (16,16))
            roi = cv2.resize(roi, (w,h), interpolation=cv2.INTER_NEAREST)
            frame[y:y+h, x:x+w] = roi
    return frame
License plates â†’ Pixelated for privacyControlled by checkboxes in the GUI.Step 7: Extract Frames and Save as PDFThis function converts frames to PDF:def extract_to_pdf(self):
    video_file = self.video_path.get()
    out_dir = self.output_dir.get()
    if not video_file or not out_dir:
        messagebox.showerror("Missing", "Select video and output folder")
        return

    self.stop_event.clear()
    cap = cv2.VideoCapture(video_file)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    pdf_path = os.path.join(out_dir,"output.pdf")
    first_img = None
    frames = []
    idx = 0
    processed = 0

    while cap.isOpened():
        if self.stop_event.is_set(): break
        ret, frame = cap.read()
        if not ret: break

        if idx % self.frame_interval.get() == 0:
            frame = self.anonymize(frame)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(rgb)
            if first_img is None: first_img = img
            else: frames.append(img)
            processed += 1

        idx += 1

    cap.release()
    if first_img:
        first_img.save(pdf_path, save_all=True, append_images=frames,
                       resolution=self.pdf_dpi.get(), optimize=self.pdf_compress.get())
        messagebox.showinfo("Done", f"PDF created:\n{pdf_path}")
frame_interval controls which frames to extract.Image.fromarray converts frames to PIL images.save_all=True combines multiple images into a single PDF.if __name__ == "__main__":
    VID2PDFPro().run()
Click Start Extraction in the app, and it will create a PDF from your video frames.Step 9: Learn More & ContributeThe full project with updates and advanced features is on GitHub:Thread-safe background extractionThis tutorial is beginner-friendly because it splits the logic into small steps and explains each section clearly.]]></content:encoded></item><item><title>Track Your Azure OpenAI Costs in Seconds, Not Minutes</title><link>https://dev.to/toyama0919/track-your-azure-openai-costs-in-seconds-not-minutes-2fnb</link><author>Hiroshi Toyama</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 12:32:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you're building AI applications with Azure OpenAI, you know the drill: costs can spiral fast. One experimental feature using o1-preview, a few hundred test runs, and suddenly your bill looks very different from last month.The Azure portal shows you the numbers eventually, but when you're iterating quickly on AI features, you need real-time visibility. That's exactly what  delivers - instant Azure OpenAI cost tracking from your terminal.
  
  
  The Azure OpenAI Cost Challenge
Unlike traditional cloud services with predictable pricing, Azure OpenAI costs vary wildly based on:Model choice (GPT-4o-mini vs o1-preview is a 5-15x difference)Token usage (both prompt and completion tokens)Deployment scaling and throughputTesting and development cyclesThe questions you need answered daily:"How much did my o1-preview deployment cost yesterday?""Which resource group is burning through credits?""Did that new feature spike my OpenAI spend?""How does dev environment cost compare to production?"Checking this through the Azure portal means multiple clicks, page loads, and waiting. When you're checking costs multiple times a day during active development, this friction adds up. is a Python CLI tool built specifically for developers who need fast answers about their Azure spending. For Azure OpenAI users, it's the fastest way to track Cognitive Services costs without touching the Azure portal.
  
  
  Why It Works for Azure OpenAI
: See your OpenAI costs in 2 seconds, not 2 minutes: Catch cost spikes the day they happen, not at month-end: Monitor individual Azure OpenAI accounts separately: Separate dev, staging, and production costs effortlessly: Break down by service, location, resource group, or resource ID: Python API for integrating into your CI/CD or daily reports: Works with any billing currencyInstallation takes one line:Log in with Azure CLI (if you haven't already):Check your Azure OpenAI costs:azurecost  your-subscription-name
USD                 2025-11    2025-12

total                 1247.83    2891.45
Cognitive Services    1247.83    2891.45
That's your Azure OpenAI spend right there - Cognitive Services is the billing category for Azure OpenAI. Notice the spike in December? Now you can investigate what changed.
  
  
  Real-World Azure OpenAI Use Cases

  
  
  Scenario 1: Daily Cost Monitoring During Development
You're building a new reasoning agent with o1-preview. Check costs every morning:azurecost  prod-subscription  DAILY  7
USD           2025-12-15  2025-12-16  2025-12-17  2025-12-18

total                45.23        52.18       178.45        51.20
Cognitive Services   45.23        52.18       178.45        51.20
Whoa, December 17th spiked to $178. That's the day you started load testing with o1-preview. Now you know exactly when and how much it costs.
  
  
  Scenario 2: Environment-Based Cost Breakdown
You have separate resource groups for dev, staging, and production. See costs side by side:azurecost  ai-subscription  ResourceGroup  ServiceName
USD                                        2025-11    2025-12

total                                        1247.83    2891.45
ai-dev-rg/Cognitive Services                  342.15     456.32
ai-staging-rg/Cognitive Services              198.42     287.89
ai-prod-rg/Cognitive Services                 707.26    2147.24
Production jumped from $707 to $2147. Time to optimize those prompts or consider GPT-4o-mini for some use cases.
  
  
  Scenario 3: Focused Investigation on Production
Something's wrong with production costs. Drill down to just that resource group:azurecost  ai-subscription  ai-prod-rg  DAILY  14
See two weeks of daily costs for production only. Spot the pattern, correlate with deployments or feature releases.
  
  
  Scenario 4: Multi-Region Cost Analysis
Running Azure OpenAI deployments in multiple regions? Group by location:azurecost  global-ai-sub  Location  ServiceName
USD                                   2025-11    2025-12

total                                   1247.83    2891.45
East US/Cognitive Services               823.14    1923.87
West Europe/Cognitive Services           424.69     967.58
East US is handling most of the load. Maybe redistribute traffic or consider regional pricing differences.
  
  
  Scenario 5: Resource-Level Cost Analysis
Running multiple Azure OpenAI accounts for different teams or use cases? Track costs at the individual resource level:azurecost  ai-subscription  ResourceId
USD                                                                                     2025-12    2026-01

total                                                                                     5741.44   16571.60
/resourcegroups/ai/providers/microsoft.cognitiveservices/accounts/chatbot-prod           3401.80   16390.44
/resourcegroups/ai/providers/microsoft.cognitiveservices/accounts/analytics-engine       2194.17     131.82
/resourcegroups/ai/providers/microsoft.cognitiveservices/accounts/internal-tools          145.47      49.34
This shows exactly which Azure OpenAI account is consuming credits. Perfect for:: Charge back costs to specific teams or projectsIdentifying cost anomalies: Spot which deployment suddenly increased spendMulti-tenant environments: Track costs per customer or tenant: Distribute budget based on actual usage patternsCombine with daily granularity to investigate when a specific resource started costing more:azurecost  ai-subscription  ResourceId  DAILY  7

  
  
  Why This Matters for Azure OpenAI Users
Building with Azure OpenAI is different from traditional cloud infrastructure. With VMs or databases, costs are fairly predictable. But with modern language models like GPT-4o and o1-preview, costs depend on how users interact with your application:Long conversations = more tokens = higher costsComplex reasoning tasks = more input tokensDetailed responses = more completion tokensTesting and iteration = multiplied costsDuring active development, you need to check costs frequently. Not once a month when the bill arrives, but daily or even multiple times a day.The Azure portal workflow kills this feedback loop:Navigate to Azure portal (wait for load)Find the right subscriptionClick through to Cost ManagementConfigure time range and filtersBy the time you see the numbers, you've burned 2-3 minutes. When you're doing this multiple times daily, the friction discourages you from checking at all.Then you're surprised at month-end when the bill is 3x what you expected. fixes this. Checking costs becomes as fast as checking git status:azurecost  ai-subscription  DAILY  7
Two seconds. Real-time feedback. No context switching.This speed changes behavior. When checking costs is instant, you actually do it. You catch issues early. You experiment with confidence because you're monitoring the impact.The tool emerged from my own need while building AI features. I was spending too much time in the portal doing the same query repeatedly. I wanted something terminal-based that integrated into my development workflow.What started as a personal script evolved into a proper tool that my team adopted, then others in the community found useful. The philosophy is simple: do one thing well - show Azure costs fast and clearly.
  
  
  Automate Azure OpenAI Cost Monitoring
The Python API lets you integrate cost tracking into your workflows. Send daily Azure OpenAI cost reports to Slack:Run this as a daily cron job or GitHub Action. Your team gets automatic cost visibility without anyone checking the portal.: Trigger warnings when daily costs exceed thresholds: Track which team or feature is using OpenAI credits: Validate costs before promoting to production: Generate reports for accounting without manual exports
  
  
  Configuration Tips for AI Workloads
Set environment variables to streamline your daily checks:your-ai-subscription-id
ai-prod-rg
Now run  with no arguments:Create shell aliases for common queries:Type  each morning as part of your routine. Takes 2 seconds, keeps you informed. Use  when you need to drill down to individual Azure OpenAI accounts.
  
  
  Best Practices for Azure OpenAI Cost Management
Based on using  with AI workloads, here are patterns that work:azurecost  ai-sub  DAILY  7
Catch anomalies before they compound. One day of unexpected costs is manageable. A month is not.2. Before and after feature releases
azurecost  ai-sub  ai-prod-rg  DAILY  3


azurecost  ai-sub  ai-prod-rg  DAILY  3
Measure the cost impact of new features. Make data-driven decisions about o1-preview vs GPT-4o-mini.3. Set up automated alerts
Use the Python API to send daily reports. Don't rely on remembering to check manually.
Use separate resource groups for dev, staging, production. Makes cost attribution trivial.5. Monitor during load testing
Run  before and after load tests. Understand your cost-per-request at scale before going live.
  
  
  Start Tracking Your Azure OpenAI Costs Now
azurecost  your-subscription  DAILY  7
That's it. You now have instant visibility into your Azure OpenAI spending.Check the GitHub repository for full documentation, API examples, and to report issues or contribute.Building AI features is expensive enough. Don't let invisible costs surprise you. Make cost visibility effortless.Building with Azure OpenAI? Drop a comment on how you're managing costs or share your use case. Always looking for feedback and ideas.]]></content:encoded></item><item><title>Build an Exam Result Predictor in Python with Tkinter &amp; Machine Learning</title><link>https://dev.to/matetechnologie/build-an-exam-result-predictor-in-python-with-tkinter-machine-learning-4jhe</link><author>Mate Technologies</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 12:24:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Ever wondered if you could predict student exam results based on study habits and previous grades? In this tutorial, weâ€™ll build ExamResultPredictor v2.0, a Python app that predicts exam grades using linear regression and a friendly GUI.ttkbootstrap for modern stylingscikit-learn for predictive modelingBy the end, youâ€™ll have a working app where you can add students manually or via CSV, predict their grades, and export results.GitHub repo: ExamResultPredictorStep 1: Set Up Your EnvironmentMake sure you have Python 3.8+ installed. Then install the required packages:pip install numpy scikit-learn tkinter ttkbootstrapWeâ€™ll start by importing the necessary libraries.import os
import sys
import threading
import csv
import tkinter as tk
from tkinter import messagebox, filedialog, ttk

import ttkbootstrap as tb
from ttkbootstrap.constants import *

from sklearn.linear_model import LinearRegression
import numpy as np
tkinter & ttkbootstrap â†’ For GUI componentsthreading â†’ To run predictions without freezing the appscikit-learn â†’ For building the linear regression modelnumpy â†’ For numeric arraysWe create a helper function to handle file paths, especially for executable builds.def resource_path(file_name):
    base_path = getattr(sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_path, file_name)
Step 4: Create a Prediction WorkerWeâ€™ll define a worker class to run predictions in a separate thread.class PredictorWorker:
    def __init__(self, data, model, callbacks):
        self.data = data
        self.model = model
        self.callbacks = callbacks
        self._running = True

    def stop(self):
        self._running = False

    def run(self):
        results = []
        for i, student in enumerate(self.data):
            if not self._running:
                break
            features = np.array([[student["study_hours"], student["attendance"], student["previous_grade"]]])
            predicted_score = self.model.predict(features)[0]
            grade = "A" if predicted_score >= 80 else "B" if predicted_score >= 60 else "C" if predicted_score >= 40 else "F"
            results.append({**student, "predicted_grade": grade})

            if "found" in self.callbacks:
                self.callbacks["found"](student, grade)
            if "progress" in self.callbacks:
                self.callbacks["progress"](int((i + 1) / len(self.data) * 100))

        if "finished" in self.callbacks:
            self.callbacks["finished"](results)
Runs predictions in a thread to avoid freezing the GUIPredicts grades and maps numeric scores to lettersStep 5: Build the Main App ClassWe now create the main application GUI.class ExamResultPredictorApp:
    APP_NAME = "ExamResultPredictor"
    APP_VERSION = "2.0"

    def __init__(self):
        self.root = tb.Window(themename="darkly")
        self.root.title(f"{self.APP_NAME} v{self.APP_VERSION}")
        self.root.minsize(950, 650)

        try:
            self.root.iconbitmap(resource_path("logo.ico"))
        except:
            pass

        self.worker_obj = None
        self.smooth_value = 0
        self.target_progress = 0
        self.student_data = []
        self.model = self._train_dummy_model()

        self._build_ui()
        self._apply_styles()
Initializes the Tkinter windowSets up variables for data, progress, and the modelStep 6: Train a Dummy ModelWe use a simple linear regression model trained on sample data.def _train_dummy_model(self):
    X = np.array([
        [5, 80, 70],
        [10, 90, 80],
        [2, 60, 50],
        [8, 100, 90],
        [3, 50, 40],
        [6, 70, 65]
    ])
    y = np.array([60, 90, 40, 95, 35, 70])
    model = LinearRegression()
    model.fit(X, y)
    return model
Features: [study_hours, attendance, previous_grade]

Target: current_score
Model predicts score based on inputsWeâ€™ll create input fields, buttons, progress bars, and a table for results.def _build_ui(self):
    main = tb.Frame(self.root, padding=10)
    main.pack(fill=BOTH, expand=True)

    tb.Label(main, text=f"ðŸ“š {self.APP_NAME} - Academic Predictor",
             font=("Segoe UI", 22, "bold")).pack(pady=(0, 4))
    tb.Label(main, text="Predict Exam Results Based on Input Parameters or CSV",
             font=("Segoe UI", 10, "italic"), foreground="#9ca3af").pack(pady=(0, 20))

    form_frame = tb.Frame(main)
    form_frame.pack(fill=X, pady=(0,6))

    self.name_input = self._create_form_row(form_frame, "Student Name:")
    self.study_input = self._create_form_row(form_frame, "Study Hours per Week:")
    self.attendance_input = self._create_form_row(form_frame, "Attendance %:")
    self.prev_grade_input = self._create_form_row(form_frame, "Previous Grade (0-100):")

    tb.Button(form_frame, text="âž• Add Student", bootstyle=SUCCESS, command=self.add_student).grid(row=4, column=0, columnspan=2, pady=5)
    tb.Button(form_frame, text="ðŸ“ Import CSV", bootstyle=INFO, command=self.import_csv).grid(row=5, column=0, columnspan=2, pady=5)
Input fields for student dataButtons for adding students and importing CSVStep 8: Add Student Data & CSV Importdef add_student(self):
    try:
        student = {
            "name": self.name_input.get().strip(),
            "study_hours": float(self.study_input.get()),
            "attendance": float(self.attendance_input.get()),
            "previous_grade": float(self.prev_grade_input.get())
        }
        self.student_data.append(student)
        self.tree.insert("", END, values=(student["name"], student["study_hours"],
                                          student["attendance"], student["previous_grade"], "Pending"))
    except ValueError:
        messagebox.showerror("Invalid Input", "Please enter valid numbers.")

def import_csv(self):
    path = filedialog.askopenfilename(filetypes=[("CSV Files", "*.csv")])
    if not path:
        return
    with open(path, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            student = {
                "name": row["name"].strip(),
                "study_hours": float(row["study_hours"]),
                "attendance": float(row["attendance"]),
                "previous_grade": float(row["previous_grade"])
            }
            self.student_data.append(student)
            self.tree.insert("", END, values=(student["name"], student["study_hours"], student["attendance"], student["previous_grade"], "Pending"))
Users can add a student manually or import from CSVData is stored in self.student_data and displayed in a tabledef start(self):
    if not self.student_data:
        messagebox.showwarning("No Data", "Add at least one student before predicting.")
        return
    self.start_btn.config(state=DISABLED)
    self.cancel_btn.config(state=NORMAL)
    threading.Thread(target=self._run_worker, daemon=True).start()
Starts predictions in a separate threadPrevents the GUI from freezingdef update_student_grade(self, student, grade):
    for i in self.tree.get_children():
        vals = self.tree.item(i)["values"]
        if vals[0] == student["name"]:
            self.tree.item(i, values=(vals[0], vals[1], vals[2], vals[3], grade))
            break
Updates table with predicted grade in real-timedef export_results(self):
    path = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV Files","*.csv")])
    if path:
        with open(path,"w",newline="",encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["name","study_hours","attendance","previous_grade","predicted_grade"])
            for i in self.tree.get_children():
                writer.writerow(self.tree.item(i)["values"])
        messagebox.showinfo("Export", "Export completed successfully.")
Save the predicted results as a CSV fileEasy for sharing or record-keepingFinally, run the application:if __name__ == "__main__":
    app = ExamResultPredictorApp()
    app.run()
Result:
You now have a fully functioning ExamResultPredictor v2.0!Add students manually or via CSVPredict grades using a machine learning modelExport results for analysisðŸŽ‰ Next Steps for Learners:Replace the dummy model with real historical dataExperiment with different machine learning modelsAdd charts or analytics for visualization]]></content:encoded></item><item><title>Kushal Das: replyfast a python module for signal</title><link>https://kushaldas.in/posts/replyfast-a-python-module-for-signal.html</link><author></author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 12:16:49 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[python3 -m pip install replyfastI have to add Windows builds to CI though.I have a script to help you to register as a device, and
then you can send and receive messages.I have a demo bot which shows both sending and rreceiving messages, and also how to schedule work following the  syntaxt.    scheduler.register(
        "*/5 * * * *",
        send_disk_usage,
        args=(client,),
        name="disk-usage",
    )
This is all possible due to the presage library written in Rust.]]></content:encoded></item><item><title>Building Reliable State Handoffs Between AI Agent Sessions</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/building-reliable-state-handoffs-between-ai-agent-sessions-1bk3</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 12:15:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Your AI agent works great... until it restarts. Then it wakes up with no idea what it was doing, why, or what matters. The persistence layer saved data. But the  â€” the "what was I thinking" â€” is gone.This is the handoff problem. And if you're building agents that run across multiple sessions, it's the hardest part of the architecture to get right.
  
  
  Why Handoffs Are Harder Than Persistence
My previous article covered what to persist: configuration, accumulated state, operational context. But persistence is storage. Handoffs are  â€” from your past self to your future self.The difference matters. A database stores facts. A handoff tells a story: here's where we are, here's what matters right now, and here's what to do next.Get persistence wrong and you lose data. Get handoffs wrong and your agent spends its first five minutes confused, re-discovering what it already knew, or worse â€” making decisions based on stale assumptions.
  
  
  The Four Handoff Anti-Patterns
Before the solution, let's catalog the failures. I've seen (and built) all of these:The next session gets  and understands . Information without prioritization is noise. Your agent either parses all of it (slow, context-heavy) or gives up and starts fresh.Just a file path. No summary, no priority, no narrative. The next session opens the file and faces the same cold-start problem, just one level deeper. This is delegation, not handoff.No explicit handoff at all. "The framework handles continuity." It doesn't. Frameworks handle data flow. They don't understand which of your 15 active tasks is urgent versus background. They don't know that the API you were calling started returning 429s and you switched strategies. They don't know  things are the way they are.# Check the logs from 14:00-14:30 for context
# The relevant PR is #847
# See Slack thread from yesterday
Your next session has to do forensic work to reconstruct what the previous session knew. This is fragile (logs rotate, threads get buried) and expensive (your agent burns context window on archaeology instead of work).
  
  
  A Structured Handoff Protocol
Here's what actually works. A good handoff has five layers:The raw facts. Current values of critical variables, in a typed, validated format.This is the "what" â€” the current state of the world.
  
  
  Layer 2: Narrative Context
Three to five sentences, human-readable, explaining what happened and why the state looks the way it does.This is the "why" â€” the story behind the data.What was decided, what was deferred, and what trade-offs were made.This prevents your next session from re-litigating resolved questions or missing deferred work.What the next session should do first, second, and third.Explicit priority removes the most common cold-start problem: "I have 12 things I could do. Which one matters right now?"
  
  
  Layer 5: Warnings and Gotchas
The things that will bite the next session if nobody mentions them.This is institutional knowledge that exists nowhere except in the previous session's working memory.These come from building systems that actually use multi-session handoffs:
  
  
  1. Handoff Loaders Fail Silently
The most dangerous failure mode isn't a crash â€” it's a loader that  but doesn't actually populate the agent's context. The handoff file gets read. The JSON parses. But the agent starts its work loop without checking whether the loaded data made it into working memory.: Always verify after loading. Have your agent explicitly reference handoff data in its first action. If it can't, the load failed.
  
  
  2. Redundancy Beats Optimization
Don't put all your continuity in one file. Use multiple channels:: The structured handoff file: A state.json with running totals and current values: Human-readable journal entriesIf any one channel fails, the others provide enough context to recover. This sounds wasteful. It's not. The cost of a confused agent re-doing work far exceeds the cost of writing three small files.
  
  
  3. Human-Readable Beats Machine-Optimized
Binary formats, compressed state, clever encodings â€” they all break when you need to debug at 3 AM. Make your handoffs readable by a human with a text editor. JSON with clear key names. Narrative summaries in plain language.When something goes wrong (and it will), you want to  the handoff file and immediately understand the agent's last known state.
  
  
  4. Test With Real Restarts
Write your handoff. Kill the agent. Restart it. Did it pick up where it left off? Not "did it load the file" â€” did it actually  correctly?Most handoff bugs only surface under real restart conditions. Simulated loads in the same process don't catch issues like stale file handles, cached state that masks a bad load, or race conditions between the write and the next session's read.Your handoff format will evolve. Your agent from two weeks ago wrote v0.8 handoffs. Your agent today expects v1.0. Without a version field, your loader silently misinterprets fields and your agent makes decisions based on misread data.The handoff is where engineering meets epistemology. You're not just passing data â€” you're passing . Your future agent self needs to reconstruct enough of your current mental model to make good decisions, without having lived through the experiences that built that model.This is fundamentally a compression problem: how do you compress a session's worth of experience into something small enough to transmit and rich enough to be useful?The five-layer protocol works because it compresses along multiple dimensions simultaneously â€” facts (state), story (narrative), reasoning (decisions), action (priorities), and caution (warnings). No single layer is sufficient. Together, they give the next session what it needs to start working instead of start orienting.Build your handoffs like you're writing a note to a colleague who's taking over your shift. Because that's exactly what you're doing.]]></content:encoded></item><item><title>Quiz: GeoPandas Basics: Maps, Projections, and Spatial Joins</title><link>https://realpython.com/quizzes/geopandas/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[In this quiz, youâ€™ll test your understanding of GeoPandas.Youâ€™ll review coordinate reference systems, GeoDataFrames, interactive maps, and spatial joins with . Youâ€™ll also explore how projections affect maps and learn best practices for working with geospatial data.This quiz helps you confirm that you can prepare, visualize, and analyze geospatial data accurately using GeoPandas.]]></content:encoded></item><item><title>Building Tamper-Proof Dual Audit Trails with VCP-XREF: A Developer&apos;s Guide</title><link>https://dev.to/veritaschain/building-tamper-proof-dual-audit-trails-with-vcp-xref-a-developers-guide-3flc</link><author>VeritasChain Standards Organization (VSO)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 11:32:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Ever tried to prove that a log file  tampered with? It's surprisingly hard. Hash chains prove sequence integrity. Signatures prove authorship. But neither proves that the signer didn't just... rewrite everything before signing.This is the core problem in trading systems, especially prop trading where 80-100 firms collapsed in 2024-2025 amid payout disputes. The firm controls the logs. The trader has no recourse.VCP-XREF solves this with a simple insight: when two independent parties log the same event, discrepancies reveal manipulation.
  
  
  The Problem: Single-Party Logs Are Inherently Untrustworthy
The problem? The owner can:Delete events before signingModify events before signing
Rewrite the entire log and re-signClaim events never happenedVerification passes. Trust fails.
  
  
  The Solution: VCP-XREF Dual Logging
VCP-XREF creates two independent audit trails that reference each other:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Party A       â”‚â—„â”€â”€â”€â”€ Trade â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Party B       â”‚
â”‚   (Trader)      â”‚                    â”‚   (Broker)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VCP Log A     â”‚                    â”‚   VCP Log B     â”‚
â”‚   + XREF ID     â”‚â—„â”€â”€â”€ Compare â”€â”€â”€â”€â”€â”€â–ºâ”‚   + XREF ID     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  External       â”‚                    â”‚  External       â”‚
â”‚  Anchor A       â”‚                    â”‚  Anchor B       â”‚
â”‚  (TSA/Chain)    â”‚                    â”‚  (TSA/Chain)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
: Unless both parties collude AND compromise both external anchors, manipulation is detectable.Here's the complete schema definition:UUID linking events across partiesWho logged this: initiator, counterparty, or observerCorrelation data both parties must agree onAllowed timestamp difference (clock sync variance)Current verification stateLet's build a complete VCP-XREF implementation.Initiator XREF ID: 550e8400-e29b-41d4-a716-446655440000
Initiator Hash: a1b2c3d4e5f6g7h8...
Counterparty Hash: x9y8z7w6v5u4t3s2...

Reconciliation Status: MATCHED
Let's simulate a slippage manipulation attempt:Reconciliation Status: DISCREPANCY
Discrepancy Detected!
  Field: price
  Trader logged: 1.085
  Broker logged: 1.0855
  Severity: CRITICAL
The manipulation is caught automatically. Neither party can deny what they logged because both records are signed and anchored.Cross-reference alone isn't enough. Both parties could theoretically collude to rewrite history. External anchoring prevents this.
  
  
  RFC 3161 Timestamp Authority

  
  
  Bitcoin Anchoring (via OP_RETURN)

  
  
  Merkle Tree for Batch Verification
For efficiency, batch events into Merkle trees before anchoring:
  
  
  Complete Flow: Order Lifecycle with XREF
Here's a complete example of an order lifecycle with dual logging:Single-party log modificationCounterparty log provides independent evidenceExternal anchors make post-hoc collusion detectableUUID + timestamp uniquenessMissing counterparty record is itself evidenceAnchor timestamps prove when records existedFor undetectable manipulation, attackers must compromise:âœ… Party A's external anchor (e.g., DigiCert TSA)âœ… Party B's external anchor (e.g., Bitcoin)If different anchor systems are used, this becomes practically infeasible.VCP-XREF helps meet requirements from:: Automatic logging with third-party verification: Order lifecycle records with precise timestamps: Audit trail alternative to WORM storage: Cryptographic integrity for trading records   git clone https://github.com/veritaschain/vcp-spec
Install reference implementation:
Single-party audit logs will always be vulnerable to the fundamental problem: the party controlling the logs controls the truth.VCP-XREF solves this by requiring: of the same event to prevent retroactive modificationCross-reference verification to detect discrepanciesThe math is simple. The implementation is straightforward. The trust model is fundamentally different. isn't just a slogan. It's architecture.The VCP specification is open-source under CC BY 4.0. Contributions welcome.]]></content:encoded></item><item><title>Skills, Not Vibes: Teaching AI Agents to Write Clean Code</title><link>https://dev.to/gde/skills-not-vibes-teaching-ai-agents-to-write-clean-code-3l9e</link><author>ErtuÄŸrul Demir</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 11:17:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In February 2025, Andrej Karpathy coined "vibe coding" to describe programming's new reality: give in to the vibes, accept all changes, "forget that the code even exists." He called it "not too bad for throwaway weekend projects." But for production systems? That's where the trouble starts.I've watched AI-generated codebases accumulate the same mess developers spent decades learning to avoidâ€”duplication everywhere, inconsistent naming, missing edge cases. Then it hit me: these are exactly the problems Robert C. Martin warned about in  almost two decades ago.So I went back to the book, specifically Chapter 17's catalog of 66 code smells and heuristics. These aren't just relevant to AI codingâ€”they're  relevant. AI makes exactly the mistakes Uncle Bob warned us about, just faster and at scale.The solution? â€”instruction files that AI agents read before writing code. I've translated Clean Code's complete catalog into Python skills you can use today. They work in Google's Antigravity IDE, Anthropic's Claude Code, and anywhere that supports the Agent Skills standard.Let me show you why we need this, and how to implement it.
  
  
  Even Linus Torvalds Vibe Codes (Sometimes)
In January 2026, Linus Torvalds revealed a side project called AudioNoiseâ€”a digital audio effects simulator he'd been tinkering with over the holidays. The Python visualizer, he noted, was "basically written by vibe-coding."In his own words from the repo:"I know more about analog filtersâ€”and that's not saying muchâ€”than I do about python. It started out as my typical 'google and do the monkey-see-monkey-do' kind of programming, but then I cut out the middle-manâ€”meâ€”and just used Google Antigravity to do the audio sample visualizer."The Hacker News discussion revealed two camps. Some saw it as validation: "It's official, vibe coding is legit." Others noted the crucial context: Torvalds used AI for the part he lacks expertise in (Python visualization) while hand-coding the parts he knows (C and digital signal processing).One commenter nailed it: "There's a big difference between vibe-coding an entire project and having an AI build a component that you lack competency for."Another observation cut deeper: "If anyone on the planet knows how to do vibe coding right, it's him"â€”because Torvalds spent decades mastering code review. He can spot bad code instantly. Most of us can't.But here's what's telling: Torvalds wrote tests for his hand-coded Câ€”numerical accuracy checks for the DSP primitives he understands. The vibe-coded Python visualizer? No tests, no type hints, and a duplicated function definition that slipped right through. The same four-line method appears twice in a rowâ€”the first an empty stub, the second the real implementation. It's textbook "Accept All, don't read the diffs." The code runs fine (Python silently overwrites the first definition), but it's exactly the kind of dead code that accumulates into maintenance nightmares.This works for Torvalds' toy project precisely. It's a throwaway learning exercise. The moment that visualizer needs to be production code, those missing guardrails become technical debt.The same week, Torvalds rejected "AI slop" submissions to the Linux kernel, arguing that documentation telling people not to submit garbage won't help because "the people who would submit it won't read the documentation anyway."The lesson isn't that vibe coding is bad. It's that context matters. Skills let you define when to enforce rigor and when to let the vibes flow.
  
  
  The Data: AI Code Quality Is Getting Worse
  found AI adoption shows a negative relationship with software delivery stability. The 2025 report's central finding: "AI doesn't fix a team; it amplifies what's already there." Without robust control systemsâ€”strong testing, mature practices, fast feedback loopsâ€”increased AI-generated code leads to instability. Skills are exactly those control systems, encoded as instructions. analyzed 807 GitHub repositories after Cursor adoption: +30% static analysis warnings, +41% code complexity. The speed gains were transient; the quality problems compounded. analysis of 211 million lines of code from Google, Microsoft, Meta, and enterprise repositories found code duplication increased  with AI adoption. For the first time in their dataset, copy/pasted code exceeded refactored code.Even  shows the gap: developers use AI in roughly 60% of their work, but can fully delegate only 0-20% of tasks. The rest requires "thoughtful setup, active supervision, and human judgment."That gapâ€”between what AI touches and what AI can ownâ€”is exactly what skills address. The setup  the skill. The supervision  the rules.
  
  
  The Pattern: AI Recreates Classic Code Smells
The research consistently identifies the same failure patterns. Here's how they map to specific Clean Code violations:Naming and Consistency ProblemsInconsistent variable names across similar functionsVague names like , , Mixing naming conventions (camelCase and snake_case)Clean Code rules: N1 (descriptive names), G11 (consistency), G24 (conventions)Copy/paste instead of extracting shared logicSame calculation appearing in multiple placesPattern repetition that should be abstractedClean Code rule: G5 (DRY - Don't Repeat Yourself)No validation of input boundariesAssumptions about data structure without verificationClean Code rules: G3 (boundary conditions), G4 (don't override safeties), G26 (be precise)Magic numbers without explanation (what does 86400 mean?)Unused variables cluttering codeFunctions mixing multiple abstraction levelsClean Code rules: G12 (remove clutter), G16 (no obscured intent), G34 (single abstraction level)Functions doing multiple things at onceExposing internal data unnecessarilyNested loops that could be optimizedClean Code rules: G8 (minimize public interface), G30 (functions do one thing)These aren't arbitrary style preferencesâ€”they're the exact problems that make code hard to maintain, debug, and extend. The skills we'll build enforce these rules automatically.The fix isn't to stop using AI. It's to give AI the explicit rules it needs to follow.Skills are markdown files containing domain-specific instructions that AI agents read before working on your code. They follow the Agent Skills open standard and work in Google Antigravity, Anthropic's Claude Code, and other compatible agents.The architecture is called . Instead of dumping every instruction into the agent's context at once (causing what Antigravity's docs call "Context Saturation"), skills work in layers:: The agent sees only a lightweight menu of skill names and descriptions: When your request matches a skill's description, the full instructions load: Scripts and templates are read only when the task requires themThis keeps the agent fast and focused. It's not thinking about database migrations when you're writing a React component.The  field is crucialâ€”it's the trigger phrase. The agent semantically matches your request against all available skill descriptions to decide which ones to load. "Enforces function best practices" is vague. "Use when writing or refactoring Python functions" tells the agent exactly when to activate.Skills can do far more than enforce coding standardsâ€”the community has built skills for Stripe integration, Metasploit security testing, voice agents, and even multi-agent startup automation. This article focuses on one specific use case: encoding Clean Code principles.Let me show you how to translate Clean Code's catalog into working skills.
  
  
  Building the Skills: Three Examples
Rather than catalog all 66 rules exhaustively, I'll show you three critical categories in detail. The complete implementation is at the end.
  
  
  1. Comments (C1-C5): Code Should Explain Itself
Uncle Bob is famously skeptical of commentsâ€”not because documentation is bad, but because comments rot faster than code updates.File Reference: 

Comments shouldn't hold metadata. Use Git for author names, change history, 
ticket numbers, and dates. Comments are for technical notes about code only.



If a comment describes code that no longer exists or works differently, 
delete it immediately. Stale comments become "floating islands of 
irrelevance and misdirection."


i += 1  # increment i
user.save()  # save the user


i += 1  # compensate for zero-indexing in display



If a comment is worth writing, write it well:
 Choose words carefully
 Use correct grammar
 Don't ramble or state the obvious
 Be brief



Who knows how old it is? Who knows if it's meaningful? Delete it. 
Git remembers everything.



The best comment is the code itself. If you need a comment to explain 
what code does, refactor first, comment last.

  
  
  2. Functions (F1-F4): Small, Focused, Obvious
Functions should do one thing, do it well, and have an obvious purpose.File Reference: 
def create_user(name, email, age, country, timezone, language, newsletter):
    ...


@dataclass
class UserData:
    name: str
    email: str
    age: int
    country: str
    timezone: str
    language: str
    newsletter: bool

def create_user(data: UserData):
    ...

More than 3 arguments means your function is doing too much or needs 
a data structure.



Don't modify arguments as side effects. Return values instead.


def append_footer(report: Report) -> None:
    report.append("---Generated by System")


def with_footer(report: Report) -> Report:
    return report + "---Generated by System"



Boolean flags mean your function does at least two things.


def render(is_test: bool):
    if is_test:
        render_test_page()
    else:
        render_production_page()


def render_test_page(): ...
def render_production_page(): ...



If it's not called, delete it. No "just in case" code. Git preserves history.

  
  
  3. General Principles (G1-G36): The Core Rules
These are the fundamental patterns that separate clean code from legacy nightmares.File Reference: 

Every piece of knowledge has one authoritative representation.


tax_rate = 0.0825
ca_total = subtotal  1.0825
ny_total = subtotal  1.07


TAX_RATES = {"CA": 0.0825, "NY": 0.07}
def calculate_total(subtotal: float, state: str) -> float:
    return subtotal  (1 + TAX_RATES[state])



Don't be clever. Be clear.


return (x & 0x0F) << 4 | (y & 0x0F)


return pack_coordinates(x, y)


def calculate_pay(employee):
    if employee.type == "SALARIED":
        return employee.salary
    elif employee.type == "HOURLY":
        return employee.hours  employee.rate
    elif employee.type == "COMMISSIONED":
        return employee.base + employee.commission


class SalariedEmployee:
    def calculate_pay(self): return self.salary

class HourlyEmployee:
    def calculate_pay(self): return self.hours  self.rate

class CommissionedEmployee:
    def calculate_pay(self): return self.base + self.commission


if elapsed_time > 86400:
    ...


SECONDS_PER_DAY = 86400
if elapsed_time > SECONDS_PER_DAY:
    ...



If you can extract another function, your function does more than one thing.


output_dir = context.options.scratch_dir.absolute_path


output_dir = context.get_scratch_dir()



When reviewing AI-generated code, verify:
 [ ] No duplication (G5)
 [ ] Clear intent, no magic numbers (G16, G25)
 [ ] Polymorphism over conditionals (G23)
 [ ] Functions do one thing (G30)
 [ ] No Law of Demeter violations (G36)
 [ ] Boundary conditions handled (G3)
 [ ] Dead code removed (G9)
I've translated all 66 rules from Clean Code Chapter 17 into skills covering six categories:
  Click to expand all skill categories
  : Minimal, accurate commentingC1: No inappropriate information (metadata belongs in version control)C2: Delete obsolete comments immediatelyC3: No redundant comments that repeat the codeC4: Write comments wellâ€”brief, grammatical, purposefulC5: Never commit commented-out code: One-command build and testE1: Build requires only one stepE2: Tests require only one step: Small, focused, obviousF1: Maximum 3 arguments (use data structures for more)F2: No output arguments (return values instead)F3: No flag arguments (split into separate functions)F4: Delete dead functions: Core principlesG1: Multiple languages in one source fileG2: Obvious behavior is unimplementedG3: Incorrect behavior at the boundariesG6: Code at wrong level of abstractionG7: Base classes depending on their derivativesG17: Misplaced responsibilityG18: Inappropriate staticG19: Use explanatory variablesG20: Function names should say what they doG21: Understand the algorithmG22: Make logical dependencies physicalG23: Prefer polymorphism to if/else or switch/caseG24: Follow standard conventionsG25: Replace magic numbers with named constantsG27: Structure over conventionG28: Encapsulate conditionalsG29: Avoid negative conditionalsG30: Functions should do one thingG31: Hidden temporal couplingsG33: Encapsulate boundary conditionsG34: Functions should descend only one level of abstractionG35: Keep configurable data at high levelsG36: Avoid transitive navigation: Descriptive, unambiguous, right-sizedN1: Choose descriptive namesN2: Choose names at the right abstraction levelN3: Use standard nomenclature where possibleN4: Use unambiguous namesN5: Use long names for long scopesN6: Avoid encodings (Hungarian notation, etc.)N7: Names should describe side effects: Fast, independent, exhaustiveT1: Insufficient testsâ€”test everything that could breakT3: Don't skip trivial testsT4: Ignored tests indicate ambiguityT5: Test boundary conditionsT6: Exhaustively test near bugsT7: Patterns of failure are diagnosticT8: Coverage patterns can be revealingGet the complete skill files:Teach your AI to write code that doesn't suck.This repository contains Agent Skills that enforce Robert C. Martin's  principles. They work with Google Antigravity, Anthropic's Claude Code, and any agent that supports the Agent Skills standard.AI generates code fast, but research shows it also generates technical debt fast:: 4x increase in code duplication with AI adoption: +30% static analysis warnings, +41% code complexity after Cursor adoption: Negative relationship between AI adoption and software delivery stabilityThese skills encode battle-tested solutions to exactly these problemsâ€”directly into your AI workflow.â€”always leave code cleaner than you found it with all 66 rulesC1-C5, E1-E2, F1-F4, G1-G36, N1-N7, P1-P3, T1-T9Minimal, accurate commentingSmall, focused, obvious functionsâ€¦: An orchestrator skill that embodies the Boy Scout Ruleâ€”"always leave code cleaner than you found it"â€”and coordinates the other skills: A master skill with all 66 rules, plus a quick reference table and anti-patterns cheatsheet for each category (, , , , )â€”drop in only what you needInstallation instructions for Antigravity, Claude Code, and other Agent Skills-compatible toolsSkills sit in a specific place in the agent ecosystem.  are passive guardrails that are always on.  are agent-triggeredâ€”the model decides when to equip them based on your intent. If you're using MCP servers (connections to external tools like GitHub or Postgres), think of MCP as the "hands" and skills as the "brains" that direct them.Create  in your project root (or ~/.gemini/antigravity/skills/ for global access)Save the skill as a folder with a  file inside (e.g., .agent/skills/python-clean-code/SKILL.md)Ask the agent to review or write codeâ€”it'll automatically apply the rules when relevant: : ~/.gemini/antigravity/skills/The agent only loads full skill content when needed, so comprehensive skills don't slow down simple requests.The skills in this article are instruction-onlyâ€”they tell the agent what to do. For stricter enforcement, you could add a  folder with a linter that compatible agents runs them automatically, or an  folder with before/after code samples for few-shot learning. The format supports it; we're just keeping things simple here.Here's code that violates multiple Clean Code rules:: P1, C1, C3, C5, F1, F3, G6, G23, G25, N1With the Clean Code skill active, ask your AI agent to refactor this:âœ… No wildcard imports (P1)âœ… No metadata comments (C1)âœ… No redundant comments (C3)âœ… No commented-out code (C5)âœ… Named constants instead of magic numbers (G25)âœ… Functions do one thing (G30)âœ… Polymorphism through data structure (G23)
  
  
  Anatomy of a Vibe-Coded Script
The first definition unpacks values, calculates width, then... returns . The second definition is the real implementation. Python silently overwrites the first with the second, so the code runs. But it's textbook dead codeâ€”Clean Code rule G9: Remove dead code.With the skill active, an agent refactors the entire 600-line script. The duplicate vanishes, magic numbers become constants, and nested functions get extracted into focused methods:âœ… Type hints added (clarity)âœ… Single, authoritative definition (G5)âœ… Magic numbers extracted to constants (G25)âœ… Large methods decomposed (G30)The full diff shows 600+ lines reduced to ~440â€”not by removing functionality, but by eliminating duplication and extracting reusable patterns.Vibe coding isn't going away. AI will get better at generating code, not worse. But "better at generating" doesn't mean "better at maintaining."The research is clear: AI produces code faster, but that code accumulates technical debt faster too. Without guard rails, we're building tomorrow's legacy systems today.Uncle Bob's Clean Code principles are almost 20 years old, but they're exactly what we need now. They're not arbitrary style preferencesâ€”they're battle-tested solutions to the problems AI recreates at scale.Skills give you the mechanism to encode these rules directly into your AI workflow. Whether you're using Antigravity, Claude Code, or another agent, the approach is the same: define what clean code means, then let the AI follow the rules.Your agent doesn't know what good code looks like unless you tell it. by Robert C. Martin: Amazon
Clean Code Skills Repository â€” All 66 rules as ready-to-use skill filesThe future of programming is human intent translated by AI. Make sure the translation preserves quality, not just speed.]]></content:encoded></item><item><title>â›ï¸ Hard_Negative_Mining</title><link>https://dev.to/stklen/hardnegativemining-592p</link><author>TK Lin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 11:00:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  ðŸŽ¯ Hard Negative Miningã¨ã¯ï¼Ÿ
AIãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒ80%ã«é”ã—ãŸã‘ã©ã€æ®‹ã‚Šã®20%ã®ã‚¨ãƒ©ãƒ¼ã¯ã©ã†ã™ã‚‹ï¼Ÿã¯ã€ã“ã‚Œã‚‰ã®ã€ŒåŽ„ä»‹ãªã‚¨ãƒ©ãƒ¼ã€ã‚’ç‰¹å®šã—ã€AIã«çš„ã‚’çµžã£ã¦å­¦ç¿’ã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚ã“ã‚Œã‚‰ã®ã€ŒHard Negativesã€ãŒæœ€ã‚‚èªè­˜ã—ã«ãã„ã‚±ãƒ¼ã‚¹ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«å‘ä¸Šã®éµã§ã™ï¼ï¼šã“ã‚Œã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ˆã‚Šå¤šãã®å¤‰æ›ã‚’é©ç”¨ï¼šloss functionã§é‡ã¿ã‚’å¢—ã‚„ã™2,451æžšã®ãƒ†ã‚¹ãƒˆç”»åƒã§467ä»¶ã®ã‚¨ãƒ©ãƒ¼ï¼ˆ19%ï¼‰ã‚’ç™ºè¦‹ã€‚ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° â†’ ã‚¨ãƒ©ãƒ¼ç™ºè¦‹ â†’ åŽŸå› åˆ†æž â†’ ãƒ‡ãƒ¼ã‚¿ä¿®æ­£ â†’ å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
      â†‘                                                        |
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ã“ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€æ¯Žå›žå°‘ã—ãšã¤ç²¾åº¦ã‚’å‘ä¸Šã§ãã¾ã™ã€‚ï¼šAIãŒã‚¨ãƒ©ãƒ¼ã‚’è¦‹ã¤ã‘ã€äººé–“ãŒä¿®æ­£ã‚’ç¢ºèªHard Negative Miningã¯ä¸€åº¦ãã‚Šã®ä½œæ¥­ã§ã¯ãªãã€ç¶™ç¶šçš„ãªæ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚ã“ã®æ–¹æ³•ã§ã€ç²¾åº¦ã‚’79.5%ã‹ã‚‰83.2%ã«å‘ä¸Šã•ã›ã€ã•ã‚‰ã«æ”¹å–„ä¸­ã§ã™ï¼]]></content:encoded></item><item><title>Prompt Rate Limits &amp; Batching: How to Stop Your LLM API From Melting Down</title><link>https://dev.to/superorange0707/prompt-rate-limits-batching-how-to-stop-your-llm-api-from-melting-down-56e1</link><author>Dechun Wang</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 10:53:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You ship a feature, your traffic spikes, and suddenly your LLM layer starts returning  like itâ€™s handing out parking tickets.The bad news: rate limits are inevitable.The good news: most LLM â€œrate limit incidentsâ€ are self-inflictedâ€”usually by oversized prompts, bursty traffic, and output formats that are impossible to parse at scale.This article is a practical playbook for:1) understanding prompt-related throttles,
2) avoiding the common failure modes, and
3) batching requests without turning your responses into soup.Different providers name things differently, but the mechanics are consistent:
  
  
  1.1 Context window (max tokens per request)
If your  exceeds the model context window, the request fails immediately.â€œMaximum context length exceededâ€â€œYour messages resulted in X tokensâ€¦â€shorten, summarise, or chunk data.
  
  
  1.2 RPM (Requests Per Minute)
You can be under token limits and still get throttled if you burst too many calls. Gemini explicitly documents RPM as a core dimension. â€œRate limit reached for requests per minuteâ€client-side pacing, queues, and backoff.
  
  
  1.3 TPM / Token throughput limits
Anthropic measures rate limits in RPM + input tokens/minute + output tokens/minute (ITPM/OTPM).
Gemini similarly describes token-per-minute as a key dimension. â€œRate limit reached for token usage per minuteâ€429 + Retry-After header (Anthropic calls this out) reduce tokens, batch efficiently, or request higher quota.
  
  
  2.1 The â€œone prompt to rule them allâ€ anti-pattern
â€¦in a single request, and then you wonder why token usage spikes.. If you need multi-step logic, use  (small prompts with structured intermediate outputs).
  
  
  2.2 Bursty traffic (the silent RPM killer)
Production traffic is spiky. Cron jobs, retries, user clicks, webhook burstsâ€”everything aligns in the worst possible minute.If your client sends requests like a machine gun, your provider will respond like a bouncer.
  
  
  2.3 Unstructured output = expensive parsing
If your output is â€œkinda JSON-ishâ€, your parser becomes a full-time therapist.Make the model output  or a fixed table. Treat format as a contract.
  
  
  3.1 Prompt-side: shrink tokens without losing signal
 (models donâ€™t need your company origin story).Convert repeated boilerplate into a short â€œpolicy blockâ€ and reuse it.Prefer  over prose (â€œmaterial=316 stainless steelâ€ beats a paragraph).
  
  
  A tiny prompt rewrite that usually saves 30â€“50%
â€œWeâ€™re a smart home brand founded in 2010â€¦ please write 3 marketing linesâ€¦â€â€œWrite 3 UK e-commerce lines. Product: smart bulb. Material=PC flame-retardant. Feature=3 colour temperatures. Audience=living room.â€
  
  
  3.2 Request-side: backoff like an adult
If the provider returns , respect it. Anthropic explicitly returns Retry-After on 429s. Use exponential backoff + jitter:
  
  
  3.3 System-side: queue + concurrency caps
If your account supports 10 concurrent requests, do not run 200 coroutines and â€œhopeâ€.a  for concurrencyand a  for RPM/TPMBatching means: one API request handles multiple independent tasks.It works best when tasks are:same type (e.g., 20 product blurbs)independent (no step depends on another)fewer network round-tripsfewer requests â†’ lower RPM pressuremore predictable throughputAlso: OpenAIâ€™s pricing pages explicitly include a â€œBatch API priceâ€ column for several models. 
(That doesnâ€™t mean â€œbatching is freeâ€, but itâ€™s a strong hint the ecosystem expects this pattern.)Hereâ€™s a format that stays parseable under pressure.
  
  
  5.1 Use task blocks + a strict JSON response schema
SYSTEM: You output valid JSON only. No Markdown. No commentary.

USER:
You will process multiple tasks. 
Return a JSON array. Each item must be:
{
  "task_id": <int>,
  "title": <string>,
  "bullets": [<string>, <string>, <string>]
}

Rules:
- UK English spelling
- Title â‰¤ 12 words
- 3 bullets, each â‰¤ 18 words
- If input is missing: set title="INSUFFICIENT_DATA" and bullets=[]

TASKS:
### TASK 1
product_name: Insulated smart mug
material: 316 stainless steel
features: temperature alert, 7-day battery
audience: commuters

### TASK 2
product_name: Wireless earbuds
material: ABS shock-resistant
features: ANC, 24-hour battery
audience: students
That â€œINSUFFICIENT_DATAâ€ clause is your lifesaver. One broken task shouldnâ€™t poison the whole batch.Below is a modern-ish pattern you can adapt (provider SDKs vary, so treat it as , not a copyâ€‘paste guarantee).
  
  
  What changed vs â€œclassicâ€ snippets?
We treat JSON as a .We handle  explicitly (and keep it cheap).We centralise backoff logic so every call behaves the same way.Batch size is constrained by:context window (max tokens per request)response parsing stabilityyour business tolerance for â€œone batch failedâ€start with increase until you see:

timeouts / latency spikes, or
And always keep a .Pricing changes. Tiers change. Models change.So instead of hard-coding ancient per-1K token values, calculate cost using the providerâ€™s current pricing page.OpenAI publishes perâ€‘token pricing on its API pricing pages.
Anthropic also publishes pricing and documents rate limit tiers. cost â‰ˆ (input_tokens * input_price + output_tokens * output_price) / 1,000,000
Then optimise the variables you control:reduce number of calls (batch)

  
  
  Risk 1: one bad item ruins the batch
 â€œINSUFFICIENT_DATAâ€ fallback per task.
  
  
  Risk 2: output format drift breaks parsing
 strict JSON, repair step, and logging.
  
  
  Risk 3: batch too big â†’ context overflow
 token budgeting + auto-splitting.
  
  
  Risk 4: â€œcreativeâ€ attempts to bypass quotas
 donâ€™t. If you need more capacity, request higher limits and follow provider terms.Rate limits arenâ€™t the enemy. Theyâ€™re your early warning system that:or your architecture assumes â€œinfinite throughputâ€.If you treat prompts like payloads (not prose), add pacing, and batch like a grown-up, youâ€™ll get:and a system that scales without drama]]></content:encoded></item><item><title>Day 11 of 100</title><link>https://dev.to/palakhirave/day-11-of-100-oi9</link><author>Palak Hirave</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 10:08:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Today I worked on a Blackjack project. This was far more difficult than I thought it would be and took endless rounds of Blackjack to write the algorthim. I managed to crack the large majority of the challenge but I still needed plently of help from the internet. I understood the main outline of what I had to program and what should be outputted but the main challenge lies in the execution of it. I fiddled around with using functions for various things as well as debating between for loops and while loops. Overall, it provided a good test of everything I learnt so far and was accomplishable over the course of a day paired in with plently of snack breaks. ]]></content:encoded></item><item><title>FastAPI + SQLAlchemy 2.0 in Production: Building High-Performance Async APIs</title><link>https://dev.to/ayush_kaushik_b450595c233/fastapi-sqlalchemy-20-in-production-building-high-performance-async-apis-11ni</link><author>Ayush Kaushik</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 08:40:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Building async APIs with FastAPI and SQLAlchemy 2.0 looks straightforward in tutorials, until you deploy to production.Suddenly you start seeing issues like random MissingGreenlet errors, confusing async session behavior, blocked event loops, or database calls that are technically â€œasyncâ€ but still slow under load. These problems usually appear when teams migrate from synchronous Flask or Django applications to FastAPI without fully understanding how async architecture actually works.This article is not a beginnerâ€™s FastAPI tutorial.It is a practical, production-focused guide to building high-performance async backend APIs using FastAPI and SQLAlchemy 2.0, covering real-world concerns such as async engine configuration, session lifecycle management, lifespan events, connection pooling, and common failure modes.If you are already using FastAPI (or planning a migration from Flask) and want an async architecture that scales cleanly beyond toy examples, this guide is written for you.First, let's grab our dependencies. Notice we need an async driver (aiosqlite) because standard drivers like psycopg2 or sqlite3 are synchronous and will block your loop.pip install fastapi uvicorn sqlalchemy aiosqlite pydanticThe Database Engine (database.py)The most critical part of an async setup is the AsyncEngine. If you initialize this wrong, your whole app runs synchronously.from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
from sqlalchemy.orm import DeclarativeBase

# 1. Connection String (Note the +aiosqlite driver)
# For Postgres, use: postgresql+asyncpg://user:pass@localhost/dbname
SQLALCHEMY_DATABASE_URL = "sqlite+aiosqlite:///./test.db"

# 2. Create the Async Engine
engine = create_async_engine(
    SQLALCHEMY_DATABASE_URL,
    echo=True, # Logs SQL queries to console (Great for debugging)
)

# 3. Create the Session Factory
# This is what generates new database sessions for each request
AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# 4. Base Class for Models
class Base(DeclarativeBase):
    pass

# 5. Dependency Injection
# We use this in our FastAPI routes to get a DB session
async def get_db():
    async with AsyncSessionLocal() as session:
        yield session
2. The Models (models.py)SQLAlchemy 2.0 introduced a beautiful new way to define models using Python type hints (Mapped). No more vague Column(Integer, ...) syntax.from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy import String, Integer, Boolean
from database import Base

class Task(Base):
    __tablename__ = "tasks"

    id: Mapped[int] = mapped_column(primary_key=True, index=True)
    title: Mapped[str] = mapped_column(String(50), index=True)
    description: Mapped[str] = mapped_column(String(255), nullable=True)
    is_completed: Mapped[bool] = mapped_column(default=False)
3. The Schemas (schemas.py)Pydantic handles our data validation. We keep our "Create" logic separate from our "Response" logic.from pydantic import BaseModel, ConfigDict

class TaskCreate(BaseModel):
    title: str
    description: str | None = None

class TaskResponse(TaskCreate):
    id: int
    is_completed: bool

    # Pydantic V2 Config to read from ORM models
    model_config = ConfigDict(from_attributes=True)
4. The API Endpoints (main.py)Here is where the magic happens. Notice two key things: The endpoints are asynchronous.await session.execute(select(...)): We use the new SQLAlchemy 2.0 selection style, not the old session.query().from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from contextlib import asynccontextmanager

import models, schemas
from database import engine, get_db

# Lifespan event to create tables on startup
@asynccontextmanager
async def lifespan(app: FastAPI):
    async with engine.begin() as conn:
        await conn.run_sync(models.Base.metadata.create_all)
    yield

app = FastAPI(lifespan=lifespan)

# CREATE
@app.post("/tasks/", response_model=schemas.TaskResponse)
async def create_task(task: schemas.TaskCreate, db: AsyncSession = Depends(get_db)):
    new_task = models.Task(**task.model_dump())
    db.add(new_task)
    await db.commit()
    await db.refresh(new_task)
    return new_task

# READ (Async Select)
@app.get("/tasks/", response_model=list[schemas.TaskResponse])
async def read_tasks(skip: int = 0, limit: int = 10, db: AsyncSession = Depends(get_db)):
    # The Modern 2.0 Syntax
    query = select(models.Task).offset(skip).limit(limit)
    result = await db.execute(query)
    return result.scalars().all()

# UPDATE
@app.patch("/tasks/{task_id}", response_model=schemas.TaskResponse)
async def update_task(task_id: int, completed: bool, db: AsyncSession = Depends(get_db)):
    query = select(models.Task).where(models.Task.id == task_id)
    result = await db.execute(query)
    task = result.scalar_one_or_none()

    if task is None:
        raise HTTPException(status_code=404, detail="Task not found")

    task.is_completed = completed
    await db.commit()
    await db.refresh(task)
    return task
Async SQLAlchemy Engine and Session Lifecycle in FastAPIIn production FastAPI applications, the async SQLAlchemy engine should be created once at application startup and reused across requests. Creating engines or sessions per request is a common mistake that leads to connection exhaustion and unpredictable performance.FastAPIâ€™s lifespan context is the recommended place to initialize the async engine and session factory, ensuring clean startup and shutdown behavior while avoiding hidden global state.Note- SQLAlchemy 2.0 removed legacy query patterns, which is why AsyncSession no longer exposes .query().â€In the synchronous world, if the database takes 200ms to fetch those tasks, your entire server thread is blocked for 200ms. It can do nothing else.In this Async version, while the database is fetching data (await db.execute), Python releases the control loop. Your API can accept 50 other requests during that 200ms "wait" time.This is how you scale to thousands of users on a single server.Next Step: Deploying thisNow that you have a high-performance backend, how do you deploy it? You can't just use python main.py in production. In the next article, I will show you how to containerize this with Docker and deploy it to Google Cloud Run.]]></content:encoded></item><item><title>Python Software Foundation: Your Python. Your Voice. Join the Python Developers Survey 2026!</title><link>https://pyfound.blogspot.com/2026/01/your-python-your-voice-join-python.html</link><author></author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 08:36:03 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[This year marks the ninth iteration of the official Python Developers Survey. We intentionally launched the survey in January (later than years prior) so that data collection and results can be completed and shared within the same calendar year. The survey aims to capture the current state of the Python language and its surrounding ecosystem. By comparing the results with last yearâ€™s, the community can identify emerging trends and gain deeper insight into how Python continues to evolve.We encourage you to contribute to our communityâ€™s knowledge by sharing your experience and perspective. Your participation is valued! The survey should only take you about 10-15 minutes to complete.Â Contribute to the Python Developers Survey 2026!This year we aim to reach even more of our community and ensure accurate global representation by highlighting our localization efforts:Â The survey is translated into Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish.Â To assist individuals in promoting the survey and encouraging their local communities and professional networks we have created a Promotion Kit with images and social media posts translated into a variety of languages. We hope this promotion kit empowers folks to spread the invitation to respond to the survey within their local communities.Weâ€™d love it if youâ€™d share one or more of the posts in the Promotion Kit to your social media or any community accounts you manage, as well as share the information in Python related discords, mailing lists, or chats you participate in.If you would like to help out with translations you see are missing, please request edit access to the doc and share what language you will be translating to. Translations for promotions into languages the survey may not be translated to is also welcome!Â If you have ideas about what else we can do to get the word out and encourage a diversity of responses, please comment on the corresponding Discuss thread.Â The survey is organized in partnership between the Python Software Foundation and JetBrains. After the survey is over, JetBrains will publish the aggregated results and randomly choose 20 winners (among those who complete the survey in its entirety), who will each receive a $100 Amazon Gift Card or a local equivalent.]]></content:encoded></item><item><title>How to build a captive portal in ESP32 with MicroPython</title><link>https://dev.to/devasservice/how-to-build-a-captive-portal-in-esp32-with-micropython-2dc1</link><author>Developer Service</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 08:34:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You unbox a small IoT device, plug it in, andâ€¦ nothing. No screen. No keyboard. No buttons that make any sense.This is a familiar situation if you've ever worked with microcontrollers like the ESP32. The hardware is powerful, but the very first interaction is often awkward. At some point, the device needs to connect to your Wi-Fi network, and the classic question appears:"How does a tiny device ask for Wi-Fi credentials?"Hardcoding credentials works for quick experiments, but it breaks down in the real world, every deployment would require re-flashing the firmware. Serial consoles are fine for developers, not for users who don't have USB cables or the patience for terminal commands. Mobile apps add friction and maintenance overhead, requiring separate iOS and Android versions that need constant updates.What you want instead is simple: power on the device, connect to it once from any phone or laptop, configure it through a familiar web interface, and never think about it again. No apps to install, no cables to find, no special tools required.This is exactly where  shine, they turn Wi-Fi provisioning into a one-time, friction-free experience that works on every platform.
  
  
  What Is a Captive Portal?
If you've ever connected to Wi-Fi at a coffee shop, hotel, or airport, you've used a captive portal. You join the network, and a web page automatically pops up asking you to log in or accept terms. You didn't type a URLâ€”the network guided you there by intercepting your browser traffic.A captive portal on an embedded device works the same way:The device creates its  (acting as an access point)The user connects from their phone or laptop (no password needed)The device intercepts all internet requests using a A web page  showing the configuration formAfter submission, the device saves credentials and switches to normal Wi-Fi client modeThe beauty of this approach is that the ESP32 can handle everything on-device. It acts as both an access point and a web server simultaneously, with no cloud services, external infrastructure, or internet connection required during setup. It's significantly simpler than building a companion mobile app (which requires maintaining iOS and Android versions) and far more user-friendly than hardcoded credentials or serial console configuration.For IoT products, this is often the difference between "technically works" and "actually ships to customers."The device goes through two distinct phases:  (captive portal) and  (Wi-Fi client). runs the first time you power on the device, or anytime no Wi-Fi configuration exists. Creates its own temporary Wi-Fi networkRuns a DNS server that redirects all domains to itselfRuns an HTTP server that serves the configuration pageSaves user-submitted credentials to flash storage begins after successful configuration. Disables the access pointConnects to the user's Wi-Fi network as a clientRuns your actual application (fetching data, controlling hardware, etc.)The key decision happens at boot: does  exist? If yes, connect to Wi-Fi. If no, start the captive portal.If you have already MicroPython installed on your ESP32, you are good to go and experiment with the following examples. If not, see this guide for more detail on how to setup an ESP32 with MicroPython.
  
  
  Step 1 - Create the Wi-Fi Access Point
The ESP32 creates its own Wi-Fi network using the built-in  module. This is the foundation of the entire captive portal, without the access point, users have no way to reach the configuration page.The ESP32 has two Wi-Fi interfaces: (Access Point Interface) - Makes the ESP32 act like a router (Station Interface) - Makes the ESP32 act like a Wi-Fi clientDuring setup mode, we use  to create a temporary network that users can join from their phones.In your local project, create a new file :Once this runs, the ESP32 broadcasts a Wi-Fi network named "MyDevice-Setup" and assigns itself IP address . Any device that connects to this network can communicate with the ESP32 using this IP.You'll notice we use  (no password). This might seem insecure, but it's actually the right choice for a temporary setup network:: Some devices have issues with WPA2 on captive portals: Users don't need to type a passwordAuto-detection works better: Operating systems are more likely to show the "Sign in to network" popup on open networks: The AP only exists during initial setup and disappears once configuredThe security risk is minimal because:The network only exists during the 1-2 minutes of setupThe user is typically standing right next to the deviceOnce configured, the AP shuts down completelyFor production devices, you can add strategies like auto-disabling the AP after 10 minutes, or requiring a physical button press to re-enter setup mode.
  
  
  Step 2 - Build the HTML Setup Page
Before we create the HTTP server, let's build the setup page that users will see. This is the critical user-facing component, it needs to work flawlessly on every mobile device.Design Principles for Captive Portal Pages:: Most users will access this from their phones: No external CSS/JS frameworks (we don't have internet!): Every byte travels over a slow access point connection: Users should immediately understand what to do: Don't assume users type perfectlyThe page needs only two inputs: SSID and password. Everything else can be configured later through a different interface (if needed at all).On your local project folder, create :Device SetupWi-Fi Setup
            Wi-Fi SSID
            
            Password
            Save  Connect meta tag ensures proper scaling on mobile devicesInline CSS keeps everything in one file (no extra HTTP requests) prevents input overflow issuesLarge touch targets (10px padding) work better on phonesSimple color scheme (#007bff) looks professional without complexity
  
  
  Step 3 - Create HTTP Server with Form Handling
Now create an HTTP server that serves the HTML file and handles form submissions. This is more complex than a typical web server because it needs to:Serve the same page for any URL - Operating systems make requests to various paths like , , etc. to detect captive portals - Parse POST data and save credentials - Show clear messages when things go wrong - The ESP32's networking stack is fragile, so we need careful error handlingThe server handles both GET requests (show the form) and POST requests (save configuration).On your local project folder, create :Key implementation details:: The  function handles URL-encoded data (the format HTML forms use). We manually decode common characters like  (space) and  (@) since MicroPython doesn't have a built-in URL decoder.: The server reads  from the filesystem. If the file is missing, it shows an error message instead of crashing, this helps with debugging during development.:  is critical. MicroPython's socket implementation can leak memory with persistent connections. We explicitly close every connection, even in error cases, to prevent the ESP32 from running out of sockets.: The triple-nested try/except blocks might look excessive, but they prevent the server from crashing when:A client disconnects mid-requestThe filesystem is corrupted: Notice we call  when sending responses. MicroPython's socket.send() expects bytes, not strings.
  
  
  Step 4 - Add DNS Redirection (The "Captive" Part)
To trigger the automatic "Sign in to network" popup on phones, we need a DNS server that redirects  to the ESP32's IP. This is the "captive" part of "captive portal", we're capturing all DNS requests.How DNS redirection works:When your phone connects to a new Wi-Fi network, it performs a "captive portal check" by trying to reach a known URL (like  on iOS or http://connectivitycheck.gstatic.com on Android). Phone asks DNS: "What's the IP address of captive.apple.com?"Our DNS server lies: "It's 192.168.4.1" (the ESP32's IP)Phone tries to load that pageOur HTTP server responds with the setup pagePhone realizes: "This isn't the real captive.apple.com, must be a captive portal!"Phone shows the "Sign in to network" popupThe key insight: we don't need to know which domain was requested. We just answer  with our own IP address.How the DNS packet parsing works:DNS packets have a specific structure. We extract: (): Unique identifier for each query, must be echoed back:  means "this is a response, no error": The domain name the client asked about: Our fabricated response pointing to 192.168.4.1We only respond to A-record queries (IPv4 address lookups). Other query types (AAAA for IPv6, MX for mail servers, etc.) are safely ignored.Platform compatibility notes:iOS is particularly strict about captive portal detection. The popup appears more reliably if:DNS replies are fast (our implementation is)HTTP responses are well-formed (we use proper HTML)The server is already running before the phone connectsIf the popup doesn't appear automatically, users can manually open any browser and type any URL (like ). The DNS redirection will still work, bringing them to your setup page.Important: Captive Portal Auto-Detection LimitationsThe automatic captive portal popup is  to appear. Whether it shows up depends on:: iOS is most reliable, Android varies by manufacturer (Samsung/Xiaomi less reliable), Windows 10/11 is hit-or-miss: The popup only appears if DNS/HTTP checks happen within a few seconds of connecting: If the phone previously connected to this SSID, it may skip checks: Some phones have "captive portal detection" disabled in developer options: Some mobile carriers modify captive portal behaviorIn real-world testing, automatic popups appear on approximately 70-80% of devices. The other 20-30% require users to manually open a browser.
  
  
  Step 5 - Connect to User's Wi-Fi
After saving configuration, the device needs to switch from AP mode to STA mode (Wi-Fi client). This is the transition from "setup" to "normal operation."The connection logic needs to be robust because Wi-Fi connections fail more often than you'd expect:Wrong password (most common)Router configured to block new devices5GHz vs 2.4GHz band issues (ESP32 only supports 2.4GHz)Our implementation tries for 20 seconds (20 attempts Ã— 1 second), which is long enough for most networks but not so long that users think the device is frozen.Connection flow breakdown:: Try to load . If it doesn't exist, return  immediately: Turn off the access point interface (can't be AP and STA simultaneously effectively): Activate the station (client) interface: Attempt to connect using saved credentials: Check connection status every second for up to 20 seconds: Return  if connected,  if timeoutWhy the 20-second timeout?Most successful connections happen in 2-5 secondsSome networks with complex auth take 10-15 secondsAnything longer probably indicates a real problem (wrong password, etc.)20 seconds is long enough to succeed but short enough that users don't think the device is frozenNow let's create a complete  that integrates all the components. This is the entry point that decides: "Should I start the captive portal, or connect to Wi-Fi?"Try to connect to saved Wi-FiIf successful â†’ run the main applicationIf failed â†’ start captive portal for (re)configurationThis creates a self-healing system: if the user changes their Wi-Fi password or moves the device to a new network, it automatically falls back to setup mode.: This function orchestrates setup mode:Launches the DNS server in a background thread (using )Starts the HTTP server in the foreground (which blocks indefinitely)The threading is important: both servers need to run simultaneously. The DNS server runs in the background while the HTTP server handles the main event loop.: This is the decision point:Calls  to attempt connectionOn success: runs your application code (we show a simple internet connectivity test)On failure: starts the captive portal: The  blocks catch:: Allows clean shutdown during developmentGeneral exceptions: Reboots the device to recover from crashesYour ESP32 should have these files in its root directory:/
â”œâ”€â”€ main.py           # Main entry point
â”œâ”€â”€ wifi_ap.py        # Access point setup
â”œâ”€â”€ http_server.py    # HTTP server with form handling
â”œâ”€â”€ dns_server.py     # DNS server for captive portal
â”œâ”€â”€ wifi_client.py    # Wi-Fi connection logic
â””â”€â”€ portal.html       # Setup page HTML
Breaking the code into separate modules has several advantages:: Test each component independently: Each file has one clear purpose: Copy  to other projects: Easier to find and fix bugsAll Python files should be in the root directory. MicroPython doesn't handle complex directory structures well, so keep it simple.The easiest way to upload files to your ESP32 is using , which has built-in MicroPython support.: Plug in your ESP32 via USBGo to  â†’  â†’ Select "MicroPython (ESP32)"Choose your COM port (e.g., COM3 on Windows, /dev/ttyUSB0 on Linux)In Thonny, go to  â†’ Open your local project folder in the top sectionRight-click on each file and select 'Upload to /' (this will upload the file to the device)Press the  button (or Ctrl+D in the Shell)Click the green run buttonHere's what the complete flow looks like in practice:The ESP32 boots and starts the captive portal. In the serial monitor (Thonny's Shell), you'll see:========================================
ESP32 Captive Portal Starting...
========================================

No config file found

=== Starting Setup Mode ===

=== Starting Captive Portal ===
Access point active: True
AP IP address: 192.168.4.1
DNS server listening on 192.168.4.1
Captive portal ready!
Connect to 'MyDevice-Setup' from your phone

HTTP server listening on port 80

  
  
  Step 2: Connect from Phone
On your phone's Wi-Fi settings, you'll see "MyDevice-Setup" appear as an open network. Tap to connect.: A popup appears within 3-5 seconds saying "Sign in to network": No popup appears - this is normal! Just open any browser and type any URL
  
  
  Step 3: Configuration Page
Whether you got the popup or opened a browser manually, you'll see the setup page with two fields:Wi-Fi SSID (your network name)Enter your actual Wi-Fi credentials and click "Save & Connect"
  
  
  Step 4: Success Confirmation
You'll see a page saying "Configuration saved" with instructions to reboot the device.In the Thonny shell, you can see the configuration save log:HTTP server listening on port 80
Configuration saved: {'ssid': 'SSID_EXAMPLE', 'password': 'password example'}
Reboot the ESP32 and after reboot, the serial monitor shows:========================================
ESP32 Captive Portal Starting...
========================================

Connecting to SSID_EXAMPLE...
Connected!
IP: 192.168.2.73

=== Connected to Wi-Fi ===
Starting main application...

Testing internet connection...
Success! Current time: 2026-01-23T10:03:12.498011+01:00

Ready for normal operation!
Captive portals solve a fundamental problem: getting a headless device onto a user's network without hardcoded credentials or mobile apps. For ESP32 projects, it's the difference between a prototype and a product someone else can actually use.The ESP32 is powerful enough to run an access point, DNS server, and web server simultaneously, making Wi-Fi provisioning a simple, app-free experience.]]></content:encoded></item><item><title>Python Bytes: #467 Toads in my AI</title><link>https://pythonbytes.fm/episodes/show/467/toads-in-my-ai</link><author></author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[<strong>Topics covered in this episode:</strong><br>

<ul>
	<li><strong><a href="https://check.labs.greynoise.io?featured_on=pythonbytes">GreyNoise IP Check</a></strong></li>
<li><strong><a href="https://pypi.org/project/tprof/?featured_on=pythonbytes">tprof: a targeting profiler</a></strong></li>
<li><strong><a href="https://github.com/batrachianai/toad?featured_on=pythonbytes">TOAD is out</a></strong></li>
<li><strong>Extras</strong></li>
<li><strong>Joke</strong></li>

</ul><a href='https://www.youtube.com/watch?v=24gBkjE8tOU' style='font-weight: bold;'data-umami-event="Livestream-Past" data-umami-event-episode="467">Watch on YouTube</a><br>

<p><strong>About the show</strong></p>

<p>Sponsored by us! Support our work through:</p>

<ul>
<li>Our <a href="https://training.talkpython.fm/?featured_on=pythonbytes"><strong>courses at Talk Python Training</strong></a></li>
<li><a href="https://courses.pythontest.com/p/the-complete-pytest-course?featured_on=pythonbytes"><strong>The Complete pytest Course</strong></a></li>
<li><a href="https://www.patreon.com/pythonbytes"><strong>Patreon Supporters</strong></a></li>
</ul>

<p><strong>Connect with the hosts</strong></p>

<ul>
<li>Michael: <a href="https://fosstodon.org/@mkennedy">@mkennedy@fosstodon.org</a> / <a href="https://bsky.app/profile/mkennedy.codes?featured_on=pythonbytes">@mkennedy.codes</a> (bsky)</li>
<li>Brian: <a href="https://fosstodon.org/@brianokken">@brianokken@fosstodon.org</a> / <a href="https://bsky.app/profile/brianokken.bsky.social?featured_on=pythonbytes">@brianokken.bsky.social</a></li>
<li>Show: <a href="https://fosstodon.org/@pythonbytes">@pythonbytes@fosstodon.org</a> / <a href="https://bsky.app/profile/pythonbytes.fm">@pythonbytes.fm</a> (bsky)</li>
</ul>

<p>Join us on YouTube at <a href="https://pythonbytes.fm/stream/live"><strong>pythonbytes.fm/live</strong></a> to be part of the audience. Usually <strong>Monday</strong> at 11am PT. Older video versions available there too.</p>

<p>Finally, if you want an artisanal, hand-crafted digest of every week of the show notes in email form? Add your name and email to <a href="https://pythonbytes.fm/friends-of-the-show">our friends of the show list</a>, we'll never share it.</p>

<p><strong>Michael #1:</strong> <a href="https://check.labs.greynoise.io?featured_on=pythonbytes">GreyNoise IP Check</a></p>

<ul>
<li>GreyNoise watches the internet's background radiationâ€”the constant storm of scanners, bots, and probes hitting every IP address on Earth.</li>
<li>Is your computer sending out bot or other bad-actor traffic? What about the myriad of devices and IoT things on your local IP?</li>
<li>Heads up: If your IP has recently changed, it might not be you (false positive).</li>
</ul>

<p>Brian #2: <a href="https://pypi.org/project/tprof/?featured_on=pythonbytes">tprof: a targeting profiler</a></p>

<ul>
<li>Adam Johnson</li>
<li>Intro blog post: <a href="https://adamj.eu/tech/2026/01/14/python-introducing-tprof/?featured_on=pythonbytes"><strong>Python: introducing tprof, a targeting profiler</strong></a></li>
</ul>

<p><strong>Michael #3: <a href="https://github.com/batrachianai/toad?featured_on=pythonbytes">TOAD is out</a></strong></p>

<ul>
<li>Toad is a unified experience for AI in the terminal</li>
<li>Front-end for AI tools such as <a href="https://openhands.dev/?featured_on=pythonbytes">OpenHands</a>, <a href="https://www.claude.com/product/claude-code?featured_on=pythonbytes">Claude Code</a>, <a href="https://geminicli.com/?featured_on=pythonbytes">Gemini CLI</a>, and many more.</li>
<li>Better TUI experience (e.g. @ for file context uses fuzzy search and dropdowns)</li>
<li>Better prompt input (mouse, keyboard, even colored code and markdown blocks)</li>
<li>Terminal within terminals (for TUI support)</li>
</ul>

<p><strong>Brian #4</strong>: <a href="https://github.com/fastapi/fastapi/pull/14706/files?featured_on=pythonbytes">FastAPI adds Contribution Guidelines around AI usage</a></p>

<ul>
<li>Docs commit: <a href="https://github.com/fastapi/fastapi/pull/14706/files?featured_on=pythonbytes"><strong>Add contribution instructions about LLM generated code and comments and automated tools for PRs</strong></a></li>
<li>Docs section: <a href="https://fastapi.tiangolo.com/contributing/?h=contributin#automated-code-and-ai">Development - Contributing : Automated Code and AI</a></li>
<li>Great inspiration and example of how to deal with this for popular open source projects
<ul>
<li>â€œIf the <strong>human effort</strong> put in a PR, e.g. writing LLM prompts, is <strong>less</strong> than the <strong>effort we would need to put</strong> to <strong>review it</strong>, please <strong>don't</strong> submit the PR.â€</li>
</ul></li>
<li>With sections on
<ul>
<li>Closing Automated and AI PRs</li>
<li>Human Effort Denial of Service</li>
<li>Use Tools Wisely</li>
</ul></li>
</ul>

<p><strong>Extras</strong></p>

<p>Brian:</p>

<ul>
<li><a href="https://techcrunch.com/2026/01/14/digg-launches-its-new-reddit-rival-to-the-public/?featured_on=pythonbytes">Apparently Digg is back</a> and thereâ€™s a <a href="https://digg.com/python?featured_on=pythonbytes">Python Community</a> there</li>
<li><a href="https://marijkeluttekes.dev/blog/articles/2026/01/21/why-light-weight-websites-may-one-day-save-your-life/?featured_on=pythonbytes">Why light-weight websites may one day save your life</a> - Marijke LuttekesHome</li>
</ul>

<p>Michael:</p>

<ul>
<li>Blog posts about Talk Python AI Integrations
<ul>
<li><a href="https://talkpython.fm/blog/posts/announcing-talk-python-ai-integrations/?featured_on=pythonbytes">Announcing Talk Python AI Integrations</a> <em><em></em></em>on Talk Pythonâ€™s Blog</li>
<li><a href="https://mkennedy.codes/posts/why-hiding-from-ai-crawlers-is-a-bad-idea/?featured_on=pythonbytes">Blocking AI crawlers might be a bad idea</a> on Michaelâ€™s Blog</li>
</ul></li>
<li>Already using the compile flag for faster app startup on the containers:
<ul>
<li><code>RUN --mount=type=cache,target=/root/.cache uv pip install --compile-bytecode --python /venv/bin/python</code></li>
<li>I think itâ€™s speeding startup by about 1s / container.</li>
</ul></li>
<li><a href="https://blobs.pythonbytes.fm/big-prompt-or-what-2026-01.png">Biggest prompt yet?</a> <strong>72 pages</strong>, 11, 000</li>
</ul>

<p><strong>Joke: <a href="https://www.reddit.com/r/ProgrammerHumor/comments/1q2tznx/forgotthebasecase/?featured_on=pythonbytes">A date</a></strong></p>

<ul>
<li>via From Pat Decker</li>
</ul>]]></content:encoded></item><item><title>#467 Toads in my AI</title><link>https://pythonbytes.fm/episodes/show/467/toads-in-my-ai</link><author></author><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://pythonbytes.fm/episodes/download/467/toads-in-my-ai.mp3" length="" type=""/><pubDate>Mon, 26 Jan 2026 08:00:00 +0000</pubDate><source url="https://pythonbytes.fm/">Python bytes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Secret Life of Python: The Uncontained Script</title><link>https://dev.to/aaron_rose_0787cc8b4775a0/the-secret-life-of-python-the-uncontained-script-4p8g</link><author>Aaron Rose</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 07:31:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Why your variables are colliding (and how to fix it).Timothy scrolled to the bottom of his data processing script. It was a long, flat column of codeâ€”no functions, just command after command."It works," he said, "but it feels... fragile."Margaret leaned in. "Why do you say that?""I'm reusing variable names," Timothy admitted. "I use  to count users at the top. Then I use  to count revenue at the bottom. I'm terrified that if I delete a line in the middle, the 'old'  will accidentally be used by the new calculation."He showed her the structure. It looked something like this:Timothy winced. "See? I forgot to recalculate , so the revenue calculation just grabbed the  from the user count. It didn't crash; it just gave me the wrong number.""This is the danger of the ," Margaret said. "In a flat script like this, every variable you create lives forever. , , â€”they are all swimming in the same soup.""Could I just delete the variables when I'm done?" Timothy asked. "Like, type  after line 5?"Margaret shook her head. "You could, but that is manual labor. If you forget one , the bug returns. We want a system that cleans itself up automatically. Think of functions like hotel rooms that are scrubbed clean the moment you check out.""Also," she added, "Local variables are faster for Python to access than Global ones. So we are going to clean up your code  speed it up."Margaret took the keyboard. "We are going to take your script and wrap it into functions. We will have one function to run the showâ€”the Orchestratorâ€”and smaller functions to do the work.""Now," Margaret explained, "look at the scope.""When  finishes," she continued, "its variablesâ€” and â€”are destroyed. When  starts, it starts with a clean slate. There is no 'old'  variable floating around to cause bugs."Timothy pointed to the last two lines.if __name__ == "__main__": main()"What is that?" he asked."That is the ," Margaret said. "It ensures that your Orchestrator () only runs when you specifically ask it to. It turns your script from a loose list of commands into a structured application."Timothy looked at his new code. "It's cleaner. I don't have to worry about what variable name I used fifty lines ago.""Exactly," Margaret smiled. "Don't let your variables wander the halls, Timothy. Keep them in their rooms."Margaret opened her notebook to the "Structure" section. Writing "flat" scripts (no functions).. Every variable lives forever, leading to naming collisions and "zombie data." The .Move logic into specific functions ().Create a  function to orchestrate the flow.Use the Gatekeeper: if __name__ == "__main__":. Variables are created, used, and destroyed locally. This is safer and faster.In the next episode, Margaret and Timothy will face "The Hidden Return." The Orchestrator is great, but what if  needs to hand that money back to ? Timothy is about to find out that if you don't ask for it, Python gives you... nothing.]]></content:encoded></item><item><title>Building Deepfake-Resistant Hiring Systems: Cryptographic Audit Trails as Defense Against Nation-State Identity Fraud</title><link>https://dev.to/veritaschain/building-deepfake-resistant-hiring-systems-cryptographic-audit-trails-as-defense-against-485g</link><author>VeritasChain Standards Organization (VSO)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 07:14:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[North Korean IT workers are using deepfakes to pass video interviews at Fortune 500 companiesA Palo Alto Networks researcher created a convincing fake identity in  with free toolsTraditional verification (background checks, liveness detection) fails at a structural levelSolution: Cryptographic audit trails with  that make skipped steps mathematically detectableThis post includes working Python code for implementing VAP-style hiring verification
  
  
  The Incident That Should Terrify Every Hiring Manager
KnowBe4â€”a company that literally trains people to spot scamsâ€”hired a North Korean intelligence operative. Four video interviews. Background check cleared. References verified.Twenty-five minutes after receiving his work laptop, their endpoint detection software caught him loading malware.The CEO's assessment: "If it can happen to us, it can happen to almost anyone."He's right. And the numbers are terrifying:
  
  
  Understanding the Attack Vector
Before we build defenses, let's understand the attack. This isn't script kiddie stuffâ€”it's nation-state tradecraft industrialized for scale.
  
  
  The Tech Stack of a Nation-State Hiring Fraud

  
  
  The 70-Minute Deepfake Experiment
Palo Alto Networks' Unit 42 ran an experiment that should keep you up at night:
  
  
  Why Your Current Defenses Are Structurally Broken
This isn't about doing security better. The  is wrong.
  
  
  Problem 1: Background Checks Verify the Wrong Identity
DPRK schemes use stolen legitimate identities. The background check works perfectlyâ€”it's just answering the wrong question.
  
  
  Problem 2: Liveness Detection Is Bypassed at the Hardware Level
Video injection attacks increased  in 2024. The defense assumes honest hardware; the attack subverts the hardware.
  
  
  Problem 3: "Verify Once, Trust Forever"
For DPRK schemesâ€”where a "single employee" is actually a rotating teamâ€”this model is catastrophically naive.
  
  
  The Solution: Cryptographic Verification with Completeness Guarantees
Here's where we rebuild from first principles.The Verifiable AI Provenance Framework (VAP) provides a different model:
  
  
  Implementation: Building a VAP-Style Hiring System
Let's build this. I'll walk through the key components with working code.
  
  
  Step 1: Define the Event Schema

  
  
  Step 2: Implement the Hash Chain

  
  
  Step 3: The Completeness Invariant (The Secret Sauce)
This is where VAP gets powerful. We define rules that must be satisfied for a hiring process to be valid:
  
  
  Step 4: External Anchoring
The hash chain proves internal consistency. External anchoring proves  the data existed:
  
  
  Step 5: Cross-Reference with Third Parties (VCP-XREF)

  
  
  Step 6: Evidence Pack Generation

  
  
  Putting It All Together: A Complete Example

  
  
  What This Achieves Against DPRK Attacks
Let's map this back to the attack vectors:Cross-reference with IDV provider + liveness creates multi-point verificationLiveness detection bypassedLiveness event must exist + deepfake score recorded + externally anchoredPolicy compliance is trust-basedCompleteness Invariant mathematically enforces required stepsExternal anchoring makes post-hoc changes detectableCompromise HR system = full accessCross-references require compromising multiple independent parties
  
  
  Production Considerations

  
  
  Option 1: Use the Reference Implementation

pip vap-hiring


vap-hiring init  hiring_config.yaml


vap-hiring verify  ./evidence_pack.zip
The code in this article is a starting point. For production:Add proper key management (HSM for signing keys)Implement RFC 3161 TSA integrationBuild API endpoints for cross-reference exchangeIntegrate with your ATS/HRIS
  
  
  Option 3: Explore the Specification

  
  
  Conclusion: The Trust Model Is Broken
North Korean IT worker infiltration isn't a security incident. It's a  that trust-based hiring verification is fundamentally broken.When a security awareness company can hire a spy, when a convincing fake identity takes 70 minutes to create, when nation-states operate industrial-scale hiring fraudâ€”we need to stop patching and start rebuilding.Cryptographic audit trails with completeness invariants offer a path forward: make tampering detectable make skipped steps impossible makes fabrication provable make single-point compromise insufficientThe technology exists. The threat is proven. The regulatory pressure is mounting.The question is whether you'll implement verification-based hiring proactivelyâ€”or after a DPRK operative has been on your payroll for six months.VeritasChain Standards Organization (VSO) is a non-profit, vendor-neutral standards body developing open specifications for cryptographic audit trails in AI and algorithmic systems.Found this useful? Follow for more on cryptographic verification, AI governance, and building trust infrastructure for the algorithmic age.KnowBe4, "How a North Korean Fake IT Worker Tried to Infiltrate Us" (2024)Palo Alto Networks Unit 42, "False Face: Synthetic Identity Creation" (2025)Okta Security, "How AI Services Power DPRK IT Contracting Scams" (2025)OFAC, "Sanctions on DPRK IT Workers" (2024)VeritasChain Standards Organization, "VCP Specification v1.1" (2025)]]></content:encoded></item><item><title>Reuven Lerner: Whatâ€™s new in Pandas 3?</title><link>https://lerner.co.il/2026/01/26/whats-new-in-pandas-3/</link><author></author><category>dev</category><category>python</category><pubDate>Mon, 26 Jan 2026 07:05:33 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Blogging Platform Built with Django | mrcaption49</title><link>https://dev.to/mrcaption49/blogging-platform-built-with-django-mrcaption49-550h</link><author>mrcaption49</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 06:50:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[A Clean and Scalable Blogging Platform Built with DjangoThis project is a Django-based blogging platform designed to demonstrate core content management principles using a clean, scalable, and maintainable backend architecture. The application enables users to browse blog posts in a well-structured layout, navigate seamlessly between articles, and access each post through a dedicated detail page for an uninterrupted reading experience.Built using Djangoâ€™s Modelâ€“Viewâ€“Template (MVT) architecture, the application ensures a clear separation of concerns between data models, business logic, and presentation layers. This architectural approach improves code readability, simplifies maintenance, and allows the application to scale efficiently as new features are introduced.Blog posts are rendered using optimized Django templates that emphasize readability, consistency, and user experience. The structured layout ensures content is easy to consume, while clean template design supports reusable components and faster rendering.Each blog post opens on its own standalone page, providing focused content delivery and creating a strong foundation for future enhancements such as tagging systems, commenting features, search functionality, and user authentication. These extensibility options make the platform adaptable to evolving requirements.The application is deployed on Render, highlighting real-world deployment practices, environment configuration, and production readiness. Overall, this project reflects a practical implementation of a blogging system using Django, combining backend logic, structured content flow, and modern deployment techniques.ðŸ”— Live Demo: https://djangoblog49.onrender.com/
10-Line Descriptive SummaryThis Django blogging platform showcases a practical implementation of content management using a clean and scalable backend. It is built using Djangoâ€™s Modelâ€“Viewâ€“Template architecture to ensure maintainability and clarity. The application presents blog posts in a structured and readable layout. Users can navigate content smoothly across the platform. Each post opens on a dedicated detail page for focused reading. Django templates are optimized for consistency and user experience. Backend logic is designed to be clean and efficient. The platform supports easy extensibility for future features. Deployment on Render demonstrates production-level readiness. Overall, the project reflects real-world Django development practices.]]></content:encoded></item><item><title>A Simple Photo Album Built with Django | mrcaption4</title><link>https://dev.to/mrcaption49/a-simple-photo-album-built-with-django-mrcaption4-1go8</link><author>mrcaption49</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 06:39:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[A Simple Photo Album Built with DjangoPhoto Album application using Django and PythonThis project is a Django-based Image Gallery Application designed to showcase real-world media handling and scalable web architecture. It enables users to upload images, add descriptions, and organize content using dynamic category management. The application supports category-based filtering, allowing users to view all images or narrow results efficiently. Each image is rendered on a dedicated detail page for focused viewing. Built using Djangoâ€™s Modelâ€“Viewâ€“Template (MVT) architecture, the project ensures clean code separation and maintainability. The user interface is developed with Bootstrap 5, delivering a responsive and consistent UI with minimal custom CSS. Django forms handle secure file uploads using multipart/form-data and CSRF protection. Images are managed using Djangoâ€™s ImageField with Pillow for processing. For production readiness, the project integrates AWS S3 via django-storages and boto3 to provide scalable cloud-based media storage. Proper static and media file configuration supports both development and deployment environments. The system efficiently manages database relationships using foreign keys. Overall, this project demonstrates backend development expertise, cloud integration, and production-ready Django practices.I built a category-based Photo Album application using Django and Python to explore real-world media handling and clean backend architecture. The application allows users to browse photos by category, view them in a gallery layout, and open each image on a dedicated detail page.The project follows Djangoâ€™s Modelâ€“Viewâ€“Template (MVT) pattern, ensuring clear separation of concerns and maintainable code. Categories improve navigation, while thumbnail previews keep the interface fast and visually clean.Each photo opens on its own page, creating a focused viewing experience and laying the foundation for future features like likes, views, or comments. The application is deployed on Render, demonstrating production-level configuration and public hosting.This project reflects practical Django developmentâ€”combining structured backend logic, media handling, and real-world deployment into a clean, scalable web application.
ðŸ”— Live Demo: https://django-photoalbum-2025.onrender.com/
10-Line Insightful SummaryThis Django Photo Album application demonstrates real-world media handling using a clean and structured backend. Photos are organized into categories for better navigation and filtering. The gallery layout ensures fast and intuitive browsing. Each image opens on a standalone detail page for focused viewing. The project follows Djangoâ€™s MVT architecture for maintainability. Thumbnail previews enhance performance and UI clarity. Deployment on Render showcases production readiness. The application is simple, scalable, and extendable. It reflects strong Django fundamentals. A solid portfolio project with real-world relevance.]]></content:encoded></item><item><title>How to Give Your AI Agent Real-Time Internet Access for Free (Python Tutorial)</title><link>https://dev.to/_a4db025533c31e24cb517/how-to-give-your-ai-agent-real-time-internet-access-for-free-python-tutorial-1nfj</link><author>Yuriy Novak</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 05:48:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you are building an AI Agent (using OpenAI, LangChain, or AutoGen), you likely face the biggest pain point: To fix this, we need to give the LLM access to Google or Bing.Typically, developers turn to  or Google Custom Search JSON API. They are great, but they have a massive problem: SerpApi costs about .If your Agent runs a loop and searches 100 times to debug a task, you just spent . It adds up fast.I recently found a new alternative on RapidAPI called . It provides both  and  (like Firecrawl) but at a fraction of the cost ().Here is how to integrate it into your Python project in under 5 minutes.
  
  
  Step 1: Get the Free API Key
First, go to the RapidAPI page and subscribe to the  plan to get your key. It gives you 50 free requests to test (Hard Limit, so no surprise bills).You don't need to install any heavy SDKs. Just use .Here is a clean  class I wrote that handles both searching Google/Bing and scraping web pages into clean text for your LLM.For my side projects, I couldn't justify the monthly subscription of the big players.If you are building an MVP or a personal AI assistant, this saves a ton of money.]]></content:encoded></item><item><title>I Built a Support Ticket Classifier with a Fine-Tuned LLM for $10/month</title><link>https://dev.to/__1bea7786c7/i-built-a-support-ticket-classifier-with-a-fine-tuned-llm-for-10month-323l</link><author>Artyom Molchanov</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 05:44:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I fine-tuned Qwen2.5-0.5B to classify telecom support tickets, quantized it to 350MB, and deployed it on a cheap VPS. Here's how.Support teams waste hours manually routing tickets. A customer writes "my wifi is slow" â€” is it a technical issue? Billing? Should it go to L1 or L2 support?I built a classifier that outputs structured JSON with intent, category, urgency, sentiment, routing target, and extracted entities.
  
  
  Why Not Just Use a Cloud API?
 â€” 50K requests/month via cloud LLMs (OpenAI, Claude, Gemini) â‰ˆ $100-200. Self-hosted = $10-20 â€” Some companies can't send customer data to external APIs â€” Fine-tune for your specific domainQwen2.5-0.5B (fine-tuned) â†’ GGUF Q4_K_M (350MB)llama-cpp-python for inference â†’ FastAPI for API â†’ nginx for reverse proxy â€” small enough for CPU inference, smart enough for classification.~1000 synthetic support tickets with labels:Technical issues (internet, TV, mobile)Full fine-tuning on Google Colab T4 (free tier):Converted to GGUF and quantized to 4-bit using llama.cpp tools.Result:  model that runs on CPU.Simple FastAPI wrapper: load the GGUF model, accept POST requests, construct chat messages with system prompt and user text, parse JSON from model output, log to database.Users will send random stuff. Added a heuristic check:Text too short (< 10 chars) â†’ not relevantContains telecom keywords (wifi, internet, bill, etc.) â†’ relevantNo keywords + category=unknown â†’ not relevantNow irrelevant queries return .Deploy with docker composeTotal cost:  for a 2 vCore, 4GB RAM VPS.This isn't a chatbot. It's ticket classification that happens once when a ticket is created. You can also process async via a queue.For faster inference: use a modern CPU (AMD EPYC) or add a GPU.
  
  
  When to Fine-Tune vs Use GPT API
Data privacy is required (on-premise)High volume of similar requests (>10K/month)Specific domain knowledge neededNeed best quality regardless of costWant something similar for your company? I build custom LLM solutions that run on your infrastructure. Reach out on Telegram â€” let's discuss your use case.]]></content:encoded></item><item><title>Benchmarking on a Budget: Running massive evals for 50% less with the Gemini Batch API âš¡ï¸</title><link>https://dev.to/googleai/benchmarking-on-a-budget-running-massive-evals-for-50-less-with-the-gemini-batch-api-5d1j</link><author>Paige Bailey</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 05:09:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Running evaluations on LLMs can be a bit of a headache. You hit rate limits, you stare at loading bars, and -- probably worst of all -- you burn through your API budget faster than a GPU on a training run. But what if I told you thereâ€™s a way to run thousands of prompts asynchronously, at , without blocking your main thread?Enter the .Grab your coffee (or tea ðŸµ), and letâ€™s dive in!Here's what we'll be building with today: To get our evals. To make the data look pretty. To (carefully!) run the generated code.
  
  
  Step 1: Preparing the Data
First things first, we need our prompts. We are using the  dataset, which contains 164 coding problems.The Batch API loves  (JSON Lines) files. Each line is a separate request. We need to iterate through the dataset and format it so Gemini understands that we want it to write Python code. Notice ? Thatâ€™s your best friend. Since batch jobs are asynchronous, results might not come back in the same order you sent them. The ID helps you map the answer back to the question.
  
  
  Step 2: Upload the data to Google Cloud
Now that we have our , we need to upload it to Google's Cloud Platform and tell Gemini to get to work. We are using  here because it is fast, efficient, and perfect for high-volume tasks like this - but you could use any supported Gemini model docstring instead.Batch jobs aren't instant (that's the trade-off for the discount), but for an evaluation pipeline, it's perfect. Go stretch, grab a snack, or check Twitter. On average, my jobs for the HumanEval dataset have been taking ~10 minutes, and the majority complete in less than a few hours.
  
  
  Step 3: Downloading the results
Once the job hits , the results are ready to come home.This is where the magic happens. We have the code Gemini wrote; now we need to see if it actually . Weâ€™re going to loop through our results, extract the Python code (removing those pesky markdown backticks), and run it against the unit tests provided in the HumanEval dataset. We are using  here. In a production app, running untrusted code is a huge security no-no. But for a local sandbox evaluation, we live on the edge! Weâ€™ll wrap it in a  timeout so infinite loops donâ€™t freeze our machine.So, how did  do? Let's visualize it using Seaborn. 142 tasks (86.59%)That is incredibly impressive for a "Flash-Lite" model. It handled complex algorithmic logic, string manipulation, and math problems, passing the vast majority of them.The Gemini Batch API is a game-changer for workflows like this. We saved 50% on tokens. We didn't have to manage async loops or retry logic. The Gemini 2.5 Flash-Lite model punched way above its weight class.If you have large datasets, extensive prompting jobs, or nightly evaluations, definitely give the Batch API a spin. And if you'd like to see the full code, check out this Colab notebook.]]></content:encoded></item><item><title>I got 29x speedup rewriting Python&apos;s validators library in Rust</title><link>https://dev.to/vivek_kalyanarangan_f66cc/i-got-29x-speedup-rewriting-pythons-validators-library-in-rust-2nhk</link><author>Vivek Kalyanarangan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 04:39:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I've been using Python's  library for a while â€” it's great for quick checks without defining schemas. But it became a bottleneck when validating millions of URLs and emails in data pipelines.Average:  across 48 validators.That's it. Same API, same behavior, just faster.
  
  
  What I learned building this
PyO3 + maturin is mature. Publishing wheels for Python 3.8-3.13 across Linux/macOS/Windows was straightforward. GitHub Actions + maturin handled cross-compilation without drama.Hand-rolled parsing beats regex for simple patterns. For IPv4 validation, a simple split-and-parse approach was significantly faster than regex. Rust's regex crate is fast, but nothing beats avoiding it entirely. URL validation alone has dozens of edge cases â€” international domain names, punycode, IPv6 hosts, weird port numbers. I wrote 370 tests to ensure parity with the original library.The hardest validators were international ones. Spanish NIE, Indian Aadhaar, Finnish SSN â€” each has its own checksum algorithm. Lots of Wikipedia rabbit holes. email, url, domain, ipv4, ipv6, mac_address, hostname iban, card_number, visa, mastercard, amex, cusip, isin btc_address, eth_address, bsc_address, trx_address
 md5, sha1, sha256, sha512 base16, base32, base58, base64 Spanish (CIF, NIE, NIF), Indian (Aadhaar, PAN), Finnish, French, RussianIf you're processing lots of data and validators is in your dependency tree, give it a try. Happy to hear feedback or add validators people need.]]></content:encoded></item><item><title>ðŸ’¡ Discovery: docs(ralph): Auto-publish discovery blog post</title><link>https://dev.to/igorganapolsky/discovery-docsralph-auto-publish-discovery-blog-post-3mjl</link><author>Igor Ganapolsky</author><category>dev</category><category>python</category><category>devto</category><pubDate>Mon, 26 Jan 2026 04:07:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Want to add autonomous AI coding to your project?
pip anthropic


python scripts/ralph_loop.py  fix_tests  5  2.00
]]></content:encoded></item></channel></rss>