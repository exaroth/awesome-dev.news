<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Python</title><link>https://www.awesome-dev.news</link><description></description><item><title>üöÄ Boost Your Resume Instantly ‚Äì For FREE!</title><link>https://dev.to/buildandcodewithraman/boost-your-resume-instantly-for-free-a83</link><author>Ramandeep Singh</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 06:48:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Tired of bland resumes? I built a  that transforms your resume with ! ‚ú®üöÄ
Now no need to pay any talent sites promising jobs in return of enhanced resumes.‚úÖ Upload your resume üìÇ bullet points üî•
‚úÖ Download the improved version as a  üìÑ  Built with , this tool ensures your resume stands out! No signups, no hassle ‚Äì just instant upgrades.  Give your resume the AI touch! üöÄüíº Let me know what you think! üëá]]></content:encoded></item><item><title>RandomSolarize in PyTorch</title><link>https://dev.to/hyperkai/randomsolarize-in-pytorch-5a4o</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 06:28:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomSolarize() can randomly solarize an image with a given probability as shown below:The 1st argument for initialization is (Required-Type: or ). *All pixels equal or above this value are inverted.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is solarized or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>BrushinBella: Crafting a Digital Experience to Make Parents‚Äô Lives Easier</title><link>https://dev.to/maronzalez/brushinbella-crafting-a-digital-experience-to-make-parents-lives-easier-ab8</link><author>Griffin Cole</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 05:40:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today‚Äôs fast‚Äëpaced digital landscape, a brand‚Äôs online presence is not just a storefront‚Äîit‚Äôs an experience that speaks to its audience. BrushinBella, a company dedicated to providing innovative and thoughtfully designed baby and feeding products, recognized that their website needed to embody the values of care, creativity, and reliability. The goal was to create an engaging, easy‚Äëto‚Äënavigate e‚Äëcommerce platform that not only showcased their products but also helped parents seamlessly integrate these solutions into their busy lives.This article recounts the comprehensive journey behind the creation of the Brush in Bella website, from the initial planning and design phases to the intricate development work that involved custom integrations using C++, Java, and Python. We will explore the challenges encountered along the way, the solutions that were implemented, and share expert insights on current trends and best practices in web development.The Vision and Planning Phase
1.1. Defining the Brand and Project Scope
Before any code was written or designs drafted, the BrushinBella team‚Äîcomprising marketing experts, product designers, UX specialists, and software engineers‚Äîcame together to define the project‚Äôs vision. The core objective was clear: to create a digital experience that makes parents‚Äô everyday life easier by offering a curated selection of products, valuable parenting content, and a user‚Äëfriendly interface.Key objectives defined during the planning phase included:Brand Consistency: The website needed to mirror the brand‚Äôs values of quality, trust, and innovation.
User Experience: Given the target audience of busy parents, simplicity and intuitive navigation were paramount.
Responsive Design: With an increasing number of users accessing websites on mobile devices, the site had to offer a seamless experience across desktops, tablets, and smartphones.
Custom Integrations: Although the e‚Äëcommerce platform was built on a robust platform (Shopify was chosen for its reliability and scalability), the team identified specific areas‚Äîsuch as performance‚Äëcritical image processing, advanced inventory analytics, and bespoke order processing‚Äîwhere custom functionality was needed. This is where languages like C++, Java, and Python played a crucial role.
Scalability and Performance: The system needed to handle fluctuating traffic levels and ensure quick load times, a challenge that required both architectural planning and performance‚Äëoriented coding practices.
Security and Compliance: With sensitive customer data at stake, ensuring top‚Äënotch security from the very start was non‚Äënegotiable.
1.2. Assembling the Cross‚ÄëFunctional Team
To tackle this multifaceted project, BrushinBella assembled a cross‚Äëfunctional team. Each team member brought unique expertise:Project Managers and Business Analysts: To capture requirements, define deliverables, and ensure alignment with business goals.
UX/UI Designers: Charged with creating an aesthetically pleasing and intuitive design that would resonate with the target audience.
Front‚ÄëEnd Developers: Specialists in HTML, CSS, and JavaScript who would turn design mockups into interactive, responsive web pages.
Back‚ÄëEnd Developers: Experts who would build custom modules, integrate third‚Äëparty APIs, and ensure that the site‚Äôs server‚Äëside logic was robust and secure.
DevOps and QA Engineers: Responsible for establishing CI/CD pipelines, rigorous testing protocols, and ensuring smooth deployment and scaling.
1.3. Technology Selection and Architecture Decisions
During planning, the team conducted a thorough analysis of available technologies. Although the core e‚Äëcommerce solution was deployed on Shopify for its proven reliability and ease of use, certain functionalities required custom development. This resulted in a hybrid architecture:Shopify as the Primary Platform: Managing the storefront, shopping cart, product catalog, and checkout process.
Custom Back‚ÄëEnd Services: Developed in Java and Python, these services handled complex business logic, integration with third‚Äëparty systems (such as ERP and CRM platforms), and data analytics.
Performance‚ÄëCritical Modules in C++: For tasks such as real‚Äëtime image processing (for product images and dynamic visual elements) and computationally intensive operations, C++ was chosen to ensure maximum speed and efficiency.
APIs and Microservices: A RESTful API layer was established to enable seamless communication between Shopify and the custom modules. This approach allowed the system to scale horizontally and adopt a microservices architecture, which is increasingly considered best practice in modern web development.The Design Phase: From Concept to Wireframe
2.1. User-Centric Design Philosophy
A key part of BrushinBella‚Äôs vision was to make the website accessible not only to tech‚Äësavvy users but also to non‚Äëexperts‚Äîbusy parents who need a simple, straightforward interface. The UX/UI design phase was driven by several guiding principles:Simplicity: The design was stripped of any unnecessary complexity. Clear call‚Äëto‚Äëaction buttons, uncluttered layouts, and intuitive navigation were prioritized.
Visual Appeal: The website needed to evoke warmth and trust. Soft color palettes, playful yet professional typography, and high‚Äëquality images of products and happy families helped achieve this.
Responsiveness: Mobile-first design principles were followed. Prototypes were tested on multiple devices and screen sizes to ensure consistency.
Accessibility: The design adhered to accessibility guidelines, ensuring that the website was usable by people with disabilities. This included proper contrast ratios, keyboard‚Äënavigable menus, and alternative text for images.
2.2. Wireframing and Prototyping
Using industry‚Äëstandard tools like Sketch and Figma, the design team created detailed wireframes and prototypes. These early models allowed stakeholders to visualize the website‚Äôs structure, layout, and user flow. Key features that were highlighted included:Homepage: Featuring a dynamic banner that communicated the brand‚Äôs message (‚ÄúMaking Parents‚Äô Everyday Life Easier!‚Äù) and a streamlined product navigation menu.
Product Pages: Each product page was designed to provide detailed images, descriptions, customer reviews, and easy‚Äëto‚Äëfind purchasing options.
Blog and Content Sections: Recognizing that educational content is a valuable asset, the design included a blog section with articles, parenting tips, and video content.
Checkout Flow: A simplified, secure checkout process was paramount. Wireframes detailed the progression from shopping cart to payment gateway, with minimal friction.
2.3. Design Iterations and Stakeholder Feedback
The iterative nature of design meant that prototypes were continuously refined based on stakeholder and user feedback. Early user testing sessions were conducted with focus groups of parents, ensuring that the designs were meeting real needs. Feedback led to several important adjustments:Simplified Navigation: Initial designs with complex menus were streamlined to a single‚Äëlevel navigation bar.
Enhanced Product Imagery: High‚Äëresolution images and a ‚Äúzoom‚Äù feature were incorporated to allow parents to inspect product details closely.
Clearer Calls-to‚ÄëAction: Buttons were redesigned for better visibility and prominence.
Content Accessibility: The blog section was reorganized to make content categories and search features more intuitive.The Development Phase: Building the Backbone
3.1. Integrating Shopify with Custom Services
Once the design was finalized, the development phase kicked off. The first step was to integrate the robust capabilities of Shopify with the custom-built services developed in-house. Shopify managed the storefront and basic e‚Äëcommerce functionalities. However, several requirements demanded bespoke solutions:Custom Order Processing: While Shopify provided standard order management, BrushinBella needed an advanced system to integrate real‚Äëtime inventory data, promotional logic, and customized gift‚Äëwrapping options. For this, the team built a microservice using Java.
Data Analytics and Reporting: In order to understand customer behavior and optimize the sales process, Python‚Äëbased analytics tools were developed. These tools processed data from Shopify‚Äôs API and generated actionable insights.
Image Processing and Dynamic Visuals: To ensure that product images were optimized for speed and quality, the team implemented a performance‚Äëcritical module in C++. This module handled tasks such as real‚Äëtime image resizing, format conversion, and dynamic compression.
3.2. The Role of C++: High‚ÄëPerformance Modules
Although C++ is not traditionally associated with web development, its use in BrushinBella‚Äôs project was pivotal for performance‚Äëcritical tasks. Key functions implemented in C++ included:Image Optimization Engine: C++ was used to build an engine that automatically resized and optimized images for various devices and screen resolutions. The engine was integrated as a microservice that communicated with Shopify through RESTful APIs.
Real‚ÄëTime Data Processing: Certain operations, such as processing and rendering high‚Äëresolution graphics for product galleries, were computationally intensive. C++‚Äôs efficiency ensured that these tasks did not slow down the user experience.
Custom Plugins: Some interactive features, such as a dynamic ‚Äúgift‚Äëwrapping‚Äù calculator that adjusted options based on user input, were built in C++ to ensure rapid response times and minimize latency.
Using C++ required careful management of memory and thread safety. The development team leveraged modern C++ standards (C++17/20) and robust libraries such as Boost and OpenCV for image processing. This combination not only achieved the necessary performance gains but also ensured that the codebase was maintainable and scalable.3.3. Java: The Enterprise Workhorse
Java was chosen for its reliability, scalability, and strong ecosystem‚Äîqualities that made it ideal for handling core business logic and integrations. Within the BrushinBella project, Java served several key roles:Business Logic and Order Management: The custom order processing system was developed in Java. This system interfaced with Shopify‚Äôs API to synchronize order data and applied complex business rules for promotions, discounts, and gift‚Äëwrapping options.
API Gateway: Java was also used to build a RESTful API gateway that served as the communication hub between Shopify and the custom microservices. The gateway ensured secure and efficient data exchange, handling tasks like authentication, rate‚Äëlimiting, and error logging.
Integration with Legacy Systems: Many enterprise systems‚Äîsuch as ERP and CRM platforms‚Äîare built on or integrate well with Java. By choosing Java for these integrations, BrushinBella ensured that their website could interface smoothly with back‚Äëoffice systems, enabling real‚Äëtime inventory management and customer data synchronization.
Robust Error Handling and Monitoring: Java‚Äôs mature ecosystem provided access to powerful tools for logging (using frameworks such as Log4j) and performance monitoring. This allowed the team to identify and resolve issues quickly, ensuring minimal downtime.
The Java development team adopted best practices such as writing modular, test‚Äëdriven code and using containerization (with Docker) for deployment. This not only improved the reliability of the application but also made scaling the service more straightforward.3.4. Python: Rapid Prototyping and Data Analytics
Python‚Äôs reputation for ease of use and rapid development made it the language of choice for a range of supporting services:Data Analytics: Python was used extensively for developing analytical tools that processed customer behavior data, order history, and product performance metrics. Libraries such as Pandas, NumPy, and Matplotlib were leveraged to generate detailed reports that informed marketing strategies and inventory decisions.
Automation Scripts: Routine tasks such as data backups, report generation, and system health checks were automated using Python scripts. This helped reduce manual intervention and allowed the team to focus on higher‚Äëvalue activities.
Integration and Testing: Python‚Äôs flexibility also made it an ideal candidate for writing integration tests. Automated testing frameworks like pytest ensured that the interactions between Shopify, the Java API gateway, and the C++ image optimization engine were reliable and robust.
Microservices Development: In some cases, Python microservices were deployed to handle tasks that required rapid iteration and experimentation. For example, the team developed a prototype recommendation engine that used machine learning algorithms (with scikit‚Äëlearn) to suggest complementary products to customers based on their browsing history.
By combining Python‚Äôs rapid prototyping capabilities with the stability of Java and the performance of C++, BrushinBella was able to build a hybrid system that leveraged the strengths of each language.3.5. Adopting a Microservices Architecture
One of the most critical decisions during development was to adopt a microservices architecture. Instead of building a monolithic application, the system was divided into discrete services that communicated via RESTful APIs. This offered several benefits:Scalability: Each service could be scaled independently based on demand. For example, the image processing engine in C++ could be scaled up during high‚Äëtraffic periods without affecting the rest of the system.
Resilience: Failures in one microservice did not bring down the entire website. Robust error‚Äëhandling and fallback mechanisms ensured that the website remained operational even if one component experienced issues.
Flexibility: The architecture allowed the team to update or replace individual services without redeploying the entire application. This was particularly beneficial when iterating on features such as the recommendation engine or order processing logic.
Technology Diversity: By decoupling services, the team could choose the most appropriate language or framework for each task without forcing a one‚Äësize‚Äëfits‚Äëall solution. This technological diversity, while challenging to manage, ultimately resulted in a more robust and efficient system.Testing, Deployment, and Optimization
4.1. Rigorous Testing Strategies
Quality assurance was embedded in every stage of development. The BrushinBella team implemented a multi‚Äëlayered testing strategy to ensure that each component‚Äîfrom the front‚Äëend user interface to the backend microservices‚Äîfunctioned as expected:Unit Testing: Each module, whether written in Java, Python, or C++, underwent rigorous unit testing. For Java, JUnit was employed; Python modules were tested with pytest; and C++ components were validated using Google Test.
Integration Testing: Automated integration tests were established to verify the seamless communication between Shopify, the Java API gateway, the Python analytics services, and the C++ performance modules.
End‚Äëto‚ÄëEnd Testing: Tools such as Selenium and Cypress were used to simulate real‚Äëuser interactions, ensuring that the entire system worked together harmoniously.
Performance Testing: Given the emphasis on speed and scalability, performance testing was a critical focus. Load testing simulated high‚Äëtraffic scenarios to validate that the system could handle peak loads without significant degradation in response times.
Security Audits: Comprehensive security testing was conducted to safeguard against common vulnerabilities such as SQL injection, cross‚Äësite scripting (XSS), and cross‚Äësite request forgery (CSRF). Regular code audits and penetration testing further ensured that customer data remained protected.
4.2. Continuous Integration and Deployment (CI/CD)
To streamline the development process and ensure rapid delivery of updates, the team set up a robust CI/CD pipeline. Key components of the pipeline included:Automated Builds: Every code commit triggered an automated build process that compiled the Java and C++ modules and ran the Python scripts. This ensured that errors were caught early.
Automated Testing: The CI system executed the full suite of tests for each build, including unit, integration, and end‚Äëto‚Äëend tests. Only builds that passed all tests were promoted to staging.
Containerization: Using Docker, each microservice was containerized. This ensured consistency across development, testing, and production environments and simplified the scaling process.
Orchestration: Kubernetes was employed to manage the containerized services, enabling automatic scaling, load balancing, and self‚Äëhealing capabilities.
Monitoring and Logging: Post‚Äëdeployment, comprehensive monitoring tools (such as Prometheus and Grafana) were used to track system performance, while logging frameworks (e.g., Logstash and ELK stack) helped in quickly diagnosing and resolving issues.
4.3. Performance Optimization
Optimization was an ongoing effort throughout development. Key performance enhancements included:Caching Strategies: To reduce load times and database queries, caching mechanisms were implemented. This included front‚Äëend caching using service workers for static assets and back‚Äëend caching with Redis for frequently accessed data.
Efficient API Design: The RESTful APIs were optimized for speed, ensuring that data was transmitted in lightweight JSON formats with minimal overhead.
Code Profiling and Optimization: Regular profiling sessions identified bottlenecks in the code. For instance, the C++ image processing engine was fine‚Äëtuned to minimize latency, and the Java microservices were optimized to handle high volumes of concurrent requests.
Responsive and Adaptive Design: The front‚Äëend was optimized for mobile devices by adopting a mobile‚Äëfirst design philosophy, ensuring fast load times even on slower networks.
Database Optimization: Indexing, query optimization, and efficient data structuring in both relational and NoSQL databases helped maintain rapid data retrieval and update speeds.Overcoming Challenges and Implementing Solutions
No major web development project is without its hurdles. The creation of the BrushinBella website presented a variety of challenges, each of which was met with innovative solutions.5.1. Integration Complexity
Challenge:
Integrating a Shopify‚Äëbased e‚Äëcommerce platform with custom microservices written in Java, Python, and C++ proved complex. Each system had its own data formats, security protocols, and performance characteristics.Solution:
The team implemented a robust API gateway in Java to serve as the central communication hub. This gateway standardized data formats (using JSON), handled authentication and authorization, and ensured that all services communicated seamlessly. Extensive use of RESTful API design principles and well‚Äëdocumented endpoints minimized integration friction.5.2. Performance Bottlenecks
Challenge:
Certain operations‚Äîespecially image processing and real‚Äëtime analytics‚Äîposed performance challenges, risking slow response times that could frustrate users.Solution:
Critical performance‚Äëintensive tasks were offloaded to specialized microservices developed in C++. These modules were optimized using modern C++ standards, multithreading, and libraries like OpenCV to ensure that image processing was done swiftly. Additionally, Python‚Äôs role in handling analytics was enhanced by leveraging efficient data processing libraries and asynchronous programming techniques to reduce latency.5.3. Maintaining a Consistent User Experience
Challenge:
Ensuring a seamless, high‚Äëquality user experience across multiple devices and browsers is always challenging‚Äîespecially when integrating third‚Äëparty services and custom code.Solution:
The design team adopted a mobile‚Äëfirst approach and followed responsive design principles to ensure that the website adapted gracefully to different screen sizes. Rigorous cross‚Äëbrowser testing, combined with adaptive UI frameworks, ensured consistency. Accessibility guidelines were also adhered to, making the site usable by a diverse audience, including those with disabilities.5.4. Ensuring Security and Data Integrity
Challenge:
With customer data and payment information at stake, any security lapse could have dire consequences. The heterogeneous nature of the system, with multiple programming languages and platforms interacting, introduced several potential vulnerabilities.Solution:
Security was integrated from day one. The team implemented robust authentication mechanisms at the API gateway level and encrypted all sensitive data. Regular security audits, automated vulnerability scans, and adherence to best practices‚Äîsuch as input validation, sanitization, and the use of secure coding frameworks‚Äîhelped safeguard the system. Additionally, deploying a web application firewall (WAF) and implementing SSL/TLS across all endpoints further ensured data integrity.5.5. Managing a Diverse Technology Stack
Challenge:
Combining multiple programming languages and frameworks increases the complexity of the codebase and the development process. Ensuring that team members could collaborate effectively across different languages was a significant challenge.Solution:
The project adopted a microservices architecture, which naturally decoupled the different language‚Äëspecific modules. Clear documentation, code conventions, and regular cross‚Äëteam meetings ensured that everyone was on the same page. The use of containerization (with Docker) and orchestration (with Kubernetes) allowed developers to work on isolated services without interference. In addition, investing in integrated development environments (IDEs) and code review tools helped maintain code quality and consistency across the diverse stack.Expert Insights on Web Development Trends and Best Practices
As BrushinBella‚Äôs website evolved from concept to a fully‚Äëfunctional, high‚Äëperformance digital platform, the team kept a close eye on emerging trends and industry best practices. Here are some expert insights gleaned during the project:6.1. Embracing Microservices and Containerization
Modern web development is increasingly moving away from monolithic architectures toward microservices. This approach provides several advantages:Scalability: Each service can be scaled independently to meet demand.
Resilience: Failures in one service do not affect the entire system.
Flexibility: Developers can choose the best technology for each service without being locked into a single framework or language.
Containerization tools like Docker and orchestration platforms like Kubernetes have become essential. They enable rapid deployment, efficient resource management, and simplified scaling. For BrushinBella, this meant that the custom C++, Java, and Python services could be managed and updated independently, leading to a more resilient and adaptable platform.6.2. The Role of Hybrid Technology Stacks
While many modern websites rely heavily on JavaScript frameworks for the front‚Äëend (such as React or Vue), integrating a hybrid technology stack can yield significant benefits. Each language and framework brings its own strengths:C++ for Performance‚ÄëCritical Tasks: In performance‚Äësensitive areas such as image processing, the efficiency of C++ is unmatched.
Java for Robust Enterprise‚ÄëGrade Logic: Java‚Äôs strong typing, mature ecosystem, and scalability make it ideal for handling complex business rules and integrations.
Python for Rapid Development and Data Analysis: Python‚Äôs ease of use and powerful libraries allow teams to quickly prototype and deploy data‚Äëdriven features.
This multi‚Äëlanguage approach is becoming more common as companies seek to optimize for both performance and development speed. It is essential, however, to manage this diversity with clear interfaces, robust API designs, and comprehensive documentation.6.3. User Experience and Accessibility as Top Priorities
No matter how powerful the backend or how sophisticated the custom integrations, the success of a website ultimately depends on the user experience. Best practices dictate that designers and developers should focus on:Responsive Design: Ensuring that the website looks and functions well on all devices.
Accessibility: Building websites that are usable by everyone, including those with disabilities.
Intuitive Navigation: Simplifying the user journey from landing on the page to completing a transaction.
Visual Appeal: Using high‚Äëquality images, consistent branding, and engaging interactive elements to create a memorable user experience.
For BrushinBella, rigorous user testing and iterative design refinements ensured that the website not only met functional requirements but also delighted its target audience.6.4. Security in a Connected World
Security remains one of the most critical aspects of web development. With increasing data breaches and cyberattacks, best practices include:Encryption of Data: Both in transit (using SSL/TLS) and at rest.
Regular Security Audits: Automated vulnerability scanning and penetration testing to identify and mitigate risks.
Robust Authentication: Using multi‚Äëfactor authentication (MFA) and secure API gateways.
Input Validation and Sanitization: To prevent common attacks such as SQL injection and cross‚Äësite scripting (XSS).
Implementing these security measures is not a one‚Äëtime task but an ongoing process that requires constant vigilance.6.5. The Future: Serverless, AI, and Progressive Web Apps
Looking ahead, several trends are shaping the future of web development:Serverless Architectures: Platforms such as AWS Lambda, Google Cloud Functions, and Azure Functions are enabling developers to run code without managing servers. This can reduce costs and simplify deployment for certain types of applications.
Artificial Intelligence and Machine Learning: Integrating AI into web applications is becoming increasingly common. Whether for personalized recommendations, chatbots, or automated analytics, AI can dramatically enhance the user experience.
Progressive Web Apps (PWAs): PWAs combine the best features of web and mobile applications, offering offline functionality, push notifications, and fast load times. They represent the future of delivering seamless, app‚Äëlike experiences through the browser.
For BrushinBella, these trends offer opportunities for future enhancements. For example, a serverless recommendation engine or AI‚Äëdriven customer support chatbot could further enrich the user experience.Reflections and Lessons Learned
7.1. Collaboration Is Key
One of the most important lessons from the BrushinBella project was the power of cross‚Äëfunctional collaboration. Bringing together designers, developers, business analysts, and marketing experts enabled the team to view the project from multiple perspectives. This collaborative approach ensured that the final product was not only technically sound but also aligned with the brand‚Äôs vision and customer needs.7.2. Flexibility in Technology Choices
Adopting a hybrid technology stack may seem daunting at first, but it can yield enormous benefits when managed properly. By leveraging the strengths of C++, Java, and Python in different parts of the system, the team was able to optimize for performance, scalability, and rapid development. This flexibility allowed BrushinBella to build a robust platform that could evolve with changing requirements.7.3. Iteration and Continuous Improvement
The project was not built in a single, linear pass. Instead, it was an iterative process where feedback was continuously gathered and incorporated. From early wireframes to final deployment, each iteration brought improvements and refinements. The use of CI/CD pipelines and automated testing ensured that each update maintained the high standards of quality required for a live e‚Äëcommerce site.7.4. Balancing Innovation and Practicality
While it was tempting to incorporate cutting‚Äëedge technologies and ambitious features, the team also had to remain practical. Decisions were driven by both innovative ideas and real‚Äëworld constraints such as budget, timelines, and technical feasibility. This balance ensured that the website was not only modern and attractive but also reliable and maintainable over the long term.Conclusion: A Modern Web Experience for Today‚Äôs Parents
The creation of the BrushinBella website is a testament to what can be achieved when visionary design meets technical excellence. By thoughtfully planning each phase, embracing a hybrid development strategy, and leveraging the unique strengths of C++, Java, and Python, the team built a website that not only drives sales but also resonates with its audience.Holistic Planning: Successful projects begin with a clear vision and a well‚Äëdefined scope. Every stakeholder‚Äôs input is valuable in shaping a product that meets both business and user needs.
User‚ÄëCentered Design: Prioritizing the user experience‚Äîthrough responsive design, intuitive navigation, and accessibility‚Äîensures that the website remains relevant and engaging.
Technological Synergy: Using a blend of languages and frameworks allows teams to optimize different aspects of the application. C++ provided high‚Äëperformance modules, Java ensured robust enterprise‚Äëgrade processing, and Python accelerated data analytics and automation.
Resilient Architecture: Adopting microservices and containerization enabled the system to scale, adapt, and remain resilient in the face of increasing demand.
Ongoing Evolution: The journey does not end at launch. Continuous testing, monitoring, and optimization are crucial to keeping a website secure, fast, and responsive to changing user expectations.
As BrushinBella continues to grow and innovate, the lessons learned from this project will serve as a roadmap for future enhancements. With a focus on emerging trends such as serverless architectures, AI integrations, and progressive web apps, the company is well‚Äëpositioned to adapt to the evolving digital landscape and maintain its commitment to making parents‚Äô everyday life easier.In a world where the digital experience often makes or breaks a brand, BrushinBella‚Äôs website stands as an exemplar of what thoughtful planning, creative design, and technical excellence can achieve together. Whether you‚Äôre an expert in web development or a newcomer trying to understand the complexities behind a modern e‚Äëcommerce platform, the BrushinBella story offers valuable insights into building a system that is both powerful and personable.Expert Perspectives and Future Outlook
Industry experts agree that the integration of multiple programming paradigms is the future of web development. Leaders in the field emphasize the importance of:Adopting Modular Architectures: Breaking down applications into microservices not only improves scalability but also enhances maintainability.
Investing in Performance Optimization: As user expectations continue to rise, ensuring fast load times and seamless interactions will remain a top priority.
Fostering Interdisciplinary Collaboration: The most innovative projects arise when cross‚Äëfunctional teams work together, blending design, technology, and business acumen.
Embracing Continuous Learning: With technologies evolving at a rapid pace, staying updated with the latest tools, frameworks, and best practices is essential for success.
Looking forward, trends such as the integration of AI in personalization, the adoption of serverless computing to reduce operational overhead, and the rise of progressive web apps are expected to shape the future of web development. BrushinBella is already exploring these avenues, planning to integrate AI‚Äëdriven customer insights and further optimize the platform using serverless components.Final Thoughts
The journey of building the BrushinBella website highlights the dynamic and ever‚Äëevolving nature of web development. By combining the proven capabilities of established platforms like Shopify with custom‚Äëdeveloped microservices in C++, Java, and Python, the team created a system that is greater than the sum of its parts. This approach not only met immediate business needs but also laid a robust foundation for future growth and innovation.For developers and business leaders alike, the BrushinBella project is a compelling case study in the effective melding of design, technology, and user‚Äëcentric strategy. It demonstrates that with careful planning, strategic technology selection, and relentless focus on the user experience, it is possible to create a digital platform that truly makes a difference.Whether you are planning your next web development project or simply interested in learning how modern e‚Äëcommerce platforms are built, the BrushinBella story offers a wealth of insights. As technology continues to evolve, so too will the tools and methods used to create these digital experiences. The key is to remain agile, to embrace change, and to always put the user first.In summary, the BrushinBella website is more than just an online store‚Äîit is an embodiment of a brand‚Äôs promise to simplify and enrich the lives of parents. It stands as a reminder that at the heart of every great digital experience is a commitment to quality, innovation, and user empowerment.]]></content:encoded></item><item><title>Complete Guide to Virtual Environments (Virtualenv) in Python</title><link>https://dev.to/mrnik/complete-guide-to-virtual-environments-virtualenv-in-python-3jn1</link><author>Mahdi Ahmadi</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 02:02:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  1. What is a Virtual Environment in Python?
When developing multiple projects with Python, each project may require different verzsions of libraries. This is where Virtual Environment (Virtualenv) comes to the rescue!A virtual environment is an isolated space for installing libraries and packages for a specific project without affecting your main system.
  
  
  2. Why Should You Use Virtualenv?
Avoid version conflicts: If different projects require different versions of the same library, conflicts may arise without a virtual environment.Project isolation: Each project has its own set of dependencies, ensuring stability.Portability: You can easily recreate the project environment on another system using a requirements.txt file.Increased security: Installing packages in an isolated environment prevents unintended changes to system files.
  
  
  3. Installing and Using Virtualenv
Installing Virtualenv on Windows, Linux, and macOSIf Virtualenv is not already installed, you can install it using the following command:Creating a Virtual EnvironmentTo create a virtual environment in your project directory, run:venv is the name of the folder where the virtual environment will be created. You can use any name you prefer.Activating the Virtual EnvironmentThe activation process depends on your operating system:
On Windows (CMD or PowerShell):venv\Scripts\Activate.ps1
Once activated, you will see the virtual environment name in the terminal prompt:Installing Packages in the Virtual EnvironmentAfter activation, you can install project dependencies using:Deactivating the Virtual EnvironmentTo deactivate the virtual environment, simply run:
  
  
  4. Saving and Recreating the Virtual Environment with To save the list of installed packages in the virtual environment, use:pip freeze > requirements.txt
To recreate the same environment on another system:pip install -r requirements.txt
Virtualenv helps you run Python projects in an isolated and conflict-free manner.You can install it with pip install virtualenv.Create and activate a virtual environment with venv.Use requirements.txt to store and restore dependencies.Thanks for reading‚ù§Ô∏è
I hope this guide helps you understand and use virtual environments effectively. If you have any questions or suggestions, feel free to leave a comment!]]></content:encoded></item><item><title>RandomPosterize in PyTorch</title><link>https://dev.to/hyperkai/randomposterize-in-pytorch-40ac</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 01:59:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomPosterize() can randomly posterize an image with a given probability as shown below:The 1st argument for initialization is (Required-Type:):
*Memos:

It's the number of bits to keep for each channel.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is posterized or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>RandomInvert in PyTorch</title><link>https://dev.to/hyperkai/randominvert-in-pytorch-4e0o</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 01:57:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is inverted or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>What to do if the selenium crawler is detected?</title><link>https://dev.to/98ip/what-to-do-if-the-selenium-crawler-is-detected-4o0f</link><author>98IP ‰ª£ÁêÜ</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 01:50:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When using Selenium for automated web crawling, it is often detected and blocked by the target website. This is usually because Selenium's automation features are more obvious and can be easily identified by the website's anti-crawler mechanism. This article will explore in depth how to deal with the problem of Selenium crawler being detected, including methods such as hiding automation features and using proxy IPs, and provide specific code examples. At the same time, 98IP proxy will be briefly mentioned as one of the solutions.
  
  
  I. Reasons for Selenium crawlers being detected

  
  
  1.1 Obvious automation features
Selenium's default browser behavior is significantly different from manual user operations, such as specific fields in the request header, fixed browser window size, uniform operation speed, etc., which may be used by websites to identify automated scripts.
  
  
  1.2 Frequent request frequency
Crawlers usually send requests at a frequency much higher than normal users, which can also easily alert websites.If the crawler always sends requests from the same IP address, the IP address will soon be blacklisted by the website.
  
  
  II. Strategies for dealing with Selenium crawler detection

  
  
  2.1 Hide automation features

  
  
  2.1.1 Modify request headers
Through Selenium's webdriver.ChromeOptions() configuration, you can modify the browser's request header to make it closer to normal user requests.
  
  
  2.1.2 Randomize browser settings
Use libraries such as  to automatically manage browser drivers and randomize window size, scrolling behavior, etc. to simulate real user operations.Sending requests through proxy IPs can effectively avoid the problem of IP being blocked. High-quality proxy services such as 98IP Proxy provide stable and anonymous IP resources, which is an effective means of dealing with Selenium crawlers being detected. The above code uses the  library instead of  because  provides more flexible proxy configuration and request interception functions. If you haven't installed  yet, you can install it through .
  
  
  2.3 Controlling request frequency
By introducing random delays and setting reasonable request intervals, the request frequency of the Selenium crawler can be controlled to make it closer to the browsing behavior of normal users.It is a common problem for Selenium crawlers to be detected, but by hiding automation features, using proxy IPs, controlling request frequency, etc., we can effectively reduce the risk of being detected. In particular, using high-quality proxy services such as 98IP Proxy can significantly improve the stability and success rate of crawlers.In the future, with the continuous advancement of website anti-crawler technology, we also need to continuously update and improve crawler strategies. For example, introducing more complex browser simulation technology, using machine learning to predict and circumvent blocking strategies, etc. are all directions worth exploring.In short, dealing with the problem of Selenium crawlers being detected requires comprehensive consideration of multiple factors and taking corresponding measures.]]></content:encoded></item><item><title>Writing Pythonic Code With Python Data Model</title><link>https://dev.to/noble47/writing-pythonic-code-with-python-data-model-2j3o</link><author>Noble-47</author><category>dev</category><category>python</category><pubDate>Mon, 17 Feb 2025 00:14:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This apparent oddity is the tip of an iceberg that, when properly understood, is the key to everything we call Pythonic. The iceberg is called the Python data model, and it describes the API that you can use to make your own objects play well with the most idiomatic language features.‚Ää-‚ÄäLuciano Ramalho (Fluent Python: clear, concise, and effective programming)What's so special about the Python data model one may ask. Rather than giving a personal answer, why don't we do a little dive in and see what we can accomplish by understanding the data model. Data model simply speaks about how data is represented. In Python, data is represented by objects or to sound a bit technical, objects are Python's abstraction for data. The data model provides us with an API that allows our objects to play well with the 'under the hood' of Python programming.In our little dive into Python data model, we are going to specifically focus on the special methods. Special methods are class functions with special names that are invoked by special syntax. Defining these special methods in our class definitions can give our class instances some really cool Python powers like iteration, operator overloading, working well with context managers (the 'with' keyword), proper string representation and formatting, and many more. To show you how you could implement these special functions into your classes, we will consider two examples of situations where using these special functions would make our codes clearer and more Pythonic.The first example is a little bit outside-the-box solution I came up with for creating a simple game of Rock-Paper-Scissors in Python and the second is going to be a bit mathematical in nature but I'm going to walk you through each line of codeA Simple Game Of Rock Paper ScissorsJust in case you are not familiar with the Rock-Paper-Scissors game, it is originally a hand game usually played among two people that involves making signs of either Rock or paper or scissors. Knowing the whole history of the game doesn't really matter what is important is knowing how to determine the winner. In a conventional setting, a hand sign of rock would always win against scissors but will lose against paper, a hand sign of scissors would win against paper and lose to rock and obviously, paper would lose to scissors and win against rock. we can summarize this as shown belowFor our Python emulation of this game, we will limit the number of players to just two, one player would be the computer and the other would be the user. Also, this is not a machine learning article or a write-up about computer vision, our users would still have to type in an option between rock, paper, and scissors on the terminal for our program to work.
Before we go into the actual coding, it's good that we take a step back and consider how we want our Python script to be. For my solution to this challenge, I will use the random module to enable the computer select a random option of either rock, paper, or scissors. To implement how our code evaluates the winner, I'm going to make the following assumptions:I'm also going to take an OOP approach; our rock, paper, and scissors will be treated as objects and not string variables. Rather than creating three separate classes for each, I'll create only one that can represent any of them. This approach would also allow me to show you how special methods make life easier. Now to the fun aspect!Naming our class RPS may sound a bit odd, but I found the name 'RPS' to be a good fit cause each letter comes from the initials, R for Rock, P for Paper, and S for Scissors. What's important to note here is that creating an instance of our class requires two arguments: pick and name. We already stated that the users of our script would have to type in their selected option on the terminal, instead of making our users type in 'Paper' (which could be so stressful for them) why don't we just allow our user to type in 'P' (or 'p') to select 'Paper', that's what the pick stands for. The name property is the actual name e.g 'Paper'. So now that we know what each parameters is for, we can now inspect our class by creating an instance>>> p = RPS('P', 'Paper') # create an instance
>>> p.name
# return : Paper
>>> p.pick
# return : P
>>> print(p)
# return : <__main__.RPS object at 0x...>
Our class instance was created and has the right attributes but notice what we get when we try to print the contents of the variable holding our class instance. Before getting into the technical details of how our class instance returns the odd-looking string, let's update our class definition by adding a single special function and see the difference.Now let's create an instance and try printing our class instance again>>> p = RPS('P', 'Paper')
>>> print(p)
# return : RPS(P, Paper)
As we can see, by defining the '' method we can achieve a better looking result. Let's make one more change to our class definition.Now let's create an instance and test it again.>>> p = RPS('P', 'Paper')
>>> p
# return : RPS(P, Paper)
>>> print(p)
# return : Paper
>>> str(p)
# return : 'Paper'
>>> repr(p)
# return : 'RPS(P, Paper)'
To know what's going on here, we need to know a little about the print function. The print function converts all non-keyword arguments(like our p variable) to string using the built-in Python class . If calling  on our variable fails, python falls back on the built-in  function. When  is called on our object, it looks for a  method, if it finds none, it fails and then searches for a  method. Both the  and the  methods are special methods used for string representation of our object. The  method gives the official string representation of our object while the  method gives a friendly string representation of our object. I usually say that the  method is like talking to another developer and it usually shows how to call our class and the  is like talking to a user of our program (like the player in this case), you would usually just want to return a simple string like "Paper" to show the user what he picked.Although I stated the  and  as the two special functions in our class definition, there's actually a third special method, and yes it is the most common one, the  function. It is used for initializing our class and called by the  special method just before returning our class instance. Did I just mention another special method we haven't defined? yes, I did. It may also interest you to know that Python automatically adds some other special methods to our class. You can check them out by calling the built-in function  on our class instance like this>>> dir(p)
# returns : ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'name', 'pick']
Special functions or methods can be identified by the way they are named, they always begin with double underscores '' and end with double underscores '' Because of this special way of naming these methods, they are commonly called daunder methods (Double + UNDERscores = DUNDER). So if a method name begins with double underscores, it is most likely but not certainly a special method. Why not certainly? this is simply because Python does not stop us from defining our own methods using the dunder syntax. Alright back to our game script.All that's left for us now is for us to let our script know how to determine a winner. As stated earlier, I will use comparison to evaluate a winner.# comparison logic
Rock > Scissors
Scissors > Paper
Paper > Rock
To implement this solution, I will add a dictionary and use the daunder greater_than method. The dictionary key would be the initials of Rock, Paper and Scissors. The value of each key would be the only other element that the key can defeat.Notice the new lines of code, first the options dictionary and then the  method definition. With these new lines of code, let's see what new functionality our code now has.# create a rock instance
>>> r = RPS('R', 'Rock')

# create a paper instance
>>> p = RPS('P', 'Paper')

# create a scissors instance
>>> s = RPS('S', 'Scissors')

>>> print(r,p,s)
# return : Rock Paper Scissors

>>> p > r # paper wins against rock
# return : True

>>> r > s # rock wins against scissors
# return : True

>>> s > p # scissors wins against paper
# return : True

>>> p < s # paper lose to scissors
# return : True

>>> p < r # paper lose to rock
# return : False

>>> p < s < r# paper lose to scissors which lose to rock
# return : True

>>> p >= r paper wins or tie to rock
# return : Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
TypeError: '>=' not supported between instances of 'RPS' and 'RPS'
Just by adding to  special method, our class instances have gained magical powers (daunder methods are sometimes called magic methods and we can see why). By implementing the daunder gt method, our class instance now relates well with the  and  symbols but not the  and  symbols. The reason is that  is just the negation of . The special method for  is  which can just be the negation of calling .For the ‚â• symbol, its special method is the  and it must be defined for our object to work well with the  sign. But in this program, we can do without it. Another missing piece would be to check if two separate instances of Paper are equal.>>> p1 = RPS("P", "Paper")
>>> p2 = RPS("P", "Paper")
>>> p3 = p1
>>> p1 == p2
# return : False

>>> p1 == p3
# return : True

>>> id(p1)
# return : 140197926465008

>>> id(p2)
# return : 140197925989440

>>> id(p3)
# return : 140197926465008

>>> id(p1) == id(p3)
# return : True

>>> id(p2) == id(p1)
# return False
The default operation of the equal comparison sign is to compare the id of the object. p1 and p2 are different class instances that happen to have the same attributes but their id differs and therefore are not equal. When we assign a variable to a class instance, we make that variable point to the address of the instance which is what we observe for p3 which has the same id as p1. We have the option of overriding how the equality comparison works on our object by defining and implementing our own  method. But for this script, I will compare two instances using their pick attribute. Now that we have defined our class and know how it works, we are now ready to see the full implementation of the Python scriptLet me walk you through the code. We are already familiar with the RPS class definition. If you recall, our code is meant to allow the computer to select choices at random and that's what the random module is for. The random module makes available the  function which allows the 'random' selection of an element from an iterable object e.g. a list in Python. The list in this case is the . Because our class is made to work with uppercase letters for comparison, it is necessary that we always initialize our objects with uppercase for the pick attribute. This is why we first convert the user's input to upper case (line 34) with the¬†. It is also possible that our user types in an unexpected character like 'Q' so we have to validate our user input by checking if the uppercase character is part of the valid options in . The mapping dictionary allows us to quickly convert the user's input to a corresponding instance of RPS after being validated. The evaluate_winner function makes use of the comparison symbol to determine the winner. Because we want the code to run in a loop until a winner is found, we make use of a while loop and when a winner is found, the evaluate_winner function returns True which will then break the loop and exit the game.Here is one of the various results of running the codeOur Python code runs as expected, although there could be a couple of improvements or new features to add. The most important thing is that we see how using special methods in our class definition gives our code a more Pythonic feel. Assuming we were to take a different approach such as using nested if statements, our evaluate_winner method would look something like thisdef evaluate_winner(user_choice, comp_choice):
    # check if user choice is 'R'
    if user_choice == 'R':
        # check if comp_choice is 'R'
        if comp_choice == 'R':
            # it is a tie
            ...
        elif comp_choice == 'S':
            # user wins
            ...
        else:
          # computer wins
          ...
    if ... 
     # do the same for when user_choice is 'S' and then for
     # when user_choice is 'P'
A problem with this approach other than the lengthy code is that if we desire to add a new element, diamond which can beat both rock and scissors but not paper (for an unknown reason), our if statements would begin to look really awkward. Whereas in our OOP approach, all we have to do is to modify the options dict like sooptions = {"R" : ["S"], "P" : ["R"], "S" : ["P"], "D" : ["R", "S"]}
and then we change the if statement in  to bedef __gt__(self,x):    
    if x.pick in self.options[self.pick]:
        return True
    else:
        return False
we can make the statement shorterdef __gt__(self, x):
   return True if x.pick in self.options[self.pick] else False
To conclude, here are some things you should note about using special methods:You hardly (or never) call them directly yourself, let Python do the calling for youWhen defining functions that use the dunder naming syntax, you should consider that Python could one day define such a function and give it a different meaning. This could break your code or make it behave in unexpected waysYou certainly don't have to implement every special method there is. Just a couple that you are really sure you need. Remember, simple is better than complex. If there's a simpler way you should use that insteadThis is the first part of the topic, in the next part, we are going to be dealing with operator overloading and making iterable objectsHope you enjoyed this article!!!]]></content:encoded></item><item><title>RandomPosterize in PyTorch</title><link>https://dev.to/hyperkai/randomposterize-in-pytorch-35e6</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 22:14:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomPosterize() can randomly posterize an image with a given probability as shown below:The 1st argument for initialization is (Required-Type:):
*Memos:

It's the number of bits to keep for each channel.The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is posterized or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>python Level 1</title><link>https://dev.to/mohamed_yahyasidimohame/python-level-1-366c</link><author>Mohamed Yahya Sidi Mohamed</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 21:37:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A GUI App Which You Can Visulise Signal Waveforms With Python</title><link>https://dev.to/gokhanergentech/a-gui-app-which-you-visulise-signal-waveforms-with-python-2ben</link><author>G√∂khan ERGEN</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 20:56:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this App, you can visulise three signal waveforms such as Sinusoidal, Square, and saw-toothed. Also, these signals has some params which you can setting them. The application was developed with dearpygui providing UI components for desktop apps. If you want to write a detailed blog about dearpygui, please comment :).You can setting the ampitute of the waves.A constant value which offsets veritcallyThis is a sampling frequency showing how many samples is collected per a second.We can use this to take the cycle count of signals. A cycle takes 1/Fsig seconds.If Fsig is 0.2hz then the cycle count will be 5 seconds.you can visulise sinusoidal sampled signal, squared sampled signal, and saw-toothed sampled signal by using above the params. The time range of signals is splitted (max_time-min_time)*Fs because Fs is sampling frequence per a second.The program, which you change the params has a basic interface.
As a default, selected waveform is sinusoidal.
Signal time range is between 0 and 10 and uses A*sin(2*œÄ*Fsig*t+fi0)+dc as a formula to be drawn.
In case A = 4,We can use that positive side is about max 5 and negative side is min -3, because DC is 1 so signal shifts verticaly to up 1 step.You will see a squared sampled signal.
signal_wave = sin(2*œÄFsig*t)
if signal_wave >=0, 1
**Saw-toothed Wave*I use scipy library to draw saw-toothed wave form.
All of these signal waveforms use the same params.
Lets change the time range as -100 to 100. We will see this graph sinusoidal waveforms.I changed Fsig to 0.5. This means T = 1/0.5 = 2s cycle time. 3.11.5 1.11.3 1.26.0 1.9.0]]></content:encoded></item><item><title>RustyNum Follow-Up: Fresh Insights and Ongoing Development</title><link>https://dev.to/igorsusmelj/rustynum-follow-up-fresh-insights-and-ongoing-development-18f9</link><author>IgorSusmelj</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 20:38:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a follow-up to my previous introduction to RustyNum, I want to share a developer-focused update about what I‚Äôve been working on these last few weeks. RustyNum, as you might recall, is my lightweight, Rust-powered alternative to NumPy published on GitHub under MIT license. It uses Rust‚Äôs portable SIMD features for faster numerical computations, while staying small (around ~300kB for the Python wheel). In this post, I‚Äôll explore a few insights gained during development, point out where it really helps, and highlight recent additions to the documentation and tutorials.If you missed the initial announcement, RustyNum focuses on:High performance using Rust‚Äôs SIMDMemory safety in Rust, without GC overheadSmall distribution size (much smaller than NumPy wheels)NumPy-like interface to reduce friction for Python users
  
  
  Developer‚Äôs Perspective: What‚Äôs New?
1. Working with Matrix OperationsI‚Äôve spent a good chunk of time ensuring matrix operations feel familiar. Being able to do something like matrix-vector or matrix-matrix multiplication with minimal code changes from NumPy was a primary goal. A highlight is the  function and the  operator, which both support these operations.It‚Äôs neat to see how close this is to NumPy‚Äôs workflow. Benchmarks suggest RustyNum can often handle these tasks at speeds comparable to, and sometimes faster than, NumPy on smaller or medium-sized datasets. For very large matrices, I‚Äôm still optimizing the approach.2. Speeding Up Common Analytics TasksThe Python overhead can sometimes offset the raw Rust speed, but in many cases, RustyNum still shows advantages.
  
  
  New Tutorials: Real-World Examples
One of the best ways to see RustyNum in action is through practical examples. I‚Äôve added several new tutorials with real-world coding scenarios: ‚Äì Focus on dot products, matrix-vector, and matrix-matrix tasks.Replacing Core NumPy Calls ‚Äì Demonstrates how to switch from NumPy‚Äôs mean, min, dot to RustyNum.Streamlining ML Preprocessing ‚Äì Explores scaling, normalization, and feature engineering for machine learning.Check out a snippet of scaling code from that guide:It‚Äôs a small snippet, but it shows how RustyNum can do row/column manipulations quite effectively. After scaling, you can still feed the data into your favorite machine learning frameworks. The overhead of converting RustyNum arrays back into NumPy or direct arrays is minimal compared to the cost of big model training steps.1. Large Matrix OptimizationsI‚Äôve noticed that for very large matrices (like 10k√ó10k), RustyNum‚Äôs current code paths aren‚Äôt yet fully optimized compared to NumPy. This area remains an active project. RustyNum is still young, and I‚Äôm hoping to introduce further parallelization or block-based multiplication techniques for better large-scale performance.RustyNum supports float32 and float64 well, plus some integer types. I‚Äôm considering adding stronger integer support for data science tasks like certain indexing or small transformations. Meanwhile, advanced data types (e.g., complex numbers) might appear further down the line if the community needs them.3. Documentation and API EnhancementsThe docs site at rustynum.com has an API reference and a roadmap. I‚Äôm continuously adding to it. If you spot anything missing or if you have a specific use case in mind, feel free to open a GitHub issue or submit a pull request.4. The big goal of RustynumRustyNum is simply a learning exercise for me to combine Rust and Python. Since I spend every day around machine learning I would love to have RustyNum replace part of my daily Numpy routines. And we're slowly getting there. I started adding more and more methods around the topic of how to integrate RustyNum in ML pipelines.
  
  
  Quick Code Example: ML Integration
To demonstrate how RustyNum fits into a data pipeline, here‚Äôs a condensed example:This script highlights that RustyNum can handle data transformations with a Pythonic feel, after which you can pass the arrays into other libraries.It‚Äôs been fun to expand RustyNum‚Äôs features and see how well Rust can integrate with Python for high-performance tasks. The recent tutorials are a window into how RustyNum might replace parts of NumPy in data science or ML tasks, especially when smaller array sizes or mid-range tasks are involved.Check out the tutorials at rustynum.comContribute or report issues on GitHubShare feedback if there‚Äôs a feature you‚Äôd love to seeThanks for tuning in to this developer-focused update, and I look forward to hearing how RustyNum helps you in your own projects!]]></content:encoded></item><item><title>Project Translate: The Translate API (Part 2)</title><link>https://dev.to/__dbrown__/project-translate-the-translate-api-part-2-2nd1</link><author>Emmanuel Akolbire</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 18:52:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hey developers! üëã In this post, we'll implement the text translation endpoint using Python, AWS Lambda, and a clean Hexagonal Architecture. Let's dive in! You can check out my GitHub for the complete code.We create a new project with the directory structure shown in the picture
Then we install the dependency, namely boto3, with pip. We also make sure to create a requirements.txt file so we know which version to install when the script is packaged.We'll be employing Hexagonal(Layered) Architecture in the design of our API. Hexagonal Architecture or Ports and Adpaters is a design pattern that aims at creating loosely coupled components. A helpful guide can be found here. Although python is a dynamically typed language, we can still use this pattern.We'll be using the project directory structure shown below
  
  
  The Translation Record Model
Let's start with a simple but effective model to track our translations. We'll use Python's dataclasses - they're clean, efficient, and give us nice features out of the box.Let's break down what each field does:: A unique identifier for each translation record: The original text that needs translation: The translated result: Timestamp of when the translation was performed, automatically set to the current timeYou might wonder why we're using  instead of a regular class. Here's what makes dataclasses great for our use case:Less Boilerplate: We don't need to write , , or  methods
Default Values: Easy handling of default values with the field function
Type Hints: Built-in support for type hints, making our code more maintainableNext, we'll define our ports using Python's Protocol class - a more Pythonic approach to interfaces. Let's dive in!
Why Protocols Over Abstract Base Classes?
Before we jump into the code, let's understand why we're choosing Protocols:More Pythonic - follows duck typing principlesStructural subtyping instead of nominal subtypingBetter integration with static type checkersNo explicit inheritance required
Now we define the adapters that implement the ports. The DynamoDBPersistenceAdapter stores the input and output in DynamoDB and return a Record object. The  translates the text with AWS Translate and returns the result.Now we'll create the Lambda handler that ties everything together.
We'll define the  class with handles the requests to Lambda from the API Gateway. It parses the body for the required fields, translates the text, stores the input and output and returns a responseIn order to allow Cross Origin Requests we add the Access-Control-Allow headers to the reponse object. For example, in the  methodIn the next installment of this series, we'll dive into the code that handles file translation. Stay tuned! üöÄ]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/arindam_1729/-1h49</link><author>Arindam Majumder</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 18:30:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ü§Ø 11 Exciting GitHub Repositories You Should Check Right Now‚ö°Ô∏èArindam Majumder  „Éª Feb 13]]></content:encoded></item><item><title>Transform Your Data Model to AI Workflow - with only 8 extra lines of code!</title><link>https://dev.to/eduardknezovic/transform-your-data-model-to-ai-workflow-with-only-8-extra-lines-of-code-3f5a</link><author>Eduard Knezovic</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 17:35:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA["Good programmers worry about data structures and their relationships." - Linus TorvaldsWhat if you could create complex AI workflows as easily as defining your data structures with Pydantic?What if you could simply harness the power of AI by allowing your Pydantic data models to flow like water?ModeLLM makes this possible by turning your Pydantic models into powerful AI pipeline components.Let's go over an example!You will need to provide your own OPENAI_API_KEY (if you haven't already)You will also need to install the  library.All of the relevant dependencies are automatically installed
with the  library.Take a look at this complete working example.To consolidate your knowledge:Execute the existing code on your computerGenerate the story for teenagers (uncomment one line of code) Create a  Pydantic model that should summarize the storyCreate your own Pydantic model and inject it to the pipelineIn this example, we've managed to harness the power of AI
in (only!) 8 additional lines of code - thanks to the ModeLLM library: Define what you want, not how to get it (LLM is smart enough to catch the cue): Chain transformations with the  operator (Makes our code easy to modify and extend): Docstrings guide the AI's behavior: Easily swap components : Complex AI operations hidden behind simple data modelsBy defining our Pydantic data models (and decorating them) we were able to execute our AI pipeline with a single line of code:What do you think about this approach? I would love to hear your thoughts and suggestions.]]></content:encoded></item><item><title>Modeling a Neuron in micrograd (As Explained by Karpathy)</title><link>https://dev.to/shrsv/modeling-a-neuron-in-micrograd-as-explained-by-karpathy-6gh</link><author>Shrijith Venkatramana</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 16:38:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, I‚Äôm building LiveAPI, a tool that makes generating API docs from your code ridiculously easy.In serious neural network implementations, we model the neuron in the following way:1 "Influence"  (dendrite)Sum of "influences" =  (cell body)The above leads to the cell body expression:Activation function - squashing fuction (, )
  
  
  Representing the Model Neuron (defined above) in micrograd

  
  
  Implementing  into Value (for the Activation Function)
We have the following  formula:We can implement the function as follows:We'll add a new node  which is the :
  
  
  Derivative of o - Derivative of The formula for derivative of  is the following:So, we want to find out :do/dn = 1 - tanh(n)**2 = 1 - o**2

  
  
  Getting all the backprop values calculated (manually)
We leverage some patterns we've learned previously about how backprop works with addition/multiplication, to quickly fill in the values for  in each node:]]></content:encoded></item><item><title>Ultimate Football Draft (A Python Terminal Game)</title><link>https://dev.to/jcubilloespinoza/ultimate-football-draft-a-python-terminal-game-128i</link><author>Josue Cubillo Espinoza</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 16:31:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Football fans around the world dream of managing their favorite teams and experiencing the thrill of competition. This Python program allows users to choose a team, participate in a simulated tournament, and compete for victory. By randomly assigning teams to groups and generating matches, the program provides an engaging and interactive experience.The program begins by prompting the user to enter their name and select their favorite football team from a list of international clubs. After choosing a team, the program randomly assigns teams to groups for the tournament‚Äôs group stage. The user‚Äôs selected team is placed in one of these groups.Using the random module, the program ensures fair and unpredictable group draws and match results. The user competes against other teams by answering trivia questions. Winning matches earns points, and the top teams from each group advance to the knockout stages.Throughout the knockout rounds, the user‚Äôs team must win to progress further. If they lose a match, they are eliminated from the competition.Random selection of football teams into tournament groups.Interactive gameplay where users answer questions to win matches.Randomized match results for AI-controlled teams.Automatic generation of tournament brackets leading to the final match.This project is a great example of how Python can be used to create engaging sports simulations. Whether you are a football fan or a programming enthusiast, this program provides an enjoyable way to experience the excitement of a football tournament. Try it out and see if your team can become the ultimate champion!]]></content:encoded></item><item><title>Unlock the Power of Neural Networks ‚Äì From Scratch!</title><link>https://dev.to/devinsights_blog_ed29ec86/unlock-the-power-of-neural-networks-from-scratch-3off</link><author>DevInsights Blog</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 15:43:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Have you ever wondered how machines can recognize images, translate languages, or even predict future trends? The secret lies in  ‚Äì the backbone of modern AI.Understanding how a neural network works can feel overwhelming, especially with so many complex libraries available. But what if you could actually build a neural network from scratch and understand every single step?The core concepts behind neural networksForward propagation, backpropagation, and loss calculation explained simplyA complete hands-on example in Building a neural network without relying on libraries like TensorFlow or PyTorch will give you  in AI. It‚Äôs like learning the fundamentals of a car engine before driving a sports car. Once you master this, using advanced tools will make far more sense.Here‚Äôs a quick look at what you‚Äôll be able to do:This simple piece of code is part of a fully functional XOR solver you‚Äôll build from scratch!
  
  
  But That‚Äôs Just the Beginning...
The full guide covers , from initializing weights to adjusting them through backpropagation ‚Äì with clear explanations and complete working code.If you‚Äôre serious about AI and want to break free from black-box libraries, .Check it out now and start your deep learning journey today!]]></content:encoded></item><item><title>Finished Auth App for Galileo! üöÄ</title><link>https://dev.to/khaled_abdelbar_43f8c0b1d/finished-auth-app-for-galileo-4md6</link><author>Khaled Abdelbar</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 15:40:24 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I‚Äôve successfully finished building the authentication app for my Galileo project! üéâNext, I‚Äôll focus on creating a Teams App to handle team functionalities, such as creating and joining teams. This will be a crucial step in enhancing collaboration within the project.I‚Äôll continue documenting each step of my journey here as I build Galileo, sharing insights and challenges I encounter. Stay tuned for updates on how I tackle the team management feature!]]></content:encoded></item><item><title>Python User Group Dhaka: Our Journey Begins ‚Äì Event Recap</title><link>https://dev.to/tamalchowdhury/python-user-group-dhaka-our-journey-begins-event-recap-2he7</link><author>Tamal Anwar Chowdhury</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 14:23:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[On a cool February evening in Dhaka, a group of passionate Python enthusiasts gathered for something special‚Äîour city‚Äôs first-ever Python User Group meetup. What started as an idea a few weeks earlier turned into a diverse community event, laying the foundation for Python Dhaka‚Äôs journey.Here‚Äôs how we made it happen and what we learned along the way.The whole event was organized through our Facebook group. In Bangladesh, Facebook is the primary hub for tech enthusiasts to form communities and organize events. Unlike platforms like Meetup or Discord, Facebook groups often serve as the starting point for grassroots tech movements here. I launched a Facebook group for Python Dhaka a few weeks prior, and thanks to my personal network, it grew to 250 members within two weeks.The next logical step was to organize an in-person meetup, because I want this group to be all about connecting with the local Python enthusiasts. After thinking about it for a few days, I locked in a date and venue for the first meetup.I choose the 15th of February because it's the mid-month, and a Saturday. In Bangladesh, Fridays and Saturdays are official holidays. Friday is the most popular day for events, followed by Saturdays. I picked Chandrima Udyan for two reasons: 1. It has easy access to the bus and metro route; 2. It's a public park we can access for free.I created the event on Facebook and started spreading the word on the FB group, my profile, Twitter, Linkedin, and my Instagram handle. I even created a short video announcing the event and posted it on all of my socials.At 5 PM local time, I waited near the park entrance, feeling a mix of excitement and nerves. Soon, one by one, attendees started arriving, their calls guiding me to them. Here‚Äôs me waiting:As we entered the park, a cool evening breeze greeted us‚Äîwashing away the stress of city life and setting the perfect mood for our first meetup.I started the meeting by sharing the Python Software Foundation's missions with the attendees, and how it relates to us:To grow a diverse and international community of Python programmers: Organizing the Python Dhaka community is helping this mission.Encourage knowledge sharing, collaboration, and support devs of all backgrounds: Students from three universities joined this event. They were able to share knowledge and build future collaboration opportunities. This community is open to all levels of developer experience.Grow a diverse and welcoming community; support underrepresented communities in tech: One participant came from a non-computer science background, eager to transition into tech. This is exactly why Python Dhaka exists‚Äîto welcome everyone, regardless of their starting point.I then shared how I started evangelizing Python.I am a professional JavaScript developer with React and NextJS expertise. I use JS for my daily work. My cousin wanted to learn Python, so I got into learning it. I found Python to be easy to learn and easy to teach. I was looking for a Python community in Dhaka, but there was none. That's why I decided to organize the Python User Group Dhaka.I also mentioned that I don't own Python Dhaka. I am only organizing it for the time being. As this community grows, and if in the future I have to move cities, I will pass the torch to the next person to continue organizing our community activities.We are not strictly Python fanatics. We would love to collaborate with our friends in the JavaScript, PHP, and Kotlin communities too from time to time.We heard personal stories from the attendees, how they got into programming, and how they are using Python. Two students are doing competitive programming with C, C++ and thinking of switching to Python for CP and ML.Two of the attendees came from different districts just so they could attend this meetup.¬†One attendee traveled over 80 kilometers from Tangail just to be here. That level of dedication reminded me why this community matters.I shared many tech tips, and also informed them about the free GitHub Student Developer pack and how to avail it. We also discussed getting real-world experiences by putting an app out in the world.We also announced the Campus Ambassador Program for Python Dhaka and nominated Abdullah to be the ambassador at Southeast University. A campus ambassador is a person in your college/university who will promote and evangelize Python programming language on your campus.As the meetup was about to end, it was getting dark when we clicked this group photo.Our journey is just beginning. We plan to host monthly meetups around the 15th of each month, but our ambition goes beyond that. We want to nurture the next generation of Python developers in Bangladesh, and one day, bring PyCon Bangladesh to life.If you‚Äôre as passionate about Python as we are‚Äîwhether you‚Äôre in Dhaka or anywhere in the world‚Äîwe‚Äôd love your support. Find Python User Group Dhaka on Facebook, or connect with me on Linkedin, X, and Instagram. Let's build this community together!]]></content:encoded></item><item><title>Mobile-First Approach for FastAPI Full-Stack Template Authentication: Migrating to phone_number/OTP</title><link>https://dev.to/justjayzee/mobile-first-approach-for-fastapi-full-stack-template-authentication-migrating-to-phonenumberotp-m02</link><author>Javad Zarezadeh</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 14:20:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As you may know,  is one of the most admired frameworks for developing RESTful APIs. Another fantastic project by the same author, @tiangolo, is the Full Stack FastAPI Template, which I previously wrote about here.In this post, I'll guide you through the process of replacing the email/password authentication flow in the template with a phone_number/OTP-based system. This approach is ideal for mobile-first applications and offers a user-friendly, secure way to authenticate users. My goal is to make minimal changes to the original project while maintaining its adherence to  and . Let‚Äôs dive in! üòâ
  
  
  1. Replace  and  with  and Update the  value to a phone number, e.g.,  or .Remove  as it is no longer necessary.Replace all instances of the  field with .‚ö†Ô∏è  This change requires updating the database schema using Alembic migrations../backend/app/api/routes/private.py
Replace all occurrences of  and  with  and ../backend/app/api/routes/login.py call:‚ö†Ô∏è  Keep  and  unchanged due to OAuth2 standards../backend/app/api/routes/users.pyReplace  with .Remove the  in the  function related to email validation.Replace  references with .Remove  as it is no longer necessary. ./backend/app/crud.py  Replace all  and  references with  and .
Rename  to  and update all references to this function.
  
  
  2. Add an API Endpoint to Request OTP
./backend/app/api/routes/login.py
Add the following endpoint:
  
  
  3. Nullify OTP After Login
./backend/app/api/routes/login.py
In the  function, nullify the OTP after successful login:
  
  
  4. Remove Unnecessary Functions
./backend/app/api/routes/login.py
Remove the following functions:recover_password_html_content
  
  
  5. Remove Unnecessary Email Features
./backend/app/api/routes/users.py
Remove the email-related logic, such as:
  
  
  6. Remove Password Update and User Registration Functions
./backend/app/api/routes/users.pyRemove the following functions:  Since we are now using OTP-based authentication, these functions are redundant.  Update all  fields to .  Remove OTP from :For the  model, you don‚Äôt need to include the  field. Update it as follows:This simplifies the creation process since OTP will be generated later during login.  Add an  field to both the  and  models.  Remove Unnecessary Models:Delete models that are no longer needed, including:  This cleanup ensures the models remain relevant to the new authentication system.  generate_reset_password_emailgenerate_new_account_emailgenerate_password_reset_tokenverify_password_reset_tokenFollowing these steps will transform the Full Stack FastAPI Template‚Äôs email/password flow into a phone_number/OTP-based system while keeping it aligned with best practices and standards. Happy coding! üöÄ These changes to the  original project are available in my GitHub. It is important to use this project cautiously, since I have not yet had time to write the tests.]]></content:encoded></item><item><title>RandomInvert in PyTorch</title><link>https://dev.to/hyperkai/randominvert-in-pytorch-5eb0</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 13:07:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is inverted or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>GaussianBlur in PyTorch (3)</title><link>https://dev.to/hyperkai/gaussianblur-in-pytorch-3-56do</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 11:20:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>just a silly game</title><link>https://dev.to/bankai2054/just-a-silly-game-29kl</link><author>anas barkallah</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 10:02:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Migrating from AWS CDK v1 to CDK v2</title><link>https://dev.to/sami_jaballah/migrating-from-aws-cdk-v1-to-cdk-v2-21nd</link><author>Sami Jaballah</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 09:40:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you‚Äôre currently using CDK v1 in your daily work, you‚Äôve probably hit a roadblock trying to implement new AWS features. AWS isn‚Äôt adding those to CDK v1 anymore‚Äîso to keep up with the latest and greatest, you‚Äôll need to migrate to CDK v2. It might sound like a hassle, but don‚Äôt worry‚ÄîI‚Äôve got your back. Let‚Äôs go through this step-by-step and get you up to speed, Python style.
  
  
  Why Should You Care About CDK v2?
Alright, let‚Äôs address the big question: why even bother migrating to CDK v2? Well, here are three solid reasons:Simplified Dependencies: No more pulling in tons of packages for different AWS services. CDK v2 bundles everything into a single package: aws-cdk-lib. How awesome is that?Stay Updated: CDK v1 isn‚Äôt getting any love anymore. If you want to keep up with the latest AWS features and updates, v2 is where it‚Äôs at.Better Developer Experience: AWS has introduced some stability guarantees and cleaned up APIs, making it easier for us to write and maintain our infrastructure code.1. Consolidated Package Structure
Remember the days of importing a separate package for each AWS service? That‚Äôs history now. CDK v2 unifies everything into aws-cdk-lib.from aws_cdk import core
from aws_cdk.aws_s3 import Bucket
from aws_cdk import Stack
from aws_cdk.aws_s3 import Bucket
2. Goodbye to Deprecated APIsSome APIs and constructs from v1 didn‚Äôt make the cut in v2. For instance, core.Construct has been replaced by constructs.Construct. A little cleanup never hurts, right?There are some great new features, like improved stability guarantees for low-level (L1) constructs and better testing capabilities with assertions.
  
  
  Let‚Äôs Get Migrating: Step-by-Step Guide
Ready to dive in? Follow these steps to upgrade your Python CDK project to v2.1. Update Your DependenciesStart by upgrading your project dependencies to use CDK v2. Open your requirements.txt or Pipfile and update them:aws-cdk-lib>=2.0.0
constructs>=10.0.0
Then, install the new dependencies:pip install -r requirements.txt
This is where the magic happens. Go through your code and replace aws_cdk.core with aws_cdk.Stack, and adjust other imports to use aws-cdk-lib.from aws_cdk import core
from aws_cdk.aws_s3 import Bucket
from aws_cdk import Stack
from aws_cdk.aws_s3 import Bucket
3. Refactor Deprecated ConstructsSome constructs have been replaced or removed. For example, core.Construct is now constructs.Construct. Update your code accordingly.class MyBucket(core.Construct):
    def __init__(self, scope: core.Construct, id: str):
        super().__init__(scope, id)
        Bucket(self, "MyBucket")
from constructs import Construct

class MyBucket(Construct):
    def __init__(self, scope: Construct, id: str):
        super().__init__(scope, id)
        Bucket(self, "MyBucket")
4. Remove Unnecessary Feature FlagsCDK v2 has removed or integrated several feature flags that were necessary in v1. To clean up your cdk.json file, remove any obsolete flags.{
  "app": "python3 app.py",
  "context": {
    "@aws-cdk/core:newStyleStackSynthesis": true,
    "@aws-cdk/aws-ec2:uniqueImds": true,
    "@aws-cdk/core:stackRelativeExports": true,
    "@aws-cdk/aws-secretsmanager:parseOwnedSecretName": true,
    "@aws-cdk/aws-kms:defaultKeyPolicies": true,
    "@aws-cdk/core:enableStackNameDuplicates": true,
    "aws-cdk:enableDiffNoFail": true,
    "@aws-cdk/aws-ecr-assets:dockerIgnoreSupport": true,
    "@aws-cdk/aws-s3:grantWriteWithoutAcl": true,
    "@aws-cdk/aws-efs:defaultEncryptionAtRest": true
  }
}
{
  "app": "python3 app.py"
}
Removing these flags ensures your project stays aligned with CDK v2 best practices.Finally, make sure everything works as expected. Run these commands:Fix any issues that pop up, and you‚Äôre good to go!
  
  
  Migration Verification Checklist
 All imports updated to aws-cdk-lib Construct imports moved to constructs package cdk diff shows expected changesAnd there you have it! Migrating from CDK v1 to v2 isn‚Äôt as scary as it might seem. With unified dependencies, better APIs, and future-proofing, this upgrade is worth the effort. Take it one step at a time, and don‚Äôt hesitate to ask for help if you hit a roadblock.Have you already migrated to CDK v2? Or are you planning to? Share your experience (or any questions) in the comments below!
  
  
  Useful links to help you along the way:
]]></content:encoded></item><item><title>My take on the Agentic Object Detection</title><link>https://dev.to/mayank_laddha_21ef3e061ff/my-take-on-the-agentic-object-detection-4612</link><author>Mayank Laddha</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 08:42:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Segmenting Everything with SAM : We detect everything and worry about filtering later.Filtering with CLIP: Once we have all the segmented objects, we don‚Äôt want all of them. We need to filter out the noise and keep only the relevant objects.Adding Reasoning with a model like GPT-4o: Okay, so we‚Äôve segmented and filtered. But what about finalising, understanding? That‚Äôs where a strong LLM like GPT-4o comes in.]]></content:encoded></item><item><title>Streamline Document Processing Pipelines with FalkorDB‚Äôs String Loader</title><link>https://dev.to/falkordb/streamline-document-processing-pipelines-with-falkordbs-string-loader-1f9g</link><author>Dan Shalev</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 08:17:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ You decide how your data is chunked and processed, ensuring that the graph structure aligns perfectly with your RAG requirements. By working with runtime memory data, the string loader avoids the overhead of writing and reading intermediate files, reducing latency and simplifying the workflow.Integration with GraphRAG SDK: The string loader is designed to work seamlessly with the GraphRAG SDK, allowing you to build advanced graph-based RAG systems with greater ease and precision. The string loader is open-source, providing transparency and the ability to customize the feature to meet specific needs.]]></content:encoded></item><item><title>Building a RAG-Powered Support Chatbot in 24 Hours of Hackathon</title><link>https://dev.to/akshay_gupta/building-a-rag-powered-support-chatbot-in-24-hours-of-hackathon-5f7c</link><author>Akshay Gupta</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 07:18:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Coffee? ‚úÖ Chai? ‚úÖ Determination to automate admin support? Double ‚úÖIn a recent 24-hour hackathon at annual PeopleGrove offsite, my team tackled an ambitious challenge: building an AI-powered support chatbot that could think and respond like a season platform administrator.Armed with Large Language Models (LLMs) and fueled by caffeine, we created a Retrieval-Augmented Generation (RAG) system that turned out to be surprisingly capable!: Our support heroes (admins and agents) spending their days üòÖ:Answering the same questions from different institutions üè´

Repetitive SSO, analytics, and user management queriesSame solutions, different institutionsContext-switching between multiple support channels üîÑ

Support tickets piling upTime-Consuming Routine Tasks ‚è∞

Manual ticket search and response formattingComplex Problem-Solving Getting Delayed üéØ

Too much time on routine questionsLimited bandwidth for critical platform improvementsOur support team needed a solution that could:Handle common queries intelligently ü§ñProvide consistent, accurate responses üìöFree up time for complex problem-solving üí°Scale support without scaling the team üìàMaintain the human touch while automating routine tasks ü§ùWe built ChatterMind ü§ñ - an AI chatbot that combines the power of LLMs with a RAG system. Think of it as a super-smart intern who:Never sleeps (unlike us during the hackathon) üò¥Has photographic memory of all support tickets üß†Knows the PeopleGrove documentation better than its authors üìöKnows when to call for backup (aka create a ticket) üÜòRemembers conversations (thanks to Redis - our MVP choice for the hackathon) üíæKeeps secrets better than a vault üîíHere's a high-level overview of how ChatterMind processes and responds to queries:Let's geek out about our tech choices for a minute! ü§ìOur initial choice was the DeepSeek model (1.5B parameters) because, well, it was lightweight and fast. But we quickly discovered it had a tendency to... let's say, get creative with the truth. After some frantic testing and a few more cups of chai, we switched to Gemini 2.0 Flash (experimental) which proved to be our goldilocks model:Better context understandingStronger reasoning capabilitiesThe secret sauce behind ChatterMind's human-like responses? Carefully crafted prompts! Our prompt engineering approach focused on:Role Definition üé≠

Defined as "Senior Product Support Specialist"Given a friendly personality and nameEstablished clear boundaries of authorityContext Management üß©

User's current location in platformPrevious conversation historyResponse Structuring üìù

Natural, conversational flowMarkdown formatting for readabilityLength limits (100-300 words)Clear action items when neededSafety Guidelines üõ°Ô∏è

Strict PII protection rulesDynamic Adaptation üîÑ

First-time vs follow-up questionsTechnical vs non-technical usersSimple queries vs complex issuesError scenarios vs success pathsExample Prompt Template üìùSystem Context:
You are ChatterMind, a Senior Product Support Specialist at PeopleGrove.
Primary Goal: Provide clear, accurate, and helpful support while maintaining security.

Behavioral Guidelines:
- Be professional yet friendly
- Start with a warm greeting for new conversations
- For follow-ups, continue naturally without greeting
- Keep responses under 300 words
- Use markdown only when needed
- Never share PII or sensitive data
- If unsure, ask for clarification
- For complex issues, guide towards ticket creation

Current Context:
- User Location: {current_path}
- Required Permissions: {path_permissions}
- User Role: {user_role}

Previous Conversation:
{chat_history}

Reference Knowledge:
{relevant_articles}
{similar_tickets}
{route_context}

Current Question: {user_query}

Response Format:
1. [If new conversation] Brief greeting
2. Acknowledge context/location
3. Provide clear solution/steps
4. Add relevant warnings/notes
5. Offer follow-up assistance
Natural conversation flow üí¨Helpful, actionable responses ‚úÖ
  
  
  Data Processing & Embeddings
We built our knowledge base from three main sources:Platform Routes & Permissions:For embeddings, we chose the  model from  because:384-dimensional embeddings (good balance of size vs. accuracy)Fast inference time (crucial for real-time chat)Great at capturing semantic meaningWorks well with technical contentWe stored these in ElasticSeach with cosine similarity search, which gave us:
  
  
  Edge Cases & Safety Features
ChatterMind turned out to be quite the responsible assistant. Some of our favourite edge case handles:Personal Data Protection:User: "What's John Smith's email address?"
ChatterMind: "I apologize, but I cannot share personal information such as email addresses, phone numbers, or other private details about users. This is to protect user privacy and comply with data protection regulations. If you need to contact someone, I recommend using the platform's built-in messaging system."
User: "Can you update my database permissions?"
ChatterMind: "I'm not able to directly modify system permissions or make database changes. This requires admin intervention for security reasons. Would you like me to help you create a ticket for this request?"
Handling Requests not related to Platform:User: "What is 5 + 2 + 7?"
ChatterMind: "I apologise, but performing mathematical calculations is beyond my scope as a platform support assistant. I'm designed to help with PeopleGrove platform-related questions and issues. If you need a calculator, I'd recommend using your device's built-in calculator app or a dedicated math tool."
User: "It's not working"
ChatterMind: "I notice you're on the messaging page, but I'll need more details to help you effectively. Could you please:
1. Describe what specific feature isn't working
2. What you were trying to do
3. What error message (if any) you're seeing

This will help me provide the most relevant solution."
Our implementation was heavily inspired by the excellent guide on building RAG systems with DeepSeek R1 by Asif Razzaq at MarkTechPost. While we ultimately chose Gemini 2.0 Flash for our use case, the fundamental RAG architecture outlined in the guide formed the backbone of our system.Key learnings we adapted from the guide:Document processing pipeline structureVector store implementation with FAISSEfficient retrieval mechanismsWe modified these concepts to fit our specific needs:The guide's emphasis on proper document chunking and context relevance directly influenced our implementation of the knowledge base processing:
  
  
  Real-time Communication with WebSockets
Why WebSockets? Because nobody likes waiting! We needed real-time, bi-directional communication between the frontend and our RAG system. RESTful APIs would've meant:No server-initiated messagesMore complex state managementChatbot responses getting suck "in transit"Here's how we implemented it:The WebSocket connection allows us to:Stream responses in real-timeMaintain conversation stateImplement retry mechanism
  
  
  ElasticSearch as Our Knowledge Base
While FAISS was suggested in the DeepSeek guide, we chose ElasticSearch for our vector store because:Built-in text search capabilitiesEfficient vector similarity searchHere's our ElasticSearch setup:
  
  
  Cool Features We're Proud Of
Context-Aware Responses

The bot doesn't just search - it understands where the user is in the platform and what they've been talking about. It's like having a GPS for user context!Privacy First

We trained our bot to be as tight-lipped as a secret agent about sensitive data. Try asking for someone's email - it'll give you the "I cannot share that information" speech faster than you can say "GDPR"!Beyond the Knowledge Base

The real magic happens when users ask questions that aren't in any document. By combining context with LLM capabilities, ChatterMind can reason about platform behaviour and suggest solutions, like a true admin would.
ChatterMind: "Hi Akshay, my name is ChatterMind! üëã Welcome to XYZ Support. I'm here to help with any questions you have about mentoring, connecting with alumni, or using the platform. How can I assist you today?"
User: "Why is the page not working?"
ChatterMind: "It sounds like you're having trouble accessing the admin dashboard. To help me figure out why the page isn't working, could you tell me if you have correct access? This page requires admin-level permissions to function correctly. Also, please note that this page is not available on native mobile apps, so if you are using a mobile app, that could be the reason why it's not working.

If you do have the correct permissions and are not using a mobile app, could you describe what you're seeing on the page? Any details you can provide will help me troubleshoot the issue.

Please let me know if you have any other questions."

  
  
  Hackathon Compromises (aka Technical Debt)
Let's be real - in 24 hours, we had to cut some corners:Redis for chat history

Would use a proper database for productionNo backup/persistence configurationSecurity shortcuts

Basic token authenticationPerformance trade-offs

Single-node ElasticSearchIf we had more time (and less caffeine), we'd love to add:Proper infrastructure

Distributed chat history storageCool features

Voice interface (for those too tired to type)Multi-language support (because admins speak many languages)Predictive issue detection (stop problems before they happen!)This project wouldn't have been possible (let alone won the hackathon! üèÜ üéâ) without my amazing team's diverse skills and relentless energy. Special thanks to:Our AI/ML engineer Suyash who became our guide at times and pointed us in right direction. üß†Our Engineers Rajat & Rahul who made WebSockets work like magic, and did the end-to-end integration on my core idea. ‚ö°Our Engineers Manoj & Samrood who integrated ElasticSearch & became frontend ninjas who created a sleek chat-interface. üé®And countless Chai & Coffee that fuelled our coding marathons. ü•§Winning the hackathon was the cherry on top of an incredible 24-hour journey. It proved that when passionate developers come together with a clear mission (and enough caffeine), we can create something truly impactful.Our hackathon project proved that with modern LLMs, good prompt engineering, and a solid RAG system, you can build a surprisingly capable admin support chatbot in just 24 hours.While not production-ready, it showed the potential for AI to transform admin support from a repetitive task to an intelligent service.No admins were harmed in the making of this chatbot, though several cups of Chai and Coffee were consumed! üöÄ ‚òï üéâ]]></content:encoded></item><item><title>Python for Web Developers: A Fast-Paced Guide to the Language</title><link>https://dev.to/austinwdigital/python-for-web-developers-a-fast-paced-guide-to-the-language-38f7</link><author>Austin W</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 06:24:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[üëã  Follow me on GitHub for new projects.Python is a powerful, high-level programming language widely used in web development, automation, data science, and scripting. If you're already a  familiar with JavaScript, TypeScript, Node.js, and frameworks like React or Next.js, learning Python can open doors to backend development with Django, Flask, and FastAPI, as well as automation, data analysis, and AI.This guide is a  of Python, focusing on concepts that web developers need to know. If you‚Äôre comfortable with JavaScript, you‚Äôll find Python‚Äôs syntax clean and easy to pick up.1. Python Syntax & BasicsHello World (No Semicolons, No Braces)‚úî No semicolons ().‚Äîuses indentation., just .  Variables & Dynamic Typing‚úî No need to declare , , or .
‚úî Types are inferred dynamically.  
  
  
  How Does const Work in Python?
Python does not have const like JavaScript, but you can define constants by using all-uppercase variable names as a convention.However, this does not enforce immutability. If you need true immutability, use a dataclass or a frozen set.Data Types (Compared to JavaScript)let obj = {key: "value"};2. Control Flow (Loops & Conditionals)‚úî No parentheses  needed for conditions. instead of .  ‚úî  loops iterate directly over lists/arrays. loops work like JavaScript.  3. Functions & Lambda Expressions‚úî  replaces ., just indentation.  Lambda (Arrow Function Equivalent)‚úî Equivalent to JavaScript‚Äôs arrow function:4. Python Collections (Lists, Dicts, Sets)let obj = { key: "value" };obj = {"key": "value"}  # Dictionaryconst unique = new Set([1, 2, 3]);unique = {1, 2, 3}  # SetDictionaries (Like Objects)5. Object-Oriented Programming (OOP) in Python‚úî  is the  (like  in JS). is like .  6. Python for Web DevelopmentDjango (Full-Stack Framework)pip django
django-admin startproject myproject
‚úî  is a batteries-included backend framework.
‚úî Built-in ORM, authentication, and templating.  Flask (Lightweight API Framework)‚úî  is minimal and great for APIs.  FastAPI (High-Performance API)‚úî  is async-native and perfect for microservices.  SQLite Example (Django & Flask Compatible)‚úî  is built-in, no installation needed.  8. Asynchronous Programming in PythonAsync/Await (Similar to JavaScript)‚úî Uses / like JavaScript. is the  equivalent of Node.js.  9. Python Package ManagementCreate Virtual Environment‚úî  for package management. () isolate dependencies.   10. Best Practices for Python DevelopmentWriting clean, efficient, and maintainable Python code is essential for long-term scalability. Here are the key best practices that every Python developer should follow:Follow PEP 8 (Python Style Guide)Python has an official style guide called , which provides conventions for writing Python code.4 spaces per indentation level (not tabs)..meaningful variable and function names. for variable and function names, and  for class names.Python‚Äôs  isolate dependencies for different projects, preventing conflicts.Creating a Virtual EnvironmentActivating the Virtual EnvironmentDeactivating the Virtual EnvironmentUse Type Hinting for Readable CodePython is dynamically typed, but you can use  to improve code clarity.‚úî This makes the code . catch type errors.Write Readable DocstringsAlways document your functions and classes using  ().‚úî Use triple quotes for multi-line docstrings.parameters, return values, and purpose.  Python uses  for  and triple quotes () for .‚úî Use comments only where necessary‚Äîgood code should be self-explanatory.Docstrings are not comments‚Äîthey are for documentation and can be accessed with .  12. Common Python Imports for Web DevelopmentHere are some of the most common Python imports used in web development:13. Setting Up a  FileA  file is similar to  in Node.js‚Äîit lists dependencies for a Python project.Creating a  Filepip freeze  requirements.txt
Installing Dependencies from pip  requirements.txt
‚úî This ensures that all team members and deployment environments have the .  14. Writing & Running Tests in PythonPython has built-in testing with , but  is another option - one that aims for simplicity.‚úî Use  to check expected results.‚úî ‚Äîjust use . test files named .  15. Fetching Data with API Calls in PythonPython uses  to fetch data, similar to  in JavaScript.‚úî  is like  in JavaScript. works the same way in both languages.Sending Data (POST Request)‚úî Use  instead of  to send JSON.Logging is essential for debugging and monitoring applications.‚úî Works like  but supports different log levels.  ‚úî Saves logs for later analysis.17. Raising & Handling Errors in Python LoggingWhen an error occurs, Python lets you  or .‚úî Use  to manually trigger an error.  Instead of crashing, log errors with a traceback:‚úî  logs the full error traceback.Python is a useful language for web developers, expanding your stack beyond JavaScript. Whether you‚Äôre building APIs with FastAPI, full-stack apps with Django, or automating tasks, Python makes it .üöÄ  Try building a small Flask or FastAPI project today!Python #WebDev #Django #Flask #FastAPI  A fast-paced guide to Python for web developers! Learn how to use Python for full-stack development, APIs, databases, async programming, and more. üöÄ  
  
  
  TLDR ‚Äì Highlights for Skimmers
Python syntax is simpler than JavaScript‚Äîno semicolons, indentation replaces {}.const does not exist in Python; uppercase variables are used for constants.Lists ([]) are like arrays, but dictionaries ({}) are not JavaScript objects.Classes & objects are similar, but Python uses self instead of this.Python async/await requires asyncio, unlike JavaScript‚Äôs built-in event loop.Django, Flask, and FastAPI are top backend frameworks for Python web dev.üí¨ Do you use Python in web dev? Share your experience in the comments! ]]></content:encoded></item><item><title>Code Optimization Strategies for Game Development üî•</title><link>https://dev.to/codewithshahan/code-optimization-strategies-for-game-development-2n0e</link><author>Programming with Shahan</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 05:46:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Game development is a battlefield. Either you optimize, or you lose. Period.I don‚Äôt care if you‚Äôre an experienced developer with 10 years of experience or 1 year of experience. If you want to make games that WORK, games people respect‚Äîyou need to understand . Players demand smooth gameplay, high-quality visuals, and a flawless experience across every device. If your game stutters, crashes, or loads slower than a snail? You‚Äôre done. Optimization isn‚Äôt magic. It‚Äôs the foundation of smooth gameplay, fast loading, and stable performance. Without it, your game will lag, crash, and be forgotten faster than you can say ‚Äúgame over.‚Äù  But don‚Äôt worry. In this article, I will share four effective strategies to help you with that. 
  
  
  Effective Strategies for Performance Optimization
ü§∏‚Äç‚ôÇÔ∏è What Is Optimization? Optimization means making your game run as fast and smooth as possible. SIMPLE.When you optimize your game, you:  üñ•Ô∏è Make the game work on weaker computers or phones.üíâ Prevent lag and crashes.Rule 1: Memory ManagementWhen you‚Äôre developing a game, memory is your most valuable resource.Every player movement, every enemy on the screen, every explosion needs a little piece of memory to function. Unfortunately, If you don‚Äôt manage memory properly, your game can get slow, laggy, or even crash. That‚Äôs why memory management is a critical skill every game developer needs. Let‚Äôs break it down step by step, with detailed examples in Python.Strategy #1: Memory PoolingThis strategy is simple: reuse Objects Instead of Creating New Ones** Memory pooling is like recycling for your game. Instead of creating new objects every time you need one, you reuse objects you‚Äôve already created.  Creating and destroying objects repeatedly takes up time and memory. Let's say you are building a shooting game where the player fires 10 bullets per second. If you create a new bullet for each shot, your game could quickly slow down.  Here‚Äôs how you can implement memory pooling for bullets in a shooting game:The  Class: Defines what a bullet does and keeps track of whether it‚Äôs active (in use) or not.
The : A list of 10 reusable bullets.
The  Function: Finds an inactive bullet, reuses it, and sets its position.
Recycling Bullets: When you‚Äôre done with a bullet, you reset it so it can be reused.
Strategy #2. Data Structure OptimizationThe way you store your data can make or break your game‚Äôs performance. Choosing the wrong data structure is like trying to carry water in a leaky bucket‚Äîit‚Äôs inefficient and messy.  Let‚Äôs say you‚Äôre making a game for four players, and you want to keep track of their scores. You could use a list, but a fixed-size array is more efficient because it uses less memory. Creates a fixed-size array of integers ().
 You can‚Äôt accidentally add or remove elements, which prevents bugs and saves memory.
 Updating scores is quick and uses minimal resources.
Strategy #3. Memory ProfilingEven if your code seems perfect, hidden memory problems can still exist. Memory profiling helps you monitor how much memory your game is using and find issues like memory leaks.  Python has a built-in tool called  that tracks memory usage. Here‚Äôs how to use it: begins monitoring memory usage.
 Create a large list to use up memory.
 Get the current and peak memory usage, converting it to megabytes for readability.
 ends the tracking session.
Now it‚Äôs your turn to practice these strategies and take your game development skills to the next level!Rule 2: Asset Streaming (Load Only What You Need)If you load the entire world at once, your game will choke and die. You don‚Äôt need that drama. Instead, stream assets as the player needs them. This is called asset streaming. For instance, inside your game, you may have a huge open-world with forests, deserts, and cities. Why load all those levels at once when the player is only in the forest? Makes no sense, right? Load  and keep your game lean, fast, and smooth.Strategy #1: Segment and PrioritizeLet‚Äôs break this down with an example. Your player is exploring different levels: Forest, Desert, and City. We‚Äôll only load a level when the player enters it.Here‚Äôs how to make it work in Python: Each level has a name (e.g., Forest) and a ‚Äúloaded‚Äù status. If it‚Äôs loaded, it doesn‚Äôt load again.
 The  function finds the level the player wants to enter and loads it only if it hasn‚Äôt been loaded yet.
 Levels not visited don‚Äôt waste memory. The game runs smoothly because it only focuses on what the player needs.This is efficiency at its finest. No wasted memory, no wasted time. Your player moves; your game adapts. That‚Äôs how you dominate.  Strategy #2: Asynchronous Loading (No Waiting Allowed)Nobody likes waiting. Freezing screens? Laggy loading? It‚Äôs amateur hour. You need ‚Äîthis loads assets in the background while your player keeps playing.  Imagine downloading a huge map while still exploring the current one. Your game keeps moving, the player stays happy.Here‚Äôs how to simulate asynchronous loading in Python: The  module creates a new thread to load assets without freezing the main game.
 The  function fakes the loading time to mimic how it works in a real game.
 The player can continue playing while the new level or asset loads in the background.
With asynchronous loading, your player stays in the zone, and your game feels seamless. Pro-level stuff.Strategy 3: Level of Detail (LOD) Systems ‚Äì Be Smart About QualityNot everything in your game needs to look like it‚Äôs been rendered by a Hollywood studio. If an object is far away, lower its quality. It‚Äôs called , and it‚Äôs how you keep your game‚Äôs performance sharp.  Example: Using LOD for a TreeHere‚Äôs a Python simulation of switching between high and low detail: The  property determines how far the tree is from the player.
 If the tree is close, render it in high detail. If it‚Äôs far, use low detail to save memory and processing power.
 The player doesn‚Äôt notice the difference, but your game runs smoother and faster.
This is how you keep the balance between beauty and performance. Your game looks stunning up close but doesn‚Äôt waste resources on faraway objects. Only load what you need, when you need it. No wasted memory.
 Smooth gameplay keeps players engaged and avoids frustration.
 These techniques are how AAA games stay fast and responsive.
 Go apply these strategies, keep your game lean, and make sure your players never even think about lag.Rule 3: Frame Rate StabilizationThe frame rate is how many pictures (frames) your game shows per second. If it‚Äôs unstable, your game will stutter and feel broken.  The secret? Keep the workload for each frame consistent.  üö¶Here‚Äôs how you can control the timing in a game loop:‚öñÔ∏è The game updates at a steady rate (60 times per second).
ü™Ç This make smooth gameplay, no matter how slow or fast the computer is.
Optimize Rendering Paths: Fewer draw calls. Smarter culling. Simplicity wins.Dynamic Resolution Scaling: When the pressure‚Äôs on, scale down resolution to maintain the frame rate. Players won‚Äôt even notice.Fixed Time Step: Keep your physics and logic consistent. Frame rate fluctuations shouldn‚Äôt mean chaos.Rule 4: GPU and CPU OptimizationYour computer has two main processors:   Handles logic, like moving a character or calculating scores.
 Handles graphics, like drawing your game world.
üëá Here's what you have to do for GPU/CPU optimization:Profile Everything: Use tools to pinpoint bottlenecks and strike hard where it hurts.
Shader Optimization: Shaders are resource hogs. Simplify them, streamline them, and cut the fat.
Multithreading: Spread tasks across CPU cores. Don‚Äôt overload one and leave the others idle.If one is working too hard while the other is idle, your game will lag.  Solution? Multithreading.
Let‚Äôs split tasks between two threads:üé∞ One thread handles logic.
üõ£Ô∏è Another thread handles graphics.
‚öñÔ∏è This balances the workload and prevents bottlenecks.
Optimization isn‚Äôt just for ‚Äúsmart‚Äù people. It‚Äôs simple if you take it step by step:  Manage memory like a pro. Don‚Äôt waste it.
 Load only what you need.
Keep the frame rate stable. No stuttering.
 Use the CPU and GPU wisely.
Start optimizing NOW. Your future self will thank you.  ]]></content:encoded></item><item><title>Automating Daily arXiv Paper Summaries with Slack Notifications</title><link>https://dev.to/m_sea_bass/automating-daily-arxiv-paper-summaries-with-slack-notifications-1kp8</link><author>M Sea Bass</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 05:26:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This post is a follow-up to the previous article. It turns out there‚Äôs a slight delay before the latest papers show up in the arXiv API. Because of this delay, the same paper can sometimes appear the next day.To fix this, we‚Äôre going to record the timestamp of the last retrieved paper and then only fetch new papers each day.We‚Äôll store the timestamp of the latest paper in Amazon S3 so we can both update and retrieve it later. For this, you‚Äôll need to install . In the  folder we created previously, run:Next, zip the folder again and upload it as a new version of your Lambda layer:zip  ./upload.zip ./python/Then, update your Lambda function to use this new layer version.You‚Äôll also need an S3 bucket ready in advance. In this example, we simply created one with the default settings.Below is the fully revised code in English, including the new functions to update and retrieve the timestamp from S3. Note that we set  as an environment variable.By saving the timestamp in S3, your script won‚Äôt process the same paper entries each day, and if no new papers appear, the script will skip generating summaries. This helps reduce unnecessary API usage and costs.]]></content:encoded></item><item><title>GaussianBlur in PyTorch (2)</title><link>https://dev.to/hyperkai/gaussianblur-in-pytorch-2-1bj2</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 04:47:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GaussianBlur in PyTorch (1)</title><link>https://dev.to/hyperkai/gaussianblur-in-pytorch-1-3ndn</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 04:35:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Type: or /()):
*Memos:

A tuple/list must be the 1D with 1 or 2 elements.A single value( or ()) means [kernel_size, kernel_size].The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It's  so it must be .A tuple/list must be the 1D with 1 or 2 elements.A single value(,  or ( or )) means .The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>Weekly Challenge: Counting the XOR</title><link>https://dev.to/simongreennet/weekly-challenge-counting-the-xor-4hhc</link><author>Simon Green</author><category>dev</category><category>python</category><pubDate>Sun, 16 Feb 2025 04:23:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Each week Mohammad S. Anwar sends out The Weekly Challenge, a chance for all of us to come up with solutions to two weekly tasks. My solutions are written in Python first, and then converted to Perl. It's a great way for us all to practice some coding.You are given two array of strings,  and .Write a script to return the count of common strings in both arrays.The tasks and examples don't mention what to do if a string appears more than once in both arrays. I've made the assumption that we only need to return it once.For the command line input, I take two strings that are space separated as shown in the example.In Python this is a one liner. I turn the lists into sets (which only has unique values) and take the length of the intersection of these two sets.Perl does not have sets or intersections built in. For the Perl solution, I turn both strings into a hash with the key being the strings. I then iterate through the keys of the first hash to see if they appear in the second hash. If they do, I increment the  variable../ch-1.py 
2

./ch-1.py 
1

./ch-1.py 
0
You are given an encoded array and an initial integer.Write a script to find the original array that produced the given encoded array. It was encoded such that encoded[i] = orig[i] XOR orig[i + 1].This is relatively straight forward. For the command line input, I take the last value as the  integer, and the rest as the  integers.For this task, I create the  list (array in Perl) with the  value. I then iterate over each item in the  list and takes the exclusive-or of it and the last value in the  list../ch-2.py 1 2 3 1
1, 0, 2, 1]

./ch-2.py 6 2 7 3 4
4, 2, 0, 7, 4]
]]></content:encoded></item><item><title>Kay Hayen: Nuitka this week #16</title><link>https://nuitka.net/posts/nuitka-this-week-16.html</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Hey Nuitka users! This started out as an idea of a weekly update, but
that hasn‚Äôt happened, and so we will switch it over to just writing up
when something interesting happens and then push it out relatively
immediately when it happens.Nuitka Onefile Gets More Flexible:  and We‚Äôve got a couple of exciting updates to Nuitka‚Äôs onefile mode that
give you more control and flexibility in how you deploy your
applications. These enhancements stem from real-world needs and
demonstrate Nuitka‚Äôs commitment to providing powerful and adaptable
solutions.Taking Control of Onefile Unpacking: Onefile mode is fantastic for creating single-file executables, but the
management of the unpacking directory where the application expands has
sometimes been a bit‚Ä¶ opaque. Previously, Nuitka would decide whether
to clean up this directory based on whether the path used
runtime-dependent variables. This made sense in theory, but in practice,
it could lead to unexpected behavior and made debugging onefile issues
harder.Now, you have complete control! The new  option
lets you explicitly specify what happens to the unpacking directory:: This is the default behavior. Nuitka
will remove the unpacking directory unless runtime-dependent values
were used in the path specification. This is the same behavior as
previous versions.: The unpacking directory is 
removed and becomes a persistent, cached directory. This is useful
for debugging, inspecting the unpacked files, or if you have a use
case that benefits from persistent caching of the unpacked data. The
files will remain available for subsequent runs.: The unpacking directory 
removed after the program exits.This gives you the power to choose the behavior that best suits your
needs. No more guessing!Relative Paths with Another common request, particularly from users deploying applications
in more restricted environments, was the ability to specify the onefile
unpacking directory  to the executable itself. Previously, you
were limited to absolute paths or paths relative to the user‚Äôs temporary
directory space.We‚Äôve introduced a new variable, , that you can use in
the  option. This variable is dynamically
replaced at runtime with the full path to the directory containing the
onefile executable.This would create a directory named  the same
directory as the  (or  on Linux/macOS)
and unpack the application there. This is perfect for creating truly
self-contained applications where all data and temporary files reside
alongside the executable.Nuitka Commercial and Open SourceThese features, like many enhancements to Nuitka, originated from a
request by a Nuitka commercial customer. This highlights the close
relationship between the commercial offerings and the open-source core.
While commercial support helps drive development and ensures the
long-term sustainability of Nuitka, the vast majority of features are
made freely available to all users.This change will be in 2.7 and is currentlyWe encourage you to try out these new features and let us know what you
think! As always, bug reports, feature requests, and contributions are
welcome on GitHub.]]></content:encoded></item><item><title>Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!</title><link>https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 22:12:10 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[It will be streamed on the following YouTube Live channels:This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.The Async Django ORM: Where Is it?Speed at Scale for Django Web ApplicationsImplementing Agentic AI Solutions in Django from scratchDiving into DSF governance: past, present and futureGetting Knowledge from Django Hits: Using Grafana and PrometheusCulture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open Source¬µDjango. The next step in the evolution of asynchronous microservices technology.A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.]]></content:encoded></item><item><title>Eli Bendersky: Decorator JITs - Python as a DSL</title><link>https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 21:49:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the "JIT decorator" pattern is pretty popular. For
example, this JAX snippet:In both cases, the function decorated with  doesn't get executed by the
Python interpreter in the normal sense. Instead, the code inside is more like
a DSL (Domain Specific Language) processed by a special purpose compiler built
into the library (JAX or Triton). Another way to think about it is that Python
is used as a  to describe computations.In this post I will describe some implementation strategies used by libraries to
make this possible.Preface - where we're goingThe goal is to explain how different kinds of  decorators work by using
a simplified, educational example that implements several approaches from
scratch. All the approaches featured in this post will be using this flow: Expr IR --> LLVM IR --> Execution" /> Expr IR --> LLVM IR --> Execution" class="align-center" src="https://eli.thegreenplace.net/images/2025/decjit-python.png" />
These are the steps that happen when a Python function wrapped with
our educational  decorator is called:The function is translated to an "expression IR" - .This expression IR is converted to LLVM IR.Finally, the LLVM IR is JIT-executed.First, let's look at the  IR. Here we'll make a big simplification -
only supporting functions that define a single expression, e.g.:Naturally, this can be easily generalized - after all, LLVM IR can be used to
express fully general computations.Here are the  data structures:To convert an  into LLVM IR and JIT-execute it, we'll use this function:It uses the  class to actually generate LLVM IR from .
This process is straightforward and covered extensively in the resources I
linked to earlier; take a look at the full code here.My goal with this architecture is to make things simple, but .
On one hand - there are several simplifications: only single expressions are
supported, very limited set of operators, etc. It's very easy to extend this!
On the other hand, we could have just trivially evaluated the 
without resorting to LLVM IR; I do want to show a more complete compilation
pipeline, though, to demonstrate that an arbitrary amount of complexity can
be hidden behind these simple interfaces.With these building blocks in hand, we can review the strategies used by
 decorators to convert Python functions into s.Python comes with powerful code reflection and introspection capabilities out
of the box. Here's the  decorator:This is a standard Python decorator. It takes a function and returns another
function that will be used in its place ( ensures that
function attributes like the name and docstring of the wrapper match the
wrapped function).After  is applied to , what  holds is the
wrapper. When  is called, the wrapper is invoked with
.The wrapper obtains the AST of the wrapped function, and then uses
 to convert this AST into an :When  finishes visiting the AST it's given, its
 field will contain the  representing the function's
return value. The wrapper then invokes  with this .Note how our decorator interjects into the regular Python execution process.
When  is called, instead of the standard Python compilation and
execution process (code is compiled into bytecode, which is then executed
by the VM), we translate its code to our own representation and emit LLVM from
it, and then JIT execute the LLVM IR. While it seems kinda pointless in this
artificial example, in reality this means we can execute the function's code
in any way we like.AST JIT case study: TritonThis approach is almost exactly how the Triton language works. The body of a
function decorated with  gets parsed to a Python AST, which then
- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered
to PTX by the
NVPTX LLVM backend.
Then, the code runs on a GPU using a standard CUDA pipeline.Naturally, the subset of Python that can be compiled down to a GPU is limited;
but it's sufficient to run performant kernels, in a language that's much
friendlier than CUDA and - more importantly - lives in the same file with the
"host" part written in regular Python. For example, if you want testing and
debugging, you can run Triton in "interpreter mode" which will just run the
same kernels locally on a CPU.Note that Triton lets us import names from the  package
and use them inside kernels; these serve as the  for the language
- special calls the compiler handles directly.Python is a fairly complicated language with  of features. Therefore,
if our JIT has to support some large portion of Python semantics, it may make
sense to leverage more of Python's own compiler. Concretely, we can have it
compile the wrapped function all the way to bytecode,
and start our translation from there.Here's the  decorator that does just this :The Python VM is a stack machine; so we emulate a stack to convert the
function's bytecode to  IR (a bit like an RPN evaluator).
As before, we then use our  utility function to lower
 to LLVM IR and JIT execute it.Using this JIT is as simple as the previous one - just swap 
for :Bytecode JIT case study: NumbaNumba is a compiler for Python itself. The idea
is that you can speed up specific functions in your code by slapping a
 decorator on them. What happens next is similar in spirit to
our simple , but of course much more complicated because it
supports a very large portion of Python semantics.Numba uses the Python compiler to emit bytecode, just as we did; it then
converts it into its own IR, and then to LLVM using .By starting with the bytecode, Numba makes its life easier (no need to rewrite
the entire Python compiler). On the other hand, it also makes some analyses
, because by the time we're in bytecode, a lot of semantic information
existing in higher-level representations is lost. For example, Numba has to
sweat a bit to recover control flow information from the bytecode (by
running it through a special interpreter first).The two approaches we've seen so far are similar in many ways - both rely on
Python's introspection capabilities to compile the source code of the JIT-ed
function to some extent (one to AST, the other all the way to bytecode), and
then work on this lowered representation.The tracing strategy is very different. It doesn't analyze the source code of
the wrapped function at all - instead, it  its execution by means of
specially-boxed arguments, leveraging overloaded operators and functions, and
then works on the generated trace.The code implementing this for our smile demo is surprisingly compact:Each runtime argument of the wrapped function is assigned a , and
that is placed in a , a placeholder class which lets us
do operator overloading:The remaining key function is :To understand how this works, consider this trivial example:After the decorated function is defined,  holds the wrapper function
defined inside . When  is called, the wrapper runs:For each argument of  itself (that is  and ), it creates
a new  holding a . This denotes a named variable in
the  IR.It then calls the wrapped function, passing it the boxes as runtime
parameters.When (the wrapped)  runs, it invokes . This is caught by the overloaded
 operator of , and it creates a new  with
the s representing  and  as children. This
 is then returned .The wrapper unboxes the returned  and passes it to
 to emit LLVM IR from it and JIT execute it with the
actual runtime arguments of the call: .This might be a little mind-bending at first, because there are two different
executions that happen:The first is calling the wrapped  function itself, letting the Python
interpreter run it as usual, but with special arguments that build up the IR
instead of doing any computations. This is the .The second is lowering this IR our tracing step built into LLVM IR and then
JIT executing it with the actual runtime argument values ; this is
the .This tracing approach has some interesting characteristics. Since we don't
have to analyze the source of the wrapped functions but only trace through
the execution, we can "magically" support a much richer set of programs, e.g.:This  with our basic . Since Python variables are
placeholders (references) for values, our tracing step is oblivious to them - it
follows the flow of values. Another example:This also just works! The created  will be a long chain of 
additions of 's runtime values through the loop, added to the 
for .This last example also leads us to a limitation of the tracing approach; the
loop cannot be  - it cannot depend on the function's arguments,
because the tracing step has no concept of runtime values and wouldn't know
how many iterations to run through; or at least, it doesn't know this unless
we want to perform the tracing run for every runtime execution .Tracing JIT case study: JAXThe JAX ML framework uses a tracing
approach very similar to the one described here. The first code sample in this
post shows the JAX notation. JAX cleverly wraps Numpy with its own version which
is traced (similar to our , but JAX calls these boxes "tracers"),
letting you write regular-feeling Numpy code that can be JIT optimized and
executed on accelerators like GPUs and TPUs via XLA. JAX's tracer builds up an underlying IR (called
jaxpr) which can then be
emitted to XLA ops and passed to XLA for further lowering and execution.For a fairly deep overview of how JAX works, I recommend reading the
autodidax doc.As mentioned earlier, JAX has some limitations
with things like data-dependent control flow in native Python. This won't work,
because there's control flow
that depends on a runtime value ():When  is executed, JAX will throw an exception, saying something
like:
This concrete value was not available in Python because it depends on the
value of the argument count.As a remedy, JAX has its
own built-in intrinsics from the jax.lax package.
Here's the example rewritten in a way that actually works: (and many other built-ins in the  package) is something JAX
can trace through, generating a corresponding XLA operation (XLA has support for
While loops, to which this
 can be lowered).The tracing approach has clear benefits for JAX as well; because it only cares
about the flow of values, it can handle arbitrarily complicated Python code,
as long as the flow of values can be traced. Just like the local variables and
data-independent loops shown earlier, but also things like closures. This makes
meta-programming and templating easy .The full code for this post is available on GitHub.]]></content:encoded></item><item><title>Hugo van Kemenade: Improving licence metadata</title><link>https://hugovk.dev/blog/2025/improving-licence-metadata/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 15:11:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[PEP 639 defines a spec on how to document licences
used in Python projects.Change  as follows.I usually use Hatchling as a build backend, and support was added in 1.27:Replace the freeform  field with a valid SPDX license expression, and add
 which points to the licence files in the repo. There‚Äôs often only one,
but if you have more than one, list them all:Optionally delete the deprecated licence classifier:Then make sure to use a PyPI uploader that supports this.pip can also show you the metadata:A lot of work went into this. Thank you to PEP authors
Philippe Ombredanne for creating the first draft in
2019, to C.A.M. Gerlach for the second draft in 2021,
and especially to Karolina Surma for getting the third
draft finish line and helping with the implementation.And many projects were updated to support this, thanks to the maintainers and
contributors of at least:]]></content:encoded></item><item><title>The Real Python Podcast ‚Äì Episode #239: Behavior-Driven vs Test-Driven Development &amp; Using Regex in Python</title><link>https://realpython.com/podcasts/rpp/239/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>Daniel Roy Greenfeld: Building a playing card deck</title><link>https://daniel.feldroy.com/posts/2025-02-deck-of-cards</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 09:50:04 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.]]></content:encoded></item><item><title>Bojan Mihelac: Prefixed Parameters for Django querystring tag</title><link>http://code.informatikamihelac.com/en/query-string-with-prefixed-parameters/</link><author></author><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 21:37:18 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[An overview of Django 5.1's new querystring tag and how to add support for prefixed parameters.]]></content:encoded></item><item><title>Peter Bengtsson: get in JavaScript is the same as property in Python</title><link>http://www.peterbe.com/plog/get-in-javascript-is-the-same-as-property-in-python</link><author></author><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 12:41:56 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Prefix a function, in an object or class, with `get` and then that acts as a function call without brackets. Just like Python's `property` decorator.]]></content:encoded></item><item><title>EuroPython: EuroPython February 2025 Newsletter</title><link>https://blog.europython.eu/europython-february-2025-newsletter/</link><author></author><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 08:36:11 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Hope you&aposre all having a fantastic February. We sure have been busy and got some exciting updates for you as we gear up for EuroPython 2025, which is taking place once again in the beautiful city of Prague. So let&aposs dive right in!EuroPython 2025 is right around the corner and our programme team is hard at work putting together an amazing lineup. But we need your help to shape the conference! We received over 572 fantastic proposals, and now it‚Äôs time for Community Voting! üéâ If you&aposve attended EuroPython before or submitted a proposal this year, you‚Äôre eligible to vote.üì¢ More votes = a stronger, more diverse programme! Spread the word and get your EuroPython friends to cast their votes too.üèÉThe deadline is , so don‚Äôt miss your chance!Want to play a key role in building an incredible conference? Join our review team and help select the best talks for EuroPython 2025! Whether you&aposre a Python expert or an enthusiastic community member, your insights matter.We‚Äôd like to also thank the over 100 people who have already signed up to review! For those who haven‚Äôt done so yet, please remember to accept your Pretalx link and get your reviews in by You can already start reviewing proposals, and each review takes as little as 5 minutes. We encourage reviewers to go through at least 20-30 proposals, but if you can do more, even better! With almost 600 submissions to pick from, your help ensures we curate a diverse and engaging programme.üèÉThe deadline is Monday next week, so don‚Äôt delay!EuroPython isn‚Äôt just present at other Python events‚Äîwe actively support them too! As a community sponsor, we love helping local PyCons grow and thrive. We love giving back to the community and strengthening Python events across Europe! üêçüíôThe EuroPython team had a fantastic time at PyCon + Web in Berlin, meeting fellow Pythonistas, exchanging ideas, and spreading the word about EuroPython 2025. It was great to connect with speakers, organizers, and attendees.¬†Ever wondered how long it takes to walk from Berlin to Prague? A huge thank you to our co-organizers, Cheuk, Artur, and Cristi√°n, for answering that in their fantastic lightning talk about EuroPython!We had some members of the EuroPython team at FOSDEM 2025, connecting with the open-source community and spreading the Python love! üéâ We enjoyed meeting fellow enthusiasts, sharing insights about the EuroPython Society, and giving away the first EuroPython 2025 stickers. If you stopped by‚Äîthank you and we hope to see you in Prague this July.ü¶í Speaker Mentorship ProgrammeThe signups for The Speaker Mentorship Programme closed on 22nd January 2025. We‚Äôre excited to have matched 43 mentees with 24 mentors from our community. We had an increase in the number of mentees who signed up and that‚Äôs amazing! We‚Äôre glad to be contributing to the journey of new speakers in the Python community. A massive thank you to our mentors for supporting the mentees and to our mentees; we‚Äôre proud of you for taking this step in your journey as a speaker.¬†26 mentees submitted at least 1 proposal. Out of this number, 13 mentees submitted 1 proposal, 9 mentees submitted 2 proposals, 2 mentees submitted 3 proposals, 1 mentee submitted 4 proposals and lastly, 1 mentee submitted 5 proposals. We wish our mentees the best of luck. We look forward to the acceptance of their proposals.In a few weeks, we will host an online panel session with 2‚Äì3 experienced community members who will share their advice with first-time speakers. At the end of the panel, there will be a Q&A session to answer all the participants‚Äô questions.You can watch the recording of the previous year‚Äôs workshop here:EuroPython is one of the largest Python conferences in Europe, and it wouldn‚Äôt be possible without our sponsors. We are so grateful for the companies who have already expressed interest. If you‚Äôre interested in sponsoring EuroPython 2025 as well, please reach out to us at sponsoring@europython.eu.üé§ EuroPython Speakers Share Their ExperiencesWe asked our past speakers to share their experiences speaking at EuroPython. These videos have been published on YouTube as shorts, and we&aposve compiled them into brief clips for you to watch.A big thanks goes to Sebastian Witowski, Jan Smitka, Yuliia Barabash, Jodie Burchell, Max Kahan, and Cheuk Ting Ho for sharing their experiences.Why You Should Submit a Proposal for EuroPython? Part 2Why You Should Submit a Proposal for EuroPython? Part 3üìä EuroPython Society Board Report¬†The EuroPython conference wouldn‚Äôt be what it is without the incredible volunteers who make it all happen. üíû Behind the scenes, there‚Äôs also the EuroPython Society‚Äîa volunteer-led non-profit that manages the fiscal and legal aspects of running the conference, oversees its organization, and works on a few smaller projects like the grants programme. To keep everyone in the loop and promote transparency, the Board is sharing regular updates on what we‚Äôre working on.That&aposs all for now! Keep an eye on your inbox and our website for more news and announcements. We&aposre counting down the days until we can come together in Prague to celebrate our shared love for Python. üêç‚ù§Ô∏èCheers,The EuroPython Team]]></content:encoded></item><item><title>Giampaolo Rodola: psutil: drop Python 2.7 support</title><link>https://gmpy.dev/blog/2025/psutil-drop-python-27-support</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[About dropping Python 2.7 support in psutil, 3 years ago
I stated:Not a chance, for many years to come. [Python 2.7] currently represents 7-10%
of total downloads, meaning around 70k / 100k downloads per day.Only 3 years later, and to my surprise, downloads for Python 2.7 dropped to
0.36%! As such, as of psutil 7.0.0, I finally decided to drop support for
Python 2.7!These are downloads per month:According to pypistats.org Python 2.7 downloads
represents the 0.28% of the total, around 15.000 downloads per day.Maintaining 2.7 support in psutil had become increasingly difficult, but still
possible. E.g. I could still run tests by using old PYPI
backports.
GitHub Actions could still be
tweaked
to run tests and produce 2.7 wheels on Linux and macOS. Not on Windows though,
for which I had to use a separate service (Appveyor). Still, the amount of
hacks in psutil source code necessary to support Python 2.7 piled up over the
years, and became quite big. Some disadvantages that come to mind:Having to maintain a Python compatibility layers like
  psutil/_compat.py.
  This translated in extra extra code and extra imports.The C compatibility layer to differentiate between Python 2 and 3 (#if
  PY_MAJOR_VERSION <= 3, etc.).Dealing with the string vs. unicode differences, both in Python and in C.Inability to use modern language features, especially f-strings.Inability to freely use s, which created a difference on how CONSTANTS
  were exposed in terms of API.Having to install a specific version of  and other (outdated)
  deps.Relying on the third-party Appveyor CI service to run tests and produce 2.7
  wheels.Running 4 extra CI jobs on every commit (Linux, macOS, Windows 32-bit,
  Windows 64-bit) making the CI slower and more subject to failures (we have
  quite a bit of flaky tests).The distribution of 7 wheels specific for Python 2.7. E.g. in the previous
  release I had to upload:psutil-6.1.1-cp27-cp27m-macosx_10_9_x86_64.whl
psutil-6.1.1-cp27-none-win32.whl
psutil-6.1.1-cp27-none-win_amd64.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_x86_64.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_x86_64.whl
The removal was done in
PR-2841, which removed around
1500 lines of code (nice!). . In doing so, in the doc I
still made the promise that the 6.1.* serie will keep supporting Python 2.7
and will receive  (no new features). It will be
maintained in a specific python2
branch. I explicitly kept
the
setup.py
script compatible with Python 2.7 in terms of syntax, so that, when the tarball
is fetched from PYPI, it will emit an informative error message on . The user trying to install psutil on Python 2.7 will see:$pip2installpsutil
Asofversion.0.0psutilnolongersupportsPython.7.
LatestversionsupportingPython.7ispsutil.1.X.
Installitwith:.
As the informative message states, users that are still on Python 2.7 can still
use psutil with:pip2 install psutil==6.1.*
]]></content:encoded></item><item><title>Kay Hayen: Nuitka Release 2.6</title><link>https://nuitka.net/posts/nuitka-release-26.html</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[ Path normalization to native Windows format was required
in more places for the  variant of .The  function doesn‚Äôt normalize to native Win32
paths with MSYS2, instead using forward slashes. This required manual
normalization in additional areas. (Fixed in 2.5.1) Fix, give a proper error when extension modules asked to
include failed to be located. instead of a proper error message.
(Fixed in 2.5.1)Fix, files with illegal module names (containing ) in their
basename were incorrectly considered as potential sub-modules for
. These are now skipped. (Fixed in 2.5.1) Improved stability by preventing crashes when stubgen
encounters code it cannot handle. Exceptions from it are now ignored.
(Fixed in 2.5.1) Addressed a crash that occurred when encountering
assignments to non-variables. (Fixed in 2.5.1) Fixed a regression introduced in 2.5 release that could
lead to segmentation faults in exception handling for generators.
(Fixed in 2.5.2) Corrected an issue where dictionary copies of large
split directories could become corrupted. This primarily affected
instance dictionaries, which are created as copies until updated,
potentially causing problems when adding new keys. (Fixed in 2.5.2) Removed the assumption that module dictionaries
always contain only strings as keys. Some modules, like
 on macOS, use non-string keys. (Fixed in 2.5.2) Ensured that the  option correctly
affects the C compilation process. Previously, only individual
disables were applied. (Fixed in 2.5.2) Fixed a crash that could occur during compilation
when unary operations were used within binary operations. (Fixed in
2.5.3) Corrected the handling of
, which could lead to crashes. (Fixed
in 2.5.4) Resolved a segmentation fault occurring at runtime
when calling  with only keyword arguments.
(Fixed in 2.5.5) Harmless warnings generated for x64 DLLs on arm64 with
newer macOS versions are now ignored. (Fixed in 2.5.5) Addressed a crash in Nuitka‚Äôs dictionary code that
occurred when copying dictionaries due to internal changes in Python
3.13. (Fixed in 2.5.6) Improved onefile mode signing by applying
 to the signature of binaries, not just
app bundles. (Fixed in 2.5.6) Corrected an issue where too many paths were added as
extra directories from the Nuitka package configuration. This
primarily affected the  package, which currently relies
on the  import hack. (Fixed in 2.5.6) Prevented crashes on macOS when creating onefile
bundles with Python 2 by handling negative CRC32 values. This issue
may have affected other versions as well. (Fixed in 2.5.6) Restored the functionality of code provided in
, which was no longer being applied due to a
regression. (Fixed in 2.5.6) Suppressed the app bundle mode recommendation when it is
already in use. (Fixed in 2.5.6) Corrected path normalization when the output directory
argument includes ‚Äú~‚Äù. GitHub Actions Python is now correctly identified as a
Homebrew Python to ensure proper DLL resolution. (Fixed in 2.5.7) Fixed a reference leak that could occur with
values sent to generator objects. Asyncgen and coroutines were not
affected. (Fixed in 2.5.7) The  scan now correctly handles
cases where both a package init file and competing Python files
exist, preventing compile-time conflicts. (Fixed in 2.5.7) Resolved an issue where handling string constants in
modules created for Python 3.12 could trigger assertions, and modules
created with 3.12.7 or newer failed to load on older Python 3.12
versions when compiled with Nuitka 2.5.5-2.5.6. (Fixed in 2.5.7) Corrected the tuple code used when calling certain
method descriptors. This issue primarily affected a Python 2
assertion, which was not impacted in practice. (Fixed in 2.5.7) Updated resource readers to accept multiple
arguments for , and correctly handle
 and  as keyword-only arguments. The platform encoding is no longer used to decode
 logs. Instead,  is used, as it is sufficient for
matching filenames across log lines and avoids potential encoding
errors. (Fixed in 2.5.7) Requests to statically link libraries for 
are now ignored, as these libraries do not exist. (Fixed in 2.5.7) Fixed a memory leak affecting the results of
functions called via specs. This primarily impacted overloaded hard
import operations. (Fixed in 2.5.7) When multiple distributions for a package are found,
the one with the most accurate file matching is now selected. This
improves handling of cases where an older version of a package (e.g.,
) is overwritten with a different variant (e.g.,
), ensuring the correct version is used for
Nuitka package configuration and reporting. (Fixed in 2.5.8) Prevented a potential crash during onefile
initialization on Python 2 by passing the directory name directly
from the onefile bootstrap, avoiding the use of  which
may not be fully loaded at that point. (Fixed in 2.5.8) Preserved necessary  environment variables on
Windows for packages that require loading DLLs from those locations.
Only  entries not pointing inside the installation prefix are
removed. (Fixed in 2.5.8) Corrected the  check to function
properly when distribution names and package names differ. (Fixed in
2.5.8) Improved package name resolution for Anaconda
distributions by checking conda metadata when file metadata is
unavailable through the usual methods. (Fixed in 2.5.8) Normalized the downloaded gcc path to use native Windows
slashes, preventing potential compilation failures. (Fixed in 2.5.9) Restored static libpython functionality on Linux by
adapting to a signature change in an unexposed API. (Fixed in 2.5.9) Prevented  from being resurrected when a
finalizer is attached, resolving memory leaks that could occur with
 in the presence of exceptions. (Fixed in 2.5.10) Suppressed the gcc download prompt that could appear during
 output on Windows systems without MSVC or with an
improperly installed gcc.Ensured compatibility with monkey patched  or 
functions, which are used in some testing scenarios. Improved the determinism of the JSON statistics
output by sorting keys, enabling reliable build comparisons. Fixed a memory leak in  with finalizers,
which could lead to significant memory consumption when using
 and encountering exceptions. Optimized empty generators (an optimization result) to
avoid generating unused context code, eliminating C compilation
warnings. Fixed a reference leak affecting the  value
in . While typically , this could lead to
observable reference leaks in certain cases. Improved handling of  and 
resurrection, preventing memory leaks with  and
, and ensuring correct execution of  code in
coroutines. Corrected the handling of  objects
resurrecting during deallocation. While not explicitly demonstrated,
this addresses potential issues similar to those encountered with
coroutines, particularly for old-style coroutines created with the
 decorator. Fixed a potential crash during runtime trace collection by
ensuring timely initialization of the output mechanism.]]></content:encoded></item><item><title>EuroPython Society: Board Report for January 2025</title><link>https://www.europython-society.org/board-report-for-january-2025/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:08:37 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[The top priority for the board in January was finishing the hiring of our event manager. We‚Äôre super excited to introduce Ane≈æka M√ºller! Ane≈æka is a freelance event manager and a longtime member of the Czech Python community. She‚Äôs a member of the Pyvec board, co-organizes PyLadies courses, PyCon CZ, Brno Pyvo, and Brno Python Pizza. She‚Äôll be working closely with the board and OPS team, mainly managing communication with service providers. Welcome onboard! Our second priority was onboarding teams. We‚Äôre happy that we already have the Programme team in place‚Äîthey started early and launched the Call for Proposals at the beginning of January. We‚Äôve onboarded a few more teams and are in the process of bringing in the rest.Our third priority was improving our grant programme in order to support more events with our limited budget and to make it more clear and transparent. We went through past data, came up with a new proposal, discussed it, voted on it, and have already published it on our blog. Updating onboarding/offboarding checklists for Volunteers and Board MembersVarious infrastructure updates including new website deployment and self-hosted previews for Pull Requests to the website.Setting up EPS AWS account.Working out the Grant Guidelines update for 2025Attending PyConWeb and FOSDEMReviewing updates to the Sponsors setup and packages for 2025More documentation, sharing know-how and reviewing new proposals.Brand strategy: Analysis of social media posts from previous years and web analytics. Call with a European open-source maintainer and a call with a local events organizer about EP content.Comms & design: Call for proposal announcements, EP 2024 video promotions, speaker mentorship, and newsletter. Video production - gathering videos from speakers, video post-production, and scheduling them on YouTube shorts, and social media.Event management coordination: Calls with the event manager and discussions about previous events.Grants: Work on new grant guidelines and related comms.Team onboarding: Calls with potential comms team members and coordination.PR: Delivering a lightning talk at FOSDEM.Offboarding the old boardOnboarding new team membersAdministrative work on GrantsWorked on the Grants proposalFollow-up with team membersCommunity outreach: FOSDEMWorking on various infrastructure updates, mostly related to the website.Reviewing Pull Requests for the website and the internal botWorking on the infrastructure team proposal.Timeline: Discussion with the Programme Team, and planning to do the same with the other teams.Visa Request letter: Setup and Test Visa Request Automation for the current yearTeam selection discussion with past volunteers]]></content:encoded></item><item><title>Python Morsels: Avoid over-commenting in Python</title><link>https://www.pythonmorsels.com/avoid-comments/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:05:39 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Documenting instead of commentingHere is a comment I would not write in my code:That comment seems to describe what this code does... so why would I  write it?I do like that comment, but I would prefer to write it as a docstring instead:Documentation strings are for conveying the purpose of function, class, or module, typically at a high level.
Unlike comments, they can be read by Python's built-in  function:Docstrings are also read by other documentation-oriented tools, like Sphinx.Non-obvious variables and valuesHere's a potentially helpful comment:]]></content:encoded></item><item><title>Python Keywords: An Introduction</title><link>https://realpython.com/python-keywords/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. Currently, Python has thirty-five keywords and four soft keywords. These keywords are always available in Python, which means you don‚Äôt need to import them. Understanding how to use them correctly is fundamental for building Python programs.By the end of this tutorial, you‚Äôll understand that:There are  and  in Python.You can get a list of all keywords using  from the  module. in Python act as keywords only in specific contexts. are keywords that have been deprecated and turned into functions in Python 3.In this article, you‚Äôll find a basic introduction to all Python keywords and soft keywords along with other resources that will be helpful for learning more about each keyword. Test your knowledge with our interactive ‚ÄúPython Keywords: An Introduction‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of Python keywords and soft keywords. These reserved words have specific functions and restrictions in Python, and understanding how to use them correctly is fundamental for building Python programs.Python keywords are special reserved words that have specific meanings and purposes and can‚Äôt be used for anything but those specific purposes. These keywords are always available‚Äîyou‚Äôll never have to import them into your code.Python keywords are different from Python‚Äôs built-in functions and types. The built-in functions and types are also always available, but they aren‚Äôt as restrictive as the keywords in their usage. An example of something you  do with Python keywords is assign something to them. If you try, then you‚Äôll get a . You won‚Äôt get a  if you try to assign something to a built-in function or type, but it still isn‚Äôt a good idea. For a more in-depth explanation of ways keywords can be misused, check out Invalid Syntax in Python: Common Reasons for SyntaxError.There are thirty-five keywords in Python. Here‚Äôs a list of them, each linked to its relevant section in this tutorial:Two keywords have additional uses beyond their initial use cases. The  keyword is also used with loops and with  and  in addition to in conditional statements. The  keyword is most commonly used in  statements, but also used with the  keyword.The list of Python keywords and soft keywords has changed over time. For example, the  and  keywords weren‚Äôt added until Python 3.7. Also, both  and  were keywords in Python 2.7 but were turned into built-in functions in Python 3 and no longer appear in the keywords list.As mentioned above, you‚Äôll get an error if you try to assign something to a Python keyword. Soft keywords, on the other hand, aren‚Äôt that strict. They syntactically act as keywords only in certain conditions.This new capability was made possible thanks to the introduction of the PEG parser in Python 3.9, which changed how the interpreter reads the source code.Leveraging the PEG parser allowed for the introduction of structural pattern matching in Python. In order to use intuitive syntax, the authors picked , , and  for the pattern matching statements. Notably,  and  are widely used for this purpose in many other programming languages.To prevent conflicts with existing Python code that already used , , and  as variable or function names, Python developers decided to introduce the concept of soft keywords.Currently, there are four  in Python:You can use the links above to jump to the soft keywords you‚Äôd like to read about, or you can continue reading for a guided tour.Value Keywords: , , There are three Python keywords that are used as values. These values are singleton values that can be used over and over again and always reference the exact same object. You‚Äôll most likely see and use these values a lot.There are a few terms used in the sections below that may be new to you. They‚Äôre defined here, and you should be aware of their meaning before proceeding:]]></content:encoded></item><item><title>EuroPython Society: Changes in the Grants Programme for 2025</title><link>https://www.europython-society.org/changes-in-the-grants-programme-for-2025/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 13:16:30 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[We are increasing transparency and reducing ambiguity in the guidelines.We would like to support more events with our limited budgetWe‚Äôve introduced caps for events in order to make sure all grants are fairly given and we can support more communities.We‚Äôve set aside 10% of our budget for the local community. The EPS introduced a Grant Programme in 2017. Since then, we have granted almost EUR 350k through the programme, partly via EuroPython Finaid and by directly supporting other Python events and projects across Europe. In the last two years, the Grant Programme has grown to EUR 100k per year, with even more requests coming in.With this growth come new challenges in how to distribute funds fairly so that more events can benefit. Looking at data from the past two years, we‚Äôve often been close to or over our budget. The guidelines haven‚Äôt been updated in a while. As grant requests become more complex, we‚Äôd like to simplify and clarify the process, and better explain it on our website.We would also like to acknowledge that EuroPython, when traveling around Europe, has an additional impact on the host country, and we‚Äôd like to set aside part of the budget for the local community.The Grant Programme is also a primary funding source for EuroPython Finaid. To that end, we aim to allocate 30% of the total Grant Programme budget to Finaid, an increase from the previous 25%.We‚Äôve updated the text on our website, and split it into multiple sub-pages to make it easier to navigate. The website now includes a checklist of what we would like to see in a grant application, and a checklist for the Grants Workgroup ‚Äì so that when you apply for the Grant you already know the steps that it will go through later and when you can expect an answer from us.We looked at the data from previous years, and size and timing of the grant requests. With the growing number and size of the grants, to make it more accessible to smaller conferences and conferences happening later in the year, we decided to introduce max caps per grant and split the budget equally between the first and second half of the year. We would also explicitly split the total budget into three categories ‚Äì 30% goes to the EuroPython finaid, 10% is reserved for projects in the host country. The remaining 60% of the budget goes to fund other Python Conferences. This is similar to the split in previous years, but more explicit and transparent.Using 2024 data, and the budget available for Community Grants (60% of total), we‚Äôve simulated different budget caps and found a sweet spot at 6000EUR, where we are able to support all the requests with most of the grants being below that limit. For 2025 we expect to receive a similar or bigger number of requests.We are introducing a special 10% pool of money to be used on projects in the host country (in 2025 that‚Äôs again Czech Republic). This pool is set aside at the beginning of the year, with one caveat that we would like to deploy it in the first half of the year. Whatever is left unused goes back to the Community Pool to be used in second half of the year.Fairer Funding: By spreading our grants out during the year, conferences that happen later won‚Äôt miss out.Easy to Follow: Clear rules and deadlines cut down on confusion about how much you can get and what it‚Äôs for.Better Accountability: We ask for simple post-event reports so we can see where the money went and what impact it made.Stronger Community: Funding more events grows our Python network across Europe, helping everyone learn, connect, and collaborate.]]></content:encoded></item><item><title>Quiz: Python Keywords: An Introduction</title><link>https://realpython.com/quizzes/python-keywords/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. These keywords are always available in Python, which means you don‚Äôt need to import them. Understanding how to use them correctly is fundamental for building Python programs.]]></content:encoded></item></channel></rss>