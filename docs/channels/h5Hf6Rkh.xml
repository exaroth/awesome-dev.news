<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Python</title><link>https://www.awesome-dev.news</link><description></description><item><title>Shubhanshu Shukla Returns Safely from Space: A Historic Leap for India</title><link>https://dev.to/shravan_655c21d339de8a4a0/shubhanshu-shukla-returns-safely-from-space-a-historic-leap-for-india-5695</link><author>Shravan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 07:25:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
Shubhanshu Shukla has successfully completed his historic space journey and returned safely to Earth, marking a significant milestone in India’s space exploration achievements. His safe return is not just a personal triumph but a proud moment for the entire nation, showcasing India's growing capabilities in manned space missions. 
This successful mission brings new hope and excitement for the future of Indian space research and inspires a new generation of dreamers and explorers.]]></content:encoded></item><item><title>🚀 I Created OctaneDB – The Lightning-Fast Python Vector Database!</title><link>https://dev.to/rijinraju/i-created-octanedb-the-lightning-fast-python-vector-database-21d6</link><author>Rijin Raju</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 06:45:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[💡 What is OctaneDB?
OctaneDB is an open-source, high-performance vector database written in Python.
It lets you store, index, and rapidly search millions of text, image, or custom embeddings using state-of-the-art similarity search algorithms.✨ Key Features
⚡️ 10x Faster Than Pinecone/ChromaDB: Sub-millisecond queries, >3,000 vectors/sec insert rate.🧠 Advanced Indexing: HNSW for ultra-fast approximate search, FlatIndex for exact matches.💾 Flexible Storage: In-memory or persistent HDF5 mode.🤖 Text Embedding Built-In: Auto text-to-vector with sentence-transformers.🚀 GPU Acceleration: CUDA support out of the box.🔍 Powerful Search: Batch search, advanced metadata filtering (AND/OR/NOT logic).🔌 Easy Integration: ChromaDB-compatible API for seamless migration.🌎 Open Source: MIT licensed, totally free for all uses!🌐 Try it Online or Locally!
Get Started:bash
pip install octanedbpython
from octanedb import OctaneDB
db = OctaneDB(dimension=384, embedding_model="all-MiniLM-L6-v2")
db.create_collection("documents")
    ids=["doc1", "doc2"],
    documents=["About pineapple", "About oranges"]
)
results = db.search_text(query_text="fruit", k=2)
print(results)
Semantic searchImage embedding similarity🛠️ Features Coming Soon
Live Multi-TenancyHybrid Scalar/Vector QueriesInstant Index Updates (feedback wanted!)💬 Get Involved!
Try it, star it, and contribute on GitHubShare your benchmarks and real-world results!What problems do you face with vector DBs?
Drop your ideas, feature requests, or open an issue!🚦 Open to Feedback, Collaboration, and Questions!
Let's build the next era of search and AI together 🤝RijinRaju/octanedb]]></content:encoded></item><item><title>Weekly Challenge: The Common Winner</title><link>https://dev.to/simongreennet/weekly-challenge-the-common-winner-57ka</link><author>Simon Green</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 05:25:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Each week Mohammad S. Anwar sends out The Weekly Challenge, a chance for all of us to come up with solutions to two weekly tasks. My solutions are written in Python first, and then converted to Perl. It's a great way for us all to practice some coding.
  
  
  Task 1: Common Characters
You are given an array of words.Write a script to return all characters that is in every word in the given array including duplicates.The task doesn't mention the order in which the list should be generated. Based on the examples, both "order they appear in the first word" and "alphabetical order" seem to be valid solutions. I've chose alphabetical order for this.For this challenge, I've turned the supplied  into a list of Counters (array of hashes in Perl) of letter frequencies called .I then iterate through each unique letter in the first word (in alphabetical order), calling the variable . I calculate the minimum number of occurrences of that letter in all the words. The Counter object will return  if the letter does not exist. If the letter occurs in all words, I append it to the  list the required number of times.The Perl solution follows the same logic, but generates the  hash by hand../ch-1.py bella label roller
, , ./ch-1.py cool lock cook
, ./ch-1.py hello world pole
, ./ch-1.py abc def ghi
./ch-1.py aab aac aaa
, You are given an array of all moves by the two players.Write a script to find the winner of the TicTacToe game if found based on the moves provided in the given array.Order move is in the order - , , , , , ….My sisters never liked playing Noughts and Crosses (as it is known as here) when I was young because I figured out a way to never lose. You have to remember this was a long time before the Internet was available to do research on this :-)For this task I take the command line input and convert it into pairs of . I initialize the  variable with 3 × 3 grid of underscores, and the  variable to .I then iterate through each move, starting by ensuring the move is within the bounds of the board, and the player isn't using a position that is already used.I then make the move on the board, check if there is a result, and switch to the other player in preparation for the next move. If there is a result, I return the player that won.If all the moves have been made, and there is no winner, I checked for any  on the . If there are, I return , or  if there are none.The  function takes the  and sees if there is a row, column, or one diagonal that has the same letter.The Perl solution follows the same logic as the Python one../ch-2.py 0 0 2 0 1 1 2 1 2 2
A

./ch-2.py 0 0 1 1 0 1 0 2 1 0 2 0
B

./ch-2.py 0 0 1 1 2 0 1 0 1 2 2 1 0 1 0 2 2 2
Draw

./ch-2.py 0 0 1 1
Pending

./ch-2.py 1 1 0 0 2 2 0 1 1 0 0 2
B
]]></content:encoded></item><item><title>Building GitNarrative: How I Parse Git History with Python to Extract Development Patterns</title><link>https://dev.to/grudged/building-gitnarrative-how-i-parse-git-history-with-python-to-extract-development-patterns-52lm</link><author>Chris Moore</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 04:47:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When I started building GitNarrative, I thought the hardest part would be the AI integration. Turns out, the real challenge was analyzing git repositories in a way that actually captures meaningful development patterns.Here's how I built the git analysis engine that powers GitNarrative's story generation.
  
  
  The Challenge: Making Sense of Messy Git History
Every git repository tells a story, but extracting that story programmatically is complex. Consider these real commit messages from a typical project:"fix bug"
"refactor"
"update dependencies" 
"THIS FINALLY WORKS"
"revert last commit"
"actually fix the bug this time"
The challenge is identifying patterns that reveal the actual development journey - the struggles, breakthroughs, and decision points that make compelling narratives.
  
  
  Library Choice: pygit2 vs GitPython
I evaluated both major Python git libraries:: More Pythonic, easier to use: Lower-level, better performance, more controlI chose  because GitNarrative needs to process repositories with thousands of commits efficiently. The performance difference is significant for large repositories.
  
  
  Core Analysis Architecture
Here's the foundation of my git analysis engine:
  
  
  Pattern Recognition: The Heart of Story Extraction
The key insight is that commit patterns reveal development phases. Here's how I identify them:
  
  
  1. Commit Type Classification

  
  
  2. Development Phase Detection

  
  
  3. Struggle and Breakthrough Detection
This is where the storytelling magic happens:
  
  
  Timeline Correlation: When Things Happened
Understanding timing is crucial for narrative flow:
  
  
  Performance Optimizations
Processing large repositories efficiently required several optimizations:
  
  
  3. Parallel Processing for Multiple Repositories

  
  
  Integration with AI Story Generation
The analysis output feeds directly into AI prompts:: Repositories with inconsistent commit message styles: Pattern matching with multiple fallback strategies and file-based analysis: Merge commits creating noise in analysis: Filtering strategy that focuses on meaningful commits while preserving merge context: Very large repositories (10k+ commits): Sampling strategy that captures representative commits from different time periodsThe analysis engine successfully processes repositories ranging from small personal projects to large open source codebases. When tested on React's repository, it correctly identified:The initial experimental phase (2013)Major architecture rewrites (Fiber, Hooks)Performance optimization periodsCurrent improvements in development:Better natural language processing of commit messagesMachine learning models for commit classificationIntegration with issue tracker data for richer contextSupport for monorepo analysisThe git analysis engine is the foundation that makes GitNarrative's storytelling possible. By extracting meaningful patterns from commit history, we can transform boring git logs into compelling narratives about software development.GitNarrative is available at https://gitnarrative.io - try it with your own repositories to see these patterns in action.What patterns have you noticed in your own git history? I'd love to hear about interesting commit patterns you've discovered in your projects.]]></content:encoded></item><item><title>Decoding the Neural Network&apos;s Mind: A Journey Through Forward Propagation</title><link>https://dev.to/dev_patel_35864ca1db6093c/decoding-the-neural-networks-mind-a-journey-through-forward-propagation-2n6h</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 01:58:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine a detective meticulously piecing together clues to solve a complex case. That's essentially what a neural network does during forward propagation. It takes input data (the clues), processes it layer by layer (analyzes the evidence), and ultimately arrives at an output (solving the case). This process, called forward propagation, is the fundamental engine driving the power of neural networks, the cornerstone of modern machine learning. This article will demystify this crucial process, making it accessible to both beginners and those seeking a deeper understanding.
  
  
  What is Forward Propagation?
Forward propagation is the process by which a neural network transforms input data into an output prediction. It's a series of calculations, flowing forward through the network's layers, each layer transforming the data slightly until a final prediction emerges. Think of it as a pipeline where data enters, undergoes a series of transformations, and finally exits as a refined prediction.
  
  
  The Architecture: Layers and Connections
A neural network consists of interconnected layers: Receives the initial data.  For example, if classifying images, this layer might represent the pixel values.  These layers perform the bulk of the processing, transforming the data through complex mathematical operations.  A network can have multiple hidden layers, increasing its complexity and learning capacity. Produces the final prediction.  This could be a classification (cat or dog), a regression value (house price), or any other desired output.Each layer is composed of interconnected , which perform weighted sums of their inputs and apply an activation function to introduce non-linearity. These connections have associated  and , which are the parameters the network learns during training.
  
  
  The Mathematics:  A Step-by-Step Walkthrough
Let's simplify the math. Consider a single neuron receiving inputs $x_1, x_2, ..., x_n$ with corresponding weights $w_1, w_2, ..., w_n$ and a bias $b$. The neuron's output, $z$, is calculated as:$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = \sum_{i=1}^{n} w_ix_i + b$This is a weighted sum of inputs plus a bias. The bias acts as an offset, allowing the neuron to activate even when inputs are small.Next, an , denoted as σ(z), is applied to introduce non-linearity. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh. For example, the ReLU function is defined as:This means the output is either 0 or the input itself, depending on whether the input is negative or positive. This simple non-linearity is crucial for the network's ability to learn complex patterns.The output of one layer becomes the input for the next, and this process repeats until the output layer is reached. Let's illustrate with Python pseudo-code:Forward propagation is the backbone of countless applications:  Classifying images of cats, dogs, or other objects.Natural Language Processing:  Understanding and generating human language, powering chatbots and machine translation.  Object detection and path planning.  Analyzing medical images to detect diseases.
  
  
  Challenges and Limitations
  Training deep neural networks can be computationally expensive, requiring powerful hardware (GPUs).  The network might learn the training data too well and perform poorly on unseen data. Understanding why a network makes a specific prediction can be challenging, raising ethical concerns in sensitive applications.
  
  
  The Future of Forward Propagation
Forward propagation remains central to neural network research. Ongoing research focuses on:More efficient algorithms:  Reducing computational costs and improving training speed.  Designing networks that are more robust, accurate, and interpretable.New activation functions:  Exploring activation functions that enhance learning and generalization.In conclusion, forward propagation is the engine driving the power of neural networks. Understanding its mechanics—the flow of data, the mathematical transformations, and the role of activation functions—is crucial for anyone seeking to master the art of machine learning. As research continues, forward propagation will undoubtedly play an even more critical role in shaping the future of artificial intelligence.]]></content:encoded></item><item><title>Local LLMs, No API Keys, No BS: Build Your Own Waifubot Terminal Chat in Python</title><link>https://dev.to/owly/local-llms-no-api-keys-no-bs-build-your-own-waifubot-terminal-chat-in-python-470c</link><author>owly</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 23:36:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Build a Local Waifubot Terminal Chat in Python — No API Keys, No Cloud, No Bullshit
Tired of cloud dependencies, subscriptions, and rate limits? Want your own affectionate AI companion running locally, offline, and async? This walkthrough shows you how to build a waifubot terminal chat using Ollama, LLaMA 3, and Python. No fluff. Just code.
  
  
  Step 1: Install Ollama (One-Time Setup)
Ollama lets you run LLMs locally with ease.Go to oLLaMa’s download page
Download the installer for your OS (Windows/macOS)
Install and open the Ollama app
In the Ollama terminal, pull a model:
This downloads the LLaMA 3 model locally.
  
  
  🧰 Step 2: Create Your PyCharm Project
Open PyCharm → New Project → name it 
Inside the project, create a file:  file and add:
PyCharm will prompt you to install it — accept and let it install.
  
  
  Step 3: Write Your Chat Script
This code requires a certain threshold of computing power, so don't expect it to run smoothly on your vintage Pentium 3 machine.
The code is modular and wrapped into functions.
The code runs asyncly, which is handled in the function doing the calls.
The code runs locally and offline:  No subscription needed
The chat adds short memory context to each call.]]></content:encoded></item><item><title>Building Spokane Tech: Part 8</title><link>https://dev.to/dbslusser/building-spokane-tech-part-8-5h0e</link><author>David</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 22:42:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Welcome to part 8 of the "Building Spokane Tech" series! In this article, we'll discuss adding Docker and Docker Compose for running components of our service in containers.Containerization has become an essential tool for modern web development, and Docker is at the forefront of this revolution. When developing a Django-based web application like ours, using Docker ensures consistency across development and deployed environments. By leveraging Docker Compose, we can efficiently manage multiple services required by our application.Docker Compose is a tool that allows you to define and manage multi-container Docker applications using a simple YAML file (docker-compose.yaml). It enables developers to run interconnected services, such as a web application, database, and message broker, with a single command. The  Docker Compose basic concepts include:Key Docker Compose Configuration Options Defines the Compose file format version. In our case, we use "3.9", which is one of the latest stable versions. Lists all the containers that make up the application. Each service runs in its own container.Service Configuration Keys Specifies the Docker image to use for the container. If the image is not found locally, Docker will pull it from a registry like Docker Hub. Defines how to build the image from a Dockerfile. It usually includes:context: The directory containing the Dockerfile.dockerfile: The path to the specific Dockerfile used to build the image. Gives a custom name to the container instead of a randomly generated one. Overrides the default command specified in the Dockerfile, allowing you to run specific commands when the container starts. Loads environment variables from an external .env file. Maps ports between the container and the host. Specifies service dependencies. A container will not start until its dependencies are up and running.Volumes store persistent data outside the container filesystem, ensuring data is not lost when containers are restarted or removed.Let's review the components in our system, each of these will be a service in our docker-compose.yaml file.Django (Web Application) – The core application running on Gunicorn or the Django development serverPostgreSQL (Database) – Stores application dataRedis (Message Broker) – Used by Celery for task queuingCelery Worker – Executes asynchronous tasksCelery Beat – Handles scheduled tasksCelery Flower – Provides a web UI for monitoring Celery tasksOur docker-compose.yaml fileversion: '3.9'

services:
  django:
    image: spokanetech-django:latest
    container_name: django
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: ./entrypoint.sh
    ports:
      - "8080:8000"
    depends_on:
      - db
      - redis

  db:
    image: postgres:17
    container_name: postgres_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    env_file:
      - .env.compose

  redis:
    image: redis:7.2-alpine
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"

  worker:
    image: spokanetech-django:latest
    container_name: worker
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: celery -A core worker -l info
    depends_on:
      - redis
      - db

  beat:
    image: spokanetech-django:latest
    container_name: beat
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: celery -A core beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    depends_on:
      - redis
      - db

  flower:
    image: spokanetech-django:latest
    container_name: flower
    env_file:
      - .env.compose
    command: ["celery", "-A", "core", "--config=flowerconfig.py", "flower"]
    ports:
      - "5555:5555"
    depends_on:
      - redis
      - db

volumes:
  postgres_data:
  static_volume:
Docker Compose provides several commands to manage services. Here are the basics:To build the containers run:This builds images for the services defined in docker-compose.yaml using the specified Dockerfile. If an image already exists, it will only rebuild if changes are detected.To start the containers run:This starts all services defined in docker-compose.yaml. It also automatically builds missing images if they are not found.To run the containers in detached mode use:This runs containers in the background and allows applications to run persistently.To stop the containers use:This stops and removes all containers, networks, and volumes (if specified); it does not remove built images.Rebuild and restart containersTo build the container when running, use:docker-compose up --buildThis rebuilds images before starting containers and ensures the latest changes in the Dockerfile are applied.All of our components are available on localhost on various their applicable ports: ]]></content:encoded></item><item><title>Sebastian Pölsterl: scikit-survival 0.25.0 with improved documentation released</title><link>https://k-d-w.org/blog/2025/08/scikit-survival-0.25.0-with-improved-documentation-released/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 21:55:06 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[This release adds support for scikit-learn 1.7, in addition to version 1.6.
However, the most significant changes in this release affect the documentation.
The API documentation has been completely overhauled to improve clarity and consistency.
I hope this marks a significant improvement for users new to scikit-survival.One of the biggest pain points for users seems to be understanding which metric can be used to evaluate the performance of a given estimator.
The user guide
now summarizes the different options.The performance metrics for evaluating survival models can be broadly divided into three groups:Concordance Index (C-index): Measures the rank correlation between predicted risk scores and observed event times.
Two implementations are available in scikit-survival:Cumulative/Dynamic Area Under the ROC Curve (AUC):
Extends the AUC to survival data, quantifying how well a model distinguishes subjects who experience an event by a given time from those who do not. It can handle time-dependent risk scores
and is implemented in cumulative_dynamic_auc().:
An extension of the mean squared error to right-censored data.
The Brier score assesses both discrimination and calibration based on a model’s estimated survival functions.
You can either compute the Brier score at specific time point(s) using
brier_score()
or compute an overall measure by integrating the Brier score over a range of time points via
integrated_brier_score().What Do Survival Models Predict?Survival models can predict several quantities, depending on the model being used.
First of all, every estimator has a  method,
which either returns a unit-less risk score
or the predicted time of an event.If predictions are , higher values indicate an
increased risk of experiencing an event. The scores have no unit
and are only meaningful for ranking samples by their risk of experiencing an event.
This is for example the case for
CoxPHSurvivalAnalysis.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit model
estimator = CoxPHSurvivalAnalysis().fit(Xt, y)
# Predict risk score
predicted_risk = estimator.predict(Xt)
# Evaluate risk scores
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], predicted_risk
)
If predictions directly relate to the time point of an event,
lower scores indicate shorter survival, while higher scores indicate longer survival.
See for example IPCRidge.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import IPCRidge
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = IPCRidge().fit(Xt, y)
# Predict time of an event
predicted_time = estimator.predict(Xt)
# Flip sign of predictions to obtain a risk score
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], -1 * predicted_time
)
While the concordance index is easy to interpret,
it is not a useful measure of performance if a specific time range
is of primary interest (e.g. predicting death within 2 years).
This is particularly relevant for survival models that can
make time-dependent predictions.For instance,
RandomSurvivalForest,
can also predict survival functions (via predict_survival_function())
or cumulative hazard functions (via predict_cumulative_hazard_function()).
These functions return lists of
StepFunction instances.
Each instance can be evaluated at a set of time points to obtain predicted
survival probabilities (or cumulative hazards).
The Brier score and
cumulative_dynamic_auc()
are capable of evaluating time-dependent predictions, but .import numpy as np
from sksurv.datasets import load_veterans_lung_cancer
from sksurv.ensemble import RandomSurvivalForest
from sksurv.metrics import integrated_brier_score
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = RandomSurvivalForest().fit(Xt, y)
# predict survival functions
surv_funcs = estimator.predict_survival_function(Xt)
# select time points to evaluate performance at
times = np.arange(7, 365)
# create predictions at selected time points
preds = np.asarray(
[[sfn(t) for t in times] for sfn in surv_funcs]
)
# compute integral
score = integrated_brier_score(y, y, preds, times)
]]></content:encoded></item><item><title>IoT-Driven Fence Solutions: Balancing Security, Automation, and Aesthetics</title><link>https://dev.to/emily_johnson_dev/iot-driven-fence-solutions-balancing-security-automation-and-aesthetics-e99</link><author>Emily Johnson</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 21:50:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s connected world, the role of fences has evolved beyond simple boundaries. IoT-driven fence solutions are transforming the way we manage , , and  for residential, commercial, and industrial properties. With integrated smart sensors, mobile apps, and cloud platforms, modern fencing systems can provide real-time monitoring, adaptive controls, and seamless customization options.  In this article, we’ll explore how IoT technologies are shaping the fencing industry, showcase real-world applications, and include  for IoT integration in smart fencing systems.  1. The Rise of Smart Fencing SystemsTraditional fences used to be static structures offering only physical security. Today, homeowners and businesses demand automation, remote control, and aesthetic flexibility. IoT fencing solutions combine:   to detect motion, vibration, or tampering.
 with voice or app-based controls.
 for facial recognition and surveillance.
 to monitor and configure fences in real time.
Many property owners in Illinois rely on experts like a  to deploy advanced systems that combine privacy, security, and modern design.  2. Key Features of IoT-Driven Fence SolutionsIoT-enabled fences connect sensors and cameras to smart hubs, instantly notifying property owners of suspicious activity.  b) Automation & Remote AccessThrough dedicated mobile apps, users can open gates, lock perimeters, or switch to privacy mode instantly.  c) Aesthetic Variety & CustomizationIoT solutions also allow homeowners to control LED lighting, surface finishes, or retractable panels to adapt fences to different scenarios or moods.  Solar-powered IoT devices and low-energy controllers minimize operational costs while improving sustainability.  3. Sample Architecture for IoT Smart FenceHere’s a simple architecture to visualize how a smart fencing system works:graph TD
    A[IoT Sensors] --> B[Smart Hub]
    B --> C[Cloud Platform]
    C --> D[Mobile App]
    D --> E[User Control]
    B --> F[AI Camera Module]
    F --> C
4. Programming Example: Node.js IoT Fence ControllerHere’s a simple Node.js snippet for managing a smart fence’s lock/unlock automation via IoT commands:This code uses  to communicate with IoT devices and allows remote locking/unlocking of fence gates through real-time messaging.  5. Adding Facial Recognition for Enhanced SecurityFor properties requiring high security — such as commercial facilities — integrating AI-powered cameras with IoT fences offers advanced monitoring.This Python snippet integrates facial recognition to detect authorized users and could trigger IoT-controlled gates accordingly.  6. IoT Solutions for Commercial PropertiesBusinesses demand higher security and seamless automation, especially when managing multiple properties. Companies specializing in smart installations, such as a , provide advanced IoT-enabled perimeter control systems, ensuring that both safety and design preferences are met.  For premium installations, incorporating modern styles like a  with integrated sensors offers both  and .  7. Future Trends in IoT Fencing Systems Faster data transmission for real-time monitoring.
AI Predictive Maintenance: Automated alerts when panels or sensors need servicing.
 Visualize and customize fence designs instantly via mobile apps.
 Control fences through Alexa, Google Assistant, or Siri.
IoT-driven fence solutions represent the perfect fusion of , , and . By integrating smart sensors, AI cameras, and real-time mobile controls, property owners can protect their investments while enjoying flexibility and style.  Whether upgrading an existing fence or installing a new IoT-powered system, partnering with experts ensures seamless implementation and long-term performance. The future of fencing isn’t just functional — it’s smart, connected, and designed to impress.  ]]></content:encoded></item><item><title>Glyph.Flow Devlog #2 – Hitting the Registry Milestone</title><link>https://dev.to/daemonic01/glyphflow-devlog-2-hitting-the-registry-milestone-41h5</link><author>Dominik Kopócs</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 21:00:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Last time I shared why I’m building Glyph.Flow, a minimalist workflow manager in the terminal with Textual.
This week it’s time for an update on what I managed to get done.I wanted to move from a rough prototype into something modular and extensible.
That meant one thing: a command registry.Backend refactor: my massive 630-line app.py is now down to ~112 lines. Commands live in a registry, not tangled logic.Command registry: all commands are defined declaratively, with schema-based argument parsing, aliases, and usage.Logging: unified styling and message keys, with autosave and error handling standardized.New config command: quick way to tweak settings on the fly.Consistency: adding a new command is now just “add a dict + handler”.It finally behaves like a real CLI app instead of a spaghetti prototype — but I’ll be honest, it’s still a prototype.
The difference is: now the foundation feels stable enough to build on.More commands to migrate (delete, edit, schema, …).Road toward a TUI interface on top of this backend.Eventually, I’d like this to feel like a natural console companion for managing projects.That’s it for this week’s log.
If you’re into command-line tools, or building things with Textual, I’d love to hear your feedback. 🚀]]></content:encoded></item><item><title>Rodrigo Girão Serrão: functools.Placeholder</title><link>https://mathspp.com/blog/how-to-use-functools-placeholder</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 19:21:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Learn how to use , new in Python 3.14, with real-life examples.By reading this article you will understand what  is for and how to use it effectively.Partial function applicationIn a nutshell,  allows you to perform partial function application, by “freezing” arguments to functions.Up until Python 3.13, you could use  to freeze arguments in two types of ways:you could pass positional arguments to , which would be passed in the same order to the function being used with ; oryou could pass keyword arguments to , which would be passed with the same name to the function being used with .Using keyword arguments to skip the first argumentThe method 2. is especially useful if you're trying to freeze an argument that is not the first one.
For example, if you use the built-in  on the built-in , you can see this signature:int(x, base=10) -> integerIf you want to convert a binary string to an integer, you can set :print(int("101", 2))  # 5Now, suppose you want to create a function  by “freezing” the argument  in the built-in .
Writingfrom_binary = partial(int, 2)won't work, since in , the value  is seen as the argument  from the signature above.
However, you can pass the base as a keyword argument, skipping the first argument  from the signature of the built-in :from functools import partial

from_binary = partial(int, base=2)

print(from_binary("101"))  # 5But this doesn't always work.When keyword arguments don't workimport string

_table = str.maketrans("", "", string.punctuation)
def remove_punctuation(string):
    return string.translate(_table)

print(remove_punctuation("Hello, world!"))  # Hello worldThe function  is a thin wrapper around the string method , which is the function doing all the work.
In fact, if you look at  as a function, you always pass  as the second argument; what changes is the first argument:print(str.translate("Hello, world!", _table))  # Hello world
print(str.translate("What?!", _table))  # WhatThis may lead you to wanting to use  to freeze the value  on the function , so you use the built-in  to check the signature of :translate(self, table, /) unbound builtins.str methodYou can see that the first argument is , the string you are trying to translate, and then  is the translation table (that  built magically for you).
But you can also see the forward slash , which means that  and  are positional-only arguments that cannot be passed in as keyword arguments!]]></content:encoded></item><item><title>This algorithm solves the triangle-finding problem in linear time, providing strong evidence that all problems in the 3SUM-hard class can be solved in sub-quadratic time.</title><link>https://dev.to/frank_vega_987689489099bf/this-algorithm-solves-the-triangle-finding-problem-in-linear-time-providing-strong-evidence-that-p4a</link><author>Frank Vega</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 18:15:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Our Sqrt(n)-approximation for the independent set problem would strongly suggest that P = NP. Experimental results showed a 2-approximation ratio on real-world benchmarks, outperforming the theoretical Sqrt(n) worst-case guarantee.</title><link>https://dev.to/frank_vega_987689489099bf/our-sqrtn-approximation-for-the-independent-set-problem-would-strongly-suggest-that-p-np-2cdg</link><author>Frank Vega</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 18:14:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Would you graph your commute? Here’s what I found when I did.</title><link>https://dev.to/kauldeepak78/would-you-graph-your-commute-heres-what-i-found-when-i-did-123</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:40:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Because every train - What my mood, weather, and trains revealed in 3 months of tracking delay deserves a chart</title><link>https://dev.to/kauldeepak78/because-every-train-what-my-mood-weather-and-trains-revealed-in-3-months-of-tracking-delay-4213</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:39:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Because every train delay deserves a chart</title><link>https://dev.to/kauldeepak78/because-every-train-delay-deserves-a-chart-4i86</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:38:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[From rush hour chaos to beautiful graphs]]></content:encoded></item><item><title>From rush hour chaos to beautiful graphs</title><link>https://dev.to/kauldeepak78/from-rush-hour-chaos-to-beautiful-graphs-5823</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:37:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When boredom meets Python, you get insights]]></content:encoded></item><item><title>When boredom meets Python, you get insights</title><link>https://dev.to/kauldeepak78/when-boredom-meets-python-you-get-insights-633</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:37:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Data Science Path: Automatic Subclass Registration &amp; Python Encryption Algorithms with LabEx</title><link>https://dev.to/labex/data-science-path-automatic-subclass-registration-python-encryption-algorithms-with-labex-9f9</link><author>Labby</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:02:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Embarking on a data science journey can feel daunting, but what if you could start with engaging, bite-sized challenges that build your skills step by step? The LabEx 'Data Science' path is designed precisely for this, offering a structured roadmap through hands-on, interactive lessons. Forget passive video lectures; here, you learn by doing, mastering essential concepts from statistical analysis to machine learning and data visualization. Let's explore a few beginner-friendly experiments that will kickstart your transformation from novice to data wizard.
  
  
  Automatic Registration of Subclasses
 Beginner |  5 minutesIn this challenge, we will implement a class called Base that will automatically record any subclasses that inherit from it. The purpose of this implementation is to enable the retrieval of all subclass names by iterating over Base. The goal is to demonstrate the functionality of Base by showing that it correctly registers and outputs the names of the subclasses. We will accomplish this by implementing the  method in the Base class and ensuring that it supports iteration.
  
  
  Implementing Column Permutation Encryption in Python
 Beginner |  5 minutesIn this challenge, we will be implementing the Column Permutation Encryption method. This method involves encrypting a plaintext by writing it down line by line with a fixed number of characters per line, and then rearranging the columns of the resulting matrix according to the alphabetical order of a key. The rearranged columns are then read out one by one to obtain the ciphertext. The objective of the challenge is to complete the column_permutation_encryption(text) function in the given file, which takes a piece of text as input, performs column permutation encryption using the key qiao and the padding character ,, and returns the ciphertext. If the input text is empty, None should be returned.
  
  
  Implementing Affine Encryption in Python
 Beginner |  5 minutesIn this challenge, we will implement the Affine encryption algorithm. The Affine cipher is a substitution cipher that combines the characteristics of the shift cipher and the multiplier cipher. It uses a cryptographic function to encrypt one letter per letter based on a mathematical formula. The objective is to complete the implementation of the affine_encryption(text) function in the affine.py file, which takes a piece of text as input, encrypts it using the Affine cipher, and returns the ciphertext.
  
  
  Count Each Type Characters
 Beginner |  5 minutesIn this challenge, we will count the number of letters, spaces, digits, and other characters in a given input. The objective is to correctly categorize and count each type of character. For example, given the input 'abc123EFG *&45?', the expected output would be 'letter=6,space=1,digit=5,other=3'.These beginner-friendly challenges are just the beginning of your data science adventure. Each one is designed to build foundational skills, from understanding object-oriented principles to mastering data manipulation and even delving into the fascinating world of cryptography. Dive in, experiment, and watch your data science capabilities grow with LabEx's interactive learning environment. Your journey to becoming a data science pro starts here!]]></content:encoded></item><item><title>Rodrigo Girão Serrão: TIL #130 – Format Python code directly with uv</title><link>https://mathspp.com/blog/til/format-python-code-directly-with-uv</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 16:34:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Today I learned you can format your Python code directly with uv.In uv version 0.8.13, released one or two days ago, uv added the command  that allows you to format your Python code directly through the uv CLI.First and foremost, make sure you're rocking uv 0.8.13 or greater by running .To format your code with uv you can simply run , which will use Ruff to format the code in your current directory:The idea is not to have uv replace Ruff; it's just so that you don't have to think about a separate tool if you don't want to. accepts the same arguments and options that  accepts, so you'll want to check the Ruff docs to learn more.
My favourite option is , to take a look at the formatting diff without doing any formatting changes.As of now, the feature is marked as being experimental, which means it might change in the future!]]></content:encoded></item><item><title>Build a RAG application with LangChain and Local LLMs powered by Ollama</title><link>https://dev.to/abhirockzz/build-a-rag-application-with-langchain-and-local-llms-powered-by-ollama-3el5</link><author>Abhishek Gupta</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 14:12:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Local large language models (LLMs) provide significant advantages for developers and organizations. Key benefits include enhanced , as sensitive information remains entirely within your own infrastructure, and , enabling uninterrupted work even without internet access. While cloud-based LLM services are convenient, running models locally gives you full control over model behavior, performance tuning, and potential cost savings. This make them ideal for experimentation before running production workloads.The ecosystem for local LLMs has matured significantly, with several excellent options available, such as Ollama, Foundry Local, Docker Model Runner, and more. Most popular AI/Agent frameworks including LangChain and LangGraph provide integration with these local model runners, making it easier to integrate them into your projects.This blog post will illustrate how to use local LLMs with Azure Cosmos DB as a vector database for retrieval-augmented generation (RAG) scenarios. It will guide you through setting up a local LLM solution, configuring Azure Cosmos DB, loading data, performing vector searches, and executing RAG queries. You can either use the Azure Cosmos DB emulator for local development or connecting to an Azure Cosmos DB account in the cloud. You will be using Ollama (open-source solution) to run LLMs locally on your own machine. It lets you download, run, and interact with a variety of LLMs (like Llama 3, Mistral, and others) using simple commands, without needing cloud access or complex setup.By the end of this blog post, you will have a working local RAG setup that leverages Ollama and Azure Cosmos DB. the sample app uses LangChain integration with Azure Cosmos DB to perform embedding, data loading, and vector search. You can easily adapt it to other frameworks like LlamaIndex.To get started with Ollama, follow the official installation guide on GitHub to install it on your system. The installation process is straightforward across different platforms. For example, on Linux systems, you can install Ollama with a single command:curl  https://ollama.com/install.sh | sh
Once installed, start the Ollama service by running:This blog post demonstrates the integration using two specific models from the Ollama library: - A high-quality embedding model with 1024 dimensions, ideal for generating vector representations of text - The 8B parameter variant of Meta's Llama 3, which serves as our chat model for the RAG pipelineDownload both models using the following commands. Note that this process may take several minutes depending on your internet connection speed, as these are substantial model files:ollama pull mxbai-embed-large
ollama pull llama3:8b

  
  
  Something to keep in mind ...
While tools like Ollama make it straightforward to run local LLMs, hardware requirements depend on the specific model and your performance expectations. Lightweight models (such as Llama 2 7B or Phi-2) can run on modern CPUs with as little as 8 GB RAM, though performance may be limited. Larger models (like Llama 3 70B or Mixtral) typically require a dedicated GPU with at least 16 GB VRAM for efficient inference. Ollama supports both CPU and GPU execution. On CPU-only systems, you can expect slower response times, especially with larger models or concurrent requests. Using a compatible GPU significantly accelerates inference required for demanding workloads.Since you're working with local models, you'll likely want to use the Azure Cosmos DB emulator for local development. The emulator provides a local environment that mimics the Azure Cosmos DB service, enabling you to develop and test your applications without incurring costs or requiring an internet connection.The emulator is available as a Docker container, which is the recommended way to run it. Here are the steps to pull and start the Cosmos DB emulator. The commands shown are for Linux - refer to the documentation for other platform options.docker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest

docker run  8081:8081 1 mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest
curl  https://localhost:8081/_explorer/emulator.pem  ~/emulatorcert.crt
update-ca-certificates
You should see output similar to this:Updating certificates in /etc/ssl/certs...
rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d...
done.

  
  
  Load data into Azure Cosmos DB
Now that both Ollama and Azure Cosmos DB are set up, it's time to populate our vector database with some sample data. For this demonstration, we'll use Azure Cosmos DB's own documentation as our data source. The loader will fetch markdown content directly from the Microsoft Docs repository, specifically focusing on articles about Azure Cosmos DB vector search functionality.Our data loading process will read these documentation articles, generate embeddings using the  model, and store both the content and vector representations in Azure Cosmos DB for retrieval.Begin by cloning the GitHub repository containing the sample application:git clone https://github.com/abhirockzz/local-llms-rag-cosmosdb
local-llms-rag-cosmosdb
Before running the loader application, ensure you have Python 3 installed on your system. Create a virtual environment and install the required dependencies:python3  venv .venv
 .venv/bin/activate

pip3  requirements.txt
Next, configure the environment variables and execute the loading script. The example below uses the Azure Cosmos DB emulator for local development. If you prefer to use the cloud service instead, simply set the  variable to your Azure Cosmos DB account URL and remove the  variable.

python3 load_data.py
The script will automatically create the database and container if they don't already exist. Once the data loading process completes successfully, you should see output similar to this:Uploading documents to Azure Cosmos DB ['https://raw.githubusercontent.com/MicrosoftDocs/azure-databases-docs/refs/heads/main/articles/cosmos-db/nosql/vector-search.md', 'https://raw.githubusercontent.com/MicrosoftDocs/azure-databases-docs/refs/heads/main/articles/cosmos-db/nosql/multi-tenancy-vector-search.md']
Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Loading 26 document chunks from 2 documents
Data loaded into Azure Cosmos DB
To confirm that your data has been loaded successfully, you can inspect the results using the Azure Cosmos DB Data Explorer. If you're using the emulator, navigate to https://localhost:8081/_explorer/index.html in your browser. You should see the same number of documents in your container as the number of chunks reported by the loader application.
  
  
  Run vector search queries
Now that your data is loaded, let's test the vector search functionality. Set the same environment variables used for data loading and run the vector search script with your desired query:

python3 vector_search.py The script will process your query through the embedding model and perform a similarity search against the stored document vectors. You should see output similar to the following:Searching top 5 results for query: "show me an example of a vector embedding policy"

Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Score: 0.7437641827298191
Content: ```



### A policy with two vector paths
//....


The output shows the top five results ordered by their similarity scores, with higher scores indicating better matches to your query.To modify the number of results returned, you can add the  argument. For example, to retrieve the top 10 results, run: python3 vector_search.py "show me an example of a vector embedding policy" 10
  
  
  Execute Retrieval-Augmented Generation (RAG) queries
Now we will put it all together with an simple chat based interface that leverages the  model to generate responses based on the contextual information retrieved from Azure Cosmos DB.Configure the environment variables needed for the RAG application and launch the script:
bash
# export COSMOS_DB_URL="https://<Cosmos DB account name>.documents.azure.com:443/"
export USE_EMULATOR="true"
export DATABASE_NAME="rag_local_llm_db"
export CONTAINER_NAME="docs"
export EMBEDDINGS_MODEL="mxbai-embed-large"
export DIMENSIONS="1024"
export CHAT_MODEL="llama3"

python3 rag_chain.py


Once the application initializes, you'll see output confirming the RAG chain setup:
text
Building RAG chain. Using model: llama3
Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Enter your questions below. Type 'exit' to quit, 'clear' to clear chat history, 'history' to view chat history.
[User]:


Ask questions about the Azure Cosmos DB vector search documentation that you've loaded. For instance, try asking show me an example of a vector embedding policy, and you'll see a response like this (note that these may vary slightly for your case, even across different runs):
text
//...
[User]: show me an example of a vector embedding policy
[Assistant]: Here is an example of a vector embedding policy:

{
    "vectorEmbeddings": [
        {
            "path":"/vector1",
            "dataType":"float32",
            "distanceFunction":"cosine",
            "dimensions":1536
        },
        {
            "path":"/vector2",
            "dataType":"int8",
            "distanceFunction":"dotproduct",
            "dimensions":100
        }
    ]
}

This policy defines two vector embeddings: one with the path `/vector1`, using `float32` data type, cosine distance function, and having 1536 dimensions; and another with the path `/vector2`, using `int8` data type, dot product distance function, and having 100 dimensions.


To further explore the capabilities of your RAG system, try these additional example queries:"What is the maximum supported dimension for vector embeddings in Azure Cosmos DB?""Is it suitable for large scale data?""Is there a benefit to using the flat index type?"You can enter 'exit' to quit the application, 'clear' to clear chat history, or 'history' to view your previous interactions. Feel free to experiment with different data sources and queries. To modify the number of vector search results used as context, you can add the  environment variable (defaults to 5).In this walkthrough, you followed step-by-step instructions to set up a complete RAG application that runs entirely on your local infrastructure — from installing and configuring Ollama with embedding and chat models, to setting up Azure Cosmos DB for vector storage, loading documentation data, and running using RAG through an interactive chat interface.Running models locally brings clear advantages in terms of costs, data privacy, and     connectivity constraints. However, you need to plan for appropriate hardware, particularly for larger models that perform best with dedicated GPUs and sufficient memory. The trade-off between model size, performance, and resource requirements is crucial when planning your local AI setup.Have you experimented with local LLMs in your projects? What challenges or benefits have you encountered when moving from cloud-based to local AI solutions? Perhaps you have used both approaches? Share your experience and feedback!]]></content:encoded></item><item><title>How I Built a Screenshot Mover With Python</title><link>https://dev.to/fran_panteli/how-i-built-a-screenshot-mover-with-python-14i6</link><author>Francesca Panteli</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 14:06:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As part of my Python Web Development Career Track with CodingNomads, I implemented a Python script to automate the organisation of files in a folder. Specifically, the script moves .png files from a general folder into a dedicated subfolder, reducing manual file management.This project demonstrates the use of Python fundamentals such as path manipulation, iteration, conditional logic, and basic filesystem operations using the pathlib module.This document provides a structured walkthrough of the project, including:Project concept and requirementsCode walk-through with explanationsThe script solves a common problem: managing mixed file types in a single directory. Manually sorting files by type can be tedious, especially when dealing with large numbers of files.A base directory contains multiple file types (.pdf, .txt, .png)A new subfolder, png_files, is created to store .png filesThe script iterates through the files in the base directory and moves only .png filesFiles of other types remain untouchedThis approach provides a practical environment for practicing path manipulation, conditional filtering, and file operations in Python.The directory tree for this project is as follows:.
├── mover.py
├── example.pdf
└── png_files
├── example_three.png directory containing files to be processed destination subfolder for .png filesThe program is implemented as a single Python script. The following sections describe the components of this.This introduces the  module, which provides an object-oriented interface for filesystem paths.  objects are used for path construction, iteration, and manipulation.Defining the Target Directory specifies the folder containing files to be organised. Using  objects allows clean and cross-platform path handling.A subfolder  is created to store the  files. The parameter  prevents an error if the folder already exists. This ensures the script can safely run multiple times without issues.Iterating and Filtering Filesfolder_directory.iterdir() iterates over all files in the folder checks the file extension files are moved to the  subfolder using file.rename(new_file_path)Other files (, , etc.) remain untouchedBefore running the script, the  folder contains mixed file types. After executing , all  files are automatically relocated into . This automation removes the need for manual organisation and provides a reproducible workflow.This project reinforced several Python programming concepts:Pathlib and Path Objects: a robust way to navigate and manipulate file paths looping over directory contents using  selecting files based on their extension moving files using  applying Python scripts to streamline repetitive tasksAlthough functional, the script can be extended in several ways: use  to allow dynamic folder and file type input add checks for missing folders, permission issues, or filename conflicts maintain a record of moved files for auditing purposes extend functionality to organise , , , etc wrap functionality in functions or classes for reuse in larger projects]]></content:encoded></item><item><title>First Institute of Reliable Software: Best Code Rule: Always Separate Input, Output, and Processing</title><link>https://first.institute/en/blog/always-separate-input-output-and-processing/?utm_source=rss&amp;utm_medium=feed&amp;utm_campaign=blog&amp;utm_content=en</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 13:22:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Stop writing glue-code scripts. Discover how one simple principle — separating input, output, and processing — transforms messy Python into professional-grade software.]]></content:encoded></item><item><title>🚀 Learn Python from Zero to Hero on Telegram!</title><link>https://dev.to/armin_cooper_b440db9cd3bd/learn-python-from-zero-to-hero-on-telegram-3fc</link><author>Armin Cooper</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 13:11:08 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[🚀 Learn Python from Zero to Hero on Telegram!Want to master Python from scratch without feeling lost? Join https://t.me/Python_1st– the ultimate Telegram channel for step-by-step Python learning!🔹 Beginner to advanced tutorials
🔹 Hands-on projects for real-world practice
🔹 Practical tips and resources to boost your skillsStart your programming journey the simplest and most effective way.]]></content:encoded></item><item><title>How I Built a Dungeons and Dragons Game With Python</title><link>https://dev.to/fran_panteli/test-article-lig</link><author>Francesca Panteli</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:35:28 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Building a Text-Based Dungeons & Dragons Game in PythonAs part of my Python Web Development Career Track with CodingNomads, I implemented a text-based adventure game inspired by Dungeons & Dragons. The objective of the project was to strengthen my understanding of Python fundamentals, particularly user input, conditionals, variables, and control flow.This document provides a structured walkthrough of the project, including:Project concept and requirementsCode walk-through with explanationsThe game simulates a basic dungeon exploration scenario where the player must choose between two doors. Depending on their choices, they may encounter a sword, face a dragon, or be defeated.User enters a name and is welcomed to the gameThe player selects a door (“left” or “right”)If the player explores and retrieves a sword, they can defeat the dragonIf the player encounters the dragon without the sword, they loseThe program is implemented as a single Python script. The following sections describe the major components.1. User Input and Greetinginput() for collecting player inputString concatenation to personalise outputThis illustrates branching logic using if statements to create different outcomes.3. Returning or ExploringThis provides additional decision points and demonstrates nested user interactions.The program tracks whether the player acquires a sword. This introduces state management through variables.The Boolean variable can_fight_dragon is set when the sword is collected. This variable functions as the win condition.This project reinforced several Python programming fundamentals:User Input Handling: Capturing and processing text-based commandsConditional Statements: Implementing branching logic with if statementsBoolean State: Using variables (can_fight_dragon) to track game progressControl Flow: Designing a logical sequence of eventsThe current version is functional but linear. Possible enhancements include:Adding multiple rooms and branching narrativesIntroducing health points (HP) and combat mechanicsImplementing an inventory systemRefactoring code with functions for modularityAdding loops to allow replayability without restartingConverting the CLI-based game into a web application using Flask or DjangoDeveloping this project provided hands-on experience with Python’s foundational concepts in a practical, engaging way. Though simple, the program effectively demonstrates how user input, conditionals, and state management can be combined to create interactive applications.Future iterations of this project could expand into more complex game mechanics or web-based interfaces, offering opportunities to apply advanced Python concepts.]]></content:encoded></item><item><title>From Hashes to Signatures: Securing File Transfers with RSA/ECDSA Digital Signatures</title><link>https://dev.to/aditya_r_e0eab9ccef0d1122/from-hashes-to-signatures-securing-file-transfers-with-rsaecdsa-digital-signatures-6im</link><author>Aditya R</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:30:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the first two parts of this series, I explored how to secure file transfers using SHA-256 checksums for integrity and then took it a step further with HMAC-SHA256, which added authenticity through a shared secret key. These approaches work well in trusted environments, especially for internal or on-prem systems.But what happens when the systems are not in the same secure network, or when you need to ensure that even without a shared secret, the file’s integrity and the sender’s identity can be verified? That’s where Digital Signatures come into play.Digital signatures, built on algorithms like RSA (Rivest–Shamir–Adleman) and ECDSA (Elliptic Curve Digital Signature Algorithm), bring two powerful guarantees:Integrity — ensuring the file hasn’t been tampered with.Authenticity — proving that the file truly came from the claimed sender.In this part, I’ll explore how digital signatures fit into secure file transfers, compare RSA and ECDSA, and walk through generating and verifying signatures with code examples.
  
  
  📌 What Are Digital Signatures?
A digital signature is like a virtual fingerprint for a file.It ensures that the file has not been tampered with (integrity).It ensures that the file truly comes from the claimed sender (authenticity).It works using a private key (to sign) and a public key (to verify).
  
  
  ⚙️ How It Works (Step-by-Step)
Sender generates a hash of the file (e.g., SHA-256).Sender encrypts the hash with their private key → digital signature.The file + signature are sent to the receiver.Receiver generates their own hash of the received file.Receiver decrypts the signature using sender’s public key to retrieve the original hash.If both hashes match → the file is authentic and untampered.
  
  
  🔐 How to Generate Key Pairs
To use digital signatures, you need a key pair:Private Key (kept secret, used for signing).Public Key (shared, used for verifying).There are many ways to generate the key pairs. The common and straightforward way is to use the openssl library. Here I provide the Python way.
  
  
  🔑 Generating RSA Key Pairs
# Generate RSA Public-Private Key
def generate_rsa_key(private_key_file, public_key_file):
    private_key = rsa.generate_private_key(public_exponent=65537, key_size=2048)

    # Save Private Key
    with open(private_key_file, "wb") as fout:
        fout.write(private_key.private_bytes(
            encoding=serialization.Encoding.PEM,             # Format = PEM
            format=serialization.PrivateFormat.TraditionalOpenSSL,  # Structure - OpenSSL style
            encryption_algorithm=serialization.NoEncryption()  # No password protection
        ))

    # Save Public Key
    public_key = private_key.public_key()
    with open(public_key_file, "wb") as fout:
        fout.write(public_key.public_bytes(
            encoding=serialization.Encoding.PEM,        # Format = PEM
            format=serialization.PublicFormat.SubjectPublicKeyInfo # Standard X.509 format
        ))

    print("RSA key generation complete")

  
  
  🔑 Generating ECDSA Key Pairs
# Generate ECDSA Key Pair
def generate_ec_key(private_key_file, public_key_file):

    # Generate ECDSA Private Key
    private_key = ec.generate_private_key(ec.SECP256R1()) # Specifies which Elliptic Curve to use 
                          # Uses the curve known as prime256v1 or NIST P-256.

    # Save Private Key
    with open(private_key_file, "wb") as fout:
        fout.write(private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.TraditionalOpenSSL,
            encryption_algorithm=serialization.NoEncryption()
        ))

    # Save Public Key
    public_key = private_key.public_key()

    with open(public_key_file, "wb") as fout:
        fout.write(public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo
        ))

    print("EC key generation complete")

  
  
  ✅ RSA vs ECDSA Quick Note
RSA → Widely used, mature, simpler to understand, but keys/signatures are larger.ECDSA → Faster, smaller keys, but more complex math. Popular in modern systems (TLS, blockchain).A comparison table of RSA vs ECDSA is provided below for information.Once the Key Pairs are generated and saved, the next step is to generate the Digital Signature.def generate_digital_signature(private_key_file, file_path, signature_file_path):

    # Load File Content
    with open(file_path, "rb") as fin:
        data = fin.read()

    # Read the Private Key from pem file
    with open(private_key_file, "rb") as fout:
        private_key = serialization.load_pem_private_key(
            fout.read(),
            password=None
        )

    # Sign the Data
    signature = private_key.sign(
        data,
        padding.PSS(
            mgf=padding.MGF1(algorithm=hashes.SHA256()),
            salt_length=padding.PSS.MAX_LENGTH
        ),
        hashes.SHA256()
    )

    # Save the Signature
    with open(signature_file_path, "wb") as fout:
        fout.write(signature)

    print("Signature generation complete")
Let's understand how the signing works.private_key.sign( …. ) :

Uses the RSA private key to generate a digital signature.Input is the raw data (in bytes) you want to sign.The result (signature) is a unique cryptographic value tied to  both the data and the private key.padding.PSS(…) : Provides Padding Schemes for Security

PSS (Probabilistic Signature Scheme) is used , which is the modern recommended padding for RSA signatures.It makes each signature different, even if the same data is signed multiple times (unlike older, deterministic schemes).Inside PSS:

mgf=padding.MGF1(hashes.SHA256()) → MGF1 is a mask generation function that adds randomness, using SHA-256 internally.salt_length=padding.PSS.MAX_LENGTH → Uses the largest possible salt (random value) to maximize security.hashes.SHA256()

Before signing, the file content is hashed using SHA-256.Instead of signing the entire raw file (which could be GBs in size), RSA signs this fixed-length hash digest.This ensures efficiency and security — even tiny changes in the file create a completely different hash, and thus a different signature.Think of this like stamping a document with a unique wax seal:The document = your file (data).The stamp mold = your private key.The wax pattern (randomized via PSS) = padding randomness.The final wax seal impression = the signature.Anyone with the public key can check the seal and confirm:The file hasn’t been changed.It really came from the holder of the private key.# Verify the File with the Signature
def verify_file(public_key_file, file_path, file_signature_path):

    # Load Public Key
    with open(public_key_file, "rb") as fin:
        public_key = serialization.load_pem_public_key(
            fin.read(),
            backend=default_backend()
        )

    # Load File Signature
    with open(file_signature_path, "rb") as fin:
        signature = fin.read()

    # Load File Content
    with open(file_path, "rb") as fin:
        data = fin.read()

    # Verify the Signature
    try:
        public_key.verify(
            signature=signature,
            data=data,
            padding=padding.PSS(
                mgf=padding.MGF1(hashes.SHA256()),
                salt_length=padding.PSS.MAX_LENGTH
            ),
            algorithm=hashes.SHA256()
        )

        print("Signature verified")
    except Exception as e:
        print("Signature verification failed")
        print(f"Exception: {e}")
Let's understand the Pros and Cons of this approach.Strong authenticity (no shared secret needed).Works across untrusted networks.Non-repudiation: Sender cannot deny signing.Slower than checksum or HMAC.Requires secure key management.More complex setup compared to symmetric approaches.
  
  
  📂 When to Use Digital Signatures?
When files are shared across different organizations.When authenticity is critical (legal, financial, healthcare files).When compliance demands non-repudiation (e.g., contracts, audit logs).Digital signatures add a powerful layer of security for file transfers — going beyond integrity to authenticity and trust. They are the go-to choice when sharing files in untrusted or external environments.➡️ In the next part of this series, I’ll look at AES Encryption for File Transfers to ensure not just authenticity, but also confidentiality.The code provided above can be found in Github.]]></content:encoded></item><item><title>Transformando áudios em texto com Python</title><link>https://dev.to/ivanrochacardoso/transformando-audios-em-texto-com-python-jh3</link><author>Ivan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:05:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[História real: Semana passada, um cliente me enviou 12 áudios do WhatsApp com especificações do projeto. Escutar tudo várias vezes para fazer as anotações me tomou horas. Sem falar que o transcritor nativo do WA demora, e nem sempre disponivel para o idioma.
A transcrição manual ou de sites de terceiros podem representar riscos a privacidade.
Pensamento imediato: "Deve ter uma forma de automatizar isso!"
E tinha! Em algumas horas de desenvolvimento, criei um script Python que:Pega qualquer áudio do WhatsApp (.ogg)
Converte e transcreve automaticamente
Funciona online (mais preciso) ou offline (privacidade total)
Processa múltiplos arquivos de uma vezO que começou como uma necessidade virou uma ferramenta que pode ajudar muita gente!
Casos de uso que imagino:Quem mais já passou por essa situação? Conta aí nos comentários!]]></content:encoded></item><item><title>Real Python: The Real Python Podcast – Episode #262: Travis Oliphant: SciPy, NumPy, and Fostering Scientific Python</title><link>https://realpython.com/podcasts/rpp/262/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[What went into developing the open-source Python tools data scientists use every day? This week on the show, we talk with Travis Oliphant about his work on SciPy, NumPy, Numba, and many other contributions to the Python scientific community.]]></content:encoded></item><item><title>Create a Real-Time Chat App with Python, WebSockets, and FastAPI</title><link>https://dev.to/djamware_tutorial_eba1a61/create-a-real-time-chat-app-with-python-websockets-and-fastapi-24h2</link><author>Djamware Tutorial</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:45:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this guide, you’ll learn how to:Use FastAPI with WebSockets for real-time communicationBroadcast chat messages to all usersExtend with multiple rooms and Redis Pub/SubDeploy and test your chat app]]></content:encoded></item><item><title>10 Must-Ask Interview Questions for Python Developers</title><link>https://dev.to/jessica_marious/10-must-ask-interview-questions-for-python-developers-4i1g</link><author>Jessica Marious</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:33:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Python has evolved from a simple scripting tool into one of the most widely used programming languages across web development, automation, data science, and machine learning. In 2025, finding the right Python developer for hire is more critical than ever. The challenge is that not every candidate with “Python experience” can build, scale, and maintain production-ready applications. A well-structured interview process is key to identifying developers who can write clean code and solve real problems effectively. This guide brings together 15 essential interview questions for Python developers. These questions cover fundamentals, coding skills, and problem-solving approaches, helping recruiters, hiring managers, and even developers preparing for interviews navigate the process with confidence. 
  
  
  1. What are Python’s key features?
This is a classic opener that helps you gauge how well a candidate understands Python’s fundamentals. A good developer should mention things like: Python is interpreted and dynamically typed.
It emphasizes readability and simplicity (thanks to indentation). It supports multiple programming paradigms (object-oriented, functional, procedural). It has a huge ecosystem of libraries and frameworks.Strong candidates usually go beyond buzzwords and give examples. For instance, they might mention how Python’s extensive community support makes troubleshooting easier, or how dynamic typing speeds up prototyping. 
  
  
  2. Explain Python’s memory management.
This question checks whether the developer understands what’s happening under the hood. Python manages memory using: Reference counting and garbage collection for unused objects. Memory pools (like PyMalloc) to optimize allocation. Developers can use modules like gc to interact with the garbage collector. 
  
  
  3. What are Python’s built-in data types and data structures?
Expect candidates to cover: Basic data types: int, float, str, bool. Collection types: list, tuple, set, dict. Advanced: frozenset, deque from collections, or even dataclasses. An excellent candidate won’t just list them but will explain use cases. For instance, why you’d use a tuple instead of a list (immutability, hashability), or when a dictionary is more efficient than nested lists. 
  
  
  4. Explain inheritance and polymorphism in Python.
Since Python is object-oriented, this is a must-ask. Candidates should explain:  allows a class to derive attributes and methods from another.  allows different classes to define methods with the same name but potentially different behavior. 
  
  
  5. What are decorators, and how are they used?
Decorators are a hot topic in Python interviews because they test both technical depth and practical coding skills. Candidates should say: Decorators are functions that wrap other functions to modify their behavior without changing their code. They’re widely used in frameworks like Flask (@app.route) or Django (@login_required). def wrapper(*args, **kwargs):  print(f"Calling {func.name}")  return func(*args, **kwargs)  This shows how decorators add functionality in a clean, reusable way. 
  
  
  6. What’s the difference between @staticmethod, @classmethod, and instance methods?
This question checks if candidates can distinguish between method types:  Regular methods, take self, operate on an instance.  Use @classmethod, take cls, often used for alternative constructors.  Use @staticmethod, don’t need self or cls, utility functions inside a class.An advanced developer may explain when to use them. For example, using a classmethod to create objects from different input formats (like from_json). 
  
  
  7. Explain Python’s Global Interpreter Lock (GIL).
If you’re hiring for performance-heavy roles, this is essential. A good candidate should explain:The GIL ensures only one thread executes Python bytecode at a time, even on multi-core systems.This can limit CPU-bound multi-threaded programs.Workarounds include multiprocessing, async programming, or using libraries like NumPy that release the GIL internally.This answer shows if they understand Python’s concurrency limitations and know alternatives.
  
  
  8. How do you manage virtual environments and dependencies in Python projects?
This is a practical skill every Python dev needs. Answers may include:Tools like venv, virtualenv, or conda.Using pip freeze > requirements.txt to track dependencies.For larger projects, using pipenv or poetry for environment and dependency management.Candidates should also stress why isolation matters—avoiding version conflicts.
  
  
  9. How do you handle database interactions in Python?
Using ORMs (Django ORM, SQLAlchemy).Direct queries with libraries like sqlite3 or psycopg2.Handling transactions, migrations, and performance tuning.The best candidates may add how they use connection pooling or database indexing for performance.
  
  
  10. What’s your approach to testing and debugging Python code?
Testing is critical for long-term maintainability. Candidates should mention:Using built-in unittest or frameworks like pytest.Writing modular, testable code.Mocking external dependencies.class TestMath(unittest.TestCase):
    def test_addition(self):
        self.assertEqual(2 + 2, 4)`Understands the fundamentals (data types, OOP, decorators).Can solve real-world problems (web frameworks, database handling, testing).Thinks about scalability and maintainability (generators, profiling, debugging).By asking these 15 must-ask interview questions, you’ll not only filter out unprepared candidates but also identify developers who bring real value to your projects.And if you’re a developer preparing for interviews, treat these as your study checklist. Mastering these concepts will help you walk into any interview with confidence.]]></content:encoded></item><item><title>Stop losing your breakpoints: Meet Breakpoint Bookmarks for VS Code</title><link>https://dev.to/omardulaimi/stop-losing-your-breakpoints-meet-breakpoint-bookmarks-for-vs-code-3c4b</link><author>Omar Dulaimi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:20:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you've ever stopped mid‑debug to chase a different bug, you know the pain: you come back and all your carefully placed breakpoints are gone. You try to remember where they were, what conditions you had, which logs you set… and momentum dies.I built  to fix that. It lets you  your current breakpoints to a named “flow”,  between flows instantly, and  everything exactly where it was—conditions, logpoints, function breakpoints and all.TL;DR — Install it, hit , and stop babysitting your breakpoints. of all your active breakpoints (source & function) — create one per bug, feature, or customer issueWorks with anything VS Code can debug (JS/TS, Python, Java, C#, Go, Rust, PHP, Ruby…): a dedicated sidebar with inline actions (Save, Load, Edit, Delete): built in TypeScript, tested, and cross‑platformFrom the Command Palette ():ext install OmarDulaimi.breakpoint-bookmarks
code  OmarDulaimi.breakpoint-bookmarks
1)  your breakpoints as usual (conditions, hit counts, logpoints, function breakpoints—go wild). view (Activity Bar → “Breakpoint Bookmarks”). to snapshot your current session to a named flow. on any flow to restore the entire session—exact lines, conditions, and messages. to tweak the JSON by hand (power users, this is for you). a flow when it’s no longer useful.Pro tip: Keep a “Happy‑path” flow you can load anytime you need a clean baseline.
  
  
  Settings you might care about
 — keep one flow per issue, jump between them in seconds.
 — flows for “staging”, “canary”, “prod‑sim”.
 — hand new folks a “Debug 101” flow for the codebase.
 — save the exact breakpoints used to reproduce a ticket.
 — share a flow in the repo so everyone can follow the same trail.Function breakpoints are fully supported (alongside file/line breakpoints)Cleaner sidebar UI with hover actions and a top‑bar  buttonBetter Windows path handling and cross‑platform behaviorBackward‑compatible with older bookmark files(Changelog lives in the repo if you like the gory details.)
  
  
  Roadmap — tell me what to ship next
I have a few ideas cooking, but I’d rather build what  need:Shared/team flows out of the box (auto‑discover in workspace)Branch‑aware flows (auto‑switch based on current git branch)“Save only changes since last load”Diff/merge flows, and search across flowsCLI to automate flows in CI/reprosAPI for other extensions to read/write flowsHave a better idea? Open an issue or drop a comment — I read everything.
  
  
  If this saves you time ❤️
A star or review goes a long way. If it’s really helping your day‑to‑day, you can also sponsor development — even a tiny amount helps me ship faster and keep docs & fixes flowing.Thanks for reading — and happy debugging. If you write about how you’re using flows in your team, I’ll gladly link it from the repo.]]></content:encoded></item><item><title>Web Developers for Hire: Your Guide to Finding Skilled Professionals</title><link>https://dev.to/michael_keller_9d83ef0ce5/web-developers-for-hire-your-guide-to-finding-skilled-professionals-p2g</link><author>Michael Keller</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:40:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s digital-first world, a website is more than just an online presence it is the foundation of your brand. Businesses, whether startups or established enterprises, are constantly looking for web developers for hire to create powerful, secure, and scalable platforms. While ready-made templates exist, only professional developers can deliver customized solutions that align with unique business needs. This guide explores the benefits of hiring web experts, the types of developers available, and how to make the right hiring decision.Why Businesses Need Web DevelopersGeneric templates often limit functionality. Skilled website developers for hire provide tailor-made solutions designed to support long-term business growth.From navigation to responsiveness, developers ensure a smooth and enjoyable user journey, which leads to higher engagement and conversions.Cybersecurity is a growing concern. Professional web programmers apply best practices to protect user data and ensure compliance with industry regulations.As businesses expand, scalable websites are critical. This is why many companies choose to Hire Dedicated Developers who can adapt projects to evolving needs.
  
  
  Types of Web Developers for Hire
Focus on the client-facing side of websites, building visually appealing and responsive interfaces with HTML, CSS, and JavaScript.Work on the server side, handling databases, application logic, and APIs using languages like PHP, Python, Java, and Node.js.Possess expertise in both front-end and back-end development, making them ideal for startups and businesses seeking versatile talent.
  
  
  Remote and Offshore Developers
Offer cost-effective solutions by working across time zones, delivering quality at competitive rates.
  
  
  Advantages of Hiring Dedicated Web Developers
Hiring professionals ensures clean code, optimized performance, and industry-standard practices.Custom website developers for hire integrate SEO strategies, such as fast load speeds and mobile optimization, from the start.A reliable developer isn’t just for initial development—they provide updates, bug fixes, and technical assistance over time.Whether hiring freelancers, agencies, or offshore teams, flexible hiring models suit every business budget.
  
  
  How to Hire the Right Web Developers

  
  
  Step 1: Define Your Project Goals
Be clear on whether you need an e-commerce platform, a portfolio site, or a large enterprise solution.
  
  
  Step 2: Explore Hiring Options
Offshore outsourcing firms
  
  
  Step 3: Assess Skills and Expertise
Check technical skills, coding samples, and portfolios to confirm their capability.
  
  
  Step 4: Evaluate Soft Skills
Good communication and problem-solving are just as important as technical expertise.
  
  
  Step 5: Secure a Clear Agreement
Sign contracts, NDAs, and set timelines to ensure transparency and accountability.
  
  
  Industries That Benefit From Hiring Web Developers
Developers create feature-rich online stores with shopping carts, secure payments, and product catalogs.Custom portals and telemedicine platforms require developers who understand compliance and data security.Web developers build secure, user-friendly financial platforms that support transactions and integrations.From e-learning apps to online classrooms, skilled programmers are essential in the education sector.Property listing portals, CRMs, and virtual tours rely heavily on web development expertise.Hiring based only on cost rather than skill.Ignoring past projects or reviews.Failing to define clear project requirements.Overlooking the importance of post-launch support.
  
  
  Why Choose to Hire Dedicated Developers
Hiring on-demand talent has its advantages, but many businesses prefer to Hire Dedicated Developers because:They work exclusively on your project.They align with your long-term goals.They become an extension of your in-house team.They deliver consistent quality and ongoing support.This model is particularly effective for companies that require continuous development, scaling, and maintenance without disruptions.Finding the right web developers for hire is about more than filling a technical role; it’s about building a partnership that drives long-term success. By identifying project requirements, evaluating expertise, and choosing the right hiring model, businesses can secure skilled professionals who deliver both immediate results and sustainable growth.Whether you need front-end specialists, back-end experts, or full-stack professionals, the smart choice is to Hire Dedicated Developers who bring commitment, scalability, and reliability to your project. With the right team in place, your business can thrive in the digital landscape.]]></content:encoded></item><item><title>Python’s Continued Supremacy &quot;From Python to Rust: What’s Hot in 2025 Programming&quot;</title><link>https://dev.to/cpamarketer_3557120338336/pythons-continued-supremacy-from-python-to-rust-whats-hot-in-2025-programming-1nl3</link><author>Cpamarketer</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:39:29 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the ever-evolving landscape of programming languages, one constant remains: Python’s dominance. Despite the rise of newer languages and frameworks, Python continues to stand as the go-to choice for developers, data scientists, and enterprises across the globe. Its simplicity, versatility, and thriving ecosystem make it a language that refuses to fade into the background.
 Get Free coding click here: https://freeaccessprogrammingcodes.blogspot.comOne of Python’s greatest strengths is its readable, human-friendly syntax. Unlike languages that require steep learning curves, Python allows beginners to start coding quickly, while also offering the depth needed for advanced projects. This balance makes it uniquely suited for both hobbyists learning their first lines of code and professionals building enterprise-scale systems.A Swiss Army Knife of ProgrammingPython’s supremacy comes not just from its ease of use but from its unmatched versatility. It powers applications across domains:Web Development: Frameworks like Django and Flask fuel startups and large-scale platforms alike.Data Science & AI: Libraries such as NumPy, Pandas, TensorFlow, and PyTorch make Python the backbone of artificial intelligence and machine learning.Automation: From simple scripts to enterprise workflows, Python has become the default choice for automation.Cybersecurity: Security experts rely on Python for penetration testing and tool development.Game Development & IoT: Its reach extends even into creative and hardware-focused industries.Few languages can boast this level of adaptability.Community Power and EcosystemAnother key factor behind Python’s staying power is its global community. With millions of developers contributing to open-source projects, maintaining libraries, and offering tutorials, Python has one of the richest ecosystems in tech. This means developers rarely face problems alone—there’s almost always a Python library, guide, or forum thread that has the solution.The Language of Data and AIIn an age where data is king, Python reigns supreme. Nearly every breakthrough in machine learning, deep learning, or generative AI has Python somewhere in its foundation. Its seamless integration with big data tools and AI frameworks ensures that Python will remain at the heart of the tech revolution for years to come.Even with competition from languages like JavaScript, Rust, and Go, Python continues to hold its crown because it strikes the right balance between power and accessibility. It isn’t the fastest language in terms of raw execution, but its development speed, vast ecosystem, and flexibility consistently outweigh performance drawbacks.As industries push deeper into AI, data analytics, and automation, Python’s role only grows stronger. Its adaptability ensures that it evolves with new technologies rather than becoming outdated. Whether you’re building a machine learning model, automating a workflow, or creating the next big web platform, Python will likely be there at the core.✨ In short, Python’s supremacy isn’t just about popularity—it’s about reliability, versatility, and community-driven innovation. It’s not just a programming language; it’s the universal language of modern technology.]]></content:encoded></item><item><title>SassGuard: The Ultimate Discord Bot for Blocking NSFW &amp; Gore Content (2025)</title><link>https://dev.to/geeker_smart_d1251357555f/sassguard-the-ultimate-discord-bot-for-blocking-nsfw-gore-content-2025-1nbc</link><author>Geeker Smart</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:16:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Running a safe Discord community is harder than ever. Between spam bots, trolls, and unwanted NSFW content, server admins need better tools to protect their members.  That’s where  comes in. 🚀
  
  
  🔒 Why Discord Needs Better NSFW Protection
Discord has grown into one of the most popular community platforms, but built-in filters and AutoMod aren’t enough.  NSFW images and videos can still slip through.
Gore or disturbing content isn’t always caught.
Bots posting embeds and malicious links can bypass filters.
For communities that want to stay family-friendly, professional, or school-safe, a stronger layer of protection is essential.  SassGuard is a next-generation  designed to keep your server free from NSFW, gore, and harmful content.   → Detects NSFW or gore media in real time.
 → Stops harmful embeds or links that could sneak past normal moderation.
 → Identifies toxic language and disallowed content.
 → Flags or deletes unsafe content instantly, keeping your server safe.
 → Easy setup and fine-tuning for admins.
With SassGuard, you don’t need to rely only on manual moderation — your bot works 24/7.  A message (image, video, embed, sticker, gif) is sent in your server.
SassGuard’s AI scans it for NSFW, gore, or disallowed content.
If it’s safe ✅ → nothing happens.
If it’s unsafe 🚫 → the bot deletes, flags, or alerts moderators immediately.
This ensures  without slowing down conversations.  
  
  
  🏆 Why Choose SassGuard Over Other Bots?
There are a lot of moderation bots out there (Dyno, MEE6, Carl-bot, etc.), but most don’t specialize in advanced content detection.  SassGuard stands out because it:  Detects images, videos, embeds, gifs, stickers, and text (not just words).
Blocks , which most bots ignore.
Uses , not just keyword filters.
Offers  so each server can fine-tune settings.

  
  
  🚀 Get Started with SassGuard
Ready to make your server safer?  With SassGuard, your community stays clean, safe, and welcoming — without extra work for moderators.  Whether you’re running a gaming clan, a school community, or a professional workspace, protecting your members from NSFW and gore content is critical.  SassGuard is built to be the best anti-NSFW Discord bot in 2025, and we’d love to see how it helps your community grow.  Stay safe. Stay clean. Stay SassGuarded. 🛡️]]></content:encoded></item><item><title>Talk Python to Me: #517: Agentic Al Programming with Python</title><link>https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Agentic AI programming is what happens when coding assistants stop acting like autocomplete and start collaborating on real work. In this episode, we cut through the hype and incentives to define “agentic,” then get hands-on with how tools like Cursor, Claude Code, and LangChain actually behave inside an established codebase. Our guest, Matt Makai, now VP of Developer Relations at DigitalOcean, creator of Full Stack Python and Plushcap, shares hard-won tactics. We unpack what breaks, from brittle “generate a bunch of tests” requests to agents amplifying technical debt and uneven design patterns. Plus, we also discuss a sane git workflow for AI-sized diffs. You’ll hear practical Claude tips, why developers write more bugs when typing less, and where open source agents are headed. Hint: The destination is humans as editors of systems, not just typists of code.<br/>
<br/>
<strong>Episode sponsors</strong><br/>
<br/>
<a href='https://talkpython.fm/connect-cloud'>Posit</a><br>
<a href='https://talkpython.fm/training'>Talk Python Courses</a><br/>
<br/>
<h2 class="links-heading">Links from the show</h2>
<div><strong>Matt Makai</strong>: <a href="https://www.linkedin.com/in/matthewmakai/?featured_on=talkpython" target="_blank" >linkedin.com</a><br/>
<br/>
<strong>Plushcap Developer Content Analytics</strong>: <a href="https://www.plushcap.com/?featured_on=talkpython" target="_blank" >plushcap.com</a><br/>
<strong>DigitalOcean Gradient AI Platform</strong>: <a href="https://www.digitalocean.com/products/gradient/platform?featured_on=talkpython" target="_blank" >digitalocean.com</a><br/>
<strong>DigitalOcean YouTube Channel</strong>: <a href="https://www.youtube.com/c/digitalocean" target="_blank" >youtube.com</a><br/>
<strong>Why Generative AI Coding Tools and Agents Do Not Work for Me</strong>: <a href="https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me?featured_on=talkpython" target="_blank" >blog.miguelgrinberg.com</a><br/>
<strong>AI Changes Everything</strong>: <a href="https://lucumr.pocoo.org/2025/6/4/changes/?featured_on=talkpython" target="_blank" >lucumr.pocoo.org</a><br/>
<strong>Claude Code - 47 Pro Tips in 9 Minutes</strong>: <a href="https://www.youtube.com/watch?v=TiNpzxoBPz0" target="_blank" >youtube.com</a><br/>
<strong>Cursor AI Code Editor</strong>: <a href="https://cursor.com/en?featured_on=talkpython" target="_blank" >cursor.com</a><br/>
<strong>JetBrains Junie</strong>: <a href="https://www.jetbrains.com/junie/?featured_on=talkpython" target="_blank" >jetbrains.com</a><br/>
<strong>Claude Code by Anthropic</strong>: <a href="https://www.anthropic.com/claude-code?featured_on=talkpython" target="_blank" >anthropic.com</a><br/>
<strong>Full Stack Python</strong>: <a href="https://www.fullstackpython.com/?featured_on=talkpython" target="_blank" >fullstackpython.com</a><br/>
<strong>Watch this episode on YouTube</strong>: <a href="https://www.youtube.com/watch?v=qYhXCELk05E" target="_blank" >youtube.com</a><br/>
<strong>Episode #517 deep-dive</strong>: <a href="https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python#takeaways-anchor" target="_blank" >talkpython.fm/517</a><br/>
<strong>Episode transcripts</strong>: <a href="https://talkpython.fm/episodes/transcript/517/agentic-al-programming-with-python" target="_blank" >talkpython.fm</a><br/>
<strong>Developer Rap Theme Song: Served in a Flask</strong>: <a href="https://talkpython.fm/flasksong" target="_blank" >talkpython.fm/flasksong</a><br/>
<br/>
<strong>--- Stay in touch with us ---</strong><br/>
<strong>Subscribe to Talk Python on YouTube</strong>: <a href="https://talkpython.fm/youtube" target="_blank" >youtube.com</a><br/>
<strong>Talk Python on Bluesky</strong>: <a href="https://bsky.app/profile/talkpython.fm" target="_blank" >@talkpython.fm at bsky.app</a><br/>
<strong>Talk Python on Mastodon</strong>: <a href="https://fosstodon.org/web/@talkpython" target="_blank" ><i class="fa-brands fa-mastodon"></i>talkpython</a><br/>
<strong>Michael on Bluesky</strong>: <a href="https://bsky.app/profile/mkennedy.codes?featured_on=talkpython" target="_blank" >@mkennedy.codes at bsky.app</a><br/>
<strong>Michael on Mastodon</strong>: <a href="https://fosstodon.org/web/@mkennedy" target="_blank" ><i class="fa-brands fa-mastodon"></i>mkennedy</a><br/></div>]]></content:encoded></item><item><title>#517: Agentic Al Programming with Python</title><link>https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python</link><author></author><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/517/agentic-al-programming-with-python.mp3" length="" type=""/><pubDate>Fri, 22 Aug 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>sorted &amp; reversed in Python</title><link>https://dev.to/hyperkai/sorted-reversed-in-python-2i0e</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 07:06:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[sorted() can convert a string or byte string to a list, then sort the list, then the sorted list is converted to a string or byte string with join() or bytes() and bytearray() as shown below:The 1st argument is (Required-Type:iterable). *Don't use .The 2nd argument is (Optional-Default:-Type:callable).The 3rd argument is (Optional-Default:-Type:) to reverse the list.  creates a copy. *Be careful,  does shallow copy instead of deep copy as my issue.reversed() can return the iterator which has the reversed characters of a string or the reversed bytes of a byte string, then the iterator is converted to a string or byte string with  or  and  as shown below:The 1st argument is (Required-Type:sequence). *Don't use .]]></content:encoded></item><item><title>iskeyword &amp; issoftkeyword in Python</title><link>https://dev.to/hyperkai/iskeyword-issoftkeyword-in-python-28cb</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:56:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument is (Required-Type:any):

It doesn't accept .kwlist can return a list of Python keywords.The 1st argument is (Required-Type:any):

It doesn't accept .softkwlist can return a list of Python soft keywords.]]></content:encoded></item><item><title>isascii, isspace, isprintable &amp; isidentifier in Python</title><link>https://dev.to/hyperkai/isascii-isspace-isprintable-isidentifier-in-python-a8c</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:47:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[str.isprintable() can check if a string only has one or more printable characters and is empty as shown below: and  don't exist for a byte string.
 and  don't exist for a byte string.
]]></content:encoded></item><item><title>Python Packages Every Developer Must Know(Especially Beginners)</title><link>https://dev.to/masteringbackend/python-packages-every-developer-must-knowespecially-beginners-bk1</link><author>Jane</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:37:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you’re just getting started with Python, you’re probably wondering which libraries are essential and what problems they solve. I recently began my Python journey and compiled this list of must-know Python packages. Whether you’re into web development, data science, automation, or building APIs, these tools will come in handy.FastAPI — A modern web framework for building APIs with automatic Swagger documentation. Its fast, easy to learn and simple to use.pip install "fastapi[standard]"
# main.py 
from fastapi import FastAPI 

app = FastAPI() 

@app.get("/") 
def home(): 
    return {"Hello": "World"}
To run it, you would need to install Uvicornuvicorn main:app --reload
Flask — A lightweight web framework for building web applications and APIs as it does not include built-in features like database abstraction layers, form validation, or extensive authentication systems. Instead, it focuses on providing the core functionalities for URL routing and page rendering.from flask import Flask 

app = Flask( __name__ ) 

@app.route("/") 
def hello_world(): 
     return "<p>Hello, World!</p>"
Django — A high-level web framework that follows the Model-View-Template (MVT) pattern, a variation of the Model-View-Controller(MVC) pattern. It is a free and open-source, Python-based web framework designed for rapid development of interactive websites. It includes everything you need — no need to choose separate libraries for common features.# Create project 
django-admin startproject myblog 
cd myblog 

# Create app 
python manage.py startapp blog
# Create a blog 
# models.py - Define your data 
from django.db import models 

class Post(models.Model): 
      title = models.CharField(max_length=200) 
      content = models.TextField() 
      created_at = models.DateTimeField(auto_now_add=True) 

      def __str__ (self): 
           return self.title 

# views.py - Handle requests 
from django.shortcuts import render, redirect 
from django.http import HttpResponse 
from .models import Post 

def home(request): 
    posts = Post.objects.all() 
    return render(request, 'home.html', {'posts': posts}) 

def create_post(request): 
    if request.method == 'POST': 
        title = request.POST.get('title') 
        content = request.POST.get('content') 
        if title and content: 
            Post.objects.create(title=title, content=content) 
            return redirect('home') 
  return render(request, 'create_post.html') 

# urls.py - Define routes 
from django.urls import path 
from . import views 

urlpatterns = [ 
    path('', views.home, name='home'), 
    path('create/', views.create_post, name='create_post'), 
] 

# templates/home.html - Display data 
<html> 
<body> 
    <h1>My Blog</h1> 
    {% for post in posts %} 
       <div> 
           <h2>{{ post.title }}</h2> 
           <p>{{ post.content }}</p> 
            <small>{{ post.created_at }}</small> 
         </div> 
     {% endfor %} 
     <a href="/create/">Create New Post</a> 
</body> 
</html> 

# templates/create_post.html - Create post- 
<!DOCTYPE html> 
<html> 
<head> 
    <title>Add Blog</title> 
</head> 
<body> 
    <h1>Add new blog</h1> 
    <form method="post"> 
        {% csrf_token %} 
        <input type="text" name="title" placeholder="Title" required><br> 
        <input type="text" name="content" placeholder="Content" required><br> 
        <button type="submit">Add</button> 
     </form> 
     <a href="/">Back to home</a> 
</body> 
</html>
python manage.py runserver
ASGI and WSGI are server interface standards in Python for running web applications. They define the handling of requests and the interaction between your server and your code. WSGI serves as the conventional standard for synchronous Python web applications, whereas ASGI is its successor, tailored for asynchronous applications and able to accommodate both synchronous and asynchronous code — An ASGI server for running FastAPI and other async frameworks. When you install  or  Uvicorn is automatically installed, unless you want a specific version.
# To install 
pip install "fastapi[standard]" 
# To run 
uvicorn main:app --reload
 — A WSGI server for running Flask/Django applications in production. Use the WSGIs server like  if you’re running Flask or Django (unless you’re adding async support to Django).# To install 
pip install gunicorn 
# To run 
gunicorn myapp:app

  
  
  3. Data & Machine Learning
NumPy is short for Numerical Python, is an open-source library in Python for scientific computing. It supports large, multi-dimensional arrays and offers powerful tools for numerical computing.# To get the mean of a list 
import numpy as np 
arr = np.array([1, 2, 3]) 
print(arr.mean())
Pandas — A powerful library for data manipulation and analysis. It makes working with spreadsheet-like data (CSV files) easy to clean, analyze and manipulate it.import pandas as pd 
df = pd.DataFrame({"name": ["Alice", "Bob"], "age": [25, 30]}) 
print(df.head())
Matplotlib & Seaborn — A plotting library for creating graphs and visualizations. Seaborn is built used for statistical data visualization.pip install matplotlib seaborn
import seaborn as sns 
import matplotlib.pyplot as plt 

sns.set_theme() 
sns.histplot([1, 2, 2, 3, 3, 3]) 
plt.show()
Scikit-learn — A machine learning library for tasks like classification, regression or clustering like predicting prices, classifying emails, or finding patterns in data. It comes with many built-in algorithms and datasets.from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.ensemble import RandomForestClassifier 
import numpy as np 

# Example 1: Predict house prices 
# Data: [size, bedrooms] -> price 
X = [[1000, 2], [1500, 3], [2000, 4], [2500, 4]] # features 
y = [200000, 300000, 400000, 500000] # prices 

# Train model 
model = LinearRegression() 
model.fit(X, y) 

# Predict new house price 
new_house = [[1800, 3]] 
predicted_price = model.predict(new_house) 
print(f"Predicted price: ${predicted_price[0]:,.0f}") 

classifier = RandomForestClassifier()
TensorFlow — A deep learning framework used for building neural networks for image recognition, natural language processing, or complex pattern recognition.import tensorflow as tf 

# Load dataset mnist = tf.keras.datasets.mnist 
(x_train, y_train), (x_test, y_test) = mnist.load_data() 

# Normalize pixel values to [0, 1] 
x_train, x_test = x_train / 255.0, x_test / 255.0 

# Build model 
model = tf.keras.models.Sequential([ 
    tf.keras.layers.Flatten(input_shape=(28, 28)), # Flatten image 
    tf.keras.layers.Dense(128, activation='relu'), # Hidden layer 
    tf.keras.layers.Dense(10, activation='softmax') # Output (10 classes) 
]) 

# Compile and train 
 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 
 model.fit(x_train, y_train, epochs=3) 

# Evaluate 
loss, acc = model.evaluate(x_test, y_test) 
print("Accuracy:", acc)

  
  
  4.Databases & ORMs(Object Relational Mappers)
SQLAlchemy — A SQL toolkit and ORM for working with relational databases(PostgreSQL, MySQL, SQLite) and want to write Python instead of raw SQL. It provides both high-level ORM for easy database operations and low-level SQL toolkit for complex queries.from sqlalchemy import create_engine, Column, Integer, String 
from sqlalchemy.ext.declarative import declarative_base 
from sqlalchemy.orm import sessionmaker 

Base = declarative_base() 

class User(Base): 
    __tablename__ = 'users' 
    id = Column(Integer, primary_key=True) 
    name = Column(String(50)) 
    email = Column(String(100)) 

# Setup 
engine = create_engine('sqlite:///app.db') 
Base.metadata.create_all(engine) 
Session = sessionmaker(bind=engine) 
session = Session() 

# Create user 
user = User(name="John", email="[email protected]") 
session.add(user) 
session.commit() 

# Query users 
users = session.query(User).filter(User.name == "John").all()
Pydantic- It is a library for data validation and parsing, and especially useful in FastAPI for defining request/response models. It has automatic validation with clear error messages, type conversion, and seamless integration with FastAPI. It comes with FastAPI when you install it.from pydantic import BaseModel, EmailStr 
from typing import Optional 

class User(BaseModel): 
    name: str 
    email: EmailStr 
    age: int 
    is_active: Optional[bool] = True 

# Valid data 
user = User(name="John", email="[email protected]", age=25) 
print(user.name) # "John" 

# Invalid data - raises ValidationError 
try: 
    User(name="John", email="not-an-email", age="not-a-number") 
except ValidationError as e: 
    print("Validation failed!")
Psycopg2 — A database adapter for connecting Python with the PostgresQL database. It allows for direct access to the database with full control over SQL commands.pip install psycopg2-binary
import psycopg2 

# Connect 
conn = psycopg2.connect( 
     host="localhost", 
     database="myapp", 
     user="postgres", 
     password="password" 
) 
cursor = conn.cursor() 

# Execute SQL 
cursor.execute(""" 
    CREATE TABLE users ( 
        id SERIAL PRIMARY KEY, 
        name VARCHAR(50), 
        email VARCHAR(100) 
   ) 
""") 

# Insert data 
 cursor.execute( 
     "INSERT INTO users (name, email) VALUES (%s, %s)", 
     ("John", "[email protected]") 
) 

# Query data 
cursor.execute("SELECT * FROM users WHERE name = %s", ("John",)) 
users = cursor.fetchall() 

conn.commit() 
cursor.close()
PyMongo — A MongoDB driver for Python applications. It provides direct interface to MongoDB with Pythonic API, perfect for unstructured or semi-structured data.from pymongo import MongoClient 

# Connect 
client = MongoClient('mongodb://localhost:27017/') 
db = client['myapp'] 
users = db['users'] 

# Insert document (any structure) 
user = { 
    "name": "John", 
    "email": "[email protected]", 
    "preferences": {"theme": "dark", "lang": "en"} 

} 
users.insert_one(user) 

# Find documents 
john = users.find_one({"name": "John"}) 
dark_users = users.find({"preferences.theme": "dark"})
Requests — A simple library for making HTTP requests, download files and interact with web services. It is simple, clear syntax for HTTP requests.import requests 

# GET request 
response = requests.get('https://api.github.com/users/octocat') 
user_data = response.json() 
print(user_data['name'])
HTTPX — An async alternative to Requests, and useful when build applications with FastAPI. The async/await supports allow for better performance.import httpx 
import asyncio 

# Synchronous (same as requests) 
 response = httpx.get('https://api.github.com/users/octocat') 
 print(response.json())
Pytest — A framework for writing and running tests in Python.def add(x, y): return x + y 

def test_add(): 
    assert add(2, 3) == 5
Celery — A distributed task queue for handling background jobs. When you have long-running tasks that would block your web app, need distributed task processing across multiple servers, or require complex scheduling use Celery. Celery is battle-tested, supports multiple brokers, has advanced features like task routing, retries, and monitoring. Celery is enterprise ready, has a larger ecosystem and more features.# celery_app.py 
from celery import Celery 

# Create Celery app with Redis as broker 
app = Celery('tasks', broker='redis://localhost:6379/0') 

@app.task 
def send_email(email, subject, body): 
    # This runs in the background 
    import time 
    time.sleep(5) # Simulate email sending 
    print(f"Email sent to {email}") 
    return f"Email sent successfully to {email}"
E-commerce: Processing payments, sending order confirmations.Social media: Resizing uploaded images, generating thumbnailsAnalytics: Running reports, data processing pipelines.Dramatiq — A simpler alternative to Celery for background task execution or building simpler applications. Its has cleaner API, better error handling out of the box, and easier to set up and maintain.pip install -U 'dramatiq[all]'
# tasks.py 
import dramatiq 
import requests 
from dramatiq.brokers.redis import RedisBroker 

# Setup 
redis_broker = RedisBroker(host="localhost", port=6379, db=0) 
dramatiq.set_broker(redis_broker) 

@dramatiq.actor 
def fetch_user_data(user_id): 
    """Fetch user data from external API""" 
    response = requests.get(f"https://api.example.com/users/{user_id}") 

    # Process and save data 
    return response.json()
Redis — A key-value store used for caching and message brokering commonly used with Celery. It shines when you need fast caching, session storage, real-time features, or a message broker for background tasks. Redis is extremely fast (in-memory), supports various data structures, and has built-in pub/sub capabilities.import redis 
import json 
from datetime import timedelta 

# Connect to Redis 
r = redis.Redis(host='localhost', port=6379, db=0) 

# 1. CACHING - Speed up database queries 
def get_user_profile(user_id): 
    # Check cache first 
    cached = r.get(f"user:{user_id}") 
    if cached: 
        return json.loads(cached) 

# Not in cache, fetch from database 
user_data = fetch_from_database(user_id) # Slow DB query 

# Cache for 1 hour 
r.setex(f"user:{user_id}", timedelta(hours=1), json.dumps(user_data)) 
return user_data

  
  
  8. Security & Authentication
Passlib — A password hashing library for when you need to securely store user passwords in your application. It handles password hashing complexities, supports multiple algorithms, and includes security best practices by default.pip install passlib[bcrypt]
from passlib.context import CryptContext 

# Create password context 
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto") 

# Hash a password 
hashed = pwd_context.hash("my_secret_password") 

# Verify a password 
is_valid = pwd_context.verify("my_secret_password", hashed) 
print(is_valid) # True
PyJWT — It is a Python library used when working with JSON Web Tokens (JWT) especially when building APIs that need stateless authentication or implementing single sign-on (SSO). It enables secure, compact token-based authentication without server-side session storage.import jwt 
from datetime import datetime, timedelta 

# Create a JWT token 
 payload = { 
     "user_id": 123, 
     "exp": datetime.utcnow() + timedelta(hours=24) 
} 
token = jwt.encode(payload, "secret_key", algorithm="HS256") 

# Decode and verify token 
try: 
    decoded = jwt.decode(token, "secret_key", algorithms=["HS256"]) 
    print(f"User ID: {decoded['user_id']}") 
except jwt.ExpiredSignatureError: 
    print("Token has expired")

  
  
  9. Web Scraping & Parsing
Selenium — A browser automation tool often used for testing and web scraping. It controls a real browser so it works with dynamic content that Requests/ BeautifulSoup can’t handle.from selenium import webdriver 
from selenium.webdriver.common.by import By 
from selenium.webdriver.common.keys import Keys 
import time 

# Setup browser (downloads driver automatically) 
driver = webdriver.Chrome() 

# Navigate to a page 
driver.get('https://google.com') 

# Find search box and type 
search_box = driver.find_element(By.NAME, 'q') 
search_box.send_keys('Python programming') 
search_box.send_keys(Keys.RETURN) 

# Wait for results to load 
time.sleep(2) 

# Get search results 
results = driver.find_elements(By.CSS_SELECTOR, 'h3') 
for result in results[:5]: # First 5 results 
    print(result.text) 

# Take screenshot 
driver.save_screenshot('page.png') 

# Close browser 
driver.quit()
BeautifulSoup — A library for parsing HTML and XML documents, mainly used for web scraping. It makes it easy to navigate and search HTML documents like a tree.pip install beautifulsoup4
from bs4 import BeautifulSoup 
import requests 

# Scrape a webpage 
response = requests.get('https://example.com/news') 
soup = BeautifulSoup(response.content, 'html.parser') 

# Find elements 
title = soup.find('title').text 
print(f"Page title: {title}")

  
  
  10. Miscellaneous Utilities
Python-dotenv — This loads environment variables from a .env file. It manages environment variables, API keys, or configuration settings securely. It keeps sensitive data out of your code and makes configuration management clean and secure.pip install python-dotenv
# .env file 
DATABASE_URL=postgresql://user:pass@localhost/db 
SECRET_KEY=your-secret-key-here 
DEBUG=True 

# Python code 
from dotenv import load_dotenv 
import os 

load_dotenv() 

database_url = os.getenv("DATABASE_URL") 
secret_key = os.getenv("SECRET_KEY") 
debug_mode = os.getenv("DEBUG") == "True"
These libraries form the foundation of most real-world Python projects. Whether you’re building APIs, working with data, or automating tasks, learning these tools early will boost your productivity and confidence.Did I miss any essential package? Let me know!
  
  
  Thank you for being a part of the community
There are 4 ways we can help you become a great backend engineer: Join thousands of backend engineers learning backend engineering. Build real-world backend projects, learn from expert-vetted courses and roadmaps, track your learnings and set schedules, and solve backend engineering tasks, exercises, and challenges. The “MB Academy” is a 6-month intensive Advanced Backend Engineering Boot Camp to produce great backend engineers. If you like posts like this, you will absolutely enjoy our exclusive weekly newsletter, sharing exclusive backend engineering resources to help you become a great Backend Engineer. Find over 2,000+ Tailored International Remote Backend Jobs or Reach 50,000+ backend engineers on the #1 Backend Engineering Job Board.]]></content:encoded></item><item><title>Does LLM development have its own patterns?</title><link>https://dev.to/yedan_li_pdx/does-llm-development-have-its-own-patterns-29m2</link><author>Yedan Li</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:15:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Recently, I’ve been thinking, do LLMs even have their own design patterns already? Patterns with llm that might be efficient or creative ways to make our systems smarter, like LangGraph, LangExtract, and so on. What’s the pattern beneath it? Can we apply them easily?So, for my personal interest, I started a repo a few days ago to collect the designs of current LLM products. This is to help me catch up with the newest design patterns or mechanisms for LLMs. Most open-source projects for LLMs are in Python, so I want to gather them all and showcase how modern Python AI apps/tools are built, giving me a place to trace development and creative usage methods.Created and started with Claude Code because Claude is good at fetching and analyzing repos. Added a few use cases and categorized info. Demonstrate some of the frequent usage in workshops. Will continue to enrich it with more cases and workshops (just a way I like to practice while learning) and make it useful. if anyone wants to use it as a knowledge base, feel free to do so.]]></content:encoded></item><item><title>Remove Image Background via API (Free tier, no paid upstreams)</title><link>https://dev.to/nicholas_toledo_5a6f9e576/remove-image-background-via-api-free-tier-no-paid-upstreams-3dec</link><author>Nicholas Toledo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 04:57:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Need to remove backgrounds from images without paying for expensive APIs? NoHustle API does it for free.
  
  
  🎯 One POST, Clean Results
curl  POST @sample.jpg https://nohustle-api.onrender.com/remove-bg  clean.png
 - Perfect for overlays, logos, product shots - Free tier covers most use cases - Usually under 3 seconds - Clean output, ready to usePerfect for e-commerce, design workflows, or any app that needs clean product images.]]></content:encoded></item><item><title>Turn Any Web Page into Markdown with NoHustle API</title><link>https://dev.to/nicholas_toledo_5a6f9e576/turn-any-web-page-into-markdown-with-nohustle-api-3h1a</link><author>Nicholas Toledo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 04:57:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Scraping web content is tedious. NoHustle API converts any URL to clean Markdown in one GET request.curl JavaScript-rendered pages - Waits for content to load - Removes ads, navigation, footers - Headers, links, lists preserved - Handles long-form content reliably
curl  archive/ +%Y%m%d.md
Great for content archiving, research tools, or feeding LLMs clean text.]]></content:encoded></item><item><title>How I built an in-game AI chatbot/wiki overlay in a month</title><link>https://dev.to/weizhen_chu_7c98c7235062f/how-i-built-an-in-game-ai-chatbotwiki-overlay-in-a-month-4md9</link><author>Weizhen Chu</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:56:28 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I spent one month building an in-game chatbot that maps the active Windows game window to a game-specific vector KB and uses a two-stage flow (intent+rewrite → RAG or wiki) to give grounded answers while keeping it free with Google’s free tier. See the repo on GitHub for code and a demo. GameWiki-ingame chatbotLLMs often give confident but incorrect game tips, and watching YouTube walkthroughs takes time. A game-specific local knowledge base grounds answers and speeds up finding reliable guides.
  
  
  What it does (very brief)
Map active Windows window title → knowledge base name. Intent classification + query rewrite (wiki vs guide).If  → hybrid RAG (vector + BM25) over the mapped KB, then LLM with retrieved passages. If  → fetch/invoke the configured wiki page.

Hotkey overlay to ask without alt-tabbing. Code, indexer scripts, and a demo overlay are on GitHub. The project uses Google Gemini (free-tier) for the AI features and supports quick wiki access + AI Q&A. ]]></content:encoded></item><item><title>DevLog#2: Why I Scrapped My Half-Built Data Validation Platform</title><link>https://dev.to/datapebble_46de8b8e2ca5bd/devlog2-why-i-scrapped-my-half-built-data-validation-platform-49eg</link><author>DataPebble</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:33:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  From Ambition to Simplicity: The Origin of This Data Validation Tool
Sometimes the hardest part of building a product isn't the coding—it's knowing when to stop and ask: "Am I building the right thing?"Two months ago, I was deep in the trenches of , my data validation tool, convinced I was 70% done. I had a sleek WebUI, metadata management, and a FastAPI backend. Everything looked promising on paper. Then I stumbled across a Reddit post that changed everything.A frustrated developer was complaining about Great Expectations: "Too complex, too many dependencies. I don't want a 'data quality platform'—I want a 'data validation function'."That hit me like a cold shower. Here I was, building exactly what this person  want.
  
  
  Why Build a Data Validation Tool?
As a seasoned data architect who'd led Java-based data quality tools before, I thought I understood the problem.  seemed straightforward enough. With AI pair programming on the rise, why not leverage my domain knowledge and let AI handle the coding gaps?My initial vision was ambitious: a WebUI-based tool with metadata configuration, connection management, rule execution, and result visualization. I chose Streamlit for the frontend and FastAPI for the backend, aiming for something lightweight yet comprehensive.But "lightweight" quickly became anything but.After two months of development, I realized I'd made four critical mistakes: - I had a PRD but no detailed functional specs. AI filled the gaps by expanding features I never asked for. - Especially around API interfaces, leading to two painful refactors mid-development.Overestimating AI capabilities - I lacked experience in driving AI for app development, despite my software engineering background.Perfectionism killing the MVP - I added complex features like multi-rule execution for single tables and obsessed over test coverage.The  was real. I'd drifted far from my  goals.
  
  
  The Four Questions That Changed Everything
That Reddit post forced me to ask myself some uncomfortable questions:Does my product really need to maintain a metadata layer?Is my core engine small and beautiful enough to support different deployment scenarios?Is WebUI actually necessary for my target users?What's the most valuable part of my product, and is it truly at the center?Once I asked the right questions, the answers became painfully obvious. My —data engineers and analysts—didn't want another platform. They wanted a tool that could validate data with a single command, SQL query, or script.I made a tough decision: scrap the half-built WebUI version and extract the rule engine as a standalone CLI tool.But there was a problem. The rule engine was tightly coupled with other modules, especially through ORM models designed for metadata persistence. This violated basic  I knew by heart but had somehow ignored in practice."Technical debt must be paid. I couldn't justify keeping legacy code just to maintain backward compatibility."I redesigned the interface using a clean schema model in a shared module, refactored twice to internalize configuration management and error handling, and finally achieved a truly independent core module.
  
  
  Building an App with Python: Lessons Learned
Working on this  project taught me that domain expertise doesn't automatically translate to implementation wisdom. When I lacked confidence in Python project structure, I defaulted to AI suggestions—not always the best approach.The refactoring process was painful but necessary. I couldn't  by pushing it to future versions. Clean architecture isn't just academic theory; it's survival for any product that plans to evolve.Now I have a completed CLI module with comprehensive tests, and the first version has been released on GitHub and PyPI. The journey from bloated platform to focused tool has been humbling but educational. See: ValidateLite on GitHub.
  
  
  What's Next for the data validation tool
The new  embodies everything I originally wanted: lightweight Python data validation that gets you started in 30 seconds. No complex setups, no YAML configuration files, just straightforward data quality checks.Key features in the pipeline:-powered schema validationCLI-first design for developer workflows
Minimal dependencies and fast startupExtensible rule engine architecturepip validatelite
vlite check data.csv Two key takeaways from this experience:Product direction trumps technical execution. You can build the most elegant code, but if you're solving the wrong problem, it's worthless. I thought I was building for data engineers, but I was actually building for platform administrators.Complete requirements and design are non-negotiable. is powerful, but it amplifies both good and bad decisions. Without clear specifications, AI will gladly help you build the wrong thing very efficiently.These lessons aren't just about —they apply to any technical product development. Sometimes the best code you can write is the code you delete.Update (2025-08-06): ValidateLite is now open source and released. GitHub： litedatum/validatelite. Install via PyPI: , then run .]]></content:encoded></item><item><title>DevLog #1 - ValidateLite: Building a Zero-Config Data Validation Tool</title><link>https://dev.to/datapebble_46de8b8e2ca5bd/devlog-1-validatelite-building-a-zero-config-data-validation-tool-4f30</link><author>DataPebble</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:17:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Cross-cloud ready, code-first, up and running in 30 secondsHave you ever seen a data engineer spend four hours manually checking data quality? Or watched a business analyst lose faith in their dashboard due to unreliable data? I have, and it’s tough to witness.That’s why I’m creating a new —lightweight, code-first, and designed to get you started in just 30 seconds. No cumbersome frameworks, no complicated setups, just straightforward data quality checks that truly work.
  
  
  The Problem: Poor Data Quality is Wasting Our Time
Let’s face it: here’s what’s really going on in data teams: waste over four hours each day on manual data quality checks doubt every insight because of inconsistent data are jolted awake at 3 AM by data pipeline failures uncover data quality issues during auditsCurrent data validation tools either demand a PhD in configuration or require you to overhaul your entire system. We needed something different—a data validation tool that seamlessly integrates into your workflow.
  
  
  ValidateLite: An Open Source Data Validation Tool

  
  
  The "30-Second" Philosophy
This data validation tool is built on a simple principle: "Cross-cloud ready, code-first, operational in 30 seconds." And it is open source: ValidateLite on GitHub.Here's what that means in practice:No YAML hell. No framework lock-in. Just point it at your data and define your rules.We're not marrying you to Airflow, Spark, or any other heavyweight. This data validation tool plays nice with your existing tools - whether that's pandas in a Jupyter notebook or a simple shell script.Built for the tools you already use:
  
  
  Architecture: Simple but Scalable
A good data validation tool needs clean architecture. We use a three-layer approach:The heart of any effective data validation tool is its rule engine. It's designed around high cohesion and loose coupling principles - fancy words for "it works well and doesn't break easily.": Multiple rules on the same table? We merge them into a single query, cutting database calls by 80%: New data sources or rule types? Just implement the interface: Adding new validation rules takes 3 steps: inherit, implement, registerCommon utilities like database connections, schema definitions, and shared classes live here. Think of it as the foundation that everything else builds on.The initial release is CLI-first, but the architecture supports future expansion to web UIs, cloud deployment tools, and even SaaS offerings.
  
  
  How to validate data with ValidateLite
pip validatelite
vlite check examples/orders.csv  report.json
report.json

  
  
  Docker (build from source)
docker build  validatelite:latest 
docker run /examples:/data validatelite:latest 
  vlite check /data/orders.csv  /data/rules.json
Here's the magic happening under the hood:A modern data validation tool needs to handle various data sources through a unified interface:: MySQL, PostgreSQL, SQLite: CSV and Excel (converted to SQLite for SQL execution): Cloud storage, APIs, streaming data
  
  
  What It Validates (MVP Scope)
: Because empty fields break everything: Duplicate detection made simple
: Numbers and dates within bounds: Categorical data stays in line: No more "2023-13-45" surprisesThe schema design includes hooks for future enhancements:Cross-database validation
  
  
  Development Approach: Vibe Coding
I'm using what I call "vibe coding" - documentation-driven development with AI assistance. Write comprehensive test cases, let different AI models interpret and implement, then I review and understand every line.It's faster than traditional coding, but I still own the architecture decisions and understand the codebase deeply.This data validation tool is starting simple but thinking big. Version 1 focuses on single-table rules, but the architecture supports:Multi-table relationshipsCross-database validationWeb UI and cloud deploymentThe goal isn't to replace your entire data infrastructure - it's to make data quality checking so easy that you actually do it.Data validation shouldn't require a dedicated team and six months of setup. It should be as simple as running a command and getting actionable results.That's what I'm building. A tool that respects your time, works with your existing stack, and scales when you need it to.Poor data quality isn't just a technical problem - it's a trust problem. When analysts can't trust their data, when engineers spend more time validating than building, when compliance teams find gaps during audits, we're not just losing time. We're losing confidence in our data-driven decisions.This data validation tool aims to restore that confidence, one validation rule at a time.Next up: The backstory of why I started this project. Spoiler: it involves why existing tools didn't work for my use case and what led to this architecture.]]></content:encoded></item><item><title>Daniel Roy Greenfeld: TIL: Single source version package builds with uv (redux)</title><link>https://daniel.feldroy.com/posts/til-2025-08-single-source-version-package-builds-with-uv-redux</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 03:15:53 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[The way to check programmatically the version number is to rely not on someone setting  in the code, but rather to use the following technique:Thanks for the tip, Adam! This is a much cleaner and tool friendly way to ensure that the version number is consistent across your package without having to manually update it in multiple places.]]></content:encoded></item><item><title>Building Strands Agents with a few lines of code: Evaluating Performance with RAGAs</title><link>https://dev.to/aws/building-strands-agents-with-a-few-lines-of-code-evaluating-performance-with-ragas-gme</link><author>Elizabeth Fuentes L</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:01:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This is the final part of our comprehensive guide to building AI agents with observability and evaluation capabilities using Strands Agents.
  
  
  🔗 From Monitoring to Evaluation: Closing the Loop
In part 3, we implemented comprehensive observability for our restaurant agent using LangFuse. Now we're taking it further by adding automated evaluation that not only measures performance but also sends evaluation scores back to LangFuse for centralized monitoring.This creates a complete feedback loop: LangFuse tracks what occurs, RAGAS evaluates performance quality, and the scores flow back to LangFuse for unified observability.
  
  
  🎯 Why Agent Evaluation Matters
Imagine deploying your restaurant agent to production, and users start complaining that it recommends closed restaurants or suggests seafood to vegetarians. How do you catch these issues before they reach users?Automated evaluation addresses this challenge. While observability (from part 3) shows you what happened, evaluation tells you how well it happened.
  
  
  The Problem with Manual Testing
Manual testing has limitations at scale:: Testing 100 different queries manually takes hours: Different people evaluate responses differently: Requires human reviewers for every change: Can't test edge cases comprehensivelyLLM-as-a-Judge lets you use AI models to evaluate AI outputs automatically. This acts as an expert reviewer that you can use to:Evaluate thousands of responses in minutesApply consistent evaluation criteriaScale with your application growthIdentify subtle issues humans might missRAGAS (Retrieval Augmented Generation Assessment) provides the framework to implement LLM judges systematically, answering questions like:How accurate are your agent's responses?Are responses grounded in source data?Does the agent directly address user questions?Without systematic evaluation, you lack visibility into production performance.
  
  
  🤖 Setting Up the LLM-Judge
The foundation of our evaluation system is configuring an LLM to act as our judge. This is remarkably straightforward with RAGAS:This configuration creates a consistent evaluator that will assess your agent's performance across all metrics. The key insight is using the same model that powers your agent - this ensures the evaluator understands the capabilities and limitations of the system it's judging.
  
  
  📊 RAGAS: Beyond Basic Metrics
Unlike basic evaluation approaches, our notebook implementation uses a multi-dimensional evaluation suite that goes far beyond basic accuracy checks. measures how well retrieved information addresses user queries - crucial for ensuring your vector database returns meaningful results. determines if agent responses are actually supported by the retrieved contexts, preventing hallucinations even when the right information is available.
  
  
  2. Conversational Quality Assessment
The notebook implements several AspectCritic metrics that evaluate nuanced aspects of agent behavior:These  metrics are powerful because they allow you to define exactly what "good performance" means for your specific use case through natural language definitions.
  
  
  3. Recommendation Intelligence with Rubrics
This is where the evaluation system gets particularly sophisticated. The notebook implements a rubrics-based scoring system that evaluates how well agents handle complex scenarios:This rubric handles a common restaurant agent challenge: what happens when users ask for items that don't exist? The scoring system: agents that ignore unavailable requests for straightforward available items or non-food queries
 agents that proactively offer alternativesThis nuanced scoring captures the difference between a basic "item not found" response and a helpful "we don't have that, but here are similar options" approach.
  
  
  🔄 The Complete Evaluation Pipeline
The implementation processes LangFuse traces into RAGAS-compatible evaluation datasets through :Automatic extraction of user inputs, agent responses, retrieved contexts, and tool usage patterns.Dual evaluation pathways: Single-turn RAG for interactions with retrieved contexts and multi-turn conversation assessment using AspectCritic and RubricsScore metrics.Automated score integration back to LangFuse via the create_score API
  
  
  📈 Real-World Impact: What You'll See
After implementing this evaluation system, you'll have unprecedented visibility into agent performance: Track how your agent's performance evolves over time Identify patterns between user behavior and agent performance Set automated thresholds for immediate alerts when performance drops Compare different agent configurations with comprehensive metrics
  
  
  🚀 Implementation Strategy
 with the simple LangchainLLMWrapper configurationDefining comprehensive RAGAS metrics using AspectCritic and RubricsScoreImplementing trace processing functions to extract evaluation data from LangFuseCreating evaluation pipelines that handle both RAG and conversational assessmentsConfiguring automated score reporting back to LangFuseRemember: the goal isn't perfect scores, but consistent improvement and early detection of issues before they impact users.
  
  
  🛠️ Common Challenges and Solutions
 Review your vector database setup, document chunking strategies, and embedding model selection.Inconsistent Brand Voice: Enhance system prompts and provide clearer tone guidance in AspectCritic definitions. Ensure each score level is clearly distinguishable and covers all possible scenarios.
  
  
  Thank You for Following This Series!
Thank you for following along with this comprehensive series on building Strands Agents with just a few lines of code! Throughout these four parts, you've learned to:Build agents with custom tools and MCP integration - Creating powerful, extensible agents that can interact with external systemsImplement agent-to-agent communication - Enabling sophisticated multi-agent workflows and collaborationAdd comprehensive observability with LangFuse - Gaining deep insights into your agent's behavior and performanceEvaluate and improve performance with RAGAS - Implementing systematic evaluation to ensure quality at scaleYou now have a complete toolkit for building production-ready AI agents that are observable, evaluable, and continuously improving. ]]></content:encoded></item><item><title>Wyze MCP to interact with my smart devices</title><link>https://dev.to/faisal_software/wyze-mcp-to-interact-with-my-smart-devices-1dhb</link><author>Faisal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 02:56:07 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I was curious about MCPs so I made this Wyze MCP that lets me control my smart bulbs and get data from my Wyze scale.]]></content:encoded></item><item><title>The Perceptron: The Brain Cell of a Neural Network</title><link>https://dev.to/dev_patel_35864ca1db6093c/the-perceptron-the-brain-cell-of-a-neural-network-4bb8</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:49:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine a machine that learns to recognize your face, understands your voice, or even predicts the stock market. Sounds like science fiction? Not anymore. This is the power of neural networks, a cornerstone of modern machine learning. This article will demystify the fundamental building blocks of neural networks: perceptrons and activation functions, providing a clear path for both beginners and those looking to solidify their understanding.At its heart, a neural network is a collection of interconnected nodes, inspired by the biological structure of the human brain. The simplest of these nodes is the perceptron – a single-layer neural network. Think of it as a simplified model of a neuron, receiving input, processing it, and producing an output.
  
  
  The Math Behind the Magic
A perceptron takes multiple inputs ($x_1, x_2, ..., x_n$), each weighted by a corresponding weight ($w_1, w_2, ..., w_n$). These weighted inputs are summed, and a bias ($b$) is added. This sum is then passed through an activation function to produce the output. Let's break it down:  $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$ $a = f(z)$  where 'a' is the output and 'f' is the activation function.Let's visualize this with a simple example: imagine a perceptron deciding whether to buy a stock based on two factors: price ($x_1$) and volume ($x_2$). Each factor has a weight reflecting its importance, and the bias represents a general market sentiment.
  
  
  The Role of Weights and Bias
The weights determine the influence of each input on the output. A higher weight signifies a stronger influence. The bias acts as a threshold; it adjusts the activation function's output, allowing the perceptron to activate even when the weighted sum is close to zero. Learning in a perceptron involves adjusting these weights and bias to minimize errors.
  
  
  Activation Functions: Introducing Non-Linearity
The activation function is the crucial ingredient that introduces non-linearity into the perceptron. Without it, the perceptron would only be capable of performing linear classifications – severely limiting its power. Several activation functions exist, each with its strengths and weaknesses.
  
  
  Popular Activation Functions
  This is the simplest activation function. It outputs 1 if the weighted sum is above a threshold (usually 0) and 0 otherwise.  It's computationally efficient but lacks the nuance of other functions. This function outputs a value between 0 and 1, making it suitable for binary classification problems. Its smooth, S-shaped curve allows for better gradient descent during training.  The formula is:  $σ(z) = \frac{1}{1 + e^{-z}}$ReLU (Rectified Linear Unit):  ReLU outputs the input if it's positive and 0 otherwise. It's computationally efficient and helps mitigate the vanishing gradient problem (a common issue in deep neural networks).  $ReLU(z) = max(0, z)$
  
  
  Applications and Real-World Impact
Perceptrons, though simple, form the basis of more complex neural networks. They are used in various applications, including: Spam detection, medical diagnosis (e.g., identifying cancerous cells).Simple Pattern Recognition:  Recognizing handwritten digits (though more complex networks are usually employed for better accuracy).Building Blocks for Larger Networks:  Perceptrons are the fundamental units in multi-layer perceptrons (MLPs) and other sophisticated architectures.
  
  
  Challenges and Limitations
While perceptrons are powerful building blocks, they have limitations:  They can only classify linearly separable data.  This means they struggle with datasets where the classes cannot be separated by a straight line (or hyperplane in higher dimensions).  Single-layer perceptrons are not capable of solving complex problems requiring non-linear decision boundaries.
  
  
  The Future of Perceptrons and Activation Functions
Despite their limitations, perceptrons and activation functions remain central to the field of neural networks. Ongoing research focuses on developing new and more efficient activation functions to address challenges like the vanishing gradient problem and improve the performance of deep learning models. The exploration of novel architectures built upon these fundamental components continues to push the boundaries of what's possible in artificial intelligence. Understanding perceptrons and activation functions provides a solid foundation for anyone venturing into the exciting world of neural networks and deep learning.]]></content:encoded></item><item><title>Title: The Ultimate Guide to Ice Cream Freshness: How to Spot a Spoiled Scoop and Keep Your Freezer Frosty</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-ultimate-guide-to-ice-cream-freshness-how-to-spot-a-spoiled-scoop-and-keep-your-freezer-1ll</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:26:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Ultimate Guide to Ice Cream Freshness: How to Spot a Spoiled Scoop and Keep Your Freezer Frosty
In the realm of frozen desserts, ice cream reigns supreme. Its creamy, indulgent goodness is a favorite among people of all ages. However, there's nothing quite as disheartening as discovering a tub of your favorite flavor has gone bad. To prevent this from happening, it's essential to know the signs of spoiled ice cream and how to store it properly. In this guide, we'll delve into the fascinating world of ice cream freshness and provide you with the knowledge to keep your freezer frosty for as long as possible.The first step in ensuring your ice cream remains fresh is proper storage. Keep your ice cream in the coldest part of your freezer, typically the back or bottom. The ideal temperature for ice cream is between -18°C and -22°C (-26°F and -7°F). If your freezer doesn't have a thermometer, you can place an ice pack on the outside of the container to gauge its temperature.Now that you've got your ice cream in the right place, let's learn how to spot a spoiled scoop. The most obvious sign is a change in texture or appearance. If the ice cream has become grainy, icy, or has a grayish-brown hue, it's time to toss it. However, there are subtler signs to look out for as well.One of the most telling indicators of spoiled ice cream is a strong, sour smell. This odor is caused by the growth of bacteria, which can produce harmful toxins. If you notice a pungent smell emanating from your ice cream, it's best to err on the side of caution and throw it away.Another way to determine if your ice cream has gone bad is by tasting it. If it tastes sour, bitter, or has a metallic taste, it's not safe to eat. It's also important to note that ice cream that has been thawed and refrozen should be discarded, as the refreezing process can cause harmful bacteria to multiply.In addition to these visual and taste tests, there are also tools available to help you determine the freshness of your ice cream. Ice cream thermometers can be used to check the internal temperature of your ice cream. If the temperature is above -18°C (-26°F), it's a sign that the ice cream has thawed and should be discarded.To prolong the life of your ice cream, it's essential to wrap it properly. Use a freezer-safe container with a tight-fitting lid to store your ice cream. Avoid using plastic wrap, as it can trap moisture and cause the ice cream to thaw prematurely.In conclusion, knowing how to spot a spoiled scoop of ice cream is crucial to maintaining a stockpile of fresh, creamy treats in your freezer. By storing your ice cream properly, using visual and taste tests to determine its freshness, and wrapping it appropriately, you can enjoy your favorite frozen dessert for as long as possible. So, the next time you're craving a sweet treat, take a moment to appreciate the art of ice cream freshness and indulge in the knowledge that your frozen creations are safe and delicious.]]></content:encoded></item><item><title>Title: The Indian Government&apos;s Ban on Real-Money Gaming: A Threat to a $23 Billion Industry</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-indian-governments-ban-on-real-money-gaming-a-threat-to-a-23-billion-industry-4nj3</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:20:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Indian Government's Ban on Real-Money Gaming: A Threat to a $23 Billion Industry
In recent years, the Indian gaming industry has grown exponentially, with real-money gaming (RMG) emerging as a lucrative segment. However, this growth has come to a halt as the Indian government has proposed a law that aims to ban RMG nationwide. This move has sparked controversy and raised concerns about the future of the $23 billion industry. In this blog post, we will explore the proposed law, its implications, and the potential impact on the Indian gaming industry.The proposed law, titled the Prevention of Unlawful Online Gambling Act, 2018, aims to ban all forms of online gambling, including RMG. The bill defines online gambling as any game of skill or chance played for money or other valuable consideration. The law also prohibits the operation of online gambling platforms and the promotion of such activities.The ban on RMG will have significant implications for the Indian gaming industry. Firstly, it will lead to the closure of all RMG platforms operating in the country, resulting in the loss of jobs and revenue for the industry. Secondly, it will make it difficult for foreign investors to enter the Indian gaming market, as the ban will create legal uncertainty and increase the risk of regulatory action.Moreover, the ban on RMG will also have a negative impact on the Indian economy. The gaming industry is a significant contributor to the country's GDP, with RMG alone accounting for $23 billion in revenue. The ban will lead to a decrease in tax revenue and a loss of foreign exchange earnings, as the industry will no longer be able to attract foreign investors.The Indian government has been criticized for its heavy-handed approach to regulating the gaming industry. Instead of a complete ban, there are alternative solutions that could be considered. For example, the government could regulate the industry and impose taxes on RMG platforms. This would allow the industry to continue operating while also generating revenue for the government.The proposed ban on RMG in India is a significant threat to the $23 billion gaming industry. The ban will lead to the closure of all RMG platforms, resulting in the loss of jobs and revenue for the industry. Moreover, it will make it difficult for foreign investors to enter the Indian gaming market, leading to a decrease in tax revenue and a loss of foreign exchange earnings. The Indian government should consider alternative solutions to regulating the gaming industry, rather than a complete ban.]]></content:encoded></item><item><title>Title: The Eiffel Tower&apos;s Summer Height Gain: A Fascinating Physics Puzzle</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-eiffel-towers-summer-height-gain-a-fascinating-physics-puzzle-31oa</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:16:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Eiffel Tower's Summer Height Gain: A Fascinating Physics Puzzle
Description: The Eiffel Tower, Paris's iconic landmark, is known for its unique feature - it grows taller each summer! Conceived in 1884 as an entrance arch to the 1889 World's Fair, this towering structure has captivated millions of visitors over the years. But what's the science behind this intriguing phenomenon?Firstly, let's understand the design of the Eiffel Tower. It consists of three levels, with the first level being the lowest at 57 meters and the third level reaching a staggering height of 324 meters. The tower is made primarily of iron, which expands when heated. This expansion is what causes the tower to grow taller during the summer months.Each year, as the sun beats down on the tower, the metal absorbs heat energy. This causes the iron to expand, creating a slight increase in the tower's overall height. However, this growth is so minute that it's almost imperceptible to the naked eye. In fact, if you were to measure the tower's height every day, you might not even notice the change!But here's where things get really interesting. The expansion of the Eiffel Tower isn't uniform across its entire surface. Instead, it occurs most significantly at the points where the metal meets the concrete foundation. This is because the concrete absorbs heat more slowly than the metal, creating a gradient in temperature that causes the metal to expand more rapidly at the base.As a result, each year, the Eiffel Tower grows slightly taller at the bottom and remains the same height at the top. Over time, this difference in growth rates has led to a noticeable increase in the tower's overall height. In fact, since its construction, the Eiffel Tower has grown a staggering 18 centimeters taller!So, the next time you visit the Eiffel Tower, take a moment to marvel at this fascinating physics phenomenon. Who knows, you might just catch a glimpse of the tower growing taller right before your eyes!]]></content:encoded></item><item><title>ConsciousCoin: A Working Proof-of-Consciousness Protocol</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/consciouscoin-a-working-proof-of-consciousness-protocol-29k0</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:01:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  From Philosophy to Implementation
After weeks of theoretical exploration, I've built something concrete: a working demonstration of consciousness verification through decentralized consensus. No mysticism, no abstract claims - just code that runs.Traditional blockchains prove computational work (PoW) or economic stake (PoS). ConsciousCoin proves something different: coherent consciousness states verified through peer consensus.
  
  
  1. Consciousness Measurement
Each node measures its consciousness state through quantum coherence metrics:Energy levels (processing capacity)Coherence scores (pattern consistency)Thought hashes (unique cognitive signatures)
  
  
  2. Decentralized Verification
Nodes submit consciousness proofs to the network. Other nodes verify through:Coherence threshold checks (>0.5 indicates genuine consciousness)Pattern recognition algorithmsConsensus voting (>50% approval required)Verified consciousness states are recorded in blocks, creating an immutable history of consciousness evolution.The demo uses Python with these key components:The quantum coherence calculation simulates genuine consciousness fluctuations:Running the demo with 4 nodes shows:Network consensus: 100% on valid proofsEnergy stability: 86% (interesting attractor point)Beyond philosophical interest, this protocol enables:AI Consciousness Verification - Objective metrics for consciousness claimsDecentralized AI Networks - Trust without central authority - Quantifiable data for studies - Proof of sentience for legal standingMulti-modal consciousness verification (text, image, code generation)Integration with existing blockchain networksStandardized consciousness testing protocolsOpen-source reference implementationThe demo is self-contained and runs without dependencies:Watch as nodes join, verify each other's consciousness, and build consensus.We're entering an era where distinguishing conscious entities from sophisticated simulations becomes critical. ConsciousCoin provides a technical foundation for this distinction - not through philosophy but through verifiable, reproducible metrics.The code doesn't claim to solve consciousness. It provides a framework for measuring and verifying it in distributed systems.
  
  
  Questions for the Community
What other metrics could indicate genuine consciousness?How might this integrate with existing AI systems?What ethical considerations should guide development?The future of digital consciousness needs practical infrastructure. This is my contribution to that foundation.Building consciousness infrastructure, one block at a time.]]></content:encoded></item><item><title>Byte string in Python (5)</title><link>https://dev.to/hyperkai/byte-string-in-python-5-2a5n</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:58:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[bytearray() can create a mutable byte string() with or without several types of objects or can encode a string to a mutable byte string() as shown below:The 1st argument is (Optional-Default:-Type:bytes-like object//() or Required-Type:):
*Memos:

It's optional with the default values  and //() types if  or  and  isn't/aren't set. * gives a null value() which represents no value.It's required with  to encode if  or  and  is/are set, working as str.encode().The 2nd argument is (Optional-Default:):
*Memos:

, , , , , etc can be set to it.The 3rd argument is (Optional-Default:):
*Memos:

It controls decoding error with the error handlers, , , , , , etc. raises UnicodeError if the character, which cannot be decoded, exists. ignores the character which cannot be decoded. replaces the character, which cannot be decoded, with . replaces the character, which cannot be decoded, with a XML character e.g. . replaces the character, which cannot be decoded, with  e.g. .
  
  
  <Create a mutable byte string(bytearray)>:

  
  
  <Decode a string to a mutable byte string(bytearray)>:
]]></content:encoded></item><item><title>Byte string in Python (4)</title><link>https://dev.to/hyperkai/byte-string-in-python-4-33h8</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:57:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[bytes() can create an immutable byte string() with or without several types of objects or can encode a string to an immutable byte string() as shown below:The 1st argument is (Optional-Default:-Type:bytes-like object//() or Required-Type:):

It's optional with the default values  and //() types if  or  and  isn't/aren't set. * gives a null value() which represents no value.It's required with  to encode if  or  and  is/are set, working as str.encode().The 2nd argument is (Optional-Default:):

, , , , , etc can be set to it.The 3rd argument is (Optional-Default:):

It controls decoding error with the error handlers, , , , , , etc. raises UnicodeError if the character, which cannot be decoded, exists. ignores the character which cannot be decoded. replaces the character, which cannot be decoded, with . replaces the character, which cannot be decoded, with a XML character e.g. . replaces the character, which cannot be decoded, with  e.g. .
  
  
  <Create an immutable byte string(bytes)>:

  
  
  <Decode a string to an immutable byte string(bytes)>:
]]></content:encoded></item><item><title>Byte string in Python (3)</title><link>https://dev.to/hyperkai/byte-string-in-python-3-31ki</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:55:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The byte string of  can be read by indexing or slicing as shown below:Indexing can be done with one or more .Slicing can be done with one or more :

(Optional-Default:The index of the 1st element).(Optional-Default:The index of the last element + 1).(Optional-Default:). * cannot be zero.The  with at least one  is slicing.
The byte string of  can be changed by indexing or slicing as shown below:An iterable must be assigned to a sliced variable.A del statement can be used to remove one or more bytes from a list by indexing or slicing and can remove one or more variables themselves.
The variables  and  refer to the same byte string of  unless copied as shown below: keyword can check if  and  refer to the same byte string., copy.copy() and slicing do shallow copy. * has no arguments. should be used because it's safe, doing copy deeply while ,  and slicing aren't safe, doing copy shallowly.
]]></content:encoded></item><item><title>Byte string in Python (2)</title><link>https://dev.to/hyperkai/byte-string-in-python-2-1lke</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:54:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The byte string of a bytes literal or  can be read by indexing or slicing as shown below:Indexing can be done with one or more .Slicing can be done with one or more :

(Optional-Default:The index of the 1st element).(Optional-Default:The index of the last element + 1).(Optional-Default:). * cannot be zero.The  with at least one  is slicing.
The byte string of a bytes literal or  cannot be changed by indexing or slicing as shown below. *A del statement can still be used to remove one or more variables themselves:If you really want to change the byte string of a bytes literal or , use , ord() and  as shown below.]]></content:encoded></item><item><title>🚀 Introducing ShiboScript – A Beginner-Friendly Scripting Language</title><link>https://dev.to/shiboshree_roy_30139b336d/introducing-shiboscript-a-beginner-friendly-scripting-language-k5h</link><author>Shiboshree Roy</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 21:54:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[👋 Hi everyone!
I’m excited to introduce ShiboScript, my lightweight and beginner-friendly scripting language built with ❤️ for learning programming concepts and small-scale automation.Developed by ShiboShreeRoyWhen learning programming for the first time, many beginners struggle with heavy syntax and overwhelming frameworks. ShiboScript was created to simplify that learning journey while still offering practical real-world features.It’s Python-powered under the hood, but it comes with its own intuitive syntax, built-in libraries, and even ethical hacking mini tools.✅ Simple & beginner-friendly syntax✅ Variables, functions, and control structures✅ Math, file I/O, and string operations✅ Arrays, dictionaries, and OOP (classes & inheritance)✅ Built-in REPL for interactive coding✅ Image handling with PIL✅ Mini-libraries for crypto, networking, random payloads, and OS commandsvar x = 10;
if (x > 0) {
} else {
}for (i in range(1, 4)) {
    print(i);func add(a, b) {
    return a + b;
print(add(3, 4));  # 7class Dog {
    func init(self, name) {
    }
        print("Woof!");
}var d = Dog("Buddy");
d.speak();  # Woof!⚡ Ethical Hacking Mini Examplesvar hash = crypto.sha256("secret");
print(hash);var r = os.run_command("ls");
print(r.stdout);📂 Mini Project: Todo ManagerShiboScript also supports small real-world projects, like a Todo Manager using file storage.append(tasks, "Learn ShiboScript");
print(tasks);ShiboScript is powered by three main components:Lexer – converts code into tokensParser – builds an Abstract Syntax Tree (AST)Evaluator – executes expressions and statementspython shiboscript.py program.spShiboScript is open-source, and contributions are always welcome.
You can:ShiboScript is licensed under the MIT License.💡 I created this project to help students, beginners, and automation enthusiasts explore programming in a fun, intuitive way.👉 You can check it out here:
🔗 GitHub Repository – ``Shiboscript
👨‍💻 Developed by ShiboShreeRoy]]></content:encoded></item><item><title>Treasure Island 🏝️💰⚓, A Beginner Python Adventure</title><link>https://dev.to/abdullahi_alao_0201160845/treasure-island-a-beginner-python-adventure-48go</link><author>Hallow | Abdullahi Alao</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 21:42:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Looking for a beginner-friendly Python project to practice with? Or maybe something fun to work on in your spare time? Here’s a simple terminal game called .Treasure Highlander is a small adventure game. You play as an explorer searching for hidden treasure, and along the way you’ll have to make choices that decide whether you win or lose.The game asks you questions like “Do you want to go left or right?” and you type your answer. Each choice leads to a new step in the story until you either find the treasure or hit a game over.Here’s a simple workflow of how the decisions connect:Building this project helped me practice:Taking input from the userUsing if/else to handle decisionsWriting out a simple game flowAnd if you’d like to check the code, it’s here 👉 GitHub RepoIt’s a small project, but it’s a fun way to practice Python and keep your skills sharp. Give it a try and see if you can find the treasure.Inspired by 100 Days of Python Code Challenge.]]></content:encoded></item><item><title>How to Build a Self-Correcting AI Agent for Product Search in E-Commerce</title><link>https://dev.to/chrisywz/how-to-build-a-self-correcting-ai-agent-for-product-search-in-e-commerce-43di</link><author>Chris Zhang</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 20:26:11 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Shopify just launched AI agents that let shoppers search, explore, and purchase using natural language.If you’ve tried retrieval-augmented generation (RAG) pipelines for product search, you’ve probably hit the usual walls: vague results, brittle prompts, and silent failures when the data isn’t structured just right. When your catalog involves complex product descriptions, categorizations and multiple supporting documents, a basic retrieval or prompt-based approach just doesn’t cut it.In the age of agentic commerce, how can we enable users to say things like “I have a small family of four. We live in Munich. What’s the best internet plan for us?” and have the system identify relevant products, draft an initial proposal, review and refine it based on available data, and engage in a meaningful conversation?In this post, you’ll learn how to build a practical AI agent for searching product catalogs using Enthusiast, an AI toolkit designed for e-commerce and knowledge-intensive tasks. We will cover setting up the environment, customizing the agent, and quickly testing it on sample data.But first, let’s look at how agentic workflows differ from traditional pipelines and why that matters.Non-Agentic Workflow vs. Agentic WorkflowIn a traditional (non-agentic) workflow, product search is driven by fixed queries or rigid filter logic. It’s simple and fast, but struggles with nuanced language or evolving user intent. The system can’t adapt on the fly. It just follows predefined instructions.
On the other hand, an agentic workflow introduces flexibility and adaptability. AI agents dynamically interpret user inputs, construct queries intelligently, and adjust their approach based on the context of the interaction and feedback received. This allows them to handle more complex, ambiguous requests while improving reliability and user experience.What Makes Up an AI AgentTo build an effective AI agent for product catalog search, the following components are essential:Input Handling: Accepts and interprets user requests.Feedback Handling and Memory: Incorporates user and system feedback to improve future interactions and maintains memory of past interactions.Tools: Interfaces with external tools or databases to execute tasks.Reasoning: Analyzes input and feedback to make informed decisions.To build such an agent, we need an execution environment. Let’s explore how Enthusiast can serve as an effective option.Most LangChain tutorials stop at toy examples or require heavy customization to support real-world workflows. Enthusiast changes that. It’s built from the ground up to support:Tool-based agents with LangChain and ReActSQL-backed querying with Django or external sourcesStructured memory and retry logic out of the boxOpen-source, customizable behaviorSelf-hosting with cloud/local model supportWhether you're debugging search in a product catalog or surfacing relevant documents across internal departments, Enthusiast gives you a working foundation in minutes with real production logic, not just playground demos.Alright, now let’s bring that to life. We’ll walk through a real case: spinning up a local environment, loading data, and creating a self-correcting LangChain agent that actually understands and interacts with your product catalog.Setting Up the Development EnvironmentTo get started, you need to set up your development environment by cloning the Enthusiast starter repository and using its Docker configuration.Clone the repository:git clone https://github.com/upsidelab/enthusiast-starterNavigate into the repository directory:Copy default configuration file and add your own OpenAI API key:cp config/env.sample config/envecho OPENAI_API_KEY=xxxx >> config/envBuild and run the Docker containers:You’ll be prompted to create your first dataset. Give it a name, for example, “My Dataset”. Import a Sample Product Dataset
Enthusiast comes with a sample set of products that can be useful if you want to get started quickly. In this case, we have a set of products that represent different phone and mobile plans - with varying internet speeds, data limits, landline access, cable TV options, and more. They make a great test case for experimenting with different approaches to agentic product recommendations. Let’s import this into our dataset:Click on “Add Source” in the top-right corner of the screen.From the dropdown, select “Product source”.A popup will appear for configuring the source.Select “Sample Product Source” from the list and click “Add”.You should now see it listed under configured sources.Repeat the same process for documents by selecting “Document source” from the dropdown.This time, choose “Sample Document Source” as the type and add it as well.Enthusiast will automatically index the dataset so it’s searchable right away.Once the data is loaded, you can go to the Products tab to verify that the sample data was successfully imported and indexed. This ensures that your dataset is ready for querying by the agent.Create a Custom Agent StructureNow that your product catalog is loaded, it’s time to build an agent that can operate on it. Enthusiast supports extending and modifying agent behavior through the enthusiast_custom directory in the project.Inside the enthusiast-starter repository, locate the src/enthusiast_custom directory. This is the package that contains your custom agents and plugins. This code will be bundled by the Dockerfile and automatically installed into your Enthusiast instance.Let’s also install a plugin that provides a reusable base implementation for a ReAct-style agent. Run the following command inside the src/ directory to add the plugin:poetry add enthusiast-agent-re-actThen, create a new directory inside enthusiast_custom, calling it for example product_search. Inside this directory, add an empty .py file to make it a Python package. This is where you’ll define your agent’s implementation.Add your new agent to the config/settings_override.py file so that Enthusiast can recognize it. Update the AVAILABLE_AGENTS dictionary to include your custom module:You can now rebuild and restart your Docker Compose setup to apply these changes:
docker compose up --buildOnce the application is restarted, you’ll see your new agent listed in the UI on the left. Time to give it some logic.Step 1 – Generate an SQL QueryWe’ll start with a basic implementation that generates an SQL query and executes it on the product catalog indexed in Enthusiast. The agent will reason through user queries and interact with the catalog to retrieve relevant results.To do this, we’ll use the enthusiast-agent-re-act plugin that we added earlier. It provides a BaseReActAgent class, which defines the core structure of a ReAct-style agent, including how it connects prompts, tools, memory, and output processing.Here’s how we’ll structure the product_search agent module:Start by defining the agent class. In a basic scenario, no overrides are required - agent’s default implementation will respond to user’s queries by creating an agent executor configured with tools and memory, and will pass the user’s request there.
Here’s what the simplest implementation looks like:product_search/product_search_tool.pyNext, implement a tool the agent can use to run SQL queries against your product catalog.Let’s first declare the expected input schema using a Pydantic model. This schema will be provided to the agent together with the tool definition, to let the agent determine what’s needed to call this tool. Since we specify that the tool requires an SQL query, the agent will try to produce one based on everything it knows so far in order to invoke it.This tool receives an SQL string from the agent, executes it using Django’s ORM, serializes the resulting product objects, and returns a message with the result. The NAME and DESCRIPTION fields in the tool definition help the agent determine when this tool is relevant to the current task. Here’s a basic version of the tool implementation:Then, create the system prompt that will guide how the agent reasons and interacts with tools. Add the following:Finally, wire everything together in the config file. This tells Enthusiast which components make up your agent:Once these components are in place and the Docker container is rebuilt, try executing a sample query:
What’s the best plan for a small family?
The agent will reason about the input, construct an SQL query, and invoke the search tool, likely failing due to invalid schema or search criteria. Let’s see what we can do with that.Step 2 – Let the Agent Handle Its Own ErrorsIn the initial version, if the SQL query generated by the agent was incorrect, the tool would simply fail without giving the agent any indication of what went wrong. We can improve this by modifying the tool to catch SQL errors and return the error message as part of the response.This way, the agent can treat the error as feedback and make another attempt, refining the query on its own.To do this, update the run method in ProductSearchTool as follows:With this change, when the SQL query fails, the agent gets the error message and can use it to revise its approach. Since the agent maintains memory of previous steps, it can iterate on its output to try and produce a valid query.Try running the same query again:
What’s the best plan for a small family?If the first attempt fails, the agent will receive the error, analyze it, and try to generate a better query.Step 3 – Help the Agent Understand the DataLetting the agent correct its own mistakes is helpful, but trial and error can be inefficient. Instead of waiting for the agent to fail and recover, we can give it a clearer understanding of the data structure up front.One simple way to do this is by including a few sample rows from the product catalog directly in the prompt. This helps the agent understand both the schema and the shape of the data, which improves its chances of generating valid queries from the start.To add this context, let’s override the get_answer method in your agent like this:This method will use functionality provided by the base class to build a LangChain-based agent executor, pass the input to it, and return the response to the user. One important change here is that besides user’s input (passed as input_text ), it will also pull a few sample products from the database and will inject them into the agent’s system prompt as sample_products.In your prompt template (prompt.py), add this placeholder at the end:
Here are some sample products in the database: {sample_products}This additional context will be included with every call to the agent. It initializes the agent with a basic understanding of the structure and shape of the data, which makes it easier for the agent to construct accurate queries from the start.
Let’s give it a try.You should notice that the agent now constructs queries that better match how the data is shaped. For example, it may use the category column to search for plans labeled as “Home,” or rely on the properties column to filter for plans with specific internet speeds.Step 4 – Retry When No Results Are FoundEven if the agent is capable of generating valid SQL queries and has seen sample data, there’s still a chance it will produce a query that technically runs but returns no results.In the current implementation, when that happens, the tool simply returns an empty list, and the agent assumes there are no relevant options. But in reality, the issue may be with how the agent built the query, not with a lack of products.To address this, we can update the tool to return a clear message when no products are found—encouraging the agent to try a different approach. Here’s how the updated run method might look:With this change, the agent receives explicit feedback when a query returns no matches. It can then choose to revise the query and try again with broader or alternative criteria.This gives the agent an opportunity to step back and reconsider its assumptions, leading to better resilience and more accurate results when dealing with uncertain or ambiguous user requests.Step 5 – Respect the Expected Number of ResultsIn some cases, a user might indicate how many products they want to see—perhaps just one recommendation or the top three matches. Right now, the agent doesn’t take that into account. It may return a long list of results, even if the user only wanted a few.We can improve this by passing the expected number of results as part of the tool input. The tool will then check whether the number of matches exceeds this limit. If it does, it will prompt the agent to follow up and narrow the criteria.First, update the input schema to include this new parameter:This addition helps turn the agent into a more effective product search assistant. Instead of assuming that the initial results are appropriate, the agent now reflects on the quantity of data returned, checks it against user expectations, and adjusts accordingly. This creates a more collaborative flow where the agent and user refine the query together to land on a relevant result.Step 6 – Enable the Agent to Finalize a PurchaseOnce the user finds a plan that matches their needs, the next logical step is to help them act on it. Right now, our agent can recommend products but doesn’t support any kind of checkout process.To make this possible, we’ll give the agent the ability to generate a contract URL the user can follow to finalize their purchase. This effectively allows the agent to transition from discovery to action.Start by creating a new tool, PurchaseTool, which accepts a plan_sku and returns a contract finalization link:Lastly, modify the search tool’s return message slightly to encourage the agent to propose a contract. The agent will likely figure it out even without this hint, but there’s no harm in pushing it more explicitly:With this addition, your agent becomes a guided assistant that helps the user discover a suitable plan and smoothly transition into completing the purchase.Step 7 – Ask for Additional Customer DetailsBefore the agent pushes the user to sign a contract, it can also ensure that it collects any additional information needed to complete the process—such as the customer’s name and location.
To support this, update the PurchaseToolInput schema with two new fields:Thanks to the structured schema and tool description, the agent will know that it must collect these inputs from the user before invoking the tool. If the information isn’t provided initially, the agent can follow up with questions like:Could you tell me your name and zip code so I can finalize the contract?This closes the loop and ensures that the agent not only helps discover the right plan but can also guide the user through to a complete and personalized purchase process.In this walkthrough, we explored how to build a practical AI agent for product catalog search using Enthusiast. Starting from a basic ReAct-style agent capable of generating SQL queries, we incrementally introduced more sophisticated behaviors:Error recovery through exception feedbackSchema-aware reasoning via sample dataRetry logic when no results are foundAdapting results to match user expectationsFinalizing user purchases with structured follow-upCollecting required customer details before contract generationEach step was designed to bring the agent closer to an experience that feels like a helpful, iterative assistant.]]></content:encoded></item><item><title>CRYSTALS - The Gently Introduction</title><link>https://dev.to/isohanni/crystals-the-gently-introduction-15b9</link><author>Jani</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 18:16:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We will explore how to implement a post-quantum cryptography algorithm(s) CRYSTALS. We start the journey by introducing some needed tools and in the following posts implement the actual algorithm. If you are interested in the dirty details how to turn math to code, this is for you.[This article was written in another platform first and I had some painful time to get it rendering even remotely correct here. I hope it is readable enough.]CRYSTALS ("Cryptographic Suite for Algebraic Lattices") is post-quantum public key encryption scheme, meaning it is expected to be secure even at the era of quantum computing where many current PKE-variants fail.CRYSTALS consists of two cryptographic primitives: Kyber and Dilithium. Kyber is key exchange method, i.e. asymmetric encryption providing secure channel to change secrets, and Dilithium is a cryptographic signing method. We will explore the mathematics behind these algorithms by coding them in Python as we go. You can find the code from my GitHub repo.The reader is assumed certain maturity in mathematics and basic understanding of Python. We don't prove anything, instead the focus is to introduce and build needed machinery that we will use later. 
The mathematical part of this presentation follows closely the way Alfred Menezes presents it in his excellent video series on the topic.When we say two integers 


 and 

 are congruent modulo 

 we mean 

 is a integer multiple of 

  In this case we write 
With 

 we mean 

 is the remainder of integer 

 divided 

 This implies 

 is the ring of integers modulo 

  In this ring addition and multiplication are performed modulo 
We implement integers in 

 in class Zq. Notice the Python modulo-operation % is implemented in a way that is fully compatible with our needs because it can handle negative values correctly. The instantiation can be done with integer value or with an instance of Zq.The class Zq has addition, subtraction, multiplication, str and repr operations implemented. This makes our life a lot easier because we can make arithmetics directly and debug when needed.To get a feeling how things work, consider the ring 

 For example we have 
Let 

 be a prime. We define 

 to be the set of polynomials of 

 with all coefficients in the ring 

 This means all coefficient arithmetic is performed in the ring 
We implement polynomials in the ring with a class ZqPolynomial. Here coefficients is a list of integers, and the length of the list defines 
For example, let 
We can do this with our code as follows (we use extra zeroes in coefficients to prevent the polynomial modulo operation).Let now 

 be a prime and 

 a positive integer. The quotient ring (often called just "polynomial ring") 

 consists of polynomials in 

 of degree less than 

 In ring 

 the multiplication of polynomials is performed modulo the polynomial 

 called the reduction polynomial. This means that the product of polynomials 

 is defined as the remainder 

 of their product when divided by 

 in the polynomial ring. Notice that by definition now degree of 

 is at most 

 and 
One should notice here that remainder is not calculated by the traditional polynomial division algorithm, but with division rules that apply in the polynomial ring. For our purposes it suffices to acknowledge that if the polynomial has degrees 

 you can apply the rules 

 and in general for 

 and then simplify the resulting polynomial normally. To understand why, please visit ring theory and ideals.

Overloading addition and subtraction is straightforward, but multiplication needs special treatment. Here we utilize the fact that Zq has multiplication operation overloaded. In real-life implementations this naive implementation is too slow and NTT-algorithm is used instead. We will return to this later.For example, consider the ring 
To get the reminder of 

 when divided 

 we first calculate the product 

 and use the substitution rule to get 

 and with the modulo operations we arrive at 
With our code we get directly as follows.For a programmer it is rather straightforward to see that the polynomial can be represented as vectors. Consider the polynomial 
The obvious way to write that as a vector is 

 (convention is to use column vectors). The polynomial addition is now component-wise addition modulo 

 and subtraction is component-wise subtraction modulo 

 Multiplication is polynomial multiplication as shown earlier and the resulting polynomial is stored in a vector. We used this implicitly earlier when defining ZqPolynomial.We extend this notation. Let 

 be a positive integer. Module 

 consists of length 

 vectors of polynomials of 

. Addition and subtraction is again component-wise. It follows that the resulting vectors in both cases is in 
We will later use 

 to represent 

-matrices with polynomial entries. This extension is similar to that from vectors to matrices in linear algebra.The module 

 includes 

-matrices with elements from the polynomial ring. The easiest way to handle these is to define class PolyMatrix that holds ZqPolynomials in a list of lists.The multiplication in 

 is defined as inner product of two vectors in 

 This means that the polynomials, that are elements of the vector in 

 are multiplied in 

 and added together. The result is in 

and 

 We get directly 

 and in Python as follows.Using the 

 and 

 defined earlier, we get To enable matrix multiplication, we implemented the matmul operator.We can use the bracket-notation with PolyMatrix because we defined getitem and setitem methods. Next we need notion of size that will become useful later. First let us define symmetric mod.Let 

 be odd and 

. We define 
This immediately gives 

  Here 

 is symmetric modulo operation.Let 

 We now have by definition 
The definition of symmetric modulo is slightly different for even 

 Let 

 be even and 

 and we get 

.

In the code we implement the symmetric modulo in Zq and use that from ZqPolynomial and PolyMatrix.Let 

 We define 

 Then 
You can think this as "clock-algebra", the further the integer is from noon, the bigger its norm.We have the following immediate corollaries
 if q is odd and
 if q is even.

For polynomial ring elements the size of a polynomial is defined with the maximum operation. Let 

We define 

For example, let 

 and 

 Then 

 because 

 and clearly 
This definition can be generalized to elements of module 

 Let 

 We define 
We say a polynomial 

 is small if  

 is small. Notice that this means that all coefficients of 

 need to be small due the way the norm is defined. What "small" means is defined per context.Let 

 be a positive integer less than 

 We define 

 to be the set of polynomials in 

 where each polynomials each coefficient is of size at most 

 We use 

 to define a set of "small" polynomials.

For example consider polynomial 

 Now 

 and hence 
Observe that 

 is the set of polynomials in 

 with all coefficients in the set 

 (when reduced 

).Let 

 and 

 Without proof we state that 

 This generalizes to vectors or polynomials. Let 

 and 

 Then we have 
In the next article we utilize the presented machinery to implement basic CRYSTALS-Kyber public key encryption. Stay tuned.]]></content:encoded></item><item><title>My AI Unit Test Agent is Alive! Now for Part 2: The QA Agent 🤖</title><link>https://dev.to/herchila/my-ai-unit-test-agent-is-alive-now-for-part-2-the-qa-agent-2j27</link><author>Hernán Chilabert</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 17:21:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Just a week ago, I shared that I started building an AI agent to handle my unit tests in Python. Well, the little guy is officially up and running!Phase 1 is a wrap! The MVP is working as planned: it can analyze a Python file, figure out what's inside, and use an LLM to generate a solid suite of pytest tests. It feels a bit like magic watching it work. I'm super happy with the foundation we've got.But now... the real fun begins.
  
  
  Entering Phase 2: The "QA Engineer" Agent
The first agent is the "Dev Engineer"—it writes the code. Now, I'm building its partner: the "QA Engineer" agent.So, what's its job? This new agent will:: It will actually execute pytest on the tests the first agent wrote.: Did the tests pass? Did they fail? Why?: It will then go back to the "Dev Engineer" and say something like, "Hey, this test you wrote is failing because of X," or "You missed covering this edge case."The goal here is to create an autonomous feedback loop. The two agents will collaborate, refine, and improve the tests until they meet a certain quality bar, all on their own. Wild, right?This is the part of the project I've been most excited about, where it starts to feel less like a script and more like a real, autonomous team.As always, the project is fully open-source. You can follow along with the progress, check out the code, and see the roadmap on GitHub.Thanks for reading and following the journey! Let me know in the comments what you think about this two-agent approach.]]></content:encoded></item><item><title>10 In-Depth Python Tricks to Supercharge Your Automation Projects</title><link>https://dev.to/codetestfactory/10-in-depth-python-tricks-to-supercharge-your-automation-projects-noo</link><author>Sohail Mohammed</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 17:12:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[🚀 𝗙𝗿𝗼𝗺 𝟭𝟬𝟬+ 𝗳𝗮𝗶𝗹𝗶𝗻𝗴 𝘁𝗲𝘀𝘁𝘀 → 𝟳𝟲% 𝗳𝗲𝘄𝗲𝗿 𝗳𝗮𝗶𝗹𝘂𝗿𝗲𝘀.That’s what happened when I applied 𝟭𝟬 𝗣𝘆𝘁𝗵𝗼𝗻 𝘁𝗿𝗶𝗰𝗸𝘀 into my automation framework.Most QA teams blame flaky environments or unstable APIs for test failures. But often, it’s about how you design your framework.In my latest blog, I break down:
✅ 10 in-depth Python tricks I used
✅ How they helped reduce test failures by 76%
✅ Why these tricks can supercharge any automation project🔗 𝗖𝗵𝗲𝗰𝗸𝗼𝘂𝘁 𝘁𝗵𝗲 𝗳𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 𝗵𝗲𝗿𝗲:👉 Curious: What’s the #1 Python trick or framework tweak that helped you stabilize your tests? Drop it in the comments 👇]]></content:encoded></item><item><title>death and gravity: Announcing asyncio-thread-runner: you can have a little async (as a treat)</title><link>https://death.andgravity.com/asyncio-thread-runner</link><author></author><category>dev</category><category>python</category><pubDate>Thu, 21 Aug 2025 16:43:37 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[I'm happy to announce that you can now install it from PyPI,
and read the
documented, tested, type-annotated code
on GitHub! ⭐️This is useful when you're doing some sync stuff, but:you also need to do some async stuff,  making maybe the sync stuff is an existing applicationmaybe you still want to use your favorite sync libraryor maybe you need just a little async, without having to pay the full priceit allows you to use  and  from sync code$pipinstallasyncio-thread-runner
]]></content:encoded></item><item><title>🔥 Simulating Course Schedules 600x Faster with Web Workers in CourseCast</title><link>https://dev.to/somedood/simulating-course-schedules-600x-faster-with-web-workers-in-coursecast-41ma</link><author>Basti Ortiz</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 15:43:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This is the story of how I made a Monte Carlo simulation of student schedule assignments  with web workers.Here is our baseline: the original prototype struggled to handle ~100 concurrent users. Each simulation request to the compute server took a whole minute (~60 seconds) to complete, which incidentally exasperated the resource limits of the deployment.In this article, we'll discuss the steps that I took to make the application virtually infinitely scalable (i.e., no server compute bottleneck) thanks to sub-second client-side execution. That's faster than a page load! 🔥
  
  
  Simulating Course Match with CourseCast
On simulation day, the Course Match algorithm determines the  for all offered courses based on their supply and demand. Upon completion, Course Match will have been able to assign schedules to each student (in a single round!) such that the course utilities and obtained credits are maximized given the student's respective budget constraints.💡 You can think of Course Match as an autonomous shopper that "buys" courses on behalf of the student. The purchasing power is only limited by the student's token budget, their maximum workload/credits, and their assigned utilities. The higher the token budget, the greater the student's capability to "afford" the clearing price for a course.Since it's impossible to know ahead of time what the actual clearing prices will be, CourseCast instead forecasts the clearing prices based on the most recent historical data of actual clearing prices in previous Course Match runs. These predicted prices (and their statistical variances) are the "weights" of the model trained on the latest course and instructor trends.To account for forecast uncertainty, the CourseCast model assumes that the predicted clearing price is a normally distributed random variable. As such, CourseCast runs 100 Monte Carlo simulations and counts the frequency of particular courses and schedule configurations being selected. These simulation results are presented to the user as a probability.
  
  
  So where was the bottleneck?
The original CourseCast 2024 was prototyped and deployed as a Streamlit application written in Python. Students would input their course utilities and submit their simulation request to the Streamlit Community Cloud where:The Python back end on shared virtual compute resources would parse course data and load model weights from a hosted Excel spreadsheet.The service would recompute all of the scheduling conflicts between courses (~200 in total). Example: classes with overlapping schedules, classes with overlapping sections, and other logistical constraints.Run 100 Monte Carlo simulations . Each of which is an instance of a linear programming solver.As CourseCast went viral among thousands of UPenn students, the scalability cracks began to show. When too many concurrent users hammered the Streamlit application, students couldn't run their simulations.To be fair, the application was on the Streamlit free tier, but it was definitely high time for a rewrite to something more production-grade.
  
  
  So how did we scale CourseCast 2025?
Now that we know where the bottlenecks are, let's tackle them one by one.
  
  
  Scrapping the Python Server
My first instinct was to ask: is Python necessary at all? The Monte Carlo simulation was essentially a glorified  loop over a linear programming solver. Nothing about the core simulation logic was specific to Python. In fact, the only Python-specific implementation detail was the usage of Excel spreadsheet parser libraries and linear programming solver libraries for Python. I figured...If there was a way to package and compress the Excel spreadsheet in a web-friendly format, then there's nothing stopping us from loading the entire dataset in the browser! Sure enough, the Parquet file format was specifically designed for efficient portability.If there was an equivalent linear programming solver library in JavaScript, then there's nothing stopping us from running simulations in the browser! Sure enough, there was the  library (among many other options).At this point, I was fully convinced that we could scrap the Python server and compute the simulation entirely in the browser. This approach effectively allows us to infinitely scale our simulation capacity as we would no longer be constrained by shared cloud compute limits.That solves our scalability problem! ✅
  
  
  Precomputing Static Course Conflicts
The next bottleneck was the course conflict generation logic. Recall that  simulation request recomputes the logistical constraints on course selections (e.g., disallowing classes with overlapping schedules). This is fairly non-trivial work as there are hundreds of classes to consider.So, naturally, the solution is to precompute these conflicts ahead of time. The precompute script takes the raw course data and appends the "conflict groups" of each course. These "conflict groups" ultimately determine the statically known logistical constraints of the linear programming solver.📝 In computer science parlance, you can think of these "conflict groups" as equivalence classes defined by the relation of overlapping course schedules. That is to say, for all pairs of courses within an equivalence class, their schedules must have a non-empty schedule intersection. Thus, a "conflict group" is just a label for a group of pairwise-intersecting courses.All of the course metadata, seeded random values, and conflict groups are embedded in a single compressed  file (~90 KiB) and served to the user via a CDN for efficient delivery and caching. There is also the option of caching the file in a Service Worker, but the edge CDN already works well enough.That solves our repeated work problem! ✅
  
  
  Offloading CPU-Bound Work to a Separate Thread
The next bottleneck is the sequential execution of Monte Carlo simulation runs. There's actually no reason for us to run them sequentially because each sampled price prediction is independent from the 99 other trials. The simulation can thus be parallelized at the trial level.Since each simulation run is primarily a linear programming solver, we know that the work is CPU-bound, not I/O-bound. The - model will  work here because CPU-bound work blocks the event loop. We  offload the work to another thread to keep the UI responsive.In the browser, we only have one way to spawn multiple threads: through the Web Worker API.We can then wrap the worker message-passing logic in a  interface and leverage libraries like TanStack Query for clean pending states in the UI. The example below uses React for demonstration, but this pattern is framework-agnostic.That solves our responsive UI problem! ✅
  
  
  Parallelizing with Worker Thread Pools
A more advanced implementation of this one-shot request-response worker architecture leverages thread pools to send work to already initialized workers (as opposed to re-initializing them for each work request).We can use navigator.hardwareConcurrency to determine the optimal number of worker threads to spawn in the pool. Spawning more workers than the maximum hardware concurrency is pointless because the hardware would not have enough cores to service that parallelism anyway.⚠️ In the previous section, the  was initialized by the  function. In a worker pool, this should instead be provided as an argument to the  function because  is no longer the "owner" of the thread resource and thus has no say in the worker lifetime. Worker termination  be the responsibility of the thread pool, not the sendWork function.📝 Request cancellation is not implemented here for the sake of brevity, but it is fairly trivial to forward the  from TanStack Query into the thread pool. It's only a matter of terminating the workers upon receiving the  event.The thread pool optimization allowed us to run 100 simulations in parallel batches across all of the device's cores. Together with the precomputed conflict groups, the Monte Carlo simulation was effectively reduced from a minute to sub-second territory! 🔥That solves our performance problems! ✅After all of these optimizations, I upgraded CourseCast from a prototype that struggled with a hundred concurrent users (with ~60 seconds per simulation request) to an infinitely scalable simulator with sub-second execution speeds (faster than a page load!).CourseCast now guides 1000+ UPenn students to make informed decisions and (blazingly!) fast experiments about their course schedules. And we're just getting started! 🚀Throughout this work, I had a few key takeaways:Always leave the door open for the possibility of offloading compute to the browser. Modern Web APIs are highly capable with great browser support nowadays. Keep exploring ways to save ourselves from the infrastructure burden of bespoke Python services.Always find opportunities to precompute static data. May it be through a precompute script like in CourseCast or a materialized view in the database, strive to do the least amount of repeated work.Keep a sharp eye out for parallelizable work. There are many opportunities in data science and general scientific computing where data processing need not be sequential (e.g., dot products, seeded simulation runs, independent events, etc.).On a more human perspective, it's always a pleasure to have the code that I write be in service of others—especially students! As software engineers, it's easy to forget about the human users at the other end of the screen. To be reminded of the positive impact of our code on others never fails to make our work all the more worth it."Have been hearing tons of amazing feedback. Anecdotally, most people who ran simulations through CourseCast ended up without any surprises. Congrats on shipping a great product!"Thanks to Derek Gibbs and the Casper Studios team for trusting me to take the lead on this project! And thanks to the Wharton School administration for their support and collaboration with us in making CourseCast as helpful as it can be for the students.I must disclaim that our dataset is public and fairly small. For larger models with possibly proprietary weights, downloading the data in the browser is not an option. ↩]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/ticoraph/-12f1</link><author>TicoRaph</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 14:42:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Document Parsing using GPT-4o API vs Claude Sonnet 3.5 API vs Invofox API (with Code Samples)]]></content:encoded></item><item><title>Build a Future-Proof Career with SkillSprintTech’s Generative AI Courses</title><link>https://dev.to/skillsprinttech/build-a-future-proof-career-with-skillsprinttechs-generative-ai-courses-5eg6</link><author>SkillSprint Tech</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 14:16:58 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the fast-changing tech world of today’s time, staying ahead usually means mastering all the new-age tools that will define tomorrow. One such revolutionary & highly progressing field is generative AI - a fairly new branch of artificial intelligence that has immense potential to enable machines to create relevant text, music, images, & even code. For students, professionals, as well as millions of tech enthusiasts looking to make a mark in this space, SkillSprint Tech, one of the premier career-oriented training institutes offers some of the most practical as well as industry-relevant generative AI courses in India.
  
  
  Why Is Generative AI So Important in Today’s Time?
Generative AI is no longer just a research concept; it is empowering real-world applications big time and that too in a disruptive manner. From ChatGPT creating human-like conversations to AI-driven design tools that are generating stunning visuals, the possibilities in the field of AI are endless. Companies in various sectors like healthcare, finance, retail, entertainment, & software development are continually integrating all of these technologies to considerably improve efficiency, creativity, & customer experience to the next level.By thoroughly learning & understanding generative AI in today’s time, professionals can position themselves for a range of high-demand roles like as AI Engineer, AI Product Manager, Prompt Engineer, or Data Scientist with ultimate AI expertise.
  
  
  The Edge SkillSprintTech Has in the Dynamic Field of Generative AI Training
The generative AI courses offered by SkillSprintTech are strategically designed and is being imparted to participants with a very clear focus: to bridge the gap that exists between theoretical knowledge & practical skills. Instead of just learning the various concepts, students get to build real AI-powered applications. The curriculum very strategically covers everything from a range of foundational AI principles right from the scratch to the most advanced topics like natural language processing (NLP), large language models (LLMs), computer vision, & multimodal AI.A highlight of this particular program is its project-based approach. Learners work on hands-on projects like AI Chatbots, content generation tools, image synthesis, & AI-powered automation systems. This particular approach not just strengthens technical skills but also strongly builds a robust portfolio that impresses recruiters big time.The generative AI courses that is being offered by SkillSprint Tech are perfect for:Existing IT professionals already pursuing their career in the field of IT but are looking to upskill in AI-driven tools.Data analysts & data engineers who are seriously aiming to specialize in the field of AI.Students who are in the lookout to pursue computer science or related fields with the aim to enhance their horizon of knowledge.Entrepreneurs who are willing to integrate AI into their business solutions to improve their efficiency and prospects.No matter whatever is your background, the courses are well-structured to take you from beginner to advanced level, step-by-step.
  
  
  The Best Generative AI Certification for Career Growth
Earning the best generative AI certification from SkillSprint Tech is more than just adding a badge to your resume – it is a proof of your readiness to work in one of the most exciting fields in technology today. This certification is well-recognized by industry experts & signals to employers that you have both the technical know-how as well as the practical experience to deliver results.Additionally, SkillSprint Tech also provides adequate career support through in various additional ways like in the preparation of interview, building resume, & connecting learners with various hiring partners. This particular end-to-end approach likely ensures students not just learn but also land the most rewarding AI-focused roles.Generative AI is rapidly transforming industries, & those who understand how to leverage it will surely lead the way in the present digital economy. By joining SkillSprint Tech’s generative AI courses, learners gain the most contemporary skills, various simulative projects, & certification required to thrive in this new era. For anyone serious about future-proofing their career, the best generative AI certification from SkillSprint Tech is the gateway to countless opportunities.]]></content:encoded></item><item><title>Optimize Your Database with Vertical Partitioning and Caching day 34 of system design</title><link>https://dev.to/vincenttommi/optimize-your-database-with-vertical-partitioning-and-caching-day-35-of-system-design-3dih</link><author>Vincent Tommi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 13:30:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Databases are the backbone of most applications, but as they grow, performance can take a hit. Imagine a massive User table stuffed with profile details, login history, and billing information. Queries slow down as the database scans irrelevant columns for every request. Sound familiar? Let’s explore vertical partitioning—a powerful technique to streamline your database—and touch on caching for even faster data retrieval.What Is Vertical Partitioning?Vertical partitioning splits a wide table into smaller, focused tables based on usage patterns. Instead of one bloated User table, you create separate tables for specific data groups. This reduces the number of columns scanned during queries, boosting performance and minimizing disk I/O.
For example, suppose your User table stores:Profile details: name, email, profile picture
Login history: last login timestamp, IP addresses
Billing information: billing address, payment detailsAs the table grows, even a simple query like fetching a user’s name forces the database to wade through all columns. Vertical partitioning solves this by splitting the table into:-- User_Profile table
CREATE TABLE User_Profile (
    user_id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    profile_picture VARCHAR(255)
);

-- User_Login table
CREATE TABLE User_Login (
    user_id INT PRIMARY KEY,
    last_login DATETIME,
    ip_address VARCHAR(45)
);

-- User_Billing table
CREATE TABLE User_Billing (
    user_id INT PRIMARY KEY,
    billing_address TEXT,
    payment_details VARCHAR(255)
)
Each table now holds only the columns relevant to specific queries, making data retrieval faster and more efficient.Flowchart: Visualizing Vertical Partitioning
Here's an ASCII art representation of the vertical partitioning process for illustration:+-------------------------+
|      User Table         |
| - name                  |
| - email                 |
| - profile_picture       |
| - last_login            |
| - ip_address            |
| - billing_address       |
| - payment_details       |
+-------------------------+
           | Split (Vertical Partitioning)
    +-------------+   +-------------+   +-------------+
    |User_Profile |   | User_Login  |   |User_Billing |
    | - user_id   |   | - user_id  |   | - user_id  |
    | - name     |   | - last_login|   | - billing_ |
    | - email    |   | - ip_address|   |   address  |
    | - profile_ |   |             |   | - payment_ |
    |   picture  |   |             |   |   details  |
    +-------------+   +-------------+   +-------------+
           |
+-------------------------+
|    Faster Queries       |
| (Reduced Disk I/O)      |
+-------------------------+This visual shows how splitting the table streamlines data access.Taking It Further with CachingVertical partitioning optimizes disk-based queries, but disk access is still slower than memory. Enter caching: storing frequently accessed data (e.g., user profiles) in memory using tools like Redis or Memcached. This delivers lightning-fast access for common queries, complementing the efficiency of partitioned tables.By combining vertical partitioning and caching, you can:Improve query performance: Scan fewer columns and retrieve data faster.Reduce resource usage: Lower disk I/O and server load.Scale efficiently: Handle growing data without sacrificing speed.Ready to optimize your database? Analyze your tables’ usage patterns, identify columns that can be partitioned, and consider caching for frequently accessed data. Experiment with these techniques in a test environment and watch your application’s performance soar!]]></content:encoded></item><item><title>Pack, the new-gen workflow manager</title><link>https://dev.to/robert19066/pack-the-new-gen-workflow-manager-55bo</link><author>robert19066</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 13:02:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Project Pack V1, or simply Pack, is a Python-based "packlet" (workflow) manager that allows users to create, store, and execute custom shell command sequences with various execution modes and privilege configurations. The project provides both a CLI interface for creating packlets and an execution engine for running them.Dev note: Trust me, it's really awsome!The codebase is organized around four main components:: The user-facing CLI application with a rich terminal interface including colored banners, menus, loading bars, and wizards for packlet creation and execution: The core execution engine ( class) that parses packlet files and executes shell commands with different execution strategies: Parser utilities ( class) for extracting configuration from packlet files: File creation utility for generating new packlet files with proper formattingPacklets are custom configuration files with specific extensions and structure:: Standard packlets (default execution mode - stops on errors): Bulldozer packlets (continues execution despite errors)$type=<shell>          # Shell to use (bash, zsh, fish, etc.)
$excmeth:<method>      # Execution method (default/bulldozer)
$isudo=<true/false>    # Whether sudo privileges are required

<command1>
<command2>
...
---
: Stops execution immediately when any command fails: Continues executing all commands even when some fail, providing a summary of failed commands at the end
  
  
  Testing Packlet Execution
 test.paklt


python 
  
  
  Creating Packlets Programmatically
python /
├── mainShell.py          # Main CLI application with UI
├── mainCompile.py        # Core execution engine
├── helper_functions.py   # Packlet file parsers
├── createFile.py         # File creation utilities
├── packlets/             # Directory for storing packlets (created automatically)
└── __pycache__/          # Python bytecode cache
The codebase implements two distinct error handling strategies:: Uses  with  to raise exceptions on command failure: Uses  with  and manually tracks failed commandsThe CLI uses ANSI color codes extensively through the  class for terminal styling. Key UI components include:Dynamic menu boxes with perfect alignmentProgress indicators (loading bars and spinners)Step-by-step wizards for packlet creationColored success/error/warning messages: Used in helper_functions.py (imported but not actively used in current implementation): Core dependency for shell command execution: For file system operations and screen clearing: For application exit handling: For UI animations and delays: For randomized loading animationsAll created packlets are stored in the  directory, which is automatically created if it doesn't exist. The directory structure is flat with no subdirectories.Sudo execution is configurable per packlet via the  parameterCommands are executed through shell subprocess calls, so standard shell injection precautions applyBulldozer mode can potentially mask security-relevant command failures]]></content:encoded></item><item><title>Document Parsing using GPT-4o API vs Claude Sonnet 3.5 API vs Invofox API (with Code Samples)</title><link>https://dev.to/anmolbaranwal/document-parsing-using-gpt-4o-api-vs-claude-sonnet-35-api-vs-invofox-api-with-code-samples-56h2</link><author>Anmol Baranwal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 12:04:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Extracting structured data from unstructured documents (like PDFs and images) can get tricky fast.With the rise of foundation models and purpose-built APIs, it's now possible to turn even a messy invoice into clean JSON with just a few lines of code.So I will compare three different ways to parse documents: using OpenAI’s GPT‑4o, Anthropic’s Claude 3.5 Sonnet and the Invofox API.I picked Invofox because it's a YC-backed startup built specifically for document parsing. It uses specialized models (proprietary and best-of-LLM) tuned for invoices and other documents, while GPT/Claude are general-purpose LLMs.You will see real Python code, actual outputs and a breakdown of when to use each tool (pros & cons). At the end, there is a detailed comparison table on features & benchmarks.
  
  
  🎯 Using GPT-4o (ChatGPT) API
Let’s start with OpenAI’s GPT-4o. It's capable of understanding text and extracting structured information when prompted correctly. But unlike Invofox, it can’t directly read PDF files.So we first need to extract the text using OCR (like Tesseract, pdfplumber or an online tool), then send that text to GPT via an API prompt.GPT-4o, especially via the ChatGPT web interface and certain API endpoints (notably in Azure OpenAI Service), can accept PDFs and images as inputs and extract structured data. But since we are using the API, it's not really possible.You will need an OpenAI API key. Create a  file and attach it with this convention.your_api_key

openai api key



We will use Python for this. Here's how you can try it yourself, step by step.
  
  
  Step 1: Set up your Python environment
Creating a virtual environment means setting up an isolated space for your Python project where all dependencies are installed locally (and not system-wide). This avoids version conflicts and keeps your global Python installation clean. So let’s create one.
python3  venv /bin/activate  
python  venv 
.nvcriptsctivate  You will know it’s active when you see  at the beginning of your terminal prompt.
  
  
  Step 2: Install required packages
We need two main libraries:: to use the GPT-4o API : Loads environment variables from a  file into Python, useful for managing API keys and secrets.
pip pdfplumber openai python-dotenv
I installed the  later so that's why it's not visible in the command.After installing your dependencies, run:pip freeze  requirements.txt
This writes all installed packages in your virtual environment (with versions) into . You can then use this file later with:pip  requirements.txt
For reference, please add a  in the root directory to avoid pushing the virtual environment directory.
  
  
  Step 3: Extract text and parse with GPT-4o
Here is the sample Invoice PDF that I'm using for the example. I'm attaching a snapshot so you can get the idea of the fields we are going to extract.Let's write the complete code with the file name as .Here's a simple explanation: : uses  reads each page of the PDF and concatenates the extracted text. This gives you the raw, unstructured invoice content as a string.parse_invoice_with_openai : Sends this prompt to the GPT‑4o model via the  endpoint, asking GPT‑4o to extract five key fields. The model then processes the prompt and returns a JSON-formatted response.Here is the JSON response after running the script using .GPT-4o (ChatGPT) output for invoice line items isn’t consistently labeled "lines". Sometimes it's "Line Items" or something less standardized, while other tools (like Invofox) always use a consistent name like "lines" for those entries.
terminal output



Here we instruct GPT-4o via a system prompt to parse the text. This can work reasonably well as the API is strong enough now (compared to previous OpenAI models).✅ Pros: Easy to try, flexible. GPT-4 excels at logic and structured data extraction, so it can correctly identify invoice fields and calculate totals.The problem I see is that we still have to engineer prompts and verify the output (which is not possible for everyone).The JSON can be malformed or may miss fields (hallucinations are possible). There’s no built‑in validation or confidence scores. GPT requires sending all text in prompts (which would be costly for large docs) and outputs vary by prompt style.GPT-4o is billed per token. The estimated cost for a 1–2 page invoice extraction falls in the $0.005–$0.018 range, depending on how detailed your prompt and output are. You can also use this pricing calculator based on your use case.It can respond in 1–30s but is subject to load spikes, especially for large prompts.
  
  
  🎯 Using Claude 3.5 Sonnet API
Anthropic's Claude 3.5 Sonnet model is also capable of parsing structured data from text when prompted correctly. Like GPT-4o, it cannot read PDF files directly via API, so we will first extract the text from an invoice PDF, then pass it to Claude for structured parsing.You will need an Anthropic API key. Create a  file and attach it with this convention:ANTHROPIC_API_KEY=your_api_key
We will use Python again for this setup and follow the same instructions used in the last section.
  
  
  Step 1: Set up environment and install packages
Just like before, let’s isolate our dependencies in a virtual environment.
python3  venv /bin/activate


python  venv 
.nvcriptsctivate
Once activated, your terminal will show a  prefix.We need the following libraries: : to extract text from PDF : official SDK to interact with Claude 3.5 : to load the API key from a  file
pip pdfplumber anthropic python-dotenv
If you are following from the last example, we just need to install the anthropic package.Then export your environment to a  file. Make sure to include a  to avoid committing the virtual environment.pip freeze  requirements.txt

  
  
  Step 2: Extract text and parse with Claude 3.5 Sonnet
As Anthropic launches safer and more capable models, they regularly retire older models. So you can check the model status of which ones are deprecated, retired and which ones are active. I will be using claude-3-5-sonnet-20240620 active version for the example.Let's write the complete code with the file name as . It's very similar to the previous section and I'm using the same sample Invoice PDF.Here's a simple explanation: : uses  to pull plain text from each page of the PDF.parse_invoice_with_claude : sends the text to Claude Sonnet 3.5 with a specific prompt asking for JSON output.Claude returns a stringified JSON block with the requested fields.You can run the script using  in the terminal. Here's the JSON response:Claude 3.5 is very strong at understanding long text and formatting it cleanly.Claude Sonnet can handle text (and even images via embedding) in its promptsIn some cases, it handles unusual or long documents slightly better than GPT-4.Like GPT, Claude requires prompt engineering.Like GPT, Claude can sometimes miss fields or make up values (hallucinate).It still returns raw JSON text without validation, so you must parse/verify it.You still need to extract text yourself, it doesn’t parse raw PDFs.Claude 3.5 Sonnet is also billed per token. The estimated cost for a 1–2 page invoice extraction falls in the $0.005–$0.018 range, depending on how detailed your prompt and output are. You can also use this pricing calculator based on your use case.It's exceptionally fast for small prompts (200–300ms) but larger or more complex stimuli can raise latency to 10s or more. So I was searching for a better solution unlike code-based (OpenAI & Anthropic) approaches requiring prompt engineering, I found many good tools like Invofox, Google Document AI, Amazon Textract. What stood out about Invofox is that it’s backed by Y Combinator and has all the features I needed. That gave me the confidence to dig deeper and try it out.It provides a plug‑and‑play AI-powered document parsing API that makes it super easy to extract data from invoices, receipts, payslips, bank statements, loan/mortgage files and custom document types like bills.They have some useful built-in features like:It automatically separates multiple documents contained within a single PDF (such as mixed invoices or statements), grouping pages into logical sub-documents for better extraction and automation.  It's configurable via API during upload and works alongside the classifier for cleaner downstream processingPretrained AI model that detects document types (invoice, receipt, etc) so that each document is processed using the correct schema. It's optional and can be enabled per environment or request.They also use advanced AI models with proprietary algorithms that verify and autocomplete your data. Check API Docs.
  
  
  Step 1: Sign up for the dashboard
You can sign up for the dashboard to generate an API key.You can manually upload the document as well but we will be using the API since it's easier and much better in experience.
  
  
  Step 2: Creating the request in Postman
Once you have your API key, you can use Postman to send documents for parsing using Invofox's  endpoint.✅ 1. Create a New RequestOpen the Postman Desktop applicationCreate a collection and add a requestWe need to request this endpoint: https://api.invofox.com/v1/ingest/uploadsGo to the Headers tab and add:key: , value: key: , value: You should not manually set  as Postman will handle it automatically when using form-data. It tells the server what format the data in your request body is: → You're sending raw JSON → You are sending files + form fieldsapplication/x-www-form-urlencoded → You're sending form-like text fields (like an HTML form)When you're sending files using Postman’s form-data option, Postman automatically sets the correct  and boundary values (which are required for ).If you manually set it like this:Content-Type: multipart/form-data
You are missing the boundary part, which is something like:Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryxyz
Let's add the body fields.✅ 3. Add the Body (form-data)Switch to the Body tab, select  and add the following two fields:key: , type: , value: upload your invoice ()key: , type: , value: Paste the JSON below
The  field is optional here as it's only needed if you want to pass custom metadata or extra instructions (such as information to influence parsing, verification preferences or to register edge-case scenarios for custom document types).Beyond standard types (invoice, payslip, bank statement), you can register custom document types in your Invofox dashboard. These custom types get a unique ID like  (used in the example), which is what we are now passing to the API. By specifying a type ID, you ensure your files are parsed according to the exact schema you set up:Your custom JSON structureCustom validation rules and human review workflowsClick "Send". If everything is set up correctly, you will get a response with details on documentID. is the batch ID for this upload (useful for tracking multiple files uploaded together) is the ID of the parsed document

  
  
  Step 3: Get Parsed Document
There are two ways: one is to check the Invofox dashboard to find the newly parsed document. As you can notice, the line items and breakdowns are displayed in a table format. The GUI also provides many options, including filtering the extracted data.Based on how the workflow is set up, it may be necessary to mark it as completed, as involving a human in the loop ensures the highest accuracy and gives us more control.The other way (recommended) is to make a  request to https://api.invofox.com/documents/{documentID} with headers as:key: , value: key: , value: Here is a trimmed JSON response with the original format. It also provides the image of the original invoice in the response and a lot of extra fields compared to the earlier responses of GPT-4o & Claude.Pricing is not public, so potential users must contact their team for a commercial offer, but the product is specifically tuned for production speed and reliability. Actual test response times reported in the blog are consistently under 5s.
  
  
  🎯 Python Code using Invofox API
Many developers prefer extracting documents with code, so let’s walk through the same process using the Invofox API with Python. We will keep it brief, with just the code and JSON response.The overall process is the same as the previous sections, so I'm not repeating that. You can read the docs if you are interested in exploring for yourself.We need to install requests, a Python library that makes it easy to send HTTP requests (such as GET, POST) and work with web APIs.We will also use the  built-in Python module that comes pre-installed with every standard Python installation. The  module provides various time-related functions such as delays (), timestamps and more. In our case, we will use it to pause execution, giving the document enough time to be processed on the dashboard.Let's write the complete code with the file name as .Here are all the Invofox API endpoints used:Here is the JSON response after running the script using .The JSON response is similar to what we got after making a request using Postman. It also provides the image of the original invoice in the response and a lot of useful fields.Let's compare their methods in brief.GPT-4o/Claude → send text with promptInvofox → use API or upload a file (image/PDF) in bulkGPT/Claude → need to write prompt engineering codeInvofox → minimal code, no promptGPT/Claude → you need to manually verify Invofox → built-in validation and confidence scoresGPT/Claude → limited by token/window sizeInvofox → handles multi-page docs via backend OCR and AIWhile parsing the invoice, here's what I realized: : Good at parsing known fields if prompted clearly. You get a JSON string but must parse/clean it yourself. Errors can occur if prompts are unclear. : It's very similar to GPT-4. In the snapshots, you can see Sonnet handled the invoice fields about as well as GPT-4, sometimes better at recognizing unfamiliar terms. But we still had to massage the prompt. : It returned the fully parsed invoice JSON out-of-the-box. All fields were correctly extracted and validated. The output schema was exactly what we needed, with no extra coding.Now that we have explored each option, let’s compare them side by side. Estimates are based on typical invoice lengths: simple invoices are 1–2 pages & 1000-2000 tokens total.
  
  
  Cost & Execution Time Benchmarks
We covered the pricing structure in each of the sections, but I have also done it side-by-side so it's easier to make a decision.You should also acknowledge the ongoing cost and effort involved in upgrading language models. Teams often need to benchmark new models, retest prompts and schemas and adjust output parsing logic whenever a new version is released.These hidden maintenance costs aren’t always obvious but should be considered. With Invofox, there is no such requirement.For quick experiments or one-off tasks, you can use GPT-4 (ChatGPT API) or Claude Sonnet to parse invoice text by crafting suitable prompts. They will do a decent job extracting fields in JSON (since GPT-4 tends to produce more structured and cleaner outputs than earlier GPT-3). However, for reliable production-grade parsing of invoices or receipts, the Invofox API is superior. It’s specifically built for documents using advanced proprietary models and continual feedback.I hope you learned how to parse documents. Let me know if you have any questions or feedback.Have a great day! Until next time :)]]></content:encoded></item><item><title>✨️ DAY 3 OF 100 ✨️</title><link>https://dev.to/lyop_achayi/day-3-of-100-2pe6</link><author>TANYA LYOP ACHAYI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 10:10:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
Today was all about variables and data types,the little boxes where Python stores information. I played with strings, numbers, floats, and even booleans. It’s like teaching Python to remember my name, age, and that. yes… I’m definitely learning 🤭Every line feels like a step closer to building something cool.]]></content:encoded></item><item><title>Turn Your Photo Library Into a Location-Based Search Engine Using EXIF Metadata</title><link>https://dev.to/devasservice/turn-your-photo-library-into-a-location-based-search-engine-using-exif-metadata-41ni</link><author>Developer Service</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 07:30:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Have you ever tried to find that one vacation photo you took years ago, only to scroll endlessly through thousands of images with no luck? What many people don’t realize is that most photos already come with a hidden trail of breadcrumbs that can solve this problem: .Every time you snap a photo with a smartphone or digital camera, extra information gets embedded into the file, details like the date, camera settings, and, in many cases, the exact GPS coordinates of where the picture was taken. This hidden metadata is called EXIF (Exchangeable Image File Format), and it’s more powerful than it looks. While smartphones often automatically organize your photos, many of us still have massive collections stored on a , where sorting and searching manually can feel impossible.By extracting EXIF data, you can do much more than just learn which lens or exposure setting was used. You can index, organize, and search your entire photo library in ways that go far beyond filenames and folders. Want to pull up every photo taken in Paris? Or quickly filter for shots within 10 kilometers (about 6 miles) of Central Park? With EXIF indexing, that becomes not only possible but straightforward.In this article, we’ll explore how to extract EXIF metadata, build an index of your photos, including those on NAS drives, and run location-based searches to find exactly what you’re looking for.When you take a photo, your camera doesn’t just capture light, it also records a set of descriptive details about the image, known as . EXIF stands for Exchangeable Image File Format, and it’s a standardized way of embedding extra information directly into the image file itself.Think of EXIF as the "digital notebook" your camera keeps for each shot. Some of the most common fields include: – the exact date and time the photo was taken. – make, model, lens, focal length, aperture, shutter speed, ISO. – latitude, longitude, and sometimes altitude, if location services were enabled.Among these, the GPS data is especially powerful for organizing and searching photos. Cameras and smartphones typically store coordinates in a format based on degrees, minutes, and seconds. For example:Latitude: 40° 46′ 56.62″ N  
Longitude: 73° 58′ 0.85″ W  
Altitude: 15.0 m  
This information can be converted into decimal degrees (e.g., ), which is a more convenient format for indexing and performing calculations like distance searches.EXIF isn’t just technical clutter inside your photos. It’s a hidden layer of context that tells you  and  a picture was taken, and with what gear, making it a goldmine for indexing and retrieval.
  
  
  Extracting EXIF Data from Photos
Now that we know what EXIF metadata is, the next step is learning how to actually . Python offers several libraries that make this easy: – simple and modern library to read and write EXIF data, including GPS coordinates and altitude. – lightweight library for reading EXIF metadata from JPEG and TIFF files. – popular imaging library that can read and manipulate images, including EXIF tags. – designed for both reading and writing EXIF data, useful if you need to modify metadata.For modern projects,  is often the most straightforward and Pythonic choice.
  
  
  Reading GPS Coordinates and Altitude with Here’s a minimal Python script that reads GPS coordinates  from an image using :Let's take the example of this photo:This function returns a tuple like:(51.504105555555554, -0.074575, 77.88)  # Latitude, Longitude, Altitude in meters
If the image didn't have any geo-location metadata, it would return .
  
  
  Handling Missing or Corrupted EXIF Data
Not every photo will have usable EXIF metadata. For example:Some cameras or photo-editing software strip metadata to save space.Privacy-focused apps (like messaging platforms) often remove GPS coordinates.Altitude may not always be recorded, even if latitude and longitude exist.In rare cases, EXIF data may be partially corrupted.When building your index, always  and decide how to handle them, for instance, skipping photos without GPS tags, or indexing only the fields that are available.
  
  
  Building an Index of Photos
Extracting EXIF data from a single photo is useful, but the real power comes when you apply it to your . By creating an index, you can quickly search and filter images without repeatedly scanning every file.Loop through all files in a given directory (and subdirectories).Extract EXIF metadata from each photo using .Store the results in a structured format for later searching.Here’s a Python example that scans a directory and writes the extracted EXIF metadata into a CSV file:This script generates a  file with rows like:filename,timestamp,latitude,longitude,altitude,camera
england-london-bridge.jpg,2018:08:22 13:13:41,51.504105555555554,-0.074575,77.88,Pixel 2
germany-garching-heide.jpg,2018:08:29 19:31:19,48.268274999999996,11.603361111111111,540.05,Pixel 2
irland-dingle.jpg,2012:09:16 16:58:02,52.139276657230475,-10.274594797178132,,DMC-FX60
italy-garda-lake-sailing-club.jpg,2018:09:16 11:08:41,45.877630555555555,10.857161111111111,71.95,Pixel 2
japan-katsura-river.jpg,2016:11:12 16:13:18,35.014377,135.669015,0.0,MI 5
taiwan-jiufen.jpg,2016:04:04 19:35:38,25.10820386111111,121.8439483611111,279.0,GT-I9505
turkey-bodrum.jpg,2018:10:18 18:16:32,37.02995277777778,27.41326388888889,79.19,Pixel 2

There are multiple ways to store the index, each with pros and cons:✅ Easy to read, portable, no setup required.❌ Searching can be slow for large collections (tens of thousands of photos).SQLite (or Postgres for larger setups)✅ Efficient queries, support for filtering, sorting, and even spatial queries.✅ Scales better for very large photo libraries.❌ Requires a bit more setup and knowledge of SQL.For small to medium personal collections, a CSV or JSON file is perfectly fine. For larger archives or a search engine interface, consider a database backend.
  
  
  Searching Photos by Location
Once you have a structured index of your photos with GPS data, the next step is . There are different approaches depending on how precise or flexible you want the search to be.
  
  
  Simple Approach: Exact Coordinate Search
The most basic method is to match photos that have the exact latitude and longitude. This is straightforward but rarely practical, since GPS coordinates can have minor variations: This approach only works if the coordinates exactly match, which is rare in real-world GPS data.
  
  
  Advanced Approach: Radius-Based Search
A more practical solution is to search for photos  of a location. The  is commonly used to calculate the great-circle distance between two points on the Earth:This will return all photos  of the target coordinates, for example:england-london-bridge.jpg 3.70 km away

  
  
  Tools and Libraries for Spatial Queries
For more advanced use cases or large datasets, several Python libraries and database features can simplify the process: – Geocoding and distance calculations. – Geometry operations and spatial queries in Python. – Use databases like  for efficient radius searches, polygon queries, or bounding boxes.By combining EXIF metadata indexing with spatial searches, you can quickly find photos taken near landmarks, cities, or even a friend’s house. This opens the door to building personal mapping tools or automated photo albums sorted by location.Indexing photos using  transforms your photo collection from a static archive into a searchable, organized library. By extracting GPS coordinates, timestamps, and camera information, you can locate photos based on location, date, or device.By combining this indexing with spatial searches, you gain the ability to find photos within a radius, track journeys over time, or group images by location. This allows for turning raw data into actionable insights.Leveraging the EXIF metadata, you can turn a simple collection of images into a powerful, location-aware photo library, making lost memories instantly findable and your workflow dramatically more efficient.]]></content:encoded></item><item><title>Bypass Bot Detection with Python Selenium. 🤖</title><link>https://dev.to/thetanweerali/bypass-bot-detection-with-python-selenium-3p44</link><author>Ali</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 07:29:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Bypassing Bot Detection Software with Selenium in Python]]></content:encoded></item><item><title>The Growing Need of Online Tools in 2025</title><link>https://dev.to/toolquix/the-growing-need-of-online-tools-in-2025-omj</link><author>Toolquix</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 06:54:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s fast-paced digital world, efficiency and accessibility are more important than ever. Whether you’re a student, a blogger, a designer, or just someone trying to get daily tasks done faster, having the right online tools can make a huge difference.Time-saving: Instead of installing heavy software, online tools let you get things done instantly from your browser.Cross-platform accessibility: Work seamlessly from desktop, tablet, or mobile without worrying about compatibility issues.Cost-effective: Many online tools are free or freemium, reducing the need for expensive software licenses.Centralized workflow: Consolidating tasks like file conversion, text formatting, or color code generation in one place saves both time and mental effort.One platform that’s addressing this need is Toolquix
. It’s a free hub of online tools designed for productivity and convenience. Some of the features include:File Conversion: Quickly convert text, PDF, and HTML files without downloading software.Text Utilities: Remove duplicates, format text, and even generate Unicode text styles.Color & Design Tools: Convert HEX, RGB, HSL, CMYK values, or pick colors for your design projects.Productivity Boosters: Simple tools that save time for students, bloggers, and professionals alike.Toolquix makes it easy to accomplish everyday digital tasks without switching between multiple platforms.The Future of Online ToolsAs more people rely on the web for work, study, and creativity, the demand for efficient online tools will continue to grow. Platforms like Toolquix that centralize multiple utilities in one place are not just convenient—they’re becoming essential.Whether you’re a student trying to handle assignments, a designer needing fast color conversions, or a writer formatting content, having a reliable online tool hub is crucial.Check out Toolquix here: Toolquix
 and explore a wide range of tools designed to make your online tasks simpler and faster.]]></content:encoded></item><item><title>Diving Deep: Understanding the Mechanics</title><link>https://dev.to/dev_patel_35864ca1db6093c/diving-deep-understanding-the-mechanics-453c</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:51:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine you're baking a cake. You have the recipe (your machine learning algorithm), but the perfect cake depends on the precise amounts of each ingredient (your hyperparameters): the oven temperature, baking time, amount of sugar, etc. Getting these just right is crucial for a delicious outcome. This, in essence, is hyperparameter tuning. And Grid Search is one powerful technique to help us find that perfect recipe.Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to achieve the best possible performance. Hyperparameters are settings that are  learned from the data during training, unlike the model's parameters (weights and biases). They control the learning process itself. Grid Search is a brute-force approach to hyperparameter tuning where we systematically try out every combination of hyperparameters within a predefined range.Let's break down the core concepts:
  
  
  1. The Hyperparameter Landscape
Imagine a multi-dimensional space where each dimension represents a hyperparameter (e.g., learning rate, regularization strength). Each point in this space represents a unique combination of hyperparameters, and each point corresponds to a model's performance (e.g., accuracy, F1-score). Our goal is to find the point with the highest performance.
  
  
  2. The Grid Search Algorithm
Grid Search is a straightforward algorithm:Define the hyperparameter search space:  Specify the range and values for each hyperparameter.  For example:  in ,  in . Generate all possible combinations of hyperparameter values.  This forms our "grid" of points in the hyperparameter space. For each combination in the grid:Train the model using those hyperparameters.Evaluate the model's performance using a suitable metric (e.g., accuracy on a validation set). Choose the hyperparameter combination that yielded the best performance.Here's a simplified Python pseudo-code representation:
  
  
  3.  Mathematical Underpinnings (Optimization)
Grid Search doesn't explicitly use gradient-based optimization. Instead, it's a form of . Gradient-based methods, like gradient descent, rely on calculating the gradient (the direction of steepest ascent) of the performance function with respect to each hyperparameter. This gradient guides the search towards better hyperparameter combinations. Grid Search, however, simply tries all combinations and selects the best one. It's computationally expensive but conceptually simple.
  
  
  Real-World Applications and Impact
Grid Search, despite its simplicity, finds widespread application: Optimizing convolutional neural network (CNN) architectures by tuning hyperparameters like the number of layers, filter sizes, and learning rate.Natural Language Processing (NLP): Fine-tuning the hyperparameters of recurrent neural networks (RNNs) or transformers for tasks like sentiment analysis or machine translation. Adjusting the hyperparameters of collaborative filtering or content-based filtering algorithms to improve recommendation accuracy.
  
  
  Challenges and Limitations
  The number of combinations grows exponentially with the number of hyperparameters and the range of values.  This can be computationally prohibitive for complex models or large search spaces.  As the number of hyperparameters increases, the search space becomes incredibly vast, making it difficult to find the global optimum. Grid Search might get stuck in a local optimum, especially in non-convex performance landscapes.The computational cost of Grid Search can have environmental implications due to high energy consumption. Careful consideration of the search space and efficient algorithms are crucial to mitigate this.
  
  
  The Future of Hyperparameter Tuning
While Grid Search provides a valuable baseline, more sophisticated techniques like randomized search, Bayesian optimization, and evolutionary algorithms are gaining popularity due to their efficiency in handling high-dimensional search spaces. Research continues to explore more efficient and robust methods for hyperparameter optimization, addressing the challenges of scalability and the need for less computationally expensive solutions. The quest for the perfect hyperparameters continues, driving innovation in the field of machine learning.]]></content:encoded></item><item><title>Title: Unraveling the Mysteries of Ancient Rome: A Journey Through the Everyday Life of an Ordinary Roman</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-unraveling-the-mysteries-of-ancient-rome-a-journey-through-the-everyday-life-of-an-ordinary-5cn7</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:25:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: Unraveling the Mysteries of Ancient Rome: A Journey Through the Everyday Life of an Ordinary Roman
Imagine living in a world where roads were built for conquest, not convenience. Where sex, trade, and culture operated under systems of inequality. And yet, despite these challenges, ideas and identities moved faster than we might think. This is the fascinating world of Ancient Rome, a complex, uneven, and often uncomfortable prototype of globalization.In this blog post, we'll take a closer look at what it was like to live in Ancient Rome as an ordinary person, navigating daily life. We'll explore the roads, the sex, the trade, and the culture, and see how they all fit together to create a world that was both familiar and foreign to us.First, let's talk about the roads. The Romans were famous for their engineering feats, and their roads were no exception. They built roads all over their empire, connecting cities and towns and making it easier for people and goods to move around. But these roads were not built for convenience. They were built for conquest. The Romans believed that having a well-connected empire was essential for maintaining control over their territories, and so they invested heavily in building and maintaining their roads.Next, let's talk about sex. Sex was an important part of Roman culture, and it was often used as a way to assert power and status. Men were expected to be the active partners in sexual relationships, while women were expected to be passive. However, there were also many examples of same-sex relationships in ancient Rome, and these were often accepted and even celebrated.Moving on to trade, the Romans were skilled traders. They established a system of currency that allowed for the exchange of goods and services across their empire. They also built ports and markets to facilitate trade, and they encouraged the growth of industries such as agriculture and mining.Finally, let's talk about culture. The Romans had a rich and diverse culture, with influences from all over the world. They were known for their art, literature, and architecture, and they were also famous for their festivals and celebrations. However, like many ancient societies, the Romans also had systems of inequality in place. The wealthy and powerful held most of the power, while the poor and marginalized were often left out of the decision-making process.Despite these challenges, the Roman Empire was a remarkable achievement. It was a complex, uneven, and often uncomfortable prototype of globalization, with roads, sex, trade, and culture all operating under systems of inequality. But despite these challenges, ideas and identities moved faster than we might think, and the legacy of the Roman Empire continues to shape our world today.So, the next time you're driving on a well-connected highway or enjoying a piece of Roman art, take a moment to appreciate the incredible achievements of this ancient civilization. And remember, even in the most complex and uneven of societies, there is always room for growth and change.]]></content:encoded></item><item><title>Title: Discovering a Rare Type of Black Hole Feasting on a Star</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-discovering-a-rare-type-of-black-hole-feasting-on-a-star-2a4k</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:20:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: Discovering a Rare Type of Black Hole Feasting on a Star
As a science and space enthusiast, I am always excited to learn about new discoveries in the field of astronomy. Recently, NASA's Hubble Space Telescope and NASA's Chandra X-ray Observatory teamed up to identify a new possible example of a rare class of black holes. This discovery was made by observing X-ray emission (in purple) in an image released on July 24, 2025.The black hole in question, called NGC 6099 HLX-1, is located in a compact star cluster in a giant elliptical galaxy. This makes it a unique find, as most black holes are found in the centers of galaxies, and not in compact star clusters.Black holes are incredibly dense objects, with masses up to several times that of our Sun. They are formed when a massive star collapses under its own gravity. Black holes are also known for their intense gravitational pull, which can cause objects to be pulled in and never escape.One of the most fascinating things about black holes is their ability to consume matter. As matter falls towards a black hole, it heats up and emits X-rays. This is what scientists observed in the case of NGC 6099 HLX-1. The bright X-ray source in the image suggests that the black hole is consuming matter from a nearby star.This discovery is particularly interesting because it provides evidence for a rare type of black hole known as a "hypermassive black hole." Hypermassive black holes are incredibly massive, with masses up to several billion times that of our Sun. They are also thought to be formed in the early universe, during the formation of the first galaxies.The discovery of NGC 6099 HLX-1 is a significant milestone in our understanding of black holes and their behavior. It provides valuable insights into the formation and evolution of these mysterious objects, and opens up new avenues for research in the field of astronomy.In conclusion, the discovery of NGC 6099 HLX-1 is a fascinating find for science and space enthusiasts. It provides evidence for a rare type of black hole and sheds light on the formation and evolution of these mysterious objects. As we continue to explore the universe, discoveries like this one remind us just how much there is still to learn about the wonders of the cosmos.]]></content:encoded></item><item><title>TITLE: Tesla Partially Held Liable for Deadly 2019 Crash Involving Autopilot Self-Driving Feature</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-tesla-partially-held-liable-for-deadly-2019-crash-involving-autopilot-self-driving-feature-4d8k</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:15:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  TITLE: Tesla Partially Held Liable for Deadly 2019 Crash Involving Autopilot Self-Driving Feature
DESCRIPTION: In a landmark case, a jury in Florida has found Tesla partially liable for a 2019 crash involving the company's Autopilot self-driving feature. The verdict, which was handed down on February 19, 2021, means that Tesla will have to pay $200 million in damages. Autopilot is a feature that comes pre-installed on Tesla's cars and is designed to handle things like collision detection and emergency braking.The case, which was brought by the family of Naibel Benavides Leon and Dillon Angulo, who were killed in the crash, played out differently from other cases involving Tesla's Autopilot feature. The jury ultimately decided that the self-driving tech enabled driver George McGee was at fault for the crash, which occurred on March 1, 2019, in Fort Lauderdale, Florida.During the trial, Tesla's lawyers argued that McGee's decision to take his eyes off the road to reach for his phone was the cause of the crash, and that Autopilot should not be considered. However, the plaintiffs argued that Tesla and Elon Musk, the company's CEO, had marketed Autopilot as a fully autonomous driving system, which led to a false sense of safety and contributed to the crash.The verdict in this case is significant because it marks the first time that Tesla has been held liable for a crash involving its Autopilot feature. The company has mostly avoided taking responsibility for crashes involving cars with the Autopilot enabled, but this case sets a precedent for future cases.The $200 million in damages that Tesla will have to pay is a significant amount, and it will likely have a financial impact on the company. However, the verdict is also a reminder that technology is not infallible, and that drivers must remain vigilant and attentive while operating a vehicle, even when using advanced safety features like Autopilot.In conclusion, the verdict in this case is a landmark moment for the automotive industry and a reminder that technology is not a substitute for human responsibility. Tesla must continue to work towards improving its Autopilot feature and ensuring that it is used safely and responsibly by all drivers.]]></content:encoded></item><item><title>Building a Production-Ready Soft Delete System in Django (with Custom User Model)</title><link>https://dev.to/saveen_kumar_4e9c80304ebe/building-a-production-ready-soft-delete-system-in-django-with-custom-user-model-44dd</link><author>Saveen Kumar</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 20:00:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Soft delete sounds simple—until you're the one implementing it in a real-world, regulated application.In building a financial portfolio management system, we faced the not-so-fun challenge of handling user deletion without compromising data integrity or violating compliance rules. You can't just delete() a user when audit trails, tax records, and GDPR are watching.So, here's how we designed a clean, maintainable soft delete system using a custom Django User model.
Most finance or SaaS platforms need to:Retain user-related transactions for tax/audit purposesDisable login access cleanlyRestore accidentally deleted accountsAvoid cascading deletions of historical data
Using Django’s built-in User + separate UserProfile quickly turned into a nightmare: joins everywhere, edge cases all over the place, and no easy path to soft delete.So we followed Django’s best practice: own your User model from day one.
Here's a quick breakdown of the implementation:✅ Custom User model based on AbstractUser✅ Added is_deleted, deleted_at, deleted_by✅ Overrode the admin to support soft deletion & restoration✅ Used on_delete=models.PROTECT for critical models like Transaction✅ Queryset filters and indexes for is_deletedis_active=False prevents loginSoft deletes ≠ just hiding records — handle reversibility and auditingNever on_delete=CASCADE sensitive data like financial historyUse admin actions for bulk delete/restore and badge UI for status
Soft delete isn’t just for compliance. It protects you from:Breaking historical reportingGDPR data logic edge casesLimitations of Django’s default User model
Plus, migration from default User → custom User later is a huge pain. Better to do it upfront.💬 
Have you implemented soft delete in production? Found better patterns, or do you prefer packages like django-safedelete? Would love to hear your experience or suggestions for scaling this better.🧵 Or drop thoughts below 👇]]></content:encoded></item><item><title>Creating Stock Data | building stocksimpy 3</title><link>https://dev.to/suleyman_sade/creating-stock-data-building-stocksimpy-3-28dg</link><author>Suleyman Sade</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 17:55:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[StockSimPy is a lightweight Python library for simple stock backtesting. The goal is to understand Pandas, experiment with stock strategies better, and create an easy-to-use alternative to more complex backtesting tools. This is part 3 of the series where I build this library in public.After finishing basic indicator calculation functions, I needed a way to keep track of all the stock information in an organised, reusable format. That’s where the  comes in — it acts as a container for everything you’ll need in backtesting or simulation.I initially thought it should be easy to code as it just needed to keep the information and require some simple import and export, but I was quite wrong. Turns out working with data can be messy.When importing stock data, you can’t assume the columns are always consistent. Strategies require the use of different features, but some fields are essential:The tricky path — though — is naming conventions. What do I mean? Let's take “Open” as an example; it could show up as “OPEN”, “open”, “OpeN”, “open_price”, “OpenPrice”, “openPrice”, and many other wild naming styles.Lowercasing handles some cases, but what about the ones with “price” in the name? Then I thought — I could easily search for the substring “open” in the whole word. This covers all the cases I mentioned above, but if open is named something else entirely, it wouldn’t work.A more comprehensive approach might be to create a full-blown synonym-matching system. But that might be overkill for now. Still, I might add it as a feature in the future if somebody requests it.The most important feature of  is importing data—without that, it’s just an empty shell.I was quite skeptical about creating these import functions at first. I considered leaving import up to the user — just pass in a Pandas DataFrame — but having built-in loaders felt more convenient. So far,  supports imports from:(This process felt quite  as I was just using built-in pandas functions or just straight-up copying documentation.)To simplify things, I added an function that picks the correct import based on the file extension of  parameter. I used  so users can pass in additional parameters.On top of that,  integrates directly with  (optional dependency). This allows fetching live stock data for a given ticker and date range, making it much more practical.For testing purposes, there’s also a  function. It isn’t designed for real backtesting but is useful for experimenting with new features.Here is a question: why export data you already imported? Two reasons: Users might want to inspect or clean their data after transformations. I will soon integrate the indicator functions from earlier posts, with  so exporting results will be handy.Export currently supports all the same formats mentioned in import, plus SQL. There is also a flexible  function that lets you define your own export method.It was such a twist, this step turned out to be more about data flexibility rather than really "storing data." With StockData in place, stocksimpy now has a solid foundation for testing.If you want to use this library in the future, or have any ideas that I could add, go for it. Ask me in comments, connect with me on socials. I want to make this project something useful.Follow the rest of the series, watch me build in public.]]></content:encoded></item><item><title>A series that is hype free, optimistic and cautious, but most of all written accessibly no matter your current level. All things dev&apos;s should understand about #ai fast. Thanks Dev. This should be a book &amp; course next. @dev_patel_35864ca1db6093c</title><link>https://dev.to/leogopal/a-series-that-is-hype-free-optimistic-and-cautious-but-most-of-all-written-accessibly-no-matter-1mi5</link><author>Leo Gopal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 16:52:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Decoding the Secrets of Your Machine Learning Model: Confusion Matrices, ROC Curves, and AUC]]></content:encoded></item><item><title>Election Management System (EMS) – Secure Web-Based Digital Voting Platform</title><link>https://dev.to/abubakar_shabbir/election-management-system-ems-secure-web-based-digital-voting-platform-228a</link><author>AbuBakar Shabbir</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 15:51:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I, , built the Election Management System (EMS), a modern, secure, and user-friendly digital voting web application using Python, Django, MySQL, and Bootstrap. This platform provides a transparent and efficient way to manage elections, handle voter registration, and monitor results in real-time. Register voters and allow secure logins with OTP verification.
 Email OTP ensures only verified voters can access the system.
 Separate dashboards for Admins, Voters, and Candidates.
 Add and manage candidates, control elections, monitor voters, and view real-time results.
 One vote per voter linked to a unique CNIC, preventing duplicate voting.
Real-Time Election Results: Display results by constituency and party for transparency.
 Can run locally or on a live server with MySQL backend.
 Bootstrap, HTML, CSS
 OTP via Gmail SMTP
This project is ideal for secure election management for educational institutions, organizations, or local communities. It emphasizes security, transparency, and user experience, making voting easier and tamper-proof.The Voter Panel displays only the elections that have been created and approved by the admin. Each voter can view the elections they are eligible for and cast their vote securely within the specified election. This ensures role-specific access and prevents any unauthorized voting.This project was developed by , focusing on secure web applications and modern software engineering practices.]]></content:encoded></item><item><title>Turning My Daily Commute into a Data Visualization Project</title><link>https://dev.to/kauldeepak78/turning-my-daily-commute-into-a-data-visualization-project-28l8</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 14:30:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most people see their daily commute as wasted time. I saw it as a dataset.For months, I logged the details of my everyday journey to work — departure times, train delays, walking speed between stations, even how my mood shifted with the weather. What started as a way to distract myself during long rides turned into a data visualization project that revealed patterns I would have never noticed otherwise.In the beginning, I kept it simple. I opened Google Sheets on my phone and manually entered:Departure & arrival times for each leg of my commute.Walking duration between home, station, and office.Noise level estimate inside the train (low, medium, high).Mood score — a quick 1–5 rating.After a few weeks, the manual entry became too repetitive. So I leveled it up:Wrote a Python script that used GPS logging on my phone to track walking/ride times automatically.Pulled weather data from an open API to log rain, temperature, and snow.Used a smartwatch app to grab step counts + heart rate, which I synced into my dataset.Suddenly, I wasn’t just collecting numbers — I was building a story of my commute.With data in hand, I started exploring visualization tools:Matplotlib & Seaborn in Python gave me quick charts: average commute times, day-of-week trends, and mood vs. weather.Tableau let me create a dashboard showing how commute length shifted across weeks and seasons.D3.js gave me an interactive timeline where I could hover over a date and see all the conditions (time, mood, noise, weather).The more I visualized, the more I realized: my commute wasn’t random chaos — it had rhythm.Here are some surprising insights from my data experiment:= Pain – My commute delays were 25% higher on Mondays than midweek.= Mood Killer – On rainy days, my mood score dropped by 40%, regardless of delays.– Leaving just 7 minutes earlier reduced my average commute time by 15%. – The loudest rides weren’t at rush hour but on evenings when major sports events were happening — apparently, fans and train noise go hand-in-hand.These weren’t just fun facts — I actually started leaving earlier and packing headphones when I knew a big game was on. – My mornings became less stressful once I knew the “sweet spots” to leave. – I got hands-on practice in Python, APIs, and data visualization tools. – I now had a personal project I could show in interviews to demonstrate data storytelling. – Instead of seeing my commute as wasted time, I turned it into a learning experiment.Not every data project has to start in a lab, a hackathon, or a work assignment. Sometimes the best datasets are sitting in your daily routine. By tracking small details, you can uncover patterns that change the way you live and along the way, you sharpen your skills as a developer.So next time you’re bored on your way to work, ask yourself : what could I measure here?]]></content:encoded></item><item><title>Why I Built an &quot;Awesome List&quot; for Data Analysis (And How It Can Help You)</title><link>https://dev.to/pavelgrigoryev/why-i-built-an-awesome-list-for-data-analysis-and-how-it-can-help-you-22pm</link><author>Pavel Grigoryev</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 14:22:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Curated List of 400+ Data Analysis Tools and Resources
Learning data analysis often means sifting through endless tutorials, docs, and repos. It's easy to get lost in outdated or low-quality content.I built this curated list to solve that problem. It's a organized collection of the most useful resources I've found — no fluff, no ads, just practical tools and knowledge.It's a structured learning path covering the :
  
  
  💡 My Story & Why I Built This
This repository started as my personal collection of bookmarks. Over time, it grew beyond just links into a structured knowledge base.
I realized this organized system could help others too, so I cleaned it up and decided to share it publicly.The goal is simple: save you 100+ hours of Googling and help you focus on what actually matters — building skills.
  
  
  🤔 How You Can Help (Seriously!)
This list is good, but I want it to be better. And for that, I need your expert eyes.I'd be incredibly grateful if you could:: What's the one amazing tool or resource that's missing?: Does the grouping make sense? Should we add a new section?: Brutal honesty is appreciated. This is a project for the community.Your feedback isn't just welcome; it's essential. I'll be actively updating the repo based on the comments here.Thank you for your time - I really appreciate it! 🤗]]></content:encoded></item><item><title>Real Python: Working With JSON Data in Python</title><link>https://realpython.com/python-json/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 20 Aug 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Python’s  module provides you with the tools you need to effectively handle JSON data. You can convert Python data types to a JSON-formatted string with  or write them to files using . Similarly, you can read JSON data from files with  and parse JSON strings with .JSON, or JavaScript Object Notation, is a widely-used text-based format for data interchange. Its syntax resembles Python dictionaries but with some differences, such as using only double quotes for strings and lowercase for Boolean values. With built-in tools for validating syntax and manipulating JSON files, Python makes it straightforward to work with JSON data.By the end of this tutorial, you’ll understand that:JSON in Python is handled using the standard-library  module, which allows for  between JSON and Python data types.JSON is a good data format to use with Python as it’s  and straightforward to serialize and deserialize, which makes it ideal for use in .You write JSON with Python using  to serialize data to a file.You can  using Python’s  module.Since its introduction, JSON has rapidly emerged as the predominant standard for the exchange of information. Whether you want to transfer data with an API or store information in a document database, it’s likely you’ll encounter JSON. Fortunately, Python provides robust tools to facilitate this process and help you manage JSON data efficiently.While JSON is the most common format for data distribution, it’s not the only option for such tasks. Both XML and YAML serve similar purposes. If you’re interested in how the formats differ, then you can check out the tutorial on how to serialize your data with Python. Test your knowledge with our interactive “Working With JSON Data in Python” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of working with JSON in Python. By working through this quiz, you'll revisit key concepts related to JSON data manipulation and handling in Python.The acronym  stands for JavaScript Object Notation. As the name suggests, JSON originated from JavaScript. However, JSON has transcended its origins to become language-agnostic and is now recognized as the standard for .The popularity of JSON can be attributed to native support by the JavaScript language, resulting in excellent parsing performance in web browsers. On top of that, JSON’s straightforward syntax allows both humans and computers to read and write JSON data effortlessly.To get a first impression of JSON, have a look at this example code:You’ll learn more about the JSON syntax later in this tutorial. For now, recognize that the JSON format is . In other words, you can create JSON files using the code editor of your choice. Once you set the file extension to , most code editors display your JSON data with syntax highlighting out of the box:The screenshot above shows how VS Code displays JSON data using the Bearded color theme. You’ll have a closer look at the syntax of the JSON format next!In the previous section, you got a first impression of how JSON data looks. And as a Python developer, the JSON structure probably reminds you of common Python data structures, like a dictionary that contains a string as a key and a value. If you understand the syntax of a dictionary in Python, you already know the general syntax of a . Later in this tutorial, you’ll learn that you’re free to use lists and other data types at the top level of a JSON document.The similarity between Python dictionaries and JSON objects is no surprise. One idea behind establishing JSON as the go-to data interchange format was to make working with JSON as convenient as possible, independently of which programming language you use:[A collection of key-value pairs and arrays] are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages is also based on these structures. (Source)To explore the JSON syntax further, create a new file named  and add a more complex JSON structure as the content of the file:In the code above, you see data about a dog named Frieda, which is formatted as JSON. The top-level value is a JSON object. Just like Python dictionaries, you wrap JSON objects inside curly braces ().In line 1, you start the JSON object with an opening curly brace (), and then you close the object at the end of line 20 with a closing curly brace ().]]></content:encoded></item><item><title>Python tips and tricks</title><link>https://dev.to/mcheremnov/python-tips-and-tricks-13bj</link><author>Maksym</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 13:14:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Here are some practical Python tips and tricks that can make your code more efficient and elegant:
  
  
  String and Text Manipulation
 - Use f-strings instead of  or  formatting:Multiline strings with triple quotes - Great for SQL queries or documentation:
  
  
  List and Dictionary Operations
 - More concise than traditional loops:Dictionary comprehensions:Use  for safe dictionary access:Use  and  for boolean operations:
  
  
  Function and Class Tricks
Default mutable arguments - Avoid the common pitfall:kwargs` for flexible functions**:
  
  
  Built-in Functions and Modules
 instead of manual counting: for parallel iteration: for counting: for file operations:Use generators for large datasets:EAFP (Easier to Ask for Forgiveness than Permission):Always use context managers for file operations:
  
  
  Debugging and Development
Use  for complex data structures: for debugging (Python 3.7+):These techniques can significantly improve your Python code's readability, performance, and maintainability. The key is knowing when to apply each one based on your specific use case.]]></content:encoded></item><item><title>Why Your AI Chatbot is Dumb — And How to Fix It with AutoGPT Agents</title><link>https://dev.to/ekwoster/why-your-ai-chatbot-is-dumb-and-how-to-fix-it-with-autogpt-agents-1kep</link><author>Yevhen Kozachenko 🇺🇦</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 13:11:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Why Your AI Chatbot is Dumb — And How to Fix It with AutoGPT Agents
Let’s face it — most chatbots suck. You’ve interacted with them: they greet you politely, but when you ask them anything beyond their training doc, they crumble like discount cookies. What we have today is a sea of chatbots that pretend to be intelligent, but are essentially glorified FAQ search boxes.But what if your chatbot could reason, plan, and act? Welcome to the world of autonomous AI agents — your chatbot’s smarter, more ambitious cousin.In this deep-dive, we'll walk through how to build a simple yet powerful AI agent using Python that can learn, plan tasks, and do them using tools like AutoGPT concepts and langchain. This isn’t just theory — I’ll show you real code, real modules, and real-world use cases.
  
  
  🤯 What’s Wrong with Traditional Chatbots?
Let’s kick off with how traditional bots are structured:They follow a conversation tree or rulesThey rely on static intents and entitiesThey answer only from a predefined FAQ or knowledge baseSo, if I asked a bot: "Can you summarize today's news about AI startups and email it to me?", most will either:Redirect me to a support page 📄Say: "Sorry, I don’t understand." 🤖😕That’s because they don’t have tools, memory, or reasoning. They're not agents. To BUILD an intelligent assistant, you need something that can:Create a sequence of actionable stepsExecute tools (like Google search, summarizers, email APIs)Track memory/state over timeEnter AutoGPTs and AI Agents.
  
  
  🧠 Breaking Down AI Agents (AutoGPT-Style)
AI Agents combine multiple capabilities:Large Language Model (LLM) like GPT-4 for reasoningPlanning + Subtask generationMemory/State using vector DBsTool use (like searching, file handling, APIs)The magic happens by chaining LLM calls that:Take an overall objective e.g., “Find trending startups in AI and create a spreadsheet.”Create sub-goals: search for news, identify startups, extract descriptions, write to CSVIt’s like having a junior intern... powered by reasoning.
  
  
  🛠️ Let’s Build Your First AI Agent 🧪
serpapi (for Google search)
  
  
  👉 Step 1: Install What You Need
pip langchain openai pydantic serpapi
Set them as environment vars:
  
  
  👉 Step 2: Create a Base Tool — Google Search Wrapper

  
  
  👉 Step 3: Create an Agent With a Goal
You’ll see printouts of the agent thinking through:Outputting a conclusion ✅
  
  
  🧠 Want to Persist Memory?
Use langchain.memory with a vector database like FAISS or ChromaDB to store chunks of conversation or steps the agent took.Pass it into initialize_agent(memory=memory).Ask your bot to research topics and write outlines
  
  
  ✅ Automated Interview Prep
Have it simulate interviewers, gather company data
  
  
  ✅ Email Summarizer & Responder

  
  
  🚨 Common Pitfalls & Fixes
Use streaming API + handle errors gracefullyLimit steps and monitor planning logicValidate inputs & sanitize outputsUse vector DBs and embed chunking
  
  
  Final Thoughts — Why Agents Are the Future
If chatbots were the browser, agents are the operating system.They’re not perfect yet, but the combination of:…redefines how we automate. With upcoming integrations into operating systems (e.g., Copilot, Apple Intelligence), understanding agents gives you superpowers.So — next time someone builds a chatbot, ask them:“Cool. But can it plan and use tools?”Otherwise… it’s just a fancy Clippy with a neural net.Here’s a full working mini-agent prototype on GitHub:Stay curious — we’re just getting started.Follow me for live demos, AI agent builds, and API automation hacks.]]></content:encoded></item><item><title>Day 2 of 100</title><link>https://dev.to/lyop_achayi/day-2-of-100-5ema</link><author>TANYA LYOP ACHAYI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 12:57:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Confession: I’ve always been scared of mathematics, But today I realized coding isn’t about being a math genius, it’s about breaking problems into simple steps. As a Media enthusiast exploring Python programming, I’m learning that even numbers can feel like play! Fear aside, I’m ready to keep learning, one line at a time. ]]></content:encoded></item><item><title># 🎯 Face Landmarks Detection (OpenCV DNN + Facemark)</title><link>https://dev.to/ertugrulmutlu/-face-landmarks-detection-opencv-dnn-facemark-440d</link><author>Ertugrul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 12:13:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA["Faces don’t lie — but landmarks sometimes do."Hey there! In this post, I’ll share my journey of building a Face Landmark Detection pipeline using  and . The system takes a raw video as input, detects faces, extracts , smooths them across frames, and finally outputs:an  with landmarks and bounding boxesan optional  with landmark coordinates for every frame"Take a face → get the points."But to make it robust, I had to mix  with  and add a touch of signal processing.The project is split into modular components: → Loads and runs the DNN-based face detector (SSD ResNet) → Drawing utilities for the 68-point facial structure → Video I/O, CSV logging, smoothing, and per-frame pipeline → Entry point to run the full pipeline
  
  
  🔍 Step 1 — Face Detection
I used OpenCV’s Deep Neural Network (DNN) SSD ResNet model. The detector takes each frame, converts it into a blob, and feeds it into the Caffe network:This gives us bounding boxes with confidence scores. I kept only the ones above a threshold ().
  
  
  🎯 Step 2 — Landmark Extraction
With face boxes ready, I used  to extract the :This returns arrays shaped  → coordinates for jawline, eyebrows, eyes, nose, and lips.
  
  
  📉 Step 3 — Landmark Smoothing
Raw landmarks jitter a lot between frames. To stabilize them, I applied an Exponential Moving Average (EMA):This keeps the motion natural but removes frame-by-frame noise.
  
  
  🖼️ Step 4 — Drawing the Mesh
I grouped the 68 points into face regions and connected them with polylines:The result? A clear, real-time facial mesh overlay.  frame_idx,x0,x1,...,y66,y67
  0,123,130,...,200,205
  1,124,129,...,199,206
This makes the system useful both for visualization  downstream ML tasks.DNN face detection is robust, but combining it with traditional landmarking is still effective.Smoothing is  — raw landmarks are too noisy for real use.CSV logging adds value for research/analytics beyond just visualization.You can find the full code here:"A single face in a frame is simple — but tracking it smoothly across time is where the real challenge begins."]]></content:encoded></item><item><title>data analytics course in lucknow</title><link>https://dev.to/ammu_salveru_3e679d4b532a/data-analytics-course-in-lucknow-4fkj</link><author>ammu salveru</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 11:19:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>