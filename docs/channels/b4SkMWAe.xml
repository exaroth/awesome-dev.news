<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>Five Great DevOps Job Opportunities</title><link>https://devops.com/five-great-devops-job-opportunities-126/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 17 Feb 2025 09:00:21 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses</title><link>https://www.wiz.io/blog/kubernetes-report-preview-2025</link><author>/u/Wownever</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 08:45:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[In the ever-evolving world of cloud-native technologies, Kubernetes continues to reign supreme - and with great power comes great responsibility. Our latest Kubernetes Security Report Refresh is coming soon and will unveil a landscape of both peril and progress.Â As a special sneak preview, let's explore the key findings that are shaping the future of container security.Â AKS clusters face probing attempts a mere 18 minutes after deployment.Picture this: Your freshly deployed public , barely out of its digital infancy, already under siege. Our research reveals a startling reality where malicious actors operate at breakneck speeds, probing for weaknesses before the digital ink has even dried on your configuration files. This finding serves as a stark reminder: in the world of Kubernetes, security can never be an afterthought.Â As of October 2024, Kubernetes 1.29 now leads the pack, dethroning last year's 1.24Â End of Support versions down to 46% from 58% last year among the managed clustersÂ Here's a reason to celebrate: Kubernetes operators are leveling up their game. We're witnessing a sea change in version management practices, with teams swiftly adopting the latest releases and bidding farewell to outdated versions. This proactive stance isn't just about chasing the newest features - it's a robust defense against lurking vulnerabilities.Â Severe vulnerabilities in exposed pods slashed by 50%Â Significant drop in high-privilege pod countsÂ The data paints a picture of leaner, meaner workloads. Security teams are tightening the screws on , resulting in a dramatic reduction of critical flaws in exposed containers. Moreover, the principle of least privilege is gaining traction, with fewer pods wielding unnecessary powers. It's a testament to the growing sophistication of Kubernetes security practices.Â While these highlights offer a glimpse into the state of Kubernetes security, they're just the tip of the iceberg. To truly navigate the complexities of  in 2025, you need the full picture.Â While you await the full report (coming soon), check out some of our other Kubernetes content, including:Â ]]></content:encoded></item><item><title>How do you scale to zero and from zero?</title><link>https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/</link><author>/u/Electronic_Role_5981</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 08:05:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Electronic_Role_5981 ]]></content:encoded></item><item><title>How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?</title><link>https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/</link><author>/u/blvck_viking</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 07:03:10 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I'm currently using the GKE free tier and was wondering:Does GKE free tier include any free persistent storage, or will I need to pay for it?What's the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?Any recommendations on storage classes and best practices?   submitted by    /u/blvck_viking ]]></content:encoded></item><item><title>Ingress Help</title><link>https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/</link><author>/u/MeerkatMoe</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 01:04:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm trying to setup ingress using ingress nginx, but I can't figure out how to get routing to work...either my frontend breaks or my api is unreachable.I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.My first attempt was two separate ingresses (not sure about terminology):--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: "nginx" spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: "nginx" nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} but that didn't work, and sometimes my API won't get routed correctly. I think it's because they get combined and I can't guarantee the order.My next try was to combine them:kubernetes.io/ingress.class: "nginx" nginx.ingress.kubernetes.io/use-regex: "true" nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: "/(?!api).*" pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} (left some stuff out to save space)but that also didn't work.What is the best way to get this working? To summarize, I just need"/api/*" -> api service port 5000 (it can route as /api/<whatever> or just <whatever>)]]></content:encoded></item><item><title>Event driven workloads on K8s - how do you handle them?</title><link>https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/</link><author>/u/sniktasy</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 00:31:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have been working with Numaflow, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etcCurious, how are you handling event-driven workloads today? Would love to hear what's working for others?]]></content:encoded></item><item><title>Issues with logrotate when logrotate failed to rotate the logs for container</title><link>https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/</link><author>/u/barely_malted</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 21:06:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5) I am facing an issue where I believe these default values are not respected. The kubelet is failing with 'Failed to rotate log for container' 'err=failed to compress log (container/pod log paths) nospace left on device' At the same time one of my pods generated 200 GB logs in one single file. How is this possible ? I was not able to find out any documentation regarding this behaviour. Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ? K8s/EKS version 1.27]]></content:encoded></item><item><title>How do devs use kubernetes services locally via ingress on the likes of docker desktop</title><link>https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/</link><author>/u/TheRandyOne</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 19:28:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it'll really suck to have the devs to add a bunch of records to the hosts fileBasically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.Am I doing this wrong? Is there some other way that I should be approaching this problem? Or can someone explain how they deal with this other than just editing hosts files.]]></content:encoded></item><item><title>Pull Request testing on Kubernetes: working with GitHub Actions and GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/</link><author>/u/nfrankel</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 18:22:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Iâ€™m continuing my series on running the test suite for each Pull Request on Kubernetes. In the previous post, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, Iâ€™ve identified gaps that Iâ€™ll implement in this blog post:Create and configure a Google Kubernetes Engine instanceCreate a Kubernetes manifest for the app, with Kustomize for customizationAllow the GitHub workflow to use the GKE instanceBuild the Docker image and store it in the GitHub Docker repoInstall the PostgreSQL Helm chartFinally, run the end-to-end testStages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because Iâ€™m more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.]]></content:encoded></item><item><title>Managing a Talos cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/</link><author>/u/simen64</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 17:36:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM's in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?]]></content:encoded></item><item><title>Measure cpu utilization per deployment?</title><link>https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/</link><author>/u/netcat23</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 16:18:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi guys, does measuring cpu utilization of a deployment brings any value?What is you opinion about it?]]></content:encoded></item><item><title>Starting a Weekly Rancher Series â€“ From Zero to Hero!</title><link>https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/</link><author>/u/abhimanyu_saharan</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 14:47:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you're new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.I'll be posting new videos every week, so if you're interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you'd like to see covered!Letâ€™s build and learn together! ðŸš€]]></content:encoded></item><item><title>Digest #160: Heroku to AWS, GPU Twists, AMI Attacks, FinOps Tools and Solving Crimes with SQL</title><link>https://www.devopsbulletin.com/p/digest-160-heroku-to-aws-gpu-twists</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/_cjuQlc62uc" length="" type=""/><pubDate>Sun, 16 Feb 2025 14:38:29 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this weekâ€™s edition of the DevOps Bulletin!We start with a big moveâ€”from Heroku to AWSâ€”and even a surprising twist about GPUs. Thereâ€™s also news on a new attack that uses AWS AMI names, ideas on keeping AI safe, and a fun debate about trailing commas in SQL.Next, our featured podcast shows how finance and engineering work hand in hand to manage cloud costs, using tools like AWS CUR and CloudWatch. If youâ€™re looking for hands-on advice, our tutorials are packed with clear guidesâ€”from Terraform production tips and using BigQuery to easy guides for Azure Kubernetes, understanding unit economics, and more.We also highlight open-source devtools like a tool to spot Nginx issues, a fun game that uses SQL to solve mysteries, a smart tool for managing config files, and even a Rust-based ping tool that tracks network speed.All this and more in this weekâ€™s DevOps Bulletinâ€”donâ€™t miss out!This podcast explores how finance and engineering teams collaborate in FinOps to manage cloud costs. Success comes from aligning goals, sharing insights, and using AWS CUR and CloudWatch. is a tool that analyzes Nginx configuration to prevent security misconfiguration and automate flaw detection. is a game where you solve crimes with SQL queries and uncover evidence through data. is a lightweight configuration management tool that updates local config files using key/value stores like etcd or Consul. provides automated cost optimization for AWS and GCP. It includes a GCP Organization Recommender for cost-saving insights and an AWS Resource Cleanup tool to remove unused resources. is a Rust-based Ping tool using the ICMP protocol, offering real-time latency tracking, visual charts, and concurrent pinging of multiple addresses.If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Help with k3s setup on wsl</title><link>https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/</link><author>/u/watterbottle800</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 11:49:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with "failed to connect to <svc cluster ip> port 15672 : couldn't connect to server" after sometime. This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks]]></content:encoded></item><item><title>Kubernetes In-Place Pod Vertical Scaling</title><link>https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/</link><author>/u/Wownever</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 10:50:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are Resize CPU and Memory Resources assigned to Containers, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.What Is In-Place Pod Vertical Scaling?Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.Whatâ€™s behind the feature gate?The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.The result of the change attempt is communicated as part of the podsâ€™ status in a field calledÂ   (for more information on the new fields, check out the Kubernetes API documentation.)Additionally, this feature introduces the  in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.Dynamic Scaling: Modify CPU and memory allocations while pods run.No Restarts: Avoid downtime caused by pod restarts.Granular Control: Enable precise resource tuning for better efficiency.The InPlacePodVerticalScaling feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Hereâ€™s a detailed breakdown of how it operates: Activating the InPlacePodVerticalScaling feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.Dynamic Resource Updates via Kube API: With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., containerd v1.6.9 or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled. The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.Limitations and ConsiderationsWhile In-Place Pod Vertical Scaling offers significant benefits, it has limitations:1. Cloud Provider SupportAWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the GKE alpha clusters documentation.Several Kubernetes policies and mechanisms govern resource scaling. These include:Resource quotas limit the total CPU and memory usage for a namespace. If an InPlacePodVerticalScaling operation exceeds these limits, the scaling request will fail. For example:apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: example-namespace
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "32Gi"
Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: example-namespace
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
Admission controllers, such as Pod Security Admission or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.Memory-Bound Applications: Applications like Java with fixed Xmx parameters.In cases where the HPA is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:HPA scaling behavior is based on CPU average utilizationA pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created. Dynamically allocate resources during training and inference phases. Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling. Reduce waste by allocating the right amount of resources to each pod in real-time. Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Googleâ€™s example, Startup CPU Boost, demonstrates how dynamic resource scaling can address such scenarios effectively.1. Enable the Feature GateAdd the following configuration to enable the InPlacePodVerticalScaling feature:apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
controllerManager:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
scheduler:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
For GKE, create a cluster with alpha features enabled:gcloud container clusters create poc \
    --enable-kubernetes-alpha \
    --no-enable-autorepair \
    --no-enable-autoupgrade
Define a deployment with initial CPU and memory requests and limits:apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
          requests:
            memory: "64Mi"
            cpu: "250m"
        resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: NotRequired
Once deployed, you can check the cpu.weight, cpu.max, memory.max, memory.min from within the container to see the initial values that the container starts with.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max 
Adjust resource allocations for a running pod dynamically:kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"750m"}}}]}}'
Confirm updated resource settings:kubectl describe pod -l app=app
Additionally, you can connect to the container and see the change in cpu.weight, cpu.max, memory.max, memory.min from within the container.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max
In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and optimizing resource usage. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.While Googleâ€™s Kube Startup CPU Boost example is just a specific use case scenario, ScaleOps provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.]]></content:encoded></item><item><title>rke2 and DNS</title><link>https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/</link><author>/u/Affectionate_Horse86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 22:35:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == "server" and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.]]></content:encoded></item><item><title>Networking in K8s</title><link>https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/</link><author>/u/I-Ad-7</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:49:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Background: Never used k8s before 4 months ago. I would say Iâ€™m pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. I was put into a position where I had to use K8s for everyday work and donâ€™t get me wrong Iâ€™m ecstatic about being an intern but already having the opportunity to work with deployments etc. What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.Right now I can pretty much do most stuff and have some cool projects running on my cluster.Iâ€™m also learning new stuff every day. But where I find myself lacking is Networking. Not just in Kubernetes but also generally. There are two examples of me getting frustrated because of my lacking networking knowledge:I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failedWas wondering why I canâ€™t see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?I donâ€™t mind it but Iâ€™m struggling to learn Networking and itâ€™s not that I donâ€™t have the basics but I donâ€™t have the advanced knowledge needed yet, so how do I actually get there?]]></content:encoded></item><item><title>How are you monitoring your cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/</link><author>/u/psavva</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:28:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.CPU and networking are always displaying 'No Data'I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0That fixes the alerts on a fresh install of the stack. 1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.Any advice would be appreciated.If anyone has a fully working example of the helm chart values that fully work, that would be awesome.]]></content:encoded></item><item><title>Questions around LoadBalancer</title><link>https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/</link><author>/u/HahaHarmonica</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 18:36:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[New to k8s. Iâ€™ve deployed rke2 and iâ€™ve got several questions. Main Question) So iâ€™m trying to install rancher UI on it. When you go to install with helm it asks for a â€œhostnameâ€ and the hostname should be the name of your load balancerâ€¦i enabled the load balancer of rke2 but I have no clue how to operate with itâ€¦how do I change the configuration to point to rancher? The instructions arenâ€™t very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. 2) During my debugging, i ran the command â€œkubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldnâ€™t have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent nodeâ€¦actually be running.3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasnâ€™t the problem completely resetting the services by running the killall scripts on both of them before it worksâ€¦so why in the world wonâ€™t k8s actually respect systemctl and restart properly without literally shutting everything down. ]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>Tech Leaders Reveal New Approaches to Corporate Sustainability</title><link>https://devops.com/executive-strategies-and-innovations-driving-corporate-sustainability/</link><author>Bonnie Schneider</author><category>devops</category><pubDate>Sat, 15 Feb 2025 05:54:47 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[Over the past two years, Iâ€™ve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a â€œnice to have.â€ Itâ€™s now a core business strategy. Thatâ€™s the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies and Innovations Driving Innovation Corporate Sustainability. Why [â€¦]]]></content:encoded></item><item><title>DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps</title><link>https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 14 Feb 2025 15:22:34 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11</title><link>https://www.youtube.com/watch?v=eHa6GhK7L0I</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/eHa6GhK7L0I?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:59:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>ChatLoopBackOff Episode 46 (Dragonfly)</title><link>https://www.youtube.com/watch?v=gd6HRgr8KcA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/gd6HRgr8KcA?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 05:56:56 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. 

This CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the Node
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial Node is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.Chicken and egg problem sequence diagramThis new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a Node resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a Node resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one â€œcorrect wayâ€ to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting â€œhostNetworkâ€ to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure providerâ€™s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azureâ€™s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud providerâ€™s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>AWS CloudTrail network activity events for VPC endpoints now generally available</title><link>https://aws.amazon.com/blogs/aws/aws-cloudtrail-network-activity-events-for-vpc-endpoints-now-generally-available/</link><author>Esra Kayabali</author><category>devops</category><pubDate>Thu, 13 Feb 2025 23:17:54 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Today, Iâ€™m happy to announce the general availability of network activity events for Amazon Virtual Private Cloud (Amazon VPC) endpoints in AWS CloudTrail. This feature helps you to record and monitor AWS API activity traversing your VPC endpoints, helping you strengthen your data perimeter and implement better detective controls.Previously, it was hard to detect potential data exfiltration attempts and unauthorized access to the resources within your network through VPC endpoints. While VPC endpoint policies could be configured to prevent access from external accounts, there was no built-in mechanism to log denied actions or detect when external credentials were used at a VPC endpoint. This often required you to build custom solutions to inspect and analyze TLS traffic, which could be operationally costly and negate the benefits of encrypted communications.With this new capability, you can now opt in to log all AWS API activity passing through your VPC endpoints. CloudTrail records these events as a new event type called network activity events, which capture both control plane and data plane actions passing through a VPC endpoint.Network activity events in CloudTrail provide several key benefits: â€“ Log all API activity traversing VPC endpoints, regardless of the AWS account initiating the action.External credential detection â€“ Identify when credentials from outside your organization are accessing your VPC endpoint.Data exfiltration prevention â€“ Detect and investigate potential unauthorized data movement attempts.Enhanced security monitoring â€“ Gain insights into all AWS API activity at your VPC endpoints without the need to decrypt TLS traffic.Visibility for regulatory compliance â€“ Improve your ability to meet regulatory requirements by tracking all API activity passing through.To enable network activity events, I go to the AWS CloudTrail console and choose  in the navigation pane. I choose  to create a new one. I enter a name in the  field and choose an Amazon Simple Storage Service (Amazon S3) bucket to store the event logs. When I create a trail in CloudTrail, I can specify an existing Amazon S3 bucket or create a new bucket to store my trailâ€™s event logs.If you set Log file SSE-KMS encryption to , you have two options: Choose  to create a new AWS Key Management Service (AWS KMS) key or choose  to choose an existing KMS key. If you chose , you need to type an alias in the  field. CloudTrail encrypts your log files with this KMS key and adds the policy for you. The KMS key and Amazon S3 must be in the same AWS Region. For this example, I use an existing KMS key. I enter the alias in the  field and leave the rest as default for this demo. I choose for the next step.In the  step, I choose  under . I choose the event source from the list of AWS services, such as , , , , and secretsmanager.amazonaws.com. I add two network activity event sources for this demo. For the first source, I select  option. For , I can use templates for common use cases or create fine-grained filters for specific scenarios. For example, to log all API activities traversing the VPC endpoint, I can choose the  template. I choose Log network activity access denied events template to log only access denied events. Optionally, I can enter a name in the  field to identify the log selector template, such as Include network activity events for Amazon EC2.As a second example, I choose  to create custom filters on multiple fields, such as  and . I can specify specific VPC endpoint IDs or filter the results to include only the VPC endpoints that match specific criteria. For Advanced event selectors, I choose  from the  dropdown, choose  as , and enter the VPC endpoint ID. When I expand the JSON view, I can see my event selectors as a JSON block. I choose  and after reviewing the selections, I choose .After itâ€™s configured, CloudTrail will begin logging network activity events for my VPC endpoints, helping me analyze and act on this data. To analyze AWS CloudTrail network activity events, you can use the CloudTrail console, AWS Command Line Interface (AWS CLI), and AWS SDK to retrieve relevant logs. You can also use CloudTrail Lake to capture, store and analyze your network activity events. If you are using Trails, you can use Amazon Athena to query and filter these events based on speciï¬c criteria. Regular analysis of these events can help you maintain security, comply with regulations, and optimize your network infrastructure in AWS.CloudTrail network activity events for VPC endpoint logging provide you with a powerful tool to enhance your security posture, detect potential threats, and gain deeper insights into your VPC network traffic. This feature addresses your critical needs for comprehensive visibility and control over your AWS environments.Network activity events for VPC endpoints are available in all commercial AWS Regions.â€” Esra]]></content:encoded></item><item><title>Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Underâ€¦</title><link>https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4</link><author>Kuseh Simon Wewoliamo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:38 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Terraform Architecture ExplainedÂ , Terraform Core, State, and Plugins: How Terraform Works Under theÂ Hood.1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. ReferencesInfrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processesÂ , â€œClickOpsâ€. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using codeÂ , it enables you to automate and use all the best practices of software development. IaC eliminates human errorsÂ , speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like softwareÂ code.Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of itâ€™s declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.2. Terraform ArchitectureTerraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components: 1 Terraform core 2 Plugins (Providers and Provisioners) Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations filesÂ , building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms viaÂ HTTPs.Plugins (Providers and Provisioners)Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named â€œterraform.tfstateâ€. You can read more about terraform state here There are two ways to manage state: Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on RemoteÂ StateTerraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly threeÂ steps: The first step is to write your terraform configuration just like any other code using any editor of yourÂ choice. This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure. The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changesÂ ,terraform will go ahead perform the desired actions as defined execution.4. Terraform Best Practices1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.2. Itâ€™s a good practice to store your work in a version controlled repository even when youâ€™re just operating as an individual.3. When working as a team, itâ€™s important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.4. Never Store your state file in a version controlled repository.5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the official Terraform page. Donâ€™t forget to add your commentsÂ , till then keepÂ coding.6. Terraform:Up & RunningÂ , Third Edition by YevgeniyÂ Brikman]]></content:encoded></item><item><title>The Micro Frontend Revolution</title><link>https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4</link><author>Adem KORKMAZ</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Free AI models: Running Local LLMS with Llama 3.3,</title><link>https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4</link><author>Joel Wembo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Part 4 of 10 Part series on DeepSeekÂ MLOpsFree AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models usingÂ OllamaStep-by-Step Guide: Installing a Web UI for Local LLMs onÂ WindowsWith the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. The solution? A web-based user interface (UI) that allows easy interaction with your localÂ LLMs.In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process. is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging GGUF (GGML Unified Format), an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.Why Use a Web UI for LocalÂ LLMs?Using a web UI for local LLMs offers several advantages:: No need to work with command-line tools.: Manage multiple models in oneÂ place.: Chat history, prompt engineering, and adjustable settings.: Access your models remotely via a webÂ browserStep 1Â : Download and InstallÂ OllamaDownload Ollama from https://ollama.com/download/windows, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of theÂ desktop.To run Ollama and start utilizing its AI models, youâ€™ll need to use a terminal on Windows. Weâ€™ll skip it here and letâ€™s see how to install WebUI for a better experience.Now open the browser and type localhost:11434 to check is Ollama is up andÂ runningAlso, Check in your systemÂ TrayNext, Open your CMD to pull some free AIÂ modelsStep 2â€Šâ€”â€ŠInstall OllamaÂ WebUIRun the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use thisÂ command:docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainTo connect to Ollama on another server, change the OLLAMA_BASE_URL to the serverâ€™s URL. So if Ollama is on a Different Server, use thisÂ command:docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainNext, Open your browser and type localhost:3000Ollama utilizes Metal on macOS and CUDA on Windows/Linux for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a persistent server in the background, managing requests via an  that communicates with models using optimized token streaming.Internally, it uses low-level memory-efficient inference kernels, minimizing VRAM and RAM usage while maintaining performance. It also supports LoRA (Low-Rank Adaptation) fine-tuning, allowing users to personalize models on their local machine with minimal compute overhead.Run the following command:ollama run deepseek-r1:671bChoosing the Right Web UI for YourÂ Needs: LM Studio (Simple setup, user-friendly UI): Oobabooga (More features, customization options): Gradio (Custom interface, lightweight solution): Open WebUI (Accessible over the internet)Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether youâ€™re a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AIÂ models.Start today with one of these web UIs and bring AI power to your local machine!Â ðŸš€Thank you for ReadingÂ !! ðŸ™ŒðŸ», donâ€™t forget to subscribe and give it aÂ CLAP, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps & Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( ]]></content:encoded></item><item><title>Understanding Container Orchestration (AWS ECS, AWS EKS &amp; Kubernetes)</title><link>https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4</link><author>Althaf Hussain</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:55 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Why Do We Need Container Orchestration?1ï¸âƒ£ We use Docker to create and run containersDocker  using a Dockerfile with :: Packages and compiles theÂ app.: Runs the app and exposesÂ ports.docker run -p 80:80 my-appNow the app is running inside a . ðŸŽ‰2ï¸âƒ£ But what if the app crashes due to highÂ traffic?Docker cannot restart or scale theÂ app.If thereâ€™s high traffic (e.g., festive season sales), .3ï¸âƒ£ Solution? We need a tool to manage containers automatically!This is where Container Orchestration Tools comeÂ in!Examples: Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift.ðŸš€ Kubernetesâ€Šâ€”â€ŠFull Control but ComplexÂ Setupâœ…  When you want  over your cluster.âœ… Manages multiple containers (Docker is just for one container). (if traffic increases, it adds more containers). (if an app crashes, Kubernetes restartsÂ it).What is Kubernetes (Self-Managed)?If you want  over your cluster, you can set up Kubernetes manually.How Kubernetes Works (Practical Steps)1ï¸âƒ£ Create a VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).2ï¸âƒ£ Install Kubernetes, kubeadm, kubectl, networking, storage, etc.3ï¸âƒ£ Set up a  and .4ï¸âƒ£ Deploy your app using a .5ï¸âƒ£ Manage scaling, auto-healing, networking, etc. manually.ðŸ› ï¸ Steps to Deploy an App Using Kubernetes:1ï¸âƒ£ Set up a server (EC2 instance orÂ VM)sudo apt update && sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.listsudo apt updatesudo apt install -y kubelet kubeadm kubectl2ï¸âƒ£ Initialize Kubernetes clustermkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configapiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app        image: my-docker-image:latest        ports:kubectl apply -f deployment.yamlapiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80kubectl apply -f service.yamlðŸŽ¯ Your app is now running inside Kubernetes! ðŸš€âœ… Advantages of Self-Managed Kubernetesâœ”  â†’ You can configure every part of the cluster.âœ”  â†’ On-premise, AWS, Azure, GCP, or hybrid cloud.âœ”  â†’ Youâ€™re not tied to AWS, Azure, or any provider.âœ”  â†’ Most companies use  for flexibility.âŒ Disadvantages of Self-Managed Kubernetesâœ–  â†’ You need to manually configure networking, storage, security, etc.âœ–  â†’ You have to patch, upgrade, and secure the cluster yourself.âœ–  â†’ Setting up and managing Kubernetes is .ðŸš€ AWS ECSâ€Šâ€”â€ŠAWS Manages Everythingâœ…  Running containers without managing Kubernetes.âœ… You donâ€™t need to set up Kubernetes.Just  what app you want to run, and it does everything. over cluster management.ðŸ› ï¸ Steps to Deploy an App in AWSÂ ECS1ï¸âƒ£  in the AWS Console.2ï¸âƒ£  (Choose Fargate or EC2).3ï¸âƒ£ :Go to ECS > Task Definitions > Create new task definition.Choose  (serverless) or  (self-managed).Define  (Docker image, ports, CPU, memory).4ï¸âƒ£ :Go to ECS > Services > CreateÂ Service.Choose the cluster and task definition youÂ created.Define  (number of tasks).5ï¸âƒ£ Â ðŸŽ‰ðŸŽ¯ Your app is running inside AWS ECS without managing infrastructure! ðŸš€âœ”  â†’ AWS takes care of the infrastructure.âœ”  â†’ No need to set up Kubernetes manually.âœ” Tightly integrated with AWS â†’ Works great with AWS services like ALB, IAM, CloudWatch, etc.âœ” Less operational overhead â†’ No need to worry about maintaining aÂ cluster.âŒ Disadvantages of AWSÂ ECSâœ–  â†’ If you want to move your app from AWS to , or , you have to set up everything from scratch.âœ–  â†’ You donâ€™t have full control over how the cluster is managed.âœ–  â†’ Most companies prefer  over ECS for multi-cloud strategies.ðŸš€ AWS EKSâ€Šâ€”â€ŠAWS Manages Kubernetes forÂ Youâœ…  When you want Kubernetes but donâ€™t want to install it manually.âœ… AWS  (no need to install manually).You  to deployÂ apps. than ECS but  than DIY Kubernetes.ðŸ› ï¸ Steps to Deploy an App in AWSÂ EKS1ï¸âƒ£  in the AWS Console.2ï¸âƒ£ :Set Cluster name, VPC, IAMÂ role.AWS will create & configure the Kubernetes control plane.3ï¸âƒ£ aws eks update-kubeconfig --region your-region --name your-cluster-name4ï¸âƒ£  (same as Kubernetes DIY)kubectl apply -f deployment.yaml5ï¸âƒ£ Expose the app using a Kubernetes service (same asÂ before).ðŸŽ¯ Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup!Â ðŸš€âœ”  â†’ Works exactly like Kubernetes, so itâ€™s easy to move to another cloud (Azure AKS, Google GKE, etc.).âœ” Fully managed control plane â†’ AWS handles the  (setting up Kubernetes).âœ”  than ECS â†’ You can tweak networking, security, and scaling.âœ”  â†’ You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).âŒ Disadvantages of AWSÂ EKSâœ–  â†’ You still need to understand Kubernetes concepts.âœ– More operational overhead â†’ Though AWS sets up Kubernetes, you still .âœ–  â†’ You  for the Kubernetes controlÂ plane.ðŸŽ¯ Real-World Example of How These WorkÂ TogetherImagine youâ€™re running an :1ï¸âƒ£ You  to package your app into a container.2ï¸âƒ£ You deploy it to Kubernetes (DIY) if you want full control.3ï¸âƒ£ If you donâ€™t want to manage Kubernetes, you use  (simplest).4ï¸âƒ£ If you need Kubernetes but donâ€™t want manual setup, you use .ðŸ“Œ Think of Kubernetes as a powerful machine where you control everything.ðŸ“Œ Think of AWS ECS as a service where AWS does the heavy lifting for you.ðŸ“Œ Think of AWS EKS as Kubernetes, but AWS helps withÂ setup.Conclusion: Which One Should YouÂ Use?ðŸ‘‰  if you  and donâ€™t care about moving to another cloud.ðŸ‘‰  if you  but donâ€™t want to set it up manually.ðŸ‘‰ Use Self-Managed Kubernetes if you  and plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).ðŸ’¡ If youâ€™re , start with .If youâ€™re building , go for .If you want , use .]]></content:encoded></item><item><title>Understanding APIs: A Developerâ€™s Guide to Building and Using APIs</title><link>https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4</link><author>Subbareddysangham</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:21 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurantâ€Šâ€”â€Šcustomers (the client application) donâ€™t need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (theÂ API).I designed and developed an e-commerce web application with HTML, CSS, and JavaScript for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts ofÂ APIs.E-commerce Web Application API FlowÂ Chart:To check the complete sourceÂ code:APIs in the E-Commerce ApplicationThis E-Commerce application consists of the following API endpoints:1. Authentication API (auth_routes): /api/auth/login (POST) â†’ Authenticates users and starts aÂ session.: /api/auth/logout (POST) â†’ Clears session and logs outÂ users.2. Product API (product_routes): /api/products (GET) â†’ Returns a list of products.: /api/products/<int:product_id> (GET) â†’ Fetches details of a specificÂ product.3. Cart API (cart_routes): /api/cart (GET) â†’ Returns the current user'sÂ cart.: /api/cart (POST) â†’ Adds a product to theÂ cart.: /api/cart/<int:item_id> (DELETE) â†’ Removes an item from theÂ cart.4. Order API (order_routes): /api/orders (POST) â†’ Places an order with the items in theÂ cart.: /api/orders/<int:order_id> (GET) â†’ Fetches details of a specificÂ order.: /api/health (GET) â†’ Provides API uptime, session data, and frontendÂ path.: /<path:filename> (GET) â†’ Serves frontendÂ files.: / (GET) â†’ Serves index.html or API runningÂ message.: Handles missing resources.500 Internal Server Error: Handles unexpected issues.: Handles invalid requests.An API consists of several key components that work together:These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,ðŸ“Œ https://api.ecommerce.com/products â†’ Retrieves a list of available products.ðŸ“Œ https://api.ecommerce.com/cart â†’ Fetches the current user's shopping cart details.ðŸ“Œ https://api.ecommerce.com/orders â†’ Handles order-related operations.These actions can be performed on the allowed endpoints. Theyâ€™re like verbs telling the API what to do with theÂ data. â†’ Read data (View products, orders, cartÂ items). â†’ Create new data (Add product, register user, placeÂ order). â†’ Update existing data (Update profile, modify cart quantity). â†’ Remove data (Delete cart item, cancelÂ order). Additional data is sent to fine-tune the API request, such as specifying which page of results you want toÂ see.: Parameters are extra details added to an API request to filter or refine theÂ results.Fetch  of products:GET /api/products?category=laptopsSecurity measures ensure that only authorized users can access theÂ API.: Ensures that only authorized users can access theÂ API.When a user logs in, the API gives aÂ :{  "message": "Login successful",  "token": "eyJhbGciOiJIUz..."}To add a product to the cart, the request must include thisÂ :POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz... Prevents unauthorized access and protects userÂ data. The structure of the data returned by the API, commonly in formats like JSON orÂ XML.: The structure of the data sent back by theÂ API. (because itâ€™s easy to read andÂ use).{  "id": 1,  "price": 799.99,} Frontend uses this data to display products toÂ users.APIs are classified according to their usage patterns and architectures.API Types According to Purposes ofÂ UseðŸ”¹ â€Šâ€”â€ŠUsed within a company, hidden from public access. Helps teams share data securely.ðŸ”¹ â€Šâ€”â€ŠAvailable to everyone, can be free or paid. Example: Google MapsÂ API.ðŸ”¹ â€Šâ€”â€ŠUsed between business partners for secure data exchange. Example: E-commerce & shipping company integration.ðŸ”¹ â€Šâ€”â€ŠCombines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in oneÂ call.API Types According to Architectural Structure:1. Web APIs (HTTP/HTTPS APIs)These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:1.1. REST (Representational State Transfer):The most popular type of web API today. REST APIs follow these principles:Stateless: Each request contains all the information neededResource-based: Everything is treated as a resource with a uniqueÂ URLUses standard HTTP methods (GET, POST, PUT,Â DELETE)Supports multiple data formats (usuallyÂ JSON)Example REST API Request in an E-Commerce Web Application:This request fetches all available products from the onlineÂ store.âœ… Request (Client â†’Â Server)GET /api/products HTTP/1.1Host: api.ecommerce.comAuthorization: Bearer <User_Token>Content-Type: application/jsonâœ… Response (Server â†’Â Client)[    {"id": 1, "name": "iPhone 15", "price": 999.99, "stock": 20},    {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99, "stock": 15}]catalog.html Fetches and DisplaysÂ Products1ï¸âƒ£ User Visits catalog.htmlThe user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).The browser  to fetch productÂ data.2ï¸âƒ£ Frontend (JavaScript) Sends an APIÂ RequestJavaScript code in catalog.html makes a GET request to the  /api/products.3ï¸âƒ£ Backend API (GET /api/products) FetchesÂ DataThe get_all_products() function runs when the frontend calls /api/products.4ï¸âƒ£ Database Retrieves Product InformationThe backend queries the  in theÂ databaseExample database response:[    {"id": 1, "name": "Laptop", "price": 799.99},    {"id": 2, "name": "Smartphone", "price": 499.99}]5ï¸âƒ£ Frontend Renders Product Data in catalog.htmlThe JavaScript loops through the  and dynamically  to display products.<div class="product-card">  <h3>Laptop</h3>  <button onclick="addToCart(1)">Add to Cart</button><div class="product-card">  <p>Price: $499.99</p>  <button onclick="addToCart(2)">Add to Cart</button></div>1.2. SOAP (Simple Object Access Protocol):SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe inÂ nature.A more rigid, protocol-specific API style used in enterprise environments:Highly structured messaging<soap:Envelope>  <soap:Header>    <Authorization>Bearer abc123</Authorization>  </soap:Header>    <GetUser>    </GetUser></soap:Envelope>A modern API query language that gives clients moreÂ control:Clients specify precisely what data theyÂ needSingle endpoint for allÂ requestsReduces over-fetching and under-fetching ofÂ dataFacebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for theÂ query.query {  user(id: "123") {    email      title  }These are programming interfaces provided by software libraries or frameworks:Used directly in yourÂ codeNo network requests areÂ neededUsually specific to a programming languageExample using a Python libraryÂ API:import pandas as pd# Using pandas API to read a CSV filedf = pd.read_csv('data.csv')These allow applications to interact with the operating system:Example using Pythonâ€™s OSÂ API:import os# Using OS API to create a directoryos.mkdir('new_folder')The foundation of web APIs, using well-defined methods and statusÂ codes:PUT: Update existingÂ dataPATCH: Partially updateÂ data2xx: Success (e.g., 200Â OK)Enables real-time, two-way communication:Ideal for chat apps and liveÂ updatesExample WebSocket connection:const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) => {    console.log('Received:', event.data);};gRPC (Google Remote Procedure Call) is a  framework for inter-service communication in microservices architecture. Unlike REST APIs that use , gRPC uses Protocol Buffers (Protobuf), making it faster and more efficient.Googleâ€™s high-performance RPC framework:Excellent for microservicesExample Protocol Buffer definition:ðŸ“Œ How gRPC Works in a Web ApplicationIn an , gRPC can be used for fast communication between microservices.A  must fetch a  from the backend .1ï¸âƒ£ Defining gRPC Service (product.proto)gRPC services use Protocol Buffers (Protobuf) to define API contracts.GetAllProducts(): Returns a list of products.GetProductById(): Fetches a single product byÂ ID.Product: Defines the product structure.2ï¸âƒ£ Implementing gRPC Server (product_server.py)The gRPC server implements the serviceÂ logic.Implements ProductService methods (GetAllProducts, GetProductById).3ï¸âƒ£ Implementing gRPC Client (product_client.py)The client  to fetchÂ data.Calls GetAllProducts() to fetch all products.Calls GetProductById() to fetch a singleÂ product.4ï¸âƒ£ Running gRPC Server &Â Client# 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.pyProductService gRPC Server is running on port 50051...Product List: products {  name: "Laptop"}  id: 2  price: 499.991. GET: Used to retrieveÂ dataIt is a request used to retrieve data. Never used to delete, update or insertÂ data.Product API in E-Commerce Web Applicationcurl -X GET http://52.90.222.178:5000/api/products[   {"id": 1, "name": "iPhone 15", "price": 999.99},   {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99}]If the API returns JSON, you can format the response usingÂ curl -X GET http://52.90.222.178:5000/api/products | jqDebugging & TroubleshootingIf youâ€™re not getting the expected response, check:Is the Flask serverÂ running?curl -X GET http://52.90.222.178:5000/api/healthHow the /api/health Endpoint Works inÂ FlaskThe endpoint is a  that provides the current status of the application, including . It helps in monitoring the system and ensuring that the API is .I have configured this /api/health endpoint in my E-Commerce Web Application.@app.route("/api/health", methods=["GET"])def health_check():    uptime = time.time() - start_time    return jsonify({        "uptime": f"{uptime:.2f} seconds",        "session_active": "username" in session    }), 2002. POST: Creates new resourcesThe  method is a request used to insert data. Posted data typeâ€Šâ€”â€ŠJSON.âœ… Authentication EndpointsThis is handled in auth_routes.py, prefixed with /api/auth. â†’ The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.User sends a  with username & password.If credentials areÂ valid:The user session isÂ stored.API returns a successÂ message.3. If credentials areÂ invalid:API returns 3. PUTÂ : Updates existing resourcesPUT method is used to create or update (replace) a resource. Useful for syncingÂ data.Ex: We can add a â€œChange Passwordâ€ feature for an existing user using a PUT /api/auth/change-password API endpoint.Steps to Implement â€œChange Passwordâ€ APIPUT request with their old and new password.curl -X PUT http://localhost:5000/api/auth/change-password      -H "Content-Type: app     -d '{"old_password": "currentPass123", "new_password": "newPass456"}'2. API verifies the old password:If incorrect, return an error (401 Unauthorized).3. If correct, update the password in the database.4. Save the new password (after hashing it for security).5. Return a successÂ message.{"message": "Password changed successfully"}In an e-commerce application like yours, a PUT method would typically be usedÂ in: â†’ PUT /api/auth/update-profileUpdating Product Information (Admin) â†’ PUT /api/products/<product_id> â†’ PUT /api/cart/<cart_id> â†’ PUT /api/orders/<order_id>4. DELETE: Removes resourcesThe DELETE method deletes the specified resource./api/cart API to Remove a Product from theÂ Cart1ï¸âƒ£ Endpoint Definition (FlaskÂ API):@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.    Expects JSON payload: { product_id }.    """        user_id = session.get('user_id')            return jsonify({"message": "User not authenticated"}), 401        product_id = data.get('product_id')            return jsonify({"message": "'product_id' is required"}), 400        connection = get_db_connection()        cursor = connection.cursor()        delete_query = "DELETE FROM cart_items WHERE user_id = %s AND product_id = %s"        cursor.execute(delete_query, (user_id, product_id))            return jsonify({"message": "Product not found in cart"}), 404        return jsonify({"message": "Product removed from cart successfully"}), 200        return jsonify({"message": "Failed to remove product from cart", "error": str(e)}), 500        close_db_connection(connection)User sends a DELETE request with the product_id they want toÂ remove.API verifies if the user is logged in (checks session['user_id']). (if missing, returns 400 Bad Request). to remove the product from the cart_items table.If the product does not exist, it returns 404 NotÂ Found., it commits the transaction and returns a success message (200Â OK).Handles database errors and ensures the connection isÂ closed.3ï¸âƒ£ Example API Request & Response:Now User wanted to delete iPhone 15 Pro from theÂ cart:Click the  button under â€œiPhone 15Â Proâ€.The item should disappear, and the cart total shouldÂ update.Run this command in the terminal:curl -X DELETE http://52.90.222.178:5000/api/cart      -H "Content-Type: application/json"      -H "Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0"     -d '{"product_id": 4}'{    "message": "Product removed from cart successfully"After deletion of thatÂ item:5. PATCH: Partially updates resourcesPATCH method is to request used to update data. Only passed data will be updated. You donâ€™t need to provide all the dataÂ set.The  method is used to  a resource. Instead of sending the entire data set, we only send the fields that need to beÂ updated.Use Case: Updating a Userâ€™s Profile (PATCH /api/auth/update-profile)Imagine a user wants to update  or  without changing their username.1ï¸âƒ£ PATCH Endpoint: PATCH /api/auth/update-profile2ï¸âƒ£ Sending a PATCHÂ RequestIf the user wants to update curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \     -H "Content-Type: application/json" \     -H "Cookie: session=your_valid_session_id" \     -d '{"email": "newemail@example.com"}'ðŸ”¹ Only the  field will beÂ updated.âœ… {    "message": "Profile updated successfully"âŒ If no fields are provided:{    "message": "No valid fields provided for update"{    "message": "User not authenticated"4ï¸âƒ£ Why Use PATCH Instead ofÂ PUT?Conclusion: Understanding APIs, Endpoints, and Methods in Web DevelopmentAPIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and aÂ server.An  acts as a bridge between two applications, enabling data exchange.Example: A shopping website uses an API to fetch product details from a database.An  is a URL that clients use to request or sendÂ data.Example: GET /api/products retrieves all products. â†’ Uses HTTP methods (GET, POST, PUT, DELETE) to manageÂ data. â†’ Lets clients request specific data fields, reducing unnecessary data transfer. â†’ Uses XML messaging, mainly in enterprise applications. â†’ Maintains a continuous connection for real-time updates (e.g., liveÂ chat).Use  (JWT, API Keys, OAuth) to restrictÂ access.Protect sensitive data with .Implement  to preventÂ abuse.APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secureÂ APIs.Iâ€™d love to hear what you think about this articleâ€Šâ€”â€Šfeel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean aÂ lot.For more insights into the world of technology and data, visit  Thereâ€™s plenty of exciting content waiting for you toÂ explore!Thank you for reading, and happy learning! ðŸš€]]></content:encoded></item><item><title>Most Developers Get This Wrong in Docker Networking!</title><link>https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4</link><author>Gaddam.Naveen</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:15 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup</title><link>https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4</link><author>th@n@n</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:10 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, weâ€™ll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard criticalÂ data.In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Letâ€™s break down eachÂ partkind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerThis StorageClass resource defines storage provisioning and management policies.Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to beÂ created.PersistentVolumeClaim (PVC) Resource:apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:  storageClassName: localstorage  accessModes:  resources:      storage: 2GiThis PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a singleÂ node.PersistentVolume (PV) Resource:apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt  storageClassName: localstorageThis PV resource represents the actual storage volume available for use by theÂ PVC.It is bound to the PVC pvc-jenkins within the jenkins namespace.The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins          image: jenkins/jenkins:lts          resources:              memory: "0.5Gi"            requests:              cpu: "125m"            - name: http-port            - name: jnlp-port          livenessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0This Deployment resource defines how Jenkins is deployed.It specifies a single replica (replicas: 1) of the Jenkins container.The container is based on the jenkins/jenkins:lts image.Resource limits and requests for CPU and memory are set to ensure resource allocation.Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.Liveness and readiness probes are configured to check the health of the container.The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.SecurityContext ensures that Jenkins runs with the appropriate user and group permissions. securityContext:        runAsUser: 0        fsGroup: 0When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000This Service resource exposes the Jenkins deployment internally within the jenkins namespace.It selects pods with the label app: jenkins-pod.The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.Once you execute all the manifiest file in kubernetes cluster.Check the host volume path ls -alÂ /mntExecute the below command to see the whether the same files are present in the jenkins containerkubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_homeThis configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.For now, thatâ€™s it guys, If you like this article donâ€™t forget to give a clap.Â ðŸ‘]]></content:encoded></item><item><title>How to Build and Deploy a Simple Frontend App with Python Backend</title><link>https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4</link><author>krth1k</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:06 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Building a full-stack web application might seem daunting, especially if youâ€™re primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend withÂ ease.In this guide, weâ€™ll walk through the processÂ of:Setting up a basic Python backend withÂ FlaskCreating a simple frontend with HTML, CSS, and JavaScriptConnecting the frontend to the backend using RESTÂ APIDeploying the app on a local Kubernetes cluster1. Setting Up the PythonÂ BackendWeâ€™ll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.Ensure you have Python installed, then installÂ Flask:Create a new file calledÂ app.py:from flask import Flask, jsonify@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)This API exposes a single endpoint /api/message that returns a JSON response.For the frontend, weâ€™ll use HTML, CSS, and JavaScript to display the data from ourÂ backend.Create an HTML File (index.html)<!DOCTYPE html><html lang="en">    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>Frontend App</title>        body {            font-family: Arial, sans-serif;            text-align: center;        }            padding: 10px 20px;        }</head>    <h1>Simple Frontend App</h1>    <button onclick="fetchMessage()">Get Message</button>    <p id="message"></p>        function fetchMessage() {            fetch('http://127.0.0.1:5000/api/message')                .then(response => response.json())                    document.getElementById("message").innerText = data.message;                .catch(error => console.error('Error:', error));    </script></html>This page has a button that fetches and displays a message from the FlaskÂ backend.3. Connecting the Frontend to theÂ BackendNow, letâ€™s serve the frontend using  itself so that both frontend and backend are accessible from the sameÂ origin.Update app.py to ServeÂ HTMLModify app.py to serve the index.html file:from flask import Flask, jsonify, send_from_directoryapp = Flask(__name__, static_folder='static')@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})@app.route('/')def serve_frontend():    return send_from_directory('static', 'index.html')if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)Move index.html to a staticÂ FolderYour project structure should now look likeÂ this:/project-folderâ”‚-- app.pyâ”‚   â””â”€â”€ index.htmlNow, visit http://127.0.0.1:5000/ in your browser, and your frontend will beÂ served!Now, letâ€™s deploy this app using .# Use Python base imageFROM python:3.9# Set the working directoryWORKDIR /app# Copy application filesCOPY . .# Install dependenciesRUN pip install flask# Expose port 5000EXPOSE 5000# Run the appCMD ["python", "app.py"]Build and Run the Docker Containerdocker build -t myapp .docker run -p 5000:5000 myappCreate a Kubernetes Deployment YAML (apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePortkubectl apply -f deployment.yamlminikube service myapp-service --urlVisit the displayed URL in yourÂ browser!In this tutorial, we covered: âœ… Creating a Flask backend âœ… Building a simple HTML/JavaScript frontend âœ… Connecting the frontend to the backend âœ… Deploying the app with Docker and KubernetesThis is a basic example, but you can expand itÂ by:Adding user authenticationUsing React or Vue.js for a modernÂ frontendStoring and retrieving data from aÂ databaseIf you found this helpful, let me know in the comments! ðŸš€]]></content:encoded></item><item><title>Best Practices For Database Authorization In Multi-Tenant Systems</title><link>https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4</link><author>Noel</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:49:43 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.This article explores the best practices and technical solutions that we adopt at Xinthe, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.Understanding Multi-Tenant Database AuthorizationDefinition Of Multi-TenancyMulti-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenantâ€™s data remains logically isolated, ensuring that no tenant can access anotherâ€™s data, while sharing underlying resources for efficiency.Key Multi-Tenancy Models:Each tenant has a dedicated database orÂ schema.Offers strong isolation and security.Higher costs and maintenance complexity due to multiple instances.Multiple tenants share the same database.Logical separation is maintained through identifiers (e.g., tenant_id or company_id).Cost-effective and scalable but requires robust authorization mechanisms.Challenges Of AuthorizationImplementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges includeÂ -Cross-Tenant DataÂ Leakage:Risk: Improper queries or configurations can expose data to unauthorized tenants.Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHEREÂ clause.Deeply nested resource structures often require joins across multipleÂ tables.Queries with extensive joins can degrade performance as data volume increases.Scalability & Maintainability:The need to balance fast access controls with a maintainable schema.Adding new authorization rules or resource types without overhauling theÂ system.Data Localization & Compliance:For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.Importance Of Nested Resource StructuresIn many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -Company â†’ Client â†’ Project â†’Â TaskA  has multipleÂ .Each  manages several .Each  contains multipleÂ .Why Nested Structures Matter:Access Control Complexity: Permissions must flow through the hierarchy (e.g., a userâ€™s access to a task must be verified against their company). Hierarchical access often necessitates multiple joins, impacting query efficiency. Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organizationâ€™s boundaries.A user from Company A should only edit tasks within their projects. Authorization must ensure that the task â†’ project â†’ client â†’ company linkage is maintained without exposing data from CompanyÂ B.Comparing Authorization ApproachesWhen implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and Tenant-Specific Databases or Tables. Each comes with its own set of benefits and trade-offs. Letâ€™s break them downÂ -1. Flat Model (Adding TenantÂ IDs)In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization. Queries can directly filter by tenant_id without traversing the hierarchy.SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id; Reduces query complexity by avoiding multiple table joins to enforceÂ access. Straightforward implementation makes it easy to debug and maintain. tenant_id is replicated across multiple tables, introducing redundancy. Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows. Schema updates (e.g., adding new relationships) might require extensive changes across multipleÂ tables.Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many smallÂ tenants.2. Hierarchical Model (Enforcing Relationships)In this approach, the relationships between resources (e.g., task â†’ project â†’ client â†’ company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy. Avoids redundant fields by relying on inherent relationships.CREATE TABLE tasks (    id SERIAL PRIMARY KEY,    project_id INT REFERENCES projects(id),    ...    id SERIAL PRIMARY KEY,    client_id INT REFERENCES clients(id),    ...    id SERIAL PRIMARY KEY,    company_id INT REFERENCES companies(id),    ... Reduces duplication of metadata like tenant_id.Relationship-Centric Queries: Makes it easier to enforce hierarchical constraints and maintain referential integrity. Queries require multiple joins to verify access, which can impact performance.SELECT t.*FROM tasks tJOIN projects p ON t.project_id = p.idJOIN clients c ON p.client_id = c.idWHERE c.company_id = :tenant_id AND t.id = :task_id; Deep hierarchies with large datasets can significantly increase query execution time. As the hierarchy grows, maintaining performance becomes challenging.Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.3. Tenant-Specific Databases OrÂ TablesThis approach creates separate databases or tables for each tenant, isolating their data entirely. Each tenantâ€™s data can be managed independently, making it easier to scale horizontally by distributing databases acrossÂ servers. Ensures complete data isolation, reducing the risk of cross-tenant dataÂ leakage. Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions. Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines. Schema updates need to be applied consistently across all tenant databases. For tenants with small datasets, the resource consumption of separate databases might be inefficient.Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).Summary Tableâ€Šâ€”â€ŠComparing ApproachesCriteria For Choosing An Authorization ModelSelecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to considerÂ -The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.High-Traffic Applications:Benefit from simpler and faster queries, such as those enabled by the .SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;Minimal joins mean lower query latency, ensuring the system performs well under heavyÂ loads.Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.:Can afford the  with more joins, as performance trade-offs are less significant.Allows for cleaner schema designs and strict relational integrity.Suitable for internal enterprise tools or smaller-scale applications.2. Regulatory RequirementsCompliance with data protection and privacy regulations often dictates how data is stored and accessed.Using Tenant-Specific Databases or Tables simplifies compliance for regulations like GDPR orÂ HIPAA.Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.An enterprise customer requires dedicated storage with separate backups and auditÂ logs.A  can still meet compliance needs with appropriate access controls and audit mechanisms.Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.The ability to handle growth in the number of tenants and data volumes is a criticalÂ factor.Planning For TenantÂ Growth:For a rapidly scaling user base, Tenant-Specific Databases or Tables provide the most flexibility -Each tenant can be distributed across servers to balanceÂ load.Tenant databases can be independently scaled based on specificÂ needs.A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenantÂ size.The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.The  may struggle as table sizes grow, requiring optimization for complexÂ joins.Ease of schema management and updates is essential for long-term maintainability.Simplified SchemaÂ Updates:The  simplifies schema updates by centralizing data attributes like tenant_id.However, redundant fields may increase the risk of errors duringÂ updates.The  enforces relational integrity, ensuring data consistency.Complex queries for nested structures may require more effort to maintain and optimize.Automated CI/CD Pipelines:For Tenant-Specific Databases, CI/CD automation becomes critical to manage schema changes across multiple databases.Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.Key Considerations SummaryDesigning Authorization Strategies For Multi-TenancyDesigning robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    FOREIGN KEY (project_id) REFERENCES projects(id));Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.Defining Relationships In Hierarchical Structures:Maintain strict referential integrity between hierarchical entities.Example for hierarchical relationships -CREATE TABLE projects (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)Flat schema enables quick lookups for tenant-specific data.Hierarchical relationships ensure data consistency and logical separation.2. Indexing Best PracticesIndexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.Compound Indexes For Tenant-Specific Queries:Use composite indexes combining tenant_id with frequently queriedÂ columns.CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);This enables efficient filtering and sorting within a tenantâ€™sÂ scope.Balancing Indexing Depth & QueryÂ Speed:Avoid over-indexing, which can slow down write operations.Prioritize indexing columns involved in filtering, joining, and sorting operations.Regularly analyze query performance using tools like  in PostgreSQL or  inÂ MySQL.Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.Horizontal Partitioning ByÂ Tenants:Partition data within a single database based on tenant_id.CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1Faster tenant-specific queries as partitions reduce the searchÂ space.Simplifies maintenance for large datasets.Database Sharding For High-Scale Systems:Distribute tenant data across multiple databases (shards).Example Sharding StrategyÂ -Use tenant_id % shard_count to assign tenants toÂ shards.Tools like  or  can manage sharding in distributed databaseÂ systems.Eliminates contention in single-database systems.Enhances fault isolation and scalability.Example Use Caseâ€Šâ€”â€ŠApplying These StrategiesAn application manages 100,000 tenants, each with thousands of projects andÂ tasks.Add tenant_id to allÂ tables.Use foreign keys to link tasks â†’ projects â†’Â clients.Create a compound index on tasks (tenant_id, project_id) for common queries likeÂ -SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;For smaller tenants, use horizontal partitioning -CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);For larger tenants, shard data across multiple databases toÂ scale.A well-designed schema with tenant_id simplifies multi-tenant data filtering.Proper indexing ensures efficient queries, even atÂ scale.Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.1. Flat Model ImplementationThe flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (client_id) REFERENCES clients(id));: Access tasks for a userâ€™s companyÂ -SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'in_progress';Simplifies authorization logic with directÂ lookups.Reduces query complexity by avoidingÂ joins.Potential schema bloat with additional tenant_id columns.2. Hierarchical Model ImplementationIn this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company â†’ Client â†’ Project â†’Â Task).CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (company_id) REFERENCES companies(id));    id BIGINT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (client_id) REFERENCES clients(id));    id BIGINT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id): Check task access by traversing relationships -SELECT t.* FROM tasks tINNER JOIN projects p ON t.project_id = p.idINNER JOIN clients c ON p.client_id = c.idINNER JOIN companies co ON c.company_id = co.idWHERE co.id = 123 AND t.status = 'in_progress';Maintains normalized relationships.Avoids redundant tenant_id columns.Complex joins increase queryÂ costs.Requires optimized indexes to maintain performance.3. Tenant-Specific Database/Table ImplementationFor scenarios requiring strict isolation, separate databases or tables for each tenant can beÂ used.Create a separate database or schema for each tenantÂ -CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);: Access tasks for a specific tenantÂ -USE company_123;SELECT * WHERE status = 'in_progress';: Use CI/CD tools like  to manage multi-tenant databases:Automate schema changes across databases.Track versioning for each tenantâ€™s database.Complete tenant isolation for security and compliance (e.g.,Â GDPR).Simplifies data archival and backup for individual tenants.Deployment complexity increases with the number ofÂ tenants.Resource-intensive for systems with many smallÂ tenants.Choosing The Right ImplementationUse the flat model for simplicity in high-traffic environments.Use the hierarchical model when data relationships must be preserved and redundancy minimized.Opt for tenant-specific databases for strict isolation and compliance requirements.Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.Security Best Practices For AuthorizationEnsuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.1. Strict AccessÂ ControlsImplementing strong access controls ensures that only authorized users can access or modify resources.Role-Based Access ControlÂ (RBAC):Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.Enforce role-specific permissions at the application and databaseÂ layers.Example: Use database roles to restrict access to tenant-specific tables.CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access:Enforce tenant-level data segregation directly at the databaseÂ layer.RLS ensures that queries automatically filter data based on the userâ€™sÂ tenant.PostgreSQL Example ForÂ RLS:CREATE POLICY tenant_policyON tasksUSING (tenant_id = current_setting('app.current_tenant')::BIGINT);ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;SET app.current_tenant = '123'; -- Simulate tenant contextSELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible2. Preventing Cross-Tenant DataÂ LeaksPreventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.Multi-Layer AccessÂ Checks:Enforce tenant isolation at both the database and application layers.Validate all queries to ensure they are scoped to the userâ€™sÂ tenant.Always include a tenant_id check in databaseÂ queries.Use database views or abstractions to simplify tenant-specific filtering.Application-Layer Validation:Add additional validation at the application level as a guardrail.Ensure that APIs restrict data access to the authenticated tenantÂ context.def get_user_tasks(user):    if user.tenant_id != request.tenant_id:        raise PermissionDenied("Cross-tenant access is not allowed.")    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.User ID and tenant ID for allÂ queries.Access attempts (successful andÂ failed).Data modification operations (insert, update,Â delete).Timestamps and IP addresses for requests.SQL Example For LoggingÂ Queries:CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPINSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');Integrating LoggingÂ Tools:Use database triggers to log operations automatically.Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());    RETURN NEW;$$ LANGUAGE plpgsql;CREATE TRIGGER audit_task_changesAFTER INSERT OR UPDATE OR DELETE ON tasksFOR EACH ROW EXECUTE FUNCTION log_task_changes();Combine RBAC, RLS, and application-level validation for comprehensive protection.Use multi-layered access checks and robust query scoping to ensure tenant isolation.Maintain detailed audit logs to track access and modifications for accountability and compliance.These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenantâ€™s data is isolated, protected, and auditable.Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.1. Balancing Performance & FlexibilityChoosing between speed and schema cleanliness can significantly impact your database design and performance.Prioritizing Performance (FlatÂ Model):Direct lookups using a tenant_id column ensure fast query execution.Reduced join complexity leads to quicker responseÂ times.: May result in data redundancy (e.g., repeating tenant IDs across multipleÂ tables).SQL Example For Optimized Query:SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'pending';Prioritizing Schema Cleanliness (Hierarchical Model):Using a normalized schema ensures a clean and consistent database structure.: Requires more complex joins and increased query times, especially for deeply nested relationships.Hierarchical QueryÂ Example:SELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.tenant_id = 123;Managing tenant-specific setups adds complexity, particularly as the number of tenantsÂ grows.Tenant-Specific Databases:Each tenant has its own database, simplifying compliance and data isolation.: Maintaining consistency across databases for schemaÂ changes.: Use automation tools like  or  to manage schema migrations acrossÂ tenants.# Liquibase command to apply migrations to multiple tenant databasesliquibase --url="jdbc:mysql://db_host/tenant1" updateliquibase --url="jdbc:mysql://db_host/tenant2" updateSingle Multi-Tenant Database:Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.: Tracking and isolating tenant data effectively without introducing query overhead.3. Handling Schema Updates In Multi-Tenant DatabasesEnsuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.Use versioned migrations to apply incremental updates across allÂ tenants.Maintain backward compatibility to prevent disruptions duringÂ updates.CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPApply updates to a subset of tenants, validate, and then roll out to theÂ rest.Use feature flags to selectively enable new schema features.Code Example For RollingÂ Updates:tenants = get_tenant_list()for tenant in tenants:    apply_schema_update(tenant_id=tenant.id)Testing & CI/CD For Multi-Tenant Systems:Test migrations on a staging environment with realistic tenant data before deploying.Use CI/CD tools like  to automate and track updates across tenant databases.Performance Vs. Flexibility:Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.Tenant-specific databases simplify compliance but require robust automation for schema management.Implement version control and rolling updates to ensure seamless schema changes across allÂ tenants.Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.Scenario 1â€Šâ€”â€ŠFlat Model For A SaaS CRMÂ AppA SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.: Each company has its own sales team, and users need quick access to customer records and salesÂ data.: Use a flat model by adding tenant_id to every table, such as customers, leads, andÂ sales.CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    name VARCHAR(255),    phone VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    customer_id BIGINT,    status VARCHAR(50),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);SELECT * FROM customers Simple queries without joins for tenant-specific data.High performance due to directÂ lookups.Wider tables due to the inclusion of tenant_id.Potential redundancy if relationships between entities are not properly normalized.Scenario 2â€Šâ€”â€ŠHierarchical Model For A Project Management ToolA project management tool with a nested structure: Company â†’ Client â†’ Project â†’ Task. Users need to manage projects while maintaining strict access control based on their organization.: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to theirÂ company.: Use a hierarchical model to enforce relationships and control access throughÂ joins.CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPSELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.company_id = 123;Clean, normalized schema without redundant data.Access naturally follows the hierarchy.Complex queries due to multi-level joins.Slower query performance for deep hierarchies.Scenario 3â€Šâ€”â€ŠSingle-Tenant Databases For An Enterprise AppAn enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.: Each tenantâ€™s data must be completely isolated to ensure compliance and scalability.: Use a single-tenant database model where each company has its own dedicated database.Database Names:  tenant_1_db  tenant_3_db: Use  to manage database updates across multipleÂ tenants.deploy:  steps:    - name: Update Tenant Databases      script: |        for db in $(list_databases); do          apply_migrations $dbComplete isolation ensures compliance with regulatory requirements.Scalability: Large tenants can have dedicated resources (e.g., separate hardware).Higher operational complexity in managing multiple databases.Requires robust CI/CD pipelines for schemaÂ updates.Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.Efficient, scalable, and secure authorization in multi-tenant databases as weâ€™ve found often at Xinthe, requires a well-thought-out approach tailored to the applicationâ€™s needs.Authorization in multi-tenant databases isnâ€™t a one-size-fits-all challenge. Developers and database architects must carefully evaluate their applicationâ€™s structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.]]></content:encoded></item><item><title>Docker, Kubernetes, and NATS â€” The Backbone of Cloud-Native Apps</title><link>https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4</link><author>Cristhian Ferrufino</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:48:09 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Docker, Kubernetes, and NATSâ€Šâ€”â€ŠThe Backbone of Cloud-Native AppsWelcome back to the ! In the last article, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, itâ€™s time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? Thatâ€™s the head chef, making sure everyone works inÂ harmony.In this article, weâ€™ll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, youâ€™ll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Letâ€™s getÂ cooking!What Is Containerization, and Why Is It Important?Imagine youâ€™re shipping a fragile package across the world. Youâ€™d want to pack it in a sturdy container, right? Thatâ€™s exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environmentsâ€Šâ€”â€Šwhether itâ€™s your laptop, a testing server, or a production cluster.Why developers love containers:âœ… : â€œWorks on my machineâ€ becomes â€œWorks everywhere.â€âœ… : No more dependency hellâ€Šâ€”â€Šeach app lives in its own bubble.âœ… : Deploy to AWS, Azure, or your grandmaâ€™s PC (if sheâ€™s cool with Kubernetes).âœ… : 10x lighter than VMs. Think EVs vs. a gas-guzzling truckIntroduction to Docker: Building, Running, and Managing ContainersDocker is the most popular tool for containerization, and for good reason. Itâ€™s simple, powerful, and widely supported. Letâ€™s break itÂ down:To create a container, you start with a â€Šâ€”â€Ša text file that defines the steps to build your appâ€™s environment. Hereâ€™s a simpleÂ example:# Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim# Set the stage for your appWORKDIR /app# Install dependencies (Pro tip: Skip the cache to shrink your image)COPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt# Copy the rest of the codeCOPY . .# Open the appâ€™s â€œfront doorâ€EXPOSE 8080CMD ["python", "app.py"]With this Dockerfile, you can build a container image using the ``Â command:ðŸš€ Run this: docker build -t my-python-app .Once youâ€™ve built your image, you can run it as a container:ðŸŽ¯ Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-appThis command starts your app and maps port 8080 on your host to port 8080 in the container. Easy,Â right?Docker also provides tools to manage your containers:: List running containers.docker logs <container_id>: View logs for a specific containerdocker stop <container_id>: Stop a running container.Docker vs. Podman: A Detailed ComparisonWhile Docker is the most popular containerization tool, itâ€™s not the only one.  is a rising star in the container world, and itâ€™s worth understanding how it compares toÂ Docker.+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker ðŸ³                           | Podman ðŸ“¦                         |+----------------------+-------------------------------------+-----------------------------------+| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       ||                      |                                     | directly)                         |+----------------------+-------------------------------------+-----------------------------------+| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  ||                      |                                     | of the box                        |+----------------------+-------------------------------------+-----------------------------------+| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  ||                      |                                     | and Dockerfiles                   |+----------------------+-------------------------------------+-----------------------------------+| Security             | Requires root privileges, which can | Rootless mode reduces attack      ||                      | be a security risk                  | surface                           |+----------------------+-------------------------------------+-----------------------------------+| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        ||                      | orchestration                       | natively                          |+----------------------+-------------------------------------+-----------------------------------+| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  ||                      |                                     | Hat                               |+----------------------+-------------------------------------+-----------------------------------+You need a mature, widely supported tool with a large ecosystem.Youâ€™re already using Docker Swarm for orchestration.Youâ€™re okay with running containers asÂ root.You want a daemonless, more secure alternative toÂ Docker.Youâ€™re working in environments where root privileges are restricted.Youâ€™re already using Kubernetes and want tighter integration.Both tools are excellent choices, so pick the one that best fits yourÂ needs.Kubernetes Overview: Orchestration, Scaling, and Self-HealingWhile Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestraâ€Šâ€”â€Šit ensures all your containers play inÂ harmony.Key Features of Kubernetes: Automates deployment, scaling, and management of containers.: Automatically adjusts the number of running containers based onÂ demand.: Restarts failed containers and replaces unhealthy ones.: Automatically assigns IP addresses and DNS names to containers.Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Hereâ€™s a simple Kubernetes deployment file:apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app        image: my-python-app:latest        ports:his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply itÂ using:ðŸ”¥ Run this: kubectl apply -f deployment.yamlHow NATS Shines in a Kubernetes EnvironmentNow, letâ€™s talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Hereâ€™s how it standsÂ out:Use Cases for NATS in KubernetesService-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetesâ€™ dynamic environment.Event-Driven Architectures: NATSâ€™s pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in realÂ time.Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.Resilience: NATSâ€™s built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.Deploying NATS on KubernetesDeploying NATS on Kubernetes is straightforward. Hereâ€™s a basic NATS deployment file:apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:        - containerPort: 4222 # The messaging highway ðŸ›£ï¸Once deployed, NATS can be used by your microservices for seamless communication.Best Practices for Containerizing MicroservicesTo wrap things up, here are some best practices for containerizing your microservices:Keep Containers Lightweight: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.: Separate the build and runtime environments to keep production imagesÂ small.Leverage Kubernetes Features: Use ConfigMaps and Secrets to manage configuration and sensitive data.: Integrate tools like Prometheus and Fluentd for monitoring andÂ logging.: Use CI/CD pipelines to automate building, testing, and deploying containers.In the next article, weâ€™ll explore â€œNATS as a Service Meshâ€Šâ€”â€ŠThe Lightweight Superhero Your Microservices Deserveâ€ and how it simplifies communication between microservices. Spoiler alert: itâ€™s like giving your microservices a supercharged walkie-talkie. StayÂ tuned!Until then, feel free to drop a comment or share your thoughts. Whatâ€™s your experience with Docker and Kubernetes? Any tips or tricks youâ€™d like to share? Letâ€™s keep the conversation going. ðŸ’¬ Whatâ€™s your #1 Kubernetes struggle? Scaling? Debugging? ShareÂ below! â¤ï¸ Happy containerizing, and stay tuned for the next chapter in the !]]></content:encoded></item><item><title>AI Coding Assistants are Not the Solution You Think</title><link>https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/</link><author>Anish Dhar</author><category>devops</category><pubDate>Thu, 13 Feb 2025 12:31:35 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernetes History Inspector, with Kakeru Ishii</title><link>http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:23:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.Do you have something cool to share? Some questions? Let us know: News of the week ]]></content:encoded></item><item><title>Sandbox environments: Creating efficient and isolated testing realms</title><link>https://www.youtube.com/watch?v=fh7-lQVmX-o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/fh7-lQVmX-o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:33 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>KitOps: AI Model Packaging Standards</title><link>https://www.youtube.com/watch?v=1TD-e_wVe4Q</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/1TD-e_wVe4Q?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Chat with us on Discord:  https://discord.gg/Tapeh8agYy

Check out our repos:
KitOps      https://github.com/jozu-ai/kitops
PyKitOps Python Library  https://github.com/jozu-ai/pykitops
KitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin]]></content:encoded></item><item><title>Training IT Teams for Multi-Cloud DevOps Environments</title><link>https://devops.com/training-it-teams-for-multi-cloud-devops-environments/</link><author>Anne Fernandez</author><category>devops</category><pubDate>Wed, 12 Feb 2025 12:11:06 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>StackGenâ€™s New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions</title><link>https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/</link><author>Tom Smith</author><category>devops</category><pubDate>Tue, 11 Feb 2025 10:17:03 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gcore Radar report reveals 56% year-on-year increase in DDoS attacks</title><link>https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/</link><author>cybernewswire</author><category>devops</category><pubDate>Tue, 11 Feb 2025 07:01:39 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Harness Merges with Traceable to Provide Integrated DevSecOps Platform</title><link>https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 19:36:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AWS Weekly Roundup: AWS Step Functions, AWS CloudFormation, Amazon Q Developer, and more (February 10, 2024)</title><link>https://aws.amazon.com/blogs/aws/aws-weekly-roundup-aws-step-functions-aws-cloudformation-amazon-q-developer-and-more-february-10-2024/</link><author>Matheus Guimaraes</author><category>devops</category><pubDate>Mon, 10 Feb 2025 19:27:49 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[We are well settled into 2025 by now, but many people are still catching up with all the exciting new releases and announcements that came out of re:Invent last year. There have been hundreds of re:Invent recap events around the world since the beginning of the year, including in-person all-day official AWS events with multiple tracks to help you discover and dive deeper into the releases you care about, as well as community and virtual events.Last month, I was lucky to be a co-host for AWS EMEA re:Invent re:Cap which was a nearly 4-hour livestream with experts featuring demos, whiteboard sessions, and a live Q&A. The good news is that you can now watch it on-demand! We had a great team and thousands of people enjoyed learning through the virtual experience. I recommend you check it out or share it with colleagues who have not been able to attend any re:Invent re:Cap events.The Korean team also did an amazing job hosting their own virtual re:Invent re:Cap event, and itâ€™s also now available on-demand. So if you speak Korean I do recommend you check it out.If youâ€™re more of a reader, then we have a treat for you. You can download the full official re:Invent re:Cap deck with all the slides covering releases across all areas by visiting community.aws! While there, you can also check all the upcoming in-person re:Invent re:Cap community events remaining across the globe for a chance to still attend one of those in a city near you.But as we know, new releases, announcements, and updates donâ€™t stop at re:Invent. Every week there are even more, and this is why we have this Weekly Roundup series that you can read every Monday to get the AWS news highlights from the week before.So hereâ€™s what caught my attention last week.Here are some other releases that caught my attention this week from a variety of other AWS services:AWS CloudFormation introduces stack refactoring â€“ You can now split your CloudFormation stacks, move resources from one stack to another, and change the logical name of resources within the same stack. This adds a lot of flexibility enabling you to keep up with changes within your organization and architectures, such as streamlining resource lifecycle management for existing stacks, keeping up with naming convention changes, and other cases. You can refactor your stacks by using the AWS command line interface (CLI) or AWS SDK.AWS Config now supports 4 new release types â€“ AWS Config is great for monitoring resources across your AWS environment and help you towards ensuring alignment with your company and security policies as well as compliance requirements. It now has four new types of resources enabling you to monitor Amazon VPC block public access settings, any exceptions made within those settings, as well as monitor S3 Express One Zone bucket policies and directory bucket settings.Upcoming changes to the AWS Security Token Service (AWS STS) global endpoint â€“ To help improve the resiliency and performance of your applications, we are making changes to the AWS STS global endpoint (https://sts.amazonaws.com), with no action required from customers. Starting in early 2025, requests to the STS global endpoint will be automatically served in the same Region as your AWS deployed workloads. For example, if your application calls  from the US West (Oregon) Region, your calls will be served locally in the US West (Oregon) Region instead of being served by the US East (N. Virginia) Region. These changes will be released in the coming weeks and we will gradually roll it out to AWS Regions that are enabled by default by mid-2025.Looking for some reading recommendations? At the beginning of every year Dr. Werner Vogles, VP and CTO of Amazon, publishes a list of recommended books that he believes should have your attention. This yearâ€™s list is looking particularly good in my opinion!AWS Public Sector Day London, February 27 â€” Join public sector leaders and innovators to explore how AWS is enabling digital transformation in government, education, and healthcare.AWS Innovate GenAI + Data Edition â€” A free online conference focusing on generative AI and data innovations. Available in multiple Regions: APJC and EMEA (March 6), North America (March 13), Greater China Region (March 14), and Latin America (April 8).Thatâ€™s it for this week! See you next time :)Matheus Guimaraes | @codingmatheus]]></content:encoded></item><item><title>CNL: Optimizing Kyverno policy enforcement performance for large clusters</title><link>https://www.youtube.com/watch?v=DWmCAUCs3bc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/DWmCAUCs3bc?version=3" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:13:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>AWS Extends AI Agent Reach into the Realm of Testing Code</title><link>https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 13:45:46 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>