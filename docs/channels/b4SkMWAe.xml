<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>Tech Leaders Reveal New Approaches to Corporate Sustainability</title><link>https://devops.com/executive-strategies-driving-corporate-sustainability/</link><author>Bonnie Schneider</author><category>devops</category><pubDate>Sat, 15 Feb 2025 05:54:47 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[Over the past two years, Iâ€™ve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a â€œnice to have.â€ Itâ€™s now a core business strategy. Thatâ€™s the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies Driving Corporate Sustainability. Why 2025 is a [â€¦]]]></content:encoded></item><item><title>How do I configure Minikube to use my local IP address instead of the cluster IP?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/</link><author>/u/Own_Appointment5630</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 02:02:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.]]></content:encoded></item><item><title>Calico apiserver FailedDiscovery Check</title><link>https://www.reddit.com/r/kubernetes/comments/1ipkg32/calico_apiserver_faileddiscovery_check/</link><author>/u/Flimsy_Tomato4847</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 20:42:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I installed the calico operator and follwing custom-resources.yaml:# This section includes base Calico installation configuration. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 192.168.0.0/16 encapsulation: None natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} Getting this error in kube-apiserver logs:E0214 20:38:09.439846 1 remote_available_controller.go:448] "Unhandled Error" err="v3.projectcalico.org failed with: failing or missing response from https://10.96.207.72:443/apis/projectcalico.org/v3: Get \"https://10.96.207.72:443/apis/projectcalico.org/v3\": dial tcp 10.96.207.72:443: connect: connection refused" logger="UnhandledError" E0214 20:38:09.445839 1 controller.go:146] "Unhandled Error" err=< Error updating APIService "v3.projectcalico.org" with err: failed to download v3.projectcalico.org: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.96.207.72:443: connect: connection refused calico-apiserver calico-api ClusterIP 10.96.207.72<none> 443/TCP 45mDo you know any things to solve this?   submitted by    /u/Flimsy_Tomato4847 ]]></content:encoded></item><item><title>DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps</title><link>https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 14 Feb 2025 15:22:34 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11</title><link>https://www.youtube.com/watch?v=eHa6GhK7L0I</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/eHa6GhK7L0I?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:59:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>ChatLoopBackOff Episode 46 (Dragonfly)</title><link>https://www.youtube.com/watch?v=gd6HRgr8KcA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/gd6HRgr8KcA?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 05:56:56 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. 

This CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the 
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial  is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.sequenceDiagram
autonumber
rect rgb(191, 223, 255)
Kubelet->>+Kube-apiserver: Create Node
Note over Kubelet: Taint: node.cloudprovider.kubernetes.io
Kube-apiserver->>-Kubelet: Node Created
end
Note over Kube-apiserver: Node is Not Ready Tainted, Missing Node Addresses*, ...
Note over Kube-apiserver: Send Updates
rect rgb(200, 150, 255)
Kube-apiserver->>+Cloud-controller-manager: Watch: New Node Created
Note over Cloud-controller-manager: Initialize Node:Cloud Provider Labels, Node Addresses, ...
Cloud-controller-manager->>-Kube-apiserver: Update Node
end
Note over Kube-apiserver: Node is Ready
This new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a  resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a  resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one â€œcorrect wayâ€ to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting â€œhostNetworkâ€ to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure providerâ€™s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azureâ€™s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud providerâ€™s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Underâ€¦</title><link>https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4</link><author>Kuseh Simon Wewoliamo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:38 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Terraform Architecture ExplainedÂ , Terraform Core, State, and Plugins: How Terraform Works Under theÂ Hood.1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. ReferencesInfrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processesÂ , â€œClickOpsâ€. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using codeÂ , it enables you to automate and use all the best practices of software development. IaC eliminates human errorsÂ , speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like softwareÂ code.Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of itâ€™s declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.2. Terraform ArchitectureTerraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components: 1 Terraform core 2 Plugins (Providers and Provisioners) Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations filesÂ , building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms viaÂ HTTPs.Plugins (Providers and Provisioners)Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named â€œterraform.tfstateâ€. You can read more about terraform state here There are two ways to manage state: Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on RemoteÂ StateTerraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly threeÂ steps: The first step is to write your terraform configuration just like any other code using any editor of yourÂ choice. This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure. The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changesÂ ,terraform will go ahead perform the desired actions as defined execution.4. Terraform Best Practices1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.2. Itâ€™s a good practice to store your work in a version controlled repository even when youâ€™re just operating as an individual.3. When working as a team, itâ€™s important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.4. Never Store your state file in a version controlled repository.5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the official Terraform page. Donâ€™t forget to add your commentsÂ , till then keepÂ coding.6. Terraform:Up & RunningÂ , Third Edition by YevgeniyÂ Brikman]]></content:encoded></item><item><title>The Micro Frontend Revolution</title><link>https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4</link><author>Adem KORKMAZ</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Free AI models: Running Local LLMS with Llama 3.3,</title><link>https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4</link><author>Joel Wembo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Part 4 of 10 Part series on DeepSeekÂ MLOpsFree AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models usingÂ OllamaStep-by-Step Guide: Installing a Web UI for Local LLMs onÂ WindowsWith the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. The solution? A web-based user interface (UI) that allows easy interaction with your localÂ LLMs.In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process. is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging GGUF (GGML Unified Format), an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.Why Use a Web UI for LocalÂ LLMs?Using a web UI for local LLMs offers several advantages:: No need to work with command-line tools.: Manage multiple models in oneÂ place.: Chat history, prompt engineering, and adjustable settings.: Access your models remotely via a webÂ browserStep 1Â : Download and InstallÂ OllamaDownload Ollama from https://ollama.com/download/windows, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of theÂ desktop.To run Ollama and start utilizing its AI models, youâ€™ll need to use a terminal on Windows. Weâ€™ll skip it here and letâ€™s see how to install WebUI for a better experience.Now open the browser and type localhost:11434 to check is Ollama is up andÂ runningAlso, Check in your systemÂ TrayNext, Open your CMD to pull some free AIÂ modelsStep 2â€Šâ€”â€ŠInstall OllamaÂ WebUIRun the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use thisÂ command:docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainTo connect to Ollama on another server, change the OLLAMA_BASE_URL to the serverâ€™s URL. So if Ollama is on a Different Server, use thisÂ command:docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainNext, Open your browser and type localhost:3000Ollama utilizes Metal on macOS and CUDA on Windows/Linux for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a persistent server in the background, managing requests via an  that communicates with models using optimized token streaming.Internally, it uses low-level memory-efficient inference kernels, minimizing VRAM and RAM usage while maintaining performance. It also supports LoRA (Low-Rank Adaptation) fine-tuning, allowing users to personalize models on their local machine with minimal compute overhead.Run the following command:ollama run deepseek-r1:671bChoosing the Right Web UI for YourÂ Needs: LM Studio (Simple setup, user-friendly UI): Oobabooga (More features, customization options): Gradio (Custom interface, lightweight solution): Open WebUI (Accessible over the internet)Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether youâ€™re a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AIÂ models.Start today with one of these web UIs and bring AI power to your local machine!Â ğŸš€Thank you for ReadingÂ !! ğŸ™ŒğŸ», donâ€™t forget to subscribe and give it aÂ CLAP, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps & Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( ]]></content:encoded></item><item><title>Understanding Container Orchestration (AWS ECS, AWS EKS &amp; Kubernetes)</title><link>https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4</link><author>Althaf Hussain</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:55 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Why Do We Need Container Orchestration?1ï¸âƒ£ We use Docker to create and run containersDocker  using a Dockerfile with :: Packages and compiles theÂ app.: Runs the app and exposesÂ ports.docker run -p 80:80 my-appNow the app is running inside a . ğŸ‰2ï¸âƒ£ But what if the app crashes due to highÂ traffic?Docker cannot restart or scale theÂ app.If thereâ€™s high traffic (e.g., festive season sales), .3ï¸âƒ£ Solution? We need a tool to manage containers automatically!This is where Container Orchestration Tools comeÂ in!Examples: Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift.ğŸš€ Kubernetesâ€Šâ€”â€ŠFull Control but ComplexÂ Setupâœ…  When you want  over your cluster.âœ… Manages multiple containers (Docker is just for one container). (if traffic increases, it adds more containers). (if an app crashes, Kubernetes restartsÂ it).What is Kubernetes (Self-Managed)?If you want  over your cluster, you can set up Kubernetes manually.How Kubernetes Works (Practical Steps)1ï¸âƒ£ Create a VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).2ï¸âƒ£ Install Kubernetes, kubeadm, kubectl, networking, storage, etc.3ï¸âƒ£ Set up a  and .4ï¸âƒ£ Deploy your app using a .5ï¸âƒ£ Manage scaling, auto-healing, networking, etc. manually.ğŸ› ï¸ Steps to Deploy an App Using Kubernetes:1ï¸âƒ£ Set up a server (EC2 instance orÂ VM)sudo apt update && sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.listsudo apt updatesudo apt install -y kubelet kubeadm kubectl2ï¸âƒ£ Initialize Kubernetes clustermkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configapiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app        image: my-docker-image:latest        ports:kubectl apply -f deployment.yamlapiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80kubectl apply -f service.yamlğŸ¯ Your app is now running inside Kubernetes! ğŸš€âœ… Advantages of Self-Managed Kubernetesâœ”  â†’ You can configure every part of the cluster.âœ”  â†’ On-premise, AWS, Azure, GCP, or hybrid cloud.âœ”  â†’ Youâ€™re not tied to AWS, Azure, or any provider.âœ”  â†’ Most companies use  for flexibility.âŒ Disadvantages of Self-Managed Kubernetesâœ–  â†’ You need to manually configure networking, storage, security, etc.âœ–  â†’ You have to patch, upgrade, and secure the cluster yourself.âœ–  â†’ Setting up and managing Kubernetes is .ğŸš€ AWS ECSâ€Šâ€”â€ŠAWS Manages Everythingâœ…  Running containers without managing Kubernetes.âœ… You donâ€™t need to set up Kubernetes.Just  what app you want to run, and it does everything. over cluster management.ğŸ› ï¸ Steps to Deploy an App in AWSÂ ECS1ï¸âƒ£  in the AWS Console.2ï¸âƒ£  (Choose Fargate or EC2).3ï¸âƒ£ :Go to ECS > Task Definitions > Create new task definition.Choose  (serverless) or  (self-managed).Define  (Docker image, ports, CPU, memory).4ï¸âƒ£ :Go to ECS > Services > CreateÂ Service.Choose the cluster and task definition youÂ created.Define  (number of tasks).5ï¸âƒ£ Â ğŸ‰ğŸ¯ Your app is running inside AWS ECS without managing infrastructure! ğŸš€âœ”  â†’ AWS takes care of the infrastructure.âœ”  â†’ No need to set up Kubernetes manually.âœ” Tightly integrated with AWS â†’ Works great with AWS services like ALB, IAM, CloudWatch, etc.âœ” Less operational overhead â†’ No need to worry about maintaining aÂ cluster.âŒ Disadvantages of AWSÂ ECSâœ–  â†’ If you want to move your app from AWS to , or , you have to set up everything from scratch.âœ–  â†’ You donâ€™t have full control over how the cluster is managed.âœ–  â†’ Most companies prefer  over ECS for multi-cloud strategies.ğŸš€ AWS EKSâ€Šâ€”â€ŠAWS Manages Kubernetes forÂ Youâœ…  When you want Kubernetes but donâ€™t want to install it manually.âœ… AWS  (no need to install manually).You  to deployÂ apps. than ECS but  than DIY Kubernetes.ğŸ› ï¸ Steps to Deploy an App in AWSÂ EKS1ï¸âƒ£  in the AWS Console.2ï¸âƒ£ :Set Cluster name, VPC, IAMÂ role.AWS will create & configure the Kubernetes control plane.3ï¸âƒ£ aws eks update-kubeconfig --region your-region --name your-cluster-name4ï¸âƒ£  (same as Kubernetes DIY)kubectl apply -f deployment.yaml5ï¸âƒ£ Expose the app using a Kubernetes service (same asÂ before).ğŸ¯ Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup!Â ğŸš€âœ”  â†’ Works exactly like Kubernetes, so itâ€™s easy to move to another cloud (Azure AKS, Google GKE, etc.).âœ” Fully managed control plane â†’ AWS handles the  (setting up Kubernetes).âœ”  than ECS â†’ You can tweak networking, security, and scaling.âœ”  â†’ You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).âŒ Disadvantages of AWSÂ EKSâœ–  â†’ You still need to understand Kubernetes concepts.âœ– More operational overhead â†’ Though AWS sets up Kubernetes, you still .âœ–  â†’ You  for the Kubernetes controlÂ plane.ğŸ¯ Real-World Example of How These WorkÂ TogetherImagine youâ€™re running an :1ï¸âƒ£ You  to package your app into a container.2ï¸âƒ£ You deploy it to Kubernetes (DIY) if you want full control.3ï¸âƒ£ If you donâ€™t want to manage Kubernetes, you use  (simplest).4ï¸âƒ£ If you need Kubernetes but donâ€™t want manual setup, you use .ğŸ“Œ Think of Kubernetes as a powerful machine where you control everything.ğŸ“Œ Think of AWS ECS as a service where AWS does the heavy lifting for you.ğŸ“Œ Think of AWS EKS as Kubernetes, but AWS helps withÂ setup.Conclusion: Which One Should YouÂ Use?ğŸ‘‰  if you  and donâ€™t care about moving to another cloud.ğŸ‘‰  if you  but donâ€™t want to set it up manually.ğŸ‘‰ Use Self-Managed Kubernetes if you  and plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).ğŸ’¡ If youâ€™re , start with .If youâ€™re building , go for .If you want , use .]]></content:encoded></item><item><title>Understanding APIs: A Developerâ€™s Guide to Building and Using APIs</title><link>https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4</link><author>Subbareddysangham</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:21 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurantâ€Šâ€”â€Šcustomers (the client application) donâ€™t need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (theÂ API).I designed and developed an e-commerce web application with HTML, CSS, and JavaScript for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts ofÂ APIs.E-commerce Web Application API FlowÂ Chart:To check the complete sourceÂ code:APIs in the E-Commerce ApplicationThis E-Commerce application consists of the following API endpoints:1. Authentication API (auth_routes): /api/auth/login (POST) â†’ Authenticates users and starts aÂ session.: /api/auth/logout (POST) â†’ Clears session and logs outÂ users.2. Product API (product_routes): /api/products (GET) â†’ Returns a list of products.: /api/products/<int:product_id> (GET) â†’ Fetches details of a specificÂ product.3. Cart API (cart_routes): /api/cart (GET) â†’ Returns the current user'sÂ cart.: /api/cart (POST) â†’ Adds a product to theÂ cart.: /api/cart/<int:item_id> (DELETE) â†’ Removes an item from theÂ cart.4. Order API (order_routes): /api/orders (POST) â†’ Places an order with the items in theÂ cart.: /api/orders/<int:order_id> (GET) â†’ Fetches details of a specificÂ order.: /api/health (GET) â†’ Provides API uptime, session data, and frontendÂ path.: /<path:filename> (GET) â†’ Serves frontendÂ files.: / (GET) â†’ Serves index.html or API runningÂ message.: Handles missing resources.500 Internal Server Error: Handles unexpected issues.: Handles invalid requests.An API consists of several key components that work together:These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,ğŸ“Œ https://api.ecommerce.com/products â†’ Retrieves a list of available products.ğŸ“Œ https://api.ecommerce.com/cart â†’ Fetches the current user's shopping cart details.ğŸ“Œ https://api.ecommerce.com/orders â†’ Handles order-related operations.These actions can be performed on the allowed endpoints. Theyâ€™re like verbs telling the API what to do with theÂ data. â†’ Read data (View products, orders, cartÂ items). â†’ Create new data (Add product, register user, placeÂ order). â†’ Update existing data (Update profile, modify cart quantity). â†’ Remove data (Delete cart item, cancelÂ order). Additional data is sent to fine-tune the API request, such as specifying which page of results you want toÂ see.: Parameters are extra details added to an API request to filter or refine theÂ results.Fetch  of products:GET /api/products?category=laptopsSecurity measures ensure that only authorized users can access theÂ API.: Ensures that only authorized users can access theÂ API.When a user logs in, the API gives aÂ :{  "message": "Login successful",  "token": "eyJhbGciOiJIUz..."}To add a product to the cart, the request must include thisÂ :POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz... Prevents unauthorized access and protects userÂ data. The structure of the data returned by the API, commonly in formats like JSON orÂ XML.: The structure of the data sent back by theÂ API. (because itâ€™s easy to read andÂ use).{  "id": 1,  "price": 799.99,} Frontend uses this data to display products toÂ users.APIs are classified according to their usage patterns and architectures.API Types According to Purposes ofÂ UseğŸ”¹ â€Šâ€”â€ŠUsed within a company, hidden from public access. Helps teams share data securely.ğŸ”¹ â€Šâ€”â€ŠAvailable to everyone, can be free or paid. Example: Google MapsÂ API.ğŸ”¹ â€Šâ€”â€ŠUsed between business partners for secure data exchange. Example: E-commerce & shipping company integration.ğŸ”¹ â€Šâ€”â€ŠCombines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in oneÂ call.API Types According to Architectural Structure:1. Web APIs (HTTP/HTTPS APIs)These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:1.1. REST (Representational State Transfer):The most popular type of web API today. REST APIs follow these principles:Stateless: Each request contains all the information neededResource-based: Everything is treated as a resource with a uniqueÂ URLUses standard HTTP methods (GET, POST, PUT,Â DELETE)Supports multiple data formats (usuallyÂ JSON)Example REST API Request in an E-Commerce Web Application:This request fetches all available products from the onlineÂ store.âœ… Request (Client â†’Â Server)GET /api/products HTTP/1.1Host: api.ecommerce.comAuthorization: Bearer <User_Token>Content-Type: application/jsonâœ… Response (Server â†’Â Client)[    {"id": 1, "name": "iPhone 15", "price": 999.99, "stock": 20},    {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99, "stock": 15}]catalog.html Fetches and DisplaysÂ Products1ï¸âƒ£ User Visits catalog.htmlThe user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).The browser  to fetch productÂ data.2ï¸âƒ£ Frontend (JavaScript) Sends an APIÂ RequestJavaScript code in catalog.html makes a GET request to the  /api/products.3ï¸âƒ£ Backend API (GET /api/products) FetchesÂ DataThe get_all_products() function runs when the frontend calls /api/products.4ï¸âƒ£ Database Retrieves Product InformationThe backend queries the  in theÂ databaseExample database response:[    {"id": 1, "name": "Laptop", "price": 799.99},    {"id": 2, "name": "Smartphone", "price": 499.99}]5ï¸âƒ£ Frontend Renders Product Data in catalog.htmlThe JavaScript loops through the  and dynamically  to display products.<div class="product-card">  <h3>Laptop</h3>  <button onclick="addToCart(1)">Add to Cart</button><div class="product-card">  <p>Price: $499.99</p>  <button onclick="addToCart(2)">Add to Cart</button></div>1.2. SOAP (Simple Object Access Protocol):SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe inÂ nature.A more rigid, protocol-specific API style used in enterprise environments:Highly structured messaging<soap:Envelope>  <soap:Header>    <Authorization>Bearer abc123</Authorization>  </soap:Header>    <GetUser>    </GetUser></soap:Envelope>A modern API query language that gives clients moreÂ control:Clients specify precisely what data theyÂ needSingle endpoint for allÂ requestsReduces over-fetching and under-fetching ofÂ dataFacebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for theÂ query.query {  user(id: "123") {    email      title  }These are programming interfaces provided by software libraries or frameworks:Used directly in yourÂ codeNo network requests areÂ neededUsually specific to a programming languageExample using a Python libraryÂ API:import pandas as pd# Using pandas API to read a CSV filedf = pd.read_csv('data.csv')These allow applications to interact with the operating system:Example using Pythonâ€™s OSÂ API:import os# Using OS API to create a directoryos.mkdir('new_folder')The foundation of web APIs, using well-defined methods and statusÂ codes:PUT: Update existingÂ dataPATCH: Partially updateÂ data2xx: Success (e.g., 200Â OK)Enables real-time, two-way communication:Ideal for chat apps and liveÂ updatesExample WebSocket connection:const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) => {    console.log('Received:', event.data);};gRPC (Google Remote Procedure Call) is a  framework for inter-service communication in microservices architecture. Unlike REST APIs that use , gRPC uses Protocol Buffers (Protobuf), making it faster and more efficient.Googleâ€™s high-performance RPC framework:Excellent for microservicesExample Protocol Buffer definition:ğŸ“Œ How gRPC Works in a Web ApplicationIn an , gRPC can be used for fast communication between microservices.A  must fetch a  from the backend .1ï¸âƒ£ Defining gRPC Service (product.proto)gRPC services use Protocol Buffers (Protobuf) to define API contracts.GetAllProducts(): Returns a list of products.GetProductById(): Fetches a single product byÂ ID.Product: Defines the product structure.2ï¸âƒ£ Implementing gRPC Server (product_server.py)The gRPC server implements the serviceÂ logic.Implements ProductService methods (GetAllProducts, GetProductById).3ï¸âƒ£ Implementing gRPC Client (product_client.py)The client  to fetchÂ data.Calls GetAllProducts() to fetch all products.Calls GetProductById() to fetch a singleÂ product.4ï¸âƒ£ Running gRPC Server &Â Client# 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.pyProductService gRPC Server is running on port 50051...Product List: products {  name: "Laptop"}  id: 2  price: 499.991. GET: Used to retrieveÂ dataIt is a request used to retrieve data. Never used to delete, update or insertÂ data.Product API in E-Commerce Web Applicationcurl -X GET http://52.90.222.178:5000/api/products[   {"id": 1, "name": "iPhone 15", "price": 999.99},   {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99}]If the API returns JSON, you can format the response usingÂ curl -X GET http://52.90.222.178:5000/api/products | jqDebugging & TroubleshootingIf youâ€™re not getting the expected response, check:Is the Flask serverÂ running?curl -X GET http://52.90.222.178:5000/api/healthHow the /api/health Endpoint Works inÂ FlaskThe endpoint is a  that provides the current status of the application, including . It helps in monitoring the system and ensuring that the API is .I have configured this /api/health endpoint in my E-Commerce Web Application.@app.route("/api/health", methods=["GET"])def health_check():    uptime = time.time() - start_time    return jsonify({        "uptime": f"{uptime:.2f} seconds",        "session_active": "username" in session    }), 2002. POST: Creates new resourcesThe  method is a request used to insert data. Posted data typeâ€Šâ€”â€ŠJSON.âœ… Authentication EndpointsThis is handled in auth_routes.py, prefixed with /api/auth. â†’ The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.User sends a  with username & password.If credentials areÂ valid:The user session isÂ stored.API returns a successÂ message.3. If credentials areÂ invalid:API returns 3. PUTÂ : Updates existing resourcesPUT method is used to create or update (replace) a resource. Useful for syncingÂ data.Ex: We can add a â€œChange Passwordâ€ feature for an existing user using a PUT /api/auth/change-password API endpoint.Steps to Implement â€œChange Passwordâ€ APIPUT request with their old and new password.curl -X PUT http://localhost:5000/api/auth/change-password      -H "Content-Type: app     -d '{"old_password": "currentPass123", "new_password": "newPass456"}'2. API verifies the old password:If incorrect, return an error (401 Unauthorized).3. If correct, update the password in the database.4. Save the new password (after hashing it for security).5. Return a successÂ message.{"message": "Password changed successfully"}In an e-commerce application like yours, a PUT method would typically be usedÂ in: â†’ PUT /api/auth/update-profileUpdating Product Information (Admin) â†’ PUT /api/products/<product_id> â†’ PUT /api/cart/<cart_id> â†’ PUT /api/orders/<order_id>4. DELETE: Removes resourcesThe DELETE method deletes the specified resource./api/cart API to Remove a Product from theÂ Cart1ï¸âƒ£ Endpoint Definition (FlaskÂ API):@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.    Expects JSON payload: { product_id }.    """        user_id = session.get('user_id')            return jsonify({"message": "User not authenticated"}), 401        product_id = data.get('product_id')            return jsonify({"message": "'product_id' is required"}), 400        connection = get_db_connection()        cursor = connection.cursor()        delete_query = "DELETE FROM cart_items WHERE user_id = %s AND product_id = %s"        cursor.execute(delete_query, (user_id, product_id))            return jsonify({"message": "Product not found in cart"}), 404        return jsonify({"message": "Product removed from cart successfully"}), 200        return jsonify({"message": "Failed to remove product from cart", "error": str(e)}), 500        close_db_connection(connection)User sends a DELETE request with the product_id they want toÂ remove.API verifies if the user is logged in (checks session['user_id']). (if missing, returns 400 Bad Request). to remove the product from the cart_items table.If the product does not exist, it returns 404 NotÂ Found., it commits the transaction and returns a success message (200Â OK).Handles database errors and ensures the connection isÂ closed.3ï¸âƒ£ Example API Request & Response:Now User wanted to delete iPhone 15 Pro from theÂ cart:Click the  button under â€œiPhone 15Â Proâ€.The item should disappear, and the cart total shouldÂ update.Run this command in the terminal:curl -X DELETE http://52.90.222.178:5000/api/cart      -H "Content-Type: application/json"      -H "Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0"     -d '{"product_id": 4}'{    "message": "Product removed from cart successfully"After deletion of thatÂ item:5. PATCH: Partially updates resourcesPATCH method is to request used to update data. Only passed data will be updated. You donâ€™t need to provide all the dataÂ set.The  method is used to  a resource. Instead of sending the entire data set, we only send the fields that need to beÂ updated.Use Case: Updating a Userâ€™s Profile (PATCH /api/auth/update-profile)Imagine a user wants to update  or  without changing their username.1ï¸âƒ£ PATCH Endpoint: PATCH /api/auth/update-profile2ï¸âƒ£ Sending a PATCHÂ RequestIf the user wants to update curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \     -H "Content-Type: application/json" \     -H "Cookie: session=your_valid_session_id" \     -d '{"email": "newemail@example.com"}'ğŸ”¹ Only the  field will beÂ updated.âœ… {    "message": "Profile updated successfully"âŒ If no fields are provided:{    "message": "No valid fields provided for update"{    "message": "User not authenticated"4ï¸âƒ£ Why Use PATCH Instead ofÂ PUT?Conclusion: Understanding APIs, Endpoints, and Methods in Web DevelopmentAPIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and aÂ server.An  acts as a bridge between two applications, enabling data exchange.Example: A shopping website uses an API to fetch product details from a database.An  is a URL that clients use to request or sendÂ data.Example: GET /api/products retrieves all products. â†’ Uses HTTP methods (GET, POST, PUT, DELETE) to manageÂ data. â†’ Lets clients request specific data fields, reducing unnecessary data transfer. â†’ Uses XML messaging, mainly in enterprise applications. â†’ Maintains a continuous connection for real-time updates (e.g., liveÂ chat).Use  (JWT, API Keys, OAuth) to restrictÂ access.Protect sensitive data with .Implement  to preventÂ abuse.APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secureÂ APIs.Iâ€™d love to hear what you think about this articleâ€Šâ€”â€Šfeel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean aÂ lot.For more insights into the world of technology and data, visit  Thereâ€™s plenty of exciting content waiting for you toÂ explore!Thank you for reading, and happy learning! ğŸš€]]></content:encoded></item><item><title>Most Developers Get This Wrong in Docker Networking!</title><link>https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4</link><author>Gaddam.Naveen</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:15 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup</title><link>https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4</link><author>th@n@n</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:10 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, weâ€™ll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard criticalÂ data.In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Letâ€™s break down eachÂ partkind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerThis StorageClass resource defines storage provisioning and management policies.Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to beÂ created.PersistentVolumeClaim (PVC) Resource:apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:  storageClassName: localstorage  accessModes:  resources:      storage: 2GiThis PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a singleÂ node.PersistentVolume (PV) Resource:apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt  storageClassName: localstorageThis PV resource represents the actual storage volume available for use by theÂ PVC.It is bound to the PVC pvc-jenkins within the jenkins namespace.The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins          image: jenkins/jenkins:lts          resources:              memory: "0.5Gi"            requests:              cpu: "125m"            - name: http-port            - name: jnlp-port          livenessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0This Deployment resource defines how Jenkins is deployed.It specifies a single replica (replicas: 1) of the Jenkins container.The container is based on the jenkins/jenkins:lts image.Resource limits and requests for CPU and memory are set to ensure resource allocation.Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.Liveness and readiness probes are configured to check the health of the container.The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.SecurityContext ensures that Jenkins runs with the appropriate user and group permissions. securityContext:        runAsUser: 0        fsGroup: 0When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000This Service resource exposes the Jenkins deployment internally within the jenkins namespace.It selects pods with the label app: jenkins-pod.The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.Once you execute all the manifiest file in kubernetes cluster.Check the host volume path ls -alÂ /mntExecute the below command to see the whether the same files are present in the jenkins containerkubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_homeThis configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.For now, thatâ€™s it guys, If you like this article donâ€™t forget to give a clap.Â ğŸ‘]]></content:encoded></item><item><title>How to Build and Deploy a Simple Frontend App with Python Backend</title><link>https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4</link><author>krth1k</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:06 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Building a full-stack web application might seem daunting, especially if youâ€™re primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend withÂ ease.In this guide, weâ€™ll walk through the processÂ of:Setting up a basic Python backend withÂ FlaskCreating a simple frontend with HTML, CSS, and JavaScriptConnecting the frontend to the backend using RESTÂ APIDeploying the app on a local Kubernetes cluster1. Setting Up the PythonÂ BackendWeâ€™ll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.Ensure you have Python installed, then installÂ Flask:Create a new file calledÂ app.py:from flask import Flask, jsonify@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)This API exposes a single endpoint /api/message that returns a JSON response.For the frontend, weâ€™ll use HTML, CSS, and JavaScript to display the data from ourÂ backend.Create an HTML File (index.html)<!DOCTYPE html><html lang="en">    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>Frontend App</title>        body {            font-family: Arial, sans-serif;            text-align: center;        }            padding: 10px 20px;        }</head>    <h1>Simple Frontend App</h1>    <button onclick="fetchMessage()">Get Message</button>    <p id="message"></p>        function fetchMessage() {            fetch('http://127.0.0.1:5000/api/message')                .then(response => response.json())                    document.getElementById("message").innerText = data.message;                .catch(error => console.error('Error:', error));    </script></html>This page has a button that fetches and displays a message from the FlaskÂ backend.3. Connecting the Frontend to theÂ BackendNow, letâ€™s serve the frontend using  itself so that both frontend and backend are accessible from the sameÂ origin.Update app.py to ServeÂ HTMLModify app.py to serve the index.html file:from flask import Flask, jsonify, send_from_directoryapp = Flask(__name__, static_folder='static')@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})@app.route('/')def serve_frontend():    return send_from_directory('static', 'index.html')if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)Move index.html to a staticÂ FolderYour project structure should now look likeÂ this:/project-folderâ”‚-- app.pyâ”‚   â””â”€â”€ index.htmlNow, visit http://127.0.0.1:5000/ in your browser, and your frontend will beÂ served!Now, letâ€™s deploy this app using .# Use Python base imageFROM python:3.9# Set the working directoryWORKDIR /app# Copy application filesCOPY . .# Install dependenciesRUN pip install flask# Expose port 5000EXPOSE 5000# Run the appCMD ["python", "app.py"]Build and Run the Docker Containerdocker build -t myapp .docker run -p 5000:5000 myappCreate a Kubernetes Deployment YAML (apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePortkubectl apply -f deployment.yamlminikube service myapp-service --urlVisit the displayed URL in yourÂ browser!In this tutorial, we covered: âœ… Creating a Flask backend âœ… Building a simple HTML/JavaScript frontend âœ… Connecting the frontend to the backend âœ… Deploying the app with Docker and KubernetesThis is a basic example, but you can expand itÂ by:Adding user authenticationUsing React or Vue.js for a modernÂ frontendStoring and retrieving data from aÂ databaseIf you found this helpful, let me know in the comments! ğŸš€]]></content:encoded></item><item><title>Best Practices For Database Authorization In Multi-Tenant Systems</title><link>https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4</link><author>Noel</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:49:43 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.This article explores the best practices and technical solutions that we adopt at Xinthe, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.Understanding Multi-Tenant Database AuthorizationDefinition Of Multi-TenancyMulti-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenantâ€™s data remains logically isolated, ensuring that no tenant can access anotherâ€™s data, while sharing underlying resources for efficiency.Key Multi-Tenancy Models:Each tenant has a dedicated database orÂ schema.Offers strong isolation and security.Higher costs and maintenance complexity due to multiple instances.Multiple tenants share the same database.Logical separation is maintained through identifiers (e.g., tenant_id or company_id).Cost-effective and scalable but requires robust authorization mechanisms.Challenges Of AuthorizationImplementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges includeÂ -Cross-Tenant DataÂ Leakage:Risk: Improper queries or configurations can expose data to unauthorized tenants.Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHEREÂ clause.Deeply nested resource structures often require joins across multipleÂ tables.Queries with extensive joins can degrade performance as data volume increases.Scalability & Maintainability:The need to balance fast access controls with a maintainable schema.Adding new authorization rules or resource types without overhauling theÂ system.Data Localization & Compliance:For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.Importance Of Nested Resource StructuresIn many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -Company â†’ Client â†’ Project â†’Â TaskA  has multipleÂ .Each  manages several .Each  contains multipleÂ .Why Nested Structures Matter:Access Control Complexity: Permissions must flow through the hierarchy (e.g., a userâ€™s access to a task must be verified against their company). Hierarchical access often necessitates multiple joins, impacting query efficiency. Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organizationâ€™s boundaries.A user from Company A should only edit tasks within their projects. Authorization must ensure that the task â†’ project â†’ client â†’ company linkage is maintained without exposing data from CompanyÂ B.Comparing Authorization ApproachesWhen implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and Tenant-Specific Databases or Tables. Each comes with its own set of benefits and trade-offs. Letâ€™s break them downÂ -1. Flat Model (Adding TenantÂ IDs)In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization. Queries can directly filter by tenant_id without traversing the hierarchy.SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id; Reduces query complexity by avoiding multiple table joins to enforceÂ access. Straightforward implementation makes it easy to debug and maintain. tenant_id is replicated across multiple tables, introducing redundancy. Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows. Schema updates (e.g., adding new relationships) might require extensive changes across multipleÂ tables.Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many smallÂ tenants.2. Hierarchical Model (Enforcing Relationships)In this approach, the relationships between resources (e.g., task â†’ project â†’ client â†’ company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy. Avoids redundant fields by relying on inherent relationships.CREATE TABLE tasks (    id SERIAL PRIMARY KEY,    project_id INT REFERENCES projects(id),    ...    id SERIAL PRIMARY KEY,    client_id INT REFERENCES clients(id),    ...    id SERIAL PRIMARY KEY,    company_id INT REFERENCES companies(id),    ... Reduces duplication of metadata like tenant_id.Relationship-Centric Queries: Makes it easier to enforce hierarchical constraints and maintain referential integrity. Queries require multiple joins to verify access, which can impact performance.SELECT t.*FROM tasks tJOIN projects p ON t.project_id = p.idJOIN clients c ON p.client_id = c.idWHERE c.company_id = :tenant_id AND t.id = :task_id; Deep hierarchies with large datasets can significantly increase query execution time. As the hierarchy grows, maintaining performance becomes challenging.Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.3. Tenant-Specific Databases OrÂ TablesThis approach creates separate databases or tables for each tenant, isolating their data entirely. Each tenantâ€™s data can be managed independently, making it easier to scale horizontally by distributing databases acrossÂ servers. Ensures complete data isolation, reducing the risk of cross-tenant dataÂ leakage. Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions. Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines. Schema updates need to be applied consistently across all tenant databases. For tenants with small datasets, the resource consumption of separate databases might be inefficient.Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).Summary Tableâ€Šâ€”â€ŠComparing ApproachesCriteria For Choosing An Authorization ModelSelecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to considerÂ -The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.High-Traffic Applications:Benefit from simpler and faster queries, such as those enabled by the .SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;Minimal joins mean lower query latency, ensuring the system performs well under heavyÂ loads.Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.:Can afford the  with more joins, as performance trade-offs are less significant.Allows for cleaner schema designs and strict relational integrity.Suitable for internal enterprise tools or smaller-scale applications.2. Regulatory RequirementsCompliance with data protection and privacy regulations often dictates how data is stored and accessed.Using Tenant-Specific Databases or Tables simplifies compliance for regulations like GDPR orÂ HIPAA.Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.An enterprise customer requires dedicated storage with separate backups and auditÂ logs.A  can still meet compliance needs with appropriate access controls and audit mechanisms.Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.The ability to handle growth in the number of tenants and data volumes is a criticalÂ factor.Planning For TenantÂ Growth:For a rapidly scaling user base, Tenant-Specific Databases or Tables provide the most flexibility -Each tenant can be distributed across servers to balanceÂ load.Tenant databases can be independently scaled based on specificÂ needs.A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenantÂ size.The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.The  may struggle as table sizes grow, requiring optimization for complexÂ joins.Ease of schema management and updates is essential for long-term maintainability.Simplified SchemaÂ Updates:The  simplifies schema updates by centralizing data attributes like tenant_id.However, redundant fields may increase the risk of errors duringÂ updates.The  enforces relational integrity, ensuring data consistency.Complex queries for nested structures may require more effort to maintain and optimize.Automated CI/CD Pipelines:For Tenant-Specific Databases, CI/CD automation becomes critical to manage schema changes across multiple databases.Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.Key Considerations SummaryDesigning Authorization Strategies For Multi-TenancyDesigning robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    FOREIGN KEY (project_id) REFERENCES projects(id));Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.Defining Relationships In Hierarchical Structures:Maintain strict referential integrity between hierarchical entities.Example for hierarchical relationships -CREATE TABLE projects (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)Flat schema enables quick lookups for tenant-specific data.Hierarchical relationships ensure data consistency and logical separation.2. Indexing Best PracticesIndexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.Compound Indexes For Tenant-Specific Queries:Use composite indexes combining tenant_id with frequently queriedÂ columns.CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);This enables efficient filtering and sorting within a tenantâ€™sÂ scope.Balancing Indexing Depth & QueryÂ Speed:Avoid over-indexing, which can slow down write operations.Prioritize indexing columns involved in filtering, joining, and sorting operations.Regularly analyze query performance using tools like  in PostgreSQL or  inÂ MySQL.Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.Horizontal Partitioning ByÂ Tenants:Partition data within a single database based on tenant_id.CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1Faster tenant-specific queries as partitions reduce the searchÂ space.Simplifies maintenance for large datasets.Database Sharding For High-Scale Systems:Distribute tenant data across multiple databases (shards).Example Sharding StrategyÂ -Use tenant_id % shard_count to assign tenants toÂ shards.Tools like  or  can manage sharding in distributed databaseÂ systems.Eliminates contention in single-database systems.Enhances fault isolation and scalability.Example Use Caseâ€Šâ€”â€ŠApplying These StrategiesAn application manages 100,000 tenants, each with thousands of projects andÂ tasks.Add tenant_id to allÂ tables.Use foreign keys to link tasks â†’ projects â†’Â clients.Create a compound index on tasks (tenant_id, project_id) for common queries likeÂ -SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;For smaller tenants, use horizontal partitioning -CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);For larger tenants, shard data across multiple databases toÂ scale.A well-designed schema with tenant_id simplifies multi-tenant data filtering.Proper indexing ensures efficient queries, even atÂ scale.Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.1. Flat Model ImplementationThe flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (client_id) REFERENCES clients(id));: Access tasks for a userâ€™s companyÂ -SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'in_progress';Simplifies authorization logic with directÂ lookups.Reduces query complexity by avoidingÂ joins.Potential schema bloat with additional tenant_id columns.2. Hierarchical Model ImplementationIn this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company â†’ Client â†’ Project â†’Â Task).CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (company_id) REFERENCES companies(id));    id BIGINT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (client_id) REFERENCES clients(id));    id BIGINT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id): Check task access by traversing relationships -SELECT t.* FROM tasks tINNER JOIN projects p ON t.project_id = p.idINNER JOIN clients c ON p.client_id = c.idINNER JOIN companies co ON c.company_id = co.idWHERE co.id = 123 AND t.status = 'in_progress';Maintains normalized relationships.Avoids redundant tenant_id columns.Complex joins increase queryÂ costs.Requires optimized indexes to maintain performance.3. Tenant-Specific Database/Table ImplementationFor scenarios requiring strict isolation, separate databases or tables for each tenant can beÂ used.Create a separate database or schema for each tenantÂ -CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);: Access tasks for a specific tenantÂ -USE company_123;SELECT * WHERE status = 'in_progress';: Use CI/CD tools like  to manage multi-tenant databases:Automate schema changes across databases.Track versioning for each tenantâ€™s database.Complete tenant isolation for security and compliance (e.g.,Â GDPR).Simplifies data archival and backup for individual tenants.Deployment complexity increases with the number ofÂ tenants.Resource-intensive for systems with many smallÂ tenants.Choosing The Right ImplementationUse the flat model for simplicity in high-traffic environments.Use the hierarchical model when data relationships must be preserved and redundancy minimized.Opt for tenant-specific databases for strict isolation and compliance requirements.Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.Security Best Practices For AuthorizationEnsuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.1. Strict AccessÂ ControlsImplementing strong access controls ensures that only authorized users can access or modify resources.Role-Based Access ControlÂ (RBAC):Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.Enforce role-specific permissions at the application and databaseÂ layers.Example: Use database roles to restrict access to tenant-specific tables.CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access:Enforce tenant-level data segregation directly at the databaseÂ layer.RLS ensures that queries automatically filter data based on the userâ€™sÂ tenant.PostgreSQL Example ForÂ RLS:CREATE POLICY tenant_policyON tasksUSING (tenant_id = current_setting('app.current_tenant')::BIGINT);ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;SET app.current_tenant = '123'; -- Simulate tenant contextSELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible2. Preventing Cross-Tenant DataÂ LeaksPreventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.Multi-Layer AccessÂ Checks:Enforce tenant isolation at both the database and application layers.Validate all queries to ensure they are scoped to the userâ€™sÂ tenant.Always include a tenant_id check in databaseÂ queries.Use database views or abstractions to simplify tenant-specific filtering.Application-Layer Validation:Add additional validation at the application level as a guardrail.Ensure that APIs restrict data access to the authenticated tenantÂ context.def get_user_tasks(user):    if user.tenant_id != request.tenant_id:        raise PermissionDenied("Cross-tenant access is not allowed.")    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.User ID and tenant ID for allÂ queries.Access attempts (successful andÂ failed).Data modification operations (insert, update,Â delete).Timestamps and IP addresses for requests.SQL Example For LoggingÂ Queries:CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPINSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');Integrating LoggingÂ Tools:Use database triggers to log operations automatically.Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());    RETURN NEW;$$ LANGUAGE plpgsql;CREATE TRIGGER audit_task_changesAFTER INSERT OR UPDATE OR DELETE ON tasksFOR EACH ROW EXECUTE FUNCTION log_task_changes();Combine RBAC, RLS, and application-level validation for comprehensive protection.Use multi-layered access checks and robust query scoping to ensure tenant isolation.Maintain detailed audit logs to track access and modifications for accountability and compliance.These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenantâ€™s data is isolated, protected, and auditable.Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.1. Balancing Performance & FlexibilityChoosing between speed and schema cleanliness can significantly impact your database design and performance.Prioritizing Performance (FlatÂ Model):Direct lookups using a tenant_id column ensure fast query execution.Reduced join complexity leads to quicker responseÂ times.: May result in data redundancy (e.g., repeating tenant IDs across multipleÂ tables).SQL Example For Optimized Query:SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'pending';Prioritizing Schema Cleanliness (Hierarchical Model):Using a normalized schema ensures a clean and consistent database structure.: Requires more complex joins and increased query times, especially for deeply nested relationships.Hierarchical QueryÂ Example:SELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.tenant_id = 123;Managing tenant-specific setups adds complexity, particularly as the number of tenantsÂ grows.Tenant-Specific Databases:Each tenant has its own database, simplifying compliance and data isolation.: Maintaining consistency across databases for schemaÂ changes.: Use automation tools like  or  to manage schema migrations acrossÂ tenants.# Liquibase command to apply migrations to multiple tenant databasesliquibase --url="jdbc:mysql://db_host/tenant1" updateliquibase --url="jdbc:mysql://db_host/tenant2" updateSingle Multi-Tenant Database:Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.: Tracking and isolating tenant data effectively without introducing query overhead.3. Handling Schema Updates In Multi-Tenant DatabasesEnsuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.Use versioned migrations to apply incremental updates across allÂ tenants.Maintain backward compatibility to prevent disruptions duringÂ updates.CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPApply updates to a subset of tenants, validate, and then roll out to theÂ rest.Use feature flags to selectively enable new schema features.Code Example For RollingÂ Updates:tenants = get_tenant_list()for tenant in tenants:    apply_schema_update(tenant_id=tenant.id)Testing & CI/CD For Multi-Tenant Systems:Test migrations on a staging environment with realistic tenant data before deploying.Use CI/CD tools like  to automate and track updates across tenant databases.Performance Vs. Flexibility:Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.Tenant-specific databases simplify compliance but require robust automation for schema management.Implement version control and rolling updates to ensure seamless schema changes across allÂ tenants.Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.Scenario 1â€Šâ€”â€ŠFlat Model For A SaaS CRMÂ AppA SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.: Each company has its own sales team, and users need quick access to customer records and salesÂ data.: Use a flat model by adding tenant_id to every table, such as customers, leads, andÂ sales.CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    name VARCHAR(255),    phone VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    customer_id BIGINT,    status VARCHAR(50),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);SELECT * FROM customers Simple queries without joins for tenant-specific data.High performance due to directÂ lookups.Wider tables due to the inclusion of tenant_id.Potential redundancy if relationships between entities are not properly normalized.Scenario 2â€Šâ€”â€ŠHierarchical Model For A Project Management ToolA project management tool with a nested structure: Company â†’ Client â†’ Project â†’ Task. Users need to manage projects while maintaining strict access control based on their organization.: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to theirÂ company.: Use a hierarchical model to enforce relationships and control access throughÂ joins.CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPSELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.company_id = 123;Clean, normalized schema without redundant data.Access naturally follows the hierarchy.Complex queries due to multi-level joins.Slower query performance for deep hierarchies.Scenario 3â€Šâ€”â€ŠSingle-Tenant Databases For An Enterprise AppAn enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.: Each tenantâ€™s data must be completely isolated to ensure compliance and scalability.: Use a single-tenant database model where each company has its own dedicated database.Database Names:  tenant_1_db  tenant_3_db: Use  to manage database updates across multipleÂ tenants.deploy:  steps:    - name: Update Tenant Databases      script: |        for db in $(list_databases); do          apply_migrations $dbComplete isolation ensures compliance with regulatory requirements.Scalability: Large tenants can have dedicated resources (e.g., separate hardware).Higher operational complexity in managing multiple databases.Requires robust CI/CD pipelines for schemaÂ updates.Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.Efficient, scalable, and secure authorization in multi-tenant databases as weâ€™ve found often at Xinthe, requires a well-thought-out approach tailored to the applicationâ€™s needs.Authorization in multi-tenant databases isnâ€™t a one-size-fits-all challenge. Developers and database architects must carefully evaluate their applicationâ€™s structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.]]></content:encoded></item><item><title>Docker, Kubernetes, and NATS â€” The Backbone of Cloud-Native Apps</title><link>https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4</link><author>Cristhian Ferrufino</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:48:09 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Docker, Kubernetes, and NATSâ€Šâ€”â€ŠThe Backbone of Cloud-Native AppsWelcome back to the ! In the last article, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, itâ€™s time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? Thatâ€™s the head chef, making sure everyone works inÂ harmony.In this article, weâ€™ll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, youâ€™ll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Letâ€™s getÂ cooking!What Is Containerization, and Why Is It Important?Imagine youâ€™re shipping a fragile package across the world. Youâ€™d want to pack it in a sturdy container, right? Thatâ€™s exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environmentsâ€Šâ€”â€Šwhether itâ€™s your laptop, a testing server, or a production cluster.Why developers love containers:âœ… : â€œWorks on my machineâ€ becomes â€œWorks everywhere.â€âœ… : No more dependency hellâ€Šâ€”â€Šeach app lives in its own bubble.âœ… : Deploy to AWS, Azure, or your grandmaâ€™s PC (if sheâ€™s cool with Kubernetes).âœ… : 10x lighter than VMs. Think EVs vs. a gas-guzzling truckIntroduction to Docker: Building, Running, and Managing ContainersDocker is the most popular tool for containerization, and for good reason. Itâ€™s simple, powerful, and widely supported. Letâ€™s break itÂ down:To create a container, you start with a â€Šâ€”â€Ša text file that defines the steps to build your appâ€™s environment. Hereâ€™s a simpleÂ example:# Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim# Set the stage for your appWORKDIR /app# Install dependencies (Pro tip: Skip the cache to shrink your image)COPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt# Copy the rest of the codeCOPY . .# Open the appâ€™s â€œfront doorâ€EXPOSE 8080CMD ["python", "app.py"]With this Dockerfile, you can build a container image using the ``Â command:ğŸš€ Run this: docker build -t my-python-app .Once youâ€™ve built your image, you can run it as a container:ğŸ¯ Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-appThis command starts your app and maps port 8080 on your host to port 8080 in the container. Easy,Â right?Docker also provides tools to manage your containers:: List running containers.docker logs <container_id>: View logs for a specific containerdocker stop <container_id>: Stop a running container.Docker vs. Podman: A Detailed ComparisonWhile Docker is the most popular containerization tool, itâ€™s not the only one.  is a rising star in the container world, and itâ€™s worth understanding how it compares toÂ Docker.+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker ğŸ³                           | Podman ğŸ“¦                         |+----------------------+-------------------------------------+-----------------------------------+| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       ||                      |                                     | directly)                         |+----------------------+-------------------------------------+-----------------------------------+| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  ||                      |                                     | of the box                        |+----------------------+-------------------------------------+-----------------------------------+| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  ||                      |                                     | and Dockerfiles                   |+----------------------+-------------------------------------+-----------------------------------+| Security             | Requires root privileges, which can | Rootless mode reduces attack      ||                      | be a security risk                  | surface                           |+----------------------+-------------------------------------+-----------------------------------+| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        ||                      | orchestration                       | natively                          |+----------------------+-------------------------------------+-----------------------------------+| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  ||                      |                                     | Hat                               |+----------------------+-------------------------------------+-----------------------------------+You need a mature, widely supported tool with a large ecosystem.Youâ€™re already using Docker Swarm for orchestration.Youâ€™re okay with running containers asÂ root.You want a daemonless, more secure alternative toÂ Docker.Youâ€™re working in environments where root privileges are restricted.Youâ€™re already using Kubernetes and want tighter integration.Both tools are excellent choices, so pick the one that best fits yourÂ needs.Kubernetes Overview: Orchestration, Scaling, and Self-HealingWhile Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestraâ€Šâ€”â€Šit ensures all your containers play inÂ harmony.Key Features of Kubernetes: Automates deployment, scaling, and management of containers.: Automatically adjusts the number of running containers based onÂ demand.: Restarts failed containers and replaces unhealthy ones.: Automatically assigns IP addresses and DNS names to containers.Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Hereâ€™s a simple Kubernetes deployment file:apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app        image: my-python-app:latest        ports:his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply itÂ using:ğŸ”¥ Run this: kubectl apply -f deployment.yamlHow NATS Shines in a Kubernetes EnvironmentNow, letâ€™s talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Hereâ€™s how it standsÂ out:Use Cases for NATS in KubernetesService-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetesâ€™ dynamic environment.Event-Driven Architectures: NATSâ€™s pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in realÂ time.Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.Resilience: NATSâ€™s built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.Deploying NATS on KubernetesDeploying NATS on Kubernetes is straightforward. Hereâ€™s a basic NATS deployment file:apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:        - containerPort: 4222 # The messaging highway ğŸ›£ï¸Once deployed, NATS can be used by your microservices for seamless communication.Best Practices for Containerizing MicroservicesTo wrap things up, here are some best practices for containerizing your microservices:Keep Containers Lightweight: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.: Separate the build and runtime environments to keep production imagesÂ small.Leverage Kubernetes Features: Use ConfigMaps and Secrets to manage configuration and sensitive data.: Integrate tools like Prometheus and Fluentd for monitoring andÂ logging.: Use CI/CD pipelines to automate building, testing, and deploying containers.In the next article, weâ€™ll explore â€œNATS as a Service Meshâ€Šâ€”â€ŠThe Lightweight Superhero Your Microservices Deserveâ€ and how it simplifies communication between microservices. Spoiler alert: itâ€™s like giving your microservices a supercharged walkie-talkie. StayÂ tuned!Until then, feel free to drop a comment or share your thoughts. Whatâ€™s your experience with Docker and Kubernetes? Any tips or tricks youâ€™d like to share? Letâ€™s keep the conversation going. ğŸ’¬ Whatâ€™s your #1 Kubernetes struggle? Scaling? Debugging? ShareÂ below! â¤ï¸ Happy containerizing, and stay tuned for the next chapter in the !]]></content:encoded></item><item><title>AI Coding Assistants are Not the Solution You Think</title><link>https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/</link><author>Anish Dhar</author><category>devops</category><pubDate>Thu, 13 Feb 2025 12:31:35 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernetes History Inspector, with Kakeru Ishii</title><link>http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:23:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.Do you have something cool to share? Some questions? Let us know: News of the week ]]></content:encoded></item><item><title>Sandbox environments: Creating efficient and isolated testing realms</title><link>https://www.youtube.com/watch?v=fh7-lQVmX-o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/fh7-lQVmX-o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:33 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>KitOps: AI Model Packaging Standards</title><link>https://www.youtube.com/watch?v=1TD-e_wVe4Q</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/1TD-e_wVe4Q?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Chat with us on Discord:  https://discord.gg/Tapeh8agYy

Check out our repos:
KitOps      https://github.com/jozu-ai/kitops
PyKitOps Python Library  https://github.com/jozu-ai/pykitops
KitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin]]></content:encoded></item><item><title>Training IT Teams for Multi-Cloud DevOps Environments</title><link>https://devops.com/training-it-teams-for-multi-cloud-devops-environments/</link><author>Anne Fernandez</author><category>devops</category><pubDate>Wed, 12 Feb 2025 12:11:06 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>StackGenâ€™s New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions</title><link>https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/</link><author>Tom Smith</author><category>devops</category><pubDate>Tue, 11 Feb 2025 10:17:03 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gcore Radar report reveals 56% year-on-year increase in DDoS attacks</title><link>https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/</link><author>cybernewswire</author><category>devops</category><pubDate>Tue, 11 Feb 2025 07:01:39 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Harness Merges with Traceable to Provide Integrated DevSecOps Platform</title><link>https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 19:36:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CNL: Optimizing Kyverno policy enforcement performance for large clusters</title><link>https://www.youtube.com/watch?v=DWmCAUCs3bc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/DWmCAUCs3bc?version=3" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:13:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>AWS Extends AI Agent Reach into the Realm of Testing Code</title><link>https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 13:45:46 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Five Great DevOps Job Opportunities</title><link>https://devops.com/five-great-devops-job-opportunities-125/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 09:18:21 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>