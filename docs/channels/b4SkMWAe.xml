<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>Navigating Failures in Pods With Devices</title><link>https://kubernetes.io/blog/2025/07/03/navigating-failures-in-pods-with-devices/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Thu, 3 Jul 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes is the de facto standard for container orchestration, but when it
comes to handling specialized hardware like GPUs and other accelerators, things
get a bit complicated. This blog post dives into the challenges of managing
failure modes when operating pods with devices in Kubernetes, based on insights
from Sergey Kanzhelev and Mrunal Patel's talk at KubeCon NA
2024. You can follow the links to
slides
and
recording.The AI/ML boom and its impact on KubernetesThe rise of AI/ML workloads has brought new challenges to Kubernetes. These
workloads often rely heavily on specialized hardware, and any device failure can
significantly impact performance and lead to frustrating interruptions. As
highlighted in the 2024 Llama
paper,
hardware issues, particularly GPU failures, are a major cause of disruption in
AI/ML training. You can also learn how much effort NVIDIA spends on handling
devices failures and maintenance in the KubeCon talk by Ryan Hallisey and Piotr
Prokop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA's Self-Healing
GeForce NOW
Infrastructure
(recording) as they see 19
remediation requests per 1000 nodes a day!
We also see data centers offering spot consumption models and overcommit on
power, making device failures commonplace and a part of the business model.However, Kubernetes’s view on resources is still very static. The resource is
either there or not. And if it is there, the assumption is that it will stay
there fully functional - Kubernetes lacks good support for handling full or partial
hardware failures. These long-existing assumptions combined with the overall complexity of a setup lead
to a variety of failure modes, which we discuss here.Understanding AI/ML workloadsGenerally, all AI/ML workloads require specialized hardware, have challenging
scheduling requirements, and are expensive when idle. AI/ML workloads typically
fall into two categories - training and inference. Here is an oversimplified
view of those categories’ characteristics, which are different from traditional workloads
like web services:These workloads are resource-intensive, often consuming entire
machines and running as gangs of pods. Training jobs are usually "run to
completion" - but that could be days, weeks or even months. Any failure in a
single pod can necessitate restarting the entire step across all the pods.These workloads are usually long-running or run indefinitely,
and can be small enough to consume a subset of a Node’s devices or large enough to span
multiple nodes. They often require downloading huge files with the model
weights.These workload types specifically break many past assumptions:Workload assumptions before and nowCan get a better CPU and the app will work faster.Require a  device (or ) to run.When something doesn’t work, just recreate it.Allocation or reallocation is expensive.Any node will work. No need to coordinate between Pods.Scheduled in a special way - devices often connected in a cross-node topology.Each Pod can be plug-and-play replaced if failed.Pods are a part of a larger task. Lifecycle of an entire task depends on each Pod.Container images are slim and easily available.Container images may be so big that they require special handling.Long initialization can be offset by slow rollout.Initialization may be long and should be optimized, sometimes across many Pods together.Compute nodes are commoditized and relatively inexpensive, so some idle time is acceptable.Nodes with specialized hardware can be an order of magnitude more expensive than those without, so idle time is very wasteful.The existing failure model was relying on old assumptions. It may still work for
the new workload types, but it has limited knowledge about devices and is very
expensive for them. In some cases, even prohibitively expensive. You will see
more examples later in this article.Why Kubernetes still reigns supremeThis article is not going deeper into the question: why not start fresh for
AI/ML workloads since they are so different from the traditional Kubernetes
workloads. Despite many challenges, Kubernetes remains the platform of choice
for AI/ML workloads. Its maturity, security, and rich ecosystem of tools make it
a compelling option. While alternatives exist, they often lack the years of
development and refinement that Kubernetes offers. And the Kubernetes developers
are actively addressing the gaps identified in this article and beyond.The current state of device failure handlingThis section outlines different failure modes and the best practices and DIY
(Do-It-Yourself) solutions used today. The next session will describe a roadmap
of improving things for those failure modes.Failure modes: K8s infrastructureIn order to understand the failures related to the Kubernetes infrastructure,
you need to understand how many moving parts are involved in scheduling a Pod on
the node. The sequence of events when the Pod is scheduled in the Node is as
follows: is scheduled on the Node is registered with the  via local gRPC uses  to watch for devices and updates capacity of
the node places a  on a Node based on the updated capacity asks  to  devices for a  creates a  with the allocated devices attached to itThis diagram shows some of those actors involved:As there are so many actors interconnected, every one of them and every
connection may experience interruptions. This leads to many exceptional
situations that are often considered failures, and may cause serious workload
interruptions:Pods failing admission at various stages of its lifecyclePods unable to run on perfectly fine hardwareScheduling taking unexpectedly long timeThe goal for Kubernetes is to make the interruption between these components as
reliable as possible. Kubelet already implements retries, grace periods, and
other techniques to improve it. The roadmap section goes into details on other
edge cases that the Kubernetes project tracks. However, all these improvements
only work when these best practices are followed:Configure and restart kubelet and the container runtime (such as containerd or CRI-O)
as early as possible to not interrupt the workload.Monitor device plugin health and carefully plan for upgrades.Do not overload the node with less-important workloads to prevent interruption
of device plugin and other components.Configure user pods tolerations to handle node readiness flakes.Configure and code graceful termination logic carefully to not block devices
for too long.Another class of Kubernetes infra-related issues is driver-related. With
traditional resources like CPU and memory, no compatibility checks between the
application and hardware were needed. With special devices like hardware
accelerators, there are new failure modes. Device drivers installed on the node:Be compatible with an appMust work with other drivers (like nccl,
etc.)Best practices for handling driver versions:Monitor driver installer healthPlan upgrades of infrastructure and Pods to match the versionHave canary deployments whenever possibleFollowing the best practices in this section and using device plugins and device
driver installers from trusted and reliable sources generally eliminate this
class of failures. Kubernetes is tracking work to make this space even better.Failure modes: device failedThere is very little handling of device failure in Kubernetes today. Device
plugins report the device failure only by changing the count of allocatable
devices. And Kubernetes relies on standard mechanisms like liveness probes or
container failures to allow Pods to communicate the failure condition to the
kubelet. However, Kubernetes does not correlate device failures with container
crashes and does not offer any mitigation beyond restarting the container while
being attached to the same device.This is why many plugins and DIY solutions exist to handle device failures based
on various signals.In many cases a failed device will result in unrecoverable and very expensive
nodes doing nothing. A simple DIY solution is a . The
controller could compare the device allocatable count with the capacity and if
the capacity is greater, it starts a timer. Once the timer reaches a threshold,
the health controller kills and recreates a node.There are problems with the  approach:Root cause of the device failure is typically not knownThe controller is not workload awareFailed device might not be in use and you want to keep other devices runningThe detection may be too slow as it is very genericThe node may be part of a bigger set of nodes and simply cannot be deleted in
isolation without other nodesThere are variations of the health controller solving some of the problems
above. The overall theme here though is that to best handle failed devices, you
need customized handling for the specific workload. Kubernetes doesn’t yet offer
enough abstraction to express how critical the device is for a node, for the
cluster, and for the Pod it is assigned to.Another DIY approach for device failure handling is a per-pod reaction on a
failed device. This approach is applicable for  workloads that are
implemented as Jobs.There are some problems with the  approach for Jobs:There is no well-known  condition, so this approach does not work for the
generic Pod caseError codes must be coded carefully and in some cases are hard to guarantee.Only works with Jobs with , due to the limitation of a pod
failure policy feature.So, this solution has limited applicability.A little more generic approach is to implement the Pod watcher as a DIY solution
or use some third party tools offering this functionality. The pod watcher is
most often used to handle device failures for inference workloads.Since Kubernetes just keeps a pod assigned to a device, even if the device is
reportedly unhealthy, the idea is to detect this situation with the pod watcher
and apply some remediation. It often involves obtaining device health status and
its mapping to the Pod using Pod Resources API on the node. If a device fails,
it can then delete the attached Pod as a remediation. The replica set will
handle the Pod recreation on a healthy device.The other reasons to implement this watcher:Without it, the Pod will keep being assigned to the failed device forever.There is no  for a pod with .There are no built-in controllers that delete Pods in CrashLoopBackoff.Problems with the :The signal for the pod watcher is expensive to get, and involves some
privileged actions.It is a custom solution and it assumes the importance of a device for a Pod.The pod watcher relies on external controllers to reschedule a Pod.There are more variations of DIY solutions for handling device failures or
upcoming maintenance. Overall, Kubernetes has enough extension points to
implement these solutions. However, some extension points require higher
privilege than users may be comfortable with or are too disruptive. The roadmap
section goes into more details on specific improvements in handling the device
failures.Failure modes: container code failedWhen the container code fails or something bad happens with it, like out of
memory conditions, Kubernetes knows how to handle those cases. There is either
the restart of a container, or a crash of a Pod if it has 
and scheduling it on another node. Kubernetes has limited expressiveness on what
is a failure (for example, non-zero exit code or liveness probe failure) and how
to react on such a failure (mostly either Always restart or immediately fail the
Pod).This level of expressiveness is often not enough for the complicated AI/ML
workloads. AI/ML pods are better rescheduled locally or even in-place as that
would save on image pulling time and device allocation. AI/ML pods are often
interconnected and need to be restarted together. This adds another level of
complexity and optimizing it often brings major savings in running AI/ML
workloads.There are various DIY solutions to handle Pod failures orchestration. The most
typical one is to wrap a main executable in a container by some orchestrator.
And this orchestrator will be able to restart the main executable whenever the
job needs to be restarted because some other pod has failed.Solutions like this are very fragile and elaborate. They are often worth the
money saved comparing to a regular JobSet delete/recreate cycle when used in
large training jobs. Making these solutions less fragile and more streamlined
by developing new hooks and extension points in Kubernetes will make it
easy to apply to smaller jobs, benefiting everybody.Failure modes: device degradationNot all device failures are terminal for the overall workload or batch job.
As the hardware stack gets more and more
complex, misconfiguration on one of the hardware stack layers, or driver
failures, may result in devices that are functional, but lagging on performance.
One device that is lagging behind can slow down the whole training job.We see reports of such cases more and more often. Kubernetes has no way to
express this type of failures today and since it is the newest type of failure
mode, there is not much of a best practice offered by hardware vendors for
detection and third party tooling for remediation of these situations.Typically, these failures are detected based on observed workload
characteristics. For example, the expected speed of AI/ML training steps on
particular hardware. Remediation for those issues is highly depend on a workload needs.As outlined in a section above, Kubernetes offers a lot of extension points
which are used to implement various DIY solutions. The space of AI/ML is
developing very fast, with changing requirements and usage patterns. SIG Node is
taking a measured approach of enabling more extension points to implement the
workload-specific scenarios over introduction of new semantics to support
specific scenarios. This means prioritizing making information about failures
readily available over implementing automatic remediations for those failures
that might only be suitable for a subset of workloads.This approach ensures there are no drastic changes for workload handling which
may break existing, well-oiled DIY solutions or experiences with the existing
more traditional workloads.Many error handling techniques used today work for AI/ML, but are very
expensive. SIG Node will invest in extension points to make those cheaper, with
the understanding that the price cutting for AI/ML is critical.The following is the set of specific investments we envision for various failure
modes.Roadmap for failure modes: K8s infrastructureThe area of Kubernetes infrastructure is the easiest to understand and very
important to make right for the upcoming transition from Device Plugins to DRA.
SIG Node is tracking many work items in this area, most notably the following:Basically, every interaction of Kubernetes components must be reliable via
either the kubelet improvements or the best practices in plugins development
and deployment.Roadmap for failure modes: device failedFor the device failures some patterns are already emerging in common scenarios
that Kubernetes can support. However, the very first step is to make information
about failed devices available easier. The very first step here is the work in
KEP 4680 (Add Resource Health Status to the Pod Status for
Device Plugin and DRA).Longer term ideas include to be tested:Integrate device failures into Pod Failure Policy.Node-local retry policies, enabling pod failure policies for Pods with
restartPolicy=OnFailure and possibly beyond that.Ability to  pod, including with the , so it can
get a new device allocated.Add device health to the ResourceSlice used to represent devices in DRA,
rather than simply withdrawing an unhealthy device from the ResourceSlice.Roadmap for failure modes: container code failedThe main improvements to handle container code failures for AI/ML workloads are
all targeting cheaper error handling and recovery. The cheapness is mostly
coming from reuse of pre-allocated resources as much as possible. From reusing
the Pods by restarting containers in-place, to node local restart of containers
instead of rescheduling whenever possible, to snapshotting support, and
re-scheduling prioritizing the same node to save on image pulls.Consider this scenario: A big training job needs 512 Pods to run. And one of the
pods failed. It means that all Pods need to be interrupted and synced up to
restart the failed step. The most efficient way to achieve this generally is to
reuse as many Pods as possible by restarting them in-place, while replacing the
failed pod to clear up the error from it. Like demonstrated in this picture:It is possible to implement this scenario, but all solutions implementing it are
fragile due to lack of certain extension points in Kubernetes. Adding these
extension points to implement this scenario is on the Kubernetes roadmap.Roadmap for failure modes: device degradationThere is very little done in this area - there is no clear detection signal,
very limited troubleshooting tooling, and no built-in semantics to express the
"degraded" device on Kubernetes. There has been discussion of adding data on
device performance or degradation in the ResourceSlice used by DRA to represent
devices, but it is not yet clearly defined. There are also projects like
node-healthcheck-operator
that can be used for some scenarios.We expect developments in this area from hardware vendors and cloud providers, and we expect to see mostly DIY
solutions in the near future. As more users get exposed to AI/ML workloads, this
is a space needing feedback on patterns used here.The Kubernetes community encourages feedback and participation in shaping the
future of device failure handling. Join SIG Node and contribute to the ongoing
discussions!This blog post provides a high-level overview of the challenges and future
directions for device failure management in Kubernetes. By addressing these
issues, Kubernetes can solidify its position as the leading platform for AI/ML
workloads, ensuring resilience and reliability for applications that depend on
specialized hardware.]]></content:encoded></item><item><title>Lightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kuberne... Asami Okina</title><link>https://www.youtube.com/watch?v=8pVt9dhHEVc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/8pVt9dhHEVc?version=3" length="" type=""/><pubDate>Wed, 2 Jul 2025 23:01:45 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Lightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kubernetes Jobs - Asami Okina, Craftsman Software, Inc.

While typical web applications do not require large amounts of resources constantly, there are cases where specific processes consume significant CPU and memory.

In this session, we will introduce an architecture that offloads such resource-intensive processes to Kubernetes Jobs.

We will explain specific methods for Job management, how to integrate web applications (Next.js, @kubernetes/client-node) with the Kubernetes API, methods for data integration between Jobs and web applications, and real-time tracking of Job progress in the UI, all while sharing practical examples. Furthermore, we will provide a detailed introduction to a pattern where Kubernetes Job definitions generated from applications are managed using ConfigMaps, enabling quick configuration switching between environments, and offer hints to optimize your applications in terms of cost, performance, and management.]]></content:encoded></item><item><title>Bitrise to Invest $3M in DevOps Cloud for Mobile Apps in EU</title><link>https://devops.com/bitrise-to-invest-3m-in-devops-cloud-for-mobile-apps-in-eu/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=bitrise-to-invest-3m-in-devops-cloud-for-mobile-apps-in-eu</link><author>Mike Vizard</author><category>devops</category><pubDate>Wed, 2 Jul 2025 19:01:59 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Amazon Nova Canvas update: Virtual try-on and style options now available</title><link>https://aws.amazon.com/blogs/aws/amazon-nova-canvas-update-virtual-try-on-and-style-options-now-available/</link><author>Matheus Guimaraes</author><category>devops</category><pubDate>Wed, 2 Jul 2025 18:41:43 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Have you ever wished you could quickly visualize how a new outfit might look on you before making a purchase? Or how a piece of furniture would look in your living room? Today, we’re excited to introduce a new virtual try-on capability in Amazon Nova Canvas that makes this possible. In addition, we are adding eight new style options for improved style consistency for text-to-image based style prompting. These features expand Nova Canvas AI-powered image generation capabilities making it easier than ever to create realistic product visualizations and stylized images that can enhance the experience of your customers.Let’s take a quick look at how you can start using these today. The first thing is to make sure that you have access to the Nova Canvas model through the usual means. Head to the Amazon Bedrock console, choose  and enable Amazon Nova Canvas for your account making sure that you select the appropriate regions for your workloads. If you already have access and have been using Nova Canvas, you can start using the new features immediately as they’re automatically available to you.The first exciting new feature is . With this, you can upload two pictures and ask Amazon Nova Canvas to put them together with realistic results. These could be pictures of apparel, accessories, home furnishings, and any other products including clothing. For example, you can provide the picture of a human as the source image and the picture of a garment as the reference image, and Amazon Nova Canvas will create a new image with that same person wearing the garment. Let’s try this out!My starting point is to select two images. I picked one of myself in a pose that I think would work well for a clothes swap and a picture of an AWS-branded hoodie.Note that Nova Canvas accepts images containing a maximum of 4.1M pixels – the equivalent of 2,048 x 2,048 – so be sure to scale your images to fit these constraints if necessary. Also, if you’d like to run the Python code featured in this article, ensure you have Python 3.9 or later installed as well as the Python packages boto3 and pillow.To apply the hoodie to my photo, I use the Amazon Bedrock Runtime invoke API. You can find full details on the request and response structures for this API in the Amazon Nova User Guide. The code is straightforward, requiring only a few inference parameters. I use the new  of . I then specify the desired settings, including both the source image and reference image, using the  object to set a few required parameters. Note that both images must be converted to Base64 strings.import base64


def load_image_as_base64(image_path): 
   """Helper function for preparing image data."""
   with open(image_path, "rb") as image_file:
      return base64.b64encode(image_file.read()).decode("utf-8")


inference_params = {
   "taskType": "VIRTUAL_TRY_ON",
   "virtualTryOnParams": {
      "sourceImage": load_image_as_base64("person.png"),
      "referenceImage": load_image_as_base64("aws-hoodie.jpg"),
      "maskType": "GARMENT",
      "garmentBasedMask": {"garmentClass": "UPPER_BODY"}
   }
}Nova Canvas uses masking to manipulate images. This is a technique that allows AI image generation to focus on specific areas or regions of an image while preserving others, similar to using painter’s tape to protect areas you don’t want to paint.You can use three different masking modes, which you can choose by setting  to the correct value. In this case, I’m using , which requires me to specify which part of the body I want to be masked. I’m using  , but you can use others such as , , or  if you want to specifically target the feet. Refer to the documentation for a full list of options.I then call the invoke API, passing in these inference arguments and saving the generated image to disk.# Note: The inference_params variable from above is referenced below.

import base64
import io
import json

import boto3
from PIL import Image

# Create the Bedrock Runtime client.
bedrock = boto3.client(service_name="bedrock-runtime", region_name="us-east-1")

# Prepare the invocation payload.
body_json = json.dumps(inference_params, indent=2)

# Invoke Nova Canvas.
response = bedrock.invoke_model(
   body=body_json,
   modelId="amazon.nova-canvas-v1:0",
   accept="application/json",
   contentType="application/json"
)

# Extract the images from the response.
response_body_json = json.loads(response.get("body").read())
images = response_body_json.get("images", [])

# Check for errors.
if response_body_json.get("error"):
   print(response_body_json.get("error"))

# Decode each image from Base64 and save as a PNG file.
for index, image_base64 in enumerate(images):
   image_bytes = base64.b64decode(image_base64)
   image_buffer = io.BytesIO(image_bytes)
   image = Image.open(image_buffer)
   image.save(f"image_{index}.png")
I get a very exciting result!And just like that, I’m the proud wearer of an AWS-branded hoodie!In addition to the  mask type, you can also use the  or  masks. With , you also provide the source and reference images, however, you provide a natural language prompt to specify which part of the source image you’d like to be replaced. This is similar to how the  and  tasks work in Nova Canvas. If you want to use your own image mask, then you choose the  mask type and provide a black-and-white image to be used as mask, where black indicates the pixels that you want to be replaced on the source image, and white the ones you want to preserve.This capability is specifically useful for retailers. They can use it to help their customers make better purchasing decisions by seeing how products look before buying. I’ve always wondered what I would look like as an anime superhero. Previously, I could use Nova Canvas to manipulate an image of myself, but I would have to rely on my good prompt engineering skills to get it right. Now, Nova Canvas comes with pre-trained styles that you can apply to your images to get high-quality results that follow the artistic style of your choice. There are eight available styles including 3D animated family film, design sketch, flat vector illustration, graphic novel, maximalism, midcentury retro, photorealism, and soft digital painting.Applying them is as straightforward as passing in an extra parameter to the Nova Canvas API. Let’s try an example.I want to generate an image of an AWS superhero using the 3D animated family film style. To do this, I specify a  of  and a  object containing two parameters:  and . The  parameter contains the prompt describing the image I want to create which in this case is “a superhero in a yellow outfit with a big AWS logo and a cape.” The  parameter specifies one of the predefined style values. I’m using "3D_ANIMATED_FAMILY_FILM" here, but you can find the full list in the Nova Canvas User Guide.inference_params = {
   "taskType": "TEXT_IMAGE",
   "textToImageParams": {
      "text": "a superhero in a yellow outfit with a big AWS logo and a cape.",
      "style": "3D_ANIMATED_FAMILY_FILM",
   },
   "imageGenerationConfig": {
      "width": 1280,
      "height": 720,
      "seed": 321
   }
}Then, I call the invoke API just as I did in the previous example. (The code has been omitted here for brevity.) And the result? Well, I’ll let you judge for yourself, but I have to say I’m quite pleased with the AWS superhero wearing my favorite color following the 3D animated family film style exactly as I envisioned.What’s really cool is that I can keep my code and prompt exactly the same and only change the value of the style attribute to generate an image in a completely different style. Let’s try this out. I set  to .inference_params = { 
   "taskType": "TEXT_IMAGE", 
   "textToImageParams": { 
      "text": "a superhero in a yellow outfit with a big AWS logo and a cape.",
      "style": "PHOTOREALISM",
   },
   "imageGenerationConfig": {
      "width": 1280,
      "height": 720,
      "seed": 7
   }
}And the result is impressive! A photorealistic superhero exactly as I described, which is a far departure from the previous generated cartoon and all it took was changing one line of code. Availability – Virtual try-on and style options are available in Amazon Nova Canvas in the US East (N. Virginia), Asia Pacific (Tokyo), and Europe (Ireland). Current users of Amazon Nova Canvas can immediately use these capabilities without migrating to a new model.For a preview of virtual try-on of garments, you can visit nova.amazon.com where you can upload an image of a person and a garment to visualize different clothing combinations.Matheus Guimaraes | @codingmatheus]]></content:encoded></item><item><title>Using Gordon to Containerize Your Apps and Work with Containers</title><link>https://www.docker.com/blog/containerize-your-apps-with-ask-gordon/</link><author>Steve Buchanan</author><category>docker</category><category>devops</category><pubDate>Wed, 2 Jul 2025 18:34:41 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[These days, almost every tech company is looking for ways to integrate AI into their apps and workflows, and Docker is no exception. They’ve been rolling out some impressive AI capabilities across their products. This is my first post as a Docker Captain and in this post, I want to shine a spotlight on a feature that hasn’t gotten nearly enough attention in my opinion:  (also known as Docker AI), which is built into Docker Desktop and CLI.Gordon is really helpful when it comes to containerizing applications. Not only does it help you understand how to package your app as a container, but it also reduces the overhead of figuring out dependencies, runtime configs, and other pieces that add to a developer’s daily cognitive load. The best part? Gordon doesn’t just guide you with responses; it can also generate or updatethe necessary files for you.The Problem: Containerizing apps and optimizing containers isn’t always easyContainerizing apps can range from super simple to a bit tricky, depending on what you’re working with. If your app has a  like Node.js, Python, or .NET Core, with clearly defined dependencies and , it will be straightforward.A basic Dockerfile will usually get you up and running without much effort. But once you start adding more complexity, like a backend, frontend, database, and caching layer, you now have the need for a multi-container app. At this point, you might be dealing with additional Dockerfile configurations and potentially a Docker Compose setup. That’s where things can start to be challenging to get going.This is where Gordon shines. It’s helpful in containerizing apps and can even handle multi-service container app setups, guiding you through what’s needed and even generating the supporting config files, such as Dockerfiles and docker-compose, to get you going.Optimizing containers can be a headache tooBeyond just containerizing, there’s also the need to  for performance, security, and image size. And let’s face it, optimizing can be tedious. You need to know what base images to use, how to slim them down, how to avoid unnecessary layers, and more.Gordon can help here too. It provides optimization suggestions, shows you how to apply best practices like multi-stage builds or removing dev dependencies, and helps you create leaner, more secure images.Why not just use general-purpose Generative AI?Sure, general-purpose AI tools like ChatGPT, Claude, Gemini, etc. are great and I use them regularly. But when it comes to containers, they can  needed for accurate and efficient help. Gordon, on the other hand, is . It has access to Docker’s ecosystem and has been trained on Docker documentation, best practices, and the nuances of Docker tooling. That means its recommendations are more likely to be precise and aligned with the latest standards.Gordon can help with containerizing applications, optimizing your containers and more. Gordon is still a Beta feature. To start using Gordon, you need Docker Desktop version 4.38 or later. Gordon is powered by Large Language Models (LLMs), and it goes beyond prompt and response: it can perform certain tasks for you as an AI agent. Gordon can have access to your local files and local images when you give it permission. It will prompt you for access if needed for a task.Please note, the examples I will show in this post are based on a single working session. Now, let’s dive in and start to explore Gordon.Enabling Gordon / Docker AIIn order to turn Gordon on, go to  check the  box as shown in the following screenshot. Figure 1: screenshot of where to enable Docker AI in beta featuresAccept the terms. The AI in Docker Desktop is in two forms. The first one is through the Docker Desktop UI and is known as Gordon. The second option is Docker AI. Docker AI is accessed through the Docker CLI. The way you activate it is by typing Docker AI in the CLI. I will demonstrate this later on in this blog post.  Figure 2: screenshot of Docker AI terms acceptance dialog boxExploring Gordon in Docker DesktopNow Gordon will appear in your Docker Desktop UI. Here you can prompt it just like any Generative AI tool. Gordon will also have examples that you can use to get started working with it.You can access Gordon throughout Docker Desktop by clicking on the AI icon as shown in the following screenshot.Figure 3: screenshot of Docker Desktop interface showing the AI icon for GordonWhen you click on the AI icon a Gordon prompt box appears along with suggested prompts as shown in the following screenshot. The suggestions will change based on the object the AI is next to, and are context-aware.Figure 4: Screenshot showing Gordon’s suggestion prompt box in Docker Desktop UIHere is another example of Docker AI suggestions being context-aware based on what area of Docker Desktop you are in.  Figure 5: Screenshot showing Docker AI context- specific suggestionsAnother common use case for Gordon is listing local images and using AI to work with them. You can see this in the following set of screenshots. Notice that Gordon will prompt you for permission before showing your local images.Figure 6: Screenshot showing Gordon referencing local imagesYou can also prompt Gordon to take action. As shown in the following screenshot, I asked Gordon to run one of my images.Figure 7: Screenshot showing Gordon promptsIf it can’t perform the action, it will attempt to help you. Figure 8: Screenshot showing Gordon prompt response to failed requestAnother cool use of Gordon is to explain a container image to you. When you ask this, Gordon will ask you to select the directory where the Dockerfile is and permission to access it as shown in the following screenshot.Figure 9: Screenshot showing Gordon’s request for particular directory accessAfter you give it access to the directory where the Dockerfile is, it will then breakdown what’s in the Dockerfile. Figure 10: Screenshot showing Gordon’s response to explaining a DockerfileAs shown in the following screenshot, I followed up with a prompt asking Gordon to display what’s in the Dockerfile. It did a good job of explaining its contents, as shown in the following screenshot.Figure 11: Screenshot showing Gordon’s response regarding Dockerfile contentsExploring Gordon in the Docker Desktop CLILet’s take a quick tour through Gordon in the CLI. Gordon is referred to as Docker AI in the CLI. To work with Docker AI, you need to launch the Docker CLI as shown in the following screenshot. Figure 12: Screenshot showing how to launch Docker AI from the CLIOnce in the CLI you can type “docker ai” and it will bring you into the chat experience so you can prompt Gordon. In my example, I asked Gordon about one of my local images. You can see that it asked me for permission. Figure 13: Screenshot showing Docker CLI request for accessNext, I asked Docker AI to list all of my local images as shown in the following screenshot. Figure 14: Screenshot showing Docker CLI response to display local imagesI then tested pulling an image using Docker AI. As you can see in the following screenshot, Gordon pulled a nodeJS image for me!Figure 15: Screenshot showing Docker CLI pulling nodeJS imageContainerizing an application with GordonNow let’s explore the experience of containerizing an application using Gordon.I started by clicking on the example for containerizing an application. Gordon then prompted me for the directory where my application code is. Figure 16: Screenshot showing where to enable access to directory for containerizing an applicationI pointed it to my apps directory and gave it permission. It then started to analyze and containerize my app. It picked up the language and started to read through my app’s README file.Figure 17: Screenshot showing Gordon starting to analyze and containerize appYou can see it understand the app was written in JavaScript and worked through the packages and dependencies.Figure 18: Screenshot showing final steps of Gordon processingGordon understands that my app has a backend, frontend, and a database, knowing from this that I would need a Docker compose file.Figure 19: Screenshot showing successful completion of steps to complete the DockerfilesFrom the following screenshot you can see the Docker related files needed for my app. Gordon created all of these.Figure 20: Screenshot showing files produced from GordonGordon created the Dockerfile (on the left) and a Compose yaml file (on the right) even picking up that I needed a Postgres DB for this application.Figure 21: Screenshot showing Dockerfile and Compose yaml file produced from GordonI then took it a step further and asked Gordon to build and run the container for my application using the prompt “Can you build and run this application with compose?” It created the Docker Compose file, built the images, and ran the containers!Figure 22: Screenshot showing completed containers from GordonI hope you picked up some useful insights about Docker and discovered one of its lesser-known AI features in Docker Desktop. We explored what Gordon is, how it compares to general-purpose generative AI tools like ChatGPT, Claude, and Gemini, and walked through use cases such as containerizing an application and working with local images. We also touched on how Gordon can support developers and IT professionals who work with containers. If you haven’t already, I encourage you to enable Gordon and take it for a test run. Thanks for reading and stay tuned for more blog posts coming soon.]]></content:encoded></item><item><title>Docker Container Exits Immediately? Here’s How to Fix ENTRYPOINT &amp; CMD Issues Fast</title><link>https://blog.devops.dev/docker-container-exits-immediately-heres-how-to-fix-entrypoint-cmd-issues-fast-24903c44530a?source=rss----33f8b2d9a328---4</link><author>Ali Hamza</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:26:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Fix Docker containers that exit immediately by understanding and correcting ENTRYPOINT and CMD issues with real examples and step-by-step solutions.When you’re working with Docker, few things are more frustrating than building an image, running a container, and — boom — it exits immediately.No error messages. No clear reason. Just silence.This is a , especially for those just starting out. The culprit? It’s often related to how you define  and  in your Dockerfile.In this guide, I’ll explain why your Docker container exits immediately, how to , and how to properly configure ENTRYPOINT and CMD so your container behaves as expected.Understanding the Problem: Why Docker Containers Exit ImmediatelyWhen a Docker container exits right after starting, it usually means the container ran the default command and completed. Unlike virtual machines, Docker containers are designed to run a single process — when that process ends, the container exits too.How to Check What HappenedYou can use the following command to check the container’s status:This shows all containers (even the stopped ones). Look at the  column to see if it exited immediately.docker logs <container_id>This often reveals errors or termination messages from your containerized app.ENTRYPOINT vs CMD: What’s the Difference?Both ENTRYPOINT and CMD are Dockerfile instructions used to define  when a container starts — but they work differently.ENTRYPOINT ["python3"]CMD ["app.py"]If you forget one or combine them wrong, the container may .Common ENTRYPOINT & CMD Mistakes That Cause Immediate ExitHere are frequent issues that result in containers exiting:CMD runs a short-lived command, like echo "Done" — it ends, so the container exits.Missing or misused ENTRYPOINT/CMD — container has no valid process to run. (string vs exec format)., so it fails silently.Step-by-Step Guide to Debug the Exit Issuedocker logs <container_id>You might see errors like:bash: ./entrypoint.sh: Permission denied2. Inspect the DockerfileLook at the CMD and ENTRYPOINT lines. Are they defined properly?3. Run the Container InteractivelyStart with a shell to look inside:docker run -it <image_name> /bin/bashThen try running your script or command manually.How to Fix ENTRYPOINT and CMD Issues (with Examples)Fix 1: Use CMD for Simple AppsFROM python:3.10-slimCOPY app.py .CMD ["python3", "app.py"]Fix 2: Use ENTRYPOINT with CMD for Flexible ArgumentsFROM ubuntuCOPY entrypoint.sh /entrypoint.shRUN chmod +x /entrypoint.shENTRYPOINT ["/entrypoint.sh"]Fix 3: Ensure Script is ExecutableFix 4: Avoid Shell Form (Unless Needed)CMD ["nginx", "-g", "daemon off;"]CMD nginx -g "daemon off;"Shell form can break argument parsing and signals.Working Dockerfile ExamplesFROM python:3.10-slimCOPY app.py .CMD ["python3", "app.py"]FROM ubuntuCOPY entrypoint.sh /entrypoint.shRUN chmod +x /entrypoint.shENTRYPOINT ["/entrypoint.sh"]If entrypoint.sh contains something like:#!/bin/bashecho "Starting my app..."Your container will now stay alive for 5 minutes (good for testing).Pro Tips to Avoid Exit Issues in the FutureAlways test your image interactively:docker run -it <image> bashStick to  (["cmd", "arg"]) for ENTRYPOINT and CMD.Check logs immediately using:docker logs <container_id>Ensure the main process runs , or your container will exit.Add health checks for production deployments to monitor container status.A Docker container that  is almost always the result of an incorrect or short-lived command — often tied to  and .Understanding how Docker runs containers,Debugging with logs and interactive shells,And configuring your Dockerfile properly,…you can  these issues and get your container running the way you want.Have you faced a similar problem? Share your debugging tips in the comments below — let’s help each other grow!]]></content:encoded></item><item><title>10 Steps To Ensure — Zero Trust Architecture in Kubernetes</title><link>https://blog.devops.dev/10-steps-to-ensure-zero-trust-architecture-in-kubernetes-615ea58146de?source=rss----33f8b2d9a328---4</link><author>Devops Diaries</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:25:58 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Implementing Zero Trust Architecture (ZTA) in Kubernetes involves designing security with the principle: “Never trust, always verify” —…]]></content:encoded></item><item><title>Modern Observability in .NET: OpenTelemetry &amp; Grafana</title><link>https://blog.devops.dev/modern-observability-in-net-opentelemetry-grafana-4c08b9d74240?source=rss----33f8b2d9a328---4</link><author>Adem KORKMAZ</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:25:53 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bash+Telegram : Simple Server Resource Alerting System</title><link>https://blog.devops.dev/bash-telegram-simple-server-resource-alerting-system-833fcb534a33?source=rss----33f8b2d9a328---4</link><author>bektiaw</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:25:51 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>11 Scripts to Transform Server Metrics into Insights</title><link>https://blog.devops.dev/11-scripts-to-transform-server-metrics-into-insights-e461d72276eb?source=rss----33f8b2d9a328---4</link><author>Obafemi</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:25:04 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Docker 101: The Complete Cheat Sheet for Devs &amp; Ops</title><link>https://blog.devops.dev/docker-101-the-complete-cheat-sheet-for-devs-ops-01ba7b13193a?source=rss----33f8b2d9a328---4</link><author>Ashish Singh</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:24:56 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OSI Model Layer — Neworking Basic &amp; Easiest Concept</title><link>https://blog.devops.dev/osi-model-layer-neworking-basic-easiest-concept-f3cb2f15cd8f?source=rss----33f8b2d9a328---4</link><author>Kamalpreet KAUR</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:24:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Let’s Deep Dive in OSI (Open System Interconnection Model)The Open System Interconnection model is one of the crucial concepts in networking. It’s always made us stuck, and it’s one of the repetitive concepts. The main aim of the OSI Model is to understand the network flow. The basic strategy is to fully understand how the data streams flow up and down within the model -> In other words, to understand the data flow that passes from one device to another.Let’s Begin with the FoundationThe OSI model was developed by the ISO (International Organization for Standardization) in the late 1970s and early 1980s. OSI is a as a standard or protocol itselfit’s the reference to think about and describe the interactions between different layers of a computer network.Imagine you are at home, and each room represents a layer (living room, study room …). These layers are interconnected with each other, which helps us to provide direction to move from one room to another. OSI reference model helps us to indicate the shared information between each other.The OSI model is like a blueprint of all layers that helps to understand each layer and its specifications (how different layers work with each other)Every Layer Functionalities :Functionality Explaination :Application Layer — User Interface [GUI] Activities Presentation Layer — HTML pages / Readable webpages Session Layer — Establish Communication Sessions Transport layer — Data must arrive in order along with acknowledgement Network Layer — Establish communication with distant devicesData Link Layer — Data must be delivered to its correct destinationPhysical Layer — Physical connectivity between devices involving cables and wires.Difference between OSI Model and TCP/ IP Model]]></content:encoded></item><item><title>How to Successfully Deploy a Three-Tier application on AWS EKS using Terraform</title><link>https://blog.devops.dev/how-to-successfully-deploy-a-three-tier-application-on-aws-eks-using-terraform-30fe80e82f88?source=rss----33f8b2d9a328---4</link><author>Pravesh Sudha</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:24:27 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Quick guide to provisioning and deploying a Three-Tier application on AWS EKS and TerraformWelcome to the world of cloud computing and automation. In this blog, we’re going to walk through an exciting real-world project — deploying a three-tier Todo List application on Amazon EKS (Elastic Kubernetes Service) using .This project is perfect if you’re looking to get hands-on experience with:Provisioning infrastructure using Terraform to containerize servicesDeploying applications on AWS using EKS, ECR, IAM, and moreWe’ll break it down step-by-step — from writing Terraform code to spinning up your Kubernetes cluster, containerizing the frontend, backend, and MongoDB services, and deploying everything seamlessly.Whether you’re new to DevOps or brushing up on your cloud skills, this guide will help you understand how everything connects in a modern microservices-based deployment.So without further ado, let’s get started and bring our infrastructure to life! 🌐🛠️🔧 Prerequisites: What You’ll Need Before We StartBefore we dive into the fun part — building and deploying — let’s quickly make sure your system is ready for action. Here’s what you’ll need:✅ If you don’t already have one, head over to aws.amazon.com and sign up. We’ll be using AWS services like EKS (Elastic Kubernetes Service), ECR (Elastic Container Registry), and IAM (Identity and Access Management), so having an account is essential.✅ We’ll use Docker to containerise the three components of our app: the frontend, backend, and MongoDB database. You can download Docker Desktop from the official Docker website and install it like any other app.✅ Terraform will be our tool of choice for provisioning the infrastructure on AWS. You can download Terraform from terraform.io. Just install it — no need to configure anything yet.That’s it! Once you have these basics set up, you’re good to go. Let’s start building!🔐 Step 1: Set Up AWS CLI and IAM UserBefore Terraform can talk to AWS and spin up resources, we need to set up the  and create an  with the right permissions. Let’s walk through it step-by-step. to your AWS account as the root user (the one you used to sign up).In the AWS Management Console, go to  and click on .Give the user a name — something like three-tier-user works great — and click .On the  page, attach the policy named .: We’re giving full admin access here just to avoid permission issues during learning and experimentation. Never use this approach in productionPrinciple of Least PrivilegeClick  and then . You’re done with the IAM part!📦 Install AWS CLI (Ubuntu/Linux)If you’re using , you can install the AWS CLI by running these commands in your terminal:curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"unzip awscliv2.zipIf you’re using a different operating system (like macOS or Windows), just head over to the official install guide here:👉 AWS CLI Installation Guide🔑 Generate Access Keys & Configure AWS CLIGo back to the  and click on your new user (three-tier-user).Under the  tab, click on .Choose Command Line Interface (CLI) as the use case, agree to the terms, and proceed.Once the keys are generated, copy the Access Key ID and Secret Access Key (you’ll need them right away!).Now, go to your terminal and configure the AWS CLI:It will prompt you to enter:: You can use us-east-1 for this demo: Enter jsonThat’s it! Your AWS CLI is now set up and ready to communicate with your AWS account 🚀🛠️ Step 2: Install Terraform and Set Up Remote BackendNow that our AWS CLI is ready and configured, let’s install , our Infrastructure as Code (IaC) tool of choice for this project. We’ll also set up a secure and scalable way to store our Terraform state using an .📥 Installing Terraform on Ubuntu (amd64)If you’re using Ubuntu on an amd64 system, follow these commands to install Terraform:sudo apt-get update && sudo apt-get install -y gnupg software-properties-commonecho "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \https://apt.releases.hashicorp.com $(grep -oP '(?<=UBUNTU_CODENAME=).*' /etc/os-release || lsb_release -cs) main" | \sudo tee /etc/apt/sources.list.d/hashicorp.listsudo apt updatesudo apt-get install terraform✅ After this, you can verify the installation with:🖥️ If you’re on a different operating system or architecture, follow the official installation guide here:👉 Terraform Install Guide🔐 AWS CLI + Terraform: Working TogetherSince we’ve already configured the AWS CLI, Terraform will automatically use the credentials (access key & secret key) stored by aws configure. This means you’re ready to provision AWS resources securely and seamlessly.☁️ Best Practice: Use Remote Backend for Terraform StateTerraform tracks the state of your infrastructure in a file called terraform.tfstate. By default, it’s stored locally, but that’s risky and not scalable. So, we’ll follow best practices and store this file remotely in an .Here’s how to create an S3 bucket to act as your Terraform :🪣 Create an S3 Bucket for State Storageaws s3api create-bucket \  --bucket pravesh-terra-state-bucket \📜 Enable Versioning for State Historyaws s3api put-bucket-versioning \  --bucket pravesh-terra-state-bucket \  --versioning-configuration Status=Enabled🔐 Enable Default Encryptionaws s3api put-bucket-encryption \  --bucket pravesh-terra-state-bucket \  --server-side-encryption-configuration '{    "Rules": [{      "ApplyServerSideEncryptionByDefault": {        "SSEAlgorithm": "AES256"    }]And that’s it! You now have a secure, versioned, and encrypted S3 bucket ready to store your Terraform state files — a key step toward building a production-grade infrastructure.📦 Step 3: Clone the Project and Provision Infrastructure with TerraformWith all the groundwork done — AWS CLI set up, Terraform installed, and the backend ready — it’s time to move on to the actual project!The codebase for our  is available on my GitHub repository:To get started, open your terminal and run the following commands:git clone https://github.com/Pravesh-Sudha/3-tier-app-Deploymentcd 3-tier-app-Deployment/Inside the cloned repo, you’ll find a folder named terra-config/. That’s where all the Terraform magic happens. Navigate into that directory:Now initialize the Terraform backend (which we configured to use your S3 bucket earlier):This will configure Terraform to use the remote backend for storing the state file. If your bucket name is different from mine (pravesh-terra-state-bucket), make sure to update the name in backend.tf.📁 Understanding the Terraform Code StructureInstead of dumping everything into a single main.tf file, I’ve broken the configuration into logical modules for clarity and scalability. Here’s a quick overview:provider.tf: Specifies the cloud provider. In our case, it’s AWS (no surprise there!).backend.tf: Configures Terraform to store state remotely in our S3 bucket.ecr.tf: Creates two public repositories in ECR: 3-tier-frontend and 3-tier-backend for storing Docker images.vpc.tf: Fetches the default VPC and subnet details.role.tf: Defines IAM roles:— One for the EKS cluster (includes AmazonEKSClusterPolicy)— One for the Node Group (includes policies like AmazonEKSWorkerNodePolicy, AmazonEC2ContainerRegistryReadOnly, and AmazonEKS_CNI_Policy)eks.tf: Provisions the EKS cluster named Three-tier-cloud.node_group.tf: Creates the worker node group for the cluster with one t2.medium EC2 instance.⏳ Apply the Terraform ConfigurationNow we’re ready to provision the infrastructure! Run the following command:terraform apply --auto-approve⏱️ This might take , especially since provisioning EKS clusters and node groups can take some time. Be patient — AWS is building your cloud infrastructure behind the scenes.🐳 Push Docker Images to ECROnce the infrastructure is up, it’s time to push our Docker images for the frontend and backend to AWS ECR.Go to your AWS Console > ECR > RepositoriesClick on the 3-tier-frontend repositoryClick on  — AWS will show you four CLI commandsNow, go to the frontend/ folder in your project directory:Run each of the four commands one by one to build the image and push it to ECR.Repeat the same steps for the 3-tier-backend repository:Go back to Select 3-tier-backend and click Navigate to the backend directory:Run the ECR commands provided to push the backend Docker image.🎉 Once done, your container images will be hosted in your private AWS ECR repositories — ready to be deployed to your EKS cluster!🌐 Step 4: Deploy to EKS with kubectl and Set Up Ingress via ALBNow that your EKS cluster and ECR repositories are ready, it’s time to interact with the cluster, deploy your workloads, and expose your application to the internet. We’ll use  for that — the command-line tool to manage Kubernetes clusters.If you’re using , run the following to install kubectl:curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl  chmod +x ./kubectl  sudo mv ./kubectl /usr/local/bin  kubectl version --short --clientIf you’re using a different OS/architecture, install it using the official instructions:👉 kubectl Install Guide🔧 Connect kubectl to Your EKS ClusterNow configure kubectl to use your EKS cluster:aws eks update-kubeconfig --region us-east-1 --name Three-tier-cloudThis updates your ~/.kube/config file so that you can interact with your new EKS cluster using kubectl.📁 Update Kubernetes ManifestsInside the repo directory 3-tier-app-Deployment/k8s_manifests/, you’ll find the Kubernetes manifests for deploying the , , and  services.Before applying them, update the image URIs in both deployment files with the correct values from ECR.🔄 Update backend_deployment.yml:spec:  containers:    image: <YOUR_IMAGE_URI>Replace <YOUR_IMAGE_URI> with the full image URL from your  ECR repo (latest tag).🔄 Update frontend_deployment.yml:Do the same in the frontend manifest with the image URI from the  ECR repo.🧱 Create a Namespace for the AppLet’s keep things clean by isolating our app into a dedicated Kubernetes namespace:kubectl create namespace workshopkubectl config set-context --current --namespace workshop🚀 Deploy the App ComponentsApply the deployment and service files for each component:kubectl apply -f frontend-deployment.yaml -f frontend-service.yamlkubectl apply -f backend-deployment.yaml -f backend-service.yaml# Deploy MongoDBcd mongo/At this point, your services are up and running within the cluster — but we still need a way to expose them to the outside world.🌍 Set Up Application Load Balancer (ALB) and IngressTo route external traffic into your Kubernetes services, we’ll use an AWS Application Load Balancer along with an .📜 Create an IAM Policy for the Load BalancerThe IAM policy json is present inside the kubernetes manifests dir:Create the IAM policy in AWS:aws iam create-policy \  --policy-name AWSLoadBalancerControllerIAMPolicy \  --policy-document file://iam_policy.json🔒 Associate OIDC Provider with EKSTo enable IAM roles for Kubernetes service accounts, associate an OIDC provider with your EKS cluster.curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp  sudo mv /tmp/eksctl /usr/local/bin  Then associate the OIDC provider:eksctl utils associate-iam-oidc-provider \  --region=us-east-1 \  --cluster=Three-tier-cloud \  --approve🔗 Create a Service Account for the Load BalancerReplace <Your-Account-Number> with your actual AWS account ID and run:eksctl create iamserviceaccount \  --cluster=Three-tier-cloud \  --namespace=kube-system \  --name=aws-load-balancer-controller \  --role-name AmazonEKSLoadBalancerControllerRole \  --attach-policy-arn=arn:aws:iam::<Your-Account-Number>:policy/AWSLoadBalancerControllerIAMPolicy \  --region=us-east-1🧰 Install Helm and Deploy the Load Balancer ControllerWe’ll use Helm to install the AWS Load Balancer Controller:sudo snap install helm --classichelm install aws-load-balancer-controller eks/aws-load-balancer-controller \  -n kube-system \  --set clusterName=Three-tier-cloud \  --set serviceAccount.create=false \  --set serviceAccount.name=aws-load-balancer-controllerkubectl get deployment -n kube-system aws-load-balancer-controller🛣️ Apply Ingress ConfigurationNow go back to the k8s_manifests/ directory and apply the ingress resource:kubectl apply -f full_stack_lb.yamlWait for 5–7 minutes to allow the ingress and ALB to be fully provisioned.🌐 Access Your Applicationkubectl get ing -n workshopYou’ll see an  field in the output. Copy that URL, paste it in your browser, and voilà 🎉 — your three-tier application is live on AWS!🧹 Step 5: Clean Up AWS ResourcesCongratulations on successfully deploying your three-tier application on AWS EKS using Terraform! 🎉Before we wrap things up, it’s important to  the resources we created — to avoid any unexpected AWS charges.🗑️ Delete Docker Images from ECRHead over to the  in the AWS Console.Under , select both three-tier-backend and three-tier-frontend.Delete the images from each repository.💣 Destroy Infrastructure with TerraformNow let’s destroy the entire infrastructure from your terminal. Navigate to the terra-config/ directory and run:terraform destroy --auto-approveTerraform will tear down the EKS cluster, node group, IAM roles, VPC config, ECR repositories, and more.🧽 Delete Terraform State File and S3 BucketAfter destroying your resources, don’t forget to remove the Terraform state file and the bucket itself:aws s3 rm s3://pravesh-terra-state-bucket/eks/terraform.tfstateThen go to the , empty the bucket manually (if needed), and delete the bucket to finish the cleanup process.⚠️ Make sure to delete the bucket, otherwise it will incur unwanted charges.✅ Conclusion: What You’ve LearnedIn this project, you’ve gone through the complete lifecycle of deploying a real-world  using modern DevOps tools and cloud infrastructure:You learned how to use  to provision infrastructure as code.You created and managed AWS resources like , , , and .You containerized applications and deployed them with .You exposed your app to the internet using an Application Load Balancer and .And finally, you followed best practices like remote state management and safe resource cleanup.This project isn’t just a demo — it’s a  you can build on for production-grade cloud-native applications.]]></content:encoded></item><item><title>Installing PHP 8 with phpenv — The Hard Way (But Right)</title><link>https://blog.devops.dev/installing-php-8-with-phpenv-the-hard-way-but-right-920a0a8ea1e5?source=rss----33f8b2d9a328---4</link><author>Gwang-Jin</author><category>devops</category><pubDate>Wed, 2 Jul 2025 16:24:25 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Automation Evolution: Is Your DevOps Ready for Tomorrow’s Innovations?</title><link>https://devops.com/automation-evolution-is-your-devops-ready-for-tomorrows-innovations/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=automation-evolution-is-your-devops-ready-for-tomorrows-innovations</link><author>Christopher Haggan</author><category>devops</category><pubDate>Wed, 2 Jul 2025 14:47:04 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Docker State of App Dev: Dev Ex &amp; Productivity</title><link>https://www.docker.com/blog/docker-state-of-app-dev-dev-ex-productivity/</link><author>Olga Diachkova</author><category>docker</category><category>devops</category><pubDate>Wed, 2 Jul 2025 13:20:43 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Report: What’s helping devs thrive — and what’s still holding them back? A look at how culture, tooling, and habits are shaping the developer experience today, per Docker’s 2025 State of Application Development Survey.Great culture, better tools — but developers often still feel stuck. From pull requests stuck in review to tasks without clear estimates, the inner loop remains cluttered with surprisingly persistent friction points. This year’s data maps the disconnect between what developers need, where they’re blocked, and how better tooling and cultural support can keep velocity on track.Here are six key insights into developer experience and productivity from Docker’s annual State of Application Development Survey, based on responses from over 4,500 industry professionals.1. How devs learn — and what’s changingSelf-guided learning is on the upswing. Across all industries, fully  of respondents turn to online courses or certifications, far outpacing traditional sources like school (), books (), or on-the-job training (). Among IT folks, the picture is more nuanced. School is still the top venue for learning to code (, up from 57% in our 2024 survey), but online resources are also trending upward. Some  of IT pros learned coding skills via online resources (up from 54% in our 2024 survey) and  favored online courses or certifications (up from 45% in 2024).Note: For this year’s report, we surveyed over three times more users across a broader spectrum of industries than for our more IT-focused 2024 report.As for  devs prefer to learn, documentation tops the list, as in last year’s report — that despite the rise in new and interactive forms of learning. Some  say they lean on documentation, edging out videos and side projects () and slightly ahead of structured online training (). AI tools play a relatively minor role in how respondents learn, with GitHub Copilot cited by just  overall — and only among IT pros. It’s also cited by  as a preferred learning method.2. Containers: the great divide?Among IT pros, container usage soared to  — up from 80% in our 2024 survey. Zoom out to a broader view across industries, however, and adoption appears considerably lower. Just  of developers say they use containers in any part of their workflow. Why the gap? Differences in app structure may offer an explanation: IT industry respondents work with microservice-based architectures more often than those in other industries ( versus ). So the higher container adoption may stem from IT pros’ need for modularity and scalability — which containers provide in spades. And among container users, needs are evolving. They want better tools for , , and — stubborn pain points across the software lifecycle.3. An equal-opportunity headache: estimating timeNo matter the role, estimating how long a task will take is the most consistent pain point across the board. Whether you’re a front-end developer (), data scientist (), or a software decision-maker (), precision in time planning remains elusive.Other top roadblocks?  and pull-request review (25%) are slowing teams down. Interestingly, where people say they need  doesn’t always match where they’re getting stuck. Case in point, testing solutions and Continuous Delivery (CD) come up often when devs talk about tooling gaps — even though they’re not always flagged as blockers.4. Productivity by persona: different hats, same strugglesWhen you break it down by role, some unique themes emerge: struggle most with time estimation (). face a three-way tie: planning, time estimation, and designing from scratch (28% each). are especially challenged by — a task not traditionally in their wheelhouse., surprisingly, list  as a challenge, closely followed by .Across personas, a common thread stands out: even seasoned professionals are grappling with foundational coordination tasks — not the “hard” tech itself, but the orchestration around it.5. Tools vs. culture: two sides of the experience equationOn the tooling side, the biggest callouts for improvement include:Designing solutions from scratch (17%)But productivity isn’t just about tools — it’s deeply cultural. When asked what’s working well, developers pointed to , location flexibility such as work from home policies (38%), and  as top cultural strengths.The weak spots? , , and . In other words: developers like where, when, and how they work, but not always .6. What’s easy? What’s not?While the dev world is full of moving parts, a few areas are surprisingly  challenging:Editing config files (8%)Writing config files (7%)Contrast that with the most taxing areas:Troubleshooting in production (9%)Debugging in production (9%)Security-related tasks (8%)It’s a reminder that production is still where the stress — and the stakes — are highest.Developer productivity isn’t about just one thing. It’s the compound effect of better tools, smarter learning, sharper planning — and yes, a healthy team culture. For orgs to excel, they need to invest not just in platforms, but also in people. Because when you improve the , you unlock the performance.]]></content:encoded></item><item><title>VS Code’s Open Source AI Revolution: A New Chapter for Developers</title><link>https://devops.com/vs-codes-open-source-ai-revolution-a-new-chapter-for-developers/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=vs-codes-open-source-ai-revolution-a-new-chapter-for-developers</link><author>Tom Smith</author><category>devops</category><pubDate>Wed, 2 Jul 2025 12:03:55 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi</title><link>https://www.youtube.com/watch?v=Xr0Eb-ybvck</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Xr0Eb-ybvck?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 20:32:44 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm

Everyone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.

In this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.

Join us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.]]></content:encoded></item><item><title>Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb</title><link>https://www.youtube.com/watch?v=hERRANApN5c</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/hERRANApN5c?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb]]></content:encoded></item><item><title>Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Con... Gordon Radlein</title><link>https://www.youtube.com/watch?v=dlDTX-aDNzg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/dlDTX-aDNzg?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Contribution - Gordon Radlein, Datadog

OpenTelemetry has accelerated the commoditization of instrumentation. Telemetry generation is becoming a solved problem, an implementation detail. But this has created a new challenge: a wealth of standardized signals with no standard meaning. Different systems instrumented with different semantics generating telemetry in their own unique language. And while signal correlation connects specific workloads, it fails when we need to understand our systems at a macro scale by joining disparate datasets.
That is, until we all agreed to speak the same language.

Just as English as a lingua franca fueled progress across the internet, OpenTelemetry Semantic Conventions are providing a shared language for our systems. In this talk we’ll discuss why semantic interoperability is the real connective tissue, how it’s fueling deeper insights into our production environments, and the key role it plays in enabling the AI systems that are rapidly ushering in the next revolution of our industry.]]></content:encoded></item><item><title>Welcome + Opening Remarks - Austin Parker, Honeycomb</title><link>https://www.youtube.com/watch?v=_rqgWHaEvgc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/_rqgWHaEvgc?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Welcome + Opening Remarks - Austin Parker, Honeycomb]]></content:encoded></item><item><title>Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere</title><link>https://www.youtube.com/watch?v=Z4umnlRdLtA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Z4umnlRdLtA?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere

Logs can get very expensive and often how useful all those logs are is unknown, some are but many are not. It is very difficult to know which logs are useful and how exactly they are used. With Chronosphere's Control plane for logs users can now get a comprehensive analysis of value and usage patterns, along with sophisticated recommendations and control actions that allow some or most of the value derived from those logs to be preserved. In order to achieve our goals we have enhanced Fluent Bit to be more flexible in which logs are actioned upon and will share useful future additions to it.]]></content:encoded></item><item><title>Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson</title><link>https://www.youtube.com/watch?v=J_hHiwa_3QU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/J_hHiwa_3QU?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson, Chronosphere

Hybrid cloud isn’t a stepping stone—it’s a destination. With 39% of CNCF survey respondents already operating in hybrid environments, this model is here to stay. But as teams pursue cloud-native architectures, many skip a critical step: developing a clear cloud strategy and an observability approach to match.
The result is predictable— widening visibility gaps, redundant tooling and data, and spiraling costs as teams try to stitch together disconnected, vendor-specific systems never meant to work in concert. Hybrid environments expose these issues quickly, especially when workloads span multiple platforms without a unified way to observe and understand them.
Modernization efforts demand open observability from the start—not as an add-on. Technologies like OpenTelemetry, Fluent Bit, and Prometheus act as connective tissue across clouds, clusters, and on-prem infrastructure, enabling standardization where it’s needed most.
This talk outlines how to center open observability in your modernization journey: where to standardize architectural layers, how to maintain a more open approach, and why these decisions have long-term payoff. 
Hybrid complexity is inevitable. Leading with open observability is how you stay in control—now and in the future.]]></content:encoded></item><item><title>Sponsored Keynote: Foundation-Led Innovation: OpenSearch&apos;s Impact on Modern Data I... Dotan Horovits</title><link>https://www.youtube.com/watch?v=C5Y3qnEJSY8</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/C5Y3qnEJSY8?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:35:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Sponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data Insights - Dotan Horovits, AWS OpenSearch]]></content:encoded></item><item><title>Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector&apos;s Per... Denton Krietz</title><link>https://www.youtube.com/watch?v=zgnY8szpKUw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/zgnY8szpKUw?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Persistent Queue - Denton Krietz, Bindplane

The OpenTelemetry Collector’s persistent queue provides a robust mechanism for handling data bursts, destination outages, and processing delays, ensuring no telemetry data is lost—but from experience, it’s consistently one of the collector's least understood features.

In this talk, we’ll explore the inner workings of the OTel Collector’s persistent queue, including how it buffers data, ensures durability, and enables replay after failures. Attendees will learn how to configure persistent queues for their unique workloads, optimize their telemetry pipeline performance, and troubleshoot common pitfalls.

Whether you’re a site reliability engineer, developer, or observability enthusiast, this talk will equip you with the knowledge to deeply understand persistent queues to optimize your telemetry pipeline in production.]]></content:encoded></item><item><title>Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner &amp; Ray Jenkins, Streamfold</title><link>https://www.youtube.com/watch?v=xeQnP8Ct7qY</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/xeQnP8Ct7qY?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner & Ray Jenkins, Streamfold

In this talk, we'll introduce Rotel—an open-source OpenTelemetry collector built in Rust. Rotel is lightweight and resource-efficient, integrating seamlessly into your development workflow. Its compact design lets you package it with your Python or NodeJS projects, so telemetry collection runs alongside your code without needing additional sidecars.

We'll explore how rethinking telemetry collection at the edge can empower developers right from the early stages of development, paving the way for broader OpenTelemetry adoption. You’ll learn how Rust’s low-overhead FFI enables native extensions for telemetry filtering, transformation, and enrichment using Python and Typescript.

By leveraging Rust’s performance strengths, Rotel avoids the overhead of garbage collection, resulting in lower memory usage and reduced latency. Its quick cold start times make it a natural fit for modern cloud-native, serverless, and edge computing environments. Join us to discover how moving telemetry collection closer to the source can help you analyze high-volume, high-fidelity signals more effectively.]]></content:encoded></item><item><title>Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTele... Diana Todea</title><link>https://www.youtube.com/watch?v=wWON2NT41lE</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/wWON2NT41lE?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTelemetry - Diana Todea, Aircall

Becoming a contributor to an open-source project is a transformative step in any developer's career. This session explores the journey from first-time contributor to active developer, covering best practices for navigating project communities, understanding codebases, and making meaningful contributions. Learn strategies for selecting the right project, mastering collaboration tools, and embracing the culture of open-source development. The audience will be inspired about my one year journey with the open source project OpenTelemetry and how I have built a proof of concept for it and achieved developer status for this project. By the end of this talk, the public will gain insights into the tools to become a better developer and how to build more engagement with the community.]]></content:encoded></item><item><title>Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchma... Henrik Rexed</title><link>https://www.youtube.com/watch?v=tZho5W9L_Z8</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/tZho5W9L_Z8?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchmark Analysis - Henrik Rexed, Dynatrace

In a push to standardize observability practices, the cloud-native community has embraced OpenTelemetry, offering a unified framework for metrics, logs, and traces. Prior to this, log processing relied on agents like fluent, evolving into fluentbit. With fluentbit's recent expansion to support additional signals and the OpenTelemetry Collector's emergence, a pertinent question arises: Which is the superior choice for performance?

This session delves into:
- Unveiling the distinctions between Fluent Bit and the OpenTelemetry Collector.
- Sharing the findings derived from a series of benchmark tests.
- Providing valuable insights to empower the community in selecting the most fitting agent for their cloud-native environments.]]></content:encoded></item><item><title>The Spec-tacular Game Show - Liudmila Molkova, Ted Young, Tyler Helmuth, Jamie Danielson, Alex Boten</title><link>https://www.youtube.com/watch?v=ipFVu0dl5Bw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/ipFVu0dl5Bw?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Panel: The Spec-tacular Game Show - Liudmila Molkova, Microsoft; Ted Young, Grafana Labs; Tyler Helmuth, Jamie Danielson & Alex Boten, Honeycomb

From OTLP to OTTL, engineers are excited about a lot of things. But there is one thing that excites them above all else and that is correcting people. Welcome to “The Spec-tacular Game Show”.

In this fun game show our panelists will be given incorrect statements about the OpenTelemetry Specification or Semantic Convention. The panelists will buzz in, identify what’s wrong, and state the correction. If none of the panelists know the answer the audience will get a chance to answer to steal the point. The panelist (or audience) with the most points wins!

After each question we’ll spend a time explaining why the Spec and Semconv is the way it is and highlight how it produces the production-quality telemetry you know and love. Join us for a fun, relaxing, (snarky) panel about everyone’s favorite part of Otel!]]></content:encoded></item><item><title>How To Think About Instrumentation Overhead - Jason Plumb, Splunk</title><link>https://www.youtube.com/watch?v=fvmzAX_ZyvM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/fvmzAX_ZyvM?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

How To Think About Instrumentation Overhead - Jason Plumb, Splunk

Novice observability practitioners are often overly obsessed with performance. They might approach instrumentation with skepticism and have concerns about latency degradation or resource consumption. This talk is a primer on the topic of instrumentation overhead, and it will teach you how to think about overhead in an observability context. We will cover the causes of overhead and why overhead is so hard to measure and even harder to predict reliably. Lastly, we will present some practical techniques for understanding overhead in your environment and some strategies for coping with it.]]></content:encoded></item><item><title>No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft</title><link>https://www.youtube.com/watch?v=fU6jsw0yaVU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/fU6jsw0yaVU?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft

The best telemetry starts at the source—inside the client libraries.
But in most cases, that means taking a dependency on the OpenTelemetry API from your library. And while it’s stable, minimal, reliable, and safely no-op unless configured—transitive dependencies are still the bane of any library developer’s existence, and most of us try to avoid them.

To work around this, people reach for abstractions, plugins, bridges, or even OTel forks that break context propagation. The result? A poor user experience. Users must find the right plugin, install it, wire it up—and still hit the diamond dependency problem, now it just affects a subset of users.

But what if you could take a truly optional dependency? If OpenTelemetry is on the classpath, instrumentation kicks in. If it’s not, no harm done.
How hard is that to pull off? How reliable? How performant?

Let’s explore that—through the lens of the next generation of Azure SDKs for Java. Spoiler: it’s easy and fast, and as a side-bonus, we can fall back to logs-based tracing if OTel is not found.]]></content:encoded></item><item><title>Closing Remarks - Austin Parker, Honeycomb</title><link>https://www.youtube.com/watch?v=eDbQfZ9eoNI</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/eDbQfZ9eoNI?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Closing Remarks - Austin Parker, Honeycomb]]></content:encoded></item><item><title>Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace</title><link>https://www.youtube.com/watch?v=di5nhYvUh6w</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/di5nhYvUh6w?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 17:34:35 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace

The OTel Java API, SDK, and ecosystem are perfectly adequate for Android developer to get OTel instrumentation into their apps. But for a host of reasons, the match is not perfect, especially for developers who only write in Kotlin, which is the recommended development language for Android by Google, not the least of which is the emergence of Kotlin Multiple Platform (KMP) as a means to share code between Android, iOS, and many other platforms.

This session will outline the reasons why we at Embrace is trying to kick-start the development of a pure Kotlin ecosystem for OTel, starting with an API and SDK implementation, and how we are doing it in a way where mobile developers can get value incrementally without having to wait until every aspect is fully built out.

We want OTel to feel natural and idiomatic for Android developers, and this is the first step towards that end.]]></content:encoded></item><item><title>The Docker MCP Catalog: the Secure Way to Discover and Run MCP Servers</title><link>https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/</link><author>Nuno Coracao</author><category>docker</category><category>devops</category><pubDate>Tue, 1 Jul 2025 13:04:20 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[The Model Context Protocol (MCP) ecosystem is exploding. In just weeks, our Docker MCP Catalog has surpassed , validating that developers are hungry for a secure way to run MCP servers. Today, we’re excited to share major updates to the Docker MCP Catalog, including enhanced discovery features and our new open submission process. With hundreds of developers already requesting to publish their MCP servers through Docker, we’re accelerating our mission to make containerized MCP servers the standard for secure AI tool distribution.The rapid adoption of MCP servers also highlights a critical problem — the current practice of running them via npx or uvx commands exposes systems to unverified code with full host access, not to mention dependency management friction. In this post, we’ll explain why Docker is investing in the MCP ecosystem, showcase the new catalog capabilities, and share how you can contribute to building a more secure foundation for AI applications.Figure 1: The new Docker MCP Catalog, built for easier discovery.Why Docker is building the MCP CatalogThe security issues in MCP distributionEvery time a developer runs npx -y @untrusted/mcp-server or uvx some-mcp-tool, they’re making a dangerous trade-off: convenience over security. These commands execute arbitrary code directly on the host system with full access to:Environment variables and secretsSome MCP clients limit environment variable access, but even that is not a universal practice. This isn’t sustainable. As MCP moves from experimentation to production, we need a fundamentally different approach.Docker has spent over a decade solving exactly these problems for cloud-native applications. We’ve built the infrastructure, tools, and trust that developers rely on to run billions of containers in production. Now, we’re applying these same principles to the MCP ecosystem.When you run an MCP server from our Catalog, you get: verifying the image hasn’t been tampered withSoftware Bill of Materials (SBOMs) documenting every component from your host system to only what the server actually needsThis isn’t about making life harder for developers—it’s about making security the path of least resistance.Introducing the enhanced MCP CatalogWe’ve reimagined the MCP Catalog to make it more accessible and easier to navigate. You can still access the MCP Catalog from Docker Hub and the MCP Toolkit in Docker Desktop just like before, or go straight to the MCP catalog. We’ve gone beyond generic container image listings by building features that help you quickly find the right MCP servers for your AI applications.  : MCP servers are organized by what they actually do:Data Integration (databases, APIs, file systems)Development Tools (IDEs, code analysis, testing)Communication (email, Slack, messaging platforms)Productivity (task management, calendars, note-taking)Analytics (data processing, visualization, reporting): Find servers by capability, tools, GitHub tags, and categories — not just by name.: Every catalog entry clearly shows whether it’s Docker-built (with transparent build signing and verification) or community-built (containerized and maintained by the publisher).Figure 2: Discover MCP servers by use cases.How we classify MCP Servers: Built by Docker vs. community-built: When you see “Built by Docker,” you’re getting our complete security treatment. We control the entire build pipeline, providing cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning.: These servers are packaged as Docker images by their developers. While we don’t control their build process, they still benefit from container isolation, which is a massive security improvement over direct execution.Tiers serve important roles: Docker-built servers demonstrate the gold standard for security, while community-built servers ensure we can scale rapidly to meet developer demand. Developers can change their mind after submitting a community-built server and opt to resubmit it as a Docker-built server.Figure 3: An example of Built by Docker MCP Server.Open for MCP server submission: Join the secure MCP movementStarting today, we’re opening our submission process to the community. Whether you’re an individual developer or an enterprise team, you can feature your MCP servers on the Docker MCP Catalog. By publishing through our catalog, you’re not just distributing your MCP server — you’re helping establish a new security standard for the entire ecosystem while getting your MCP tools available to millions of developers already using Docker via Docker Hub and Docker Desktop. Your containerized server becomes part of the solution, demonstrating that production-ready AI tools don’t require compromising on security. How to submit your MCP server – Package your MCP server as a Docker image – Opt for Docker-built (we handle the build) or community-built (you build and maintain it)We’re committed to a fast, transparent review process. Quality MCP servers that follow our security guidelines will be published quickly, helping you reach Docker’s 20+ million developer community.ClickHouse is one of the first companies to take advantage of Docker’s MCP Catalog, and they opted for the Docker-built tier to ensure maximum security. Here’s why they chose to partner with Docker:, we deliver the fastest analytics database – open-source, and designed for real-time data processing and analytics at scale. As agentic AI becomes more embedded in modern applications, developers are using the ClickHouse MCP server to support intelligent, data-driven workflows that demand low latency, high concurrency, and cost efficiency.To make it easier for developers to deploy these workloads, we’re featuring  on Docker’s MCP Catalog, which provides a powerful way to reach 20M+ developers and makes it easier for Docker users to discover and use our solution. We opted for “Built by Docker” with the highest security standard, including cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning. Together with Docker, developers can run ClickHouse MCP Server with confidence, knowing it’s secured, verified, and ready for their agentic applications.” – Tanya Bragin, VP of Product and Marketing ClickhouseWe’re preparing for the future of cloud-native AI applications. Remote MCP servers will enable:Managed MCP services that scale automaticallyShared capabilities across teams without distributing codeStricter security boundaries for sensitive operationsIntegration with the official MCP registryWe’re actively collaborating with the MCP community on the upcoming official registry. Our vision is complementary:The official registry provides centralized discovery – the “yellow pages” of available MCP serversDocker provides the secure runtime and distribution for those listingsTogether, we create a complete ecosystem where discovery and security work hand-in-handThe explosive growth of our MCP Catalog, 1 million pulls and hundreds of publisher requests, tells us developers are ready for change. They want the power of MCP, but they need it delivered securely.By establishing containers as the standard for MCP server distribution, we’re not trying to own the ecosystem — we’re trying to secure it. Every MCP server that moves from npx execution to containerized deployment is a win for the entire community.Explore the enhanced MCP Catalog: Visit the MCP Catalogto discover MCP servers that solve your specific needs securely.Use and test hundreds of MCP Servers: Download Docker Desktopto download and use any MCP server in our catalog with your favorite clients: Gordon, Claude, Cursor, VSCode, etc: Star our repository and watch for updates on the MCP Gateway release and remote server capabilities.Together, we’re building more than a catalog — we’re establishing the secure foundation that the MCP ecosystem needs to grow from experimental tool to production-ready platform. Because when it comes to AI applications, security isn’t optional. It’s fundamental.]]></content:encoded></item><item><title>Build the highest resilience apps with multi-Region strong consistency in Amazon DynamoDB global tables</title><link>https://aws.amazon.com/blogs/aws/build-the-highest-resilience-apps-with-multi-region-strong-consistency-in-amazon-dynamodb-global-tables/</link><author>Donnie Prakoso</author><category>devops</category><pubDate>Mon, 30 Jun 2025 20:30:48 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[While tens of thousands of customers are successfully using Amazon DynamoDBglobal tables with eventual consistency, we’re seeing emerging needs for even stronger resilience. Many organizations find that the DynamoDB multi-Availability Zone architecture and eventually consistent global tables meet their requirements, but critical applications like payment processing systems and financial services demand more.For these applications, customers require a zero Recovery Point Objective (RPO) during rare Region-wide events, meaning you can direct your app to read the latest data from any Region. Your multi-Region applications always need to access the same data regardless of location.Starting today, you can use a new Amazon DynamoDB global tables capability that provides multi-Region strong consistency (MRSC), enabling zero RPO. This capability, first announced as a preview at AWS re:Invent 2024, simplifies building highly resilient global applications.Here’s how you can enable MRSC starting with an existing empty DynamoDB table:With MRSC, DynamoDB now offers you the highest level of resilience for your applications. If application processing is interrupted in a Region, you can redirect traffic to another Region containing a MRSC replica and know that you’ll be processing the latest data.Let me walk you through how you can use this new capability.To start using MRSC, I need to create a global table from an existing DynamoDB table that doesn’t contain any data. I navigate to my existing table, select the  tab and select .The MRSC availability architecture requires three AWS Regions. I can configure MRSC with either three full replicas or with two replicas and a witness. A witness contains only replicated change data to provide the required availability without maintaining a full copy of my table data.The following screenshot shows how I configure MRSC with two replicas and a witness. To configure MRSC with three full replicas instead, I can clear Configure Region 2 as Witness.If I need to update my existing table programmatically, I can use the Amazon Q CLI to generate the required command with the following prompt:> Hey Q! Update my existing DynamoDB table called "demo-mrsc" in us-east-1 with multi-Region strong consistency across us-east-2 with witness in us-west-2 RegionsShortly after, Q CLI will respond back with following command:> To update your DynamoDB table with multi-Region strong consistency, you'll need to use the update-table command with the appropriate parameters. Here's how you would do it:

aws dynamodb update-table \ 
    --table-name demo-mrsc \ 
    --replica-updates '[{"Create": {"RegionName": "us-east-2"}}]' \ 
    --global-table-witness-updates '[{"Create": {"RegionName": "us-west-2"}}]' \ 
    --multi-region-consistency STRONG \ 
    --region us-east-1
After it’s finished processing, I can check the status of my MRSC global table. I can see I have a  configured for my DynamoDB global table. A witness reduces costs while still providing the resilience benefits of multi-Region strong consistency.Then, in my application, I can use  to read data with strong consistency. Here’s a Python example:import boto3

# Configure the DynamoDB client for your region
dynamodb = boto3.resource('dynamodb', region_name='us-east-2')
table = dynamodb.Table('demo-mrsc')

pk_id = "demo#test123"

# Read with strong consistency across regions
response = table.get_item(
    Key={
        'PK': pk_id
    },
    ConsistentRead=True
)

print(response)
For operations that require the strongest resilience, I can use . For less critical operations where eventual consistency is acceptable, I can omit this parameter to improve performance and reduce costs.Here are a couple of things to note: – The Amazon DynamoDB multi-Region strong consistency capability is available in following AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Osaka, Seoul, Tokyo), and Europe (Frankfurt, Ireland, London, Paris)Learn more about how you can achieve the highest level of application resilience, enable your applications to be always available and always read the latest data regardless of the Region by visiting Amazon DynamoDB global tables.]]></content:encoded></item><item><title>Survey: Pace of Increased Adoption of GitOps Varies Widely</title><link>https://devops.com/survey-pace-of-increased-adoption-of-gitops-varies-widely/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=survey-pace-of-increased-adoption-of-gitops-varies-widely</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 30 Jun 2025 19:51:43 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kusari Adds AI Security Tool to Inspect Code as Pull Requests Are Made</title><link>https://devops.com/kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 30 Jun 2025 19:12:59 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Beyond a RHEL Clone: How Rocky Linux Is Evolving Into Something More</title><link>https://devops.com/beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more</link><author>Nathan Blackham</author><category>devops</category><pubDate>Mon, 30 Jun 2025 18:32:50 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New Amazon EC2 C8gn instances powered by AWS Graviton4 offering up to 600Gbps network bandwidth</title><link>https://aws.amazon.com/blogs/aws/new-amazon-ec2-c8gn-instances-powered-by-aws-graviton4-offering-up-to-600gbps-network-bandwidth/</link><author>Channy Yun (윤석찬)</author><category>devops</category><pubDate>Mon, 30 Jun 2025 18:01:32 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[You can use C8gn instances to run the most demanding network intensive workloads, such as security and network virtual appliances (virtual ﬁrewalls, routers, load balancers, proxy servers, DDoS appliances), data analytics, and tightly-coupled cluster computing jobs.EC2 C8gn instances specifications C8gn instances provide up to 192 vCPUs and 384 GiB memory, and offer up to 30 percent higher compute performance compared Graviton3-based EC2 C7gn instances.Here are the specs for C8gn instances:If you’re using C7gn instances now, you will have straightforward experience migrating network intensive workloads to C8gn instances because the new instances offer similar vCPU and memory ratios. To learn more, check out the collection of Graviton resources to help you start migrating your applications to Graviton instance types.]]></content:encoded></item><item><title>AWS Weekly Roundup: Project Rainier, Amazon CloudWatch investigations, AWS MCP servers, and more (June 30, 2025)</title><link>https://aws.amazon.com/blogs/aws/aws-weekly-roundup-project-rainier-amazon-cloudwatch-investigations-aws-mcp-servers-and-more-june-30-2025/</link><author>Channy Yun (윤석찬)</author><category>devops</category><pubDate>Mon, 30 Jun 2025 16:39:17 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Every time I visit Seattle, the first thing that greets me at the airport is Mount Rainier. Did you know that the most innovative project at Amazon Web Services (AWS) is named after this mountain?Project Rainier is a new project to create what is expected to be the world’s most powerful computer for training AI models across multiple data centers in the United Stages. Anthropic will develop the advanced versions of its Claude models with five times more computing power than its current largest training cluster.The key technology powering Project Rainier is AWS custom-designed Trainium2 chips, which are specialized for the immense data processing required to train complex AI models. Thousands of these Trainium2 chips will be connected in a new type of Amazon EC2 UltraServer and EC2 UltraCluster architecture that allows ultra-fast communication and data sharing across the massive system.Learn about the AWS vertical integration of Project Rainer, where it designs every component of the technology stack from chips to software, allows it to optimize the entire system for maximum efficiency and reliability. Here are some launches that got my attention:Amazon S3 access for Amazon FSx for OpenZFS – You can access and analyze your FSx for OpenZFS file data through Amazon S3 Access Points, enabling seamless integration with AWS AI/ML, and analytics services without moving your data out of the file system. You can treat your FSx for OpenZFS data as if it were stored in S3, making it accessible through the S3 API for various applications including Amazon Bedrock, Amazon SageMaker, AWS Glue, and other S3 based cloud-native applications.Amazon S3 with sort and z-order compaction for Apache Iceberg tables – You can optimize query performance and reduce costs with new sort and z-order compaction. With S3 Tables, sort compaction automatically organizes data files based on defined column orders, while z-order compaction can be enabled through the maintenance API for efficient multicolumn queries.Amazon CloudWatch investigations – You can accelerate your operational troubleshooting in AWS environments using the Amazon CloudWatch AI-powered investigation feature, which helps identify anomalies, surface related signals, and suggest remediation steps. This capability can be initiated through CloudWatch data widgets, multiple AWS consoles, CloudWatch alarm actions, or Amazon Q chat and enables team collaboration and integration with Slack and Microsoft Teams.Amazon Bedrock Guardrails Standard tier – You can enhance your AI content safety measures using the new Standard tier. It offers improved content filtering and topic denial capabilities across up to 60 languages, better detection of variations including typos, and stronger protection against prompt attacks. This feature lets you configure safeguards to block harmful content, prevent model hallucinations, redact personally identifiable information (PII), and verify factual claims through automated reasoning checks.Amazon Route 53 Resolver endpoints for private hosted zone – You can simplify DNS management across AWS and on-premises infrastructure using the new Route 53 DNS delegation feature for private hosted zone subdomains, which works with both inbound and outbound Resolver endpoints. You can delegate subdomain authority between your on-premises infrastructure and Route 53 Resolver cloud service using name server records, eliminating the need for complex conditional forwarding rules.Amazon Q Developer CLI for Java transformation – You can automate and scale Java application upgrades using the new Amazon Q Developer Java transformation command line interface (CLI). This feature perform upgrades from Java versions 8, 11, 17, or 21 to versions 17 or 21 directly from the command line. This tool offers selective transformation options so you can choose specific steps from transformation plans and customize library upgrades.New AWS IoT Device Management managed integrations – You can simplify Internet of Things (IoT) device management across multiple manufacturers and protocols using the new managed integrations feature, which provides a unified interface for controlling devices whether they connect directly, through hubs or third-party clouds. The feature includes pre-built cloud-to-cloud (C2C) connectors, device data model templates, and SDKs that support ZigBee, Z-Wave, and Wi-Fi protocols, while you can still create custom connectors and data models. Check your calendars and sign up for these upcoming AWS events:AWS re:Invent – Register now to get a head start on choosing your best learning path, booking travel and accommodations, and bringing your team to learn, connect, and have fun. If you’re an early-career professional, you can apply to the All Builders Welcome Grant program, which is designed to remove financial barriers and create diverse pathways into cloud technology.AWS Builders Online Series – If you’re based in one of the Asia Pacific time zones, join and learn fundamental AWS concepts, architectural best practices, and hands-on demonstrations to help you build, migrate, and deploy your workloads on AWS.That’s all for this week. Check back next Monday for another Weekly Roundup!]]></content:encoded></item><item><title>Fast Code, Real Risks: Guardrails for AI-Generated Software</title><link>https://devops.com/fast-code-real-risks-guardrails-for-ai-generated-software/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=fast-code-real-risks-guardrails-for-ai-generated-software</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 30 Jun 2025 15:51:10 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Next Version of Grok Includes Advanced Coding Assistance: Reports</title><link>https://devops.com/next-version-of-grok-includes-advanced-coding-assistance-reports/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=next-version-of-grok-includes-advanced-coding-assistance-reports</link><author>Jon Swartz</author><category>devops</category><pubDate>Mon, 30 Jun 2025 15:43:44 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tool Calling with Local LLMs: A Practical Evaluation</title><link>https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/</link><author>Ignasi Lopez Luna</author><category>docker</category><category>devops</category><pubDate>Mon, 30 Jun 2025 13:48:42 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Which local model should I use for tool calling?When building GenAI and agentic applications, one of the most pressing and persistent questions is: “Which local model should I use for tool calling?”  We kept hearing again and again, from colleagues within Docker and the developer community, ever since we started working on , a local inference engine that helps developers run and experiment with local models. It’s a deceptively simple question with a surprisingly nuanced answer. Even when we tried to answer it for a very specific case: “What if I just expose 5 simple tools to the model?”We realized we had no definite answer forthat. Local LLM models offer control, cost-efficiency, and privacy, but when it comes to structured tool use, deciding when and how to act, they can behave very differently. We decided to dig deep and test this properly. We started with manual experimentation, then built a framework to scale our testing. This blog documents that journey and shares which models ranked highest on our tool-calling leaderboard.The first attempt: Manual testingOur first instinct was to build something quickly and try it out manually.So we created, an AI-powered shopping assistant that lets users interact via chat to build, modify, and check out a shopping cart. Through a natural conversation, users can discover products, add or remove items, and complete or cancel their purchase, all from the chat interface.To support testing across different LLMs, we added a model selector that makes it easy to switch between local models (via Docker Model Runner or Ollama) and hosted models using the OpenAI API.OpenAI’s GPT-4 or GPT-3.5 worked as expected, and the experience was fairly smooth. Called tools when they were neededAvoided unnecessary tool usageHandled tool responses naturallyBut the local models? That’s where the challenges started to surface.What went wrong with local modelsWe started experimenting with some of the local models listed on the Berkeley Function-Calling Leaderboard. Our goal was to find smaller models, ideally with fewer than 10 billion parameters, so we tested xLAM-2-8b-fc-r and watt-tool-8B. We quickly ran into several recurring issues:: Tools were being called even for greeting messages like “Hi there!”: The model would search when it should have added, or tried to remove when the cart was empty: Parameters like product_name or quantity were missing or malformed: The model often failed to respond to tool output, leading to awkward or incomplete conversationsAt this point, it was clear that manual testing wouldn’t scale. Different models failed in different ways, some struggled with invocation logic, while others mishandled tool arguments or responses.  Testing was not only slow, but also unreliable. Because these models are non-deterministic, we had to run each scenario multiple times just to get a reliable read on behavior.We needed a testing setup that was repeatable, measurable, and fast.Our second attempt: A scalable testing toolOur goal wasn’t academic rigor.It was: “Give us good-enough answers in 2–3 days, not weeks.”In a couple of days, we created, This is a flexible project with the following capabilitiesDefine real-world  with multiple valid tool call sequencesRun them against many models (local & hosted)Track , , and Log  for analysis (or eventual fine-tuning)The core idea behind model-test is simple: simulate realistic tool-using conversations, give the model room to reason and act, and check whether its behavior makes sense.A  (e.g. “Add iPhone to cart”)The  (optional)One or more , because there’s often more than one right answer{
  "prompt": "Add iPhone to cart",
  "expected_tools_variants": [
    {
      "name": "direct_add",
      "tools": [{ "name": "add_to_cart", "arguments": { "product_name": "iPhone" } }]
    },
    {
      "name": "search_then_add",
      "tools": [
        { "name": "search_products", "arguments": { "query": "iPhone" } },
        { "name": "add_to_cart", "arguments": { "product_name": "iPhone 15" } }
      ]
    }
  ]
}
In this case, we consider both  and “search first, then add the result” as acceptable. Even though “iPhone” isn’t a real product name, we’re fine with it. We weren’t aiming for overly strict precision, just realistic behavior.Each test case belongs to a test suite. We provide two built-in suites. However, you can run an entire suite, individual test cases, or a selection of multiple test cases. Additionally, you can create your own custom suites to group tests as needed. : Greetings, single-step actions: Multi-step reasoning and tool chainingTo make tests feel closer to how real agents behave, we simulate an agent loop up to .User: Model: “Let me search for iPhone 5…”Tool: Model: “Adding product X to cart…”Model:  → Great, test passed!But if the model still wants to keep going after round 5?That’s it, my friend,  . Time’s up.We deliberately avoided designing tests that require perfect predictions.We didn’t demand that the model always know the exact product name.What mattered was: did the tool sequence make sense for the intent?This helped us focus on the kind of reasoning and behavior we actually want in agents, not just perfect token matches.Our test outputs distilled down to a final F1 score, encapsulating three core dimensions:Did the model realize a tool was needed?Did it choose the right tool(s) and use them correctly?Whether the tool call arguments were correct?The F1 score is the harmonic mean of two things: precision (how often the model made valid tool calls) and recall (how often it made the tool calls it was supposed to).We also tracked latency, the average runtime in seconds, but that wasn’t part of the F1 calculation; it simply helped us evaluate speed and user experience.21 models and 3,570 tests later: Which models nailed tool calling?We tested 21 models across  using 210 batch runs.Overall Rankings (by Tool Selection F1):claude-3-5-sonnet-20241022Among all models, OpenAI’s GPT-4 came out on top with a tool selection F1 score of 0.974, completing responses in just under 5 seconds on average. While hosted and not the focus of our local model exploration, it served as a reliable benchmark and provided some ground truths.On the local side, Qwen 3 (14B) delivered outstanding results, nearly matching GPT-4 with a 0.971 F1 score, though with significantly higher latency (~142 seconds per interaction).If you’re looking for something faster, Qwen 3 (8B) also achieved an F1 score of 0.933, while cutting latency nearly in half (~84 seconds), making it a compelling balance between speed and tool-use accuracy.Hosted models like Claude 3 Haiku also performed very well, hitting 0.933 F1 with exceptional speed (3.56 seconds average latency), further illustrating the high bar set by cloud-based offerings.Not all models handled tool calling well. The quantized Watt 8B model struggled with parameter accuracy and ended up with a tool selection F1 score of just 0.484. Similarly, the LLaMA-based XLam 8B variant often missed the correct tool path altogether, finishing with an F1 score of 0.570. These models may be suitable for other tasks, but for our structured tool use test, they underdeliver.We also experimented with both  and  variants for some models, and in all cases observed no significant difference in tool-calling behavior or performance. This suggests that quantization is beneficial for reducing resource usage without negatively impacting accuracy or reasoning quality, at least for the models and scenarios we tested.If your goal is maximum tool-calling accuracy, then Qwen 3 (14B) or Qwen 3 (8B) are your best bets, both local, both precise, with the 8B variant being notably faster.For a good trade-off between speed and performance, Qwen 2.5 stood out as a solid option. It’s fast enough to support real-time experiences, while still maintaining decent tool selection accuracy.If you need something more lightweight, especially for resource-constrained environments, the LLaMA 3 Groq 7B variant offers modest performance at a much lower compute footprint.What we learned and why this mattersOur testing confirmed that the Qwen family of models leads the pack among open-source options for tool calling. But as always, there’s a trade-off; you’ll need to balance between accuracy and latency when designing your application: Even the 8B version of Qwen3 outperformed any other local model: Higher-accuracy models take longer, often significantly.Tool calling is core to almost every real-world GenAI application. Whether you’re building agents or creating agentic workflows, your LLM must know when to act and how. Thanks to this simple framework, “We don’t know which model to pick” became “We’ve narrowed it down to three great options, each with clear pros and cons.”]]></content:encoded></item><item><title>Still Running Vulnerable Log4j Instances?</title><link>https://devops.com/still-running-vulnerable-log4j-instances/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=still-running-vulnerable-log4j-instances</link><author>Ofer Regev</author><category>devops</category><pubDate>Mon, 30 Jun 2025 11:07:54 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Serverless CI/CD: Redefining Continuous Delivery in the Modern DevOps Era</title><link>https://devops.com/serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era</link><author>Harikrishna Kundariya</author><category>devops</category><pubDate>Mon, 30 Jun 2025 10:54:43 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How DevOps Services Improve Software Delivery and Quality</title><link>https://devops.com/how-devops-services-improve-software-delivery-and-quality/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-devops-services-improve-software-delivery-and-quality</link><author>Vinay Pasilkar</author><category>devops</category><pubDate>Mon, 30 Jun 2025 09:55:46 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Revolutionizing CI/CD: A Framework for Integrating Generative AI Across the Software Delivery Lifecycle</title><link>https://devops.com/revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle</link><author>Anirban Biswas</author><category>devops</category><pubDate>Mon, 30 Jun 2025 09:32:40 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Software Migrations Fail: It’s Not the Code</title><link>https://devops.com/why-software-migrations-fail-its-not-the-code/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=why-software-migrations-fail-its-not-the-code</link><author>Nishil Macwan</author><category>devops</category><pubDate>Mon, 30 Jun 2025 08:53:33 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Harnessing AI and Automation for the Future of Innovation in DevOps</title><link>https://devops.com/harnessing-ai-and-automation-for-the-future-of-innovation-in-devops/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=harnessing-ai-and-automation-for-the-future-of-innovation-in-devops</link><author>Tony Barbagallo</author><category>devops</category><pubDate>Mon, 30 Jun 2025 07:28:22 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Self-Driving Help Desk: Agentic AI’s Role in the Next DevOps Era</title><link>https://devops.com/the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era</link><author>Venkat Thiruvengadam</author><category>devops</category><pubDate>Mon, 30 Jun 2025 07:13:14 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>