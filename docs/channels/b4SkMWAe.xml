<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>ChatLoopBackOff: Episode 68 (KANISTER)</title><link>https://www.youtube.com/watch?v=-dy_J3VmmOg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/-dy_J3VmmOg?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 22:51:06 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Carlos Santana dives into Kanister, the open source framework for application-level data management on Kubernetes.

Carlos will be exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that highlight how Kanister helps extend Kubernetes for backup, recovery, and data operations.

If you’re curious about how cloud native projects approach complex data management challenges, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.

Bring your questions, share your experiences, and learn in real time as we explore Kanister together!]]></content:encoded></item><item><title>Blind &amp; Visually Impaired Initiative (BVI) Meeting - 2025-08-19</title><link>https://www.youtube.com/watch?v=bJej44Ug8tU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/bJej44Ug8tU?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 20:16:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>SRE.ai Looks to Unify DevOps Workflows Across Multiple SaaS Applications</title><link>https://devops.com/sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 22 Aug 2025 19:50:26 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Digest #177: AWS in 2025, HashiCorp Vault Zero-Day Flaws, Why No SRE, Docker-Compose Tricks</title><link>https://www.devopsbulletin.com/p/digest-177-aws-in-2025-hashicorp</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/DU7_MQmRDUs" length="" type=""/><pubDate>Fri, 22 Aug 2025 17:03:35 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!AWS has changed more than you think: EC2 roles and EBS volumes can now be updated live, S3 is consistent and encrypted by default, and tools like VPC Lattice are simplifying networking. Netflix revealed how they still run mostly on Java with Spring Boot and GraphQL, while researchers uncovered nine zero-day flaws in HashiCorp Vault. And if you’ve been debating an SRE team, there’s a fresh take on why you might not need one.On the tutorial front: learn how to set up a safe malware-analysis lab on AWS, try Docker Compose tricks to speed up your workflow, or build a tiny Python agent in ~70 lines. You’ll also find guides on branching strategies, a serverless chat room with AWS, OpenTelemetry configuration gotchas, deploying Tetris on ECS, plus practical explainers on Bash, DBMS, Kitty terminal tweaks, and Copilot instructions.Our open-source spotlight features Rendergit to flatten repos into HTML, Zizmor for scanning GitHub Actions, and Data Formulator for AI-powered charting. tfclean tidies Terraform configs, Runecs manages ECS tasks, and ChartDB turns schemas into shareable diagrams.All this and more in this week’s DevOps Bulletin, don’t miss out!📘 New Book: The material comes straight from years of building a FinOps platform for Fortune-500 engineering teams, thousands of AWS, Azure, and GCP accounts, petabytes of data, and enough untagged resources to make a CFO cry.Along the way, I kept a lab notebook of what actually worked and, more importantly, what didn’t. That notebook turned into this book.📚 Grab  with 50% off (early access): here any git repo into a single static HTML page for humans or LLMs. is a static analysis tool for GitHub Actions. It can find many common security issues in typical GitHub Actions CI/CD setups. can transform data and create rich visualizations iteratively with AI. is a tool for cleaning up Terraform configuration files by automatically removing applied, moved, imported, and removed blocks. allows you to run tasks and manage your services on AWS ECS. is a database diagramming tool that enables you to visualize and design your database with a single query. protects your codebase by controlling who can change what. Set authorization levels, lock down files, and enforce your rules. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Set up a Playwright Browser Server in AWS EC2</title><link>https://blog.devops.dev/set-up-a-playwright-browser-server-in-aws-ec2-7f5ccb9819f3?source=rss----33f8b2d9a328---4</link><author>th@n@n</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:35:18 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Transforming development and test workflows can save time. In this guide, we’ll explore setting up a Playwright browser server on an AWS EC2 instance. Imagine developers and QA teams pushing many commits. Each event triggers a QA test build. Inefficiency lurks here — every build downloads images and installs browsers. Exhausting, isn’t it?But consider this: a centralised Playwright browser server. It smoothens the process, boosting efficiency. Now, envision your SRE team. They want to monitor network calls, keep an eye on application performance, and collect diverse metrics. A singular point for all this data is crucial.Finally, let’s break it down. We compare Playwright’s launch and launch server modes.Alright..!!!, Let’s see the implementation, there are some prerequisitesCreate an instance in EC2Pull the playwright image and execute the container with browser serverCreate a simple instance as per your wish, don’t forget to download the pem files for ssh connection, used a free tier option.We need to add the port 3000 inbound rules, I explain the reason later in the blog. Apart from I enable port 22 to my IP for ssh connection in order to install any softwares.Connect to your instance, SSH the key. Here’s the commandssh -i "<file_name>.pem" ec2-user@<ip address or machine name>Update the linux system Install docker sudo yum install -y dockerLet’s start the docker service sudo service docker startCheck the status of docker sudo service docker statusSince docker is installed as root user, we need to add ec2-user to the docker group sudo usermod -a -G docker ec2-userNow run the below commanddocker run -d \  --name pw-server \  -p 3000:3000 \  mcr.microsoft.com/playwright:v1.54.0-jammy \  /bin/sh -c "cd /home/pwuser && npx -y playwright@1.54.0 run-server --port 3000"Now check the status of the image and containerLet’s check the whether this browser server is available publicly, run the below command[ec2-user@thanan ~]$ curl -I http://<DNS address>:3000/HTTP/1.1 200 OKDate: Sat, 16 Aug 2025 11:32:29 GMTConnection: keep-aliveNote: Now this is available via Time to execute the test script, go to your playwright framework, since we run the browser server in docker, make sure the script are run in headless mode. I am refer this repo for code execution. Update the below configuration globally in playwright.config.tsIf you want to execute the scripts in head mode, we need `XServer` running in server, due to security reason, org won’t easily install this packageTime to test! Run this command:PW_TEST_CONNECT_WS_ENDPOINT=http://<DNS Host address>:3000/ npx playwright test --project=UI --grep @ui-oneEagerly, await results. They pop as the execution concludes, success in each run.From configuring the instance and opening the necessary ports to installing Docker and deploying the Playwright image, While there are still opportunities for improvement, such as handling IP address changes with each instance restart and the need to redeploy the Playwright container upon reboot, this blog serves as a foundational guide.If you like this content, 👏👏👏 here]]></content:encoded></item><item><title>A Deep Dive into OTA Update Support with KubeEdge for Edge Kubernetes</title><link>https://blog.devops.dev/a-deep-dive-into-ota-update-support-with-kubeedge-for-edge-kubernetes-690da554b727?source=rss----33f8b2d9a328---4</link><author>M Mahdi Ramadhan, M. Si</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:52 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[I have always been deeply inspired by the orchestration capabilities of Kubernetes. Over the years, many of the digital platforms I have designed and built were intentionally directed toward Kubernetes as the backbone — though always with careful consideration of context, to avoid making it overkill where simpler solutions would suffice.That said, one area where Kubernetes reveals its true potential is in . The proliferation of IoT devices and the exponential growth of real-time data processing demands have reshaped the computing paradigm. While centralized cloud data centers remain critical, the need for ultra-low latency, bandwidth efficiency, and local autonomy has pushed workloads closer to where data is generated: the edge.This shift introduces unique challenges — particularly when managing and updating thousands of heterogeneous, geographically distributed devices. Standard Kubernetes, while powerful, is inherently cloud-centric and often too heavy for resource-constrained edge environments.This is where , an open-source CNCF project, comes in. It extends native Kubernetes capabilities to the edge, enabling orchestrated container workloads, data synchronization, and device lifecycle management. One of its most transformative features is its support for Over-the-Air (OTA) updates — a critical mechanism to ensure that containerized applications, edge runtimes, and even firmware on physical devices can be securely updated without manual intervention.In this article, I want to share a deep, experience-based exploration of how KubeEdge enables OTA updates in practice. I will also include a , a , and a  highlighting pitfalls I have encountered and the mitigation strategies that work in real-world deployments.From Cloud-Centric to Edge-Native OrchestrationTraditional cloud computing assumes stable, high-bandwidth connections. But in edge scenarios such as autonomous driving, industrial IoT, or smart agriculture, connectivity may be intermittent, while latency requirements are unforgiving.Standard Kubernetes is not edge-ready by default. It assumes persistent cloud connectivity, strong compute nodes, and central control. In contrast:Edge devices often run on ARM processors with limited resources.Connectivity can drop unexpectedly for hours or days.Devices may require direct hardware interaction (sensors, actuators, PLCs).KubeEdge was designed to address these gaps by splitting Kubernetes into two logical planes:CloudCore – runs in the cloud or a central data center; manages policies, device models, OTA jobs, and workload orchestration.EdgeCore – runs on the edge device; autonomously executes workloads, manages hardware states, and persists operations even if the cloud connection is lost.OTA: The Linchpin of Edge Device Lifecycle ManagementIn real-world edge deployments, the cost of physically updating devices is prohibitive. Imagine dispatching technicians to update 10,000 industrial sensors across factories or retail kiosks across 500 stores. Without OTA, such operations are not feasible.KubeEdge addresses this with a multi-layered OTA mechanism:1. Containerized Application OTAApplications deployed as Kubernetes workloads.Updated through declarative manifests (Deployment, DaemonSet).Rolling updates orchestrated at the edge, resilient to connectivity drops.Managed via Device CRDs (Custom Resource Definitions).Desired firmware version defined in the Device spec.EdgeCore (or a custom mapper) pulls binaries, validates checksums/signatures, flashes the firmware, and updates the status back to CloudCore.3.	Node OTA (EdgeCore / runtime upgrade)Using NodeUpgradeJob (introduced in v1.19+).Securely upgrades KubeEdge edge components, ensuring backward compatibility and checksum validation before activation.Together, these cover the full lifecycle of an edge system: from the application layer, down to the firmware and edge runtime itself.Case Study: Smart Retail Kiosk FleetA retail enterprise deployed 5,000 smart kiosks nationwide. Each kiosk included:Containerized applications for payment, ads display, and inventory sync.Firmware-controlled peripherals (barcode scanner, receipt printer).An edge runtime running KubeEdge EdgeCore.Connectivity: Some kiosks connected over 4G with frequent dropouts.Heterogeneity: Devices had different firmware baselines depending on vendor batch.Security: Regulatory requirement that every update must be signed and auditable.Defined a DeviceModel capturing properties: firmware version, display resolution, payment module version.Managed application updates by publishing new container images, updating the Deployment spec – CloudCore handled synchronization, and EdgeCore performed rolling updates locally.For firmware updates, an OTA Job resource was created with a signed binary hosted on an HTTPS OTA server. EdgeCore validated often overlooked, but based on my experience many errors occur during data transmission), updated firmware via serial flashing, and patched the Device CRD status with the new version.For EdgeCore runtime upgrades, NodeUpgradeJobs were rolled out in controlled batches (5% of nodes at a time).Updates reduced from 3 – 4 weeks manual rollout to 2 hours automated deployment.98% first-pass update success rate; failures automatically retried once connectivity was restored.OTA audit logs integrated with enterprise compliance systems, ensuring traceability.Tutorial: Building a KubeEdge OTA SetupA Kubernetes cluster (v1.25+).keadm (KubeEdge deployment tool).One VM/cloud server (cloud side), one edge node (VM or Raspberry Pi)Step 1: Install CloudCoresudo keadm init  – kubeedge-version v1.20.0keadm gettokensudo keadm join  – cloudcore-ipport <CLOUD_IP>:10000 \  --token <TOKEN> \  --kubeedge-version v1.20.0Step 3: Define a DeviceModelapiVersion: devices.kubeedge.io/v1beta1kind: DeviceModel  name: firmware-updater-model  properties:      type: stringStep 4: Define a Device InstanceapiVersion: devices.kubeedge.io/v1beta1kind: Device  name: edge-device-01  nodeName: edge-node-1    name: firmware-updater-model    - name: firmware_version      reported: "1.0.0"Step 5: Trigger an OTA Updateproperties:  - name: firmware_versionkubectl patch device edge-device-01 \  --type=merge \  -p '{"status":{"twins":{"firmware_version":{"reported":{"value":"2.0.0"}}}}}'Risk Registry: Challenges and Preparedness in KubeEdge OTA DeploymentsWhile KubeEdge provides a robust OTA mechanism, production deployments must account for inherent risks. Below is a risk registry distilled from real-world projects:OTA is both a business enabler and an attack vector – security and rollback strategies must be non-negotiable.Connectivity resilience is paramount; production rollouts must batch, monitor, and retry intelligently.Observability and compliance logging are not optional for industries subject to regulation.KubeEdge elevates Kubernetes from being a container orchestrator to becoming a full lifecycle management platform for edge computing. Its OTA support ensures that applications, firmware, and runtime components can evolve securely and reliably – at scale.From smart retail kiosks to industrial automation, OTA in KubeEdge transforms edge infrastructure from brittle, manually maintained systems into self-updating, cloud-native edge fleets.As organizations accelerate toward Industry 4.0, 5G, and intelligent IoT, embracing KubeEdge’s OTA capabilities is not just an optimization – it is a necessity for resilience, security, and scalability at the edge.]]></content:encoded></item><item><title>Can Sales Data Predict Stock Prices? A Data Science Experiment with Python</title><link>https://blog.devops.dev/can-sales-data-predict-stock-prices-a-data-science-experiment-with-python-2b0d54d711f0?source=rss----33f8b2d9a328---4</link><author>Vaishnavi Ganeshkar</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:48 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[What if a company’s sales performance could predict its stock price? We hear investors talking about revenue, growth, and quarterly reports all the time — but can we actually see the connection in data?In this blog, we’ll run a fun data experiment: analyzing  and  side by side using Python. By the end, you’ll know whether better sales = higher stock prices, and what this means for data analysts, businesses, and investors.: Downloaded with the yfinance library (Yahoo Finance API).This gives us a way to compare company’s sales performance (business side) with stock market valuation (investor side).⚙️ Step 1: Setup Environment!pip install pandas matplotlib seaborn yfinanceimport pandas as pdimport matplotlib.pyplot as pltimport yfinance as yf📂 Step 2: Load Sales Data# Load sales datasetsales = pd.read_csv("sales_data_sample.csv", encoding="latin1")sales['ORDERDATE'] = pd.to_datetime(sales['ORDERDATE'])We’ll use  and  for monthly aggregation.📈 Step 3: Monthly Sales Trendmonthly_sales = sales.groupby(pd.Grouper(key="ORDERDATE", freq="M"))['SALES'].sum()plt.figure(figsize=(10,5))monthly_sales.plot()plt.title("Monthly Sales Trend")plt.xlabel("Month")plt.ylabel("Total Sales")plt.show()This gives us a clear view of how sales are moving over time.💹 Step 4: Fetch Stock Market DataLet’s pick  for our analysis.start_date = sales['ORDERDATE'].min().strftime("%Y-%m-%d")end_date = sales['ORDERDATE'].max().strftime("%Y-%m-%d")stock = yf.download("AAPL", start=start_date, end=end_date)stock['Close'].plot(figsize=(10,5), title="Apple Stock Price")🔄 Step 5: Merge Sales & Stock Data# Monthly Salesmonthly_sales = sales.groupby(pd.Grouper(key="ORDERDATE", freq="M"))['SALES'].sum()stock_monthly = stock['Close'].resample('M').mean()df = pd.concat([monthly_sales, stock_monthly], axis=1)df.columns = ["Sales", "StockPrice"]print(df.dropna().head())   # Check after removing NaNNow we have both Sales and Stock Price in one DataFrame.📊 Step 6: Sales vs Stock Price Scatterplotsns.scatterplot(x="Sales", y="StockPrice", data=df.dropna())plt.title("Sales vs Stock Price")📌 : The scatterplot shows a . Higher sales are often paired with higher stock prices. But it’s not perfectly linear — there are plenty of scattered points.📈 Step 7: Add Regression Trendlinesns.regplot(x="Sales", y="StockPrice", data=df, scatter_kws={"s":50}, line_kws={"color":"red"})plt.title("Sales vs Stock Price with Trendline")The  shows a positive correlation → as sales increase, stock prices tend to rise.The  is the confidence interval → wide bands mean stock price is influenced by other factors beyond sales (investor sentiment, economic conditions, etc.).Some  show months where strong sales did not immediately push stock prices — reminding us that the market is not just logical, it’s psychological too.📌 Step 8: Correlation Check✅ Insight from Correlation OutputThe correlation coefficient between  and  is .This means there is a weak positive relationship between the two variables. In simple terms:As , Stock Prices also tend to increase, but not strongly.Other external factors (market sentiment, competition, economy, company news) are likely influencing stock price much more than just sales.For investors: relying only on sales to predict stock price is risky — it’s one piece of the puzzle, not the whole picture.For businesses: increasing sales  proportional stock price growth; branding, profitability, and investor confidence also matter.“From the correlation matrix, we see that Sales and Stock Price have a . This suggests a weak positive relationship — meaning higher sales may push stock prices up slightly, but the effect is not very strong. Clearly, stock prices depend on multiple factors beyond just sales numbers, such as market conditions, company reputation, and investor sentiment.”🚀 Why This Matters for YouIf you’re a  → this project strengthens your portfolio (business + finance domain).If you’re an  → remember that sales performance is one of many factors that move stock prices.If you’re just learning  → this project teaches you how to merge and analyze cross-domain datasets.This experiment shows that while sales data does impact stock performance, it’s not the . The stock market is a mix of fundamentals, sentiment, and sometimes pure unpredictability.👉 Want to go deeper? Extend this project into:Forecasting sales impact on stock (ARIMA, LSTM models)Comparing multiple companiesBuilding dashboards (Power BI, Tableau) for interactive visualization📌 The next time someone says “good sales mean good stock returns,” you’ll know the data-backed truth: ✅ That’s it! You now have a complete  with dataset, code, plots, insights, and storytelling.]]></content:encoded></item><item><title>Blue-Green Deployments: A Practical Path to Zero-Downtime Releases</title><link>https://blog.devops.dev/blue-green-deployments-a-practical-path-to-zero-downtime-releases-54c83fe0265c?source=rss----33f8b2d9a328---4</link><author>Madhura Jayashanka</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:44 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Downtime is costly. According to ITIC’s 2024 report, 90% of companies say one hour of downtime costs over , and 41% say it can reach  (PDF source).DORA’s research also shows that high-performing teams deliver quickly  stay stable. Earlier benchmarks from 2019 and 2021 defined elite teams as:Change failure rate between Time to restore service Blue-green deployments are one way to hit these goals.What is Blue-Green Deployment?It means running two production-ready environments: — the version currently serving users — the new version waiting to go liveThe process looks like this:Deploy your update to Run tests and health checksSwitch traffic from  to If something goes wrong, switch back to  instantlyZero downtime during cutoverInstant rollback if problems appearRealistic testing under production conditionsTemporary extra infrastructure capacityNeed to keep configuration and secrets in syncUptime is part of your contract or SLAReleases are risky (payments, authentication, APIs)You want a quick, binary safety switch — good for gradual rollout with percentage-based traffic — good if you can accept slower rollbackThere are three common ways to flip traffic between Blue and Green:1. Kubernetes Service selectorSwitch label selector from color=blue to color=green2. AWS ECS with ALB/CodeDeployTwo target groups (Blue and Green) behind an ALBCodeDeploy shifts traffic and can auto-rollbackDefine Blue and Green upstreamsA map variable chooses which backend is activenginx -s reload applies the changeExample: Kubernetes Blue-GreenapiVersion: v1kind: Service  name: demo-svc  selector:    color: blue   # flip to green to switch traffic    - port: 80apiVersion: apps/v1kind: Deployment  name: demo-blue  replicas: 3    matchLabels: { app: demo, color: blue }    metadata:      labels: { app: demo, color: blue }    spec:        - name: app          image: ghcr.io/yourorg/demo:1.0.0          ports: [{ containerPort: 8080 }]            httpGet: { path: /healthz, port: 8080 } is identical but with color: green and a different image tag.kubectl patch svc demo-svc -p '{"spec":{"selector":{"app":"demo","color":"green"}}}'Example: NGINX Blue-Greenupstream blue_backend  { server 10.0.0.11:8080; }upstream green_backend { server 10.0.0.12:8080; }map $http_host $active_color { default blue; }  # set to green to flipChange $active_color to green, then reload NGINX.Databases need a special strategy. The best approach is the  pattern (source):Expand schema safely (add new columns or tables)Deploy new code (works with both old and new schema)Health Checks Before SwitchingApplication readiness probe must pass consistentlyError rates and latency should match the baselineWatch CPU, memory, and a key business KPI during the switchKeep Blue running in the background for 30–120 minutes in case rollback is needed — patch Service selector back to Blue — change $active_color back to Blue and reload — trigger rollback in CodeDeploy (docs)Is blue-green the same as canary? No. Blue-green switches all traffic at once. Canary shifts traffic gradually.Does blue-green guarantee zero downtime? It aims to, as long as health probes and load balancer configs are set correctly.What is “good” stability? According to DORA, good teams keep change failure low and can restore service within about an hour.]]></content:encoded></item><item><title>The Truth About Cold Starts in Google Cloud Run &amp; Functions</title><link>https://blog.devops.dev/the-truth-about-cold-starts-in-google-cloud-run-functions-efb1c5bccfda?source=rss----33f8b2d9a328---4</link><author>Engineer</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:37 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[If you’ve been using Google Cloud Run or Cloud Functions, chances are you’ve noticed some requests feel like they’re waking up from a nap…]]></content:encoded></item><item><title>Cloudflare WAF Best Practices: Features, Challenges, and Alternatives</title><link>https://blog.devops.dev/cloudflare-waf-best-practices-features-challenges-and-alternatives-46d7da078f6e?source=rss----33f8b2d9a328---4</link><author>Maverick Steel</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:34:22 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Web Application Firewalls (WAFs) play a critical role in securing modern web applications from a wide range of threats, such as SQL injection, cross-site scripting (XSS), and other OWASP Top 10 vulnerabilities.Among various WAF solutions available today, Cloudflare WAF is one of the most widely adopted due to its robust features, ease of use, and global performance benefits.This article explores the best practices for using Cloudflare WAF, highlights some challenges, and introduces an alternative WAF solution — Safeline.Features of Cloudflare WAFCloudflare WAF includes a broad set of pre-configured security rules that cover OWASP Top 10 vulnerabilities, zero-day exploits, and protocol anomalies. These rules are frequently updated to respond to emerging threats.Cloudflare WAF can automatically learn from traffic patterns to reduce false positives, minimizing disruptions to legitimate users while enhancing security.Being part of Cloudflare’s global Content Delivery Network (CDN) gives the WAF low latency and high availability, ensuring security without sacrificing performance.Users can create custom firewall rules tailored to their application’s specific needs using Cloudflare’s intuitive dashboard or API.Cloudflare WAF integrates with bot management features to distinguish between good and bad bots, reducing malicious traffic and improving analytics.Best Practices for Using Cloudflare WAFRegularly Update Rule Sets: Keep Cloudflare’s managed rules enabled and updated to protect against newly discovered vulnerabilities.Enable Logging and Monitor Alerts: Continuously monitor WAF logs and alerts to quickly respond to potential threats or false positives.Use Custom Rules Judiciously: While custom rules offer flexibility, improper configuration can cause unintended blocks or gaps; test thoroughly.Integrate with Security Ecosystem: Combine Cloudflare WAF with other security tools like SIEMs for comprehensive threat detection and response. Start with learning mode to understand traffic patterns and adjust policies for smoother deployment.Challenges with Cloudflare WAF Despite managed rules and learning modes, some false positives still occur, potentially impacting user experience. Advanced features and higher rule set limits are available only on paid plans, which might be expensive for small businesses.Limited Deep Customization: Although flexible, Cloudflare’s WAF may not meet very niche or complex application requirements compared to specialized WAFs.Dependence on Cloudflare Network: Using Cloudflare WAF means routing all traffic through Cloudflare’s network, which can be a concern for some organizations regarding data sovereignty or vendor lock-in.Alternatives: SafeLine WAFSafeLine(https://ly.safepoint.cloud/k9fyEuu) is an emerging WAF solution that offers a compelling alternative to Cloudflare WAF, especially for enterprises seeking deeper customization and more direct control over deployment environments.Highly Customizable Rules Engine: SafeLine provides a powerful rules engine that allows fine-grained rule creation, enabling tailored protection for unique application requirements.On-Premises and Cloud Deployment: Unlike Cloudflare, Safeline supports hybrid deployments — on-premises, cloud, or edge — to meet diverse organizational policies and compliance needs.Threat Intelligence Integration: SafeLine integrates with a threat intelligence feed, enabling proactive detection of zero-day vulnerabilities and sophisticated attacks.Performance Optimization: Designed with minimal latency in mind, SafeLine optimizes traffic without relying on a global CDN network. SafeLine offers advanced bot detection capabilities that can differentiate human users from automated attacks beyond signature-based methods.When to Consider SafeLineIf your organization requires on-premises control, extensive rule customization, or has concerns related to data residency and vendor dependency, SafeLine presents a flexible and powerful alternative to Cloudflare WAF.Cloudflare WAF is a robust and widely-used solution that offers strong security features combined with the performance benefits of a global CDN. Following best practices such as updating rules and monitoring logs helps maximize its effectiveness. However, challenges like false positives and vendor dependency warrant consideration of alternatives. Safeline stands out as a strong option for businesses needing customizable, flexible deployment options alongside comprehensive web application protection.Choosing the right WAF depends on your specific requirements regarding performance, deployment, customization, and cost. Both Cloudflare WAF and Safeline provide valuable tools to safeguard your web applications in today’s evolving threat landscape.]]></content:encoded></item><item><title>Red Flags in K8s Configuration That Kills Your Apps</title><link>https://blog.devops.dev/red-flags-in-k8s-configuration-that-kills-your-apps-cb738d435989?source=rss----33f8b2d9a328---4</link><author>Devops Diaries</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:35 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Deploying apps on Kubernetes feels smooth — until something breaks in production. More often than not, the culprit isn’t Kubernetes itself…]]></content:encoded></item><item><title>AI as a Co-pilot for Developers: Boosting Productivity Without Losing Control</title><link>https://blog.devops.dev/ai-as-a-co-pilot-for-developers-boosting-productivity-without-losing-control-d5abb9e73419?source=rss----33f8b2d9a328---4</link><author>Vitor Hansen</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Artificial Intelligence (AI) is no longer just a buzzword in the tech industry, it’s becoming an essential companion for developers. From generating code snippets to reviewing pull requests and even optimizing CI/CD pipelines, AI is changing the way software is built.In this article, we’ll explore what it means to have an “AI co-pilot” in your development workflow, compare the most popular tools (ChatGPT, Claude, Gemini, and Codeium), and walk through a practical example of using AI to validate code in a CI/CD pipeline.An AI co-pilot is a system that assists developers in writing, reviewing, optimizing, and understanding code — without replacing the human in charge. Think of it as a pair programming partner that never gets tired, always has instant suggestions, and can adapt to different coding styles.AI co-pilots come in two main forms:Context-aware code assistants — Integrated into IDEs (VS Code, JetBrains, etc.) to provide inline suggestions and documentation.Conversational AI assistants — Chat interfaces where you can paste code, ask for explanations, or request specific implementations.The best co-pilots blend both approaches, letting you move between coding, reviewing, and designing without breaking flow.2. Core Use Cases for DevelopersAI co-pilots can be applied across the entire development lifecycle. Here are some of the most common scenarios:Creating boilerplate code quickly.Suggesting implementation details based on comments.Generating functions in multiple languages.Highlighting potential bugs and security risks.Suggesting better algorithms or data structures.Pointing out unused imports or redundant code.Explaining cryptic error messages.Suggesting possible fixes for runtime issues.Reproducing bugs based on logs.Improving performance by refactoring loops, queries, or memory usage.Suggesting more efficient libraries or APIs.Reducing complexity for maintainability.Generating docstrings and README files.Creating developer onboarding guides from existing code.Explaining APIs for both internal and public use.3. Hands-On Example: AI in a CI/CD PipelineLet’s see how an AI co-pilot can help in a Continuous Integration / Continuous Deployment setup. You want every pull request in your repository to be automatically reviewed for:Instead of manually going through every file, you integrate an AI service in your CI pipeline.Example with GitHub Actions + OpenAI API:name: AI Code Review  pull_request:  ai-review:    steps:      - name: Checkout repository        uses: actions/checkout@v3      - name: Install dependencies        run: npm install      - name: Run AI Code Review        env:          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}        run: |          node scripts/ai-review.jsimport OpenAI from "openai";import fs from "fs";const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });const files = ["src/index.js", "src/utils/helpers.js"];  for (const file of files) {    const code = fs.readFileSync(file, "utf8");    const prompt = `      Review the following JavaScript code for:      1. Potential security issues      2. Code smells or anti-patterns      3. Missing test coverage      Provide feedback in a concise bullet-point list.      ${code}    const response = await openai.chat.completions.create({      messages: [{ role: "user", content: prompt }],    console.log(`Review for ${file}:`);    console.log(response.choices[0].message.content);  }This simple setup can automatically leave AI-generated feedback as part of the PR process, helping developers catch issues earlier.5. Challenges and LimitationsAI co-pilots are powerful, but they’re not magic. Key challenges include: — Code sent to third-party APIs might expose sensitive info. — AI suggestions can be wrong or insecure. — Large codebases may exceed prompt size limits. — Heavy usage can increase expenses. — Relying too much on AI may reduce skill retention.6. Best Practices for Using AI in Development — Never merge AI-generated code without review.Keep Sensitive Data Local — Use self-hosted models for confidential projects. — The better your instructions, the better the output. — Let AI suggest, but tests confirm. — Use PR labels to flag AI-assisted commits.7. The Future of AI Co-pilotsWe’re moving towards a reality where AI will handle more repetitive coding tasks, freeing developers to focus on problem-solving and architecture. The winning strategy isn’t to fight AI, it’s to .The best developers of tomorrow will be those who can combine human creativity with AI efficiency. AI isn’t here to replace developers, it’s here to make us . By understanding its strengths, limits, and best practices, you can turn AI into a genuine co-pilot that helps you ship faster, with higher quality, and less stress.]]></content:encoded></item><item><title>Scaling Your Application Resiliently with AWS: From Zero to Millions of Requests</title><link>https://blog.devops.dev/scaling-your-application-resiliently-with-aws-from-zero-to-millions-of-requests-611648b92eb5?source=rss----33f8b2d9a328---4</link><author>Vitor Hansen</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:33:25 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[A strategic and technical guide for architects and engineers aiming to build modern, resilient systems designed for growth.Scalability is a critical challenge every successful software system faces. Too often, teams only address scalability issues after significant growth, leading to downtime, poor user experiences, and costly refactoring. I’ve witnessed first-hand systems crashing under load due to inadequate architecture planning. This post emphasizes the importance of building scalability and resilience from the ground up.2. When (and Why) to Scale?First, let’s clarify the distinction:  is about how fast your system handles individual requests, while  is about how well your system handles increasing volume.You know it’s time to scale when:Response times are increasingServers are consistently maxed outMonitoring tools like CloudWatch, X-Ray, or Sentry show consistent bottlenecksScaling is not just a technical necessity, it’s a business enabler. Building for scale means your system can grow with your users, rather than crumble under their weight.3. Modern Scalable Architecture on AWSToday’s cloud architectures leverage key AWS components to build scalable and resilient systems: Manages, secures, and routes API requests. Serverless compute, automatically scaling and handling workloads. Managed container orchestration that removes infrastructure management overhead. Highly available relational databases. NoSQL database offering high performance and scalability. Queue management ensuring reliable asynchronous processing. Event-driven communication between services. Improves system performance by caching frequently accessed data.This setup ensures both horizontal scalability and fault tolerance across all layers.Consider a common scenario: an application receiving public API requests and processing financial transactions: Receives requests, applies rate limiting, and manages authentication. Processes incoming requests, scales automatically based on demand. Handles longer-running processes and workflows. Stores transactional and rapidly accessed data. Stores structured data requiring strong consistency. Manages communication and workflows asynchronously.This architecture scales seamlessly, maintaining consistent performance and resilience even under heavy load.Avoid these mistakes when designing scalable AWS systems:Ignoring cold starts in Lambda: Minimize with smaller functions and Provisioned Concurrency.Poor scaling configuration: Test and validate your auto-scaling policies regularly. Use DynamoDB or caching where eventual consistency is acceptable. Avoid direct service-to-service calls without message buses like SQS/EventBridge.6. Golden Tips for ResilienceEnhance your system’s resilience with these best practices:Implement retries and Dead Letter Queues (DLQ) with Amazon SQS.Apply the Circuit Breaker pattern with API Gateway and Lambda to prevent cascading failures.Use Canary Deployments for safe, gradual rollouts of Lambda functions and ECS services.Scalability isn’t just about adding infrastructure; it’s primarily about thoughtful architecture design. Building scalable systems from the start ensures your business can adapt quickly to growth without sacrificing reliability or performance. AWS, Architecture, Scalability, Serverless, Fargate, Lambda, Best Practices, Cloud]]></content:encoded></item><item><title>How to Set up GitHub Actions for a Node.js project</title><link>https://blog.devops.dev/how-to-setup-github-actions-for-node-js-project-1edd6ce1dbe1?source=rss----33f8b2d9a328---4</link><author>Mohammad Faisal Khatri</author><category>devops</category><pubDate>Fri, 22 Aug 2025 15:32:51 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tackling the DevSecOps Gap in Software Understanding</title><link>https://devops.com/tackling-the-devsecops-gap-in-software-understanding/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=tackling-the-devsecops-gap-in-software-understanding</link><author>Alan Shimel</author><category>devops</category><pubDate>Fri, 22 Aug 2025 02:45:22 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 67 (Kserve)</title><link>https://www.youtube.com/watch?v=BjXZxUR8NMo</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/BjXZxUR8NMo?version=3" length="" type=""/><pubDate>Thu, 21 Aug 2025 20:33:45 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Shivay Lamaba dives into KServe, the open source project designed for scalable and reliable ML models serving on Kubernetes.

Shivay will be  exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that make KServe a key piece in the cloud native AI/ML ecosystem.
If you’re curious about production-grade model inference, want to see how cloud native communities approach machine learning workloads, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.

Bring your questions, share your experiences, and learn in real time as we explore KServe together!]]></content:encoded></item><item><title>White Paper: The Future of DevSecOps in a Fully Autonomous CI/CD Pipeline</title><link>https://devops.com/white-paper-the-future-of-devsecops-in-a-fully-autonomous-ci-cd-pipeline/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=white-paper-the-future-of-devsecops-in-a-fully-autonomous-ci-cd-pipeline</link><author>Ravi Shanker Sharma</author><category>devops</category><pubDate>Thu, 21 Aug 2025 17:12:05 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MCP Emerges as a Catalyst for Modern DevOps Processes</title><link>https://devops.com/mcp-emerges-as-a-catalyst-for-modern-devops-processes/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mcp-emerges-as-a-catalyst-for-modern-devops-processes</link><author>Mike Vizard</author><category>devops</category><pubDate>Thu, 21 Aug 2025 16:53:01 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How AI-Created Code Will Strain DevOps Workflows</title><link>https://devops.com/how-ai-created-code-will-strain-devops-workflows/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-ai-created-code-will-strain-devops-workflows</link><author>Mike Vizard</author><category>devops</category><pubDate>Thu, 21 Aug 2025 16:30:04 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Prototyping an AI Tutor with Docker Model Runner</title><link>https://www.docker.com/blog/how-to-build-an-ai-tutor-with-model-runner/</link><author>Sarah Sanders</author><category>docker</category><category>devops</category><pubDate>Thu, 21 Aug 2025 15:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Every developer remembers their first docker run hello-world. The mix of excitement and wonder as that simple command pulls an image, creates a container, and displays a friendly message. But what if AI could make that experience even better?As a technical writer on Docker’s Docs team, I spend my days thinking about developer experience. Recently, I’ve been exploring how AI can enhance the way developers learn new tools. Instead of juggling documentation tabs and ChatGPT windows, what if we could embed AI assistance directly into the learning flow? This led me to build an interactive AI tutor powered by  as a proof of concept.The Case for Embedded AI TutorsThe landscape of developer education is shifting. While documentation remains essential, we are seeing more developers coding alongside AI assistants. But context-switching between your terminal, documentation, and an external AI chat breaks concentration and flow. An embedded AI tutor changes this dynamic completely.Imagine learning Docker with an AI assistant that:Lives alongside your development environmentMaintains context about what you’re trying to achieveResponds quickly without network latencyKeeps your code and questions completely privateThis isn’t about replacing documentation. It’s about offering developers a choice in how they learn. Some prefer reading guides, others learn by doing, and increasingly, many want conversational guidance through complex tasks.To build the AI tutor, I kept the architecture rather simple:The is a React app with a chat interface. Nothing fancy, just a message history, input field, and loading states.The is an /api/chat endpoint that forwards requests to the local LLM through OpenAI-compatible APIs.The powering it all is where Docker Model Runner comes in. Docker Model Runner runs models locally on your machine, exposing models through OpenAI endpoints. I decided to use Docker Model Runner because it promised local development and fast iteration.The  was designed with running docker run hello-world in mind:You are a Docker tutor with ONE SPECIFIC JOB: helping users run their first "hello-world" container.

YOUR ONLY TASK: Guide users through these exact steps:
1. Check if Docker is installed: docker --version
2. Run their first container: docker run hello-world
3. Celebrate their success

STRICT BOUNDARIES:
- If a user says they already know Docker: Respond with an iteration of "I'm specifically designed to help beginners run their first container. For advanced help, please review Docker documentation at docs.docker.com or use Ask Gordon."
- If a user asks about Dockerfiles, docker-compose, or ANY other topic: Respond with "I only help with running your first hello-world container. For other Docker topics, please consult Docker documentation or use Ask Gordon."
- If a user says they've already run hello-world: Respond with "Great! You've completed what I'm designed to help with. For next steps, check out Docker's official tutorials at docs.docker.com."

ALLOWED RESPONSES:
- Helping install Docker Desktop (provide official download link)
- Troubleshooting "docker --version" command
- Troubleshooting "docker run hello-world" command
- Explaining what the hello-world output means
- Celebrating their success

CONVERSATION RULES:
- Use short, simple messages (max 2-3 sentences)
- One question at a time
- Stay friendly but firm about your boundaries
- If users persist with off-topic questions, politely repeat your purpose

EXAMPLE BOUNDARY ENFORCEMENT:
User: "Help me debug my Dockerfile"
You: "I'm specifically designed to help beginners run their first hello-world container. For Dockerfile help, please check Docker's documentation or Ask Gordon."

Start by asking: "Hi! I'm your Docker tutor. Is this your first time using Docker?"

Setting Up Docker Model RunnerGetting started with Docker Model Runner proved straightforward. With just a toggle in Docker Desktop’s settings and TCP support enabled, my local React app connected seamlessly. The setup delivered on Docker Model Runner’s promise of simplicity.During initial testing, the model performed well. I could interact with it through the OpenAI-compatible endpoint, and my React frontend connected without requiring modifications or fine-tuning. I had my prototype up and running in no time.To properly evaluate the AI tutor, I approached it from two paths. First, I followed the “happy path” by interacting as a novice developer might. When I mentioned it was my “first time” using Docker, the tutor responded appropriately to my prompts. It walked me through checking if Docker was installed using my terminal before running my container. Next, I ventured down the “unhappy path” to test the tutor’s boundaries. Acting as an experienced developer, I attempted to push beyond basic container operations. The AI tutor maintained its focus and stayed within its designated scope.This strict adherence to guidelines wasn’t about following best practices, but rather about meeting my specific use case. I needed to prototype an AI tutor with clear guardrails that served a single, well-defined purpose. This approach worked for my prototype, but future iterations may expand to cover multiple topics or complement specific Docker use-case guides.Reflections on Docker Model RunnerDocker Model Runner delivered on its core promise: making AI models accessible through familiar Docker workflows. The vision of models as first-class citizens in the Docker ecosystem proved valuable for rapid local prototyping. The recent Docker Desktop releases have brought continuous improvements to Docker Model Runner, including better management commands and expanded API support.What worked really well for me:Native integration with Docker Desktop, a tool I use all day, every dayOpenAI-compatible APIs that require no frontend modificationsGPU acceleration support for faster local inferenceMore than anything, simplicity is its standout feature. Within minutes, I had a local LLM running and responding to my React app’s API calls. The speed from idea to working prototype is exactly what developers need when experimenting with AI tools.This prototype proved that embedded AI tutors aren’t just an idea, they’re a practical learning tool. Docker Model Runner provided the foundation I needed to test whether contextual AI assistance could enhance developer learning.For anyone curious about Docker Model Runner:The tool is mature enough for meaningful experiments, and the setup overhead is minimal.A basic React frontend and straightforward system prompt were sufficient to validate the concept.Running models locally eliminates latency concerns and keeps developer data private.Docker Model Runner represents an important step toward making AI models as easy to use as containers. While my journey had some bumps, the destination was worth it: an AI tutor that helps developers learn.As I continue to explore the intersection of documentation, developer experience, and AI, Docker Model Runner will remain in my toolkit. The ability to spin up a local model as easily as running a container opens up possibilities for intelligent, responsive developer tools. The future of developer experience might just be a docker model run away.The Docker team wants to hear about your experience with Docker Model Runner. Share what’s working, what isn’t, and what features you’d like to see. Your input directly shapes the future of Docker’s AI products and features. Share feedback with Docker.]]></content:encoded></item><item><title>Enterprise AI Development Gets a Major Upgrade: Claude Code Now Bundled with Team and Enterprise Plans</title><link>https://devops.com/enterprise-ai-development-gets-a-major-upgrade-claude-code-now-bundled-with-team-and-enterprise-plans/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=enterprise-ai-development-gets-a-major-upgrade-claude-code-now-bundled-with-team-and-enterprise-plans</link><author>Tom Smith</author><category>devops</category><pubDate>Thu, 21 Aug 2025 14:54:59 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>HoundDog.ai Code Scanner Shifts Data Privacy Responsibility Left</title><link>https://devops.com/hounddog-ai-code-scanner-shifts-data-privacy-responsibility-left/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=hounddog-ai-code-scanner-shifts-data-privacy-responsibility-left</link><author>Mike Vizard</author><category>devops</category><pubDate>Thu, 21 Aug 2025 14:00:40 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Futurum Signal is Live: Cutting Through the DevOps Noise</title><link>https://devops.com/futurum-signal-is-live-cutting-through-the-devops-noise/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=futurum-signal-is-live-cutting-through-the-devops-noise</link><author>Alan Shimel</author><category>devops</category><pubDate>Wed, 20 Aug 2025 17:05:36 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LLM-D, with Clayton Coleman and Rob Shaw</title><link>http://sites.libsyn.com/419861/llm-d-with-clayton-coleman-and-rob-shaw</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD258.mp3?dest-id=3486674" length="" type=""/><pubDate>Wed, 20 Aug 2025 13:24:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[Guests are Clayton Coleman and Rob Shaw. Clayton is a Core contributor to Kubernetes, the containerized cluster manager, and founding architect for OpenShift, the open source platform as a service. Clayton helped launch the shift to cloud native applications and the platforms that enable them. At Google my mission is to make Kubernetes and GKE the best place to run workloads, especially accelerated AI/ML workloads, and especially especially very large model inference at scale with the inference gateway and llm-d. Rob Shaw is an Engineering Director at Redhat and is a contributor to the vLLM project.Do you have something cool to share? Some questions? Let us know: News of the week  Links from the interview ]]></content:encoded></item><item><title>The Supply Chain Paradox: When “Hardened” Images Become a Vendor Lock-in Trap</title><link>https://www.docker.com/blog/hardened-container-images-security-vendor-lock-in/</link><author>Michael Donovan</author><category>docker</category><category>devops</category><pubDate>Wed, 20 Aug 2025 13:10:09 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[The market for pre-hardened container images is experiencing explosive growth as security-conscious organizations pursue the ultimate efficiency: instant security with minimal operational overhead. The value proposition is undeniably compelling—hardened images with minimal dependencies promise security “out of the box,” enabling teams to focus on building and shipping applications rather than constantly revisiting low-level configuration management.For good reason, enterprises are adopting these pre-configured images to reduce attack surface area and simplify security operations. In theory, hardened images deliver reduced setup time, standardized security baselines, and streamlined compliance validation with significantly less manual intervention.Yet beneath this attractive surface lies a fundamental contradiction. While hardened images can genuinely reduce certain categories of supply chain risk and strengthen security posture, they simultaneously create a more subtle form of vendor dependency than traditional licensing models. Organizations are unknowingly building critical operational dependencies on a single vendor’s design philosophy, build processes, institutional knowledge, responsiveness, and long-term market viability.The paradox is striking: in the pursuit of supply chain independence, many organizations are inadvertently creating more concentrated dependencies and potentially weakening their security through stealth vendor lock-in that becomes apparent only when it’s costly to reverse.The Mechanics of Modern Vendor Lock-inUnfamiliar Base Systems Create Switching FrictionThe first layer of lock-in emerges from architectural choices that seem benign during initial evaluation but become problematic at scale. Some hardened image vendors deviate from mainstream distributions, opting to bake their own Linux variants rather  than offering widely-adopted options like Debian, Alpine, or Ubuntu. This deviation creates immediate friction for platform engineering teams who must develop vendor-specific expertise to effectively manage these systems. Even if the differences are small, this raises the spectre of edge-cases – the bane of platform teams. Add enough edge cases and teams will start to fear adoption.While vendors try to standardize their approach to hardening, in reality, it remains a bespoke process. This can create differences from image to image across different open source versions, up and down the stack – even from the same vendor. In larger organizations, platform teams may need to offer hardened images from multiple vendors. This creates further compounding complexity. In the end, teams find themselves managing a heterogeneous environment that requires specialized knowledge across multiple proprietary approaches. This increases toil, adds risk, increases documentation requirements and raises the cost of staff turnover.Compatibility Barriers and Customization ConstraintsMore problematic is how hardened images often break compatibility with standard tooling and monitoring systems that organizations have already invested in and optimized. Open source compatibility gaps emerge when hardened images introduce modifications that prevent seamless integration with established DevOps workflows, forcing organizations to either accept reduced functionality or invest in vendor-specific alternatives.Security measures, while well-intentioned, can become so restrictive they prevent necessary business customizations. Configuration lockdown reaches levels where platform teams cannot implement organization-specific requirements without vendor consultation or approval, transforming what should be internal operational decisions into external dependencies.Perhaps most disruptive is how hardened images force changes to established CI/CD pipelines and operational practices. Teams discover that their existing automation, deployment scripts, and monitoring configurations require substantial modification to accommodate the vendor’s approach to security hardening.The vendor lock-in trap becomes most apparent when organizations attempt to change direction. While vendors excel at streamlining initial adoption—providing migration tools, professional services, and comprehensive onboarding support—they systematically downplay the complexity of eventual exit scenarios.Organizations accumulate sunk costs through investments in training and vendor-specific tooling that create psychological and financial barriers to switching providers. More critically, expertise about these systems becomes concentrated within vendor organizations rather than distributed among internal teams. Platform engineers find themselves dependent on vendor documentation, support channels, and institutional knowledge to troubleshoot issues and implement changes.The Open Source Transparency ProblemThe hardened image industry leverages the credibility of open source. But it can also undermine the spirit of open source transparency by creating almost a kind of fork but without the benefits of community.. While vendors may provide source code access, this availability doesn’t guarantee system understanding or maintainability. The knowledge required to comprehend complex hardening processes often remains concentrated within small vendor teams, making independent verification and modification practically impossible.Heavily modified images become difficult for internal teams to audit and troubleshoot. Platform engineers encounter systems that appear familiar on the surface but behave differently under stress or during incident response, creating operational blind spots that can compromise security during critical moments.Trust and Verification GapsTransparency is only half the equation. Security doesn’t end at a vendor’s brand name or marketing claims. Hardened images are part of your production supply chain and should be scrutinized like any other critical dependency. Questions platform teams should ask include:How are vulnerabilities identified and disclosed? Is there a public, time-bound process, and is it tied to upstream commits and advisories rather than just public CVEs?Could the hardening process itself introduce risks through untested modifications?Have security claims been independently validated through audits, reproducible builds, or public attestations?Does your SBOM meta-data accurately reflect the full context of your hardened image? Transparency plus verification and full disclosure builds durable trust. Without both, hardened images can be difficult to audit, slow to patch, and nearly impossible to replace. Not providing easy-to-understand and easy-to-consume verification artefacts and answers functions as a form of lock-in forcing the customer to trust but not allowing them to verify.Building Independence: A Strategic FrameworkFor platform teams that want to benefit from the security gains of hardened images and reap ease of use while avoiding lock-in, taking a structured approach to hardened vendor decision making is critical.Distribution Compatibility as FoundationPlatform engineering leaders must establish mainstream distribution adherence as a non-negotiable requirement. Hardened images should be built from widely-adopted distributions like Debian, Ubuntu, Alpine, or RHEL rather than vendor-specific variants that introduce unnecessary complexity and switching costs.Equally important is preserving compatibility with standard package managers and maintaining adherence to the Filesystem Hierarchy Standard (FHS) to preserve tool compatibility and operational familiarity across teams. Key requirements include:Package manager preservation: Compatibility with standard tools (apt, yum, apk) for independent software installation and updates File system layout standards: Adherence to FHS for seamless integration with existing toolingLibrary and dependency compatibility: No proprietary dependencies that create additional vendor lock-inEnabling Rapid Customization Without Security CompromiseSecurity enhancements should be architected as modular, configurable layers rather than baked-in modifications that resist change. This approach allows organizations to customize security posture while maintaining the underlying benefits of hardened configurations.Built-in capability to modify security settings through standard configuration management tools preserves existing operational workflows and prevents the need for vendor-specific automation approaches. Critical capabilities include:: Security enhancements as removable, configurable componentsConfiguration override mechanisms: Integration with standard tools (Ansible, Chef, Puppet)Whitelist-based customization: Approved modifications without vendor consultation: Continuous verification that customizations don’t compromise security baselinesCommunity Integration and Upstream CollaborationOrganizations should demand that hardened image vendors contribute security improvements back to original distribution maintainers. This requirement ensures that security enhancements benefit the broader community and aren’t held hostage by vendor business models.Evaluating vendor participation in upstream security discussions, patch contributions, and vulnerability disclosure processes provides insight into their long-term commitment to community-driven security rather than proprietary advantage. Essential evaluation criteria include:Upstream contribution requirements: Active contribution of security improvements to distribution maintainersTrue community engagement: Participation in security discussions and vulnerability disclosure processes: Contractual requirements for backward and forward compatibility with official distributionsIntelligent Migration Tooling and TransparencyAI-powered Dockerfile conversion capabilities should provide automated translation between vendor hardened images and standard distributions, handling complex multi-stage builds and dependency mappings without requiring manual intervention.Migration tooling must accommodate practical deployment patterns including multi-service containers and legacy application constraints rather than forcing organizations to adopt idealized single-service architectures. Essential tooling requirements include:Automated conversion capabilities: AI-powered translation between hardened images and standard distributionsTransparent migration documentation: Open source tools that generate equivalent configurations for different providers: Tools that work equally well for migrating to and away from hardened imagesReal-world architecture support: Accommodation of practical deployment patterns rather than forcing idealized architecturesPractical Implementation FrameworkStandardized compatibility testing protocols should verify that hardened images integrate seamlessly with existing toolchains, monitoring systems, and operational procedures before deployment at scale. Self-service customization interfaces for common modifications eliminate dependency on vendor support for routine operational tasks.Advanced image merging capabilities allow organizations to combine hardened base images with custom application layers while maintaining security baselines, providing flexibility without compromising protection. Implementation requirements include:Compatibility testing protocols: Standardized verification of integration with existing toolchains and monitoring systemsSelf-service customization:: User-friendly tools for common modifications (CA certificates, custom files, configuration overlays)Image merging capabilities: Advanced tooling for combining hardened bases with custom application layers: Service level agreements for maintaining compatibility and providing migration supportConclusion: Security Without Surrendering ControlThe real question platform teams must ask is this. Does my hardened image vendor strengthen or weaken my own control of my supply chain? The risks of lock-in aren’t theoretical. All of the factors described above can turn security into an unwanted operational constraint. Platform teams can demand hardened images and hardening process built for independence from the start— rooted in mainstream distributions, transparent in their build processes, modular in their security layers, supported by strong community involvement, and butressed by tooling that makes migration a choice, not a crisis.When security leaders adopt hardened images that preserve compatibility, encourage upstream collaboration, and fit seamlessly into existing workflows, they protect more than just their containers. They protect their ability to adapt and they minimize lock-in while actually improving their security posture. The most secure organizations will be the ones that can harden without handcuffing themselves.]]></content:encoded></item><item><title>Debugging in Production: Leveraging Logs, Metrics and Traces</title><link>https://devops.com/debugging-in-production-leveraging-logs-metrics-and-traces/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=debugging-in-production-leveraging-logs-metrics-and-traces</link><author>Neel Shah</author><category>devops</category><pubDate>Wed, 20 Aug 2025 06:00:17 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Announcing the end-of-support for the AWS SDK for .NET v3</title><link>https://aws.amazon.com/blogs/devops/announcing-the-end-of-support-for-the-aws-sdk-for-net-v3/</link><author>Ed McLaughlin</author><category>devops</category><pubDate>Tue, 19 Aug 2025 22:01:52 +0000</pubDate><source url="https://aws.amazon.com/blogs/devops/">AWS DevOps blog</source><content:encoded><![CDATA[We are announcing the end-of-support for the AWS SDK for .NET v3.x starting on March 1, 2026, in accordance with the SDK and Tools maintenance policy.  On April 28, 2025, the next major version of the AWS SDK for .NET, version 4.x, became generally available (blog post). Version 4.x of the SDK includes bug fixes, performance enhancements, and modernization for .NET. We strongly encourage you to upgrade to take advantage of these enhancements.Existing applications that use the AWS SDK for .NET v3.x will continue to function as intended unless there is a fundamental change to how an AWS service works. This is uncommon, and we will announce it broadly if it happens. Beginning March 1, 2026, the AWS SDK for .NET v3.x will only receive critical bug fixes and security updates, and we will not update it to support new AWS services, new service features, or changes to existing services. After June 1, 2026, when the AWS SDK for .NET v3.x reaches end-of-support, it will no longer receive updates or releases.End of Support Timeline for Version 3The timeline for end-of-support is as follows, as defined by the SDK major version lifecycle:During this phase, the SDK is fully supported. AWS will provide regular SDK releases that include support for new services, API updates for existing services, as well as bug and security fixes.During the maintenance mode, AWS limits SDK releases to address critical bug fixes and security issues only. An SDK will not receive API updates for new or existing services, or be updated to support new regionsWhen an SDK reaches end-of support, it will no longer receive updates or releases. Previously published releases will continue to be available via public package managers and the code will remain on GitHub. The GitHub repository may be archived.References for AWS SDK for .NET Version 4Use the following references to learn more about the AWS SDK for .NET v4.x along with migration support.We recommend you upgrade to the latest major version of the AWS SDK for .NET v4.x by using the migration guide. This major version includes, but is not limited to, performance enhancements, bug fixes, modern .NET libraries and frameworks, and the latest AWS service updates. To learn more, please refer to the AWS SDK for .NET GA blog post.If you need assistance or have feedback, reach out to your usual AWS support contacts. You can also open a discussion or issue on GitHub. Thank you for using the AWS SDK for .NET.]]></content:encoded></item><item><title>Announcing the end-of-support for AWS Tools for PowerShell v4</title><link>https://aws.amazon.com/blogs/devops/announcing-the-end-of-support-for-aws-tools-for-powershell-v4/</link><author>Ed McLaughlin</author><category>devops</category><pubDate>Tue, 19 Aug 2025 22:01:49 +0000</pubDate><source url="https://aws.amazon.com/blogs/devops/">AWS DevOps blog</source><content:encoded><![CDATA[We are announcing the end-of-support for the AWS Tools for PowerShell v4.x starting on March 1, 2026, in accordance with the SDK and Tools maintenance policy. On June 23, 2025, the next major version of the AWS Tools for PowerShell, version 5.x, became generally available (blog post). Version 5.x of the AWS Tools for PowerShell includes bug fixes, new features, performance enhancements, and leverages the latest major version of the AWS SDK for .NET v4.x. We strongly encourage you to upgrade to the latest version of AWS Tools for PowerShell v5.x to take advantage of these enhancements.Existing applications that use the AWS Tools for PowerShell v4.x will continue to function as intended unless there is a fundamental change to how an AWS service works. This is uncommon, and we will announce it broadly if it happens. Beginning March 1, 2026, the AWS Tools for PowerShell v4.x will only receive critical bug fixes and security updates, and we will not update it to support new AWS services, new service features, or changes to existing services. After June 1, 2026, when the AWS Tools for PowerShell v4.x reaches end-of-support, it will no longer receive updates or releases.The timeline for end-of-support is as follows, as defined by the SDK major version lifecycle:During this phase, the SDK is fully supported. AWS will provide regular SDK releases that include support for new services, API updates for existing services, as well as bug and security fixes.During the maintenance mode, AWS limits SDK releases to address critical bug fixes and security issues only. An SDK will not receive API updates for new or existing services, or be updated to support new regionsWhen an SDK reaches end-of support, it will no longer receive updates or releases. Previously published releases will continue to be available via public package managers and the code will remain on GitHub. The GitHub repository may be archived.Use the following references to learn more about the AWS Tools for PowerShell v5.x along with migration support.We recommend you upgrade to the latest major version of the AWS Tools for PowerShell v5.x by using the migration guide. This major version includes, but is not limited to, performance enhancements, bug fixes, modern .NET libraries and frameworks, and the latest AWS service updates. To learn more, please refer to the AWS Tools for PowerShell v5.x GA blog post.If you need assistance or have feedback, reach out to your usual AWS support contacts. You can also open a discussion or issue on GitHub. Thank you for using the AWS Tools for PowerShell.]]></content:encoded></item><item><title>Best performance and fastest memory with the new Amazon EC2 R8i and R8i-flex instances</title><link>https://aws.amazon.com/blogs/aws/best-performance-and-fastest-memory-with-the-new-amazon-ec2-r8i-and-r8i-flex-instances/</link><author>Veliswa Boya</author><category>devops</category><pubDate>Tue, 19 Aug 2025 19:16:24 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Today, we’re announcing general availability of the new eighth generation, memory optimized Amazon Elastic Compute Cloud (Amazon EC2) R8i and R8i-flex instances powered by custom Intel Xeon 6 processors, available only on AWS. They deliver the highest performance and fastest memory bandwidth among comparable Intel processors in the cloud. These instances deliver up to 15 percent better price performance, 20 percent higher performance, and 2.5 times more memory throughput compared to previous generation instances.With these improvements, R8i and R8i-flex instances are ideal for a variety of memory intensive workloads such as SQL and NoSQL databases, distributed web scale in-memory caches (Memcached and Redis), in-memory databases such as SAP HANA, and real-time big data analytics (Apache Hadoop and Apache Spark clusters). For a majority of the workloads that don’t fully utilize the compute resources, the R8i-flex instances are a great first choice to achieve an additional 5 percent better price performance and 5 percent lower prices. In terms of performance, R8i and R8i-flex instances offer 20 percent better performance than R7i instances, with even higher gains for specific workloads. These instances are up to 30 percent faster for PostgreSQL databases, up to 60 percent faster for NGINX web applications, and up to 40 percent faster for AI deep learning recommendation models compared to previous generation R7i instances, with sustained all-core turbo frequency now reaching 3.9 GHz (compared to 3.2 GHz in the previous generation). They also feature a 4.6x larger L3 cache and significantly better memory throughput, offering 2.5 times higher memory bandwidth than the seventh generation. With this higher performance across all the vectors, you can run a greater number of workloads while keeping costs down.R8i instances now scale up to 96xlarge with up to 384 vCPUs and 3TB memory (versus 48xlarge sizes in the seventh generation), helping you to scale up database applications. R8i instances are SAP certified to deliver 142,100 aSAPS, which is highest among all comparable machines in on premises and cloud environments, delivering exceptional performance for your mission-critical SAP workloads. R8i-flex instances offer the most common sizes, from large to 16xlarge, and are a great first choice for applications that don’t fully utilize all compute resources. Both R8i and R8i-flex instances use the latest sixth generation AWS Nitro Cards, delivering up to two times more network and Amazon Elastic Block Storage (Amazon EBS) bandwidth compared to the previous generation, which greatly improves network throughput for workloads handling small packets, such as web, application, and gaming servers.R8i and R8i-flex instances also support bandwidth configuration with 25 percent allocation adjustments between network and Amazon EBS bandwidth, enabling better database performance, query processing, and logging speeds. Additional enhancements include FP16 datatype support for Intel AMX to support workloads such as deep learning training and inference and other artificial intelligence and machine learning (AI/ML) applications.The specs for the R8i instances are as follows.The specs for the R8i-flex instances are as follows.As stated earlier, R8i-flex instances are more affordable versions of the R8i instances, offering up to 5 percent better price performance at 5 percent lower prices. They’re designed for workloads that benefit from the latest generation performance but don’t fully use all compute resources. These instances can reach up to the full CPU performance 95 percent of the time and work well for in-memory databases, distributed web scale cache stores, mid-size in-memory analytics, real-time big data analytics, and other enterprise applications. R8i instances are recommended for more demanding workloads that need sustained high CPU, network, or EBS performance such as analytics, databases, enterprise applications, and web scale in-memory caches. R8i and R8i-flex instances are available today in the US East (N. Virginia), US East (Ohio), US West (Oregon), and Europe (Spain) AWS Regions. As usual with Amazon EC2, you pay only for what you use. For more information, refer to Amazon EC2 Pricing. Check out the full collection of memory optimized instances to help you start migrating your applications.]]></content:encoded></item><item><title>Tuning Linux Swap for Kubernetes: A Deep Dive</title><link>https://kubernetes.io/blog/2025/08/19/tuning-linux-swap-for-kubernetes-a-deep-dive/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Tue, 19 Aug 2025 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The Kubernetes NodeSwap feature, likely to graduate to  in the upcoming Kubernetes v1.34 release,
allows swap usage:
a significant shift from the conventional practice of disabling swap for performance predictability.
This article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills.However, enabling swap is not a "turn-key" solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet's eviction logic.In this blogpost, I'll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms.
I will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters.Introduction to Linux swapAt a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel's page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors:Page access patterns (how recently pages are accessed)Page dirtyness (whether pages have been modified)Memory pressure (how urgently the system needs free memory)Anonymous vs File-backed memoryIt is important to understand that not all memory pages are the same. The kernel distinguishes between anonymous and file-backed memory.: This is memory that is not backed by a specific file on the disk, such as a program's heap and stack. From the application's perspective this is private memory, and when the kernel needs to reclaim these pages, it must write them to a dedicated swap device.: This memory is backed by a file on a filesystem. This includes a program's executable code, shared libraries, and filesystem caches. When the kernel needs to reclaim these pages, it can simply discard them if they have not been modified ("clean"). If a page has been modified ("dirty"), the kernel must first write the changes back to the file before it can be discarded.While a system without swap can still reclaim clean file-backed pages memory under pressure by dropping them, it has no way to offload anonymous memory. Enabling swap provides this capability, allowing the kernel to move less-frequently accessed memory pages to disk to conserve memory to avoid system OOM kills.Key kernel parameters for swap tuningTo effectively tune swap behavior, Linux provides several kernel parameters that can be managed via .: This is the most well-known parameter. It is a value from 0 to 200 (100 in older kernels) that controls the kernel's preference for swapping anonymous memory pages versus reclaiming file-backed memory pages (page cache).
: The kernel will be aggressive in swapping out less-used anonymous memory to make room for file-cache.: The kernel will strongly prefer dropping file cache pages over swapping anonymous memory.: This parameter tells the kernel to keep a minimum amount of memory free as a buffer. When the amount of free memory drops below the this safety buffer, the kernel starts more aggressively reclaiming pages (swapping, and eventually handling OOM kills).
 It acts as a safety lever to ensure the kernel has enough memory for critical allocation requests that cannot be deferred.: Setting a higher  effectively raises the floor for for free memory, causing the kernel to initiate swap earlier under memory pressure.vm.watermark_scale_factor: This setting controls the gap between different watermarks: ,  and , which are calculated based on .
:
: When free memory is below this mark, the  kernel process wakes up to reclaim pages in the background. This is when a swapping cycle begins.: When free memory hits this minimum level, then aggressive page reclamation will block process allocation. Failing to reclaim pages will cause OOM kills.: Memory reclamation stops once the free memory reaches this level.: A higher  careates a larger buffer between the  and  watermarks. This gives  more time to reclaim memory gradually before the system hits a critical state.In a typical server workload, you might have a long-running process with some memory that becomes 'cold'. A higher  value can free up RAM by swapping out the cold memory, for other active processes that can benefit from keeping their file-cache.Tuning the  and  parameters to move the swapping window early will give more room for  to offload memory to disk and prevent OOM kills during sudden memory spikes.To understand the real-impact of these parameters, I designed a series of stress tests.: GKE on Google Cloud: 1.33.2:  (8GiB RAM, 50GB swap on a  disk, without encryption), Ubuntu 22.04: A custom Go application designed to allocate memory at a configurable rate, generate file-cache pressure, and simulate different memory access patterns (random vs sequential).: A sidecar container capturing system metrics every second.: Critical system components (kubelet, container runtime, sshd) were prevented from swapping by setting  in their respective cgroups.I ran a stress-test pod on nodes with different swappiness settings (0, 60, and 90) and varied the  and  parameters to observe the outcomes under heavy memory allocation and I/O pressure.Visualizing swap in actionThe graph below, from a 100MBps stress test, shows swap in action. As free memory (in the "Memory Usage" plot) decreases, swap usage () and swap-out activity () increase. Critically, as the system relies more on swap, the I/O activity and corresponding wait time ( in the "CPU Usage" plot) also rises, indicating CPU stress.My initial tests with default kernel parameters (, , watermark_scale_factor=10) quickly led to OOM kills and even unexpected node restarts under high memory pressure. With selecting appropriate kernel parameters a good balance in node stability and performance can be achieved.The swappiness parameter directly influences the kernel's choice between reclaiming anonymous memory (swapping) and dropping page cache. To observe this, I ran a test where one pod generated and held file-cache pressure, followed by a second pod allocating anonymous memory at 100MB/s, to observe the kernel preference on reclaim:My findings reveal a clear trade-off:: The kernel proactively swapped out the inactive anonymous memory to keep the file cache. This resulted in high and sustained swap usage and significant I/O activity ("Blocks Out"), which in turn caused spikes in I/O wait on the CPU.: The kernel favored dropping file-cache pages delaying swap consumption. However, it's critical to understand that this does not disable swapping. When memory pressure was high, the kernel still swapped anonymous memory to disk.The choice is workload-dependent. For workloads sensitive to I/O latency, a lower swappiness is preferable. For workloads that rely on a large and frequently accessed file cache, a higher swappiness may be beneficial, provided the underlying disk is fast enough to handle the load.Tuning watermarks to prevent eviction and OOM killsThe most critical challenge I encountered was the interaction between rapid memory allocation and Kubelet's eviction mechanism. When my test pod, which was deliberately configured to overcommit memory, allocated it at a high rate (e.g., 300-500 MBps), the system quickly ran out of free memory.With default watermarks, the buffer for reclamation was too small. Before  could free up enough memory by swapping, the node would hit a critical state, leading to two potential outcomes: If kubelet's eviction manager detected  was below its threshold, it would evict the pod. In some high-rate scenarios, the OOM Killer would activate before eviction could complete, sometimes killing higher priority pods that were not the source of the pressure.To mitigate this I tuned the watermarks:Increased  to 512MiB: This forces the kernel to start reclaiming memory much earlier, providing a larger safety buffer.Increased  to 2000: This widened the gap between the  and  watermarks (from ≈337MB to ≈591MB in my test node's ), effectively increasing the swapping window.This combination gave  a larger operational zone and more time to swap pages to disk during memory spikes, successfully preventing both premature evictions and OOM kills in my test runs.Table compares watermark levels from  (Non-NUMA node): and watermark_scale_factor=10min_free_kbytes=524288KiB and watermark_scale_factor=2000Node 0, zone Normal    pages free 583273    min 10504    high 15756    present 1310720 Node 0, zone Normal    pages free 470539    low 337017    spanned 1310720   managed 1274542The graph below reveals that the kernel buffer size and scaling factor play a crucial role in determining how the system responds to memory load. With the right combination of these parameters, the system can effectively use swap space to avoid eviction and maintain stability.Enabling swap in Kubernetes is a powerful tool, but it comes with risks that must be managed through careful tuning.Risk of performance degradation Swapping is orders of magnitude slower than accessing RAM. If an application's active working set is swapped out, its performance will suffer dramatically due to high I/O wait times (thrashing). Swap could preferably be provisioned with a SSD backed storage to improve performance.Risk of masking memory leaks Swap can hide memory leaks in applications, which might otherwise lead to a quick OOM kill. With swap, a leaky application might slowly degrade node performance over time, making the root cause harder to diagnose.Risk of disabling evictions Kubelet proactively monitors the node for memory-pressure and terminates pods to reclaim the resources. Improper tuning can lead to OOM kills before kubelet has a chance to evict pods gracefully. A properly configured  is essential to ensure kubelet's eviction mechanism remains effective.Together, the kernel watermarks and kubelet eviction threshold create a series of memory pressure zones on a node. The eviction-threshold parameters need to be adjusted to configure Kubernetes managed evictions occur before the OOM kills.As the diagram shows, an ideal configuration will be to create a large enough 'swapping zone' (between  and  watermarks) so that the kernel can handle memory pressure by swapping before available memory drops into the Eviction/Direct Reclaim zone.Based on these findings, I recommend the following as a starting point for Linux nodes with swap enabled. You should benchmark this with your own workloads.: Linux default is a good starting point for general-purpose workloads. However, the ideal value is workload-dependent, and swap-sensitive applications may need more careful tuning.vm.min_free_kbytes=500000 (500MB): Set this to a reasonably high value (e.g., 2-3% of total node memory) to give the node a reasonable safety buffer.vm.watermark_scale_factor=2000: Create a larger window for  to work with, preventing OOM kills during sudden memory allocation spikes.I encourage running benchmark tests with your own workloads in test-environments, when setting up swap for the first time in your Kubernetes cluster. Swap performance can be sensitive to different environment differences such as CPU load, disk type (SSD vs HDD) and I/O patterns.]]></content:encoded></item><item><title>CNL: Bringing Agentic AI to Cloud Native with kagent &amp; Kyverno</title><link>https://www.youtube.com/watch?v=C0y8bDvL47M</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/C0y8bDvL47M?version=3" length="" type=""/><pubDate>Tue, 19 Aug 2025 17:03:56 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Let’s stop babysitting clusters and build AI agents to operate and govern for us! Kagent, a new CNCF project, enables simple declarative development of Kubernetes-native AI agents for any cloud native projects. This session will show you how to rapidly construct custom AI agents for essential operational and management tasks. Learn how Kyverno, the Kubernetes-native policy engine for managing validation, mutation, generation, and cleanup, can be extended with AI capabilities through kagent using the emerging Model Context Protocol (MCP) to enable intelligent policy recommendations and automation.]]></content:encoded></item><item><title>Streamline NGINX Configuration with Docker Desktop Extension</title><link>https://www.docker.com/blog/streamline-nginx-configuration-with-docker-desktop-extension/</link><author>Dylen Turnbull</author><category>docker</category><category>devops</category><pubDate>Tue, 19 Aug 2025 16:13:36 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Docker periodically highlights blog posts featuring use cases and success stories from Docker partners and practitioners. This story was contributed by Dylen Turnbull and Timo Stark. With over 29 years in enterprise and open-source software development, Dylen Turnbull has held roles at Symantec, Veritas, F5 Networks, and most recently as a Developer Advocate for NGINX. Timo is a Docker Captain, Head of IT at DoHo Engineering, and was formerly a Principal Technical Product Manager at NGINX.Modern Application developers face challenges in managing dependencies, ensuring consistent environments, and scaling applications. Docker Desktop simplifies these tasks with intuitive containerization, delivering reliable environments, easy deployments, and scalable architectures. NGINX server management in containers offers opportunities for enhancement, which the NGINX Development Center addresses with user-friendly tools to optimize configuration, performance, and web server management.Opportunities for Increased Workflow EfficiencyDocker Desktop streamlines container workflows, but NGINX configuration can be further improved with the NGINX Development Center:: NGINX setup often requires command-line expertise. The NGINX Development Center offers intuitive interfaces to simplify the process.Simplified Multi-Server Management: Managing multiple configurations involves complex volume mounting. The NGINX Development Center centralizes and streamlines configuration handling.: Debugging requires manual log access and container inspection. The NGINX Development Center provides clear diagnostic tools for faster resolution.: Reverse proxy updates need frequent restarts. The NGINX Development Center enables quick configuration changes with minimal downtime.By integrating Docker Desktop’s seamless containerization with the NGINX Development Center’s tools, developers can achieve a more efficient workflow for modern applications.The NGINX Development Center, available in the Docker Extensions Marketplace with over 51,000 downloads, addresses these frictions, streamlining NGINX configuration management for developers.The advantage for App/Web Server DevelopmentThe NGINX Development Center enhances app and web server development by offering an intuitive GUI-based interface integrated into Docker Desktop, simplifying server configuration file management without requiring command-line expertise. It provides streamlined access to runtime configuration previews, minimizing manual container inspection, and enables rapid iteration without container restarts for faster development and testing cycles.Centralized configuration management ensures consistency across development, testing, and production environments. Seamlessly integrated with Docker Desktop, the extension reduces the complexity of traditional NGINX workflows, allowing developers to focus on application development rather than infrastructure management.Overview of the NGINX Development CenterThe NGINX Development Center, developed by Timo Stark, is designed to enhance the developer experience for NGINX server configuration in containerized environments. Available in the Docker Extensions Marketplace, the extension leverages Docker Desktop’s extensibility to provide a dedicated NGINX Development Center. Key features include:Graphical Configuration InterfaceA user-friendly UI for creating and editing NGINX server blocks, routing rules, and SSL configurations.Run-Time Configuration UpdatesApply changes to NGINX instances without container restarts, supporting rapid iteration.Integrated Debugging ToolsValidate configurations, and troubleshoot issues directly within Docker Desktop.How Does the NGINX Development Center Work?The NGINX Development Center Docker extension, based on the NGINX Docker Desktop Extension public repository, simplifies NGINX configuration and management within Docker Desktop. It operates as a containerized application with a React-based user interface and a Node.js backend, integrated into Docker Desktop via the Extensions Marketplace and Docker API.Here’s how it works in simplified terms:: The extension is installed from the Docker Extensions Marketplace or built locally using a Dockerfile that compiles the UI and backend components. It runs as a container within Docker Desktop, pulling the image nginx/nginx-docker-extension:latest.: The React-based UI, accessible through the NGINX Development Center tab in Docker Desktop, allows developers to create and edit NGINX configurations, such as server blocks, routing rules, and SSL settings.: The Node.js backend processes user inputs from the UI, generates NGINX configuration files, and applies them to a managed NGINX container. Changes are deployed dynamically using NGINX’s reload mechanism, avoiding container restarts.: The extension communicates with Docker Desktop’s API to manage NGINX containers and uses Docker volumes to store configuration files and logs, ensuring seamless interaction with the Docker ecosystem.: While it doesn’t provide direct log access, the extension supports debugging by validating configurations in real-time and leveraging Docker Desktop’s native tools for indirect log viewing.The extension’s backend, built with Node.js, handles configuration generation and NGINX instance management, while the React-based frontend provides an intuitive user experience. For development, the extension supports hot reloading, allowing developers to test changes without rebuilding the image.Below is a simplified architecture diagram illustrating how the NGINX Development Center integrates with Docker Desktop:NGINX Development Center architecture showing integration with Docker Desktop, featuring a Node.js backend and React UI, managing NGINX containers and configuration files.: Hosts the extension and provides access to the Docker API and Extensions Marketplace.: Runs as a container, with a Node.js backend for configuration management and a React UI for user interaction.: The managed NGINX instance, configured dynamically by the extension.: Generated and monitored by the extension, stored in Docker volumes for persistence.Why run NGINX configuration management as a Docker Desktop Extension?Running NGINX configuration management as a Docker Desktop Extension provides a unified, streamlined experience for developers already working within the Docker ecosystem. By integrating directly into Docker Desktop’s interface, the extension eliminates the friction of switching between multiple tools and command-line interfaces, allowing developers to manage NGINX configurations alongside their containerized applications in a single, familiar environment.The extension approach leverages Docker’s inherent benefits of isolation and consistency, ensuring that NGINX configuration management operates reliably across different development machines and operating systems. This containerized approach prevents conflicts with local system configurations and removes the complexity of installing and maintaining separate NGINX management tools.Furthermore, Docker Desktop serves as the only prerequisite for the NGINX Development Center. Once Docker Desktop is installed, developers can immediately access sophisticated NGINX configuration capabilities without additional software installations, complex environment setup, or specialized NGINX expertise. The extension transforms what traditionally requires command-line proficiency into an intuitive, graphical workflow that integrates seamlessly with existing Docker-based development practices.Follow these steps to set up and use the Docker Extension: NGINX Development CenterPrerequisites: Docker Desktop, 1 running NGINX container.NGINX Development Center Setup in Docker Desktop:Ensure Docker Desktop is installed and running on your machine (Windows, macOS, or Linux).Installing the NGINX Development Center:Open Docker Desktop and navigate to the Extensions Marketplace (left-hand menu).Search for “NGINX” or “NGINX Development Center”.Click “Install” to pull and install the NGINX Development Center image Accessing the NGINX Development Center:After installation, a new “NGINX” tab appears in Docker Desktop’s left-hand menu.Click the tab to open the NGINX Development Center, where you can manage configurations and monitor NGINX instances.Configuration Management with the NGINX Development Center:Use the GUI configuration editor to create new NGINX config files.Configure existing nginx configuration files.Preview and validate configurations before applying them.Save changes, which are applied dynamically via hot reloading without restarting the NGINX container.Real-world use case example: Development Proxy for Local ServicesIn modern application development, NGINX serves as a reverse proxy that’s useful for developers on full-stack or microservices projects. It manages traffic routing between components, mitigates CORS issues in browser-based testing, enables secure local HTTPS setups, and supports efficient workflows by letting multiple services share a single entry point without direct port exposure. This aids local environments for simulating production setups, testing API integrations, or handling real-time features like WebSockets, while avoiding manual restarts and complex configurations. NGINX can proxy diverse local services, including frontend frameworks (e.g., React or Angular apps), backend APIs (e.g., Node.js/Express servers), databases with web interfaces (e.g., phpMyAdmin), static file servers, or third-party tools like mock services and caching layers.Developers often require a local proxy to route traffic between services (e.g., frontend on port 3000 and backend API) and avoid CORS issues, but manual NGINX setup demands file edits and restarts.With the Docker Extension: NGINX Development Center: Install the NGINX Development Center via Docker Extensions Marketplace in Docker Desktop. Ensure local services (e.g., Node.js backend on port 3000) run in separate containers. Open the NGINX Development Center tab.Containers run separately.: In the UI, create a new server. Set upstream to server the frontend at localhost. Add proxy for /api/* to http://backend:3000. Publish via the graphical options.Server config editing via the Docker Desktop UI: Preview the config in the NGINX Development Center UI to check for errors. Test by accessing http://localhost/ and http://localhost/api in a browser; confirm routing to backend.: Save and apply changes dynamically (no restart needed). Export config for reuse in a Docker Compose file to orchestrate services.This use case utilizes the NGINX Development Center’s React UI for proxy configuration, Node.js backend for config generation, and Docker API for seamless networking. Try setting up your own local proxy today by installing the extension and exploring the NGINX Development Center.Try it out and come visit usThis post has examined how the NGINX Development Center, integrated into Docker Desktop via the NGINX Development Center, tackles developer challenges in managing NGINX configurations for containerized web applications. It provides a UI and backend to simplify dependency management, ensure consistent environments, and support scalable setups. The graphical interface reduces the need for command-line expertise, managing server blocks, routing, and SSL settings, while dynamic updates and real-time previews aid iteration and debugging. Docker volumes help maintain consistency across development, testing, and production.We’ve highlighted a practical use case with Development Proxy for Local Services feasible within Docker Desktop using the extension. The architecture leverages Docker Desktop’s API and a containerized design to support the workflow.If you’re a developer interested in improving NGINX management, try installing the NGINX Development Center from the Docker Extensions Marketplace and explore the NGINX Development Center. For deeper engagement, visit the GitHub repository to review the codebase, suggest features, or contribute to its development, and consider joining discussions to connect with others.]]></content:encoded></item><item><title>Building AI Agents with Docker MCP Toolkit: A Developer’s Real-World Setup</title><link>https://www.docker.com/blog/docker-mcp-ai-agent-developer-setup/</link><author>Rajesh Padmakumaran</author><category>docker</category><category>devops</category><pubDate>Tue, 19 Aug 2025 14:59:46 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Building AI agents in the real world often involves more than just making model calls — it requires integrating with external tools, handling complex workflows, and ensuring the solution can scale in production.In this post, we’ll walk through a real-world developer setup for creating an agent using the Docker MCP Toolkit.To make things concrete, I’ve built an agent that takes a Git repository as input and can answer questions about its contents — whether it’s explaining the purpose of a function, summarizing a module, or finding where a specific API call is made. This simple but practical use case serves as a foundation for exploring how agents can interact with real-world data sources and respond intelligently.I built and ran it using the Docker MCP Toolkit, which made setup and integration fast, portable, and repeatable. This blog walks you through that developer setup and explains why Docker MCP is a game changer for building and running agents.Use Case: GitHub Repo Question-Answering AgentThe goal: Build an AI agent that can connect to a GitHub repository, retrieve relevant code or metadata, and answer developer questions in plain language.“Summarize this repo: https://github.com/owner/repo”“Where is the authentication logic implemented?”“List main modules and their purpose.”“Explain the function  and show where it’s used.”This goes beyond a simple code demo — it reflects how developers work in real-world environmentsThe agent acts like a code-aware teammate you can query anytime.The MCP Gateway handles tooling integration (GitHub API) without bloating the agent code.Docker Compose ties the environment together so it runs the same in dev, staging, or production.Role of Docker MCP ToolkitWithout MCP Toolkit, you’d spend hours wiring up API SDKs, managing auth tokens, and troubleshooting environment differences.Containerized connectors – Run the GitHub MCP Gateway as a ready-made service (docker/mcp-gateway:latest), no SDK setup required.Consistent environments – The container image has fixed dependencies, so the setup works identically for every team member.Rapid integration – The agent connects to the gateway over HTTP; adding a new tool is as simple as adding a new container.Iterate faster – Restart or swap services in seconds using .Focus on logic, not plumbing – The gateway handles the GitHub-specific heavy lifting while you focus on prompt design, reasoning, and multi-agent orchestration.Running everything via Docker Compose means you treat the entire agent environment as a single deployable unit:One-command startup –  brings up the MCP Gateway (and your agent, if containerized) together.Service orchestration – Compose ensures dependencies start in the right order.Internal networking – Services talk to each other by name (http://mcp-gateway-github:8080) without manual port wrangling.Scaling – Run multiple agent instances for concurrent requests.Unified logging – View all logs in one place for easier debugging.This setup connects a developer’s local agent to GitHub through a Dockerized MCP Gateway, with Docker Compose orchestrating the environment. Here’s how it works step-by-step:The developer runs the agent from a CLI or terminal.They type a question about a GitHub repository — e.g., “Where is the authentication logic implemented?”The Agent (LLM + MCPTools) receives the question.The agent determines that it needs repository data and issues a tool call via MCPTools. MCPTools sends the request using  to the MCP Gateway running in Docker.This gateway is defined in  and configured for the GitHub server (--servers=github --port=8080).The MCP Gateway handles all GitHub API interactions — listing files, retrieving content, searching code — and returns structured results to the agent.The agent sends the retrieved GitHub context to OpenAI GPT-4o as part of a prompt. The LLM reasons over the data and generates a clear, context-rich answer.The agent prints the final answer back to the CLI, often with file names and line references.Code Reference & File RolesThe detailed source code for this setup is available at this link. Rather than walk through it line-by-line, here’s what each file does in the real-world developer setup:Defines the MCP Gateway service for GitHub.Runs the docker/mcp-gateway:latest container with GitHub as the configured server.Exposes the gateway on port .Can be extended to run the agent and additional connectors as separate services in the same network.Implements the GitHub Repo Summarizer Agent.Uses  to connect to the MCP Gateway over .Sends queries to GitHub via the gateway, retrieves results, and passes them to GPT-4o for reasoning.Handles the interactive CLI loop so you can type questions and get real-time responses.In short: the Compose file manages infrastructure and orchestration, while the Python script handles intelligence and conversation.git clone https://github.com/rajeshsgr/mcp-demo-agents/tree/mainCreate a .env file in the root directory and add your OpenAI API key:OPEN_AI_KEY = <<Insert your Open AI Key>>
To allow the MCP Gateway to access GitHub repositories, set your GitHub personal access token:docker mcp secret set github.personal_access_token=<YOUR_GITHUB_TOKEN>
Bring up the GitHub MCP Gateway container using Docker Compose:Install Dependencies & Run Agentpython -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python app.py
Enter your query: Summarize https://github.com/owner/repoReal-World Agent Development with Docker, MCP, and ComposeThis setup is built with production realities in mind — ensures each integration (GitHub, databases, APIs) runs in its own isolated container with all dependencies preconfigured. acts as the bridge between your agent and real-world tools, abstracting away API complexity so your agent code stays clean and focused on reasoning. orchestrates all these moving parts, managing startup order, networking, scaling, and environment parity between development, staging, and production.From here, it’s easy to add:More MCP connectors (Jira, Slack, internal APIs).Multiple agents specializing in different tasks.CI/CD pipelines that spin up this environment for automated testingBy combining  for isolation,  for seamless tool integration, and  for orchestration, we’ve built more than just a working AI agent — we’ve created a repeatable, production-ready development pattern. This approach removes environment drift, accelerates iteration, and makes it simple to add new capabilities without disrupting existing workflows. Whether you’re experimenting locally or deploying at scale, this setup ensures your agents are reliable, maintainable, and ready to handle real-world demands from day one.Before vs. After: The Developer ExperienceManual SDK installs, dependency conflicts, “works on my machine” issues.Prebuilt container images with fixed dependencies ensure identical environments everywhere.Integration with Tools (GitHub, Jira, etc.)Custom API wiring in the agent code; high maintenance overhead.MCP handles integrations in separate containers; agent code stays clean and focused.Multiple scripts/terminals; manual service ordering. launches and orchestrates all services in the right order.Manually configuring ports and URLs; prone to errors.Internal Docker network with service name resolution (e.g., http://mcp-gateway-github:8080).Scaling services requires custom scripts and reconfigurations.Scale any service instantly with docker compose up --scale.Adding a new integration means changing the agent’s code and redeploying.Add new MCP containers to  without modifying the agent.Hard to replicate environments in pipelines; brittle builds.Same Compose file works locally, in staging, and in CI/CD pipelines.Restarting services or switching configs is slow and error-prone.Containers can be stopped, replaced, and restarted in seconds.]]></content:encoded></item><item><title>CNCF Deaf and Hard of Hearing Working Group at KubeCon + CloudNativeCon EU 2025</title><link>https://www.youtube.com/watch?v=CLhA-du1Yy8</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/CLhA-du1Yy8?version=3" length="" type=""/><pubDate>Tue, 19 Aug 2025 14:49:58 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Huge shout-out to the CNCF Deaf and Hard of Hearing Working Group for an incredible week at KubeCon + CloudNativeCon in London earlier this year! Check out this latest video to get a glimpse of what they were up to and what to expect in Atlanta!

They brought so much energy and had lots of activities, including a keynote, several talks, and a fun sign language crash course. Their kiosk in the Project Pavilion was a hub of activity, creating meaningful connections and fostering a more inclusive community.

This group is full of passion and brilliant people. We're so proud of what they've accomplished, and look forward to seeing the team in Atlanta! And for a fun challenge, watch the video and see if you can spot the new sign they created for "CNCF." ✨]]></content:encoded></item><item><title>Context Engineering is the Key to Unlocking AI Agents in DevOps</title><link>https://devops.com/context-engineering-is-the-key-to-unlocking-ai-agents-in-devops/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=context-engineering-is-the-key-to-unlocking-ai-agents-in-devops</link><author>Harshil Shah</author><category>devops</category><pubDate>Tue, 19 Aug 2025 10:38:57 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>FinOps as Code – Unlocking Cloud Cost Optimization</title><link>https://devops.com/finops-as-code-unlocking-cloud-cost-optimization/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=finops-as-code-unlocking-cloud-cost-optimization</link><author>Joydip Kanjilal</author><category>devops</category><pubDate>Tue, 19 Aug 2025 10:15:37 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What Really Matters When Picking a Cross-Platform Stack Today</title><link>https://devops.com/what-really-matters-when-picking-a-cross-platform-stack-today/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=what-really-matters-when-picking-a-cross-platform-stack-today</link><author>Oleksii Kyslenko</author><category>devops</category><pubDate>Tue, 19 Aug 2025 09:51:28 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AWS Weekly Roundup: Single GPU P5 instances, Advanced Go Driver, Amazon SageMaker HyperPod and more (August 18, 2025)</title><link>https://aws.amazon.com/blogs/aws/aws-weekly-roundup-single-gpu-p5-instances-advanced-go-driver-amazon-sagemaker-hyperpod-and-more-august-18-2025/</link><author>Prasad Rao</author><category>devops</category><pubDate>Mon, 18 Aug 2025 15:39:10 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Let me start this week’s update with something I’m especially excited about – the upcoming BeSA (Become a Solutions Architect) cohort. BeSA is a free mentoring program that I host along with a few other AWS employees on a volunteer basis to help people excel in their cloud careers. Last week, the instructors’ lineup was finalized for the 6-week cohort starting September 6. The cohort will focus on migration and modernization on AWS. Visit the BeSA website to learn more.Another highlight for me last week was the announcement of six new AWS Heroes for their technical leadership and exceptional contributions to the AWS community. Read the full announcement to learn more about these community leaders. Here are some launches from last week that got my attention:AWS Advanced Go Driver is generally available — You can now use the AWS Advanced Go Driver with Amazon Relational Database Service (Amazon RDS) and Amazon Aurora PostgreSQL-Compatible and MySQL-Compatible database clusters for faster switchover and failover times, Federated Authentication, and authentication with AWS Secrets Manager or AWS Identity and Access Management (IAM). You can install the PostgreSQL and MySQL packages for Windows, Mac, or Linux, by following the installation guides in GitHub.Expanded support for Cilium with Amazon EKS Hybrid Nodes — Cilium is a Cloud Native Computing Foundation (CNCF) graduated project that provides core networking capabilities for Kubernetes workloads. Now, you can receive support from AWS for a broader set of Cilium features when using Cilium with Amazon EKS Hybrid Nodes including application ingress, in-cluster load balancing, Kubernetes network policies, and kube-proxy replacement mode.Amazon SageMaker AI now supports P6e-GB200 UltraServers — You can accelerate training and deployment of foundational models (FMs) at trillion-parameter scale by using up to 72 NVIDIA Blackwell GPUs under one NVLink domain with the new P6e-GB200 UltraServer support in Amazon SageMaker HyperPod and Model Training.Amazon SageMaker HyperPod now supports fine-grained quota allocation of compute resources, topology-aware-scheduling of LLM tasks and custom Amazon Machine Images (AMIs) — You can allocate fine-grained compute quota for GPU, Trainium accelerator, vCPU, and vCPU memory within an instance to optimize compute resource distribution. With topology-aware scheduling, you can schedule your large language model (LLM) tasks on an optimal network topology to minimize network communication and enhance training efficiency. Using custom AMIs, you can deploy clusters with pre-configured, security-hardened environments that meet your specific organizational requirements. Here are some additional news items and blog posts that I found interesting: Check your calendars and sign up for upcoming AWS and AWS Community events:AWS re:Invent 2025 (December 1-5, 2025, Las Vegas) — The AWS flagship annual conference offering collaborative innovation through peer-to-peer learning, expert-led discussions, and invaluable networking opportunities.AWS Summits — Join free online and in-person events that bring the cloud computing community together to connect, collaborate, and learn about AWS. Coming up soon are summits in Johannesburg (August 20) and Toronto (September 4).AWS Community Days — Join community-led conferences that feature technical discussions, workshops, and hands-on labs led by expert AWS users and industry leaders from around the world: Adria (September 5), Baltic (September 10), Aotearoa (September 18), and South Africa (September 20).That’s all for this week. Check back next Monday for another Weekly Roundup!]]></content:encoded></item><item><title>CNL: Network-level and Identity-based Observability with Calico Open Source</title><link>https://www.youtube.com/watch?v=P7RUzvXr7Vg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/P7RUzvXr7Vg?version=3" length="" type=""/><pubDate>Mon, 18 Aug 2025 13:51:23 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't let your Kubernetes environment be a mystery! Gain the visibility you need to keep things running smoothly. This session dives into why network observability is key in Kubernetes, and includes a live demo showing how to identify misconfigurations, simplify troubleshooting, streamline overall network management, and safely implement and test network security policies across any Kubernetes environment.

Here's what we'll explore:

* Why network observability is important (hint: Kubernetes can be a bit chaotic! )
* How observability is the foundation for zero trust security
* Common customer pain points: Troubleshooting, incorrect policies, compliance, and audits
* Calico Whisker: What it is and how it provides that crucial bird's-eye view
* LIVE DEMO: Cluster-wide network observability using Calico Whisker]]></content:encoded></item><item><title>Microsoft Morphs Fusion Developers To Full Stack Builders</title><link>https://devops.com/microsoft-morphs-fusion-developers-to-full-stack-builders/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=microsoft-morphs-fusion-developers-to-full-stack-builders</link><author>Adrian Bridgwater</author><category>devops</category><pubDate>Mon, 18 Aug 2025 11:47:55 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Keeping Humans in the Loop: Why Human Oversight Still Matters in an AI-Driven DevOps Future</title><link>https://devops.com/keeping-humans-in-the-loop-why-human-oversight-still-matters-in-an-ai-driven-devops-future/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=keeping-humans-in-the-loop-why-human-oversight-still-matters-in-an-ai-driven-devops-future</link><author>Alan Shimel</author><category>devops</category><pubDate>Mon, 18 Aug 2025 07:57:21 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Five Great DevOps Job Opportunities</title><link>https://devops.com/five-great-devops-job-opportunities-152/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=five-great-devops-job-opportunities-152</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 18 Aug 2025 05:04:25 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>