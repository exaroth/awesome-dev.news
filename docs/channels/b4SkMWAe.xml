<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>Digest #161: GitLab 300GB Loss, DIY Data Center, OAuth Attacks, Netflix AWS Security, Docker Hub Limits &amp; More!</title><link>https://www.devopsbulletin.com/p/digest-161-gitlab-300gb-loss-diy</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd377709-8e11-4b69-9726-1029a1b30588_1447x991.jpeg" length="" type=""/><pubDate>Sun, 23 Feb 2025 11:03:19 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!First, see how GitLab lost 300GB of production data and look into what it takes to build your data center. We also share the findings from six months of research into OAuth application attacks, update you on Docker Hub’s new limits, explain Cortex Cloud’s merge with Prisma Cloud, and show how monday.com is managing its trace volume.Discover how to secure thousands of AWS accounts in our featured podcast without slowing down developers from Netflix’s cloud security engineer.In the tutorials section, learn how to replace Docker Compose with Quadlet for servers and organize Terraform code for better scalability to refactoring code with GitHub Copilot, understand Azure Data Transfer pricing, fix AWS Serverless image handlers, set up cross-region disaster recovery on AWS, explore multi-cluster fault tolerance with k8gb, storing Terraform state in Azure, and even a look at HTTP3.We also highlight some cool open-source devtools:•  – like Wireshark for Docker, letting you see all network requests.•  – a handy script to switch from Docker to Podman.•  – a tool to track which Chrome extensions make suspicious DNS requests.All this and more in this week’s DevOps Bulletin—don’t miss out! is an open-source project management platform focused on simplicity and efficiency. is Wireshark for your Docker containers. It lets devs see all incoming and outgoing requests to resolve production issues faster. is a small bash script that helps you migrate from Docker to Podman. helps track which Chrome extensions are making suspicious DNS requests. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Amazon EKS Downgrade: A Practical Solution</title><link>https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/</link><author>/u/Complete-Emu-6287</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 11:00:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup & restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.🔗 Read the full article here: If you've faced EKS downgrade challenges, let's discuss your experiences! ⬇️ #AWS #Kubernetes #EKS #DevOps]]></content:encoded></item><item><title>Amazon EKS Downgrade: A Practical Solution</title><link>https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/</link><author>/u/Complete-Emu-6287</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 11:00:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup & restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.🔗 Read the full article here: If you've faced EKS downgrade challenges, let's discuss your experiences! ⬇️ #AWS #Kubernetes #EKS #DevOps]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author>/u/Aciddit</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 09:30:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[By Antonio Ojea, Michael McCune |
Friday, February 14, 2025Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the Node
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial Node is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.Chicken and egg problem sequence diagramThis new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a Node resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a Node resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: cloud-controller-manager
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cloud-controller-manager
      annotations:
        kubernetes.io/description: Cloud controller manager for my infrastructure
    spec:
      containers: # the container details will depend on your specific cloud controller manager
      - name: cloud-controller-manager
        command:
        - /bin/my-infrastructure-cloud-controller-manager
        - --leader-elect=true
        - -v=1
        image: registry/my-infrastructure-cloud-controller-manager@latest
        resources:
          requests:
            cpu: 200m
            memory: 50Mi
      hostNetwork: true # these Pods are part of the control plane
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: cloud-controller-manager
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 120
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 120
      - effect: NoSchedule
        key: node.cloudprovider.kubernetes.io/uninitialized
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/not-ready
        operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>Talos on IPv6 only network?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/</link><author>/u/Moleventions</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 22:15:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Does anyone know if you can deploy Talos on an IPv6 only network in AWS?   submitted by    /u/Moleventions ]]></content:encoded></item><item><title>Why K8s when there’s k3s with less resource requirements?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/</link><author>/u/Crafty0x</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 21:57:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I don’t get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.   submitted by    /u/Crafty0x ]]></content:encoded></item><item><title>How to implement dynamic storage provisioning for onPrem cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/</link><author>/u/Impossible_Nose_2956</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 15:43:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.I dont want to handle PVs for my onPrem clusters.What options are available?Do let me know if my understanding of things is wrong anywhere. ]]></content:encoded></item><item><title>anyone tried kro for kubernetes resource management yet?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/</link><author>/u/AnnualRich5252</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 12:39:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it’s smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.it’s worth noting kro isn’t trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.looking forward to hearing your thoughts!]]></content:encoded></item><item><title>What&apos;s a good combination of tools to get a proper application observation solution together?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/</link><author>/u/tofagerl</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 12:03:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I work for a company with tons of k8s clusters, but they haven't really got the whole "let's provide all the benefits of this to the product teams" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. So now I'd like to pressure the infra teams to provide something better for us, but I can't just say "use Anthos Service Mesh", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. ]]></content:encoded></item><item><title>I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/</link><author>/u/sabir8992</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 11:13:52 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please   submitted by    /u/sabir8992 ]]></content:encoded></item><item><title>Thought We Had Our EKS Upgrade Figured Out… We Did Not</title><link>https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/</link><author>/u/rohit_raveendran</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 09:43:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[You ever think you’ve got everything under control, only for prod to absolutely humble you? Yeah, that was us.Lower environments? ✅ Tested a bunch.Version mismatches? ✅ All within limits.EKS addons? ✅ Using the standard upgrade flow.So we run Terraform on upgrade day. Everything’s looking fine—until kube-proxy upgrade just straight-up fails. Some pods get stuck in  Great.Cool, thanks, very helpful. We hadn’t changed anything on kube-proxy beyond the upgrade, so what the hell?At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can’t leave it like that.And then we find it: a tiny note in the AWS docs added just a few days ago. Turns out, kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions (link).And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom—everything worked.You’re never actually prepared. We tested everything, but something always slips through. The real test is how fast you fix it.Karpenter is great, but don’t let it go rogue. We’re now explicitly blocking unsupported instance families.Anyway, if you guys have ever had one of those “we did everything right, and it still blew up” moments, drop your stories. Misery loves company.]]></content:encoded></item><item><title>Revisiting Docker Hub Policies: Prioritizing Developer Experience</title><link>https://www.docker.com/blog/revisiting-docker-hub-policies-prioritizing-developer-experience/</link><author>Tushar Jain</author><category>docker</category><category>devops</category><pubDate>Sat, 22 Feb 2025 05:41:50 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[At Docker, we are committed to ensuring that Docker Hub remains the best place for developers, engineering teams, and operations teams to build, share, and collaborate. As part of this, we previously announced plans to introduce image pull consumption fees and storage-based billing. After further evaluating how developers use Docker Hub and what will best support the ecosystem, we have refined our approach—one that prioritizes developer experience and enables developers to scale with confidence while reinforcing Docker Hub as the foundation of the cloud-native ecosystem.We’re making important updates to our previously announced pull limits and storage policies to ensure Docker Hub remains a valuable resource for developers:No More Pull Count Limits or Consumption Charges – We’re cancelling pull consumption charges entirely. Our focus is on making Docker Hub the best place for developers to build, share, and collaborate—ensuring teams can scale with confidence.Unlimited Pull rates for Paid Users (As Announced Earlier) – Starting , all  will have unlimited image pulls (with fair use limits) to ensure a seamless experience.Updated Pull Rate Limits for Free & Unauthenticated Users – To ensure a reliable and seamless experience for all users, we are updating authenticated and free pull limits:
: Limited to 10 pulls per hour (as announced previously): Increased to 100 pulls per hour (up from 40 pulls / hour)System accounts & automation: As previously shared, automated systems and service accounts can easily authenticate using Personal Access Tokens (PATs) or Organizational Access Tokens (OATs), ensuring access to higher pull limits and a more reliable experience for automated authenticated pulls.Storage Charges Delayed Indefinitely – Previously, we announced plans to introduce , but we have decided to indefinitely delay any storage charges. Instead, we are focusing on delivering  that will allow users to actively manage their storage usage. Once these tools are available, we will assess storage policies in the best interest of our users. If and when storage charges are introduced, we will provide a six-month notice, ensuring teams have ample time to adjust.The Best Place to Build and Share – Docker Hub remains the world’s leading container registry, trusted by over 20 million developers and organizations. We’re committed to keeping it the best place to distribute and consume software. – We’re making these changes to support more developers, teams, and businesses as they scale, reinforcing Docker Hub as the foundation of the cloud-native world. – Our focus is on delivering more capabilities that help developers move faster, from better storage management tostrengthening security to better protect the software supply chain. – Every decision we make is about strengthening the platform and enabling developers to build, share, and innovate without unnecessary barriers.We appreciate your feedback, and we’re excited to keep evolving Docker Hub to meet the needs of developers and teams worldwide. Stay tuned for more updates, and as always—happy building! ]]></content:encoded></item><item><title>Best way to develop talos locally?</title><link>https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/</link><author>/u/obviouslyGAR</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 23:19:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am currently learning and building a cluster using talos.One thing I want to know is how are you all developing locally? Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?   submitted by    /u/obviouslyGAR ]]></content:encoded></item><item><title>Reading the Source Code</title><link>https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/</link><author>/u/TopNo6605</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 20:52:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?]]></content:encoded></item><item><title>Streamline Kubernetes Management with Rancher</title><link>https://youtube.com/shorts/fOVTDobiwIE?feature=share</link><author>/u/abhimanyu_saharan</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 19:58:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Meetup: All in Kubernetes (Munich)</title><link>https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/</link><author>/u/simplyblock-r</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 16:50:52 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)We're running our second meetup of the "All in Kubernetes" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.Community is focused around stateful workloads in Kubernetes. The sessions lined up are:Architecting and Building a K8s-based AI PlatformDatabases on Kubernetes: A Storage Story]]></content:encoded></item><item><title>Announcing CDK Garbage Collection</title><link>https://aws.amazon.com/blogs/devops/announcing-cdk-garbage-collection/</link><author>Kaizen Conroy</author><category>devops</category><pubDate>Fri, 21 Feb 2025 16:37:47 +0000</pubDate><source url="https://aws.amazon.com/blogs/devops/">AWS DevOps blog</source><content:encoded><![CDATA[For CDK developers that leverage assets at scale, they may notice over time that the bootstrapped bucket or repository accumulated old or unused data. If users wanted to clean this data on their own, CDK didn’t provide a clear way of determining which data is safe to delete. To solve this problem, we are excited to announce the preview launch of CDK Garbage Collection, a new feature of the CDK that automatically deletes old assets in your bootstrapped Amazon S3 Bucket and Amazon ECR Repository, saving users time and money. This feature is available starting in AWS CDK version 2.165.0.We expect CDK Garbage Collection to help AWS CDK customers save on storage costs associated with using the product while not affecting how customers use CDK.CDK Garbage Collection is exposed as a CDK CLI command named gc. To use CDK Garbage Collection in its default configuration, run the following command on a terminal in your CDK application.The  flag is meant to acknowledge that CDK Garbage Collection is in preview mode. This indicates that the scope and API of the feature might still change, but otherwise the feature is generally production ready and fully supported.CDK Garbage Collection works at the environment level, so it will attempt to delete isolated assets in the AWS account / region that you call it in. For the purposes of this walkthrough, you will be re-bootstrapping the environment with a custom qualifier so that you do not delete isolated assets before you are ready.cdk bootstrap --qualifier=abcdef --toolkit-stack-name=CDKToolkitDemoYou now have a new bootstrap template under the name CDKToolkitDemo and bootstrap resources associated with it. Next, set up a CDK application with both Amazon S3 and Amazon ECR assets:mkdir garbage-collection-demo && cd garbage-collection-demo
cdk init -l typescript app
Your next step is to replace the existing code In lib/garbage-collection-demo-stack.ts with the following CDK Stack:import * as path from 'path';
import * as cdk from 'aws-cdk-lib';
import { Construct } from 'constructs';
import * as lambda from 'aws-cdk-lib/aws-lambda';

export class GarbageCollectionDemoStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    const fn1 = new lambda.Function(this, 'my-function-s3', {
    code: lambda.Code.fromAsset(path.join(__dirname, '..', 'lambda')),
    runtime: lambda.Runtime.NODEJS_LATEST,
    handler: 'index.handler',
    });

    const fn2 = new lambda.Function(this, 'my-function-ecr', {
    code: lambda.Code.fromAssetImage(path.join(__dirname, '..', 'docker')),
    runtime: lambda.Runtime.FROM_IMAGE,
    handler: lambda.Handler.FROM_IMAGE,
    });
  }
}This creates two AWS Lambda functions, one which uses an Amazon S3 asset as its source code and one that uses an Amazon ECR image as its source code. You need to add the assets that are referenced to our CDK application. In  add a simple Lambda function:exports.handler = async function(event) {
  const response = require('./response.json');
  return response;
};And in  add a simple Docker image:FROM public.ecr.aws/docker/library/alpine:latestNow you can run  and get your initial CDK application set up in your AWS Account.cdk deploy \
  --toolkit-stack-name=CDKToolkitDemo \
  --context='@aws-cdk/core:bootstrapQualifier=abcdef'At this point you can check to make sure that assets have been correctly added into the bootstrapped Amazon S3 bucket and Amazon ECR repository:Two objects exist in the bootstrapped Amazon S3 Bucket after the initial AWS CDK Deploy.One image exists in the bootstrapped Amazon ECR Repository after the initial AWS CDK Deploy.The output shows that you have the data you expect in both bootstrapped resources. The Amazon S3 Bucket also stores the json file of the AWS CloudFormation Template that was generated when you ran cdk deploy.You can now simulate a typical CDK development cycle by updating both assets. Add a small change to the Amazon S3 asset that lives in :exports.handler = async function(event) {
  console.log('hello world');
  const response = require('./response.json');
  return response;
};And do the same in :FROM public.ecr.aws/docker/library/alpine:latest
CMD echo 'Hello World'You can now run  again, and both assets should be re-uploaded under a new hash.Four objects exist in the bootstrapped Amazon S3 Bucket after the second AWS CDK Deploy.Two images exist in the bootstrapped Amazon ECR Repository after the second AWS CDK Deploy.This output confirms that everything is as expected and the new assets have been added in. Because you are using new bootstrapped resources, you can still tell which resources are currently isolated and which are not. Right now, only the zipfile prefixed with 50f409b9 is referenced in AWS CloudFormation, and in Amazon ECR, only the image prefixed  is referenced. That means that every other asset — 3 objects in Amazon S3 and 1 object in Amazon ECR — are isolated and can be deleted.One item to note is the additional files in Amazon S3 that are not your local assets — these are AWS CloudFormation templates that are uploaded to Amazon S3 as an intermediary step before being sent to AWS CloudFormation. They are not needed after being copied over and are a perfect candidate for deletion via CDK Garbage Collection.Here is where CDK Garbage Collection comes in. With the right parameters, you are able to clean up the isolated objects while not disturbing the assets that are actively in use.cdk gc \
  --unstable=gc \
  --bootstrap-stack-name=CDKToolkitDemo \
  --rollback-buffer-days=0 \
  --created-buffer-days=0Because you want to delete assets immediately, and not tag them for deletion later, set rollback-buffer-days to 0. You also want to delete assets that were just created, so be sure to set created-buffer-days to 0 as well. The default for created-buffer-days is 1. ⏳ Garbage Collecting environment aws://912331974472/us-east-1...
Found 3 objects to delete based off of the following criteria:
- objects have been isolated for > 0 days
- objects were created > 0 days ago

Delete this batch (yes/no/delete-all)? CDK Garbage Collection found three assets to be deleted from Amazon S3, which is to be expected. It prompts you to verify that you want to delete, which you do, so enter . You will then get this response:[100.00%] 4 files scanned: 0 assets (0.00 MiB) tagged, 3 assets (0.02 MiB) deleted.Found 1 image to delete based off of the following criteria:
- images have been isolated for > 0 days
- images were created > 0 days ago

Delete this batch (yes/no/delete-all)?Once again, this is to be expected for Amazon ECR, so you enter yes again. You then get the response:[100.00%] 2 files scanned: 0 assets (0.00 MiB) tagged, 1 assets (3.90 MiB) deleted.At this point, CDK Garbage Collection is finished.CDK Garbage Collection exposes some parameters to help you customize the experience to your specific scenario. These options help you determine how aggressive you want your garbage collection to be.rollback-buffer-days: this is the amount of days an asset has to be marked as isolated before it is eligible for deletion.created-buffer-days: this is the amount of days an asset must live before it is eligible for deletion.Rollback Buffer Days should be considered when you are not using  and instead use a deployment method that operates on templates only, like a pipeline. If your pipeline can rollback without any involvement of the CDK CLI, this parameter will help ensure that assets are not prematurely deleted. When used, instead of deleting unused objects, cdk gc tags them with the current date. Subsequent runs of  will check this tag and delete the asset only after it has been tagged for longer than the specified buffer days.Created Buffer Days should be considered if you want to be extra safe about assets that have been recently uploaded. When used,  filters out any assets that have not persisted that number of days. Note that this may not include assets that have been shared across multiple CDK Apps CDK reuses assets that are identical, and its possible that a recent deploy of a CDK App references an asset that was uploaded earlier.For example, if you want to ensure that only assets that are over a month old and have been isolated for a week are deleted, you can specify:cdk gc --unstable --rollback-buffer-days=7 --created-buffer-days=30.Decision flow diagram of an asset as it gets audited for garbage collection.Limitations of CDK Garbage CollectionDuring CDK Garbage Collection, we collect all stack templates to see what assets are in use. If garbage collection runs between the asset upload and stack deployment, there is a chance that it does not pick up the latest stack deployment, but it does pick up the latest asset. In this scenario, CDK Garbage Collection may delete those assets.We recommend not deploying stacks while running CDK Garbage Collection. If that is unavoidable, setting  will help as garbage collection will avoid deleting assets that are recently created. Finally, if you do experience a failed deployment, the mitigation is to redeploy, as the asset upload step will be able to re-upload the missing asset. In practice, this race condition is only for a specific edge case and unlikely to happen. However, we are working on a new method of storing CDK Assets to reduce the risk of this race condition. That work is being tracked in this issue.CDK Garbage Collection helps users manage the lifecycle of unused CDK Assets in their AWS account. As users continue to scale with the CDK, tools like CDK Garbage Collection will play a crucial role in maintaining clean, efficient, and cost-effective cloud environments. We encourage CDK users to explore this feature, provide feedback, and incorporate it into their workflows to optimize their AWS resource management.]]></content:encoded></item><item><title>Bugs with k8s snap and IPv6 only</title><link>https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/</link><author>/u/hblok</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 14:13:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.The first is in the k8s-apiserver-proxy config file /var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json. It looks like this, where the the last part is the port number 6443. The service does not start with a "failed to parse endpoint" error:{"endpoints":["dead:beef:1234::1:6443"]} When correcting the address to use brackets, it will start up correctly.{"endpoints":["[dead:beef:1234::1]:6443"]} Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with "Failed to listen and serve" err="listen tcp 0.0.0.0:10250: bind: address already in use". Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly related to this report.]]></content:encoded></item><item><title>Endor Labs Extends Microsoft SCA Alliance to GitHub</title><link>https://devops.com/endor-labs-extends-microsoft-sca-alliance-to-github/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 21 Feb 2025 13:20:15 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Has DevSecOps Failed?</title><link>https://devops.com/why-has-devsecops-failed/</link><author>Edouard Viot</author><category>devops</category><pubDate>Fri, 21 Feb 2025 12:56:33 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Is this architecture possible without using haproxy but nginx(in rocky linux 9)?</title><link>https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/</link><author>/u/Keeper-Name_2271</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 12:37:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alerting from Prometheus and Grafana with kube-prometheus-stack</title><link>https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/</link><author>/u/HumanResult3379</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 12:36:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[In Grafana page's , I find the built-in alert rules named .I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/</link><author>/u/gctaylor</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 11:00:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>ChatLoopBackOff Episode 47 (external-secrets)</title><link>https://www.youtube.com/watch?v=F1VRkXR1UG0</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/F1VRkXR1UG0?version=3" length="" type=""/><pubDate>Fri, 21 Feb 2025 06:02:12 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[External-Secrets, a CNCF Sandbox project, is an open source project that simplifies the management and retrieval of secrets from external secret management systems (like AWS Secrets Manager, HashiCorp Vault, or Google Secret Manager). It bridges the gap between cloud native applications running in Kubernetes and secure external secret stores, ensuring secure and efficient secret management. 

Join CNCF Ambassador, Edson Ferreira as he explores how external-secrets can be valuable for teams operating in hybrid or multi-cloud environments with stringent security needs.]]></content:encoded></item><item><title>Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st</title><link>https://docs.docker.com/docker-hub/usage/</link><author>/u/onedr0p</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 00:06:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Starting April 1, 2025, all users with a Pro, Team, or Business
subscription will have unlimited Docker Hub pulls with fair use.
Unauthenticated users and users with a free Personal account have the
following pull limits:Unauthenticated users: 10 pulls/hourAuthenticated users with a free account: 100 pulls/hourThe following table provides an overview of the included usage and limits for each
user type, subject to fair use:Number of public repositoriesNumber of private repositories10 per IPv4 address or IPv6 /64 subnetFor more details, see the following:When utilizing the Docker Platform, users should be aware that excessive data
transfer, pull rates, or data storage can lead to throttling, or additional
charges. To ensure fair resource usage and maintain service quality, we reserve
the right to impose restrictions or apply additional charges to accounts
exhibiting excessive data and storage consumption.Docker Hub has an abuse rate limit to protect the application and
infrastructure. This limit applies to all requests to Hub properties including
web pages, APIs, and image pulls. The limit is applied per IPv4 address or per
IPv6 /64 subnet, and while the limit changes over time depending on load and
other factors, it's in the order of thousands of requests per minute. The abuse
limit applies to all users equally regardless of account level.You can differentiate between the pull rate limit and abuse rate limit by
looking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes
a link to documentation.]]></content:encoded></item><item><title>Using one ingress controller to proxy to another cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/</link><author>/u/djjudas21</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 22:15:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to app.example.com on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: "HTTPS" nginx.ingress.kubernetes.io/ssl-passthrough: "true" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks]]></content:encoded></item><item><title>CustomResourceDefinitions to provision Azure resources such as storage blob</title><link>https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/</link><author>/u/Valuable-Ad3229</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 22:11:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?I am mindful that this may be anti-pattern but I am curious. Thank you!]]></content:encoded></item><item><title>CNL: Optimizing cost, performance, and security in K8s with policy-as-code</title><link>https://www.youtube.com/watch?v=O5YBwJO6FCw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/O5YBwJO6FCw?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 22:10:31 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Cloud Native Live: Insights from the Radar</title><link>https://www.youtube.com/watch?v=Sxnqk6EoB-s</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Sxnqk6EoB-s?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 22:00:02 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Insights from the Radar: Exploring trends & ecosystem gaps from the CNCF AI & Multicluster radar report]]></content:encoded></item><item><title>Learning Project - Deploy Flask App With MySQL on Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/</link><author>/u/kchandank</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 20:53:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).kubectl installed and configured to interact with your Kubernetes cluster.Docker installed on your machine to build and push the Docker image of the Flask app.Docker Hub account to push the Docker image.You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:Create a app.py file with following contentfrom flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): """ Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD """ host = os.environ.get("MYSQL_HOST", "localhost") database = os.environ.get("MYSQL_DB", "flaskdb") user = os.environ.get("MYSQL_USER", "flaskuser") password = os.environ.get("MYSQL_PASSWORD", "flaskpass") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f"Error connecting to MySQL: {e}") return None u/app.route("/") def index(): return f"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!" u/app.route("/dbtest") def db_test(): """ A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. """ connection = get_db_connection() if connection is None: return jsonify({"error": "Failed to connect to MySQL database"}), 500 try: cursor = connection.cursor() cursor.execute("SELECT NOW();") current_time = cursor.fetchone() return jsonify({ "message": "Successfully connected to MySQL!", "current_time": current_time[0] }) except Error as e: return jsonify({"error": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == "__main__": debug_mode = os.environ.get("DEBUG", "false").lower() == "true" app.run(host="0.0.0.0", port=5000, debug=debug_mode) FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update && apt-get install -y iputils-ping && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD ["python", "app.py"] docker build -t becloudready/my-flask-app It will show a 6 digit Code, which you need to enter to following URLPush the Image to DockerHubdocker push becloudready/my-flask-app You should be able to see the Pushed ImageapiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: "false" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql kubectl create secret generic db-credentials \ --namespace=flask-app \ --from-literal=username=flaskuser \ --from-literal=password=flaskpass \ --from-literal=database=flaskdb apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: "mysql-svc" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: ["sh", "-c", "rm -rf /var/lib/mysql/*"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi storageClassName: do-block-storage kubectl apply -f namespaces.yamlDeploy ConfigMaps and Secrets:kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yamlkubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yamlkubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yamlkubectl get svc -n flask-appNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEflask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53sUnable to connect to MySQL from Flask AppLogin to the Flask app pod to ensure all values are loaded properlykubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 51 (cert-manager)</title><link>https://www.youtube.com/watch?v=UR64KulZDCM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/UR64KulZDCM?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 19:51:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Cert-manager is a recently graduated CNCF project designed to automate the management and issuance of TLS certificates. It simplifies the process of obtaining, renewing, and revoking certificates for applications running on Kubernetes.

Join CNCF Ambassador, Ronit Banerjee as he explores how cert-manager automates many aspects of managing TLS certificates, ensuring that they are always up to date, valid, and properly configured.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 51 (WasmEdge)</title><link>https://www.youtube.com/watch?v=Cxz7pC9Lq2k</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Cxz7pC9Lq2k?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 19:37:32 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[WasmEdge is a CNCF Sandbox project that is a high-performance WebAssembly runtime optimized for cloud-native and edge computing use cases. It’s designed to run WebAssembly (Wasm) applications at the edge and in the cloud, offering a fast, efficient, and secure way to execute containerized applications.

Unlike traditional container runtimes, WasmEdge is designed to work efficiently with cloud-native ecosystems. Join CNCF Ambassador, Faeka Ansari as she explores how WasmEdge allows developers to run Wasm modules with low overhead.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 50 (Tekton Pipelines)</title><link>https://www.youtube.com/watch?v=vHnI_hty9zc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/vHnI_hty9zc?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 19:18:14 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Tekton Pipelines is a project that focuses on providing CI/CD (Continuous Integration/Continuous Delivery) systems. Tekton is designed to facilitate the automation of the software development lifecycle, from code commit to deployment, and is deeply integrated into Kubernetes.

Join CNCF Ambassador, Chamod Perera as he explores this robust and flexible CI/CD solution suitable for teams using Kubernetes who want a Kubernetes-native approach to CI/CD.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 49 (Linkerd)</title><link>https://www.youtube.com/watch?v=WltDqvMzZIw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/WltDqvMzZIw?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 19:07:20 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Linkerd is a CNCF graduated project as of July 2021. This ultra light, simple, and powerful cloud native project adds security, observability, and reliability to Kubernetes, without the complexity.

Linkerd is an advanced service mesh written in Rust, and has been in production for over 8 years with over 500 contributors. Join CNCF Ambassador ChengHao Yang as he explores Linkerd for the first time with the hopes of learning about its benefits, and why companies like Adidas and Xbox chose it for their tech stack.]]></content:encoded></item><item><title>Arm Extension for GitHub Copilot: Accelerating Migration to Arm Architecture</title><link>https://devops.com/arm-extension-for-github-copilot-accelerating-migration-to-arm-architecture/</link><author>Tom Smith</author><category>devops</category><pubDate>Thu, 20 Feb 2025 17:57:51 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>EKS vs. GKE differences in Services and Ingresses for their respective NLBs and ALBs</title><link>https://www.reddit.com/r/kubernetes/comments/1itx2uh/eks_vs_gke_differences_in_services_and_ingresses/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 12:12:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Weekly: This Week I Learned (TWIL?) thread</title><link>https://www.reddit.com/r/kubernetes/comments/1itvy0m/weekly_this_week_i_learned_twil_thread/</link><author>/u/gctaylor</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 11:00:29 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Did you learn something new this week? Share here!]]></content:encoded></item><item><title>EKS Auto Mode a.k.a managed Karpenter.</title><link>https://www.reddit.com/r/kubernetes/comments/1itumdr/eks_auto_mode_aka_managed_karpenter/</link><author>/u/lynxerious</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 09:26:54 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[It's relatively new, has anyone tried it before? Someone just told me about it recently.https://aws.amazon.com/eks/pricing/ The pricing is a bit strange, it adds up cost to EC2 pricing instead of Karpenter pods. And there are many type of instance I can't search for in that list.]]></content:encoded></item><item><title>AI Tools for Kubernetes: What Have I Missed?</title><link>https://www.reddit.com/r/kubernetes/comments/1ittpj1/ai_tools_for_kubernetes_what_have_i_missed/</link><author>/u/Electronic_Role_5981</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 08:20:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[karpor (kusionstack subproject)Intelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teamskube-copilot (personal project from Azure)Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).Diagnose and analyze potential issues for Kubernetes workloads.Generate Kubernetes manifests based on provided prompt instructions.Utilize native  and  commands for Kubernetes cluster access and security vulnerability scanning.Access the web and perform Google searches without leaving the terminal.some cost related `observibility and analysis`I did not check if all below projects focus on k8s.Are there any ai-for-k8s projects that I miss?]]></content:encoded></item><item><title>How to Perform Cleanup Tasks When a Pod Crashes (Including OOM Errors)?</title><link>https://www.reddit.com/r/kubernetes/comments/1itt3ja/how_to_perform_cleanup_tasks_when_a_pod_crashes/</link><author>/u/SamaDinesh</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 07:37:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a requirement where I need to delete a specific file in a shared volume whenever a pod goes down.I initially tried using the  lifecycle hook, and it works fine when the pod is deleted normally (e.g., via ). However, the problem is that  does not trigger when the pod crashes unexpectedly, such as due to an OOM error or a node failure. I am looking for a reliable way to ensure that the file is deleted even when the pod crashes unexpectedly. Has anyone faced a similar issue or found a workaround?lifecycle: preStop: exec: command: ["/bin/sh", "-c", "rm -f /data/your-file.txt"] ]]></content:encoded></item><item><title>CNL: Optimizing Kyverno policy enforcement performance for large clusters</title><link>https://www.youtube.com/watch?v=DWmCAUCs3bc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/DWmCAUCs3bc?version=3" length="" type=""/><pubDate>Thu, 20 Feb 2025 06:05:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Cluster restoration</title><link>https://www.reddit.com/r/kubernetes/comments/1itq9c8/cluster_restoration/</link><author>/u/Upper-Aardvark-6684</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 04:38:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Upper-Aardvark-6684 ]]></content:encoded></item><item><title>How to run VM using kubevirt in kind cluster in MacOS (M2)?</title><link>https://www.reddit.com/r/kubernetes/comments/1itpzch/how_to_run_vm_using_kubevirt_in_kind_cluster_in/</link><author>/u/Wooden_Departure1285</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 04:23:07 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Wooden_Departure1285 ]]></content:encoded></item><item><title>how advancements like Dynamic Resource Allocation (DRA) and the Container Device Interface (CDI) are shaping Kubernetes for AI workloads</title><link>https://furiosa.ai/blog/the-next-chapter-of-kubernetes-enabling-ml-inference-at-scale</link><author>/u/woowookim</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 03:56:07 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[CDI is designed to solve two problems. First, all device vendors expose their devices to containers in different ways. Second, device support in container runtimes is fragmented. This imposes high costs on both AI chip vendors and the open-source community. Yes, these circumstances clearly illustrate why a standard interface is necessary. Now AI chip vendors can support various container runtimes through standardized interfaces by implementing CDI. Docker also supports it as an experimental feature, but it is expected to become a default feature in the near future since containerd now supports it as a default feature. Originally, CDI was part of the DRA. However, it now has a greater influence on the entire container ecosystem.In addition to standardized device exposure, CDI delivers several benefits for enterprise deployments using different kinds of AI hardware (such as RNGD):Simplified runtime supportReduced implementation costsFuriosa has built RNGD to be the best accelerator for real world deployments using large language models, multimodal models and agentic AI systems. To achieve this, we will leverage new Kubernetes functionality like CDI and DRA.  RNGD uses the CDI to define how devices are assigned to containers. The CDI specification lays out the format, guidelines, and interfaces needed to properly describe these devices. For example, RNGD’s vendor-specific interfaces like device nodes and sysfs files can be expressed using the interfaces provided by the CDI. This makes it simpler for container-related systems to understand which system resources need to be present when running workloads on RNGD hardware. We’re currently building a DRA plugin for flexible and efficient resource scheduling. When we release the plugin in 2025, users will be able to request RNGD resources defined through the DRA interface. One key benefit of DRA is that it allows users to specify their desired hardware topology. For example, a user might say they need four RNGD devices under a single physical CPU socket, or two RNGD devices under a specific PCIe switch. They can even specify that multiple servers must be located beneath a particular network switch. When the scheduler hands off the RNGD scheduling task to the Furiosa DRA plugin, the plugin calculates which server meets the user’s topology requirements. Among the servers that match these conditions, it then assigns the requested RNGD resources to the user’s container (reality is binding the user's pod on that server).Future outlook for Kubernetes and ML workloadsThe evolution of Kubernetes, driven by advancements like DRA and CDI, is crucial for the future of AI. As the industry moves beyond traditional GPUs and embraces more efficient chip architectures like RNGD, the ability to effectively orchestrate and manage these resources will become even more critical.This will accelerate the adoption of specialized hardware, leading to faster, more efficient, and more cost-effective ML inference.The ongoing collaboration between the Kubernetes community and AI chip vendors is essential to ensure that Kubernetes continues to meet the evolving needs of the AI landscape. By working together, we can unlock the full potential of AI and drive innovation across industries. We believe that sharing our experience with developing and deploying RNGD will contribute to this important effort. We also hope to foster more open-source contributions that will speed progress toward an open ecosystem that supports a wide range of AI-specific hardware to serve different needs in the industry.]]></content:encoded></item><item><title>Dapr.io - enterprise enabler for boosting developer productivity</title><link>https://www.youtube.com/watch?v=XoNcJYoJkRY</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/XoNcJYoJkRY?version=3" length="" type=""/><pubDate>Wed, 19 Feb 2025 21:51:07 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Master Node Migration</title><link>https://www.reddit.com/r/kubernetes/comments/1itfclm/master_node_migration/</link><author>/u/BrockWeekley</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Wed, 19 Feb 2025 20:14:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello all, I've been running a k3s cluster for my home lab for several months now. My master node hardware has begun failing - it is always maxed out on CPU and is having all kinds of random failures. My question is, would it be easier to simply recreate a new cluster and apply all of my deployments there, or should mirroring the disk of the master to new hardware be fairly painless for the switch over?I'd like to add HA with multiple master nodes to prevent this in the future, which is why I'm leaning towards just making a new cluster, as switching from an embedded sqlite DB to a shared database seems like a pain. ]]></content:encoded></item><item><title>Kubemgr: Open-Source Kubernetes Config Merger</title><link>https://www.reddit.com/r/kubernetes/comments/1ite641/kubemgr_opensource_kubernetes_config_merger/</link><author>/u/RAPlDEMENT</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Wed, 19 Feb 2025 19:27:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm excited to share a personal project I've been working on recently. My classmates and I found it tedious to manually change environment variables or modify Kubernetes configurations by hand. Merging configurations can be straightforward but often feels cumbersome and annoying.To address this, I created Kubemgr, a Rust crate that abstracts a command for merging Kubernetes configurations:KUBECONFIG=config1:config2... kubectl config view --flatten Available on crates.io, this CLI makes the process less painful and more intuitive.But that's not all! For those who prefer not to install the crate locally, I also developed a user interface using Next.js and WebAssembly (WASM). The goal was to ensure that both the interface and the CLI use the exact same logic while keeping everything client-side for security reasons.I understand that this project might not be useful for everyone, especially those who are already experienced with Kubernetes. However, it was primarily a learning exercise for me to explore new technologies and improve my skills. I'm eager to get feedback and hear any ideas for new features or improvements that could make Kubemgr more useful for the community.The project is open-source, so feel free to check out the code and provide recommendations or suggestions for improvement on GitHub. Contributions are welcome!If you like the project, please consider starring the GitHub repo!]]></content:encoded></item><item><title>CNCF Research End User Group: Managing Kubeflow deployments and updates at CERN (February 19, 2025)</title><link>https://www.youtube.com/watch?v=QvNIS0M0VJE</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/QvNIS0M0VJE?version=3" length="" type=""/><pubDate>Wed, 19 Feb 2025 18:08:51 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>TOC Meeting 2025-02-18</title><link>https://www.youtube.com/watch?v=deyssJesSII</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/deyssJesSII?version=3" length="" type=""/><pubDate>Wed, 19 Feb 2025 16:53:52 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Sawmills Emerges From Stealth to Apply AI to Managing Telemetry Data</title><link>https://devops.com/sawmills-emerges-from-stealth-to-apply-ai-to-managing-telemetry-data/</link><author>Mike Vizard</author><category>devops</category><pubDate>Wed, 19 Feb 2025 15:27:16 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>