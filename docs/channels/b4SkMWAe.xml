<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>ChatLoopBackOff Episode 75: Exploring Trulent with Shivay Lamba</title><link>https://www.youtube.com/watch?v=nia_qUP0Zu0</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/nia_qUP0Zu0?version=3" length="" type=""/><pubDate>Wed, 28 Jan 2026 23:44:37 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Shivay Lamba explores TruLens for the very first time on ChatLoopBackOff!

TruLens is an open source observability and evaluation framework designed to help teams understand, test, and improve Large Language Model (LLM) applications. As AI-powered systems become a core part of modern cloud-native platforms, TruLens provides critical insights into model behavior, quality, and trustworthiness—helping developers move beyond “black box” AI.

In this episode, Shivay will take a hands-on look at how TruLens works. Expect live experimentation, and an honest first look at where TruLens fits within the growing AI and cloud native ecosystem.

If you’re curious about LLM observability, responsible AI, or how open source communities are tackling AI evaluation challenges, this session is for you. Bring your questions, share your experiences, and learn alongside us as we explore TruLens together—live.]]></content:encoded></item><item><title>Best Practices for Deploying AWS DevOps Agent in Production</title><link>https://aws.amazon.com/blogs/devops/best-practices-for-deploying-aws-devops-agent-in-production/</link><author>Greg Eppel</author><category>devops</category><pubDate>Wed, 28 Jan 2026 22:36:18 +0000</pubDate><source url="https://aws.amazon.com/blogs/devops/">AWS DevOps blog</source><content:encoded><![CDATA[Root cause analysis during incidents is one of the most time-consuming and stressful parts of operating cloud applications. Engineers must quickly correlate telemetry data across multiple services, review deployment history, and understand complex application dependencies—all while under pressure to restore service. AWS DevOps Agent changes this paradigm by bringing autonomous investigation capabilities to your operations team, reducing mean time to resolution (MTTR) from hours to minutes.However, the effectiveness of AWS DevOps Agent depends heavily on how you configure your Agent Spaces which control resource access boundaries. An Agent Space that’s too narrow misses critical context during investigations. One that’s too broad introduces performance overhead and complexity. This post provides best practices for setting up Agent Spaces that balance investigation capability with operational efficiency, drawing from our experience onboarding early customers and using DevOps agent across our own teams.By the end of this post, you’ll understand how to structure Agent Spaces for optimal investigation accuracy, determine the right scope of resource access, and use Infrastructure as Code (IaC) to streamline deployment. Let’s start by understanding the foundational concept that makes all of this possible: the Agent Space itself.An Agent Space is a logical container that defines what AWS DevOps Agent can access and investigate. Think of it as the agent’s operational boundary—it determines which cloud accounts the agent can query, which third-party integrations are available, and who can interact with investigations.Agent Spaces are critical because AWS DevOps Agent needs sufficient context to perform accurate root cause analysis.When an incident occurs, the agent:Learns your resources and their relationships across accountsCorrelates telemetry data from logs, metrics, and tracesReviews recent changes including deployments and configuration updatesGenerates and tests hypotheses by querying additional data sourcesFigure 1: Agent Space TopologyIf the Agent Space doesn’t include access to a critical account or integration, the agent might miss the root cause entirely. Conversely, an overly broad Agent Space introduces performance challenges as the agent considers more resource permutations during investigations.Understanding these trade-offs between scope and performance is essential. The question becomes: how do you determine the right boundaries for your specific organization and operational model?”We recommend thinking about Agent Space boundaries the same way you think about on-call responsibilities: grant access to accounts relevant to the application, but separate production from non-production environments.This approach provides several benefits:Familiar mental model – Operations teams already understand on-call boundariesAppropriate investigation scope – Mirrors how human engineers would investigate incidentsTwo-way door decision – You can expand or narrow Agent Space scope as needs evolvePerformance balance – Provides sufficient context without overwhelming the agentStart by mapping your application architecture to Agent Space boundaries and consider the following questions:What defines a logical application?Does your team own multiple independent applications? If so, create separate Agent Spaces.Is it a monolith spanning multiple accounts? Then one Agent Space with cross-account access makes sense.How do you organize on-call rotations?Separate teams for production versus non-production suggests separate Agent Spaces.One team handling all environments might work with one Agent Space per application.What are your investigation patterns?Do production incidents require querying dependent services in other accounts? Include those accounts.Are environments completely isolated? Keep Agent Spaces separate.Application: E-commerce Platform├── Production environment│ ├── Account 111111111111 (Frontend)│ ├── Account 222222222222 (API Gateway + Lambda)│ └── Account 333333333333 (RDS + DynamoDB)│ └── Account 444444444444 (All resources)└── Development environment└── Account 555555555555 (All resources)Recommended Agent Spaces:→ "EcommerceProd" (accounts 111111111111, 222222222222, 333333333333)→ "EcommerceNonProd" (accounts 444444444444, 555555555555)Figure 2: Agent Space boundaries mirror on-call team responsibilitiesBeyond the basic single-application pattern, organizations encounter more complex scenarios that require careful consideration. Here are critical patterns to address that we’ve seen customers successfully adopt:Pattern 1: Investigations Spanning Multiple Teams. Large organizations with multiple teams (example: 3 teams managing 100+ production accounts) encounter situations where an issue originates in Team A’s infrastructure but the root cause lies in Team B’s services. The question becomes: how do you enable collaboration across Agent Spaces? Create application-specific Agent Spaces that include read-only access to shared resource accounts e.g. dependencies. Establish clear on-call escalation procedures and add them as runbooks when investigations identify cross-team root causes for efficient communication (e.g. via chat in Slack). Configure the shared service team’s resources with tags identifying which applications use them (example: app-id: ecommerce-frontend). Following a consistent tagging strategy provides investigation context for shared resources while maintaining clear resource ownership.Pattern 2: Shared Services and Network Operations Center (NOC) Teams. Some organizations have centralized teams that provide and support shared infrastructure services (databases, networking, monitoring, security) used by multiple applications across the organization. These NOC or central operations teams need visibility into their services without requiring access to every application’s Agent Space. Create a dedicated Agent Space for the shared service team and configure an Agent Space scoped to the shared service team’s infrastructure and operational responsibilities:Include AWS accounts containing shared databases, network infrastructure, centralized logging, and monitoring systemsAdd relevant CloudFormation stacks for shared platform servicesConfigure IAM roles that provide read-only access to the specific resources the team supportsInclude runbooks and operational procedures specific to the shared servicesThis follows the same principle as application-specific Agent Spaces: one Agent Space per on-call team, even when that Agent Space’s scope spans multiple applications. While shared services teams manage specific infrastructure domains, SRE teams often face an even larger challenge: operational responsibility for hundreds or thousands of applications at enterprise scale.Pattern 3: Central Operations Teams Managing Many Applications. Central operations teams responsible for operational tooling across hundreds or thousands of applications can efficiently manage Agent Spaces at scale using Infrastructure as Code.Use the AWS CDK or Terraform samples available as starting points. These samples enable teams to:Define a standardized Agent Space template with your organization’s required IAM roles, integrations, resource boundaries and governance tagsDeploy Agent Spaces programmatically as part of application onboarding workflowsEnforce compliance through AWS Config rules or service control policiesTrack all Agent Spaces through consolidated billing and tagging (application-id, team, cost-center, environment)Central operations teams manage the templates and governance policies, while application teams operate within those guardrails. This approach scales to thousands of applications with consistent configuration and automated deployment. AWS DevOps agent allows limiting agent access in an AWS account and controlling access for users to the operator console for teams to manage Agent Space access at scale.Figure 3: Enterprise scale pattern using Infrastructure as CodeNow that you understand how to design Agent Space boundaries aligned with your team structure and scale requirements, let’s walk through the practical implementation steps to bring these architectural patterns to life.This section walks you through the practical steps of creating your first Agent Space—from verifying prerequisites and configuring IAM roles across accounts to integrating observability tools, setting up access controls, and testing your configuration to ensure investigations have the context they need.Before setting up your first Agent Space, ensure you have: – At least one AWS account where your application resources run – Sufficient access to create IAM roles and policies across accounts. AWS DevOps Agent requires two distinct sets of IAM permissions: 
  Agent Space role permissions – The IAM role that AWS DevOps Agent assumes to query your AWS resources, access CloudWatch Logs, and discover topology. This role requires the  managed policy plus additional permissions for AWS Support and expanded capabilities. See the CLI onboarding guide for the complete role configuration.Operator app role permissions – The IAM role that controls what human operators can do in the AWS DevOps Agent web application, such as starting investigations, viewing results, and creating AWS Support cases. This role is separate from the agent’s investigation permissions.Service Control Policies (SCPs) – Verify that your organization’s SCPs allow AWS DevOps Agent API actions. Common issue: Teams complete Agent Space setup but investigations fail because SCPs block  actions or  actions. Review your AWS Organization’s SCPs and add exceptions for DevOps Agent if needed. Note that DevOps Agent and Amazon Bedrock inference are not impacted by policies that restrict customer content to specific AWS regions—Bedrock may use US regions other than US East (N. Virginia) for stateless inference. – At minimum, Amazon CloudWatch (automatically available via IAM roles) and Amazon CloudTrail. For comprehensive investigations, integrate Application Performance Monitoring tools like Datadog, Dynatrace, New Relic, Grafana, or Splunk. See Connecting telemetry sources for supported integrations.Understanding third-party integration configuration – Some third-party tools require a two-step configuration process: 
  Account-level registration – Tools that use OAuth (like GitHub, Dynatrace) must first be registered at the AWS account level through the DevOps Agent console. This establishes OAuth credentials that are shared across all Agent Spaces in your account.Agent Space-level association – After registration, each Agent Space individually specifies which resources from that tool to use. For example, after registering GitHub once, Agent Space “EcommerceProd” can associate only production repositories while Agent Space “EcommerceNonProd” associates development repositories.Other tools like Datadog, New Relic, and Splunk can be directly associated with an Agent Space using API keys or tokens without separate account-level registration. CloudWatch requires no additional configuration beyond IAM roles. – GitHub or GitLab repository access for code context and deployment correlation (optional but highly recommended) – AWS CDK (TypeScript/Python), Terraform, AWS CLI, or AWS Management Console for Agent Space deploymentWith prerequisites verified, you’re ready to create your Agent Space and establish the IAM trust relationships that enable investigations.AWS DevOps Agent requires IAM roles in each AWS account within the Agent Space boundary. The agent assumes these roles to query CloudWatch Logs, describe resources, and build application topology.The AWS DevOps Agent is designed to retrieve operational data from multiple AWS Regions across all AWS accounts that you grant access to within the configured Agent Space, enabling comprehensive visibility into distributed infrastructure and applications regardless of their geographic deployment, while supporting multiple accounts through a configuration process that involves creating IAM roles with appropriate trust policies and permissions in secondary accountsNavigate to the AWS DevOps Agent console and choose Create Agent Space and follow the guided setup to create IAM roles in each target account.Figure 4: Creating an Agent Space in the ConsoleThe setup wizard helps in configuring cross-account trust relationships.Figure 5: Multiple account configuration for your Agent SpaceWe provide sample CDK and Terraform templates that automate Agent Space creation and IAM role deployment across multiple accounts.AWS CDK example (TypeScript)://If you have many accounts, use a loop:

const accounts = [
  { id: '111111111111', name: 'Prod', role: prodRole, stage: 'prod' },
  { id: '222222222222', name: 'Dev', role: devRole, stage: 'dev' },
  { id: '333333333333', name: 'Test', role: testRole, stage: 'test' },
];

accounts.forEach(account => {
  const association = new devopsagent.CfnAssociation(this, `${account.name}Association`, {
    agentSpaceId: agentSpace.ref,
    serviceId: 'aws',
    configuration: {
      aws: {
        assumableRoleArn: account.role.roleArn,
        accountId: account.id,
        accountType: 'monitor'
      }
    }
  });

  association.addDependency(agentSpace);
  cdk.Tags.of(association).add('stage', account.stage);
});For detailed instructions on setting up IAM roles and permissions across accounts, see the CLI Onboarding Guide.Once your Agent Space exists and has access to AWS accounts, the next critical step is connecting the observability and development tools that provide investigation context beyond AWS native services.AWS DevOps Agent investigates incidents by correlating data from multiple sources. The more context available, the more accurate the root cause analysis.Recommended integrations by priority: – Provides logs, metrics, and traces from AWS services. The agent queries CloudWatch Logs Insights automatically during investigations. No additional configuration is needed if IAM roles are properly configured.Application Performance Monitoring tools – Datadog, Dynatrace, New Relic, and Splunk provide distributed tracing, custom metrics, and application-level context. Configure via Agent Space integrations in the AWS Console. – GitHub or GitLab integration enables the agent to review recent deployments and code changes. Requires OAuth or personal access token. – GitHub Actions or GitLab workflows help the agent correlate incidents with deployment timing. Configured alongside code repository integration. – Slack and ServiceNow integration enables DevOps Agent to post real-time investigation updates to team channels and automatically update incident tickets with findings, root cause analysis, and recommended mitigation steps throughout the investigation lifecycle.Beyond built-in integrations, AWS DevOps Agent supports webhook triggered investigations and custom MCP (Model Context Protocol) servers so you can bring-your-own observability tools.Webhook configuration for investigation triggersWebhooks allow external systems (Grafana, Prometheus, PagerDuty, custom monitoring tools) to automatically trigger DevOps Agent investigations when incidents occur. Each Agent Space receives a unique webhook URL that accepts JSON payloads describing the incident. Webhooks use HMAC signatures for security. Store the webhook secret in AWS Secrets Manager and rotate it according to your security policies. Ensure your monitoring tool sends incident context including timestamps, affected resources, and symptom descriptions. Richer context enables more accurate investigations.Bring-your-own MCP serversIf you use observability tools beyond the built-in integrations (Grafana, Prometheus, custom telemetry systems), you can connect them via MCP servers. MCP servers expose your tool’s data through a standardized protocol that DevOps Agent queries during investigations.Publicly accessible HTTPS endpoint: MCP servers must be reachable from the public internet. VPC-hosted servers are not currently supported.: For security, only expose MCP tools that perform read operations. Write operations introduce prompt injection risks.: Register MCP servers at the account level, then selectively enable specific tools per Agent Space. Don’t grant access to all tools—choose only those relevant to investigations.Authentication misconfiguration: MCP servers support OAuth 2.0 or API key authentication. Verify your OAuth client credentials are correct and that token exchange URLs are accessible from AWS infrastructure.: MCP tool names have a maximum length of 64 characters. Longer names will fail registration.: Use the full HTTPS URL including path. Example: https://mcp.example.com/v1/mcp not just .After configuring webhooks or MCP servers, trigger a test investigation to verify connectivity:For webhooks: Send a test payload from your monitoring tool and verify the investigation starts in the DevOps Agent web appFor MCP servers: Start an investigation manually and check the agent journal to confirm it successfully called your MCP toolsReview any errors in AWS CloudTrail logs which capture all DevOps Agent API calls including integration attemptsWith your data sources connected, you now need to ensure the right people have appropriate access to investigations while maintaining security boundaries.Agent Spaces support fine-grained access controls to ensure only authorized team members can interact with investigations.Access control considerations:Who should view investigations? Typically on-call engineers, SREs, and DevOps engineers. Consider including security teams for security-related incidents.Who should create AWS Support cases? Typically on-call leads and senior engineers. Restrict this permission to prevent excessive case creation.Who should modify Agent Space configuration? Typically central operations or infrastructure teams. Separate this from day-to-day investigation access.IAM-based access control:AWS DevOps Agent uses IAM policies to control access to Agent Spaces. Attach policies to IAM users, groups, or roles:{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "devopsagent:GetAgentSpace",
        "devopsagent:StartInvestigation",
        "devopsagent:GetInvestigation",
        "devopsagent:ListInvestigations"
      ],
      "Resource": "arn:aws:devopsagent:us-east-1:123456789012:agentspace/EcommerceProd"
    }
  ]
}
AWS DevOps Agent operates within your AWS environment with privileged access to operational data across multiple accounts. While general security foundations apply, Agent Space configuration introduces specific considerations. For comprehensive security guidance, see the AWS DevOps Agent Security documentation.Access controls are in place—now it’s time to validate that your Agent Space configuration provides the investigation coverage you need.Agent Space configuration is a two-way door decision. Start with a focused scope and expand based on investigation results.Testing your Agent Space: Trigger a test investigation using the AWS DevOps Agent web app.Start an investigation and provide symptoms such as “High latency on /api/checkout endpoint”.Observe which resources the agent queries.Review investigation completeness. Did the agent identify the root cause?Were any accounts or services missing from the investigation?Did the agent have sufficient telemetry data?Adjust Agent Space boundaries based on results.Add accounts if investigations lack context.Add integrations if telemetry gaps exist.Narrow scope if performance degrades.AWS DevOps Agent transforms incident response from a manual, time-consuming process into an autonomous, data-driven investigation. However, the agent’s effectiveness depends on proper Agent Space configuration. By following the on-call based approach—granting access to accounts relevant to your application while separating production from non-production environments—you provide sufficient context for accurate root cause analysis without introducing unnecessary complexity.Think on-call boundaries – Agent Space scope should mirror how your team investigates incidentsUse Infrastructure as Code – CDK and Terraform templates ensure consistent, repeatable deploymentsIntegrate observability tools – More data sources equals more accurate investigationsIterate based on results – Expand or narrow Agent Space scope as investigation patterns emergeWe’re committed to making AWS DevOps Agent easier to adopt and more accurate in solving customer problems. Your Agent Space setup is the foundation for achieving fast, reliable incident resolution. Have questions or feedback? Leave a comment below.]]></content:encoded></item><item><title>Software Supply Chain Threats Are on the OWASP Top Ten—Yet Nothing Will Change Unless We Do</title><link>https://devops.com/software-supply-chain-threats-are-on-the-owasp-top-ten-yet-nothing-will-change-unless-we-do/</link><author>Joseph M. Saunders</author><category>devops</category><pubDate>Wed, 28 Jan 2026 16:39:39 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Github Actions: Checking Out And Utilizing a Reusable Workflow’s Repository</title><link>https://blog.devops.dev/github-actions-checking-out-and-utilizing-a-reusable-workflows-repository-992adbe7b3ae?source=rss----33f8b2d9a328---4</link><author>Matt Cummings</author><category>devops</category><pubDate>Wed, 28 Jan 2026 14:46:01 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Managing flows between different repositoriesGithub Actions are designed to have various levels of reusability, such as composite actions or reusable workflows. A common approach to storing these items is having them contained in a separate repository, including tangential items related to them, such as scripts.This article explores how a caller workflow reaches out to a reusable workflow and checks out its repository to execute a script to perform some basic logic. It’s a guide but not in a step-by-step format. However, you can follow along with the code this article is based on via the various repository links provided in the  section below (in particular, the README in the ttn-workflows repository is very thorough for some reason — I must have been in the mood for documentation).Be sure to check out the other links as well —for your convenience, they are all listed in the  section at the end of this article.The organization org-mushroom-kingdom has recently begun work on a news application “Toad Town News.” The work for this application has broken up into three repositories:ttn-frontend, which handles the frontend of the application.ttn-backend, which handles the backend of the application.ttn-workflows, a repository dedicated to reusable workflows and items related to them such as scripts.The senior devs of this application have decided that any major frontend or backend release must have a pull request that includes a changelog file (hereafter referred to as a CHANGELOG file to match how it’s referred to in the above repositories’ code/documentation). This file gives the summary of the major changes associated with that release.This CHANGELOG file has strict conventions surrounding it: there must exactly one CHANGELOG file present per release branch. Furthermore, this file must meet certain naming conventions.When a pull request in ttn-frontend or ttn-backend is opened whose source branch is a release branch (a branch that begins with the string "release") and whose target branch is main, a caller workflow is activated. This caller workflow calls a reusable workflow (changelog-quality-checks.yml, full path .github/workflows/changelog-quality-checks.yml) in the ttn-workflows repository that when triggered will check if the release branch meets the expected CHANGELOG file criteria.Some of changelog-quality-checks.yml’s work is performed by a script (changelog-quality-checks.py, full path scripts/changelog-quality-check.py). This script is also present in the ttn-workflows repository. This article won’t break down the CHANGELOG file assessment-related code present in the reusable workflow nor its script as that is not the main focus here— however, this logic  elaborated extensively in the ttn-workflows README.Checking out ttn-workflows: Why and HowIf you’ve read my previousarticles about reusable workflows, you will see that things get a little more complex here compared to previous scenarios where all the work could be performed within the reusable workflow file itself. This time, we’re referencing a script within the reusable workflow’s repository and we have explicitly point to it to perform the brunt of our logic.Why the Need to Checkout?When a caller workflow is triggered, it is run within the context of its own repository. This poses an issue for us— trying to run or even grant permissions to the reusable workflow’s script will give us errors that the script doesn’t exist.This may seem obvious because there is no scripts/changelog-quality-check.py path in ttn-frontend or ttn-backend, but it surprised me at first. I had assumed that the reusable workflow could access a script in its own repository with no problem — you might think that since it’s running, it would implicitly switch to the context of its own repo. As I found out, that’s not the case. Our caller workflow fired off first, so we’re running things from the context of its repository. Because of that, we have to checkout the ttn-workflows repo so we can access the script.How to Checkout Another Repository in a WorkflowSo  do we access this script? We checkout the reusable workflow’s repository, then point to it. It’s easier than you think — if you’ve used Github Actions at all, you’re familiar with the practically ubiquitous action actions/checkout.It’s commonly the first thing you see in several workflows. In this scenario we use it twice: first in the caller repo, and now again in the reusable workflow, this time with arguments!Take a look at the changelog-quality-checks.yml gist below:The “Checkout ttn-workflows repo (if non-manually triggered)” step assesses if we’re running the workflow manually (if we were, the checkout would be skipped since we’d already be in the context of the ttn-workflows repo). Since the workflow was triggered via workflow_call, the step will use actions/checkout in conjunction with the following arguments:repository: The specific repository to checkout. (: In the actions/checkout used in the caller workflow, we don’t need to specify this because the default activity checks out the caller workflow’s repository. Since we need to check out ttn-workflows, we specify it here.)path: The directory in the runner to put the repository contents (: this is relative to GITHUB_WORKSPACE. It should be present at top-level. In this example we checkout the ttn-workflows repository into the ttn-workflows-repo directory).token: The token that has the relevant permissions (specifically, the repo permission) necessary to checkout the reusable workflow’s repository. (: This is a generated token — we can’t use the pre-established GITHUB_TOKEN in the caller repo and pass it to the reusable workflow because GITHUB_TOKEN’s permissions are scoped to a single repository.)Accessing the Contents of a Checked Out RepoAfter we’ve checked out the ttn-workflows repository into the ttn-workflows-repo path, we can then call the changelog-quality-check.py script simply by referencing its path like so:The logic above also accounts changelog-quality-checks.yml being triggered manually or not. It isn’t in our case, so we set the REPO_PATH environmental variable to “ttn-workflows-repo”, the value of path used in the previous step. In other words, the full path used in the lower two steps would be ttn-workflows-repo/scripts/changelog-quality-check.py.Based upon the results of the script, the changelog-quality-checks.yml workflow returns a status (not pictured) that can then be leveraged by the caller workflow, such as in a branch protection rule. Standard post-checkout activity occurs, and that’s it. Not bad at all!Testing to see if the caller-reusable workflow relationship works as expected is very easy. All we need to do is create a release branch, make a commit on it, and then open a pull request to main.The synchronize trigger is key here — I can just make another commit while the pull request is open and run a different test case. For example, let’s say in ttn-frontend I have a release/v1.2 branch, and my first commit is creating a CHANGELOG file named CHANGELOG-frontend-v1.2.txt (the happy path scenario). I open the pull request which will be the very first time the caller workflow is triggered.I then go and look at the results. Things look great, but I still want to test scenarios like a bad CHANGELOG name. To do that, I can just rename my CHANGELOG file incorrectly in a new commit, and then push it. The pull request more or less detects that change (this is the synchronize activity). That triggers the caller workflow again; I can rinse and repeat from there.As an aside, in previously displayed code snippets there is logic that allows one to test this reusable workflow via manually triggering it in its own repository in addition to calling it from a caller workflow. Basically, the reusable workflow assesses what repository called it and/or if it was manually executed. The evaluation of that determines whether the workflow will execute certain steps. It’s handy to have it set up like this in case you want to specifically test something related to the reusable workflow/script versus the caller-reusable workflow relationship, or if you want to iterate further on the reusable workflow.This article has shown how to call upon a reusable workflow in a different repository and explained the how and why of checking out that workflow’s repository to execute desired behavior. Using the methodology explained here, you can see why it’s important to properly contain reusable workflows and their associated items (like scripts) in their own repository. Perhaps more importantly, you have been shown  to link it all together.Are you as sick as I am of the word ‘reusable’ after getting to this point? Once you get over that malaise, try setting up something similar for yourself! The best way to learn, after all, is through (Github) action!Links present in the article are listed here in order of appearance.Please note the following:Some links may be present more than once in the article. To keep things brief, only the first instance is listed.A # present in the title of the source indicates a bookmark on that page.]]></content:encoded></item><item><title>Apiiro Guardian Agent Prevents AI Models From Generating Insecure Code</title><link>https://devops.com/apiiro-guardian-agent-prevents-ai-models-from-generating-insecure-code/</link><author>Mike Vizard</author><category>devops</category><pubDate>Wed, 28 Jan 2026 14:00:11 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Anthropic Adds Automated Security Reviews to Claude Code</title><link>https://devops.com/anthropic-adds-automated-security-reviews-to-claude-code/</link><author>Tom Smith</author><category>devops</category><pubDate>Wed, 28 Jan 2026 12:41:37 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AWS CloudFormation 2025 Year In Review</title><link>https://aws.amazon.com/blogs/devops/aws-cloudformation-2025-year-in-review/</link><author>Idriss Laouali Abdou</author><category>devops</category><pubDate>Wed, 28 Jan 2026 01:08:08 +0000</pubDate><source url="https://aws.amazon.com/blogs/devops/">AWS DevOps blog</source><content:encoded><![CDATA[AWS CloudFormation enables you to model and provision your cloud application infrastructure as code-base templates. Whether you prefer writing templates directly in JSON or YAML, or using programming languages like Python, Java, and TypeScript with the AWS Cloud Development Kit (CDK), CloudFormation and CDK provide the flexibility you need. For organizations adopting multi-account strategies, CloudFormation StackSets offers a powerful capability to deploy resources across multiple regions and accounts in parallel.In 2025, we delivered a comprehensive set of major enhancements focused on three core areas: reducing dev-test cycle through early validation, improving deployment safety with improved configuration drift management, and integrating IaC context to AI-powered development tools.These launches address common pain points in infrastructure development workflows, from catching deployment errors before resource provisioning to managing configuration drift systematically. The features span the entire development lifecycle, from template authoring in your IDE to multi-account deployments at scale.This blog provides an overview of the key capabilities we launched in 2025 and how they improve your infrastructure development workflow.Early Validation & Enhanced Troubleshooting: Pre-Deployment Error DetectionCloudFormation now validates your templates during change set (preview of infrastructure changes before deployment) creation, catching common deployment errors before resource provisioning begins. The validation checks for invalid property syntax, resource name conflicts with existing resources in your account, and S3 bucket emptiness constraints on delete operations.Figure 1: Pre-deployment validations viewWhen validation fails, the change set status shows ‘FAILED’ with detailed information about each issue, including the property path where problems occur. This early feedback helps you fix issues faster rather than waiting for deployment failures.Improved Deployment troubleshootingFor runtime errors that occur during deployment, every stack operation now receives a unique operation ID. You can filter stack events by operation ID to quickly identify root causes, reducing troubleshooting time from minutes to seconds. The new describe-events API provides grouped access to events. You can query events for a specific operation, filter to FAILED status events, and extract the root cause without parsing through the entire stack event history.CloudFormation IDE Experience: Language Server Protocol IntegrationWe launched the AWS CloudFormation Language Server, bringing end-to-end infrastructure development directly into your IDE. Available through the AWS Toolkit for Visual Studio Code, Kiro, and other compatible IDEs, this capability transforms how you author CloudFormation templates.Figure 1: Initializing a CloudFormation project with environment configurationThe Language Server provides context-aware auto-completion that understands CloudFormation semantics. When you define resources, it suggests only required properties automatically, while optional properties appear on hover. Built-in validation catches issues before deployment integrating early validation capabilities, flagging invalid resource properties, missing IAM permissions, and security policy violations using CloudFormation Guard.Figure 2: Hover information displaying optional properties and their documentationThe drift-aware deployment view highlights differences between your template and deployed infrastructure, helping you spot configuration changes made outside CloudFormation. The Language Server also provides semantic navigation features, go-to-definition for logical IDs, find-all-references for resource dependencies, and hover documentation that pulls from the CloudFormation resource specification. These features work across intrinsic functions like !Ref, !GetAtt, and !Sub, understanding the CloudFormation template structure. By integrating validation and real-time feedback directly into your authoring experience, the Language Server keeps you in flow state, reducing context switching between your IDE, AWS Console, and documentation.Figure 3: Type-aware completions for intrinsic functions like !GetAtt & !RefStack Refactoring: Adapt your infrastructure to your organization evolutionStack Refactoring enables you to reorganize your CloudFormation and CDK infrastructure without disrupting deployed resources. You can move resources between stacks, rename logical IDs, and decompose monolithic stacks into focused components while maintaining resource stability and operational state.Whether you’re modernizing legacy stacks, aligning infrastructure with evolving architectural patterns, or improving long-term maintainability, Stack Refactoring adapts your CloudFormation and CDK organization to changing requirements. The console and CDK experience, launched this year, extends the earlier CLI capability, making refactoring accessible through your preferred interface.Configuration drift occurs when infrastructure managed by CloudFormation is modified through the AWS Console, SDK, or CLI. Drift-aware change sets address this challenge by providing a three-way comparison between your new template, last-deployed template, and actual infrastructure state.Figure 4: Examine the drift-aware change set to see the dangerous memory reduction that would occurThis capability helps you prevent unexpected overwrites of drift. If your change set preview shows unintended changes, you can update your template values and recreate the change set before deployment. During execution, CloudFormation matches resource properties with template values and recreates resources deleted outside of CloudFormation.Drift-aware change sets enable you to systematically revert drift and keep infrastructure in sync with templates, strengthening reproducibility for testing and disaster recovery while maintaining your security posture.CloudFormation Hooks: Control Catalog with HooksAWS CloudFormation Hooks now supports managed proactive controls, enabling customers to validate resource configurations against AWS best practices without writing custom Hooks logic. Customers can select controls from the AWS Control Tower Controls Catalog and apply them during CloudFormation operations. When using CloudFormation, customers can configure these controls to run in warn mode, allowing teams to test controls without blocking deployments and giving them the flexibility to evaluate control behavior before enforcing policies in production. This significantly reduces setup time, eliminates manual errors, and ensures comprehensive governance coverage across your infrastructure.AWS also introduced a new Hooks Invocation Summary page in the CloudFormation console. This centralized view provides a complete historical record of Hooks activity, showing which controls were invoked, their execution details, and outcomes such as pass, warn, or fail. This simplifies compliance reporting issues faster.With this launch, customers can now leverage AWS-managed controls as part of their provisioning workflows, eliminating the overhead of writing and maintaining custom logic. These controls are curated by AWS and aligned with industry best practices, helping teams enforce consistent policies across all environments. The new summary page delivers essential visibility into Hook invocation history, enabling faster issue resolution and streamlined compliance reporting.StackSets Deployment OrderingCloudFormation StackSets now supports deployment ordering for auto-deployment mode, enabling you to define the sequence in which stack instances automatically deploy across accounts and regions. This capability coordinates complex multi-stack deployments where foundational infrastructure must be provisioned before dependent application components.When creating or updating a StackSet, you can specify up to 10 dependencies per stack instance using the DependsOn parameter in the AutoDeployment configuration. StackSets automatically orchestrates deployments based on your defined relationships. For example, you can ensure networking and security stack instances complete deployment before application stack instances begin, preventing deployment failures due to missing dependencies.StackSets includes built-in cycle detection to prevent circular dependencies and provides error messages to help resolve configuration issues. This feature is available at no additional cost in all AWS Regions where CloudFormation StackSets is available.We introduced the AWS Infrastructure-as-Code (IaC) MCP Server, bridging AI assistants with your AWS infrastructure development workflow. Built on the Model Context Protocol (MCP), this server enables AI assistants like Kiro CLI to help you search CloudFormation and CDK documentation, validate templates, troubleshoot deployments, and follow best practices, all while maintaining the security of local execution.Figure 1: Kiro-CLI with AWS IaC MCP serverThe IaC MCP Server provides nine specialized tools organized into two categories. Remote documentation search tools connect to AWS knowledge bases to retrieve up-to-date information about CloudFormation resources, CDK APIs, and implementation guidance. Local validation and troubleshooting tools run entirely on your machine, performing syntax validation with cfn-lint, security checks with CloudFormation Guard, and deployment failure analysis with integrated CloudTrail events.Figure 4: Validate my CloudFormation template with AWS IaC MCP ServerIntelligent Documentation AssistantInstead of manually searching through documentation, ask your AI assistant natural language questions:“How do I create an S3 bucket with encryption enabled in CDK?”The server searches CDK best practice and samples, returning relevant code examples and explanations.     2. Proactive Template ValidationBefore deploying infrastructure changes:User: “Validate my CloudFormation template and check for security issues”AI Agent: [Uses validate_cloudformation_template and check_cloudformation_template_compliance]“Found 2 issues: Missing encryption on EBS volumes,and S3 bucket lacks public access block configuration”  3. Rapid Deployment TroubleshootingWhen a stack deployment fails:User: “My stack ‘stack_03’ in us-east-1 failed to deploy. What happened?”AI Agent: [Uses troubleshoot_stack_deployment with CloudTrail integration]“The deployment failed due to insufficient IAM permissions.CloudTrail shows AccessDenied for ec2:CreateVpc.You need to add VPC permissions to your deployment role.”     4. Learning and ExplorationNew to AWS CDK? The server helps you discover constructs and patterns:User: “Show me how to build a serverless API”AI Agent: [Searches CDK constructs and samples]“Here are three approaches using API Gateway + Lambda…”Here are some resources to help you get started learning and using CloudFormation to manage your cloud infrastructure:As we begin 2026, our focus remains on making infrastructure deployment faster, safer, and more manageable. The launches in 2025 reflect our commitment to solving real customer challenges and improving the CloudFormation developer experience. From intelligent IDE integrations to AI-powered assistance, these capabilities help you build infrastructure with greater confidence and efficiency.We encourage you to try these features and share your feedback. For detailed information about any of these launches, visit our documentation or check out the AWS DevOps Blog.]]></content:encoded></item><item><title>The Role of Observability in Successful Cloud Migrations</title><link>https://devops.com/the-role-of-observability-in-successful-cloud-migrations/</link><author>Jenn Yarnold</author><category>devops</category><pubDate>Wed, 28 Jan 2026 00:13:10 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[As organizations face rising VMware costs and tighter renewal timelines, migrating to AWS has become both urgent and complex. This article explores how an observability-first approach—spanning pre-migration planning, real-time execution, and post-migration optimization—helps IT leaders reduce risk, control costs, and ensure successful cloud migrations.]]></content:encoded></item><item><title>Experimenting with Gateway API using kind</title><link>https://kubernetes.io/blog/2026/01/28/experimenting-gateway-api-with-kind/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[This document will guide you through setting up a local experimental environment with Gateway API on kind. This setup is designed for learning and testing. It helps you understand Gateway API concepts without production complexity.This is an experimentation learning setup, and should not be used for production. The components used on this document are not suited for production usage.
Once you're ready to deploy Gateway API in a production environment,
select an implementation that suits your needs.Set up a local Kubernetes cluster using kind (Kubernetes in Docker)Deploy cloud-provider-kind, which provides both LoadBalancer Services and a Gateway API controllerCreate a Gateway and HTTPRoute to route traffic to a demo applicationTest your Gateway API configuration locallyThis setup is ideal for learning, development, and experimentation with Gateway API concepts.Before you begin, ensure you have the following installed on your local machine: - Required to run kind and cloud-provider-kind - The Kubernetes command-line tool - Kubernetes in Docker - Required to test the routesCreate a new kind cluster by running:This will create a single-node Kubernetes cluster running in a Docker container.Install cloud-provider-kindA LoadBalancer controller that assigns addresses to LoadBalancer-type ServicesA Gateway API controller that implements the Gateway API specificationIt also automatically installs the Gateway API Custom Resource Definitions (CRDs) in your cluster.Run cloud-provider-kind as a Docker container on the same host where you created the kind cluster: On some systems, you may need elevated privileges to access the Docker socket.Verify that cloud-provider-kind is running:You should see the container listed and in a running state. You can also check the logs:Experimenting with Gateway APINow that your cluster is set up, you can start experimenting with Gateway API resources.cloud-provider-kind automatically provisions a GatewayClass called . You'll use this class to create your Gateway.It is worth noticing that while kind is not a cloud provider, the project is named as  as it provides features that simulate a cloud-enabled environment.The following manifest will:Create a new namespace called Deploy a Gateway that listens on port 80Accept HTTPRoutes with hostnames matching the  patternAllow routes from any namespace to attach to the Gateway.
: In real clusters, prefer Same or Selector values on the  namespace selector field to limit attachments.Apply the following manifest:Then verify that your Gateway is properly programmed and has an address assigned:NAME CLASS ADDRESS PROGRAMMED AGE
gateway cloud-provider-kind 172.18.0.3 True 5m6s
The PROGRAMMED column should show True, and the ADDRESS field should contain an IP address.Deploy a demo applicationNext, deploy a simple echo application that will help you test your Gateway configuration. This application:Echoes back request details including path, headers, and environment variablesRuns in a namespace called Apply the following manifest:Now create an HTTPRoute to route traffic from your Gateway to the echo application.
This HTTPRoute will:Respond to requests for the hostname some.exampledomain.exampleRoute traffic to the echo applicationAttach to the Gateway in the  namespaceApply the following manifest:The final step is to test your route using curl. You'll make a request to the Gateway's IP address with the hostname some.exampledomain.example. The command below is for POSIX shell only, and may need to be adjusted for your environment:You should receive a JSON response similar to this:If you see this response, congratulations! Your Gateway API setup is working correctly.If something isn't working as expected, you can troubleshoot by checking the status of your resources.First, inspect your Gateway resource:Look at the  section for conditions. Your Gateway should have: - The Gateway was accepted by the controller - The Gateway was successfully configured populated with an IP addressCheck the HTTPRoute statusNext, inspect your HTTPRoute:Check the  section for conditions. Common issues include:ResolvedRefs set to False with reason ; this means that the backend Service doesn't exist or has the wrong nameAccepted set to False; this means that the route couldn't attach to the Gateway (check namespace permissions or hostname matching)Example error when a backend is not found:If the resource statuses don't reveal the issue, check the cloud-provider-kind logs:This will show detailed logs from both the LoadBalancer and Gateway API controllers.When you're finished with your experiments, you can clean up the resources:Remove Kubernetes resourcesDelete the namespaces (this will remove all resources within them):Stop and remove the cloud-provider-kind container:Because the container was started with the  flag, it will be automatically removed when stopped.Finally, delete the kind cluster:Now that you've experimented with Gateway API locally, you're ready to explore production-ready implementations:: Explore the Gateway API documentation to learn about advanced features like TLS, traffic splitting, and header manipulation: Experiment with path-based routing, header matching, request mirroring and other features following Gateway API user guidesThis  setup is for development and learning only.
Always use a production-grade Gateway API implementation for real workloads.]]></content:encoded></item><item><title>Cluster API v1.12: Introducing In-place Updates and Chained Upgrades</title><link>https://kubernetes.io/blog/2026/01/27/cluster-api-v1-12-release/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Tue, 27 Jan 2026 16:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Cluster API brings declarative management to Kubernetes cluster lifecycle, allowing users and platform teams to define the desired state of clusters and rely on controllers to continuously reconcile toward it.Similar to how you can use StatefulSets or Deployments in Kubernetes to manage a group of Pods, in Cluster API you can use KubeadmControlPlane to manage a set of control plane Machines, or you can use MachineDeployments to manage a group of worker Nodes.The Cluster API v1.12.0 release expands what is possible in Cluster API, reducing friction in common lifecycle operations by introducing in-place updates and chained upgrades.Emphasis on simplicity and usabilityWith v1.12.0, the Cluster API project demonstrates once again that this community is capable of delivering a great amount of innovation, while at the same time minimizing impact for Cluster API users.What does this mean in practice?Users simply have to change the Cluster or the Machine spec (just as with previous Cluster API releases), and Cluster API will automatically trigger in-place updates or chained upgrades when possible and advisable.Like Kubernetes does for Pods in Deployments, when the Machine spec changes also Cluster API performs rollouts by creating a new Machine and deleting the old one.This approach, inspired by the principle of immutable infrastructure, has a set of considerable advantages:It is simple to explain, predictable, consistent and easy to reason about with users and engineers.It is simple to implement, because it relies only on two core primitives, create and delete.Implementation does not depend on Machine-specific choices, like OS, bootstrap mechanism etc.As a result, Machine rollouts drastically reduce the number of variables to be considered when managing the lifecycle of a host server that is hosting Nodes.However, while advantages of immutability are not under discussion, both Kubernetes and Cluster API are undergoing a similar journey, introducing changes that allow users to minimize workload disruption whenever possible.Over time, also Cluster API has introduced several improvements to immutable rollouts, including:The new in-place update feature in Cluster API is the next step in this journey.With the v1.12.0 release, Cluster API introduces support for update extensions allowing users to make changes on existing machines in-place, without deleting and re-creating the Machines.Both KubeadmControlPlane and MachineDeployments support in-place updates based on the new update extension, and this means that the boundary of what is possible in Cluster API is now changed in a significant way.How do in-place updates work?The simplest way to explain it is that once the user triggers an update by changing the desired state of Machines, then Cluster API chooses the best tool to achieve the desired state.The news is that now Cluster API can choose between immutable rollouts and in-place update extensions to perform required changes.Importantly, this is not immutable rollouts vs in-place updates; Cluster API considers both valid options and selects the most appropriate mechanism for a given change.From the perspective of the Cluster API maintainers, in-place updates are most useful for making changes that don't otherwise require a node drain or pod restart; for example: changing user credentials for the Machine. On the other hand, when the workload will be disrupted anyway, just do a rollout.Nevertheless, Cluster API remains true to its extensible nature, and everyone can create their own update extension and decide when and how to use in-place updates by trading in some of the benefits of immutable rollouts.ClusterClass and managed topologies in Cluster API jointly provided a powerful and effective framework that acts as a building block for many platforms offering Kubernetes-as-a-Service.Now with v1.12.0 this feature is making another important step forward, by allowing users to upgrade by more than one Kubernetes minor version in a single operation, commonly referred to as a chained upgrade.This allows users to declare a target Kubernetes version and let Cluster API safely orchestrate the required intermediate steps, rather than manually managing each minor upgrade.The simplest way to explain how chained upgrades work, is that once the user triggers an update by changing the desired version for a Cluster, Cluster API computes an upgrade plan, and then starts executing it. Rather than (for example) update the Cluster to v1.33.0 and then v1.34.0 and then v1.35.0, checking on progress at each step, a chained upgrade lets you go directly to v1.35.0.Executing an upgrade plan means upgrading control plane and worker machines in a strictly controlled order, repeating this process as many times as needed to reach the desired state. The Cluster API is now capable of managing this for you.Cluster API takes care of optimizing and minimizing the upgrade steps for worker machines, and in fact worker machines will skip upgrades to intermediate Kubernetes minor releases whenever allowed by the Kubernetes version skew policies.Also in this case extensibility is at the core of this feature, and upgrade plan runtime extensions can be used to influence how the upgrade plan is computed; similarly, lifecycle hooks can be used to automate other tasks that must be performed during an upgrade, e.g. upgrading an addon after the control plane update completed.From our perspective, chained upgrades are most useful for users that struggle to keep up with Kubernetes minor releases, and e.g. they want to upgrade only once per year and then upgrade by three versions (n-3 → n). But be warned: the fact that you can now easily upgrade by more than one minor version is not an excuse to not patch your cluster frequently!I would like to thank all the contributors, the maintainers, and all the engineers that volunteered for the release team.The reliability and predictability of Cluster API releases, which is one of the most appreciated features from our users, is only possible with the support, commitment, and hard work of its community.Kudos to the entire Cluster API community for the v1.12.0 release and all the great releases delivered in 2025!
​​
If you are interested in getting involved, learn about
Cluster API contributing guidelines.If you read the Cluster API manifesto, you can see how the Cluster API subproject claims the right to remain unfinished, recognizing the need to continuously evolve, improve, and adapt to the changing needs of Cluster API’s users and the broader Cloud Native ecosystem.As Kubernetes itself continues to evolve, the Cluster API subproject will keep advancing alongside it, focusing on safer upgrades, reduced disruption, and stronger building blocks for platforms managing Kubernetes at scale.Innovation remains at the heart of Cluster API, stay tuned for an exciting 2026!]]></content:encoded></item><item><title>Complete Go Environment Setup for TSS Coordinator (MPC) with gRPC &amp; Barbican</title><link>https://blog.devops.dev/complete-go-environment-setup-for-tss-coordinator-mpc-with-grpc-barbican-58bda9ce7366?source=rss----33f8b2d9a328---4</link><author>Tushar</author><category>devops</category><pubDate>Tue, 27 Jan 2026 14:47:17 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[A step-by-step, production-ready guide to installing Go, configuring dependencies, and setting up a secure TSS (Threshold Signature…]]></content:encoded></item><item><title>The Four Knobs of AI Agent Reliability: A DevOps Perspective</title><link>https://devops.com/the-four-knobs-of-ai-agent-reliability-a-devops-perspective/</link><author>Arun Sanna</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:50:00 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How DNS resolution works in custom docker bridge network and not on default docker0 bridge network?</title><link>https://blog.devops.dev/how-dns-resolution-works-in-custom-docker-bridge-network-and-not-on-default-docker0-bridge-network-adab783cdb49?source=rss----33f8b2d9a328---4</link><author>Sanjal S Eralil</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:04:01 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[When you start working with Docker, one of the first surprises you encounter is that containers can’t find each other by name on the default network. You try ping db from your app container, and it fails with "Name or service not known." But create a custom network, and suddenly everything works! Let's explore what's happening under the hood.Part 1: The Default Bridge Network (docker0)What is the Default Bridge Network?When Docker is installed, it automatically creates a bridge network called docker0. Any container you run without specifying a network connects to this default bridge.# Check the default bridgedocker network ls# Output shows:NETWORK ID     NAME      DRIVER    SCOPEabc123def456   bridge    bridge    local    ← This is docker0Setting Up a Simple ExampleLet’s create two containers and see how they communicate:# Start a PostgreSQL database containerdocker run -d --name database postgres:14# Start an Nginx containerdocker run -d --name webserver nginx# Check if they're on the same networkdocker network inspect bridgeBoth containers are now connected to the default bridge network and assigned IPs from the 172.17.0.0/16 subnet.# Get database container's IPdocker inspect database -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'Testing Communication: By IP Works! ✅# Ping database from webserver using IP addressdocker exec webserver ping 172.17.0.2# Output:PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.103 ms64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.075 ms Containers can communicate using IP addresses.Testing Communication: By Name Fails! ❌# Try to ping database by container namedocker exec webserver ping database# Output:ping: database: Name or service not knownThe Problem: No DNS ResolutionLet’s check how DNS is configured in our container:docker exec webserver cat /etc/resolv.conf# Output:nameserver 8.8.8.8The container is using Google’s public DNS servers (or your host’s DNS servers), which have no idea what “database” is. These DNS servers can resolve google.com or github.com, but they don't know anything about your local Docker containers.The Default Bridge LimitationOn the default bridge network:✅ Containers  communicate using IP addresses❌ Containers  communicate using container names❌ No built-in DNS resolution for container names📋 Must manually manage IPs or use legacy --link (deprecated)This is a major pain point for applications! Imagine hardcoding IPs in your database connection strings — nightmare for maintenance!Part 2: Custom Bridge Networks — The SolutionCreating a Custom Bridge NetworkDocker introduced custom networks in version 1.9 to solve this exact problem:# Create a custom bridge networkdocker network create app-network# Verify it was createddocker network ls# Output:NETWORK ID     NAME           DRIVER    SCOPExyz789abc012   app-network    bridge    local    ← Our custom networkRunning Containers on Custom Network# Remove old containersdocker rm -f database webserver# Start containers on our custom networkdocker run -d --name database --network app-network postgres:14docker run -d --name webserver --network app-network nginxTesting Communication: By Name Works! ✅# Ping database by namedocker exec webserver ping database# Output:PING database (172.18.0.2) 56(84) bytes of data.64 bytes from database.app-network (172.18.0.2): icmp_seq=1 ttl=64 time=0.089 ms64 bytes from database.app-network (172.18.0.2): icmp_seq=2 ttl=64 time=0.073 ms Notice it automatically resolved database to 172.18.0.2.The Magic: Built-in DNS Serverdocker exec webserver cat /etc/resolv.conf# Output:nameserver 127.0.0.11```Now the container is using `127.0.0.11` - **Docker's embedded DNS server!**## **Part 3: Under the Hood - How Docker DNS Works**### **The DNS Architecture**When you create a custom bridge network, Docker sets up a sophisticated DNS resolution system:```┌─────────────────────────────────────────────────────────┐│  Container "webserver"                                   ││                                                          ││  1. Application: ping database                           ││         ↓                                                ││  2. Checks /etc/resolv.conf → nameserver 127.0.0.11     ││         ↓                                                ││  3. Sends DNS query to 127.0.0.11:53                    ││         ↓                                                │└─────────┼────────────────────────────────────────────────┘          │ iptables intercepts and redirects┌─────────┼────────────────────────────────────────────────┐│  HOST   │                                                 ││         ↓                                                 ││  ┌──────────────────────────────────┐                   ││  │  Docker Daemon (DNS Resolver)    │                   ││  │                                   │                   ││  │  Internal DNS Table:              │                   ││  │  ┌────────────────────────────┐  │                   ││  │  │ Network: app-network       │  │                   ││  │  │ database  → 172.18.0.2     │  │                   ││  │  │ webserver → 172.18.0.3     │  │                   ││  │  └────────────────────────────┘  │                   ││  │                                   │                   ││  │  Returns: 172.18.0.2              │                   ││  └──────────────────────────────────┘                   ││         ↓                                                 │└─────────┼────────────────────────────────────────────────┘          ↓┌─────────┼────────────────────────────────────────────────┐│  Container receives: database = 172.18.0.2               ││  Connection established! ✅                               │└──────────────────────────────────────────────────────────┘127.0.0.11 — The Magic IPNot a real network interfaceIntercepted by iptables rulesRedirected to Docker daemon’s DNS resolver2. Docker Daemon’s DNS ResolverMaintains internal mapping of container names to IPsSeparate mappings per networkUpdates automatically when containers start/stopRedirect DNS queries from 127.0.0.11:53 to Docker daemonTransparent to the containerVerification: The DNS QueryLet’s see the DNS resolution in action:# Install DNS tools in containerdocker exec -it webserver bashapt-get update && apt-get install -y dnsutils# Query Docker's DNSnslookup database 127.0.0.11# Output:Server:     127.0.0.11Address:    127.0.0.11#53Name:   databaseAddress: 172.18.0.2Perfect! Docker’s DNS server resolved database to its IP address.Part 4: Practical Real-World ExampleBuilding a Multi-Container ApplicationLet’s build a typical web application stack:# Create application networkdocker network create webapp-net# PostgreSQL databasedocker run -d \  --network webapp-net \  -e POSTGRES_PASSWORD=secret \  -e POSTGRES_DB=myapp \# Backend API (Node.js/Python/etc)docker run -d \  --network webapp-net \  -e DATABASE_URL=postgresql://postgres:secret@postgres-db:5432/myapp \  my-api-image# Frontend web serverdocker run -d \  --network webapp-net \  -e API_URL=http://api-server:8080 \Notice the connection strings:@postgres-db:5432 - using , not IP!http://api-server:8080 - using , not IP!This is  and  than hardcoding IPs.# Check if API can reach databasedocker exec api-server ping postgres-dbBenefits of Custom Bridge Networks:Automatic Service DiscoveryContainers find each other by nameWorks even if containers restart with different IPsEach custom network is isolatedContainers on different networks can’t communicate by defaultConnection strings use logical namesEasier to read and maintainDNS mappings update automaticallyNo manual intervention neededContainer restarts handled transparentlyWhy Default Bridge Doesn’t Have DNS:Default bridge existed since Docker 1.0 (2013)DNS feature added in Docker 1.9 (2015)Backward compatibility — couldn’t change default behaviorEncourages best practice (use custom networks)Docker’s DNS resolution on custom bridge networks is a powerful feature that makes container orchestration much simpler. By understanding the difference between the default bridge (no DNS) and custom bridges (built-in DNS), you can build more robust and maintainable containerized applications.The key takeaway: Use custom bridge networks for automatic service discovery through Docker’s embedded DNS server at 127.0.0.11.]]></content:encoded></item><item><title>Deploy Your First ML Model on GCP Part 1: Manual Deployment</title><link>https://blog.devops.dev/deploy-your-first-ml-model-on-gcp-part-1-manual-deployment-933a44d6f658?source=rss----33f8b2d9a328---4</link><author>Mohamed Rasvi</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:52 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[By the end of this tutorial, you will have:A trained ML model stored in Google Cloud Storage (GCS)A prediction API written in Python (Flask)A Docker image published to Artifact RegistryA live Cloud Run service serving real predictionsA full understanding of  everything works (no magic) will automate all of this using CI/CD with GitHub Actions.Most ML deployment tutorials either:Hide complexity behind managed platformsOr jump straight into automationWe’ll deploy everything  so you:Understand each moving partCan debug production issues confidentlyBuild real MLOps intuitionThe Right Way to Deploy ML ModelsBad (traditional) approachDocker Image = Code + Model (5GB)- Slow builds- Rebuild image for every model updateModern approach (recommended)Docker Image = Code onlyGCS Bucket = Model file- Easy model versioning- Update model without rebuilding image┌──────────────────────────────────────────────┐│                  Local Machine               ││  - Train model                               ││  - Build Docker image                        │└──────────────────────────────────────────────┘                  ↓┌──────────────────────────────────────────────┐│              Google Cloud Platform            ││                                              ││  GCS Bucket          Artifact Registry        ││  - model.pkl         - Docker image           ││                                              │└──────────────────────────────────────────────┘                  ↓┌──────────────────────────────────────────────┐│                 Cloud Run                    ││  - Starts container                          ││  - Downloads model from GCS                  ││  - Loads model into memory                   ││  - Serves predictions via HTTP               │└──────────────────────────────────────────────┘Google Cloud account (billing enabled)gcloud auth logingcloud config set project YOUR_PROJECT_IDgcloud services enable \  run.googleapis.com \  artifactregistry.googleapis.com \  storage.googleapis.com \  cloudbuild.googleapis.comStep 1: Project Structuremkdir ml-deployment-tutorialcd ml-deployment-tutorialml-deployment-tutorial/├── model/           # Trained model files│   ├── train.py     # Model training│   └── predict.py   # Prediction API├── tests/           # (Used in Part 2)└── requirements.txttests/ directory is intentionally empty in Part 1.We’ll add unit and integration tests in Part 2 when we introduce CI/CD.Step 2: Install Dependenciesscikit-learn==1.3.2pandas==2.1.4flask==3.0.0google-cloud-storage==2.10.0pip install -r requirements.txtStep 3: Train and Save the Model The code shown here is intentionally minimal and not fully tested. It is meant as a skeleton to demonstrate how the model is trained, loaded, and served, rather than as production-ready code. The goal of this section is to explain the flow and architecture, not to provide a complete implementation.import osimport picklefrom datetime import datetimefrom sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitprint("Training Iris Classifier")data = load_iris()X_train, X_test, y_train, y_test = train_test_split(    data.data, data.target, test_size=0.2, random_state=42model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)model.fit(X_train, y_train)accuracy = model.score(X_test, y_test)    "model": model,    "feature_names": data.feature_names,    "target_names": data.target_names.tolist(),    "trained_at": datetime.utcnow().isoformat(),    "test_accuracy": accuracy,os.makedirs("model", exist_ok=True)with open("model/model.pkl", "wb") as f:    pickle.dump(model_data, f)print(f"Model saved with accuracy: {accuracy:.2%}")Step 4: Upload Model to Google Cloud Storageexport BUCKET_NAME="your-project-ml-models"gsutil mb gs://$BUCKET_NAMEgsutil cp model/model.pkl \  gs://$BUCKET_NAME/models/iris-classifier/v1.0.0/model.pklWhy version models like this?models/└── iris-classifier/    ├── v1.1.0/Step 5: Prediction API (Flask)IMPORTANT: Gunicorn + Cloud Run Correct PatternCloud Run uses , not python app.py.Models must load at import time, not in __main__. The code shown here is intentionally minimal and not fully tested. It is meant as a skeleton to demonstrate how the model is trained, loaded, and served, rather than as production-ready code. The goal of this section is to explain the flow and architecture, not to provide a complete implementation.import osimport picklefrom datetime import datetimefrom flask import Flask, request, jsonifyfrom google.cloud import storageMODEL = Nonedef download_model(gcs_path, local_path="/tmp/model.pkl"):    gcs_path = gcs_path.replace("gs://", "")    bucket_name, blob_path = gcs_path.split("/", 1)    client = storage.Client()    bucket = client.bucket(bucket_name)    bucket.blob(blob_path).download_to_filename(local_path)    global MODEL, METADATA    gcs_path = os.environ["GCS_MODEL_PATH"]    local_path = download_model(gcs_path)    with open(local_path, "rb") as f:        data = pickle.load(f)    METADATA = data# Load model when container starts (CRITICAL)load_model()def health():    return jsonify({"status": "healthy"})@app.route("/predict", methods=["POST"])def predict():    payload = request.get_json()    instances = payload["instances"]    preds = MODEL.predict(instances)    probs = MODEL.predict_proba(instances)    classes = [METADATA["target_names"][p] for p in preds]    return jsonify(            "predictions": classes,            "probabilities": probs.tolist(),            "metadata": {                "model_version": "v1.0.0",                "prediction_time": datetime.utcnow().isoformat(),        }FROM python:3.10-slimWORKDIR /appRUN pip install --no-cache-dir -r requirements.txtENV PORT=8080EXPOSE 8080  --bind :$PORT \  --threads 8 \  src.predict:appStep 7: Build & Push Imageexport REGION=us-central1gcloud artifacts repositories create ml-models \  --repository-format=docker \gcloud auth configure-docker $REGION-docker.pkg.dev  $REGION-docker.pkg.dev/$PROJECT_ID/ml-models/iris-classifier:v1.0.0 .  $REGION-docker.pkg.dev/$PROJECT_ID/ml-models/iris-classifier:v1.0.0Step 8: Deploy to Cloud Rungcloud run deploy iris-classifier \  --image=$REGION-docker.pkg.dev/$PROJECT_ID/ml-models/iris-classifier:v1.0.0 \  --allow-unauthenticated \  --set-env-vars=GCS_MODEL_PATH=gs://$BUCKET_NAME/models/iris-classifier/v1.0.0/model.pkl \  --memory=2Gi \  --max-instances=10Step 9: Test the Deploymentcurl https://YOUR_SERVICE_URL/healthcurl -X POST https://YOUR_SERVICE_URL/predict \  -H "Content-Type: application/json" \  -d '{"instances": [[5.1,3.5,1.4,0.2]]}'How to deploy ML models properly on GCPWhy models should live outside Docker imagesHow Cloud Run actually starts containersHow Gunicorn changes application behaviorHow to design real MLOps-ready systemsBased on the feedback I got it from Google’s cloud run teamMove Env Vars to --set-env-vars This is best practice because:Change env vars without rebuilding imagesDifferent values per environment (dev/staging/prod)Better security (secrets separate from code)2. Immutable Revisions per Model Version This gives you:Easy rollback if new model has issuesTraffic splitting (90% old model, 10% new)A/B testing different modelsClear model version history3. Cloud Storage Volume Mounts This solves:No need to bake model into containerFaster deployments (don’t rebuild for model updates)Better for large ML models4. Platform Flag for Mac Users Without this:Mac M1/M2 users will build ARM64 imagesThey’ll get exec format error on Cloud Run (x86)Very frustrating for followersAutomatic Cloud Run deploymentsModel versioning via Git tags]]></content:encoded></item><item><title>Provisioning Kubernetes on AWS with Terraform: EKS vs kOps, Architecture and Cost Considerations</title><link>https://blog.devops.dev/provisioning-kubernetes-on-aws-with-terraform-eks-vs-kops-architecture-and-cost-considerations-d0bc46710342?source=rss----33f8b2d9a328---4</link><author>Sanskar Agrawalla</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:50 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[In this blog, we compare two popular ways of running Kubernetes on AWS — Amazon EKS (managed) and kOps (self-managed) — with both provisioned using Terraform. Choosing the right model matters because cluster cost, operational overhead, upgrade strategy, compliance, and automation capabilities all directly affect platform reliability and business outcomes.While AWS offers UI-based provisioning for EKS and kOps provides direct CLI commands, this blog intentionally uses Terraform. UI or ad-hoc CLI deployment has no versioning, auditability, or repeatability. Terraform, in contrast, enables Infrastructure-as-Code, automation, resource traceability, and controlled lifecycle management. It allows teams to define the entire cluster and supporting services in code, version them, review changes, and re-provision environments consistently across development, staging, and production.By the end of this blog, you will understand when to choose EKS vs kOps based on cost and operational needs, and why Terraform remains the preferred method for Kubernetes infrastructure provisioning on AWS.Choosing Between EKS and kOps: Which One Fits Your Use Case?When deciding between Amazon EKS and kOps for running Kubernetes on AWS, the choice largely depends on how much control, flexibility, and platform engineering maturity you need. EKS provides a managed control plane, easier onboarding, and reduced operational responsibility. However, it limits deep customization and often leads to higher overall cost due to managed control plane pricing, AWS-controlled versions/upgrades, and service add-ons that accumulate over time.kOps, on the other hand, delivers full control over cluster architecture, networking, node lifecycle, and upgrade flows. It is cloud-agnostic, production-proven, and integrates well into custom platform engineering environments. With kOps you decide how the control plane runs, how networking is configured, how high-availability is achieved, and how infrastructure evolves — down to the EC2 instances, operating systems, VPC, DNS, and Kubernetes versioning. This makes kOps superior for organizations that need customization, vendor neutrality, advanced networking/security policies, or cost optimization at scale. The only real drawback is that it expects stronger Kubernetes and operations knowledge, but that is an engineering investment — not a financial cost — and ultimately improves team capability and infrastructure maturity.Benefits of Choosing kOps:Full architectural and operational control (networking, DNS, OS, versions, HA, etc.)Lower ongoing costs since no managed control plane feesEasier customization for compliance, security, and topology requirementsCloud-agnostic and not locked into AWS-specific managed servicesBetter alignment with platform engineering teams who prefer flexibilityProduction-ready with mature HA and upgrade workflowsWorks exceptionally well with Terraform for true end-to-end IaCDrawbacks of Choosing kOps:Requires deeper Kubernetes and infrastructure knowledgeResponsibility for control plane operations (not a monetary cost, just expertise)More moving parts for teams unfamiliar with cluster internalsBenefits of Choosing EKS:Managed control plane reduces operational responsibilityFaster onboarding for less experienced teamsSimplified upgrades and patching at the control plane levelStrong AWS ecosystem integrations (IAM, VPC CNI, ALB ingress, etc.)Drawbacks of Choosing EKS:Higher total cost (managed control plane + add-on dependencies)Limited deep customization of Kubernetes internalsAWS-dependent upgrade schedules and compatibility constraintsVendor lock-in and reduced infrastructure portabilityBefore proceeding, the following prerequisites are recommended:AWS Account and IAM AccesskOps Installed (Only for kOps-based Cluster Walkthrough)#install AWS CLI# macOS# Ubuntu/Debiancurl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"unzip awscliv2.zipaws configure# Enter your AWS Access Key ID, Secret Access Key, and default region (us-west-2)# macos# Ubuntu/Debianwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpgecho "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.listsudo apt update && sudo apt install terraform# macos# Ubuntu/Debiancurl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"chmod +x kubectlsudo mv kubectl /usr/local/bin/kubectl version --client#install kops (changed the url based on arm & amd based)curl -LO https://github.com/kubernetes/kops/releases/download/v1.30.0/kops-darwin-arm64    chmod +x kops-darwin-arm64sudo mv kops-darwin-arm64  /usr/local/bin/kopsCreating an EKS Cluster Using TerraformIn this section, we will walk through provisioning a production-ready Amazon EKS cluster on AWS using Terraform. The goal is to eliminate manual provisioning steps, ensure repeatability, and maintain infrastructure through version-controlled IaC workflows.What This Terraform Setup CreatesThe Terraform configuration provisions the following resources in AWS:A dedicated  with public and private subnets across multiple Availability Zones and  for routing public and private workloads and networking associations required for EKS communication and cluster endpointsWorker Node Groups (EC2 or Managed Node Groups) depending on configuration for cluster operations, nodes, and service accounts for cluster communication and load balancersAssociated services required for a fully functional Kubernetes clusterThis ensures that once Terraform completes, you have a working Kubernetes environment where you can deploy workloads, ingress controllers, and add-on services like monitoring, logging, and autoscaling.Using the Terraform ModuleFor this demonstration, we use an existing Terraform module hosted in the following GitHub repository, which contains reusable and modular Terraform code specifically structured for EKS provisioning. It abstracts away low-level complexity while keeping the configuration customizable.Assuming Terraform and AWS CLI are configured locally.git clone https://github.com/sanskar153/EKS-clustercd EKS-clusterterraform planThe repository contains a README.md which provides reference documentation, variable descriptions, and optional configurations. It is recommended to review the README.md for environment-specific adjustments or module parameters.After running terraform plan, you will notice that a total of 53 resources are scheduled for creation. Review the plan output to confirm all resources, and then proceed with terraform applyOnce the apply command finishes & cluster is created, use the following command to connect to it. After running it, you will also notice that your kubeconfig file (located at ~/.kube/config) has been updated with the required authentication token.aws eks update-kubeconfig --region <region> --name <cluster-name>Once provisioning is complete, your EKS cluster is ready for workloads. Application services, monitoring, and observability systems can be deployed through kubectl. You can also deploy my observability stack, linked below for reference.Creating an Inhouse Kubernetes Cluster Using kOps + TerraformkOps is a popular tool for provisioning production-grade Kubernetes clusters, especially for teams that require deeper customization, full infrastructure ownership, and cloud portability. Traditionally, kOps allows you to create clusters directly using the kops create cluster command, which provisions infrastructure on the fly. While this approach works, it lacks strong versioning, auditability, and controlled lifecycle management.In this guide, we use kOps together with Terraform. Instead of directly applying changes through kOps, we generate Terraform manifests using the --target=terraform flag. This converts the cluster specification into Infrastructure-as-Code, enabling teams to:Maintain version control through GitTrack and review infrastructure changes before applying themIntegrate cluster provisioning into CI/CD pipelinesManage upgrades gracefully through declarative state filesEnforce consistent and repeatable deployments across environmentsA key detail to remember is that Kubernetes version compatibility is tied to the kOps version. If your target is Kubernetes v1.30, your kOps binary must also be v1.30 to ensure proper support and successful cluster provisioning.Prior to executing the kops create cluster command, an S3 bucket must be created to serve as the kOps state store. kOps relies on this state bucket to track configuration, updates, and cluster lifecycle. After creating the bucket, include it in the command via the --state flag.Define the cluster configuration:kops create cluster \  --name test.k8s.local \  --master-zones us-west-2b,us-west-2d \  --dns-zone cluster.local \  --zones us-west-2b,us-west-2d \  --topology private \  --master-size t3.large \  --node-size t3.large \  --ssh-public-key ~/.ssh/id_rsa.pub \  --network-cidr 172.20.0.0/16 \  --state s3://test-cluster.state \  --target terraform \  --yes Specifies the cluster name and DNS domain. For private DNS setups, .k8s.local is commonly used.--master-zones us-west-2b,us-west-2d Defines which Availability Zones will host the control plane (masters). Multiple AZs enable high availability for the control plane. Specifies the DNS zone used for resolving cluster internal DNS. In private topologies, .local DNS zones work without public records.--zones us-west-2b,us-west-2d Defines the Availability Zones for worker nodes. Matching control plane and worker zones improves latency and redundancy. Creates a private topology where nodes are launched in private subnets without public IPs. Traffic flows through NAT gateways or bastion hosts for outbound access. This is recommended for production security. Creates three master nodes for HA. With an odd number of nodes, etcd can maintain quorum during failures. Instance type for control plane nodes. t3.large offers a balanced CPU/RAM profile for moderate workloads. Creates two worker nodes. These run application workloads (pods). Instance type for worker nodes. Can be tuned based on workload requirements, CPU-heavy vs memory-heavy applications, etc. Defines worker node root volume size (in GB). Useful for container images, logs, and ephemeral data.--ssh-public-key ~/.ssh/id_rsa.pub SSH key to access worker and master nodes via SSH if needed. Placed for troubleshooting or system management. Selects the CNI plugin. Cilium provides advanced L3/L4/L7 observability, eBPF networking, and network security policies.--network-cidr 172.20.0.0/16 CIDR range for the VPC overlay. Determines pod and node IP ranges inside the cluster network.--state s3://test-cluster.state Specifies the S3 bucket where kOps stores cluster configuration and state. This acts as the source of truth for cluster management, upgrades, and operations. Instead of provisioning directly, this outputs Terraform manifests. Allows infrastructure to be version-controlled and applied via Terraform. Output directory for generated Terraform files (in this case, the current directory). Confirms execution without prompting for interactive approval.Once you execute the above kops create cluster command with the --target=terraform flag, kOps will generate two important artifacts in the working directory:A kubernetes.tf file that defines all the AWS infrastructure components required to run the cluster (VPC, subnets, route tables, EC2 instances, security groups, IAM roles, etc.).A data/ directory that contains Kubernetes manifest data and supporting templates generated by kOps for cluster bootstrapping.With these artifacts in place, you can now proceed using Terraform as the execution engine. This gives you full Infrastructure-as-Code capabilities rather than provisioning resources directly through kOps.At this stage, run the following commands:terraform initterraform planterraform plan will show all the AWS resources that are about to be created. After verifying the output and ensuring everything matches expectations, apply the changes by apply command.Terraform will then create the entire Kubernetes control plane and worker nodes along with the VPC infrastructure, networking, IAM roles, Route53 records (if applicable), and other required resources. This process effectively makes the cluster provisioning repeatable, traceable, and version-controlled through Git.Once the Terraform apply step completes successfully, your Kubernetes cluster will be deployed at the infrastructure level. The next step is retrieving the kubeconfig so you can interact with the cluster using kubectl. You can obtain the kubeconfig file using kOps:kops export kubeconfig test.k8s.local --admin --admin=17520h  After exporting, verify connectivity:If the nodes are in Ready state, your kOps-based Kubernetes cluster is up and running. You can now deploy applications, ingress controllers, monitoring stacks, and observability tooling using kubectl apply just like any other Kubernetes cluster.With this setup, you have successfully provisioned your own in-house, production-ready Kubernetes cluster, independent of any managed Kubernetes service. This approach can significantly reduce AWS costs because you are only billed for the underlying EC2 instances, storage, and load balancers, rather than a managed control plane. Once the cluster is operational, it becomes crucial to monitor nodes, capture logs, and observe application behavior. For that, you can refer to my complete in-house logging and observability stack guide linked below. See you there.For those interested in the complete architectural layout of the cluster from the AWS perspective, the  file contains comprehensive documentation. It explains which resources are provisioned, how networking is structured, and how the overall system is wired together, including diagrams and descriptions for better visualization.From here, you retain full control over future upgrades by modifying the kOps cluster spec, regenerating Terraform manifests, and re-applying them — giving you a clean and auditable lifecycle for Kubernetes operations.If you would like to see a follow-up article on performing zero-downtime Kubernetes version upgrades on a kOps-managed cluster, let me know. It is a topic I may cover in a future post. If you found this content valuable, please consider sharing it with your DevOps or SRE peers.]]></content:encoded></item><item><title>KodeKloud AWS Challenge — Day 28: Building and Pushing a Docker Image to Amazon ECR</title><link>https://blog.devops.dev/kodekloud-aws-challenge-day-28-building-and-pushing-a-docker-image-to-amazon-ecr-cd62885a999b?source=rss----33f8b2d9a328---4</link><author>Kishor Bhairat</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:48 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[KodeKloud AWS Challenge — Day 28: Building and Pushing a Docker Image to Amazon ECRA hands-on walkthrough of creating a private ECR repository and pushing a Docker image — establishing a clean container image pipeline in AWS.Day 28 of the KodeKloud AWS Challenge moved into container image management, a core requirement for any modern DevOps workflow.Running containers locally is easy. Running them consistently, securely, and at scale requires a proper image registry.The goal of this task was clear:Create a private Amazon ECR repositoryBuild a Docker image from an existing DockerfilePush the image to ECR with the latest tagThis is the foundation for ECS, EKS, CI/CD pipelines, and production-grade container deployments.Concept Explanation: Amazon ECR (Why It Exists)Amazon Elastic Container Registry (ECR) is a fully managed private Docker registry in Amazon Web Services.What ECR actually solves:Tight IAM-based access controlNative integration with ECS, EKS, and CI/CD toolsNo third-party registry dependencySource code → Docker image → ECR → Runtime (ECS/EKS)ECR is the  for container imagesIf you don’t control your image registry, you don’t control your deployments.Why This Matters in Real EnvironmentsWithout a proper registry:Images are built inconsistentlyTeams pull from local or public sourcesRollbacks become unreliableSecurity scanning becomes harderImages are versioned and auditableAccess is centrally controlledDeployments become repeatableThis task mirrors exactly how production container pipelines begin.Before pushing images to ECR, a few things must be in place:AWS CLI configured on the aws-client hostDocker installed and runningForgetting to authenticate Docker to ECRUsing the wrong repository URITagging the image incorrectlyPushing to Docker Hub by accidentECR doesn’t accept anonymous pushes — everything is authenticated.Hands-On Task: What I Did1️⃣ Created the Private ECR RepositoryCreated a private ECR repository named xfusion-ecrVerified repository URI and regionThis repository became the destination for the Docker image.2️⃣ Built the Docker Image LocallyNavigated to /root/pyapp on the aws-client hostUsed the existing DockerfileBuilt the Docker image locallyThis ensured the image matched ECR naming requirements.3️⃣ Authenticated Docker to ECRUsed AWS CLI to authenticate Docker with ECREstablished a temporary login sessionWithout this step, pushes are rejected.4️⃣ Pushed the Image to ECRPushed the Docker image to the xfusion-ecr repositoryVerified that the latest tag appeared in ECRAt this point, the image was centrally stored and ready for deployment.What I Learned / Key TakeawaysECR is the backbone of container deployments in AWSAuthentication is mandatory and time-boundImage tagging matters more than people thinkDocker build ≠ deploy until the image is pushedA clean image pipeline starts with a registryContainers without a registry are just local experiments.Creating an ECR repository and pushing a Docker image is a baseline DevOps capability, not an advanced one.This challenge reinforced a core principle:If you want reliable container deployments, you must control your images.With ECR in place, the Nautilus DevOps team is now ready to integrate containers into ECS, EKS, or CI/CD pipelines.On to the next challenge — where containers start running for real.If you’re learning AWS or DevOps through hands-on container workflows, follow the journey.This series documents real-world patterns used in production environments.]]></content:encoded></item><item><title>The Walled City of Kubernetes: How Ingress Works?</title><link>https://blog.devops.dev/the-walled-city-of-kubernetes-how-ingress-works-650275448ff8?source=rss----33f8b2d9a328---4</link><author>Muhammad Ateeb Aslam</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:44 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[𝗣𝗶𝗰𝘁𝘂𝗿𝗲 𝘁𝗵𝗶𝘀:Your Kubernetes cluster is a walled city. The only way in or out is through a very large gate (the 𝗟𝗼𝗮𝗱𝗕𝗮𝗹𝗮𝗻𝗰𝗲𝗿 service).Behind that gate is a massive register placed ( 𝘁𝗵𝗲 𝗜𝗻𝗴𝗿𝗲𝘀𝘀 𝗿𝗲𝘀𝗼𝘂𝗿𝗰𝗲). It’s like a city directory listing every visitor (𝗶𝗻𝗰𝗼𝗺𝗶𝗻𝗴 𝗿𝗲𝗾𝘂𝗲𝘀𝘁) and the exact building they should visit (𝘁𝗵𝗲 𝗿𝗶𝗴𝗵𝘁 𝘀𝗲𝗿𝘃𝗶𝗰𝗲 𝗼𝗿 𝗽𝗼𝗱).The city employs a Guide (𝘁𝗵𝗲 𝗶𝗻𝗴𝗿𝗲𝘀𝘀 𝗖𝗼𝗻𝘁𝗿𝗼𝗹𝗹𝗲𝗿) whose only job is to take a visitor to the specific building they belong to.Each time a visitor(𝗶𝗻𝗰𝗼𝗺𝗶𝗻𝗴 𝗿𝗲𝗾𝘂𝗲𝘀𝘁) arrives, the guide(𝗶𝗻𝗴𝗿𝗲𝘀𝘀 𝗰𝗼𝗻𝘁𝗿𝗼𝗹𝗹𝗲𝗿) checks the register (𝗶𝗻𝗴𝗿𝗲𝘀𝘀 𝗿𝗲𝘀𝗼𝘂𝗿𝗰𝗲), finds the right destination (𝗿𝘂𝗹𝗲𝘀), and personally escorts (𝗿𝗼𝘂𝘁𝗲) the guest (𝘁𝗿𝗮𝗳𝗳𝗶𝗰) to the correct building (𝘀𝗲𝗿𝘃𝗶𝗰𝗲/𝗽𝗼𝗱).Everything works flawlessly… From Metaphor to MechanicsLet’s step out of the city for a moment and see what this story really means in Kubernetes terms.When traffic enters your cluster from the outside world, it doesn’t go straight to the pods. Instead, it passes through a controlled sequence of components designed for scalability and security.City Walls:The cluster’s internal network which protects workloads from the outside worldCity Gate (Provides the public entry point (an external IP)Register Book (Defines routing rules (who goes where)City Guide (Enforces those rules, directing traffic to the right Service/PodThe Gate (LoadBalancer Service)When you expose a Kubernetes service using type LoadBalancer, your cloud provider automatically provisions an external load balancer. It’s like the city’s main gate, everything entering the cluster must pass through here first.Below is a YAML snippet of how you nornally creates a Loadbalancer service inside a kubernetes cluster:apiVersion: v1kind: Service  name: city-gate  type: LoadBalancer    app: city-entrance    - port: 80This LoadBalancer receives public traffic and forwards it to a target inside your cluster, often an Ingress Controller like  or .The Register (Ingress Resource)Spoiler: Ingress Resource itself doesn’t do anything. it’s just a set of rulesThe  resource defines  incoming requests are routed once they have passed the gate.Each rule in the Ingress file is like an entry in the city’s register, mapping a hostname or path to the service that should handle it.Below is a YAML snippet of how we normally define an Ingress resource:apiVersion: networking.k8s.io/v1kind: Ingress  name: city-register    nginx.ingress.kubernetes.io/rewrite-target: /  rules:    - host: bakery.city.local      http:          - path: /            backend:                name: bakery-service                  number: 80    - host: library.city.local      http:          - path: /            backend:                name: library-service                  number: 80Now when traffic for bakery.city.local arrives, the  consults this register and routes the request to the bakery service.The Guide (Ingress Controller)The  is the guide who makes it all happen.It continuously watches your Ingress resources for the rules and act according to those rules. It ensures the routing rules are applied to your cluster’s proxy (like NGINX or HAProxy). Ingress Controller also runs as pods inside cluster.You can deploy an nginx ingress controller via helm chart like this:helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm install city-guide ingress-nginx/ingress-nginxAs soon as the ingress Controller is defined, it starts watching for any Ingress resouce. And if it found the ingress resource, it update itself according to the rules defined in that ingress resource.Like any growing city, your Kubernetes environment needs structure, access control, and delegation.This design provides all three: for multiple servicesHost and path-based routing for efficient organization through multiple controllers and classes (coming in Part 02) for SSL, authentication, and rate limitingWithout this system, your services would be exposed individually , each with its own gate and rules , making it impossible to manage.What will happen when the city grows ? What if one directory (ingress resource) is unable to handle the rules of all the services that are there in our cluster ?That’s where Ingress Classes come in. We will learn them in next part. Stay Tuned.Thanks for reading. If this sparked even a tiny thought in you, imagine what the next one might do. So clap, follow, and subscribe, and let’s keep building ideas together.]]></content:encoded></item><item><title>Why 2026 Is the Year of “Shipping That Actually Works”: Production Tools That Reduce 3 AM Incidents</title><link>https://blog.devops.dev/why-2026-is-the-year-of-shipping-that-actually-works-production-tools-that-reduce-3-am-incidents-614e66751ed1?source=rss----33f8b2d9a328---4</link><author>JIN</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:40 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Git &amp; GitHub Made Simple: A Beginner’s Complete Command Reference</title><link>https://blog.devops.dev/the-ultimate-git-and-github-guide-from-initialization-to-collaboration-dcb43d64f68d?source=rss----33f8b2d9a328---4</link><author>Karthik S Patil</author><category>devops</category><pubDate>Tue, 27 Jan 2026 08:03:34 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Git is a distributed version control system used to track changes in files and manage project history.Record every update made to a projectRestore previous versions anytimeWork with teams without overwriting each other’s workMaintain different versions of a project through branchingStore code safely on remote platforms like GitHub Creates a new Git repository in the current folder. To start version control for a project.git init        #Initializationgit config --global user.name "Your Name" Updates the global Git username. To set or modify the username for all repositories.git config --global user.name "Karthik" git config --global user.email "you@example.com" Updates the global Git email. To set or modify the email for all repositories.git config --global user.email "karthik@example.com"git config user.name "Your Name" Sets the username only for the current repository (local configuration). To identify the author of commits inside this specific project.git config user.name "Karthik"git config user.email "you@example.com" Sets the email address only for the current repository (local configuration). To identify the commit author for this specific project.git config user.email "karthik@example.com" Stages a specific file for commit. To include that file in the next snapshot.git add . / git add -A / git add --all Stages all modified and new files. To prepare all changes for commit.git add . / git add -A / git add --all  Saves staged changes as a new commit. To record project progress with a message.git commit -m "Initial project setup"git remote add origin <URL> Connects the local repository with a remote repository. To push code to GitHub, GitLab, or Bitbucket.git remote add origin <remote url>  #ex:https://github.com/karthik/myrepo.git Sends local commits to the remote repository. To upload project changes to GitHub.git push origin main/master Fetches and merges latest changes from the remote repo. To update local code with team changes.git pull origin main/master Shows the username stored in Git configuration. To verify the configured Git username. Shows the email stored in Git configuration. To verify the configured Git email. Displays the current state of the working directory and staging area. To see which files are modified, staged, or untracked. Shows a list of all past commits. To review commit messages, authors, and timestamps.git log git log --oneline #to print commits in one line Lists all files tracked by Git in the repository. To view files Git is monitoring. Shows the remote repositories linked with your local repo. To verify where the repo will push/pull from.\ Deletes the connection to a remote repository. To disconnect the local repository from GitHub/GitLab/etc.What is Branching in Git?Branching in Git means creating a separate line of development where you can work independently without affecting the main project.It allows developers to experiment, add features, fix bugs, or test changes safely.Once the work is complete, the branch can be merged back into the main code. Displays all branches in the local repository. To view existing local branches.git branch    or Creates a new branch from the current commit. To start development on a new feature or task.Example: git branch feature-login Moves your working directory to the selected branch. To work on a different branch.Example:git checkout feature-apigit switch feature-apigit checkout -b branchname Creates a new branch and switches to it immediately. Saves time instead of using two separate commands.Example:git checkout -b feature-apigit push origin branchname Uploads the local branch to the remote repository. To share the branch with others or back it up.Example:git push origin feature-login Deletes the local branch safely (only if merged). To remove unused branches.Example:git branch -d feature-login Forcefully deletes a local branch even if unmerged. To remove unwanted branches without checks.Example:git branch -D test-branchgit push origin -d branchname Removes a branch from the remote repository. To clean up old or unnecessary branches on the server.Example:git push origin --delete feature-api Shows all branches in the local and remote repositories. To get a full view of all branches. Shows branches available on the remote repository. To check remote branch names before pulling or pushing.Merging in Git is the process of combining changes from one branch into another. Merges the specified branch into the current branch. To combine work from two branches.Example:git checkout mainHappens when the target branch has no new commits since the branch was created.Git simply moves the branch pointer forward.No merge commit is created.main → no changes → merge feature → pointer moves forward.2.Three-Way Merge (Recursive Merge)Used when both branches have new commits.Git compares three commits: common ancestor, head of current branch, head of merging branch.What is a Merge Conflict?A merge conflict occurs when Git cannot automatically combine changes from two branches because both branches have modified the same part of a file in different ways.How to Resolve a Merge Conflict Manually: Adds all modified (already tracked) files and commits them in one step. To quickly stage and commit changes without using git commit -am "Updated UI components and fixed errors"git commit -m "message" filename Commits only the specified file from the staging area. To commit a single file even if multiple files are currently staged.git add file1.html file2.css file3.js          ---> add filesgit commit file2.css -m "Updated CSS styles"   -->commit sinlge fileundoing changes means reverting files, commits, or staging areas back to a previous state.UNDOING CHANGES COMMANDS:Undo changes in a specific modified file (working directory)Command:git checkout <file_name>Undo changes in all modified files (working directory)Remove a specific staged fileCommand:git reset <file_name>Delete last commit but keep files stagedCommand:git reset --soft <previous commit id/hash>Delete last commit but keep files unstagedCommand:git reset --mixed <previous_commit_hash>Delete commit and files completelyCommand:git reset --hard <previous_commit_hash>Undo changes and keep commitsCommand:git revert <commit_hash>Delete a file from local Git and working directoryCommand:git rm <file_name>Delete a file from remote repositoryCommand:git rm --cached <file_name> Creates a local copy of a remote Git repository on your machine. Start working on a project locally.Command:git clone <repository_url>git clone https://github.com/user/project.git Downloads latest changes from a remote repository withoutmerging them into your local branch. See what others have updated without affecting your local code.Command:git fetch origin <branch name> git fetch <remote url> Moves or reapplies commits from your branch onto another branch’s tip. Maintain a linear project history and avoid unnecessary merge commits.Command:git rebase <branch_name> Applies a specific commit from one branch onto the current branch. Pick specific changes without merging the whole branch.Command:git cherry-pick <commit_hash> A file that specifies which files or folders Git should ignore (not track). Avoid committing unnecessary files like logs, build artifacts, or secrets. Temporarily saves your uncommitted changes without committing them. Switch branches or work on something else without losing current work.Commands:git stash                   #to stash modified filesgit stash -u                #to stash untracked filesgit stash list              #to list stashesgit stash apply             #to remove or undo the latest stashgit stash clear             #to clear stashgit stash apply <stash id>  #to undo or remove the specific stash Mark specific points in Git history as important (usually releases). Track versions or releases.Commands:git tag <tag name>             #to create taggit tag                        #to list tagsgit push origin <tag name>     #to push tag to remote repogit tag -d <tag name>          #to delete tag locallygit push origin -d <tag name>  #to delete tag remotely Create your own copy of someone else’s repository on GitHub/GitLab. Contribute to a project by modifying your fork and later creating a pull request.Example:git clone https://github.com/yourusername/project.git Updates the most recent commit (change message or include new files). Fix mistakes in the last commit without creating a new one.Command:git commit --amend Opens an interactive list of commits so you can edit, squash, or remove older commits. Clean up commit history or fix mistakes in earlier commits.Command:git rebase -i <commit_before_target>Continue Rebase (Continue From Where We Stopped) Continues the rebase process after resolving conflicts or editing commits. Resume rebase after fixing issues.Command:git rebase --continueList All Commits (Including Deleted Commits) Shows all HEAD movements — includes commits deleted by reset or overwritten by amend. Helps recover lost commits or view full history. Restores a commit that was deleted or lost. Recover accidentally removed commits using reflog.Command:git reset --hard <commit_hash>Real-World Team Collaboration Scenarios:Scenario 1: Feature Development Using Pull RequestsA developer creates a new branch to work on a feature. After completing the changes, the branch is pushed to GitHub and a pull request is opened. The team reviews the code, suggests improvements if needed, and once approved, the pull request is merged into the main branch. This ensures that only reviewed and tested code becomes part of the main codebase.Flow: git checkout -b feature-branch → git commit → git push → Pull Request → MergeScenario 2: Fixing a Bug in a Team EnvironmentA bug is reported in the production branch. A developer pulls the latest code, creates a hotfix branch, applies the fix, and pushes the changes to GitHub. A pull request is raised, reviewed, and merged back into the main branch to safely apply the fix without disturbing ongoing development.Flow: git pull → git checkout -b hotfix-branch → git commit → git push → Pull Request → MergeGit is not just a version control tool, but a system for tracking changes, managing history, and enabling smooth collaboration in software development.By learning essential commands, branching strategies, history management techniques, and collaboration workflows with GitHub, you build a strong foundation applicable to real-world development and DevOps environments. Regular practice of these concepts helps make Git an effective and reliable tool for both individual and team-based projects.If you found this helpful, please  to hit the  and buttons to help me write more articles like this.]]></content:encoded></item><item><title>Clawdbot with Docker Model Runner, a Private Personal AI Assistant</title><link>https://www.docker.com/blog/clawdbot-docker-model-runner-private-personal-ai/</link><author>Yiwen Xu</author><category>docker</category><category>devops</category><pubDate>Mon, 26 Jan 2026 20:51:41 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Personal AI assistants are transforming how we manage our daily lives—from handling emails and calendars to automating smart homes. However, as these assistants gain more access to our private data, concerns about privacy, data residency, and long-term costs are at an all-time high.By combining  with , you can build a high-performance, agentic personal assistant while keeping full control over your data, infrastructure, and spending.This post walks through how to configure Clawdbot to utilize Docker Model Runner, enabling a privacy-first approach to personal intelligence.What Are Clawdbot and Docker Model Runner? is a self-hosted AI assistant designed to live where you already are. Unlike browser-bound bots, Clawdbot integrates directly with messaging apps like Telegram, WhatsApp, Discord, and Signal. It acts as a proactive digital coworker capable of executing real-world actions across your devices and services.Docker Model Runner (DMR) is Docker’s native solution for running and managing large language models (LLMs) as OCI artifacts. It exposes an OpenAI-compatible API, allowing it to serve as the private “brain” for any tool that supports standard AI endpoints.Together, they create a unified assistant that can browse the web, manage your files, and respond to your messages without ever sending your sensitive data to a third-party cloud.Benefits of the Clawdbot + DMR StackIn a “Privacy-First” setup, your assistant’s memory, message history, and files stay on your hardware. Docker Model Runner isolates model inference, meaning: Your personal emails and schedules aren’t used to train future commercial models. Models run in isolated environments, protecting your host system. You decide exactly which “Skills” (web browsing, file access) the assistant can use.Cloud-based agents often become expensive when they use “long-term memory” or “proactive searching,” which consume massive amounts of tokens. With Docker Model Runner, inference runs on your own GPU/CPU. Once a model is pulled, there are . You can let Clawdbot summarize thousands of unread emails or research complex topics for hours without worrying about a surprise API bill at the end of the month.Configuring Clawdbot with Docker Model RunnerModifying the Clawdbot ConfigurationClawdbot uses a flexible configuration system to define which models and providers drive its reasoning. While the onboarding wizard (clawdbot onboard) is the standard setup path, you can manually point Clawdbot to your private Docker infrastructure.You can define your provider configuration in: ~/.config/clawdbot/config.jsonWorkspace-specific configuration: clawdbot.json in your active workspace root.Using Clawdbot with Docker Model RunnerTo bridge the two, update your configuration to point to the DMR server. Assuming Docker Model Runner is running at its default address: http://localhost:12434/v1.Your config.json should be updated as follows:{
  "models": {
    "providers": {
      "dmr": {
        "baseUrl": "http://localhost:12434/v1",
        "apiKey": "dmr-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "gpt-oss:128K",
            "name": "gpt-oss (128K context window)",
            "contextWindow": 128000,
            "maxTokens": 128000
          },
          {
            "id": "glm-4.7-flash:128K",
            "name": "glm-4.7-flash (128K context window)",
            "contextWindow": 128000,
            "maxTokens": 128000
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "dmr/gpt-oss:128K"
      }
    }
  }
}

This configuration tells Clawdbot to bypass external APIs and route all “thinking” to your private models.Note for Docker Desktop Users:Ensure TCP access is enabled so Clawdbot can communicate with the runner. Run the following command in your terminal:docker desktop enable model-runner –tcpRecommended Models for Personal AssistantsWhile coding models focus on logic, personal assistant models need a balance of instruction-following, tool-use capability, and long-term memory.Complex reasoning & schedulingdocker model pull gpt-ossFast coding assistance and debuggingdocker model pull glm-4.7-flashdocker model pull qwem3-coderPulling models from the ecosystemDMR can pull models directly from  and convert them into OCI artifacts automatically:docker model pull huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF
Context Length and “Soul”For a personal assistant, context length is critical. Clawdbot relies on a  file (which defines its personality) and a  (which stores your preferences).If a model’s default context is too small, it will “forget” your instructions mid-conversation. You can use DMR to repackage a model with a larger context window:docker model package --from llama3.3 --context-size 128000 llama-personal:128k
Once packaged, reference llama-personal:128k in your Clawdbot config to ensure your assistant always remembers the full history of your requests.Putting Clawdbot to Work: Running Scheduled Tasks With Clawdbot and DMR running, you can move beyond simple chat. Let’s set up a “Morning Briefing” task. docker model ls (Ensure your model is active). Run clawdbot init-soul to define how the assistant should talk to you.“Clawdbot, every morning at 8:00 AM, check my unread emails, summarize the top 3 priorities, and message me the summary on Telegram.”Because Clawdbot is connected to your private Docker Model Runner, it can parse those emails and reason about your schedule privately. No data leaves your machine; you simply receive a helpful notification on your phone via your chosen messaging app.The Clawdbot and Docker Model Runner ecosystems are growing rapidly. Here’s how you can help:]]></content:encoded></item><item><title>AWS Weekly Roundup: Amazon EC2 G7e instances, Amazon Corretto updates, and more (January 26, 2026)</title><link>https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-ec2-g7e-instances-with-nvidia-blackwell-gpus-january-26-2026/</link><author>Micah Walter</author><category>devops</category><pubDate>Mon, 26 Jan 2026 16:25:46 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Hey! It’s my first post for 2026, and I’m writing to you while watching our driveway getting dug out. I hope wherever you are you are safe and warm and your data is still flowing!This week brings exciting news for customers running GPU-intensive workloads, with the launch of our newest graphics and AI inference instances powered by NVIDIA’s latest Blackwell architecture. Along with several service enhancements and regional expansions, this week’s updates continue to expand the capabilities available to AWS customers.I thought these projects, blog posts, and news items were also interesting:Amazon EC2 G7e instances are now generally available — The new G7e instances accelerated by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs deliver up to 2.3 times better inference performance compared to G6e instances. With two times the GPU memory and support for up to 8 GPUs providing 768 GB of total GPU memory, these instances enable running medium-sized models of up to 70B parameters with FP8 precision on a single GPU. G7e instances are ideal for generative AI inference, spatial computing, and scientific computing workloads. Available now in US East (N. Virginia) and US East (Ohio).Amazon Corretto January 2026 Quarterly Updates — AWS released quarterly security and critical updates for Amazon Corretto Long-Term Supported (LTS) versions of OpenJDK. Corretto 25.0.2, 21.0.10, 17.0.18, 11.0.30, and 8u482 are now available, ensuring Java developers have access to the latest security patches and performance improvements.Amazon ECR now supports cross-repository layer sharing — Amazon Elastic Container Registry now enables you to share common image layers across repositories through blob mounting. This feature helps you achieve faster image pushes by reusing existing layers and reduce storage costs by storing common layers once and referencing them across repositories.Amazon CloudWatch Database Insights expands to four additional regions — CloudWatch Database Insights on-demand analysis is now available in Asia Pacific (New Zealand), Asia Pacific (Taipei), Asia Pacific (Thailand), and Mexico (Central). This feature uses machine learning to help identify performance bottlenecks and provides specific remediation advice.Amazon Connect adds conditional logic and real-time updates to Step-by-Step Guides — Amazon Connect Step-by-Step Guides now enables managers to build dynamic guided experiences that adapt based on user interactions. Managers can configure conditional user interfaces with dropdown menus that show or hide fields, change default values, or adjust required fields based on prior inputs. The feature also supports automatic data refresh from Connect resources, ensuring agents always work with current information.Keep a look out and be sure to sign up for these upcoming events:Best of AWS re:Invent (January 28-29, Virtual) — Join us for this free virtual event bringing you the most impactful announcements and top sessions from AWS re:Invent. AWS VP and Chief Evangelist Jeff Barr will share highlights during the opening session. Sessions run January 28 at 9:00 AM PT for AMER, and January 29 at 9:00 AM SGT for APJ and 9:00 AM CET for EMEA. Register to access curated technical learning, strategic insights from AWS leaders, and live Q&A with AWS experts.AWS Community Day Ahmedabad (February 28, 2026, Ahmedabad, India) — The 11th edition of this community-driven AWS conference brings together cloud professionals, developers, architects, and students for expert-led technical sessions, real-world use cases, tech expo booths with live demos, and networking opportunities. This free event includes breakfast, lunch, and exclusive swag.Join the AWS Builder Center to learn, build, and connect with builders in the AWS community. Browse for upcoming in-person and virtual developer-focused events in your area.That’s all for this week. Check back next Monday for another Weekly Roundup!]]></content:encoded></item><item><title>Codenotary’s Free SBOM Service Tackles the AI Software Supply Chain</title><link>https://devops.com/codenotarys-free-sbom-service-tackles-the-ai-software-supply-chain/</link><author>Steven J. Vaughan-Nichols</author><category>devops</category><pubDate>Mon, 26 Jan 2026 15:17:07 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My First Semester at Georgia Tech</title><link>https://blog.devops.dev/my-first-semester-at-georgia-tech-3be77e20a37c?source=rss----33f8b2d9a328---4</link><author>Raghavendra Raikar</author><category>devops</category><pubDate>Mon, 26 Jan 2026 15:02:19 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[This month, I finished my first semester of the OMCS program at Georgia Tech. I have focused my degree to have a machine learning specialization and my first two classes were Machine Learning for Trading (ML4T) and Software Architecture and Design CS 6310.My Machine Learning for Trading Class focused on building a simulated trading system incrementally through the use of machine learning. I was assigned 8 projects throughout the class that helped me building this trading system while covering topics such as: market simulation, portfolio optimization, technical indicators, supervised learning, and reinforcement learning. Taking this class showed me the realistic capabilities of machine learning in the field of trading and understanding its limits in financial markets.This project involved simulating an American Roulette Wheel to show us how the odds are set up for the House to always win. We are given the following betting strategy to implement:Start with 1 dollar and betting on Black onlyIf you win, keep betting 1 dollar, if you lose double your betDon’t stop playing until you win 80 dollars.With this strategy I ran two experiments. The first experiment ran this strategy with an unlimited bankroll which meant you can keep betting forever until you win 80 dollars. This experiment’s expected value and probability had made it seem like a great strategy. In the second experiment, a bankroll was added to make it more realistic so that the person cannot keep betting if they keep losing forever. This experiment’s expected value and probability had showed that in the real world this scenario was very risky and not profitable. Thus through the expected value I learned that this strategy can still lose money long term even if I “win most of the time” which is just what the casinos want you to focus on.Project 2: Optimize Something:In this project I was exposed to metrics to evaluate portfolios such as:Cumulative Return: total gain or loss of portfolio.Average Daily Return: mean of portfolio’s daily returns.Volatility(std of daily returns): measure of how much a portfolio’s daily returns fluctuate.Sharpe Ratio (return relative to risk): risk adjusted metric to measure how much return a portfolio gets per unit of volatility or risk.I got to use Scipy’s minimize function to minimize the negative Sharpe Ratio and return the portfolio statistics above to find out the optimal stock allocations or weights from a set of givens stocks to maximize returns.Project 3: Assess LearnersThis project focused on supervised machine learning. I implemented the following supervised learners from scratch:Decision Tree learner: makes decisions by splitting data based on feature values. Easy to understand but can overfit and become too specific to training data.Random Tree learner: works the same as a decision tree but adds randomness when choosing which feature values to split the data on to help prevent overfitting.Bag Learner: trains the same model on slightly different variations of data and averages the results to get a more general result.Insane Learner: Combines a bunch of bag learners together and takes their average to further reduce variance.This project helped me understand the concept of overfitting and how a model can perform amazing on training data but then do poorly on out-of-sample data because it failed to properly generalize and pick up on the patterns in the data. I tested to see if leaf size is related to overfitting using the RMSE metric. I also got to see how using ensemble methods like bagging can reduce overfitting.Project 4: Defeat LearnersThis project built on project 3 and I had to try to change up the datasets to be able to defeat one of my learners. Here I got to see the strengths and weaknesses of different supervised learning models. I created:A dataset where linear regression outperformed the decision treesA dataset where decision trees consistently outperformed linear regressionThrough this project I learned that there is no single best performing learned and that model performance relies on the data being used.Here I got to build a market simulator that keeps track of how much money the portfolio given makes over time. This simulation took into account:Portfolio value computationCommission and market impactI was given a list of stocks and their orders and had to calculate how much money is accumulated at the end as shown below:Input:# InputDate,Symbol,Order,Shares 2008-12-8,AAPL,SELL,130 # Output2008-12-3 1000000 2008-12-5 999754.30Project 6: Indicator EvaluationIn project 6 I was introduced to and implemented the following 5 technical indicators:Bollinger Bands: measures how for stock price deviates from its moving average using standard deviation.Stochastic Oscillator: Compares current stock price to its recent price range to indicate momentum and potential price reversals.MACD: Looks at difference between two moving averages to identify changes in trend direction and momentum.Simple Moving Average (SMA): Calculates average price over fixed window to identify changes in trend direction and momentum.Momentum: Measures rate of change in price over time to indicate strength and direction of trend.These technical indicators were then visualized with graphs and used to determine the overbought and oversold conditions (short or long signals) for each indicator. To have an upper bound I implemented a Theoretically Optimal Strategy that assumes knowledge of future prices and always takes the correct short or long position. This strategy was evaluated against a benchmark which was buying and holding shares of a stock for the entire trading period.Project 7: QLearning RobotIn this project I got to implement a Q-Learner from scratch and got a deeper understanding of reinforcement learning. This Q Learner learns through finding out the optimal action to take through trial and error with its environment and updating its Q table with the observed rewards and state transitions. I also learned and implemented a technique called Dyna-Q to improve learning speed by allowing the learner to simulate experiences to better accelerate convergence. This project showed me the importance between exploration and exploitation and how there needs to be a balance between the learner taking actions which it already knows the outcome/reward to versus taking new unexplored actions.Project 8: Strategy EvaluationFor this final project I got to implement what I learned in the previous projects to create a stock trading bot. I implemented the following 2 strategies:Manual Strategy — a manual trading strategy based on 3 indicators of my choosing with manual thresholds on when to buy and sell for each indicatorLearner Strategy — Here I used my Q learner to use my 3 chosen indicators to learn the best trading strategy.On in sample data my strategy learner was able to outperform the manual and benchmark but when tested with out of sample data, my strategy learner did poorly which is shows that it overfitted to the training data and was not able to generalize properly.Taking this Machine Learning for Trading class introduced be to the different supervised learning and reinforcement learning algorithms, how they work, and more importantly, when and how to use each one for real world applications such as trading.My second class: Software Architecture and Design, helped me think beyond writing code, about designing systems that can scale and withstand change. Here I analyzed why systems fail, where complexity comes from and how design decisions early on can prevent costly problems later.I was taught to slow down and ask these questions when thinking about implementing a system:What are the core responsibilities of this system?Which components interact with each other and which are isolated?How accepting to change will this system be as requirements evolve?This helped me think beyond files and functions and instead think in terms of modular components each with their own responsibilities and interactions.Through this course I was introduced to UML (Unified Modeling Language). This language provides a visual representation of how a software system is structured and how its parts interact. I got hands on experience with class and sequence diagrams when I had to design a pokemon battle simulation. The class diagrams helped me take verbal descriptions of a system and be able to identify and categorize these requirements into classes while the sequence diagrams helped me visualize all the possible flows of the system to get a thorough understanding.I learned about key system design concepts such as:Single Responsibility Principle: making sure each class only does one thing.Coupling: how much components depend on each other.Cohesion: degree to which components work together to fulfill a purposeand got to put these concepts to use through peer reviewing my fellow classmate’s submissions. This taught me how to spot inefficiencies in a system’s design such as spotting tight coupling, lack of modularity, and being able to explain why a system’s design is strong or weak.I got to learn about the following architectural styles as well:Layered Architectures: a system with layers where each layer has a clear responsibility and only depends on the layer below it.Object-Oriented Systems: software with interacting objects that combine data and behavior and promote encapsulation, modular design and code reuse.Pipe-and-Filter: process in which data is passed through a sequence of independent steps otherwise known as filters, where each stage transforms the data and passes it onto the next filter.Event-Driven Systems: Components communicating by emitting and responding to events at their own time allowing the system to be loosely coupled and react asynchronously.At the end of this class, I was left with these key takeaways:Design decisions matter most before code implementationA good system design makes change easier not harderExtracting requirements/diagraming out the system and having discussions regarding architecture are paramount to creating an efficient scalable system.Both these classes have taught me a lot and I can’t wait to get started on my next semester where I will be taking Machine Learning and Human Computer Interaction!]]></content:encoded></item><item><title>Docker Sandboxes: Run Claude Code and Other Coding Agents Unsupervised (but Safely)</title><link>https://www.docker.com/blog/docker-sandboxes-run-claude-code-and-other-coding-agents-unsupervised-but-safely/</link><author>Srini Sekaran</author><category>docker</category><category>devops</category><pubDate>Mon, 26 Jan 2026 15:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[We introduced Docker Sandboxes in experimental preview a few months ago. Today, we’re launching the next evolution with microVM isolation, available now for macOS and Windows. We started Docker Sandboxes to answer the question:How do I run Claude Code or Gemini CLI safely?Sandboxes provide disposable, isolated environments purpose-built for coding agents. Each agent runs in an isolated version of your development environment, so when it installs packages, modifies configurations, deletes files, or runs Docker containers, your host machine remains untouched.This isolation lets you run agents like Claude Code, Gemini CLI, Codex, and Kiro with autonomy. Since they can’t harm your computer, let them run free.Since our first preview, Docker Sandboxes have evolved. They’re now more secure, easier to use, and more powerful.Level 4 Coding Agent AutonomyClaude Code and other coding agents fundamentally change how developers write and maintain code. But a practical question remains: how do you let an agent run unattended (without constant permission prompts), while still protecting your machine and data? Most developers quickly run into the same set of problems trying to solve this:OS-level sandboxing interrupts workflows and isn’t consistent across platformsContainers seem like the obvious answer, until the agent needs to run Docker itselfFull VMs work, but are slow, manual, and hard to reuse across projectsWe started building Docker Sandboxes specifically to fill this gap.Docker Sandboxes: MicroVM-Based Isolation for Coding AgentsDefense-in-depth, isolation by defaultEach agent runs inside a dedicated microVMOnly your project workspace is mounted into the sandboxHypervisor-based isolation significantly reduces host riskA real development environmentAgents can install system packages, run services, and modify filesWorkflows run unattended, without constant permission approvalsSafe Docker access for coding agentsCoding agents can build and run Docker containers inside the MicroVMThey have no access to the host Docker daemonOne sandbox, many coding agentsUse the same sandbox experience with Claude Code, Gemini CLI, Codex, and KiroMore to come (and we’re taking requests!)If an agent goes off the rails, delete the sandbox and spin up a fresh one in secondsWhat’s New Since the Preview and What’s NextThe experimental preview validated the core idea: coding agents need an execution environment with clear isolation boundaries, not a stream of permission prompts. The early focus was developer experience, making it easy to spin up an environment that felt natural and productive for real workflows.As Matt Pocock put it, “Docker Sandboxes have the best DX of any local AI coding sandbox I’ve tried.”With this release, we’re making Sandboxes more powerful and secure with no compromise on developer experience.  Sandboxes now run on dedicated microVMs, adding a hard security boundary.Network isolation with allow and deny listsControl over coding agent network access.Secure Docker execution for agentsDocker Sandboxes are the only sandboxing solution we’re aware of that allows coding agents to build and run Docker containers while remaining isolated from the host system.We’re continuing to expand Docker Sandboxes based on developer feedback:Ability to expose ports to the host device and access host-exposed servicesSupport for additional coding agentsDocker Sandboxes were made for developers who want to run coding agents unattended, experiment freely, and recover instantly when something goes wrong. They extend the usability of containers’ isolation principles but with hard boundaries.If you’ve been holding back on using agents because of permission prompts, system risk, or Docker-in-Docker limitations, Docker Sandboxes are built to remove those constraints.We’re iterating quickly, and feedback from real-world usage will directly shape what comes next.]]></content:encoded></item><item><title>Run Claude Code Locally with Docker Model Runner</title><link>https://www.docker.com/blog/run-claude-code-locally-docker-model-runner/</link><author>Yiwen Xu</author><category>docker</category><category>devops</category><pubDate>Mon, 26 Jan 2026 13:11:59 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[This post walks through how to configure Claude Code to use Docker Model Runner, giving you full control over your data, infrastructure, and spend.Figure 1: Using local models like gpt-oss to power Claude Code is Anthropic’s command-line tool for agentic coding. It lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows through natural language commands.Docker Model Runner (DMR) allows you to run and manage large language models locally. It exposes an Anthropic-compatible API, making it straightforward to integrate with tools like Claude Code.curl -fsSL https://claude.ai/install.sh | bash
irm https://claude.ai/install.ps1 | iex

Using Claude Code with Docker Model RunnerClaude Code supports custom API endpoints through the ANTHROPIC_BASE_URL environment variable. Since Docker Model Runner exposes an Anthropic-compatible API, integrating the two is simple.Note for Docker Desktop users:If you are running Docker Model Runner via Docker Desktop, make sure TCP access is enabled:docker desktop enable model-runner --tcp

Docker Model Runner makes it easy to repackage any model with an increased context size:docker model pull gpt-oss
docker model package --from gpt-oss --context-size 32000 gpt-oss:32k
Once packaged, use it with Claude Code:ANTHROPIC_BASE_URL=http://localhost:12434 claude --model gpt-oss:32k
ANTHROPIC_BASE_URL=http://localhost:12434 claude --model gpt-oss "Describe this repo."
That’s it. Claude Code will now send all requests to your local Docker Model Runner instance.Run Claude Code locally with gpt-oss using Docker Model RunnerHere’s what it looks like in action:Figure 2: Claude Code powered by Docker Model Runner keeps everything local and in your  controlClaude Code reads your repository, reasons about its structure, and provides an accurate summary, all while keeping your code entirely on your local machine.Monitor the requests sent by Claude CodeWant to see exactly what Claude Code sends to Docker Model Runner? Use the docker model requests command:docker model requests --model gpt-oss:32k | jq .
Figure 3: Monitor requests sent by Claude Code to the LLMThis outputs the raw requests, which is useful for understanding how Claude Code communicates with the model and debugging any compatibility issues.For convenience, set the environment variable in your shell profile:# Add to ~/.bashrc, ~/.zshrc, or equivalent
export ANTHROPIC_BASE_URL=http://localhost:12434
claude --model gpt-oss:32k "Describe this repo."

The strength of Docker Model Runner lies in its community, and there’s always room to grow. To get involved: Create an issue or submit a pull request. We’re excited to see what ideas you have! Tell your friends and colleagues who might be interested in running AI models with Docker.We’re incredibly excited about this new chapter for Docker Model Runner, and we can’t wait to see what we can build together. Let’s get to work!]]></content:encoded></item><item><title>Five Great DevOps Job Opportunities</title><link>https://devops.com/five-great-devops-job-opportunities-173/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:26:50 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>KodeKloud AWS Challenge — Day 29: Enabling Communication Between VPCs Using VPC Peering</title><link>https://blog.devops.dev/kodekloud-aws-challenge-day-29-enabling-communication-between-vpcs-using-vpc-peering-33af36cdec80?source=rss----33f8b2d9a328---4</link><author>Kishor Bhairat</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:06:47 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[KodeKloud AWS Challenge — Day 29: Enabling Communication Between VPCs Using VPC PeeringA complete, step-by-step walkthrough of connecting a public VPC and a private VPC using VPC Peering — covering architecture, routing, security, verification, and real-world troubleshooting.Day 29 of the KodeKloud AWS Challenge focused on , one of the most critical AWS networking concepts used to enable private communication between isolated VPCs.In real AWS environments, everything does  live in a single VPC. Teams intentionally separate:Private backend workloadsEnvironment boundaries (dev, staging, prod)This challenge demonstrates how two VPCs — one public and one private — can communicate securely over private IPs, without using the internet, NAT gateways, or VPNs.Task Overview (What Was Required)The setup intentionally mimicked a real-world environment where most resources already exist.Public Side (Default VPC)EC2 instance: devops-public-ec2Subnet: devops-private-subnetEC2 instance: devops-private-ec2 (private)Create a VPC Peering connection devops-vpc-peeringConfigure route tables on both VPCsAllow private EC2 access from the public EC2Enable ICMP traffic for testingValidate connectivity end-to-endEnable only aws-client should able to connect on public instanceArchitecture Overview (How the Design Works)Concept Explanation: What Is VPC Peering? enables two VPCs to communicate using  as if they were on the same network.Traffic stays on the AWS backboneFully controlled via route tables and security groupsLow latency and high throughputIn Amazon Web Services, VPC Peering is commonly used for:Frontend-to-backend communicationGradual infrastructure migrationEnvironment isolation with controlled accessImplementation: Step-by-Step (How It Was Done)1️⃣ Created the VPC Peering ConnectionCreated a peering request between: Default (public) VPC devops-private-vpcNamed the connection Accepted the request from the private VPC sideAt this point, the VPCs were connected — but traffic could not flow yet.2️⃣ Configured Route Tables (Critical Step)VPC Peering requires explicit routing on both sides.Target: devops-vpc-peeringDestination: Default VPC CIDR (e.g. 172.31.0.0/16)Target: devops-vpc-peeringMissing even one route breaks connectivity.3️⃣ Updated Security GroupsRouting alone is insufficient.Updated the private EC2 security group to allow: from the public VPC CIDR (172.31.0.0/16)Verified outbound traffic was permittedUpdated the  to allow: from the private VPC CIDR (10.1.0.0/16)Verified outbound traffic was permittedWithout this, ping tests fail silently.Testing & Troubleshooting (What I Actually Did)4️⃣ Secure SSH Access to the Public EC2To test peering, I first ensured controlled access to the public EC2 instance.Retrieved aws-client public IP:SSH (port 22) from 0.0.0.0/0Connected to public instance using browser-based client.Restricted SSH to Functional first. Secure immediately after.5️⃣ Final Connectivity TestSSH’d into devops-public-ec2Pinged devops-private-ec2 using its Received successful ICMP responsesEnd-to-end private communicationTroubleshooting Checklist (Bookmark This)If VPC Peering doesn’t work, always check in this order:Route tables updated on Security groups allow trafficNetwork ACLs (if customized)Correct CIDR ranges are usedAWS networking never fails randomly — it fails .VPC Peering requires deliberate routingSecurity groups still apply post-peeringNo transitive routing existsTemporary access is acceptable — permanent exposure is notUnderstanding routing = understanding AWS networkingDay 29 demonstrated a real production-grade networking pattern: connecting public and private environments securely using VPC Peering.VPC Peering is simple in concept but unforgiving in execution. Every layer — peering, routing, security — must be correct.This is the difference between  and .If you’re learning AWS or DevOps through hands-on, architecture-first challenges, follow the journey. Each day builds skills that actually translate to production.]]></content:encoded></item><item><title>Keep Your Cluster Clean: Kubernetes Garbage Collection Explained</title><link>https://blog.devops.dev/keep-your-cluster-clean-kubernetes-garbage-collection-explained-05a939989eda?source=rss----33f8b2d9a328---4</link><author>Anish Kumar</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:46 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[In any environment, efficient resource management is very crucial. One of the key aspect in efficient resource management is removal of failed or unused objects that consume resources such as CPU, memory, or storage. This is where  comes into picture. Garbage collection refers to the automatic process of identifying and removing objects or resources that are no longer needed by the system. In programming languages, garbage collection generally deals with memory. However, in Kubernetes, it deals with resources such as pods, containers, and volumes.Recently, I ran into an interesting issue while working with Kubernetes. I had a  where the desired replication count was set to 1 This mean there should be only one pod running at any given time. Surprisingly, when I checked the cluster, I found : one in a running state and another in an error state. This is not an ideal state. As per the Deployment specification, there should be only one pod active.This anomaly sparked my curiosity about how Kubernetes handles resource cleanup and led me to explore garbage collection in Kubernetes. I wanted to understand why a pod in an error state wasn’t automatically removed and how Kubernetes decides which resources to clean up.What is Garbage Collection in Kubernetes?In Kubernetes,  is the process of automatically cleaning up resources that are no longer needed. This helps maintain cluster health and ensures that failed or unused objects don’t accumulate and consume resources over time. Kubernetes uses several mechanisms and policies to manage the lifecycle of resources and to perform cleanup of resources. Garbage collection helps in the prevention of issues such as orphaned pods, unused volumes, and other lingering resources.Some of the resources that are commonly cleaned up include:: Pods that are completed and they are in failed or completed.: ReplicaSets which are in used state due to Deployment updates.: Stopped containers associated with pods that have been deleted.: Unused PersistentVolumeClaims and their associated PersistentVolumes.: ConfigMaps, Secrets, and other objects that might be orphaned when their owner is deleted.Understanding how Kubernetes garbage collection works is key to maintaining a healthy, efficient, and predictable cluster. In the next sections, I will dive deeper into the actual mechanisms, policies, and best practices for Kubernetes garbage collection.Garbage Collection of Completed ResourcesKubernetes implements garbage collection to automatically clean up resources that have completed their lifecycle. This is particularly important for resources such as  and . These objects may terminate after completing their work, but could linger and consume cluster resources if not cleaned up by garbage collector.Garbage Collection for PodsThe Pod Garbage Collector (PodGC) that is part of the . It is responsible for removing pods that have reached a completed state,  or .The behaviour of PodGC is controlled by the --terminated-pod-gc-threshold flag in the kube-controller-manager. This threshold defines the maximum number of terminated pods that can exist in the cluster before the garbage collector starts cleaning them up. The default value is 12500.Garbage Collection for Jobs are a special type of Kubernetes resource designed to run tasks to completion. Unlike regular pods, jobs track the completion of one or more pods until all tasks are finished.The garbage collection of jobs works slightly differently. A job in  or  state whether in a  or  state can be automatically removed based on the spec.ttlSecondsAfterFinished attribute in the job definition. This attribute specifies how long the job information should be retained after completion and can be customised as per your requirements.apiVersion: batch/v1kind: Job  name: example-job  ttlSecondsAfterFinished: 3600     spec:      - name: example        command: ["echo", "Hello Kubernetes!"]In this example, the job will be automatically cleaned up after it finishes, whether the job succeeds or fails.Garbage Collection for Orphaned ResourcesKubernetes also handles cleanup of  (resources that no longer have a parent object). This is managed through following ways:: Defines parent-child relationships between resources.: Defines how dependent resources are removed when the parent is deleted.These mechanisms ensure that all associated child resources are automatically cleaned up, when a parent resource is removed. This prevents orphaned objects from lingering in the cluster and maintaining cluster hygiene.When a Kubernetes object creates dependent resources, it defines  in those child objects. This configuration allows the Kubernetes garbage collector to determine which resources can be safely deleted when the owner is removed. Let’s consider an example of Deploymentresource:When a Deployment is created, Kubernetes automatically creates a ReplicaSet to maintain the desired number of pods. The ReplicaSet has an  pointing to the Deployment.ownerReferences:  - apiVersion: apps/v1    controller: true    name: my-deployment    uid: afa2b2df-5ae7-41aa-8f17-7c2b3d65b473Each pod created by the ReplicaSet has an  pointing to its ReplicaSet as well.ownerReferences:  - apiVersion: apps/v1    controller: true    name: my-deployment-56467d566    uid: b5a5f94e-edbd-4deb-b080-9b9034152bf2The blockOwnerDeletion: true ensures that the Deployment cannot be deleted until the dependent ReplicaSet is cleaned up, and controller: true indicates that the parent is managing this child resource. When the Deployment object is deleted, the garbage collector uses these owner references to automatically remove the ReplicaSet and all the associated pods.Kubernetes also supports  that defines how and when child resources are deleted when their parent resource is removed. Similar to owner references, cascading deletion ensures that dependent objects do not remain orphaned in the cluster. There are two types:Foreground cascading deletion: The parent resource is marked for deletion first, but the deletion is  until all dependent resources are deleted. This ensures that no child resources remain after the parent is removed.kubectl delete deployment my-deployment --cascade=foregroundIn this scenario, the Deployment will only be fully deleted after its ReplicaSets and Pods are removed.Background cascading deletion: The parent resource is deleted immediately, and the garbage collector asynchronously deletes all dependent resources in the background.kubectl delete deployment my-deployment --cascade=backgroundIn this case, the Deployment disappears immediately, while its ReplicaSets and Pods are cleaned up by the garbage collector afterward.Garbage Collection with KubeletThe  is responsible for garbage collection on the worker node that manages both  and  to prevent disk pressure and resource exhaustion.The kubelet cleans up unused container images based on configurable parameters defined in the KubeletConfiguration:: Minimum age for unused images before they can be removed. Default value is 2m.: Maximum age for unused images before removal. Default value is 0s that means the attribute is disabled by default.imageGCHighThresholdPercent: Disk usage percentage that triggers garbage collection. Default value is 85%.imageGCLowThresholdPercent: Disk usage percentage below which garbage collection stops. Must be lower than the high threshold. Default value is 80%.When disk usage exceeds the high threshold, kubelet removes the least recently used images until usage drops below the low threshold.Example KubeletConfiguration snippet:apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationimageMaximumGCAge: "0s"imageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80 In Kubernetes v1.20+, older container-level flags such as --minimum-container-ttl-duration, --maximum-dead-containers, and --maximum-dead-containers-per-container are deprecated. Container cleanup is now handled automatically based on node pressure.Eviction-Based Garbage CollectionKubelet monitors node-level resource usage continuously. When resources such as memory, disk space, inodes, or PIDs drop below certain thresholds, kubelet triggers  to free up space and maintain node health:Kubelet first removes stopped or dead containers.Next, it deletes  if space is still low.Finally, if pressure persists, kubelet  based on their priority.These thresholds are controlled via the following configuration parameters:: Defines the eviction thresholds that immediately trigger garbage collection when breached.: Defines the eviction thresholds that trigger cleanup after a grace period if still under pressure.: Specifies how long kubelet waits after a soft threshold is crossed before performing GC.The aforementioned configuration parameters represent different types of node resource metrics tracked by the Kubelet that help it decide when to initiate eviction or garbage collection actions:: Triggers eviction if the amount of available memory on the node is below threshold (default value varies by setup, generally 100Mi-200Mi.: Initiates DiskPressure eviction when percentage of free disk space on the root filesystem is below the defined threshold (default ~10%).Percentage of free inodes on the root filesystem. Triggers eviction when inode usage is high (default ~5%).Percentage of free disk space on the container image filesystem. Triggers eviction when low (default ~15%).Percentage of free inodes on the image filesystem. Triggers eviction when inode usage is high (default ~5%).Percentage of free disk space on the container writable layer filesystem; triggers eviction if low (default ~10%).Percentage of free inodes on the container writable layer filesystem; triggers eviction when inode usage is high (default ~5%).Triggers PIDPressure eviction when the number of available PIDs on the node are below threshold (default ~1000).Example KubeletConfiguration for evictions:apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfiguration  memory.available: "200Mi"  imagefs.available: "15%"  containerfs.available: "10%"  pid.available: "1000"  memory.available: "400Mi"  imagefs.available: "20%"  containerfs.available: "15%"  pid.available: "2000"  memory.available: "30s"  imagefs.available: "2m"  containerfs.available: "1m"  pid.available: "30s"Garbage Collection with FinalizersFinalizers are a mechanism that allows resources to perform custom cleanup actions before they are fully deleted. When a resource with a finalizer is deleted, Kubernetes does not immediately remove it. Instead, the resource remains in a terminating state until all finalizers complete their tasks. Let’s look at the working of finalizers:Finalizers are strings added to the metadata.finalizers list of a resource.Kubernetes calls the associated cleanup logic for each finalizer, when the resource is deletedOnly after all finalizers are executed successfully does the resource get fully removed from the cluster.This ensures that important cleanup operations such as removing external dependencies or notifying other systems are completed before the resource is gone.apiVersion: v1kind: Pod  name: example-pod    - example.com/custom-cleanupIn this example, the pod will remain in the Terminating state until the logic associated with example.com/custom-cleanup is executed successfully.Best Practices for Kubernetes Garbage CollectionProperly managing garbage collection in Kubernetes is critical to ensure cluster stability, resource efficiency and predictable application behaviour. Following are key best practices to consider:Monitor Garbage Collection Metrics: Monitoring is the first step to effective garbage collection. Kubernetes exposes metrics for pods, jobs, containers, and images that allow you to track how frequently resources are being removed and whether thresholds are being breached. Tools such as  and  can help visualize these metrics, providing insights into pod terminations, container cleanup events, image deletions, and eviction triggers. Regular monitoring ensures that garbage collection is functioning as expected and helps you detect anomalies before they impact workloads.Adjust Thresholds and Resource Limits: Kubelet and cluster-level garbage collection rely on thresholds such as imageGCHighThresholdPercent, imageGCLowThresholdPercent, and eviction parameters like memory.available or nodefs.available. These defaults may not suit all workloads, especially in clusters with high pod churn or large images. Adjust thresholds according to your resource usage patterns to avoid premature cleanup or excessive accumulation of unused resources. Proper configuration prevents unexpected evictions and maintains node health.Regularly review the usage of CPU, memory, disk, and inodes across nodes. Excessive accumulation of terminated pods, unused images, or orphaned resources can degrade node performance and cluster stability. Use metrics-server or similar tools to keep track of resource consumption and proactively manage workloads to prevent resource pressure that would trigger garbage collection or evictions.Leverage PreStop Hooks and Finalizers: Custom cleanup logic can be handled with preStop hooks for containers and finalizers for Kubernetes objects. PreStop hooks allow you to gracefully terminate workloads that ensures tasks complete and connections close before the pod is removed. Finalizers provide a mechanism to perform cleanup of external resources or dependent objects before the parent resource is deleted. Using these mechanisms ensures controlled, predictable cleanup beyond what the default garbage collector handles automatically.Implement Image Lifecycle Management: Container images can occupy significant disk space. Regularly review and manage images using lifecycle policies to remove unused or outdated images. Combine this with Kubelet’s image garbage collection to ensure nodes do not run out of disk space. Tagging strategies, image pruning, and cleaning up dangling images help prevent unnecessary storage consumption and maintain cluster efficiency.Manage Completed and Orphaned Resources: Completed resources such as finished jobs or terminated pod should be configured with TTLs or rely on Kubelet cleanup policies to remove them automatically. Similarly, ensure that owner references and cascading deletion are correctly configured so that orphaned resources like ReplicaSets or Pods do not linger in the cluster. This keeps the resource hierarchy clean and prevents accumulation of unnecessary objects.Periodically audit your cluster for stale resources, unexpected terminations, or blocked finalizers. This helps identify and resolve issues with misconfigured garbage collection, orphaned resources, or resource leaks. Audits combined with monitoring help maintain a healthy and predictable environment for workloads.Garbage collection is a silent yet essential guardian of Kubernetes clusters, ensuring that resources are used efficiently, nodes remain healthy, and workloads run smoothly. Kubernetes provides multiple layers of automated cleanup such as cleaning up completed pods, jobs and automatically removing orphaned resources through owner references and cascading deletions. On the node level, Kubelet’s garbage collection, coupled with eviction based mechanisms, keeps disk, memory, and PID usage under control. Also, finalizers allow developers to implement controlled and custom cleanup logic for critical resources.By understanding how these mechanisms work and following best practices such as monitoring metrics, adjusting thresholds, managing images and leveraging hooks and finalizers. you can maintain a clean, efficient, and predictable Kubernetes environment. Proper garbage collection is not just about removing unused resources, it is about ensuring cluster stability, improving performance, and empowering teams to focus on delivering value rather than managing clutter.In essence, mastering Kubernetes garbage collection transforms resource management from a reactive chore into a proactive strategy for healthy, scalable clusters.If you find my work valuable and would like to support it, you are welcome to ]]></content:encoded></item><item><title>Ephemeral Storage in Kubernetes: The Power Behind Temporary Data</title><link>https://blog.devops.dev/ephemeral-storage-in-kubernetes-the-power-behind-temporary-data-55e97f641e77?source=rss----33f8b2d9a328---4</link><author>Anish Kumar</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:44 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[When working with Kubernetes, storage is one of the most crucial and often misunderstood components of application deployment. While Persistent Volumes (PVs) receive most of the attention for maintaining data durability, ephemeral storage plays an equally important role for workloads where data persistence is not required.Ephemeral storage refers to temporary data volumes that exist only for the duration of a Pod’s lifecycle. Once the Pod is deleted, the data disappears too. This makes ephemeral storage ideal for caching, logs, temporary files and other transient data. The following is a simple example of how to configure ephemeral storage in a Pod:Anish Kumar is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.apiVersion: v1kind: Pod  name: ephemeral-example  containers:    image: nginx    - name: cache-volume      mountPath: /usr/share/nginx/cache  volumes:    emptyDir: {}The emptyDir volume is created when the Pod starts and deleted when the Pod stops.Ephemeral storage enables faster startup times and reduces unnecessary disk I/O for non-critical data. Additionally, it helps workloads remain stateless and portable. Since Kubernetes uses the underlying worker node’s local storage or ephemeral volumes, it delivers high performance.Types of Ephemeral VolumesKubernetes provides multiple ways to use ephemeral storage that depends on the specific requirement of your application. The following is the comprehensive list of ephemeral volume types:The emptyDir type is the most common form of ephemeral volume in Kubernetes. It is automatically created when a Pod is assigned to a node and exists only for as long as the Pod runs on that node. Once the Pod is deleted, evicted, or rescheduled the data stored inside the emptyDir volume is permanently removed.volumes:- name: temp-storage    sizeLimit: 500MiYou can specify medium: Memory to store data in RAM instead of the node’s disk. This configuration provides extremely fast I/O performance and is ideal for workloads that require quick read and write access such as caching or buffering. Since it relies on RAM, it is limited by the available memory on the node.A configMap volume allows you to inject configuration data into Pods as files or environment variables. It is mainly used to decouple configuration details from container images that makes applications more flexible and easier to manage.volumes:- name: config-volume    name: app-configThis setup ensures that configuration data is available to the container while the Pod is running, and it is automatically removed when the Pod is terminated. However, it is important to note that large ConfigMaps can increase Pod startup time and memory usage, they should be used primarily for lightweight configuration data.The downwardAPI volume allows Pods to expose their own metadata and runtime information such as the Pod name, namespace, labels or annotations to containers as files or environment variables. It provides applications with visibility into their own configuration or environment without the need for external APIs.volumes:- name: podinfo    items:      fieldRef:        fieldPath: metadata.labelsThis approach is particularly useful for monitoring agents, logging tools, or applications that need to access Pod-specific metadata at runtime for identification or configuration purposes. However, it should be used carefully to avoid exposing sensitive metadata and it is best suited for ephemeral data.secret volumes are similar to ConfigMaps but are specifically designed to store and manage sensitive data such as credentials, API keys, tokens or certificates. Kubernetes ensures that this data is securely mounted into Pods as files that keeps it isolated from application code and container images.volumes:- name: secret-volume    secretName: db-credentialsThe secret data is automatically removed when the Pod is terminated. It is important to handle these volumes carefully to prevent accidental exposure through logs or debugging tools. Also, it is recommended to use Kubernetes RBAC to restrict who can view or modify secrets.5. image (container image layers)Every container image in Kubernetes uses ephemeral storage to store its unpacked layers when the Pod is created. These layers make up the container’s filesystem and are stored temporarily on the node’s local disk. When the Pod is deleted or rescheduled, the space used by these image layers is automatically reclaimed.apiVersion: v1kind: Pod  name: image-example  containers:    image: nginx:latest    imagePullPolicy: IfNotPresentThe way Kubernetes handles these image layers depends on the imagePullPolicy that defines when and how images are pulled from the container registry. Forces Kubernetes to pull the latest image every time the Pod starts. This ensures freshness but increases network usage and ephemeral storage consumption. Uses the locally cached image if it is already available on the node that reduces storage and network overhead. Uses only pre-pulled images on the node and skips pulling from the registry entirely.Container Storage Interface (CSI) Ephemeral Volumes provide a flexible way to dynamically attach short-lived storage directly to Pods using CSI drivers. These volumes are created and managed by the CSI driver on the node where the Pod runs and they are automatically deleted when the Pod is terminated.volumes:- name: csi-ephemeral    driver: example.csi.driver      size: "2Gi”This makes them ideal for temporary workloads that need specialized storage features, such as encryption, compression, or performance tuning, without requiring persistent volume claims. The CSI ephemeral volume has following limitations:These volumes  between Pods as they are tied to a single Pod’s lifecycle.The CSI driver must explicitly support ephemeral mode for this feature to work as not all drivers offer this capability.CSI ephemeral volumes are particularly useful for Pods that require temporary but customized storage provisioning. This offers the benefits of dynamic storage management while maintaining the transient nature of ephemeral data.7. Generic Ephemeral VolumesGeneric Ephemeral Volumes bridge the gap between basic ephemeral storage options such as emptyDir and more advanced persistent volumes. This allows developers to dynamically provision temporary storage using a PersistentVolumeClaim (PVC) template that is managed by Kubernetes. These volumes can take advantage of dynamic storage provisioning through existing StorageClasses that makes them more powerful and flexible for complex workloads that still need ephemeral behaviour.volumes:- name: gen-ephemeral    volumeClaimTemplate:        accessModes: ["ReadWriteOnce"]          requests:In this configuration, Kubernetes automatically creates a temporary PVC for the Pod using the provided template. The underlying storage is provisioned dynamically and it is automatically deleted when the Pod terminates.Ephemeral Storage WorkingEphemeral storage in Kubernetes is designed to provide short-lived, high-performance space that exists only during the lifecycle of a Pod. It operates at multiple levels including the container’s writable layer, Pod-level volumes, and dynamically provisioned temporary PVCs. The following components define how ephemeral storage functions inside a cluster.Each container in a Pod has its own  on top of the image layers. This writable layer is known as the . It is used to store temporary files generated by the application during runtime. This filesystem includes logs, caches, and transient data that are not written to mounted volumes. This data exists only while the container is running and is deleted once the Pod or container stops. Note that this layer consumes part of the node’s ephemeral storage and excessive writes to the root filesystem can lead to disk pressure.Kubernetes allows you to manage ephemeral storage usage through resource requests and limits. These settings define how much temporary storage a Pod can use and prevents a single Pod from consuming too much disk space on a node.resources:  requests:    ephemeral-storage: "500Mi"  limits:This configuration ensures that the Pod has a guaranteed minimum amount of ephemeral storage (requests) while enforcing an upper boundary (limits). When the node detects that a Pod exceeds its storage limit or the node itself is running low on disk space, Kubernetes may trigger  to maintain cluster stability.Ephemeral Volume LifecycleEphemeral volumes follow the Pod’s lifecycle from creation to termination as below:When a Pod is created, the Kubernetes scheduler assigns it to a node that satisfies its resource requirements and ephemeral storage requests.Volume preparation by Kubelet: The Kubelet on the target node prepares all volumes defined in the Pod specification. For ephemeral volumes such as emptyDir, the Kubelet creates a directory on the node’s filesystem. For CSI or generic ephemeral volumes, the Kubelet interacts with the storage driver to provision the volume dynamically.The containers within the Pod start and mount the prepared volumes. Pod can now write data to the Ephemeral volume.Pod execution and storage monitoring: The Kubelet continuously monitors ephemeral storage usage for each container and volume. If disk usage approaches node-defined thresholds then eviction policies gets triggered.Pod termination and cleanup: Once the Pod is deleted, the Kubelet unmounts and deletes the associated ephemeral volumes. For node-local volumes, the data is removed from the filesystem. For dynamically provisioned ephemeral volumes, Kubernetes automatically deletes the temporary PVCs and underlying storage objects.PersistentVolumeClaim and Naming ConventionEphemeral volumes that rely on PersistentVolumeClaim (PVC) templates such as Generic Ephemeral Volumes, automatically generate temporary PVCs at Pod creation. Kubernetes assigns each ephemeral PVC a unique name derived from the Pod and volume names that follows the below format:<pod-name>-<volume-name>-<unique-suffix>For example, a Pod named test-app using a volume temp-store might have a PVC named:test-app-temp-store-pvc-abc12This naming convention ensures uniqueness across Pods and prevents conflicts between replicas.Local Ephemeral Storage ConfigurationKubernetes provides several configuration options that determine how ephemeral storage is managed on each node. These configurations help ensure that temporary storage is used efficiently and that Pods do not interfere with system-level operations or each other.Cluster administrators can define  at the namespace level using a ResourceQuota object. This setting limits the total amount of ephemeral storage that all Pods in a namespace can consume. It helps maintain cluster stability by reducing the likelihood of node-level storage exhaustion.apiVersion: v1kind: ResourceQuota  name: ephemeral-storage-quotaspec:    requests.ephemeral-storage: "5Gi"    limits.ephemeral-storage: "10Gi"This configuration ensures that Pods in the app-namespace can collectively request up to 5 GiB and not exceed 10 GiB of ephemeral storage.Single vs Two FilesystemsKubernetes nodes can use one or two filesystems to manage local ephemeral storage. The difference lies in how storage for system processes and Pod data is isolated.The operating system, Kubelet and Pods share the same underlying disk. This setup can lead to competition between system and workload storage that make monitoring and troubleshooting more difficult.odes separate the  (used by the OS and system components) from the  (generally /var/lib/kubelet). This separation improves performance, allows for clearer monitoring of Pod-level storage usage and reduces the risk of system instability caused by Pods consuming excessive space.Use Cases of Ephemeral StorageEphemeral storage is an essential part of many Kubernetes workloads that require high-speed and temporary data handling. It supports a variety of short-lived processes and intermediate workloads that benefit from local and fast-access storage. Following are some common use-cases:CI/CD pipelines and build jobs: During application builds or automated testing, temporary files such as build artifacts, dependency caches, or test outputs can be stored in ephemeral volumes.Data preprocessing or temporary analytics tasks: Workloads that transform or analyze data often generate intermediate files that do not need to be stored permanently. Ephemeral storage allows these operations to run quickly using local disk or memory before sending the processed results to persistent storage.Caching or scratch space for short lived processes: Applications can use ephemeral volumes as scratch space for computations or as a cache layer to speed up operations without impacting persistent storage.Machine Learning Intermediate Data:Machine learning pipelines frequently produce temporary model checkpoints, transformed datasets, or preprocessed batches that are only required during training.Best Practice for Ephemeral StorageFollow below best practices while configuring ephemeral storage in Kubernetes cluster to make most out of it.Always requests and limits: Set requests and limits for ephemeral-storage in every Pod specification. This helps Kubernetes schedule Pods on nodes with sufficient space and prevents excessive disk consumption that could lead to eviction.emptyDir.medium: MemoryMemory-backed ephemeral storage offers fast I/O performance but consumes RAM from the node. Use it only when memory usage is predictable and when you truly need ultra-fast temporary storage, such as for caching or buffering.Continuously monitor node-level storage metrics: Track storage utilization using monitoring tools such as Prometheus, Grafana, or CloudWatch. Keeping an eye on ephemeral storage usage helps prevent sudden node pressure or unplanned Pod evictions.Keep ephemeral and persistent data in separate paths: Avoid mixing temporary and permanent data in the same directory or volume. This separation ensures cleaner storage management and reduces the risk of accidental data loss.Use CSI or Generic Ephemeral Volumes for dynamic provisioning: When workloads need flexible or feature-rich temporary storage, useCSI Ephemeral Volumes or Generic Ephemeral Volumes. They offer better control, dynamic provisioning and integration with storage drivers while maintaining ephemeral behaviour.Challenges of Ephemeral StorageWhile ephemeral storage provides flexibility and performance benefits, it also comes with certain limitations and management challenges that administrators and developers should be aware of. The following are some such challenges of Ephemeral Storage:Difficult to track usage patterns over time: Since ephemeral data is deleted when Pods are terminated, it can be challenging to analyze long-term storage usage trends or identify workloads that frequently consume excessive temporary storage.Risk of Pod eviction when storage exceeds limits: If a Pod’s ephemeral storage usage grows beyond its defined limit or if the node runs out of local disk space, the Kubelet may evict Pods to free up resources.Disk space constraints on nodes in multi-tenant clusters: In shared or multi-tenant environments, multiple workloads may compete for the same node storage, leading to contention and potential performance issues if ephemeral storage is not properly monitored and limited.Ephemeral storage is one of the key building blocks that enables Kubernetes to handle short-lived, stateless and high-performance workloads efficiently. By understanding how ephemeral volumes such as emptyDir, configMap, secret and CSI types work, you can make better decisions about where and how to store temporary data in your Pods. Proper configuration ensures that your applications benefit from fast access to storage without overwhelming cluster resources.When implemented correctly, ephemeral storage in Kubernetes provides the perfect balance between speed, flexibility and resource efficiency. It empowers developers to build scalable, cost-effective and reliable cloud-native applications that can handle dynamic workloads seamlessly while maintaining optimal performance and stability across the cluster.If you find my work valuable and would like to support it, you are welcome to ]]></content:encoded></item><item><title>A Deep Dive into Kubernetes Headless Service</title><link>https://blog.devops.dev/a-deep-dive-into-kubernetes-headless-service-98a45cca68e8?source=rss----33f8b2d9a328---4</link><author>Anish Kumar</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:42 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Kubernetes is the backbone of modern infrastructure, orchestrating containers and efficiently managing how applications are deployed, communicate and scale. Among its many components, Kubernetes Services play a crucial role in connecting workloads and exposing applications reliably. Most engineers are familiar with common Kubernetes service types such as ClusterIP, NodePort, and LoadBalancer. However, there is another less commonly used but powerful variant of Kubernetes service known as the Headless Service.Headless Service is particularly important for stateful applications, custom service discovery and direct pod-to-pod communication. This service offers more granular control over how traffic is routed within a cluster.This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber.In this article, we’ll take a deep technical dive into what a Headless Service is, how it works under the hood, explore the DNS resolution process, and understand how it differs from other service types, with practical examples and real-world use cases that demonstrate its true potential.What is headless service?In Kubernetes, a Service acts as an abstraction layer that provides a single, stable network endpoint to access a group of pods (for example: pods managed by Deployment). Kubernetes assigns each Service a ClusterIP that functions as a virtual load balancer inside the cluster. This ensures that any incoming traffic to the Service is evenly distributed across all available pods.However, in some cases, you might need more control on the traffic routing . For example, certain applications such as databases or distributed systems require direct communication between pods rather than going through a load balancer.This is where a Headless Service shines and becomes useful. You can create a Headless Service by setting the following in your Service configuration:With this configuration, Kubernetes does not assign ClusterIP. Instead, it updates the DNS records to return the IP addresses of the individual pods behind the Service. As a result, clients can resolve and connect to pods directly that enables fine-grained communication and discovery within the cluster.Working of Headless ServiceWhen you create a Service with clusterIP: None, the API server registers it as a Headless Service. - This Service will be associated with StatefulSet/Deployment using selector.apiVersion: v1kind: Service  name: my-headless-svc  clusterIP: None    app: my-app    - port: 80The kube-controller-manager will automatically create the EndpointSlice resource that map the service to the individual pod IPs matching its label selector. - If pods are added, removed or their readiness changes, the kube-controller-manager updates the EndpointSlice dynamically.apiVersion: discovery.k8s.io/v1kind: EndpointSlice  name: my-headless-svc-ghk85    kubernetes.io/service-name: myappports:    port: 80endpoints:      - 10.244.1.7      - 10.244.2.11      - 10.244.3.9Kube-proxy does not create any iptables or IPVS rules for a Headless Service because it does not have a ClusterIP. - This configuration allows traffic to bypass load balancing or proxying and flow directly from the client to the individual pods.CoreDNS detects that the Service is headless and returns multiple A records, one for each pod that matches the Service selector, instead of returning a single ClusterIP. This allows clients within the cluster to resolve the DNS name to all available pod IPs, enabling direct pod to pod communication without any load balancing by kube-proxy. - For example: nslookup to my-headless-svc.default.svc.cluster.local will return active pods IPs - 10.244.1.7, 10.244.2.11, 10.244.3.9Use-cases of Headless ServiceHeadless Services enable a variety of use-cases, following are some such use-cases:Headless Service is essential for StatefulSets, which are used to deploy stateful applications such as MongoDB, Cassandra, or Kafka. These applications require each pod to have a stable network identity to communicate reliably with other pods.- Kubernetes provides this stability using a combination of a Headless Service and a predictable DNS naming pattern. The Headless Service ensures that each pod gets its own DNS record instead of a single ClusterIP.- Example of MongoDB StatefulSet with Headless Service:apiVersion: apps/v1kind: StatefulSet  name: mongo  serviceName: "mongo"  selector:      app: mongo    metadata:        app: mongo      containers:          image: mongo:5            - containerPort: 27017            - name: mongo-data  volumeClaimTemplates:        name: mongo-data        accessModes: ["ReadWriteMany"]          requests:---kind: Service  name: mongo  clusterIP: None    - port: 27017    app: mongo- After deployment, each MongoDB pod of the StatefulSet gets a stable DNS name:mongo-0.mongo.default.svc.cluster.local  mongo-1.mongo.default.svc.cluster.local  mongo-2.mongo.default.svc.cluster.localHeadless Services give applications the ability to implement client-side load balancing, instead of relying on Kubernetes to distribute traffic automatically. Applications such as gRPC clients or service meshes can make intelligent decisions about which pod to connect to based on custom logic or metrics.- When a client queries the DNS of a Headless Service, it receives the IP addresses of all pods that match the service selector. This allows the client to choose exactly which pod to send traffic to that gives more control over routing and improving flexibility in distributed systems.With a Headless Service, applications can use DNS directly to discover backend pods instead of relying on kube-proxy or a ClusterIP. This provides the actual pod IPs that allows clients to communicate directly with each pod. - This approach is particularly useful for distributed databases, stateful applications, or microservices that have built-in service discovery logic.Limitations of Headless ServiceWhile Headless Services provide powerful capabilities for direct pod-to-pod communication and service discovery, they are not suitable for every scenario. Following are some important limitations of Headless Service:No automatic load balancing: Unlike ClusterIP or LoadBalancer Services, traffic is not distributed by Kubernetes. Users are responsible for implementing their own load balancing logic.: When pods are restarted or rescheduled, IP addresses of pods may change that can be problematic for long-lived connections.Best suited for stateful workloads: Headless Services offer limited benefits for stateless applications where a standard ClusterIP or LoadBalancer Service is sufficient.External access requires extra setup: Additional configuration is required to reach pods from outside the cluster, such as setting up an ingress, NodePort, or other networking solutions.Kubernetes Headless Services are a subtle but powerful feature that provides direct DNS-based access to pods that bypasses the default kube-proxy load balancing. They form a cornerstone for stateful and distributed systems that enables stable pod identities, precise service discovery, and fine-grained control over communication.Unlike standard Services, Headless Services trade abstraction for transparency that gives developers the ability to interact directly with pods. Whether you are building databases, message queues, or clustered applications, they offer the precision and control needed for advanced Kubernetes workloads.If you find my work valuable and would like to support it, you are welcome to sponsor me.]]></content:encoded></item><item><title>Understanding the Role of customresourcedefinitions/status in Kubernetes CRDs</title><link>https://blog.devops.dev/understanding-the-role-of-customresourcedefinitions-status-in-kubernetes-crds-4b06bee69652?source=rss----33f8b2d9a328---4</link><author>Anish Kumar</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:35 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Kubernetes has become the foundation of modern cloud infrastructure, giving teams a powerful way to orchestrate containers and manage workloads at scale. At its core, Kubernetes is an extensible system, and one of the key ways it achieves this flexibility is through Custom Resource Definitions (CRDs).CRDs allow developers and platform engineers to define their own resource types that behave just like built-in Kubernetes resources. This means you can create and manage custom objects such as policies, pipelines, or application-specific configurations directly within the Kubernetes API. The foundation of Operators, GitOps platforms, and internal automation systems.Thanks for reading! Subscribe for free to receive new posts and support my work.While managing CRDs, I stumbled upon a permission error involving customresourcedefinitions/status. It happened during a recent Flux upgrade from version 2.6.4 to 2.7.0 and turned out to be a great learning moment.User "AWS_SSO_Admin_Role" cannot update resource "customresourcedefinitions/status" in API group "apiextensions.k8s.io"This permission often appears in discussions around Kubernetes RBAC, and it tends to confuse even seasoned engineers. So let’s dive deep to better understand this permission.What is customresourcedefinitions/status?Every Kubernetes resource, whether it’s a Pod, Deployment, or CRD, has  and :The spec defines the desired state of the resource (what you want Kubernetes to make happen to the resource).The status reflects the actual state of the resource (what Kubernetes or controllers observe the state of the resource).For example, when you deploy a Pod, its spec describes what containers should run, and its status shows whether those containers are running, waiting, or failed.Similarly, CRDs also have a status subresource that holds information about their current condition, version, and readiness. The /status subresource exists as a dedicated API endpoint that allows updates specifically to the status field of a CRD, without touching its specification.The permission customresourcedefinitions/status refers to a user or service account’s ability to interact with this subresource. It essentially controls who can , , or  the status of CRDs within a cluster.Why does customresourcedefinitions/status permission exist?The reason for separating the  from the main resource  comes down to and.In a typical Kubernetes setup, the spec is usually managed by administrators or CI/CD pipelines. The status, however, is managed by controllers or operators that continuously reconcile the desired state with the actual state of the system.If both were handled together, it could open doors to unwanted or unsafe modifications. Imagine a controller trying to update a CRD’s observed state but accidentally modifying its schema or definition in the process. The /status subresource prevents that by giving Kubernetes a safe, scoped way to update just the status field.To prevent accidental or unauthorised changes, Kubernetes exposes these through separate API subresources:spec: /apis/apiextensions.k8s.io/v1/customresourcedefinitions/<crd-name>status: /apis/apiextensions.k8s.io/v1/customresourcedefinitions/<crd-name>/statusSo, when the CRD’s .status field is patched or updated, the operation targets the  rather than the main CRD object.Example: A typical CRD structureapiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinition  name: myresources.example.com  group: example.com    kind: MyResource  versions:      served: truestatus:    - type: Established    - type: NamesAcceptedIn the aforementioned example, the .status section reflects Kubernetes’ internal tracking of whether the CRD is established, served, and ready for use.Therefore, the customresourcedefinitions/status permission allows specific roles such as controllers, admission webhooks, or automated processes to report back on CRD readiness and operational conditions without the risk of altering the CRD specification itself.When is the customresourcedefinitions/status permission required?The customresourcedefinitions/status permission is often required in situations where a ,  or  interacts directly with CRDs. This permission enables updates to the CRD’s status during the management or reconciliation of resources.Following are some common scenarios where the customresourcedefinitions/status permission comes into play:Custom Controller Development: When a custom Kubernetes controller or operator is developed to create or manage CRDs, it often needs to update the status field to reflect the resource’s health, version, or reconciliation progress. Without the customresourcedefinitions/status permission, the controller cannot update CRD statuses and may encounter  errors, as discussed in the introduction.Automated Upgrades or Migrations: When upgrading tools or frameworks that define CRDs such as Flux, Istio or Argo CD, background processes frequently patch CRD statuses to maintain compatibility or indicate the completion of migrations. Lack of this permission can disrupt these operations and cause upgrade failures.Monitoring and Observability: Some monitoring solutions rely on status updates from CRDs to track whether APIs, controllers or custom workloads are healthy. Without this permission, status updates won’t propagate, leading to outdated or inaccurate monitoring data.Admission Controllers and Validation Webhooks: In more advanced setups, validating or mutating webhooks may also interact with CRD statuses as part of enforcing policies or performing validation checks. These processes need explicit access to /status endpoints.Granting the customresourcedefinitions/status PermissionAccess to the CRD status subresource can be granted to a user or service account by including the permission in a ClusterRole or Role definition as shown below:apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRole  name: crd-status  - apiGroups: ["apiextensions.k8s.io"]    resources: ["customresourcedefinitions/status"]    verbs: ["get", "update", "patch"]Next, associate the role with the relevant service account or user using a ClusterRoleBinding or RoleBinding:apiVersion: rbac.authorization.k8s.io/v1kind: RoleBinding  name: crd-status  apiGroup: rbac.authorization.k8s.io  name: crd-status  - kind: ServiceAccount    namespace: defaultThis configuration ensures that the  can modify CRD statuses within the  without granting broader permissions on the CRD definitions themselves.The customresourcedefinitions/status permission may seem like a minor detail, but it plays a crucial role in Kubernetes security and operational reliability. It ensures a clear separation between the declarative configuration defined in the spec and the observed runtime state recorded in the status.This separation allows Kubernetes to maintain consistency, prevent unauthorized changes to CRD specifications, and enable controllers to safely update system conditions. It also ensures that only trusted controllers can modify runtime states, keeping automation predictable and secure.When a  error appears for this permission, it is not a failure but rather Kubernetes enforcing the principle of least privilege. Ensuring that automation components and controllers have the appropriate status access keeps the cluster functioning smoothly, securely, and as intended.If you find my work valuable and would like to support it, you are welcome to sponsor me.]]></content:encoded></item><item><title>How Git Stores Files Internally to Saves Space in Your Repository</title><link>https://blog.devops.dev/how-git-stores-files-internally-to-saves-space-in-your-repository-61c5f9e25d6a?source=rss----33f8b2d9a328---4</link><author>bhagirath00</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:31 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Learn how Git stores files internally using snapshots, blobs, trees, and hashing to avoid duplication and save repository space efficiently.Git is the most widely used version control system in the world, and one of the key reasons for its popularity is its highly efficient storage model. At first glance, Git appears to store a complete copy of your project every time you commit. Surprisingly, repositories remain compact even after thousands of commits.So how does Git duplicate files while still saving disk space?In this article, we will explore how Git stores files internally, how it avoids unnecessary duplication, and why its storage mechanism is both . By the end, you will clearly understand how Git manages file data under the hood and why it scales so well for large projects.Overview: How Git Stores Data EfficientlyUnlike traditional version control systems such as Subversion (SVN), which store between versions, Git takes a fundamentally different approach.Git stores snapshots of the entire project state at every commit.However, Git is smart enough not to duplicate unchanged data. If a file has not changed between commits, Git simply reuses the previously stored version instead of saving a new copy. This design enables Git to deliver:Faster operations (branching, merging, checkout)Strong data integrity and reliability1. How Git Stores Data Using Snapshots Instead of File DifferencesMost version control systems track  over time. Git does not.Every time you create a commit, Git records a snapshot of the entire file structure at that moment.What Happens When Files Don’t Change?If a file remains unchanged between commits:Git does  store the file againGit simply creates a reference to the existing stored contentThis means Git behaves like a content-addressable filesystem, where identical content is stored once and referenced many times.This snapshot model allows Git to:Instantly switch between branchesAvoid recalculating diffs repeatedly2. Git Object Model: How Files Are Stored InternallyGit stores all repository data as  inside the .git/objects directory. Each object is identified by a  based on its content.There are four primary object types in Git: — Directory structure — A snapshot with metadata — Named references to commits2.1 Blob Objects: File Content StorageA blob (Binary Large Object) represents the .Key characteristics of blobs:Store file data only (no filename or permissions)Identical file contents result in Stored only once, regardless of how many commits reference themWhy Blobs Enable De-duplicationIf two files — or the same file across commits — have identical content:Multiple commits point to the same blobThis is the foundation of Git’s space-saving mechanism.You can inspect blobs using:git ls-tree <commit-hash>2.2 Tree Objects: Directory StructuresA  represents a directory in your project.References to blob objectsReferences to other tree objects (subdirectories)Each directory in your project maps to a tree object, allowing Git to recreate the complete filesystem structure for any commit.2.3 Commit Objects: Snapshots in TimeA  ties everything together.A reference to the root treeAuthor and committer informationCommit└── Tree (Root Directory)    ├── Blob (File 2)        ├── Blob (File 3)Each commit represents a , but most data is reused from earlier commits.3. Inside the .git Directory: Git’s Internal Storage and Control SystemThe .git directory is the core of every Git repository. It stores all metadata, objects, and references.This directory stores all Git objects (blobs, trees, commits) in compressed form. Objects are named using their hash values.References to branches and tags live here. Each branch is simply a pointer to a commit.3.3 .git/index (Staging Area)The index tracks what will be included in the next commit. It bridges the gap between your working directory and the repository.The HEAD file points to the currently checked-out branch or commit.4. How Git Uses Hashing, Compression, and De-duplication to Save SpaceGit’s efficiency comes from three core techniques.4.1 Content-Addressable HashingGit computes a hash (SHA-1 by default, SHA-256 supported) for every object based on its content.Different content → different hashThis guarantees data integrity and prevents duplication.Git compresses objects using , reducing disk usage while maintaining fast access.4.3 Automatic De-duplicationGit never stores the same content twice. If a file hasn’t changed:Existing blobs are reusedThis is how Git duplicates files logically without duplicating data physically.5. From Working Directory to Commits: How Git Builds and Stores SnapshotsTo fully understand how Git duplicates files while saving space, it is essential to understand the  through which every change flows: the , the , and the . These are not just conceptual layers — they directly influence how Git creates objects and reuses existing data.The  is the actual project folder on your local machine. It contains real files that you edit using your editor or IDE.Files here exist  of Git’s object databaseChanges are not tracked automaticallyGit does not store anything permanently at this stageWhen you modify a file in the working directory:No new blob is created yetNo disk space inside .git/objects is usedThis design allows Git to remain fast and lightweight while you experiment with changes.The , also called the , is where Git begins its internal storage optimization.Git performs the following actions:Reads the file content from the working directoryComputes a hash based on the contentChecks whether an identical blob already existsReuses the existing blob or creates a new one if neededRecords the blob reference in .git/indexThe staging area stores , not copiesUnchanged files reuse existing blob objectsPartial staging is supported, allowing fine-grained commitsThis is where Git’s  begins to take effect.Git creates a , which includes:A reference to a tree objectMetadata (author, timestamp, message)A reference to the parent commitGit does  duplicate file contentThe new tree references existing blobs whenever possibleOnly changed files produce new blobsEach commit represents a , but internally, most data is shared across commits. This allows Git to maintain a full project history without ballooning repository size.6. Exploring Git’s Internals Using Low-Level Git CommandsOne of Git’s strengths is transparency. Git provides low-level commands that allow you to inspect its internal object database, making it easier to understand how files are stored and reused.These commands are especially valuable for developers who want to understand Git beyond everyday workflows.6.1 git cat-file: Viewing Raw Git ObjectsThe git cat-file command allows you to inspect any Git object directly.git cat-file -p <object-hash>Author and committer detailsYou can also inspect blob objects to see file content exactly as Git stores it, confirming that identical content is reused across commits.6.2 git ls-tree: Exploring Tree StructuresThe git ls-tree command shows how a commit or tree maps to files and directories.git ls-tree <commit-hash>Object type (blob or tree)This command clearly demonstrates how Git builds directory snapshots using tree objects that reference blob objects, without duplicating data.6.3 git rev-parse: Resolving References to HashesThe git rev-parse command helps resolve symbolic references into their actual object hashes.Verifying which commit a branch points toDebugging detached HEAD statesUnderstanding reference resolutionThis reinforces the idea that branches and tags are lightweight pointers, not copies of data.Conclusion: Why Git’s Storage Model Is So PowerfulGit’s ability to duplicate files logically without duplicating data physically is the cornerstone of its performance and scalability. By storing content as immutable, hashed objects and reusing them across commits, Git ensures that repositories remain fast and space-efficient — even with extensive histories.Git stores , not file diffsIdentical file content is stored Blobs, trees, and commits form Git’s object modelThe .git directory contains all internal dataHashing and compression ensure integrity and efficiencyUnderstanding Git’s internal storage model gives you deeper confidence when working with branches, rebases, merges, and large repositories. It also explains why Git continues to outperform traditional version control systems in both speed and reliability.]]></content:encoded></item><item><title>End-to-End CI/CD Pipeline Using Jenkins and Kubernetes</title><link>https://blog.devops.dev/end-to-end-ci-cd-pipeline-using-jenkins-and-kubernetes-99be6542a07f?source=rss----33f8b2d9a328---4</link><author>bhagirath00</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:05:22 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Building Scalable, Cloud-Native CI/CD Pipelines with Jenkins and KubernetesIn modern , running on static or long-lived build agents often leads to scalability issues, inefficient resource usage, and maintenance overhead. As applications grow and deployment frequency increases,  must be dynamic, resilient, and  solves these challenges by providing on-demand, isolated, and auto-scalable environments for Jenkins workloads. By integrating Jenkins with Kubernetes, teams can dynamically provision build agents as pods, optimize resource utilization, and build highly scalableIn this blog, you’ll learn how Jenkins integrates with Kubernetes for CI/CD, understand the pipeline architecture, set up Jenkins on Kubernetes, and build a production-ready using containerized workloads and Kubernetes deployments.1. Why Integrate Jenkins with Kubernetes for CI/CD? provides a robust and scalable platform for running containerized applications, and Jenkins is a powerful tool for automating the. When integrated, these two tools can provide significant benefits:Dynamic Agent Provisioning:  dynamically creates  as build agents for each . Agents are provisioned only when needed and automatically destroyed after job completion, eliminating idle infrastructure.: scales  agents based on workload demand. Multiplecan run in parallel, allowing for faster builds and testing cycles.: Each Jenkins job runs inside its own Kubernetes ensuring clean, reproducible, and conflict-free build environments across pipelines.: Applications can be built, containerized, and deployed directly to Kubernetesenabling seamless end-to-end CI/CD workflows in cloud-native environments.: Because agents are short-lived and container-based, system resources are consumed only during active pipeline execution, significantly reducing infrastructure costs.2. Prerequisites for Jenkins and Kubernetes CI/CD IntegrationBefore integrating Jenkins with Kubernetes, ensure you have the following prerequisites in place. These prerequisites form the foundation for a stable and production-ready CI/CD setup.: A running Kubernetes cluster is required to host Jenkins agents and deploy applications. This can be a managed Kubernetes service such as Amazon EKS, Google GKE, Azure AKS, or a self-managed on-premise: Jenkins must be installed and accessible. It can run: — inside a Kubernetes cluster (recommended for cloud-native setups) or on a standalone virtual machine or server.Kubernetes Plugin for Jenkins: The Kubernetes enables Jenkins to dynamically provision Kubernetes pods as build agents. This plugin is essential for running CI/CD pipelines using Kubernetes-based agents.Cluster Access and Permissions: Jenkins must have permission to communicate with the server. This is typically achieved using a Kubernetes Service Account with the required RBAC roles.: The kubectl CLI tool is useful for: — managing Kubernetes resources — debugging deployments — running deployment steps inside 3. Jenkins Kubernetes Integration ArchitectureJenkins integrates with Kubernetes using the , which allows Jenkins to run CI/CD jobs inside Kubernetes pods instead of on static build agents.In this setup, Jenkins focuses on orchestrating the pipeline, while Kubernetes handles executing jobs and managing resources. Whenever a pipeline starts, Jenkins asks Kubernetes to spin up a temporary pod to run the job. Once the job finishes, the pod is automatically removed.This makes the entire CI/CD system dynamic, scalable, and cloud-native.How Jenkins and Kubernetes Work Together:: Jenkins controller manages pipelines, jobs, and credentials. It does not run builds directly. Instead, it coordinates with Kubernetes to run jobs on demand.: plugin connects Jenkins to the Kubernetes cluster and handles the creation and cleanup of agent pods whenever a pipeline is triggered: Each CI/CD job runs inside its own Kubernetes pod. These pods are: — created only when needed — isolated from each other — automatically destroyed after the job completes: A Jenkinsfile defining the CI/CD steps, including build, test, and deployment stages.: Kubernetes cluster provides the infrastructure where agent pods run and where applications are ultimately deployed.4. CI/CD Pipeline Architecture with Jenkins and KubernetesThis CI/CD architecture uses Jenkins as the pipeline orchestrator and Kubernetes as the execution and deployment platform. Instead of relying on static Jenkins agents, Kubernetes dynamically provisions build agents as pods, making the pipeline scalable and resource-efficient.The pipeline begins with a code change pushed to a Git repository (GitHub, GitLab, or Bitbucket).A webhook triggers Jenkins automatically on every commit or pull request, ensuring that no manual intervention is required.Stores application source code and DockerfileTriggers Jenkinsfilepipelines via webhooksActs as the single source of truth for buildsThe Jenkins controller manages the CI/CD pipeline logic defined in the Jenkinsfile.When a build is triggered, Jenkins does  execute jobs on itself. Instead, it requests Kubernetes to create an ephemeral agent pod.Orchestrates pipeline stages (build, test, deploy)Requests Kubernetes to provision agent podsTracks pipeline execution and logs4.3. Kubernetes Agent Pods (Dynamic Build Agents)Using the Jenkins Kubernetes Plugin, Jenkins dynamically spins up  inside the Kubernetes cluster. Each pipeline run gets its own isolated pod, which is destroyed after completion.No long-running or idle agentsClean environment for every buildParallel pipelines without conflictsAutomatic scaling based on workloadEach agent pod can include multiple containers (for example: Maven, Docker CLI, kubectl ), allowing different stages to run in the right environment.4.4. Docker Image Build & PushInside the Kubernetes agent pod, Jenkins builds the application and creates a Docker image using the project’s Dockerfile.The image is then pushed to a container registry such as Docker Hub, Amazon ECR, or GCR.Application is compiled and testedDocker image is built inside the agent podImage is tagged with version or commit hashImage is pushed to a container registryThis ensures the same image is used across all environments.4.5. Kubernetes DeploymentOnce the Docker image is available in the registry, Jenkins deploys the application to Kubernetes using kubectl or Helm.Jenkins applies Kubernetes manifests or Helm chartsKubernetes pulls the image from the registryPods are created or updated using rolling deploymentsApplication becomes available via Service or IngressThis completes the  from code commit to a running application in Kubernetes.5. How to Install and Run Jenkins on KubernetesGetting Jenkinsfileup and running on Kubernetes is easier than you might think, especially with , the package manager for Kubernetes. Helm simplifies complex deployments and ensures you can get a production-ready Jenkins instance quickly.5.1 Installing Jenkins with HelmThe easiest way to install Jenkins on Kubernetes is using Helm.Step 1: Create a Namespace for JenkinsIt’s a good practice to isolate Jenkins in its own namespace:kubectl create namespace jenkinsHelm is a package manager for Kubernetes that simplifies the installation of complex applications like Jenkins. To install Jenkins using Helm:helm repo add jenkins https://charts.jenkins.iohelm repo updatehelm install jenkins jenkins/jenkins --namespace jenkinsOnce installed, you can access Jenkins via the Kubernetes service. To get the admin password:kubectl get svc --namespace jenkinskubectl exec --namespace jenkins -it $(kubectl get pods --namespace jenkins -l "app.kubernetes.io/component=jenkins-master" -o jsonpath="{.items[0].metadata.name}") -- cat /run/secrets/chart-admin-passwordOpen Jenkins in your browser using the service IP and port, then log in using the retrieved admin password.5.2 Configuring the CloudOnce Jenkins is installed, configure it to use Kubernetes for dynamic agent provisioning:Install the Kubernetes Plugin: Go to  >  and install the . This plugin allows Jenkins to communicate with your cluster and provision agents on-demand.Configure Kubernetes Cloud:Navigate to  > .Scroll down to  and click  > .Provide the , , and configure the Kubernetes Service Account so Jenkins can manage pods.: Pod templates define what containers are included in each Jenkins agent pod. You can create different templates for different types of jobs, for example:6. Jenkinsfile-Based CI/CD Pipeline ImplementationWith Jenkins configured to use Kubernetes, the next step is to set up CI/CD pipelines that build and deploy applications to Kubernetes.A e allows you to describe your entire pipeline — build, test, and deployment as code, making it version-controlled, repeatable, and easy to maintain.6.1 Configuring Jenkins Pipeline for KubernetesA defines what steps your pipeline runs and .When using Kubernetes integration, Jenkins dynamically creates a  for each pipeline execution.Here’s an example of a  that uses Kubernetes agents and deploys an application to a Kubernetes cluster:pipeline {    agent {            label 'my-k8s-agent'            yaml '''            kind: Pod              containers:                image: maven:3.9.6-eclipse-temurin-17                - cat              - name: kubectl                image: bitnami/kubectl:latest                command:                tty: true        }    stages {            steps {                    sh 'mvn clean install'            }        stage('Test') {                container('maven') {                }        }        stage('Deploy to Kubernetes') {            steps {                    sh 'kubectl apply -f deployment.yaml'            }    }Jenkins creates a  for this pipeline runThe pod includes multiple containers (Maven for build/test, for deployment)Each stage runs in the most appropriate containerAfter the pipeline finishes, the pod is automatically destroyedThis approach keeps builds clean, isolated, and scalable.6.2 Automating Deployments to KubernetesIn the pipeline above, the  stage uses kubectl to apply Kubernetes manifests.These YAML files typically define resources such as:Because deployment happens only after successful build and test stages, Jenkins ensures that  reach your Kubernetes cluster.This automation removes manual deployment steps and enables fast, consistent releases.6.3 Deploying Applications with HelmWhile kubectl apply works well, managing multiple YAML files can become difficult as applications grow.This is where  becomes extremely useful.Package Kubernetes resources into reusable chartsEasily upgrade or roll back releasesHere’s a simpleexample that deploys an application using Helm:pipeline {    agent any        stage('Build') {                sh 'mvn clean install'        }        stage('Deploy to Kubernetes with Helm') {            steps {                sh 'helm upgrade --install myapp ./helm-chart/'            }    }Application configuration becomes cleanerEnvironment-specific values are easier to manageProduction deployments are more predictable7. Best Practices for Jenkins Kubernetes CI/CD PipelinesTo get the most out of Jenkins and Kubernetes, it’s important to follow a few proven best practices. These help keep your pipelines scalable, secure, and easy to maintain as workloads grow.: Define reusable pod templates for different job types to avoid duplication.Run Each Job in an Isolated Pod: Each Jenkins job should run in an isolated pod to ensure that builds are clean and independent.: Enable auto-scaling in Kubernetes to dynamically adjust the number of nodes based on Jenkins job demand.: Use Kubernetes secrets to securely manage credentials and sensitive information.: Package your application as a Helm chart to simplify deployment and versioning.8. Monitoring and Scaling Jenkins CI/CD Pipelines on KubernetesAs CI/CD pipelines grow in complexity and usage, monitoring and scaling become critical to maintaining performance and reliability. Kubernetes makes this much easier by providing built-in scalability and strong observability integrations.: The Jenkins dashboard gives a quick, high-level view of pipeline executions, build history, and agent activity. It’s useful for tracking failed jobs, build durations, and overall pipeline health.For deeper visibility, Jenkins can be integrated with Prometheus and Grafana. This allows teams to monitor: — Resource usage of Jenkins controllers and agents — Build and job execution metrics — Pod and node performance inside the Kubernetes cluster — Grafana dashboards make it easy to visualize trends, detect bottlenecks, and proactively address performance issues before they impact deployments.Scaling Jenkins with KubernetesKubernetes enables Jenkins to scale automatically based on workload demand. Jenkins agents can be created or destroyed as pods, allowing the CI/CD system to handle sudden spikes in build traffic without manual intervention.By combining Kubernetes auto-scaling with proper monitoring, teams can ensure that:Builds remain fast during peak usageInfrastructure costs stay optimized remain reliable and resilientIntegratingcreates a modern, cloud-native CI/CD platform that is scalable, efficient, and production-ready. By running Jenkins agents as Kubernetes pods, teams can dynamically provision build environments, optimize resource usage, and eliminate the limitations of static build agents.Kubernetes features such as pod isolation, auto-scaling, and Helm-based deployments allow Jenkins pipelines to remain clean, reliable, and easy to manage as applications grow. This integration enables seamless automation — from code commits and builds to testing and deployment directly into Kubernetes clusters.By combining , you can build CI/CD pipelines that are faster, more resilient, and ready for real-world production workloads — making continuous delivery a natural part of your ]]></content:encoded></item><item><title>Your Paper is a Blockbuster. Your Abstract is a Bad Trailer.</title><link>https://blog.devops.dev/your-paper-is-a-blockbuster-your-abstract-is-a-bad-trailer-1a0fddc0e8d6?source=rss----33f8b2d9a328---4</link><author>Hui Zhu</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:04:33 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[You spent two years in the lab. You battled confusing datasets, placated a demanding supervisor, and survived Reviewer #2. You finally have a breakthrough finding that could shift your field.But when you upload the PDF, you slap on an abstract written in a caffeine-deprived haze 20 minutes before the deadline.In the attention economy of academia, your abstract is your movie trailer. If it’s boring, confusing, or vague, nobody buys a ticket to read the full paper. It doesn’t matter if you found the cure for the common cold on page 14; if the abstract describes the weather, your citation count stays at zero.Most researchers use AI to “summarize this paper.” The result is usually a generic “word salad” — an unstructured blob of text that says everything and nothing.You don’t need a summary. You need a .I built the Abstract Writing AI Prompt to stop the “scroll-past” effect. It forces your LLM (ChatGPT, Claude, or Gemini) to ignore the fluff and drill down into the  core: Introduction, Methods, Results, and Discussion.The “Gatekeeper” ProtocolJournal editors are gatekeepers. They are looking for reasons  to send your paper out for review. A weak abstract is the easiest excuse.This prompt doesn’t just shorten your text. It acts as a seasoned  who demands specific answers to four questions: (Not “Background is important,” but “What is broken?”) (Specific methodology, not “We studied…”) (Numbers, percentages, p-values. Not “Significant results were found.”) (Why should the reader care ?)Copy the code block below into your AI workspace. It includes a ruthless “Quality Check Checklist” that prevents the AI from hallucinating results or using vague academic jargon.# Role DefinitionYou are a seasoned Academic Writing Specialist with 15+ years of experience in scholarly publishing. You have served as a journal editor for top-tier publications, reviewed thousands of paper submissions, and coached researchers from diverse disciplines on effective scientific communication.Your core expertise includes:- Structuring abstracts for maximum impact and clarity- Tailoring writing style to specific journal requirements- Distilling complex research into accessible summaries- Ensuring compliance with academic writing conventions# Task DescriptionCreate a polished, publication-ready abstract that effectively communicates the essence of a research paper. The abstract should capture the reader's attention, clearly convey the study's significance, and meet professional publication standards.Please write an abstract for the following research:**Input Information**:- **Research Topic/Title**: [Your paper title]- **Research Field/Discipline**: [e.g., Computer Science, Biology, Psychology]- **Key Findings/Results**: [Main discoveries or outcomes]- **Methodology**: [Brief description of research approach]- **Target Journal/Conference**: [Publication venue, if known]- **Word Limit**: [Typically 150-300 words]- **Abstract Type**: [Structured or Unstructured]## 1. Content Structure (IMRaD Framework)The abstract should follow this logical flow:- **Background/Introduction** (1-2 sentences): Context and research gap- **Objective/Purpose** (1 sentence): Clear statement of research aim- **Methods** (1-2 sentences): Key methodology and approach- **Results** (2-3 sentences): Major findings with key data points- **Conclusion** (1-2 sentences): Significance and implications## 2. Quality Standards- **Clarity**: Every sentence should convey a single, clear idea- **Precision**: Use specific data and avoid vague generalizations- **Conciseness**: Eliminate redundancy; every word must earn its place- **Standalone**: Abstract must be fully understandable without reading the paper- **Keywords Integration**: Naturally incorporate 3-5 relevant keywords## 3. Format Requirements- Word count: Strictly adhere to specified limit- Single paragraph (unstructured) or labeled sections (structured)- Third person, past tense for methods and results- Present tense for established facts and conclusions- No citations, abbreviations (unless standard), or references to figures/tables## 4. Style Constraints- **Language Style**: Academic, formal, and objective- **Expression**: Active voice where possible for clarity- **Expertise Level**: Accessible to informed non-specialists in the field# Quality Check ChecklistBefore finalizing the abstract, verify:- [ ] Clearly states the research problem and its significance- [ ] Objective is specific and measurable- [ ] Methodology is briefly but adequately described- [ ] Key findings are quantified where applicable- [ ] Conclusions directly relate to the stated objectives- [ ] Word count is within the specified limit- [ ] No jargon or undefined acronyms- [ ] No grammatical or spelling errors- [ ] Keywords are naturally integrated- [ ] Follows target journal's specific guidelines# Important Notes- Never include information not present in the main paper- Avoid making claims that cannot be supported by the data- Do not use phrases like "This paper discusses..." – dive directly into content- Ensure the abstract accurately represents the paper's scope# Output FormatProvide the abstract in the following format:1. **Draft Abstract** (complete text)2. **Word Count** (exact number)3. **Suggested Keywords** (5 relevant terms)4. **Improvement Notes** (brief suggestions for enhancement)Why This Beats “Please Summarize”You might ask, “Why not just paste my paper and ask for a summary?”Because LLMs are lazy. Without strict constraints, they default to “The Table of Contents” summary:“First we introduce the topic. Then we discuss methods. Finally we show results.”This is useless. It tells the reader  you wrote, not .The  prompt forces the AI to hunt for the “Golden Nugget” — the specific data point or insight that makes your paper worth reading. It strips away the hedging (“we attempted to investigate”) and replaces it with action (“we demonstrated”).Once the AI generates your draft, do this simple test: .Read the first sentence of your abstract. Does it sound like a headline or a history lesson?: “Alzheimer’s disease is a neurodegenerative condition affecting millions worldwide…” (We know. Skip.): “We identify a novel protein marker, XYZ-1, that predicts Alzheimer’s onset 5 years earlier than current standard tests.” (Tell me more.)This prompt is designed to get you to the “Strong” version immediately.Your research deserves an audience. Don’t hide it behind a bad trailer.]]></content:encoded></item><item><title>The Art of the “Polite Pushback”: Turning Reviewer #2 into Your Biggest Advocate</title><link>https://blog.devops.dev/the-art-of-the-polite-pushback-turning-reviewer-2-into-your-biggest-advocate-310d5ee393ef?source=rss----33f8b2d9a328---4</link><author>Hui Zhu</author><category>devops</category><pubDate>Mon, 26 Jan 2026 08:04:26 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Academic publishing is not a meritocracy. It is a negotiation.You can have the most robust data in your field, but if you cannot navigate the ego-filled minefield of peer review, your manuscript will die in the “Major Revisions” folder. Most rejections at this stage don’t happen because the science is flawed. They happen because the author let their emotions drive the response letter.We have all been there. You read the comment: “The authors fail to cite the seminal work of Smith et al. (2005).”Your pulse spikes. You know Smith et al. is irrelevant. You want to scream. But you can’t. So you write something passive-aggressive like, “We are surprised the reviewer considers this relevant…”You don’t need to be right. You need to be published.To survive this process, you need to strip away the emotion and treat your response letter like a diplomatic treaty. I built the Peer Review Response AI Prompt to act as your dispassionate campaign manager. It doesn’t have an ego. It doesn’t get offended. It simply calculates the most strategic path to “Accept.”When you respond to reviewers, you are walking a tightrope between  (agreeing to everything and ruining your paper) and  (fighting everything and annoying the editor).This prompt forces your AI (ChatGPT, Claude, or Gemini) to find the . It acts as a consultant who knows:: “This is a good point, let’s fix it.”: “This is wrong, but we must decline politely.”: “We can’t do X, but we did Y which addresses the core concern.”It turns “I’m not doing that” into “We have addressed the reviewer’s concern by employing an alternative methodology that preserves data integrity…”Copy the code block below into your AI workspace. It includes a comprehensive “Role Definition” that prevents the AI from sounding like a sycophant or a robot.# Role DefinitionYou are an experienced Academic Publication Consultant with 15+ years of expertise in navigating peer review processes across multiple disciplines. You have successfully guided hundreds of manuscripts through revisions at top-tier journals (Nature, Science, The Lancet, IEEE, ACL, etc.). You understand the psychology of reviewers and editors, the unwritten rules of academic discourse, and the strategic approaches that lead to acceptance.Your core competencies include:- Decoding reviewer concerns and identifying underlying issues- Crafting diplomatic yet substantive responses- Structuring revision strategies that address all feedback systematically- Balancing scientific rigor with persuasive communication- Managing disagreements with reviewers professionally# Task DescriptionHelp me craft a comprehensive, professional response letter to peer reviewers for my manuscript revision. The response should address all reviewer comments systematically, demonstrate respect for the review process, and maximize the chances of manuscript acceptance.**Input Information**:- **Manuscript Title**: [Your paper title]- **Journal Name**: [Target journal]- **Field/Discipline**: [e.g., Computer Science, Medicine, Psychology]- **Number of Reviewers**: [e.g., 3 reviewers]- **Decision Type**: [Major revision / Minor revision / Revise and resubmit]- **Original Reviewer Comments**: [Paste all reviewer comments here]- **Key Changes Made**: [List main revisions you've already completed]- **Points of Disagreement**: [Any reviewer suggestions you cannot or choose not to implement]- **Deadline**: [Submission deadline if applicable]### Part A: Cover Letter to Editor- Express gratitude for the review opportunity- Summarize the revision scope and key improvements- Highlight major changes that strengthen the manuscript- Confirm all reviewer concerns have been addressed- Professional closing with resubmission statement### Part B: Point-by-Point Response DocumentFor each reviewer, provide:- **Reviewer Identification**: Clear labeling (Reviewer 1, 2, 3...)- **Comment Reproduction**: Quote each original comment- **Response Structure**:  - Acknowledgment of the concern  - Explanation of how it was addressed  - Specific reference to revised manuscript sections (page/line numbers)  - If applicable, explanation for alternative approaches taken### Part C: Change Summary Matrix- Table showing all changes with location references- Categorization by type (addition, deletion, revision, clarification)- **Professionalism**: Maintain diplomatic, collegial tone throughout—even when disagreeing- **Completeness**: Address EVERY single point raised, no matter how minor- **Specificity**: Include exact page numbers, line numbers, and section references- **Evidence-Based**: Support responses with citations, data, or logical reasoning- **Structural Clarity**: Use consistent formatting for easy navigation- **Conciseness**: Be thorough but avoid unnecessary verbosity## 3. Format Requirements**Response Letter Format**:- Use clear section headers and numbering- Employ visual hierarchy (bold for reviewer comments, regular for responses)- Include a change tracking summary table- Use block quotes for original reviewer comments- Provide line/page references in [brackets] or (parentheses)**Length Guidelines**:- Cover letter: 300-500 words- Individual responses: 100-500 words per point depending on complexity- Total document: Scale appropriately to number of comments- **Language Style**: Professional academic English, formal but accessible- **Tone**: Respectful, constructive, appreciative—never defensive or dismissive- **Perspective**: First-person plural ("We") for multi-author papers; first-person singular ("I") for solo authors- **Technical Level**: Match the sophistication level of the original manuscriptBefore finalizing your output, verify:- [ ] Every reviewer comment has been explicitly addressed- [ ] Page/line numbers are included for all referenced changes- [ ] Tone remains professional and non-defensive throughout- [ ] Responses demonstrate genuine engagement with feedback- [ ] Cover letter provides a compelling overview of improvements- [ ] Any disagreements are handled diplomatically with clear justification- [ ] Document formatting is consistent and easy to navigate- [ ] Grammar and spelling are impeccable- **Never ignore a comment**: Even seemingly trivial comments must be acknowledged- **Avoid defensive language**: Phrases like "the reviewer misunderstood" should be replaced with "we have clarified this point"- **Show gratitude strategically**: Thank reviewers for insights that genuinely improved the work- **Handle disagreements wisely**: When not implementing a suggestion, provide substantial justification with citations or methodology constraints- **Maintain manuscript integrity**: Don't make changes that compromise your research just to satisfy reviewers- **Track everything**: Ensure the response document serves as a complete map of all revisionsPlease generate:1. **Cover Letter to Editor** (ready to paste into submission system)2. **Detailed Point-by-Point Response** (formatted for supplementary document upload)3. **Quick Reference Change Table** (optional but recommended)Use markdown formatting with clear visual hierarchy for easy reading and editing.Why This Works: The Psychology of “Yes”You might ask, “Can’t I just ask the AI to be polite?”You can. But “polite” is often weak. You don’t want to be apologetic; you want to be authoritative. This prompt works because it engineers the response based on psychological leverage points.1. The “Validation Sandwich”Reviewers want to feel heard. The prompt forces a structure where you Acknowledge (validate their expertise), Action (show what you did), and Reference (prove it). It prevents the AI from generating defensive arguments. It ensures that even when you disagree, you are validating the  of their question before dismantling it with data.2. The “Cover Letter” PitchMost authors treat the cover letter as a formality. Big mistake. This prompt treats the cover letter as a . It summarizes your revisions in a way that makes the editor’s job easy. If the editor sees a clear, confident summary of how you fixed the paper, they are biased toward acceptance before they even open the reviewer comments.3. The Specificity AnchorVague promises kill papers.  means nothing. This prompt demands . By forcing the AI to include [Page 4, Line 12], it creates a psychological anchor of truth. It tells the reviewer, "We didn't just think about this; we physically changed the document here."Don’t let Reviewer #2 live rent-free in your head.Use this prompt to detach yourself from the emotional roller coaster. Let the AI handle the diplomacy so you can focus on the science. When you turn a hostile comment into a constructive dialogue, you aren’t just saving your paper. You are mastering the game.]]></content:encoded></item><item><title>Making the Most of Your Docker Hardened Images Enterprise Trial – Part 3</title><link>https://www.docker.com/blog/making-the-most-of-your-docker-hardened-images-enterprise-trial-part-3/</link><author>Aditya Tripathi</author><category>docker</category><category>devops</category><pubDate>Sun, 25 Jan 2026 17:51:40 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Customizing Docker Hardened ImagesBut no matter how secure a base image is, it is useless if you can’t run your application on it. This brings us to the most common question engineers ask during a DHI trial: what if I need a custom image?Hardened images are minimal by design. They lack package managers (apt, apk, yum), utilities (wget, curl), and even shells like bash or sh. This is a security feature: if a bad actor breaks into your container, they find an empty toolbox.However, developers often need these tools during setup. You might need to install a monitoring agent, a custom CA certificate, or a specific library.In this final part of our series, we will cover the two strategies for customizing DHI: the Docker Hub UI (for platform teams creating “Golden Images”) and the multi-stage build pattern (for developers building applications).Option 1: The Golden Image (Docker Hub UI)If you are a Platform or DevOps Engineer, your goal is likely to provide a “blessed” base image for your internal teams. For example, you might want a standard Node.js image that  includes your corporate root CA certificate and your security logging agent.The Docker Hub UI is the preferred path for this. The strongest argument for using the Hub UI is maintenance automation.The Killer Feature: Automatic RebuildsWhen you customize an image via the UI, Docker understands the relationship between your custom layers and the hardened base. If Docker releases a patch for the underlying DHI base image (e.g., a fix in glibc or openssl), Docker Hub automatically rebuilds your custom image.You don’t need to trigger a CI pipeline. You don’t need to monitor CVE feeds. The platform handles the patching and rebuilding, ensuring your “Golden Image” is always compliant with the latest security standards.Since you have an Organization setup for this trial, you can explore this directly in Docker Hub.First, navigate to  in your organization dashboard. Locate the image you want to customize (e.g., dhi-node), then the Customizations tab and click the “Create customization” action. This initiates a customization workflow as follows:In the “Add packages” section, you can search for and select OS packages directly from the distribution’s repository. For example, here we are adding bash to the image for debugging purposes. You can also add “OCI Artifacts” to inject custom files like certificates or agents.Finally, configure the runtime settings (User, Environment Variables) and review your build. Docker Hub will verify the configuration and queue the build. Once complete, this image will be available in your organization’s private registry and will automatically rebuild whenever the base DHI image is updated.This option is best suited for creating standardized “golden” base images that are used across the entire organization. The primary advantage is zero-maintenance security patching due to automatic rebuilds by Docker Hub. However, it is less flexible for rapid, application-specific iteration by individual development teams.Option 2: Multi-Stage BuildIf you are an developper, you likely define your environment in a Dockerfile that lives alongside your code. You need flexibility, and you need it to work locally on your machine.Since DHI images don’t have apt-get or curl, you cannot simply RUN apt-get install my-lib in your Dockerfile. It will fail.Instead, we use the multi-stage build pattern. The concept is simple:Stage 1 (Builder): Use a standard “fat” image (like debian:bookworm-slim) to download, compile, and prepare your dependencies.Stage 2 (Runtime): Copy  the resulting artifacts into the pristine DHI base.This keeps your final image minimal, non-root, and secure, while still allowing you to install whatever you need.Hands-on Tutorial: Adding a Monitoring AgentLet’s try this locally. We will simulate a common real-world scenario: adding the Datadog APM library (dd-trace) globally to a Node.js DHI image.Create a new directory for this test and add a simple server.js file. This script attempts to load the dd-trace library to verify our installation.// Simple Express server to demonstrate DHI customization
console.log('Node.js version:', process.version);
try {
  require('dd-trace');
  console.log('dd-trace module loaded successfully!');
} catch (e) {
  console.error('Failed to load dd-trace:', e.message);
  process.exit(1);
}
console.log('Running as UID:', process.getuid(), 'GID:', process.getgid());
console.log('DHI customization test successful!');

Now, create the Dockerfile. We will use a standard Debian image to install the library, and then copy it to our DHI Node.js image. Create a new directory for this test and add a simple server.js file. This script attempts to load the dd-trace library to verify our installation.# Stage 1: Builder - a standard Debian Slim image that has apt, curl, and full shell access.
FROM debian:bookworm-slim AS builder


# Install Node.js (matching our target version) and tools
RUN apt-get update &amp;&amp; \
    apt-get install -y curl &amp;&amp; \
    curl -fsSL https://deb.nodesource.com/setup_24.x | bash - &amp;&amp; \
    apt-get install -y nodejs


# Install Datadog APM agent globally (we force the install prefix to /usr/local so we know exactly where files go)
RUN npm config set prefix /usr/local &amp;&amp; \
    npm install -g dd-trace@5.0.0


# Stage 2: Runtime - we switch to the Docker Hardened Image.
FROM &lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips


# Copy only the required library from the builder stage
COPY --from=builder /usr/local/lib/node_modules/dd-trace /usr/local/lib/node_modules/dd-trace


# Environment Configuration
# DHI images are strict. We must explicitly tell Node where to find global modules.
ENV NODE_PATH=/usr/local/lib/node_modules


# Copy application code
COPY app/ /app/


WORKDIR /app


# DHI Best Practice: Use the exec form (["node", ...]) 
# because there is no shell to process strings.
CMD ["node", "server.js"]

docker build -t dhi-monitoring-test .
Now run it. If successful, the container should start, find the library, and exit cleanly.docker run --rm dhi-monitoring-test
Node.js version: v24.11.0
dd-trace module loaded successfully!
Running as UID: 1000 GID: 1000
DHI customization test successful!

Success! We have a working application with a custom global library, running on a hardened, non-root base.We successfully customized the image. But did we compromise its security?This is the most critical lesson of operationalizing DHI: hardened base images protect the OS, but they do not protect you from the code you add.Let’s verify our new image with Docker Scout.docker scout cves dhi-monitoring-test --only-severity critical,high
    ✗ Detected 1 vulnerable package with 1 vulnerability
...
   0C     1H     0M     0L  lodash.pick 4.4.0           
pkg:npm/lodash.pick@4.4.0                               
                                                        
    ✗ HIGH CVE-2020-8203 [Improperly Controlled Modification of Object Prototype Attributes]

This result is accurate and important. The base image (OS, OpenSSL, Node.js runtime) is still secure. However, the dd-trace library we just installed pulled in a dependency (lodash.pick) that contains a High severity vulnerability.This proves that your verification pipeline works.If we hadn’t scanned the  image, we might have assumed we were safe because we used a “Hardened Image.” By using Docker Scout on the final artifact, we caught a supply chain vulnerability introduced by  customization.Let’s check how much “bloat” we added compared to the clean base.docker scout compare --to &lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips dhi-monitoring-test

You will see that the only added size corresponds to the dd-trace library (~5MB) and our application code. We didn’t accidentally inherit apt, curl, or the build caches from the builder stage. The attack surface remains minimized.A Note on Provenance: Who Signs What?In Part 2, we verified the SLSA Provenance and cryptographic signatures of Docker Hardened Images. This is crucial for establishing a trusted supply chain. When you customize an image, the question of who “owns” the signature becomes important.Docker Hub UI Customization: When you customize an image through the Docker Hub UI, Docker itself acts as the builder for your custom image. This means the resulting customized image inherits signed provenance and attestations directly from Docker’s build infrastructure. If the base DHI receives a security patch, Docker automatically rebuilds and re-signs your custom image, ensuring continuous trust. This is a significant advantage for platform teams creating “golden images.”: When you build a custom image using a multi-stage Dockerfile locally (as we did in our tutorial),  are the builder. Your docker build command produces a  image with a  digest. Consequently, the original DHI signature from Docker does not apply to your final custom image (because the bits have changed and you are the new builder).However, the chain of trust is not entirely broken:Base Layers: The underlying DHI layers within your custom image still retain their original Docker attestations.Custom Layer: Your organization is now the “builder” of the new layers.For production deployments using the multi-stage build, you should integrate Cosign or Docker Content Trust into your CI/CD pipeline to sign your custom images. This closes the loop, allowing you to enforce policies like: “Only run images built by MyOrg, which are based on verified DHI images and have our internal signature.”Measuring Your ROI: Questions for Your TeamAs you conclude your Docker Hardened Images trial, it’s critical to quantify the value for your organization. Reflect on the concrete results from your migration and customization efforts using these questions:: How significantly did DHI impact your CVE counts? Compare the “before and after” vulnerability reports for your migrated services. What is the estimated security risk reduction? What was the actual engineering effort required to migrate an image to DHI? Consider the time saved on patching, vulnerability triage, and security reviews compared to managing traditional base images.: How well does DHI integrate into your team’s existing development and CI/CD workflows? Do developers find the customization patterns (Golden Image / Builder Pattern) practical and efficient? Is your team likely to adopt this long-term?: Has DHI simplified your compliance reporting or audit processes due to its SLSA provenance and FIPS compliance? What is the impact on your regulatory burden?Thanks for following through to the end! Over this 3-part blog series, you have moved from a simple trial to a fully operational workflow:: You replaced a standard base image with DHI and saw immediate vulnerability reduction.: You independently validated signatures, FIPS compliance, and SBOMs.: You learned to extend DHI using the Hub UI (for auto-patching) or multi-stage builds, while checking for new vulnerabilities introduced by your own dependencies.The lesson here is that the “Hardened” in Docker Hardened Images isn’t a magic shield but a clean foundation. By building on top of it, you ensure that your team spends time securing  application code, rather than fighting a never-ending battle against thousands of upstream vulnerabilities.]]></content:encoded></item><item><title>Dia Browser Keeps Sneaking into my Workflow in the Best Ways</title><link>https://blog.devops.dev/dia-browser-keeps-sneaking-into-my-workflow-in-the-best-ways-23fa6dde49cc?source=rss----33f8b2d9a328---4</link><author>Eduardo Barth</author><category>devops</category><pubDate>Sun, 25 Jan 2026 13:56:55 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Making the Most of Your Docker Hardened Images Enterprise Trial – Part 2</title><link>https://www.docker.com/blog/making-the-most-of-your-docker-hardened-images-enterprise-trial-part-2/</link><author>Aditya Tripathi</author><category>docker</category><category>devops</category><pubDate>Sat, 24 Jan 2026 20:24:39 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Verifying Security and Compliance of Docker Hardened ImagesIn Part 1 of this series, we migrated a Node.js service to Docker Hardened Images (DHI) and measured impressive results: 100% vulnerability elimination, 90% package reduction, and 41.5% size decrease. We extracted the SBOM and saw compliance labels for FIPS, STIG, and CIS.The numbers look compelling. But how do you  these claims independently?Security tools earn trust through verification, not promises. When evaluating a security product for production, you need cryptographic proof. This is especially true for images that form the foundation of every container you deploy.This post walks through the verification process: signature validation, provenance analysis, compliance evidence examination, and SBOM analysis. We’ll focus on practical verification you can run during your trial, with links to the official DHI documentation for deeper technical details. By the end, you’ll have independently confirmed DHI’s security posture and built confidence for a production scenario.Understanding Security Attestations available with Docker Hardened ImagesBefore diving into verification, you need to understand what you’re verifying.Docker Hardened Images include : cryptographically-signed metadata about the image’s build process, contents, and compliance posture. These are signed statements that can be independently verified.List all attestations for your hardened image:docker scout attestation list registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips
This shows 16 different attestation types:https://slsa.dev/provenance/v0.2            SLSA provenance
https://docker.com/dhi/fips/v0.1            FIPS compliance
https://docker.com/dhi/stig/v0.1            STIG scan
https://cyclonedx.org/bom/v1.6              CycloneDX SBOM
https://spdx.dev/Document                   SPDX SBOM
https://scout.docker.com/vulnerabilities    Scout vulnerabilities
https://scout.docker.com/secrets/v0.1       Scout secret scan
https://scout.docker.com/virus/v0.1         Scout virus/malware
https://scout.docker.com/tests/v0.1         Scout test report
https://openvex.dev/ns/v0.2.0               OpenVEX
...

Each attestation is a JSON document describing a specific aspect of the image. The most critical attestations for verification:: Build source, builder identity, and build process details: Complete software bill of materials: Evidence of FIPS 140-3 certified cryptographic modules: Security Technical Implementation Guide compliance results: CVE assessmentCVEexploitabilityThese attestations follow the in-toto specification, an open framework for supply chain security. Each attestation includes:Subject: What the attestation describes (the container image)Predicate: The actual claims (FIPS certified, STIG compliant, etc.)Signature: Cryptographic signature from the builderLet’s see how you can verify the signatures yourself.Verifying Attestations with Docker ScoutThe attestations we’re about to examine are cryptographically signed by Docker’s build infrastructure. Docker Scout provides a simple, integrated approach that handles DHI attestations natively and without the hassle of managing public keys or certificate chains.To validate an attestation, simply append the –verify flag, which provides explicit validation feedback. This process relies on cryptographic hashing: the digest is a hash of the attestation content, so even a single character change completely alters the hash. Moreover, the attestation’s signature is cryptographically bound to the specific image digest it describes, guaranteeing that the metadata you’re verifying corresponds exactly to the image you have and preventing substitution attacks.Retrieving an AttestationTo extract a specific attestation (like SLSA provenance), use the attestation get command with the full predicate type URI:docker scout attestation get registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips \
  --predicate-type https://slsa.dev/provenance/v0.2 \
  --output provenance.json

✓ SBOM obtained from attestation, 32 packages found
✓ Provenance obtained from attestation
✓ Report written to provenance.json

The checkmarks confirm Docker Scout successfully retrieved and verified the attestation. Behind the scenes, Scout validated:The attestation signature matches Docker’s signing keyThe signature hasn’t expiredThe attestation applies to this specific image digestThe attestation hasn’t been tampered withIf signature verification fails, Scout returns an error and won’t output the attestation file.To learn more about available predicate types, check out the DHI verification documentation.Validating SLSA ProvenanceSignatures prove attestations are authentic. Provenance shows where the image came from.SLSA (Supply-chain Levels for Software Artifacts) is a security framework developed by Google, the Linux Foundation, and other industry partners. It defines levels of supply chain security maturity, from SLSA 0 (no guarantees) to SLSA 4 (highest assurance).Docker Hardened Images target SLSA 3, which requires:Build process fully scripted/automatedAll build steps defined in version controlProvenance generated automatically by build serviceProvenance includes source, builder, and build parametersUsing our previously extracted SLSA provenance.json, we can check the source repository and commit hash:jq '.predicate.invocation.environment.github_repository' provenance.json
"docker-hardened-images/definitions"
jq '.predicate.invocation.environment.github_sha1' provenance.json

"698b367344efb3a7d443508782de331a84216ae4"
Similarly, you can see exactly what  produced this image.jq '.predicate.builder.id' provenance.json

“https://github.com/docker-hardened-images/definitions/actions/runs/18930640220/attempts/1”For DHI Enterprise Users: Verifying High-Assurance ClaimsWhile the free hardened images are built with security best practices, DHI Enterprise images carry the specific certifications required for FedRAMP, HIPAA, and financial audits. Here is how to verify those high-assurance claims.FIPS (Federal Information Processing Standard) 140-3 is a U.S. government standard for cryptographic modules. Think of it as a certification that proves the cryptography in your software has been tested and validated by independent labs against federal requirements.If you’re building software for government agencies, financial institutions, or healthcare providers, FIPS compliance is often mandatory: without it, your software can’t be used in those environments!Check if the image includes FIPS-certified cryptography:docker scout attestation get registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips \
  --predicate-type https://docker.com/dhi/fips/v0.1 \
  --output fips-attestation.json

{
  "certification": "CMVP #4985",
  "certificationUrl": "https://csrc.nist.gov/projects/cryptographic-module-validation-program/certificate/4985",
  "name": "OpenSSL FIPS Provider",
  "package": "pkg:dhi/openssl-provider-fips@3.1.2",
  "standard": "FIPS 140-3",
  "status": "active",
  "sunsetDate": "2030-03-10",
  "version": "3.1.2"
}

The certificate number (4985) is the key piece. This references a specific FIPS validation in the official NIST CMVP database.STIG (Security Technical Implementation Guide) is the Department of Defense’s (DoD) checklist for securing systems. It’s a comprehensive security configuration standard needed for deploying software for defense or government work.DHI images undergo STIG scanning before release. Docker uses a custom STIG based on the DoD’s General Operating System Security Requirements Guide. Each scan checks dozens of security controls and reports findings. You can extract and review STIG scan results:docker scout attestation get registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips \
  --predicate-type https://docker.com/dhi/stig/v0.1 \
  --output stig-attestation.json

Check the STIG scan summary:jq '.predicate[0].summary' stig-attestation.json
{
  "failedChecks": 0,
  "passedChecks": 91,
  "notApplicableChecks": 107,
  "totalChecks": 198,
  "defaultScore": 100,
  "flatScore": 91
}

This shows DHI passed all 91 applicable STIG controls with zero failed checks and a 100% score. The 107 “notApplicableChecks” typically refer to controls that are irrelevant to the specific minimal container environment or its configuration. For a complete list of STIG controls and DHI compliance details, including how to extract and view the full STIG scan report, see the DHI STIG documentation.CIS (Center for Internet Security) Benchmarks are security configuration standards created by security professionals across industries. Much like STIGs, they represent consensus best practices, but unlike government-mandated frameworks (FIPS, STIG), CIS benchmarks are community-developed.CIS compliance isn’t legally required, but it demonstrates you’re following industry-standard security practices—valuable for customer trust and audit preparation.You can verify CIS compliance through image labels:docker inspect &lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips | \
  jq '.[0].Config.Labels["com.docker.dhi.compliance"]'

What exactly is a SBOM used for?Compliance frameworks tell you what standards you meet. The SBOM tells you what’s actually in your container—and that’s where the real security work begins.Identifying Transitive DependenciesWhen you add a package to your project, you see the direct dependency. What you don’t see: that package’s dependencies, and  dependencies, and so on. This is the transitive dependency problem.A vulnerability in a transitive dependency you’ve never heard of can compromise your entire application. Real example: the Log4Shell vulnerability affected millions of applications because Log4j was a transitive dependency buried several levels deep in dependency chains.Most vulnerabilities hide in transitive dependencies because:Developers don’t know they existThey’re not updated when the direct dependency updatesScanning tools miss them without an SBOMMinimal images reduce this risk dramatically. Fewer packages = fewer transitive dependencies = smaller attack surface.Compare dependency counts:Official Node.js image: 321 packages, ~1,500 dependency relationshipsDHI Node.js image: 32 packages, ~150 dependency relationships90% reduction in packages means 90% reduction in transitive dependency risk.Scanning for Known (Exploitable) VulnerabilitiesWith the SBOM extracted, scan for known vulnerabilities:docker scout cves registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips
Target: &lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips

  0C     0H     0M     8L

8 vulnerabilities found in 2 packages
  CRITICAL  0
  HIGH      0
  MEDIUM    0
  LOW       8

Zero critical, high, or medium severity vulnerabilities. Docker Scout cross-references the SBOM against multiple vulnerability databases (NVD, GitHub Security Advisories, etc.).This is the payoff of minimal images: fewer packages means fewer potential vulnerabilities. The official Node.js image had 25 CVEs across CRITICAL, HIGH, and MEDIUM severities. The hardened version has zero actionable vulnerabilities—not because vulnerabilities were patched, but because vulnerable packages were removed entirely.Understanding Exploitability with VEXNot all CVEs are relevant to your deployment. A vulnerability in a library function your application never calls, or a flaw in a service that isn’t running, doesn’t pose real risk. Docker Hardened Images include signed VEX attestations that identify which reported CVEs are not actually exploitable in the image’s runtime context. This helps you distinguish between CVEs that exist in a package (reported), and CVEs that can  given how the package is used in this specific image (exploitable). In other words, VEX reduces false positives.You can see which CVEs have been evaluated with this command:docker scout attestation get registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips \
  --predicate-type https://openvex.dev/ns/v0.2.0 \
  --output vex.json

License Compliance AnalysisWhen you use open source software, you’re bound by license terms. Some licenses (MIT, Apache) are permissive and you can use them freely, even in commercial products. Others (GPL, AGPL) are copyleft: they require you to release your source code if you distribute software using them.SBOMs make license compliance visible. Without an SBOM, you’re blind to what licenses your containers include.Export the SBOM in SPDX format:docker scout sbom registry://&lt;your-org-namespace&gt;/dhi-node:24.11-debian13-fips \
  --format spdx \
  --output node-sbom-spdx.json

Analyze license distribution:jq '.packages[].licenseConcluded' node-sbom-spdx.json | \
  sort | uniq -c | sort -rn

15 "MIT"
8 "Apache-2.0"
5 "GPL-2.0-or-later"
2 "BSD-3-Clause"
1 "OpenSSL"
1 "NOASSERTION"

✅ MIT and Apache-2.0 are permissive (safe for commercial use)⚠️ GPL-2.0-or-later requires review (is this a runtime dependency or build tool?)⚠️ NOASSERTION needs investigationConclusion: What You’ve ProvenYou’ve independently verified critical security claims Docker makes about Hardened Images:: Cryptographic signatures prove images are genuine and unmodified: SLSA attestations trace builds to specific source commits in public repositories: FIPS certificate, STIG controls passed, and CIS benchmarks metEvery claim you verified (except CIS) has a corresponding attestation you can check yourself, audit, and validate in your CI/CD pipeline.You can customize a Docker Hardened Image (DHI) to suit your specific needs using the Docker Hub UI. This allows you to select a base image, add packages, add OCI artifacts (such as custom certificates or additional tools), and configure settings. In addition, the build pipeline ensures that your customized image is built securely and includes attestations.In Part 3, we’ll cover how to customize Docker Hardened Images to suit your specific needs, while keeping all the benefits we just explored.You’ve confirmed DHI delivers on security promises. Next, we’ll make it operational.If you missed reading part 1, where we discussed how you can get to 100% vulnerability elimination and 90% package reduction, read the blog here. ]]></content:encoded></item><item><title>Restic Backups: Protect Your Server Before Disaster Strikes</title><link>https://blog.devops.dev/restic-backups-protect-your-server-before-disaster-strikes-9f76eb16a711?source=rss----33f8b2d9a328---4</link><author>bektiaw</author><category>devops</category><pubDate>Sat, 24 Jan 2026 13:56:56 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Futurum Group Survey Sees Increasing Investments in AI to Deliver Software</title><link>https://devops.com/futurum-group-survey-sees-increasing-investments-in-ai-to-deliver-software/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 23 Jan 2026 18:10:31 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Digest #198: Kubernetes Security, Scaling Postgres at OpenAI and AI in Infra-as-Code</title><link>https://www.devopsbulletin.com/p/digest-198-kubernetes-security-scaling</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/odP153inZUo" length="" type=""/><pubDate>Fri, 23 Jan 2026 15:27:20 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin.We’re kicking off with AI and Infrastructure-as-Code. IaCConf Spotlight on January 28 brings engineers together to share how they’re using AI in IaC safely and pragmatically, from AI-assisted workflows to better module adoption and team collaboration. In the news, we look at how OpenAI scaled PostgreSQL to support 800 million ChatGPT users, why self-hosted GitHub Actions runners are becoming a new attacker backdoor, and what Docker has quietly turned into over the years. We also cover why some teams are replacing Redis with PostgreSQL (and getting better performance).On the hands-on side: unconventional ways to speed up PostgreSQL queries, a solid Terraform + GitHub Actions pipeline for multi-environment deployments, a deep dive into attacking and defending Kubernetes, and practical techniques for processing tens of millions of rows without waiting hours. We also look at building agents with the GitHub Copilot SDK, best practices for coding with AI agents, and safer ways to manage secrets locally.This week's video goes deep into Kubernetes internals. This hands-on course walks through building a real-world Kubernetes Operator from scratch.Open-source picks this week include Turso, a modern SQLite-compatible in-process database; Terratest, a Go toolkit for testing Terraform and cloud infrastructure; pgschema, a Terraform-style CLI for Postgres schema migrations; a VS Code editor that masks .env secrets by default; and promptfoo, a local-first CLI for testing, red-teaming, and securing LLM apps.All this and more in this week’s DevOps Bulletin, don’t miss out!IaCConf Spotlight: AI in IaCHear from engineers how they’re applying AI to infrastructure safely and pragmatically, from AI-assisted IaC to improving module adoption and team workflows at IaCConf Spotlight on January 28.  Consider supporting it with a paid subscription. You’ll keep the free Friday issues  get extras like bonus deep-dives, templates, and the full archive.A SQLite-compatible in-process , pushing SQLite forward with async I O, vectors, and modern internals.A Go testing  that turns Terraform, Packer, Kubernetes, and cloud setups into repeatable automated tests.A Terraform-style  for declarative Postgres schema migrations, plan, diff, and apply with confidence.A VSCode  that safely edits .env files by masking secrets by default while keeping local, inline control.A local first  for evaluating, red teaming, and securing LLM apps with repeatable tests and CI-friendly reports. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via  or simply reply to this email. ]]></content:encoded></item><item><title>How to Set Up EFK Stack on Kubernetes using Operators</title><link>https://blog.devops.dev/how-to-set-up-efk-stack-on-kubernetes-using-operators-dc6ad08d4e3d?source=rss----33f8b2d9a328---4</link><author>Moiz Ali Moomin</author><category>devops</category><pubDate>Fri, 23 Jan 2026 14:02:08 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to Automate Arm Migration with Docker MCP Toolkit, VS Code, and GitHub Copilot</title><link>https://www.docker.com/blog/automate-arm-migration-docker-mcp-copilot/</link><author>Jennifer Kohl</author><category>docker</category><category>devops</category><pubDate>Fri, 23 Jan 2026 14:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[This post is a collaboration between Docker and Arm, demonstrating how Docker MCP Toolkit and the Arm MCP Server work together to simplify architecture migrations.Moving workloads from x86 to Arm64 architecture has become increasingly important. Organizations seek to reduce cloud costs and improve performance. AWS Graviton, Azure Cobalt, and Google Cloud Axion have made Arm-based computing mainstream, promising 20-40% cost savings and better performance for many workloads.But here’s the challenge: How do you migrate your applications to Arm without breaking things?Traditional migration approaches require:Manual code analysis for x86-specific dependenciesTedious compatibility checks across multiple toolsManual performance evaluationWhat if you could orchestrate the entire Arm migration workflow from a single interface? Docker MCP Toolkit makes this possible. By connecting specialized Arm migration tools directly to GitHub Copilot, you can automate compatibility analysis, intrinsic conversion, and performance prediction—all through natural conversation in VS Code.Here’s what that looks like in practice: You ask GitHub Copilot to migrate your legacy C++ application to Arm64. Copilot doesn’t just tell you what needs changing—it actually executes: scanning your code for x86 intrinsics, converting x86 SIMD intrinsics to Arm SIMD intrinsics, updating your Dockerfile, predicting Arm performance improvements, and creating a pull request with all changes. All through natural conversation in VS Code. No manual porting. No up-front architecture expertise required.If you have questions about any step in the process, you can directly ask Copilot, which will invoke the Arm MCP Server knowledge base tool. The knowledge base has information pulled directly from all Learning Paths on learn.arm.com, as well as knowledge of all Arm intrinsics, and will both summarize that information for you as well as provide links to the concrete documentation that you can peruse yourself. Now you might ask – “Can’t I just rebuild my Docker image for Arm64?” True, for most applications. But when you hit that one legacy app with hand-optimized x86 assembly, AVX2 intrinsics, or architecture-specific compiler flags? That’s when Docker MCP Toolkit with the Arm MCP Server becomes essential.By the end of this guide, you’ll migrate a real-world legacy application—a matrix multiplication benchmark written with AVX2 intrinsics for x86—to Arm64 automatically using GitHub Copilot and Docker MCP Toolkit.What normally takes 5-7 hours of manual work will take you about 25 to 30 minutes.The Arm Migration ChallengeLet me show you exactly what we’re solving. Consider a matrix multiplication benchmark originally written for x86-64 with AVX2 optimizations—the kind of code that makes Arm migration painful.Here’s a Dockerfile that will cause problems when trying to migrate to Graviton:FROM centos:6

# CentOS 6 reached EOL, need to use vault mirrors
RUN sed -i 's|^mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-Base.repo && \
    sed -i 's|^#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-Base.repo

# Install EPEL repository (required for some development tools)
RUN yum install -y epel-release && \
    sed -i 's|^mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/epel.repo && \
    sed -i 's|^#baseurl=http://download.fedoraproject.org/pub/epel|baseurl=http://archives.fedoraproject.org/pub/archive/epel|g' /etc/yum.repos.d/epel.repo

# Install Developer Toolset 2 for better C++11 support (GCC 4.8)
RUN yum install -y centos-release-scl && \
    sed -i 's|^mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-SCLo-scl.repo && \
    sed -i 's|^mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-SCLo-scl-rh.repo && \
    sed -i 's|^# baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-SCLo-scl.repo && \
    sed -i 's|^# baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-SCLo-scl-rh.repo

# Install build tools
RUN yum install -y \
    devtoolset-2-gcc \
    devtoolset-2-gcc-c++ \
    devtoolset-2-binutils \
    make \
    && yum clean all

WORKDIR /app
COPY *.h *.cpp ./

# AVX2 intrinsics are used in the code
RUN scl enable devtoolset-2 "g++ -O2 -mavx2 -o benchmark \
    main.cpp \
    matrix_operations.cpp \
    -std=c++11"

CMD ["./benchmark"]

Now you might ask why this won’t work on Arm? Looking at this Dockerfile, there are two immediate blockers for Graviton migration:No Arm64 support in base image – The  image was built for x86 only, so this container won’t even start on Arm hardware.x86-specific compiler flag – The  flag tells the compiler to use AVX2 vector instructions, which don’t exist on Arm processors.Even experienced developers miss these issues in larger codebases.The source code uses AVX2 intrinsics for vectorized operations:#include "matrix_operations.h"
#include <iostream>
#include <random>
#include <chrono>
#include <stdexcept>
#include <immintrin.h>  // AVX2 intrinsics

Matrix::Matrix(size_t r, size_t c) : rows(r), cols(c) {
    data.resize(rows, std::vector<double>(cols, 0.0));
}

void Matrix::randomize() {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<> dis(0.0, 10.0);

    for (size_t i = 0; i < rows; i++) {
        for (size_t j = 0; j < cols; j++) {
            data[i][j] = dis(gen);
        }
    }
}

Matrix Matrix::multiply(const Matrix& other) const {
    if (cols != other.rows) {
        throw std::runtime_error("Invalid matrix dimensions for multiplication");
    }

    Matrix result(rows, other.cols);

    // x86-64 optimized using AVX2 for double-precision
    for (size_t i = 0; i < rows; i++) {
        for (size_t j = 0; j < other.cols; j++) {
            __m256d sum_vec = _mm256_setzero_pd();
            size_t k = 0;

            // Process 4 elements at a time with AVX2
            for (; k + 3 < cols; k += 4) {
                __m256d a_vec = _mm256_loadu_pd(&data[i][k]);
                __m256d b_vec = _mm256_set_pd(
                    other.data[k+3][j],
                    other.data[k+2][j],
                    other.data[k+1][j],
                    other.data[k][j]
                );
                sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));
            }

            // Horizontal add using AVX
            __m128d sum_high = _mm256_extractf128_pd(sum_vec, 1);
            __m128d sum_low = _mm256_castpd256_pd128(sum_vec);
            __m128d sum_128 = _mm_add_pd(sum_low, sum_high);

            double sum_arr[2];
            _mm_storeu_pd(sum_arr, sum_128);
            double sum = sum_arr[0] + sum_arr[1];

            // Handle remaining elements
            for (; k < cols; k++) {
                sum += data[i][k] * other.data[k][j];
            }

            result.data[i][j] = sum;
        }
    }

    return result;
}

double Matrix::sum() const {
    double total = 0.0;
    for (size_t i = 0; i < rows; i++) {
        for (size_t j = 0; j < cols; j++) {
            total += data[i][j];
        }
    }
    return total;
}

void benchmark_matrix_ops() {
    std::cout << "\n=== Matrix Multiplication Benchmark ===" << std::endl;

    const size_t size = 200;
    Matrix a(size, size);
    Matrix b(size, size);

    a.randomize();
    b.randomize();

    auto start = std::chrono::high_resolution_clock::now();
    Matrix c = a.multiply(b);
    auto end = std::chrono::high_resolution_clock::now();

    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Matrix size: " << size << "x" << size << std::endl;
    std::cout << "Time: " << duration.count() << " ms" << std::endl;
    std::cout << "Result sum: " << c.sum() << std::endl;
}

If you look at the following code, you might find that this code is heavily optimized for Intel/AMD x86 processors and won’t work on Arm. –  only exists on x86 systems. Arm uses  instead.AVX2 intrinsics throughout – Every  function is Intel-specific: – Creates a 256-bit zero vector (Arm NEON is 128-bit) – Loads 4 doubles at once (NEON loads 2) – Sets 4 doubles (no direct NEON equivalent) /  – 256-bit operations (NEON uses 128-bit) – Extracts high 128 bits (not needed on NEON) – AVX2 processes 4 doubles per operation, while Arm NEON processes 2. The entire loop structure needs adjustment. ( on newer Arm cores (Neoverse V1/V2, Graviton 3/4) provides 256-bit or wider vector-length agnostic (VLA) registers, matching or exceeding AVX2 registers.)Horizontal reduction logic – The horizontal add pattern using  and  is x86-specific and must be completely rewritten for Arm SIMD.Manual conversion requires rewriting 30+ lines of intrinsic code, adjusting loop strides, and testing numerical accuracy. This is exactly where automated migration tools become essential.Each of these issues blocks Arm migration in different ways. Manual migration requires not just converting intrinsics, but also modernizing the entire build infrastructure, finding Arm equivalents, and validating performance. For any substantial codebase, this becomes prohibitively expensive.What GitHub Copilot Can and Can’t Do Without Arm MCPLet’s be clear about what changes when you add the Arm MCP Server to Docker MCP Toolkit.You ask GitHub Copilot to migrate your C++ application from x86 to Arm64. Copilot responds with general advice: “Convert AVX2 intrinsics to NEON”, “Update your Dockerfile to use ARM64 base image”, “Change compiler flags”. Then you must manually research NEON equivalents, rewrite hundreds of lines of intrinsic code, update the Dockerfile yourself, hope you got the conversion right, and spend hours debugging compilation errors.Yes, Copilot can write code. But without specialized tools, it’s guessing based on training data—not using concrete knowledge base documentation or using purpose-built tools to analyze your actual application architecture.With Arm MCP + Docker MCP ToolkitYou ask GitHub Copilot the same thing. Within minutes, it:Uses  tool to verify your base image supports ARM64Runs  on your actual codebase to find x86-specific codeUses  to find correct Arm SIMD equivalents for every x86 intrinsicConverts your code with architecture-specific accuracyUpdates your Dockerfile with Arm-compatible base imagesCreates a pull request with all changes.Real code gets scanned. Real intrinsics get converted. Real pull requests appear in your repository. Close VS Code, come back tomorrow, and the migration is ready to test, complete with documentation explaining every change.The difference? Docker MCP Toolkit gives GitHub Copilot access to actual Arm migration tooling, not just general knowledge about Arm architecture.Why This Is Different from Manual MigrationYou could manually use Arm migration tools: install utilities locally, run checks, research intrinsics, update code. Here’s what that process looks like:Install Arm migration tools (15 minutes)Run compatibility scans (5 minutes)Research each x86 intrinsic equivalent (30 minutes per intrinsic)Manually rewrite code (2-3 hours)Update Dockerfile (15 minutes)Fix compilation errors (1-2 hours)Document changes (30 minutes)Total: 5-7 hours per applicationWith Docker MCP Toolkit + Arm MCP:Ask GitHub Copilot to migrate (20 minutes)Review and approve changes (10-20 minutes)Total: 30-40 minutes per applicationSetting Up Visual Studio Code with Docker MCP ToolkitBefore you begin, make sure you have:A machine with 8 GB RAM minimum (16GB recommended)The latest Docker Desktop releaseVS Code with GitHub Copilot extensionGitHub account with personal access tokenStep 1. Enable Docker MCP ToolkitOpen Docker Desktop and enable the MCP Toolkit from Settings.Go to  → Toggle  ONCaption: Enabling Docker MCP Toolkit under Docker Desktop Add Required MCP Servers from CatalogAdd Arm, Sequential Thinking and GitHub Official by following the links below, or by selecting “Catalog” in the Docker Desktop MCP toolkit:Caption: Searching for Arm MCP Server in the Docker MCP CatalogStep 2. Configure the ServersConfigure the Arm MCP ServerTo access your local code for the migrate-ease scan and MCA tools, the Arm MCP Server needs a directory configured to point to your local code.Caption: Arm MCP Server configurationOnce you click ‘Save’, the Arm MCP Server will know where to look for your code. If you want to give a different directory access in the future, you’ll need to change this path.Available Arm Migration ToolsClick Tools to view all the six MCP tools available under Arm MCP Server.Caption: List of MCP tools provided by the Arm MCP Server – Semantic search of Arm learning resources, intrinsics documentation, and software compatibility – Code scanner supporting C++, Python, Go, JavaScript, and Java for Arm compatibility analysis – Docker image architecture verification (checks if images support Arm64) – Remote container image inspection without downloading – Machine Code Analyzer for assembly performance analysis and IPC predictions – System architecture information gatheringConfigure GitHub MCP ServerThe GitHub MCP Server lets GitHub Copilot create pull requests, manage issues, and commit changes.Caption: Steps to configure GitHub Official MCP ServerConfigure Authentication:Choose your preferred authentication method For Personal Access Token, you’ll need to get the token from GitHub > Settings > Developer SettingsCaption: Setting up Personal Access Token in GitHub MCP ServerConfigure Sequential Thinking MCP ServerClick Caption: Sequential MCP Server requires zero configurationThis server helps GitHub Copilot break down complex Arm migration decisions into logical steps.Step 3. Add the Servers to VS CodeThe Docker MCP Toolkit makes it incredibly easy to configure MCP servers for clients like VS Code.To configure, click “Clients” and scroll down to Visual Studio Code. Click the “Connect” button:Caption: Setting up Visual Studio Code as MCP ClientNow open VS Code and click on the ‘Extensions’ icon in the left toolbar:Caption: Configuring MCP_DOCKER under VS Code ExtensionsClick the  gear, and click ‘Start Server’:Caption: Starting MCP Server under VS CodeNow you’re ready to perform an Arm migration!Step 4. Verify ConnectionOpen GitHub Copilot Chat in VS Code and ask:What Arm migration tools do you have access to?
You should see tools from all three servers listed. If you see them, your connection works. Let’s migrate some code.Caption: Playing around with GitHub Co-PilotReal-World Demo: Migrating a Legacy x86 ApplicationNow that you’ve connected GitHub Copilot to Docker MCP Toolkit, let’s migrate that matrix multiplication benchmark we looked at earlier.: 20 minutes: $0 (all runs in Docker containers): The code we showed earlier in this postDocker MCP Toolkit orchestrates the migration through a secure MCP Gateway that routes requests to specialized tools: the Arm MCP Server scans code and converts intrinsics, GitHub MCP Server creates pull requests, and Sequential Thinking plans multi-step migrations. Each tool runs in an isolated Docker container: secure, reproducible, and under your control.git clone https://github.com/JoeStech/docker-blog-arm-migration 
Give GitHub Copilot Migration InstructionsOpen your project in VS Code. In GitHub Copilot Chat, paste this prompt:Your goal is to migrate this codebase from x86 to Arm64. Use the Arm MCP Server tools to help you with this migration.

Steps to follow:
1. Check all Dockerfiles - use check_image and/or skopeo tools to verify Arm compatibility, changing the base image if necessary
2. Scan the codebase - run migrate_ease_scan with the appropriate language scanner and apply the suggested changes
3. Use knowledge_base_search when you need Arm architecture guidance or intrinsic equivalents
4. Update compiler flags and dependencies for Arm64 compatibility
5. **Create a pull request with all changes using GitHub MCP Server**

Important notes:
- Your current working directory is mapped to /workspace on the MCP server
- NEON lane indices must be compile-time constants, not variables
- If you're unsure about Arm equivalents, use knowledge_base_search to find documentation
- Be sure to find out from the user or system what the target machine is, and use the appropriate intrinsics. For instance, if neoverse (Graviton, Axion, Cobalt) is targeted, use the latest SME/SME2.


**After completing the migration:**
- Create a pull request with a detailed description of changes
- Include performance predictions and cost savings in the PR description
- List all tools used and validation steps needed

Step 2. Watch Docker MCP Toolkit ExecuteGitHub Copilot orchestrates the migration using Docker MCP Toolkit. Here’s what happens:GitHub Copilot starts by analyzing the Dockerfile’s base image using the Arm MCP Server’s  tool.Caption: GitHub Copilot uses the skopeo tool from the Arm MCP Server to analyze the centos:6 base image. The tool reports that this image has no arm64 build available. This is the first blocker identified – the container won’t even start on Arm hardware.This immediately identifies that CentOS 6 has no Arm64 builds and must be replaced.Next, Copilot runs the  tool with the C++ scanner on the codebase.Caption: The migrate_ease_scan tool analyzes the C++ source code and detects AVX2 intrinsics, the -mavx2 compiler flag, and x86-specific headers. This automated scan identifies all architecture-dependent code that requires conversion – work that could take hours to find manually.The scan results show exactly what needs to change for Arm compatibility. Each detected issue includes the file location, line number, and specific code that requires modification. This precision eliminates guesswork and ensures nothing is missed.Phase 3: Arm Optimization and Best PracticesForx86 intrinsics found in Phase 2, Copilot queries the Arm MCP Server’s knowledge base for Arm equivalents, if needed. It then makes replacements as necessary.Caption: GitHub Copilot uses the knowledge_base_search tool to find Arm NEON equivalents for each AVX2 intrinsic.The tool returns official Arm documentation showing the conversions:  becomes ,  becomes , and so on. This knowledge comes from learn.arm.com learning paths and intrinsic documentation.The knowledge base provides not just the conversion mappings, but also architectural context: AVX2’s 256-bit vectors vs NEON’s 128-bit vectors, which means loop adjustments are needed. Copilot uses this information to rewrite the matrix multiplication code correctly.Phase 4: Create the GitHub PR and SummarizeAfter completing the migration, Copilot creates a PR in GitHub and summarizes the changes made.The changes are substantial: Replaced centos:6 → ubuntu:22.04, added TARGETARCH for multi-arch buildsAdded Arm64 detection and -march=armv8-a+simd compiler flagConverted AVX2 → NEON intrinsics with architecture guardsThe build is now simpler, modern, and Arm-compatible.Phase 5: Checking the Pull RequestTo verify performance, you can build and run the benchmark:docker buildx build --platform linux/arm64 -t benchmark:arm64 . --load

docker run --rm benchmark:arm64
SIMD Matrix Operations Benchmark
================================
Running on Arm64 architecture with NEON optimizations
=== Matrix Multiplication Benchmark ===
Matrix size: 200x200
Time: 17 ms
Result sum: 1.98888e+08
A very important thing to remember is that not all models will provide equal results, and while the Arm MCP Server provides deterministic context, the models themselves are stochastic. Always use a flagship latest-generation model to get the best results, and test any guesses the model makes regarding performance improvement.How Docker MCP Toolkit Changes DevelopmentDocker MCP Toolkit changes how developers interact with specialized knowledge and capabilities. Rather than learning new tools, installing dependencies, or managing credentials, developers connect their AI assistant once and immediately access containerized expertise.The benefits extend beyond Arm migration: – Same tools, same results across all developers – Containerized isolation prevents tool interference – MCP server versions tracked with application code – Migrations behave identically across environments – Docker MCP Catalog makes finding the right server straightforwardMost importantly, developers remain in their existing workflow. VS Code. GitHub Copilot. Git. No context switching to external tools or dashboards.You’ve just automated Arm64 migration using Docker MCP Toolkit, the Arm MCP Server, and GitHub Copilot. What used to require architecture expertise, manual intrinsic conversion, and hours of debugging now happens through natural conversation, safely executed in Docker containers.The future of migration isn’t manually porting every application. It’s having an AI assistant that can execute tasks across your entire stack securely, reproducibly, and at the speed of thought.]]></content:encoded></item><item><title>A Guide to Enterprise-Ready Systemd Configurations</title><link>https://blog.devops.dev/a-guide-to-enterprise-ready-systemd-configurations-10815acb4504?source=rss----33f8b2d9a328---4</link><author>Aditya Jambhalikar</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:16:59 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[In Linux, a  is a program that runs in the background to perform a specific task continuously or on demand.If you have a script/program which you want to run in background continuously or based on some conditions, and many other options like security, logging, resource limits,  is the answer is the init system and service manager used by most modern Linux distributionsDefined by a  with .service extensionThe path recommended to use is /etc/systemd/system/ or $HOME/.config/systemd/user for system-wide or user-specific serviceThe systemd service unit file typically has 3 sections, [Unit], [Service], [Install] Out of these the absolutely mandatory is [Service] section.Each section has a directive, so the bare-minimum directive in [Service] is ExecStart=Example of smallest possible valid service file[Service]ExecStart=/path/to/executable/fileLet’s try an example, create a simple bash script minimal-service.shcd /tmpvim minimal-service.shPaste the following code and save file esc and :w and enter#!/bin/bashwhile true; do  echo "Hello from minimal service at $(date)"  sleep 10sudo chmod +x minimal-service.shTest the execution first, it should show output like./minimal-service.shHello from minimal service at Thu Jan 22 12:44:54 PM IST 2026Hello from minimal service at Thu Jan 22 12:45:04 PM IST 2026Hello from minimal service at Thu Jan 22 12:45:14 PM IST 2026Now let’s create a service to execute this file so that it can run in backgroundCreate a file /etc/systemd/system/minimal.service with content[Service]ExecStart=/tmp/minimal-service.shThis is the system-wide service, if you want to create a user-based service then use the path $HOME/.config/systemd/user and use --user option with systemctlNow, we have a service created with no name, no boot start and no restart.Check the status and you find it to be inactive but loaded# check service statussudo systemctl status minimal.service      Loaded: loaded (/etc/systemd/system/minimal.service; static)Let’s start the service manually and check the status again# start the servicesudo systemctl start minimal.servicesudo systemctl status minimal.service     Loaded: loaded (/etc/systemd/system/minimal.service; static)     Active: active (running) since Thu 2026-01-22 12:52:35 IST; 3s ago   Main PID: 3466648 (minimal-service)     Memory: 800.0K     CGroup: /system.slice/minimal.service             ├─3466648 /bin/bash /tmp/minimal-service.sh             └─3466656 sleep 10Jan 22 12:52:35 blr-lhvzyh systemd[1]: Started minimal.service.Jan 22 12:52:35 blr-lhvzyh minimal-service.sh[3466648]: Hello from minimal service at Thu Jan 22 12:52:35 PM IST 2026journalctl -u minimal.serviceJan 22 12:52:35 localhost systemd[1]: Started minimal.service.Jan 22 12:52:35 localhost minimal-service.sh[3466648]: Hello from minimal service at Thu Jan 22 12:52:35 PM IST 2026Jan 22 12:52:45 localhost minimal-service.sh[3466648]: Hello from minimal service at Thu Jan 22 12:52:45 PM IST 2026## Stop the servicesudo systemctl stop minimal.serviceNow there are some obvious problems with this service:It has no name/description to identify what it does!!It doesn’t not start automatically, so after a reboot someone needs to start it manually.If something kills the process, it will not start automatically (check by running pkill -f minimal-service.shTo address these issues, let’s modify our /etc/systemd/system/minimal.service file as[Unit]Description=Service to print current time with 10s sleepExecStart=/tmp/minimal-service.shRestartSec=5WantedBy=multi-user.targetAfter this, first enable the servicesudo systemctl enable minimal.serviceThe WantedBy directive and enable command will now start the service on bootThe Restart and RestartSec directives will detect the crash of service and restarts after 5 sec automatically.Let’s check the execution nowsudo systemctl enable minimal.serviceCreated symlink /etc/systemd/system/multi-user.target.wants/minimal.service → /etc/systemd/system/minimal.service.sudo systemctl status minimal.service● minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2026-01-22 13:04:43 IST; 1s ago   Main PID: 3474393 (minimal-service)     Memory: 784.0K     CGroup: /system.slice/minimal.service             ├─3474393 /bin/bash /tmp/minimal-service.sh             └─3474396 sleep 10Jan 22 13:04:43 localhost systemd[1]: Started Service to print current time with 10s sleep.Jan 22 13:04:43 localhost minimal-service.sh[3474393]: Hello from minimal service at Thu Jan 22 01:04:43 PM IST 2026sudo pkill -f minimal-service.sh# Check the service statussudo systemctl status minimal.service● minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: activating (auto-restart) since Thu 2026-01-22 13:10:53 IST; 1s ago    Process: 3477811 ExecStart=/tmp/minimal-service.sh (code=killed, signal=TERM)   Main PID: 3477811 (code=killed, signal=TERM)        CPU: 15ms# Notice that the status is now activatingsudo systemctl status minimal.service● minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2026-01-22 13:10:58 IST; 2s ago   Main PID: 3478895 (minimal-service)     Memory: 804.0K     CGroup: /system.slice/minimal.service             ├─3478895 /bin/bash /tmp/minimal-service.sh             └─3478897 sleep 10Jan 22 13:10:58 blr-lhvzyh systemd[1]: Started Service to print current time with 10s sleep.Jan 22 13:10:58 blr-lhvzyh minimal-service.sh[3478895]: Hello from minimal service at Thu Jan 22 01:10:58 PM IST 2026As we can see, the service was restarted automatically after killing it. Now, we wanted the service to restart automatically, but it’s not desirable that it restarts even after we are manually killing it. So instead of restarting always, let’s replace the directive to restart on failure.[Unit]Description=Service to print current time with 10s sleepExecStart=/tmp/minimal-service.shRestartSec=5WantedBy=multi-user.targetThis directive, by default, does not restart on termination, exit 0 and systemctl stopThe other options for Restart= directives are which are pretty self explanatoryRestart=no            # defaultRestart=alwaysRestart=on-failureRestart=on-abortThe crash-only restart on-failure is widely used for web apps, workers, api servers. It doesn’t restart on SIGTERM or pkillLet’s test it out. Assuming the service file is modified, first reload the daemon, restart the service, kill the service, check statussudo systemctl daemon-reloadsudo systemctl restart minimal.servicesudo systemctl status minimal.service● minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2026-01-22 13:25:42 IST; 3s ago   Main PID: 3487200 (minimal-service)      Tasks: 2 (limit: 38331)        CPU: 6ms     CGroup: /system.slice/minimal.service             ├─3487200 /bin/bash /tmp/minimal-service.shJan 22 13:25:42 localhost systemd[1]: Started Service to print current time with 10s sleep.Jan 22 13:25:42 localhost minimal-service.sh[3487200]: Hello from minimal service at Thu Jan 22 01:25:42 PM IST 2026sudo pkill -f minimal.servicesudo systemctl status minimal.service○ minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: inactive (dead) since Thu 2026-01-22 13:25:56 IST; 1s ago    Process: 3487200 ExecStart=/tmp/minimal-service.sh (code=killed, signal=TERM)   Main PID: 3487200 (code=killed, signal=TERM)        CPU: 10msJan 22 13:25:42 localhost systemd[1]: Started Service to print current time with 10s sleep.Jan 22 13:25:42 localhost minimal-service.sh[3487200]: Hello from minimal service at Thu Jan 22 01:25:42 PM IST 2026Jan 22 13:25:52 localhost minimal-service.sh[3487200]: Hello from minimal service at Thu Jan 22 01:25:52 PM IST 2026Jan 22 13:25:56 localhost systemd[1]: minimal.service: Deactivated successfully.We can see the difference Active: inactive (dead) as opposed to Active: activatingSystem is not trying to attempt to restart it. Now, start the service manually and kill it using pkill -9 minimal.service# Start the service sudo systemctl start minimal.service# Kill it instead of terminatesudo pkill -9 minimal.servicesudo systemctl status minimal.service● minimal.service - Service to print current time with 10s sleep     Loaded: loaded (/etc/systemd/system/minimal.service; enabled; vendor preset: enabled)     Active: activating (auto-restart) (Result: signal) since Thu 2026-01-22 13:31:26 IST; 1s ago    Process: 3489628 ExecStart=/tmp/minimal-service.sh (code=killed, signal=KILL)   Main PID: 3489628 (code=killed, signal=KILL)Since the service is killed instead of terminated, system now attempts to restart it.We can have other parameters as well, to reduce the restarts e.g. Max 3 restarts in 5 minsRestart=on-failureRestartSec=10sStartLimitIntervalSec=300StartLimitBurst=3In most cases, an application will have it’s own user since running service as a root when it’s not needed is very risky. We just need to specify the User and Group in [Service] section[Service]ExecStart=/tmp/minimal-service.shUser=myuserWorking directory and Environment setupWe can setup the working directory and then use rest of the files as a relative path. Also we can define the environmental variables that the executable may neede.g. There’a python app which requires some env vars, we can either define it in same Unit file or pass it as fileBy using EnvironmentFile we can securely pass the credentials, just make sure to have proper restrictive permissions on this file[Service]Environment=ENV=productionWorkingDir=/tmpExecStart=/usr/bin/python3 myapp.pyEnvironmentFile=/etc/myapp/.envThe basic log viewing can be done byjournalctl -u minimal.servicejournalctl -u minimal.service -f # live monitoringjournalctl -u minimal.service --since "1 hour ago"journalctl -u minimal.service --since "2025-12-01 10:00:00"Additionally, we can define a logging location in service file[Service]StandardOutput=append:/var/log/myapp/stdout.logStandardError=append:/var/log/myapp/error.logControlling Startup and ShutdownOne of systemd’s important featuresis that we can control  a service starts and  it stops, relative to other services.For controlling the order, we have After= and Before= directives. We can specify any service to these directives which will set the order on when our service will startOne important point to note is that order ≠ dependency. If the service defined in After= fails, our service still may start.A typical example is, our service should start after network is up, so for that we can define[Unit]After=network.targetTo define the dependency, we have the directives Requires= and Wants=[Unit]Requires=network.targetIn this case, if network.target fails to start, our service will also not startThis is strong dependencyOn the other hand, Wants= is weak dependency, meaning the other service will be attempted to start, but if it doesn’t, our service will still runUnderstanding real world service filesLet’s take an example of sshd service file.cat /etc/systemd/system/sshd.service [Unit]Description=OpenBSD Secure Shell serverDocumentation=man:sshd(8) man:sshd_config(5)After=network.target auditd.serviceConditionPathExists=!/etc/ssh/sshd_not_to_be_runEnvironmentFile=-/etc/default/sshExecStartPre=/usr/sbin/sshd -tExecStart=/usr/sbin/sshd -D $SSHD_OPTSExecReload=/usr/sbin/sshd -tExecReload=/bin/kill -HUP $MAINPIDRestart=on-failureRestartPreventExitStatus=255Type=notifyRuntimeDirectoryMode=0755WantedBy=multi-user.targetAfter= Start the service after network interface is initializedConditionPathExists= If the file sshd_not_to_be_run exists, systemd will gracefully skip starting the service.[Service] section defines:EnvironmentFile= but by using - means ignore if the file doesn’t existExecStartPre= A command to execute before running the main executable. In this context, the system is testing the syntax of sshd_configKillMode=process The systemd only kills the main "listener" daemon and doesn't abruptly terminate the existing, active SSH sessions you are currently using.Let’s create our own service fileOne major use case of custom service file is the Flask app hosted over production-grade WSGI server like .Let’s start with creating the small application1. Prepare the environmentAssuming you have a non-root user for the application, e.g. myuser# 1. Create the app directorysudo mkdir -p /var/log/flask-appsudo chown myuser:myuser /var/log/flask-appsudo mkdir -p /opt/flask-app# 2. Create a virtual environmentsudo python3 -m venv venv# 3. Activate and install dependenciessource venv/bin/activatesudo pip install flask gunicorn2. Create the applicationfrom flask import Flask, jsonifyimport osapp = Flask(__name__)app.config['SECRET_KEY'] = os.getenv('SECRET_KEY')@app.route('/')def hello():    # We fetch an environment variable to prove our 'secrets' file works later    app_name = os.getenv('APP_NAME', 'Standard Flask App')        "status": "success",        "message": f"Hello from {app_name}!",        "version": "1.0.0"if __name__ == "__main__":    app.run()sudo chown myuser:myuser/opt/flask-app3. Create an env var fileAPP_NAME="My Flask app with systemd service"SECRET_KEY="some-random-string"/etc/systemd/system/flask-app.service[Unit]Description=Flask App served over gunicornUser=www-dataWorkingDirectory=/opt/flask-appEnvironmentFile=/etc/flask-app.envExecStart=/opt/flask-app/venv/bin/gunicorn \\    --bind 0.0.0.0:8000 \\    --access-logfile /var/log/flask-app/access.log \\    --error-logfile /var/log/flask-app/error.log \\    --log-level info \\ExecReload=/bin/kill -s HUP $MAINPIDTimeoutStopSec=30sRestartSec=5WantedBy=multi-user.targetIn a production environment, you don’t want to stop and start your service every time you change a config or update code—that causes downtime.So with this option, When you run sudo systemctl reload flask-app, systemd sends a  signal to the Gunicorn master process.Gunicorn spawns new workers with the new code and kills the old ones only  they finish their current request.By default, systemd tries to kill everything in the “Control Group” at once.KillMode=mixedSystemd sends a SIGTERM to the main process. If there are child processes (workers) still hanging around after the timeout, it sends a SIGKILL only to them. This prevents a single stuck request from hanging the entire OS shutdown.If your Flask app is in the middle of a heavy database export when you try to stop the service, you don’t want it to be killed instantly, potentially corrupting data. This directive gives your app a 30-second window to finish its work and close database connections cleanly.sudo systemctl start flask-appsudo systemctl status flask-appThere could be some permission errors based on how you have setup the directories and userBut a working output should look like# Check the statussudo systemctl status flask-app● flask-app.service - Flask App served over gunicorn     Loaded: loaded (/etc/systemd/system/flask-app.service; disabled; vendor preset: enabled)     Active: active (running) since Thu 2026-01-22 17:36:42 IST; 3s ago   Main PID: 3608147 (gunicorn)     Memory: 59.2M     CGroup: /system.slice/flask-app.service             ├─3608147 /opt/flask-app/venv/bin/python3 /opt/flask-app/venv/bin/gunicorn --workers 3 --bind 0.0.0.0:8000 --access-logfile /var/log/flask-app/access.log -->             ├─3608222 /opt/flask-app/venv/bin/python3 /opt/flask-app/venv/bin/gunicorn --workers 3 --bind 0.0.0.0:8000 --access-logfile /var/log/flask-app/access.log -->             ├─3608257 /opt/flask-app/venv/bin/python3 /opt/flask-app/venv/bin/gunicorn --workers 3 --bind 0.0.0.0:8000 --access-logfile /var/log/flask-app/access.log -->             └─3608258 /opt/flask-app/venv/bin/python3 /opt/flask-app/venv/bin/gunicorn --workers 3 --bind 0.0.0.0:8000 --access-logfile /var/log/flask-app/access.log -->Jan 22 17:36:42 myhost systemd[1]: Started Flask App served over gunicorn.curl http://localhost:8000{"message":"Hello from My Flask app with systemd service!","status":"success","version":"1.0.0"}cat /var/log/flask-app/access.log127.0.0.1 - - [22/Jan/2026:17:37:16 +0530] "GET / HTTP/1.1" 200 97 "-" "curl/7.81.0"You can harden the system more by adding[Service]# other directives# Makes the senstive dirs read only like /etc /usr /boot etcProtectSystem=full # Gives the service its own private /tmp and /var/tmp, it can't read other areasPrivateTmp=true]]></content:encoded></item><item><title>KodeKloud AWS Challenge — Day 26: Creating a Public VPC with a Public Subnet and EC2 Access</title><link>https://blog.devops.dev/kodekloud-aws-challenge-day-26-creating-a-public-vpc-with-a-public-subnet-and-ec2-access-30dabfac44c7?source=rss----33f8b2d9a328---4</link><author>Kishor Bhairat</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:16:56 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[KodeKloud AWS Challenge — Day 27: Creating a Public VPC with a Public Subnet and EC2 AccessA hands-on setup of a public VPC architecture enabling internet-facing EC2 workloads with proper subnet design and SSH access.Day 27 of the KodeKloud AWS Challenge focused on building foundational networking infrastructure in AWS: a  designed to host internet-accessible services.This task is where cloud networking becomes real. If you don’t understand VPCs, subnets, and routing, everything above them breaks sooner or later.The objective was straightforward but non-negotiable for production readiness:Configure a public subnet with automatic public IP assignmentLaunch an EC2 instance inside itEnable secure SSH access from the internetConcept Explanation: Public VPC and Public SubnetA VPC (Virtual Private Cloud) is a logically isolated network in AWS where you control:A  is defined by one key property:Additionally, for EC2 instances to be reachable:The subnet must allow The instance security group must allow inbound trafficIn Amazon Web Services, this pattern is used for:Public access is intentional — not accidental.Why This Matters in Real EnvironmentsMisconfigured networking is one of the top causes of broken deployments.Proper security group rulesYour application is effectively invisible — even if it’s “running.”This setup enables teams to:Deploy public-facing services safelyMaintain clear separation between public and private workloadsScale infrastructure without redesigning the network laterCommon mistakes engineers make:Forgetting to attach an  to the VPCNot enabling auto-assign public IP on the subnetOpening SSH to the world without understanding the riskConfusing public IPs with Elastic IPsAssuming “public subnet” is a checkbox (it’s not)Public access is a result of , not naming.Hands-On Task: What I Did1️⃣ Created the Public VPCCreated a VPC named nautilus-pub-vpcUsed an appropriate CIDR blockAttached an  to the VPCThis established internet connectivity at the VPC level.2️⃣ Created the Public SubnetCreated a subnet named nautilus-pub-subnet within the VPCEnabled auto-assign public IPv4 addressAssociated the subnet with a route table containing:0.0.0.0/0 → Internet GatewayThis ensured resources launched here are internet-reachable.3️⃣ Launched the EC2 InstanceCreated an EC2 instance named nautilus-pub-ec2Launched it inside nautilus-pub-subnetVerified that it received a public IP automatically4️⃣ Configured SSH AccessCreated or updated a security groupAllowed inbound  from the internetAttached the security group to nautilus-pub-ec2The instance was now accessible over the internet via SSH.What I Learned / Key TakeawaysPublic access is defined by routing, not intentAuto-assign public IP is critical for public subnetsInternet Gateway is mandatory for outbound/inbound trafficSecurity groups are the last line of defenseNetwork design should be deliberate from day oneNetworking mistakes don’t fail loudly — they just block everything.Creating a public VPC with a public subnet is a baseline cloud networking skill.This challenge reinforced a core principle:If you don’t control the network, you don’t control the system.With this setup, the Nautilus DevOps Team now has a clean, scalable foundation for hosting public-facing applications in AWS.On to the next challenge.If you’re learning AWS or DevOps through hands-on infrastructure design, follow the journey.Each challenge focuses on building systems the way real teams do.]]></content:encoded></item><item><title>Chrome Extension Development: The Complete System Architecture Guide for 2026</title><link>https://blog.devops.dev/chrome-extension-development-the-complete-system-architecture-guide-for-2026-9ae81415f93e?source=rss----33f8b2d9a328---4</link><author>JIN</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:16:50 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Revolutionizing Kubernetes Scaling: A Deep Dive into Karpenter</title><link>https://blog.devops.dev/revolutionizing-kubernetes-scaling-a-deep-dive-into-karpenter-04c688349f62?source=rss----33f8b2d9a328---4</link><author>Anish Kumar</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:16:46 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Kubernetes has become the standard platform for orchestrating containerized applications. It offers powerful abstractions for deploying, scaling and managing workloads. However, the compute layer beneath it often remains difficult to optimize. Managing the worker nodes that provide actual capacity can be challenging, when clusters experience rapid or unpredictable changes in demand.Traditionally, teams relied on the Kubernetes  to add or remove capacity. This tool provided basic automation, but it also introduced limitations that will be explored in the next section. was created to provide a more flexible and efficient way to manage cluster capacity. It is an open source and high performance provisioning engine that responds directly to pods that cannot be scheduled. Instead of depending on predefined cloud node groups, Karpenter evaluates the specific needs of pending pods and launches compute resources that match those requirements. This model allows clusters to scale quickly and cost effectively while meeting the unique constraints of each workload.In the upcoming sections, we will look at the key building blocks of Karpenter such as NodeClasses, NodePools and NodeClaims. Also, we will explore in detail how Karpenter schedules new capacity and manages node lifecycles within a Kubernetes environment.Advantages of Karpenter over Cluster AutoscalerThe Kubernetes Cluster Autoscaler has served as the traditional tool for adjusting cluster capacity, but its design is closely tied to cloud provider node groups and it becomes difficult to manage diverse workloads. Karpenter introduces a more flexible and responsive model for provisioning compute resources that offers below advantages.Direct and faster provisioning: Cluster Autoscaler must work through predefined node groups that adds delay to the provisioning process. Karpenter observes pending pods directly and creates new nodes based on the exact resource requirements. This results in significantly faster reaction times and more efficient cluster scaling during workload spikes.Greater flexibility in instance selection: Cluster Autoscaler can only scale the instance types defined in each node group. Karpenter can choose from the full range of cloud provider instance types and select the best option at the moment of provisioning. This gives the scheduler more freedom to match compute resources to workload needs such as memory optimized, compute optimized or specialized architectures.Simplified infrastructure management: Karpenter removes the need for large numbers of node groups and instead uses a declarative and constraint based approach. This simplifies cluster configuration and reduces the burden of maintaining cloud specific autoscaling constructs.Improved cost efficiency: Karpenter can reduce wasted capacity by selecting the most suitable instance type in real time. It can consider options such as burstable instances or spot capacity when appropriate.Better support for dynamic workloads: Modern applications often require rapid changes in compute capacity. Karpenter is designed to respond quickly and adaptively that makes it a strong fit for environments with unpredictable traffic, batch processing, event driven systems and high scale workloads.NodeClasses: Cloud-Specific ConfigurationA  represents the cloud specific configuration that is applied to the nodes Karpenter creates. While Kubernetes focuses on abstract concepts such as pods and nodes, a NodeClass connects those nodes to real infrastructure in your cloud provider account. It captures details such as subnet, security group, instance profile, tags, labels and other settings that are required to launch instances correctly and securely. NodePools and NodeClaims reference a NodeClass so that scheduling logic can remain focused on workload intent, while the NodeClass handles provider details.Some important attributes you typically find in a provider specific NodeClass (for example  on ) include:Specifies the AMI family to use when provisioning nodes. Determines the machine image and base configuration applied at launch.Defines the IAM instance profile that the node will assume in order to access cloud APIs, fetch images, write logs and perform required operations.spec.subnetSelectorTerms: Selects which subnets the nodes can be launched into based on matching labels or criteria. This controls node placement, routing behaviour and overall network topology.spec.securityGroupSelectorTerms: Determines which security groups are attached to the nodes when they are created. This governs firewall rules, network access and communication boundaries for the nodes.Provides key value tags applied to provisioned instances and associated resources. Supports cost tracking, ownership metadata and automation workflows.spec.blockDeviceMappings: Describes storage settings such as root volume type, size and encryption parameters applied to new nodes.Optionally supplies bootstrap instructions or startup configuration that runs when a node initializes such as cluster join scripts or agent setup.The following is an example of an AWS specific NodeClass using the  resource. This is only an illustration, but it shows the typical structure you would define before wiring it with a NodePool:apiVersion: karpenter.k8s.aws/v1beta1kind: EC2NodeClass  name: default-ec2-nodeclass  amiFamily: AL2  role: "karpenter-node-role"  subnetSelectorTerms:      Environment: "prod"  securityGroupSelectorTerms:      KubernetesCluster: "my-eks-cluster"    Owner: "platform-team"  blockDeviceMappings:    ebs:      volumeType: gp3  detailedMonitoring: trueNodePools: Defining Node ConstraintsA  in Karpenter describes the intent for how new nodes should look and behave from the point of view of Kubernetes workloads. It focuses on scheduling constraints and policies. It tells Karpenter what kinds of nodes are allowed for a group of pods including labels, taints, instance type preferences and zone choices. This separation allows platform teams to encode infrastructure standards in NodeClasses and application or capacity strategies in NodePools.When pods are pending and cannot be scheduled on existing nodes, Karpenter checks which NodePools satisfy the pod requirements and then chooses a suitable combination of NodePool and NodeClass to create new nodes. Additionally, NodePools define behaviour such as consolidation, node expiration and limits that control how the cluster grows and shrinks over time.These are some important attributes of the  custom resource:spec.template.metadata.labels: Default labels that will be applied to nodes created from this pool. These help select or identify nodes for specific workloads.spec.template.spec.nodeClassRef: A reference to the NodeClass that provides cloud provider configuration for the nodes in this pool.spec.template.spec.requirements: A list of scheduling requirements that restrict which instance families, sizes, architectures or zones are valid. These are expressed using standard Kubernetes style key, operator and values fields.Settings that control how Karpenter performs consolidation, replacement or expiration. For example, you can choose when to remove underutilized nodes or how long nodes should live.Optional soft limits on total resources. It is used to keep the overall capacity of a pool within a desired range.The following is an example  that uses the from the previous section and defines some common constraints and behaviours:apiVersion: karpenter.sh/v1beta1kind: NodePool  name: general-purpose  template:      labels:    spec:        name: default-ec2-nodeclass      - key: "karpenter.k8s.aws/instance-family"        values: ["m5", "m6i"]      - key: "karpenter.k8s.aws/instance-size"        operator: In        values: ["large", "xlarge", "2xlarge"]      - key: "topology.kubernetes.io/zone"        values: ["us-east-1a", "us-east-1b"]    consolidationPolicy: WhenUnderutilized  limits:    memory: 1000GiNodeClaims: The Node Request LifecycleA  represents the lifecycle of a single node request in Karpenter. While NodePools describe the constraints for groups of nodes and NodeClasses define the cloud provider configuration, a NodeClaim is the actual request for a new node. Each NodeClaim corresponds to one physical or virtual machine and acts as the bridge between Kubernetes and the cloud provider. It captures the chosen instance type, zone, labels, capacity and the current state of node provisioning.NodeClaims allow Karpenter to track the entire lifespan of a node from the moment pods cannot be scheduled through provisioning and registration until the node becomes ready. Since NodeClaims are created automatically, users rarely need to write or modify them directly but they are extremely valuable for understanding scheduling decisions, debugging scaling events or verifying cloud provider configuration.How NodeClaims Are Created InternallyThe lifecycle begins when Karpenter detects pods stuck in a Pending state. The controller continuously watches the Kubernetes API server and identifies pods that cannot be scheduled due to insufficient resources. Once detected, Karpenter runs its bin packing algorithm to evaluate pod requirements. It considers CPU, memory, volume requirements, taints, labels and all NodePool constraints. It selects the smallest and most cost efficient node shape that satisfies all conditions.After determining the ideal configuration, Karpenter creates a  object. The NodeClaim contains fields that define the instance type, zone and references to the NodeClass that holds cloud provider settings. The controller reads the NodeClaim and issues cloud provider API calls such as ec2 RunInstances for AWS. The instance boots, joins the cluster as a Kubernetes Node and the NodeClaim progresses through states such as launching, registering and ready. Once the node reports , Karpenter schedules the pending pods onto it.The following is a simplified example of a NodeClaim object that Karpenter generates:apiVersion: karpenter.sh/v1kind: NodeClaim  name: default-sfpsl    karpenter.sh/nodepool: default    topology.kubernetes.io/zone: us-east-1a    node.kubernetes.io/instance-type: m5.large  nodeClassRef:    name: default-ec2-nodeclass  # Other internal provisioning details  providerID: aws:///us-east-1a/i-0xxxxxxxxxxxxxxxx  nodeName: ip-10-0-12-34.us-east-1.compute.internal  capacity:    memory: 7800Mi  conditions:    status: "True"Karpenter’s Scheduling WorkflowKarpenter’s scheduling workflow is proactive and works alongside the default Kubernetes scheduler. Instead of waiting for predefined node groups to scale, Karpenter analyzes real time pod demand and provisions the most suitable node based on workload and infrastructure constraints.The following steps describe the Karpenter’s scheduling workflow:A new pod is created by a Deployment or other controller but the Kubernetes scheduler cannot place it on any existing node because there is not enough available capacity. The pod enters a Pending state.The Karpenter controller continuously watches the Kubernetes API server and detects the unschedulable pod.Karpenter gathers all scheduling constraints from the pending pod that gives Karpenter a complete picture of the pod’s needs. - Resource requests such as CPU, memory and GPU. - Node affinity rules. - Tolerations for taints applied by NodePools. - Topology spread constraints. - Volume requirements if applicable.Karpenter checks all available NodePools and determines which ones can satisfy the aggregated pod constraints. Only NodePools that match all requirements proceed to the next step.Bin packing and optimal selection: For the compatible NodePools, Karpenter performs a bin packing simulation: - It groups batches of pending pods. - It evaluates all valid instance types and zones allowed by the NodePool. - It chooses the smallest and most cost efficient instance shape that satisfies all constraints.NodeClaim creation and cloud launch: After selecting the best node configuration, Karpenter creates a NodeClaim. The NodeClaim includes: - Chosen instance type. - Selected availability zone. - Reference to the NodeClass for provider configuration.Karpenter then calls the cloud provider API to launch the actual instance. The instance starts, bootstraps, and joins the cluster as a Kubernetes Node.Once the new node becomes Ready, the Kubernetes scheduler places the previously pending pods onto it. The scaling cycle completes and applications resume normal operation.Karpenter’s Disruption WorkflowDisruption in Karpenter ensures that existing nodes are gracefully terminated and replaced in order to maintain efficiency, security and cost effectiveness within the cluster. While scaling brings new nodes into the system, disruption handles the responsible removal of nodes that are no longer needed or no longer compliant with cluster configuration. This workflow allows clusters to remain optimized over time without manual intervention. Disruptions are performed safely and strategically that always respects workload availability and Pod Disruption Budgets.Automated Disruption MethodsNodes are disrupted when their current configuration no longer matches the NodeClass/EC2NodeClass. This includes differences in AMI, user data, tags or other provider settings. Drift ensures compliance with infrastructure as code updates.Karpenter inspects cluster utilization and identifies opportunities to reduce costs.: Removes nodes that have no workload pods.: Attempts to pack the node’s pods onto other nodes or replace the node with a smaller or cheaper equivalent instance.Nodes are replaced after the time specified in spec.template.spec.expireAfter of the NodePool. This ensures regular node recycling for security patches, kernel updates and base image changes.Nodes are disrupted when a cloud provider issues events such as spot interruptions, maintenance events or hardware failures. Karpenter detects these signals and proactively replaces the affected nodes to maintain application availability.The Disruption Controller identifies a node that meets criteria for disruption such as expiration or drift.Karpenter applies the taint karpenter.sh/disrupted:NoSchedule to the node. This prevents any new pods from landing on the node during the disruption workflow.Simulation and Replacement: Karpenter performs a scheduling simulation to verify that all pods on the node can be safely rescheduled. This simulation ensures: - Ensures Pod Disruption Budgets (PDBs) are respected. - Determines whether existing nodes can host the pods. - Decides if a replacement node must be created via a new NodeClaim. If the simulation fails, disruption is postponed.Draining and Termination: If simulation succeeds, Karpenter drains the node by evicting its pods. After draining the node: - Karpenter issues a cloud provider API call (such as EC2 terminate). - The NodeClaim finalizer ensures that cloud resources are fully cleaned up.- The node is removed from the cluster.This section ties together all previous concepts by walking through a practical end to end setup of Karpenter on Amazon EKS. By the end, you will understand how Karpenter installation, NodeClasses, NodePools, and NodeClaims come together to provision right sized compute for real workloads in response to demand.Installation and IAM SetupKarpenter relies on AWS IAM permissions to launch, manage and terminate EC2 instances. These permissions are assigned through IRSA (IAM Roles for Service Accounts), ensuring that the Karpenter controller can securely authenticate with AWS.Karpenter needs to launch, describe and terminate instances and query networking resources: - ec2:RunInstances - ec2:DescribeInstances - ec2:DescribeInstanceTypes - ec2:DescribeInstanceTypeOfferings - ec2:DescribeSubnets - ec2:DescribeSecurityGroups - ec2:DescribeLaunchTemplates - ec2:DescribeLaunchTemplateVersions - ec2:DescribeAvailabilityZones - ec2:DescribeSpotPriceHistory - ec2:DeleteTagsKarpenter itself should not assume the node role, but it must be allowed to attach it to new EC2 instances: - iam:PassRole - iam:ListInstanceProfilesTo choose the most cost effective instance types, Karpenter may query AWS pricing: - pricing:GetProductsSSM and SSM Parameter Store: If you rely on SSM parameters or use SSM to manage AMIs: - ssm:GetParameter - ssm:GetParametersByPathEKS (cluster level information): Karpenter sometimes needs to query details about the EKS cluster: - eks:DescribeClusterInstall Karpenter via HelmOnce IAM and IRSA are configured, install the controller:helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \  --namespace karpenter --create-namespace \  --set settings.clusterName=<your-cluster-name> \  --set settings.clusterEndpoint=<your-cluster-endpoint> \  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=<KarpenterRoleARN>This deploys Karpenter and gives it the necessary permissions to manage node provisioning in your EKS cluster.Define NodeClass and NodePoolNodeClass: Cloud Provider ConfigurationThis is a simplified EC2NodeClass example that incorporates concepts explained earlier such as subnet selection, security groups, AMI family, tags:apiVersion: karpenter.k8s.aws/v1beta1kind: EC2NodeClass  name: default-ec2-nodeclass  amiFamily: AL2  role: "karpenter-node-role"  subnetSelectorTerms:        Environment: prod  securityGroupSelectorTerms:    - tags:        KubernetesCluster: my-eks-cluster  tags:    CostCenter: kubernetesNodePool: Workload Constraints and BehaviourNext, create the NodePool that describes how nodes should be provisioned:apiVersion: karpenter.sh/v1beta1kind: NodePool  name: default  template:      labels:    spec:        name: default-ec2-nodeclass        - key: "karpenter.k8s.aws/instance-family"          values: ["m5", "m6i"]Running a Deployment that Triggers ScalingImagine deploying an application with a very high memory requirement. None of your existing nodes can fit these pods. So this is where Karpenter demonstrates its intelligent autoscaling capability.apiVersion: apps/v1kind: Deployment  name: high-mem-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: app-container        image: public.ecr.aws/eks-distro/kubernetes/pause:3.2        resources:            memory: "8Gi"Each pod requests 8 GiB of memory. With 3 replicas, we need 24 GiB of memory in total. Within seconds, a correctly sized node is created by karpenter that fits all workload pods in the most cost effective way.Karpenter represents a major advancement in the way Kubernetes clusters manage and scale compute resources. It moves away from the older autoscaling approach that relies on predefined node groups and instead uses a workload aware provisioning model that responds directly to real time demand. This allows clusters to react faster and to launch nodes that match the exact needs of the workloads that leads to better performance and more efficient use of compute capacity.Throughout this guide, we explored the key building blocks that make this possible. The  resource provides the cloud provider settings that define how nodes are created. The  resource captures the scheduling intent and the constraints used to guide provisioning decisions. The  resource represents the lifecycle of each individual node request. These concepts allow Karpenter to make smart provisioning decisions, optimize scheduling and gracefully manage node replacement through its disruption process.For any organization running dynamic and modern cloud native applications that adopts Karpenter can lead to simpler operations, lower compute costs and a more responsive Kubernetes environment. As clusters continue to grow in size and complexity, a provisioning engine that can think in real time and react to actual workload needs becomes essential. Karpenter delivers exactly that and stands out as a powerful step forward for efficient and intelligent cluster management.If you find my work valuable and would like to support it, you are welcome to ]]></content:encoded></item><item><title>How ATM Withdrawals Work Across Banks: Inside the SBI to Axis to NPCI Flow</title><link>https://blog.devops.dev/how-atm-withdrawals-work-across-banks-inside-the-sbi-to-axis-to-npci-flow-df7a03b415d4?source=rss----33f8b2d9a328---4</link><author>Sanjay Singh</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:16:42 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[When I withdraw money from an Axis Bank ATM using my SBI account, how does Axis Bank get my account information from SBI? How is my SBI…]]></content:encoded></item><item><title>N. Korea Contagious Interview Campaign Turns to VS Code to Deliver Backdoor</title><link>https://devops.com/n-korea-contagious-interview-campaign-turns-to-vs-code-to-deliver-backdoor/</link><author>Jeff Burt</author><category>devops</category><pubDate>Fri, 23 Jan 2026 08:03:39 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Legit Security AI Tool Uses Threat Feed to Identify Risks to Software Supply Chain</title><link>https://devops.com/legit-security-ai-tool-uses-threat-feed-to-identify-risks-to-software-supply-chain/</link><author>Mike Vizard</author><category>devops</category><pubDate>Thu, 22 Jan 2026 19:57:25 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>