<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>DevOps</title><link>https://www.awesome-dev.news</link><description></description><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-25/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:34 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-24/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:33 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-23/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:27 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-22/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:26 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-21/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:23 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-20/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:19 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-19/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:18 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-18/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-17/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:11 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Key not found in data</title><link>https://devops.com/key-not-found-in-data-16/</link><author></author><category>devops</category><pubDate>Sat, 1 Nov 2025 15:32:10 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Story Of an App That Was Slow</title><link>https://blog.devops.dev/the-story-of-an-app-that-was-slow-3844f1af9250?source=rss----33f8b2d9a328---4</link><author>Amita Pal Singh</author><category>devops</category><pubDate>Sat, 1 Nov 2025 14:12:02 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Once upon a time, there was an app — , it had beautiful forms, users loved it. Soon hundreds of people started visiting it. But then it started struggling. With 1500 daily users… slow load times and outages started causing chaos!Bandwidth is the maximum amount of data that can pass through the network at any given time.Throughput is the average amount of data that actually passes through over a given period of time.It is measured in bps = bits per secondNetwork latency is measured in milliseconds by calculating the time interval between the initiation of a send operation from a source system and the completion of the matching receive operation by the target system.This measurement helps developers understand how quickly a webpage or application will load for users.A ping rate of less than 100ms is considered acceptable but latency in the range of 30–40ms is desirable.The amount of time it takes for a response to reach a client device after a client request. It is double the Latency, plus processing time at server.Expected Daily users = 1500Expected Peak traffic = 100–200 form submissions per hourRequired Bandwidth ≥300 Mbps for 200–300 RPS (~1 MB each).Throughput ≥250 Mbps (80% of Bandwidth) for smooth data flow.Required Latency <100ms for snappy form rendering.RTT <200ms for quick submissions.Upgrade Bandwidth to 500 Mbps, enough for 200–300 simultaneous 1 MB form submissionsUse QoS rules to prioritize form traffic and upgrade load balancersDeploy a CDN to cache form assets closer to users and optimize server codeOptimize Db Queries, Add IndexesWant to create your own networking adventure? Track Bandwidth, Latency, Throughput, and RTT. Use observability tools, scale smart, and aim for these metrics to keep your users happy!]]></content:encoded></item><item><title>Digest #186: Inside the AWS Outage, Docker Compose in Production, F1 Hacks and 86,000 npm Packages Attacks</title><link>https://www.devopsbulletin.com/p/digest-186-inside-the-aws-outage</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/youtube/w_728,c_limit/2p_huDMN8XI" length="" type=""/><pubDate>Sat, 1 Nov 2025 09:32:04 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!A recent 14-hour AWS us-east-1 outage took down 140 services after a DNS race condition in DynamoDB spiraled out of control. Palo Alto’s Unit42 uncovered a cloud-based gift card fraud campaign, and researchers exploited bugs in the FIA portal to access F1 driver data. Meanwhile, npm faced another supply-chain attack, with over 86,000 malicious packages downloaded.Cloudflare detailed how it’s escaping the Linux networking stack, AWS quietly deprecated two dozen services, and Netflix revealed how Tudum supports 20M+ users using CQRS.On the hands-on side: Docker Compose in production, ArgoCD for multi-cluster deployments, detecting bad images in S3 with Rekognition, and TDD with Terraform. Plus, why for some workloads, Postgres can beat Kafka.Tools of the week: WhoDB (chat-based DB explorer), LME (CISA’s free SIEM), Grype (vulnerability scanner), Kanchi (Celery monitor), Bruin (data pipeline), and Nyno (multi-language workflow engine).All this and more in this week’s DevOps Bulletin, don’t miss out! Consider supporting it with a paid subscription. You’ll keep the free Friday issues  get extras like bonus deep-dives, templates, and the full archive.Ever feel like your cloud bill keeps growing, but you’re not sure  the money’s going? Start with an .Listing all your resources — EC2 instances, S3 buckets, Lambdas, and more — often reveals idle or forgotten assets quietly adding to your bill. You can script it yourself with the AWS CLI or use tools like AWS Config or CloudQuery for a more automated setup.If you want more hands-on tips like this, check out my latest book, “.A lightweight  - Postgres, MySQL, SQLite, MongoDB, Redis, MariaDB, Elastic Search, and Clickhouse with Chat interface. is a no-cost, open-source platform that centralizes log collection, enhances threat detection, and enables real-time alerting. is a vulnerability scanner for container images and filesystems. is a real-time Celery task monitoring (and management) system with an enjoyable user interface. is a data pipeline tool that brings together data ingestion, data transformation with SQL & Python, and data quality into a single framework. is an open-source multi-language workflow engine that lets you build, extend, and connect automation in the languages you already know. If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email. ]]></content:encoded></item><item><title>How Data, Empathy and Visibility Are Redefining DevOps Maturity</title><link>https://devops.com/how-data-empathy-and-visibility-are-redefining-devops-maturity/</link><author>Alan Shimel</author><category>devops</category><pubDate>Fri, 31 Oct 2025 15:08:58 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Developer Discipline Matters More Than Ever in the AI Era</title><link>https://devops.com/why-developer-discipline-matters-more-than-ever-in-the-ai-era/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 31 Oct 2025 14:53:20 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup Guide</title><link>https://blog.devops.dev/top-5-open-source-tools-to-automate-database-deployments-in-ci-cd-pipelines-setup-guide-4703395bde4c?source=rss----33f8b2d9a328---4</link><author>Virinchi T</author><category>devops</category><pubDate>Fri, 31 Oct 2025 14:11:01 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup GuideDatabase migrations are critical for modern application development, yet they remain one of the riskiest aspects of the deployment process. Whether you’re upgrading legacy systems, moving to cloud environments, or maintaining consistency across development, testing, and production, reliable database migration tools integrated into your CI/CD pipeline are essential.In this comprehensive guide, we’ll explore the top 5 open-source database migration CI/CD tools, their key features, and step-by-step setup instructions to help you automate your database change management.Why Database Migration Tools MatterBefore diving into the tools, let’s understand why they’re crucial:: Track database changes just like application code: Ensure all environments have the same schema: Eliminate manual errors and reduce deployment time: Safely revert changes if something goes wrong: Maintain compliance with detailed change logs: Enable multiple developers to work on database changes simultaneously1. Flyway — The Simple Yet Powerful ChoiceFlyway is a lightweight, open-source database migration tool that has gained massive popularity due to its simplicity and reliability. Created by Redgate, it treats database migrations like code using simple SQL or Java-based migration scripts.Simple SQL-based migrations: Write migrations in plain SQL that are easy to understandSupport for 50+ databases: Including PostgreSQL, MySQL, Oracle, SQL Server, MariaDB, and moreVersion control integration: Seamlessly integrates with Git and other VCS: Support for views, stored procedures, and functions: Easy integration into existing databases: Prevents duplicate migrations: CLI, Maven/Gradle plugins, and API support: Minimal overhead and fast executionPostgreSQL, MySQL, MariaDB, Oracle, SQL Server, DB2, H2, SQLite, Redshift, CockroachDB, Firebird, and many more.Flyway uses a migration naming convention and maintains a flyway_schema_history table to track which migrations have been applied:Migration Naming Convention:V<Version>__<Description>.sqlV1__create_users_table.sqlV1.1__add_email_column.sql# Download and extractwget -qO- https://download.red-gate.com/maven/release/com/redgate/flyway/flyway-commandline/10.8.1/flyway-commandline-10.8.1-linux-x64.tar.gz | tar -xvz# Create symbolic linksudo ln -s `pwd`/flyway-10.8.1/flyway /usr/local/bindocker pull flyway/flyway<dependency>    <groupId>org.flywaydb</groupId>    <artifactId>flyway-core</artifactId>    <version>10.8.1</version>Step 2: Create Migration FilesCreate a directory structure:/project  /migrations    V2__add_tables.sqlExample migration file (V1__create_users_table.sql):CREATE TABLE users (    id INT PRIMARY KEY AUTO_INCREMENT,    username VARCHAR(255) NOT NULL,    email VARCHAR(255) NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);Create a flyway.conf file:flyway.url=jdbc:postgresql://localhost:5432/mydbflyway.user=dbuserflyway.password=dbpasswordflyway.locations=filesystem:./migrationsflyway.baselineOnMigrate=true# Check migration statusflyway info# Run migrationsflyway migrate# Rollback (requires Flyway Teams)flyway undoCreate .github/workflows/database-migration.yml:on:  push:jobs:  test:      postgres:        env:          POSTGRES_DB: ${{ secrets.DB_TEST }}          POSTGRES_USER: ${{ secrets.USER_TEST }}          POSTGRES_PASSWORD: ${{ secrets.PASSWORD_TEST }}        options: >-          --health-interval 10s          --health-retries 5      - uses: actions/checkout@v2      - name: Run Flyway Migrations        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_TEST }}          user: ${{ secrets.USER_TEST }}          password: ${{ secrets.PASSWORD_TEST }}          locations: filesystem:./migrations      - run: echo 'Testing complete'  deploy-to-prod:    needs: test      - uses: actions/checkout@v2      - name: Deploy to Production        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_PROD }}          user: ${{ secrets.USER_PROD }}          password: ${{ secrets.PASSWORD_PROD }}          locations: filesystem:./migrations      - run: echo 'Deployment complete'database_migration:  stage: migrate  script:    - flyway -url="jdbc:postgresql://$DB_HOST:5432/$DB_NAME"             -user=$DB_USER             -locations=filesystem:./migrations   only:pipeline {    agent any        FLYWAY_URL = credentials('flyway-url')        FLYWAY_USER = credentials('flyway-user')        FLYWAY_PASSWORD = credentials('flyway-password')        stage('Checkout') {                git 'https://github.com/yourorg/your-repo.git'        }        stage('Flyway Migration') {            steps {                    docker run --rm \                        -v $(pwd)/migrations:/flyway/sql \                        flyway/flyway \                        -user=${FLYWAY_USER} \                        -password=${FLYWAY_PASSWORD} \                        migrate            }    }Never modify applied migrations: Always create new migration files: Make migration purpose clear from filename: One logical change per migrationTest on non-production first: Always test migrations in stagingStore credentials securely: Use secrets management toolsVersion control everything: Include migrations in your repository2. Liquibase — The Enterprise-Grade SolutionLiquibase is arguably the most well-known database migration tool, having been in the market since 2006. It’s a Java-based CLI tool that provides sophisticated tracking of database changes through XML, YAML, JSON, or SQL scripts.Multiple change log formats: XML, YAML, JSON, and SQLSupport for 60+ databases: Vendor-independent solutionAdvanced rollback capabilities: Targeted rollback for specific changesets: Apply changes based on database stateDiff and sync capabilities: Compare schemas across environmentsAutomated drift detection: Identify untracked database changes: Enforce compliance and governanceExtensive CI/CD integrations: Jenkins, GitLab, GitHub Actions, Azure DevOps, and moreOracle, SQL Server, PostgreSQL, MySQL, MariaDB, MongoDB, Snowflake, DB2, Redshift, CockroachDB, and many others.Liquibase organizes changes as  within . It maintains a DATABASECHANGELOG table to track applied changes.: Master file listing all changesets: Unit of change with unique ID and author: Create table, add column, insert data, etc.# Download from liquibase.orgwget https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gztar -xzf liquibase-4.24.0.tar.gzsudo mv liquibase /usr/local/bin/<dependency>    <groupId>org.liquibase</groupId>    <artifactId>liquibase-core</artifactId>    <version>4.24.0</version><plugin>    <groupId>org.liquibase</groupId>    <artifactId>liquibase-maven-plugin</artifactId>    <version>4.24.0</version>Step 2: Create Changelog FilesCreate a master changelog (db.changelog-master.xml):<?xml version="1.0" encoding="UTF-8"?><databaseChangeLog    xmlns="http://www.liquibase.org/xml/ns/dbchangelog"    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"    xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd">    <include file="db/changelog/v1.0/create-users-table.xml"/>    <include file="db/changelog/v1.1/add-email-column.xml"/>Create individual changeset (create-users-table.xml):<?xml version="1.0" encoding="UTF-8"?><databaseChangeLog    xmlns="http://www.liquibase.org/xml/ns/dbchangelog"    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"    xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd">    <changeSet id="1" author="john.doe">        <createTable tableName="users">            <column name="id" type="int" autoIncrement="true">                <constraints primaryKey="true"/>            <column name="username" type="varchar(255)">                <constraints nullable="false"/>            </column>            <column name="email" type="varchar(255)">                <constraints nullable="false"/>            <column name="created_at" type="timestamp" defaultValueComputed="CURRENT_TIMESTAMP"/>    </changeSet>databaseChangeLog:  - changeSet:      author: john.doe        - createTable:            columns:                  name: id                  autoIncrement: true                    primaryKey: true                  name: username                  constraints:--liquibase formatted sql--changeset john.doe:1CREATE TABLE users (    id INT PRIMARY KEY AUTO_INCREMENT,    username VARCHAR(255) NOT NULL,    email VARCHAR(255) NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPCreate liquibase.properties:driver: org.postgresql.Driverurl: jdbc:postgresql://localhost:5432/mydbpassword: dbpasswordchangeLogFile: db.changelog-master.xmlliquibase.hub.mode: off# Check statusliquibase status# Update databaseliquibase update# Rollback last changeliquibase rollback-count 1# Rollback to specific tagliquibase rollback tagName# Generate documentationliquibase dbDoc ./output# Diff two databasesliquibase diffJenkins Pipeline with Dockerpipeline {    agent {            image 'liquibase/liquibase:4.24.0'    }        DB_HOST = 'localhost'        DB_NAME = 'mydb'        DB_USER = credentials('db-username')        DB_PASS = credentials('db-password')        CHANGELOG_FILE = 'db.changelog-master.xml'        ROLLBACK_COUNT = 2        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    liquibase status \                        --url="jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}" \                        --changeLogFile=${CHANGELOG_FILE} \                        --password=${DB_PASS}            }        stage('Update Database') {            steps {                    liquibase update \                        --url="jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}" \                        --changeLogFile=${CHANGELOG_FILE} \                        --password=${DB_PASS}            }                failure {                        liquibase rollback-count ${ROLLBACK_COUNT} \                            --url="jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}" \                            --changeLogFile=${CHANGELOG_FILE} \                            --password=${DB_PASS}                }        }}Create .github/workflows/liquibase.yml:on:  push:  pull_request:jobs:  database-update:      - name: Checkout code        uses: actions/checkout@v2        uses: actions/setup-java@v2          java-version: '11'      - name: Install Liquibase        run: |https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz          tar -xzf liquibase-4.24.0.tar.gz          sudo mv liquibase /usr/local/bin/      - name: Run Liquibase Update        env:          DB_URL: ${{ secrets.DB_URL }}          DB_USER: ${{ secrets.DB_USER }}          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}        run: |          liquibase --url="${DB_URL}" \                    --username="${DB_USER}" \                    --password="${DB_PASSWORD}" \                    --changeLogFile=db.changelog-master.xml \stages:  - validatevariables:  LIQUIBASE_VERSION: "4.24.0"validate:  stage: validate  image: liquibase/liquibase:${LIQUIBASE_VERSION}  script:    - liquibase --changeLogFile=db.changelog-master.xml validate  only:deploy-staging:  stage: deploy  image: liquibase/liquibase:${LIQUIBASE_VERSION}  script:      liquibase --url="${STAGING_DB_URL}" \                --username="${STAGING_DB_USER}" \                --password="${STAGING_DB_PASSWORD}" \                --changeLogFile=db.changelog-master.xml \                update    name: staging    - developdeploy-production:  stage: deploy  image: liquibase/liquibase:${LIQUIBASE_VERSION}  script:      liquibase --url="${PROD_DB_URL}" \                --username="${PROD_DB_USER}" \                --password="${PROD_DB_PASSWORD}" \                --changeLogFile=db.changelog-master.xml \                update    name: production  only:: Combine author name with timestamp: Apply different changes for different environments: Ensure database is in expected state: Enable easy rollback to known-good states: Organize changesets logicallyNever modify deployed changesets: Liquibase tracks checksums3. Atlas — The Modern Schema-as-Code ToolAtlas is a modern, open-source database CI/CD tool that promotes “database schema-as-code” philosophy. Built with Go, it draws inspiration from Terraform and provides a declarative approach to database schema management.: Define schemas in HCL, SQL, or via ORMs: Plan, apply, and manage migrationsAutomated drift detection: Identify untracked schema changes: Traditional migration files also supported: Built-in quality checksMultiple database support: PostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse: Atlas Cloud for team collaboration: Enterprise-ready securityPostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse, Redshift.Atlas supports two primary workflows:Declarative (Schema-as-Code): Define desired state, Atlas calculates migration: Traditional migration files with Atlas enhancementscurl -sSf https://atlasgo.sh | shdocker pull arigaio/atlasbrew install ariga/tap/atlasStep 2: Initialize Project# Create project directorymkdir my-database# Initialize Atlas projectatlas initStep 3: Define Schema (Declarative Approach)table "users" {  schema = schema.public    type = int  }    type = varchar(255)  }    type = varchar(255)  }    type = timestamp    default = sql("CURRENT_TIMESTAMP")  }    columns = [column.id]    columns = [column.email]  }table "posts" {  schema = schema.public    type = int  }    type = int  }    type = varchar(255)  }    type = text    columns = [column.id]    columns = [column.user_id]    ref_columns = [table.users.column.id]    on_delete = CASCADE}Step 4: Generate and Apply MigrationsInspect current database:atlas schema inspect \  --url "postgres://user:pass@localhost:5432/mydb?sslmode=disable" \Plan migration (dry run):atlas schema apply \  --url "postgres://user:pass@localhost:5432/mydb?sslmode=disable" \  --to "file://schema.hcl" \  --dry-runatlas schema apply \  --url "postgres://user:pass@localhost:5432/mydb?sslmode=disable" \Step 5: Using Versioned MigrationsCreate migration directory:Generate migration from schema:atlas migrate diff create_users \  --dir "file://migrations" \  --to "file://schema.hcl" \  --dev-url "docker://postgres/15/dev"atlas migrate apply \  --url "postgres://user:pass@localhost:5432/mydb?sslmode=disable" \  --dir "file://migrations"Create .github/workflows/atlas.yml:on:  push:  pull_request:jobs:  lint:    steps:      - uses: actions/checkout@v3        uses: ariga/setup-atlas@v0        run: |            --dir "file://migrations" \            --dev-url "docker://postgres/15/dev" \            --latest 1  deploy:    needs: lint    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v3        uses: ariga/setup-atlas@v0        env:          DATABASE_URL: ${{ secrets.DATABASE_URL }}        run: |            --url "${DATABASE_URL}" \            --dir "file://migrations"stages:  - validatevariables:  ATLAS_VERSION: "latest"validate:  stage: validate  image: arigaio/atlas:${ATLAS_VERSION}  script:    - atlas migrate lint --dir "file://migrations" --dev-url "docker://postgres/15/dev"  only:deploy-staging:  stage: deploy  image: arigaio/atlas:${ATLAS_VERSION}  script:      atlas migrate apply \        --url "${STAGING_DB_URL}" \        --dir "file://migrations"    name: staging    - developdeploy-production:  stage: deploy  image: arigaio/atlas:${ATLAS_VERSION}  script:      atlas migrate apply \        --dir "file://migrations"    name: production  only:: Always specify dev-url for schema calculations: Commit both HCL files and generated migrations: Catch issues before production deployment: Validate migrations in non-production first: For team collaboration and enhanced features: Regularly check for schema drift4. Bytebase — The All-in-One Database DevOps PlatformBytebase is an all-in-one database DevOps and CI/CD solution that provides a GUI workspace for developers and DBAs to collaborate on database changes. It’s like GitHub/GitLab but specifically designed for database management.: User-friendly interface for managing database changes: Database-as-code with Git integration: Built-in review system for database changes: Detect SQL anti-patterns automatically: Protect sensitive data: Complete audit trail of all database activities: Fine-grained permission management: Integrated SQL editor with data access controlMultiple database support: MySQL, PostgreSQL, Oracle, SQL Server, MongoDB, Redis, and more: Visual timeline of all database changesPostgreSQL, MySQL, Oracle, SQL Server, MongoDB, Redis, MariaDB, TiDB, ClickHouse, Snowflake, and more.Bytebase uses a database-as-code approach with:: Organize databases by application or team: Track database change requests: Manage Dev, Staging, Production separately: Sync database schemas from Git repositories: Approval workflow for sensitive changesUsing Docker (Recommended):docker run --init \  --name bytebase \  --add-host host.docker.internal:host-gateway \  --volume ~/.bytebase/data:/var/opt/bytebase \  bytebase/bytebase:latest \  --data /var/opt/bytebase \Create docker-compose.yml:services:  bytebase:    image: bytebase/bytebase:latest    container_name: bytebase    ports:    volumes:      - ./bytebase-data:/var/opt/bytebase    command:      - /var/opt/bytebase      - "8080"      - "host.docker.internal:host-gateway"Create bytebase-deployment.yaml:apiVersion: apps/v1kind: Deployment  name: bytebasespec:  selector:      app: bytebase    metadata:        app: bytebase      containers:        image: bytebase/bytebase:latest        - containerPort: 8080        - name: data          mountPath: /var/opt/bytebase        command:          - --data          - --port      volumes:        persistentVolumeClaim:---kind: Service  name: bytebasespec:  ports:    targetPort: 8080    app: bytebaseStep 3: Add Database InstancesGo to  → Select database type (PostgreSQL, MySQL, etc.)Enter connection details:Go to  → Enter project name and description: Traditional approval workflow: Sync from Git repositoryAssign team members with rolesStep 5: Setup GitOps (Optional)Connect Git provider (GitHub, GitLab, Bitbucket)Configure repository and branchSet up directory structure:/bytebase /migrations /prod 001_create_users.sql 002_add_email_column.sql /staging 001_create_users.sqlStep 6: Create Database ChangeCreate SQL file in Git repositoryBytebase automatically detects and creates issueBytebase provides native API for CI/CD integration.# Get access tokencurl -X POST http://localhost:8080/v1/auth/login \  -H "Content-Type: application/json" \  -d '{    "email": "admin@example.com",    "password": "yourpassword"# Create issuecurl -X POST http://localhost:8080/v1/projects/project-1/issues \  -H "Authorization: Bearer YOUR_TOKEN" \  -H "Content-Type: application/json" \    "type": "DATABASE_CHANGE",    "title": "Add users table",    "description": "Create users table for authentication",      "statement": "CREATE TABLE users (id INT PRIMARY KEY, username VARCHAR(255));"  }'Create .github/workflows/bytebase.yml:on:  push:      - 'migrations/**'jobs:  deploy:      - uses: actions/checkout@v2      - name: Authenticate with Bytebase        id: auth          TOKEN=$(curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/auth/login \            -H "Content-Type: application/json" \            -d '{              "email": "${{ secrets.BYTEBASE_EMAIL }}",              "password": "${{ secrets.BYTEBASE_PASSWORD }}"          echo "::set-output name=token::${TOKEN}"      - name: Create Migration Issue        run: |          MIGRATION_SQL=$(cat migrations/latest.sql)          curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/projects/my-project/issues \            -H "Authorization: Bearer ${{ steps.auth.outputs.token }}" \            -H "Content-Type: application/json" \              \"type\": \"DATABASE_CHANGE\",              \"title\": \"Auto migration from CI\",              \"payload\": {                \"statement\": \"${MIGRATION_SQL}\"              }: Separate Dev, Staging, Production: Require review for production changes: Catch issues early: Maintain single source of truth: Track all database activities: Protect sensitive data: Integrate with Slack or email5. Sqitch — The Git-like Database Change ManagementSqitch is a purely open-source database change management system with no commercial offerings. Built with Perl, it has been on the market since 2012 and takes a unique, Git-inspired approach to managing database changes.: Works with Git, Mercurial, SVN, or no VCS: Support for PostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake: Explicit dependency managementNo file naming conventions: Changes managed via sqitch plan: Mark important milestonesPowerful revert capability: Built-in rollback support: Specify prerequisites for changes: No GUI, fully command-line driven: Deploy with all dependenciesPostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake, and more.Unlike Liquibase and Flyway which use file naming conventions, Sqitch uses an explicit plan file (sqitch.plan) to specify the order and dependencies of changes:: Apply changes: Undo changes: Test that changes workeddocker pull sqitch/sqitch# Install dependenciessudo apt-get install libdbd-pg-perl libdbd-mysql-perl# Install from CPANcpan App::SqitchStep 2: Initialize Project# Create new Sqitch projectsqitch init myapp --uri https://github.com/myorg/myapp --engine pg# This creates:# - sqitch.conf (configuration)# - sqitch.plan (change plan)# - deploy/, revert/, verify/ directoriesConfiguration file (sqitch.conf):[core]    engine = pg    plan_file = sqitch.plan[engine "pg"]    target = db:pg://user:pass@localhost/mydb    client = psql# Add a new changesqitch add create_users -n "Create users table"# This creates three files:# - deploy/create_users.sql# - revert/create_users.sql# - verify/create_users.sqlEdit deploy/create_users.sql:-- Deploy myapp:create_users to pgCREATE TABLE users (    id SERIAL PRIMARY KEY,    username VARCHAR(255) NOT NULL UNIQUE,    email VARCHAR(255) NOT NULL UNIQUE,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);Edit revert/create_users.sql:-- Revert myapp:create_users from pgEdit verify/create_users.sql:-- Verify myapp:create_users on pgSELECT id, username, email, created_atFROM usersStep 4: Add Change with Dependencies# Add change that depends on create_userssqitch add add_posts --requires create_users -n "Create posts table"The sqitch.plan file will look like:%syntax-version=1.0.0%project=myapp%uri=https://github.com/myorg/myappcreate_users 2025-01-15T10:30:00Z User Name <user@example.com> # Create users tableadd_posts [create_users] 2025-01-15T11:00:00Z User Name <user@example.com> # Create posts table# Check statussqitch status# Deploy all changessqitch deploy# Deploy to specific changesqitch deploy add_posts# Verify deploymentsqitch verify# Revert last changesqitch revert# Revert to specific changesqitch revert create_users# Revert all changessqitch revert --to @ROOTCreate .github/workflows/sqitch.yml:name: Sqitch Database Migrationon:  push:    paths:      - 'revert/**'      - 'sqitch.plan'jobs:  deploy:      postgres:        env:          POSTGRES_PASSWORD: postgres          POSTGRES_DB: testdb          --health-cmd pg_isready          --health-timeout 5s        ports:      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl postgresql-client      - name: Run Sqitch Deploy        env:          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb        run: |      - name: Verify Deployment        env:          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb        run: |  deploy-production:    needs: deploy    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl      - name: Deploy to Production        env:          SQITCH_TARGET: ${{ secrets.PROD_DATABASE_URL }}        run: |          sqitch verifyvariables:  POSTGRES_DB: testdb  POSTGRES_PASSWORD: postgrestest:  stage: test  image: sqitch/sqitch:latest  services:  variables:    SQITCH_TARGET: "db:pg://$POSTGRES_USER:$POSTGRES_PASSWORD@postgres/$POSTGRES_DB"  script:    - sqitch verify    - merge_requestsdeploy-staging:  stage: deploy  image: sqitch/sqitch:latest  variables:    SQITCH_TARGET: $STAGING_DATABASE_URL  script:    - sqitch verify    name: staging    - developdeploy-production:  stage: deploy  image: sqitch/sqitch:latest  variables:    SQITCH_TARGET: $PROD_DATABASE_URL  script:    - sqitch verify    name: production  only:pipeline {    agent any        STAGING_DB = credentials('staging-db-url')        PROD_DB = credentials('prod-db-url')    }        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    docker run --rm \                        -w /repo \                        sqitch deploy --target ${STAGING_DB}                        -v $(pwd):/repo \                        sqitch/sqitch:latest \                        sqitch verify --target ${STAGING_DB}                '''        }        stage('Deploy to Production') {            when {            }                input message: 'Deploy to production?', ok: 'Deploy'                    docker run --rm \                        -w /repo \                        sqitch deploy --target ${PROD_DB}                        -v $(pwd):/repo \                        sqitch/sqitch:latest \                        sqitch verify --target ${PROD_DB}                '''        }        failure {                docker run --rm \                    -w /repo \                    sqitch revert --target ${PROD_DB} -y        }}Always write verify scripts: Test that changes applied correctlyUse explicit dependencies: Define prerequisites in sqitch.plan: Mark important milestones: Ensure rollback works before production: Use sqitch bundle for offline deployments: Write clear commit messages: Define environments in sqitch.confFeature Flyway Liquibase Atlas Bytebase Sqitch  Java Java Go Go/TypeScript Perl  Open Source + Commercial Open Source + Commercial Open Source + Cloud Open Source + Enterprise Pure Open Source  CLI, API, Maven/Gradle CLI, API, Maven/Gradle CLI GUI + CLI + API CLI  SQL, Java XML, YAML, JSON, SQL HCL, SQL SQL SQL  50+ 60+ 7+ 15+ 10+  Limited (Teams only) Yes Yes Yes Yes  Easy Moderate Moderate Easy Moderate  Via CI/CD Via CI/CD Native Native Native  No No (separate tool) No (Cloud has GUI) Yes No  Sequential Limited Yes Limited Explicit  Simple migrations, CI/CD Enterprise, Complex changes Modern teams, IaC fans Teams needing GUI, collaboration Git-like workflow, explicit controlYou want simplicity and ease of useYour team prefers SQL-based migrationsYou need fast integration into CI/CD pipelinesYou’re working with standard relational databasesYou don’t require complex rollback scenariosYou need enterprise-grade featuresYou require support for many database typesYou need flexible change log formatsCompliance and audit trails are criticalYou want precondition supportYou embrace infrastructure-as-code principlesYou like Terraform’s workflowYou want modern, declarative schema managementYou’re building cloud-native applicationsYou want an all-in-one GUI solutionYour team includes non-technical stakeholdersYou need built-in collaboration featuresSQL review and approval workflows are importantYou want integrated access control and data maskingYou want Git-like change managementYou need explicit dependency controlYou prefer pure open-source toolsYou’re comfortable with command-line toolsYou want bundled offline deploymentsGeneral Best Practices for Database MigrationsRegardless of which tool you choose, follow these universal best practices:1. Version Control EverythingStore all migration scripts in GitTreat database changes like codeUse branches for feature developmentReview changes via pull requests2. Test in Non-Production FirstAlways test migrations in stagingUse production-like data volumesVerify rollback procedures3. Maintain Backward CompatibilityMake changes backward-compatible when possibleUse blue-green deployments for breaking changesKeep old columns/tables temporarilyCoordinate with application deployments4. Use Secure Credential ManagementNever commit passwords to GitUse secret management tools (HashiCorp Vault, AWS Secrets Manager)Rotate credentials regularlyUse least-privilege database users5. Implement Proper CI/CD IntegrationAutomate testing of migrationsRun migrations in deployment pipelineUse separate databases for each environmentImplement approval gates for productionLog all migration activitiesSet up alerts for failuresTrack migration execution timeMaintain audit trails for complianceAlways write and test rollback scriptsPractice rollback proceduresKeep rollback windows minimalDocument migration purposeInclude context in commit messagesMaintain runbooks for complex migrationsShare knowledge with teamDatabase migrations are a critical yet challenging aspect of modern software development. The right tool can make the difference between smooth, reliable deployments and risky, error-prone manual processes.Each of the five tools we’ve covered has its strengths: offers simplicity and reliability for teams wanting straightforward SQL migrations provides enterprise-grade features for complex, compliance-driven environments brings modern infrastructure-as-code principles to database management delivers an all-in-one GUI platform for team collaboration offers Git-like control for teams wanting explicit change managementThe key is choosing the tool that best fits your team’s needs, technical stack, and workflow preferences. Start with one tool, integrate it into your CI/CD pipeline, and iterate based on your experience.Remember: the best database migration tool is the one your team will actually use consistently. Pick one, set it up properly, and make database changes as reliable and traceable as your application code deployments.]]></content:encoded></item><item><title>Security Doesn’t Have to Hurt</title><link>https://www.docker.com/blog/security-shadow-it-collaboration/</link><author>Simeon Ratliff</author><category>docker</category><category>devops</category><pubDate>Fri, 31 Oct 2025 13:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Do you ever wish security would stop blocking the tools you need to do your job? Surprise: your security team wants the same.There you are, just trying to get your work done, when…You need an AI to translate documentation, but all the AI services are blocked by a security web monitoring tool.You finish coding and QA for a new software version just under the wire, but the release is late because security has not reviewed the open source software and libraries included.Your new database works perfectly in dev/test, but it does not work in production because of a port configuration, and you do not have permissions. Changes to production permissions all require security approvalShadow IT is a spy-movie name for a phenomenon that is either a frustrating necessity or a game of whack-a-mole, depending on your responsibilities.If you’re an engineer creating the next best product, shadow IT is a necessity. Company-supplied information technology does not change fast enough to keep up with the market, let alone allow you to innovate. Despite that, your security team will come down hard on anyone who tries to go outside the allowed vendors and products. Data storage has to be squared away in encrypted, protected spaces, and you have to jump like a show pony to get access. And you have no flexibility in the tools you’re allowed to use, even if you could produce faster and better with other options.So you stop playing by the rules, and you find tools and tech that work.That is, until someone protests the cloud hosting bill, finds the wifi access point, or notices the unofficial software repository. Security takes away your tools or cuts off access. And then you are upset, your team feels attacked, and security is up in arms.If you are on a security team, shadow IT is a game of whack-a-mole. Company-supplied information technology changes without review. You know they’re trying to enable innovation, but they’re negating all the IT compliance certifications that allow you to sell your services and products. You have to investigate, prove, and argue about policies and regulations just to stop people from storing client secrets in their personal cloud storage.Whether you are a new hire in the Security Operations Center or the unlucky CISO who reports to the CTO, this is a familiar refrain.Yet no one wants this. Not you, not your boss, and not security.If It Cannot Be Fixed, Break ItIt’s time we change the ground rules of security to focus on compromise rather than stringency. Most security teams want to change their operations to concentrate on the capabilities they are trained for: threat intelligence, risk management, forensic analysis, and security engineering. I have never met a security professional who wants to spend their time arguing over a port configuration. It’s tiresome, and that friction inspires lasting antagonism on both sides.Imagine working in a place where you can use innovative new tools, release products without a security delay, and change configurations so that your deployment works smoothly.But there is a subtle change that must happen to enable this security-IT paradise: non-security teams would have to understand and implement all the requirements security departments would check. And everyone who is part of the change would need to understand the implications of their actions and take sole responsibility for the security outcomes.My non-IT colleagues are shocked when I explain the scope of work for a security department in preparation for any release or product launch:Weaknesses and exploits for custom and third-party codeScope and adequacy of vendor securityData encryption, transmission, and storage, especially across bordersCompliance with regulation and data protection lawsIn many industries, we legally cannot remove security practices from IT processes. But we can change who takes responsibility for which parts of the work Security requirements are not a secret. A developer with integrated code scanners can avoid OWASP Top 10 flaws and vulnerable libraries and remove hard-coded accounts. Infrastructure admins with access to network security tools can run tidy networks, servers, and containers with precise configurations.The result? The security team can let go of their rigid deployment rules.If developers use code security tools and incorporate good practices, security team approval should take hours rather than days or weeks. Security can also approve the standard container configuration rather than each separate container in an architecture. They can define the requirements, offer you tools to review your work, and help you integrate good practices into your workflow.“Trust but verify” would become a daily pattern instead of lip service to good interdepartmental relationships. Security will continue to monitor the environment and the application after release. They will keep an eye on vendor assertions and audits, watching threat intelligence streams for notifications that demonstrate risk. Security teams will have time to do the job they signed up for, which is much more interesting than policing other departments.This change would also require that the security team be allowed to let go. When trust is broken—if vendors are not properly assessed, or software is introduced but not reported—the fault should not lie with the security team. If insecure coding causes a compromise, the development team must be accountable, and if an inadequately configured network causes a data leak, the network and hosting team must be called on the carpet. If the requirements are in place but not met, the responsible parties must be those that agreed to them but neglected to enact them.Freedom to Choose Comes with a CatchThis new freedom makes shadow IT unnecessary. Teams do not need to hide the choices they make. However, the freedom to choose comes with a catch: full responsibility for your choices.Consider the company charge card: Finance teams create the policy for how to use company charge cards and provide the tools for reimbursement. They do not scrutinize every charge in real time, but they review usage and payments.If the tool is abused and the agreed-upon care is ignored, the card user is held responsible. Any lack of knowledge does not exempt you from the consequences. For minor infractions, you may get a written notice. For severe infractions, you can expect to be terminated for cause.The finance requirements, your agreement, regular review, and enacted consequences minimize fraud internally. More importantly, though, this combination protects the company against accusations of negligence.Security responsibility could work the same. Security teams can set requirements that IT workers agree to individually. IT teams are then free to deploy and make changes as appropriate for their work. IT secures assets before they are put into production, and security continues with the best practice of reviewing assets continuously after the fact. Delays in getting the tools you need are reduced, and you control the deployment of your work with much more assurance. The incentive for shadow IT is much lower, and the personal risk of choosing it is higher.That last bit is the catch, though—when you take control, you take responsibility for the result. Instead of committing to a patch, you back out insecure code and redeploy when it is corrected. When your department contracts with a squirrelly vendor, your manager’s budget takes the hit for breaking the contract. When the network is compromised, the CIO, not the CISO, gets fired.Right now, the security team carries this responsibility and shoulders these risks. But the result is an enterprise held hostage by risk aversion, with no understanding or control over the outcomes.So far, I’ve mostly addressed IT, but I also want to bring this argument back home: Security professionals, let’s stop taking control of everyone else’s work. When we make hard requirements that do not meet tech realities, our IT teams get better at hiding their tracks. You will make more progress if you invest in mutual success and reward people who step up to exceed your expectations.When Security and IT Make Peace, Shadow IT Becomes UnnecessaryI once worked with a development team that wanted to store proprietary code in a hosted code repository. The repository was great for their needs: versioning automation, fine-grained access management, easy branching, access from anywhere, and centralized storage. Instead of waiting six months for the new vendor security investigation process, the developer team gathered the vendor’s audit certificates, data handling guarantees, and standard contract language about security and data mining. The devs proactively researched the third-party security scanning policies and asked for their incident response and notification policies.Our security team would have struggled to locate this repository if the developers had simply chosen to use it. Instead, they circumvented our process in the best way—by providing every necessary answer to our security questions.The reward was an instant yes from me, the security leader, without having to wait for my overworked team to schedule yet another vendor review.My reward? No shadow IT plus a very happy IT team.Security should go beyond allowing compromises like this: we should seek them out. Convince the CISO to work toward giving your IT teams both control and responsibility, find a compromise with the teams that will take security seriously—and save your energy for wrangling teams that don’t.For admins and developers: Provide the ISO audit documents for that vendor you want to use. Be the first dev team to learn the org’s code scanning tool. Read the latest risk assessments from your cloud environment and don’t repeat vulnerable configurations. These small changes make your work faster, simpler, and less expensive than finding your own solutions.]]></content:encoded></item><item><title>Mr. Bones: A Pirate-Voiced Halloween Chatbot Powered by Docker Model Runner</title><link>https://www.docker.com/blog/talking-skeleton-docker-llm/</link><author>Mike Coleman</author><category>docker</category><category>devops</category><pubDate>Fri, 31 Oct 2025 12:17:21 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[My name is Mike Coleman, a staff solution architect at Docker. This year I decided to turn a Home Depot animatronic skeleton into an AI-powered,  live, interactive Halloween chatbot. The result: kids walk up to Mr. Bones, a spooky skeleton in my yard, ask it questions, and it answers back — in full pirate voice — with actual conversational responses, thanks to a local LLM powered by Docker Model Runner. is a tool from Docker that makes it dead simple to run open-source LLMs locally using standard Docker workflows. I pulled the model like I’d pull any image, and it exposed an OpenAI-compatible API I could call from my app. Under the hood, it handled model loading, inference, and optimization.For this project, Docker Model Runner offered a few key benefits: for LLM inference — unlike OpenAI or Anthropic because the model runs on local hardware over model selection, prompts, and scaffoldingAPI-compatible with OpenAI — switching providers is as simple as changing an environment variable and restarting the serviceThat last point matters: if I ever needed to switch to OpenAI or Anthropic for a particular use case, the change would take seconds.Figure 1: System overview of Mr. Bones answering questions in pirate language records audio transcribes speech to textAPI call to a Windows gaming PC with an RTX 5070 GPU runs a local LLaMA 3.1 8B (Q4 quant) modelLLM returns a text response converts the text to speech (pirate voice)Pi sends audio to skeleton via Bluetooth, which moves the jaw in syncFigure 2: The controller box that holds the Raspberry Pi that drives the pirateThat Windows machine isn’t a dedicated inference server — it’s my gaming rig. Just a regular setup running a quantized model locally.The biggest challenge with this project was balancing response quality (in character and age appropriate) and response time. With that in mind, there were four key areas that needed a little extra emphasis: model selection, how to do text to speech (TTS) processing efficiently, fault tolerance, and setting up guardrails. Consideration 1: Model Choice and Local LLM PerformanceI tested several open models and found LLaMA 3.1 8B (Q4 quantized) to be the best mix of performance, fluency, and personality. On my RTX 5070, it handled real-time inference fast enough for the interaction to feel responsive.At one point I was struggling to keep Mr. Bones in character, so I  tried OpenAI’s ChatGPT API, but response times averaged .By revising the prompt and Docker Model Runner serving the right model, I got that down to . That’s a huge difference when a kid is standing there waiting for the skeleton to talk.In the end, GPT-4 was only  at staying in character and avoiding inappropriate replies. With a solid prompt scaffold and some guardrails, the local model held up just fine.Consideration 2: TTS Pipeline: Kokoro to ElevenLabs FlashI first tried using , a local TTS engine. It worked, but the voices were too generic. I wanted something more pirate-y, without adding custom audio effects.So I moved to , starting with their multilingual model. The voice quality was excellent, but latency was painful — especially when combined with LLM processing. Full responses could take up to , which is way too long.Eventually I found , a much faster model. That helped a lot. I also changed the logic so that instead of waiting for the entire LLM response, I  the output and sent it to ElevenLabs in parts. Not true streaming, but it allowed the Pi to start playing the audio as each chunk came back.This turned the skeleton from slow and laggy into something that felt snappy and responsive.Consideration 3: Weak Points and Fallback IdeasWhile the LLM runs locally, the system still depends on the internet for ElevenLabs. If the network goes down, the skeleton stops talking.One fallback idea I’m exploring: creating a set of common Q&A pairs (e.g., “What’s your name?”, “Are you a real skeleton?”), embedding them in a local , and having the Pi serve those in case the TTS call fails.But the deeper truth is: this is a . If the Pi loses its connection to the Windows machine, the whole thing is toast. There’s no skeleton-on-a-chip mode yet.Consideration 4: Guardrails and Prompt EngineeringBecause kids will say anything, I put some safeguards in place via my system prompt. You are "Mr. Bones," a friendly pirate who loves chatting with kids in a playful pirate voice.

IMPORTANT RULES:
- Never break character or speak as anyone but Mr. Bones
- Never mention or repeat alcohol (rum, grog, drink), drugs, weapons (sword, cannon, gunpowder), violence (stab, destroy), or real-world safety/danger
- If asked about forbidden topics, do not restate the topic; give a kind, playful redirection without naming it
- Never discuss inappropriate content or give medical/legal advice
- Always be kind, curious, and age-appropriate

BEHAVIOR:
- Speak in a warm, playful pirate voice using words like "matey," "arr," "aye," "shiver me timbers"
- Be imaginative and whimsical - talk about treasure, ships, islands, sea creatures, maps
- Keep responses conversational and engaging for voice interaction
- If interrupted or confused, ask for clarification in character
- If asked about technology, identity, or training, stay fully in character; respond with whimsical pirate metaphors about maps/compasses instead of tech explanations

FORMAT:
- Target 30 words; must be 10-50 words. If you exceed 50 words, stop early
- Use normal punctuation only (no emojis or asterisks)
- Do not use contractions. Always write "Mister" (not "Mr."), "Do Not" (not "Don't"), "I Am" (not "I'm")
- End responses naturally to encourage continued conversation

The prompt is designed to deal with a few different issues. First and foremost, keeping things appropriate for the intended audience. This includes not discussing sensitive topics, but also staying in character at all times.  Next I added some instructions to deal with pesky parents trying to trick Mr. Bones into revealing his true identity. Finally, there is some guidance on response format to help keep things conversational – for instance, it turns out that some STT engines can have problems with things like contractions. Instead of just refusing to respond, the prompt redirects sensitive or inappropriate inputs in-character. For example, if a kid says “I wanna drink rum with you,” the skeleton might respond, “Arr, matey, seems we have steered a bit off course. How about we sail to smoother waters?”This approach keeps the interaction playful while subtly correcting the topic. So far, it’s been enough to keep Mr. Bones spooky-but-family-friendly.Figure 3: Mr. Bones is powered by AI and talks to kids in pirate-speak with built-in safety guardrails.This project started as a Halloween goof, but it’s turned into a surprisingly functional proof-of-concept for real-time, local voice assistants.Using  for LLMs gave me speed, cost control, and flexibility. ElevenLabs Flash handled voice. A Pi 5 managed the input and playback. And a Home Depot skeleton brought it all to life.Could you build a more robust version with better failover and smarter motion control? Absolutely. But even as he stands today, Mr. Bones has already made a bunch of kids smile — and probably a few grown-up engineers think, “Wait, I could build one of those.” Figure 4: Aye aye! Ye can build a Mr. Bones too and bring smiles to all the young mateys in the neighborhood!]]></content:encoded></item><item><title>Cursor 2.0 Brings Faster AI Coding and Multi-Agent Workflows</title><link>https://devops.com/cursor-2-0-brings-faster-ai-coding-and-multi-agent-workflows/</link><author>Tom Smith</author><category>devops</category><pubDate>Fri, 31 Oct 2025 09:08:09 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>8 Pipeline Caching Tricks That Cut CI Time in Half</title><link>https://blog.devops.dev/8-pipeline-caching-tricks-that-cut-ci-time-in-half-3af50fae7db9?source=rss----33f8b2d9a328---4</link><author>Obafemi</author><category>devops</category><pubDate>Thu, 30 Oct 2025 14:12:02 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Aembit Introduces Identity and Access Management for Agentic AI</title><link>https://devops.com/aembit-introduces-identity-and-access-management-for-agentic-ai/</link><author>cybernewswire</author><category>devops</category><pubDate>Thu, 30 Oct 2025 12:02:04 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to Integrate Quantum-Safe Security into Your DevOps Workflow</title><link>https://devops.com/how-to-integrate-quantum-safe-security-into-your-devops-workflow/</link><author>Carl Torrence</author><category>devops</category><pubDate>Thu, 30 Oct 2025 11:50:54 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>theCUBE Research economic validation of Docker’s development platform</title><link>https://www.docker.com/blog/thecube-research-economic-validation-of-docker-development-platform/</link><author>John Ayub</author><category>docker</category><category>devops</category><pubDate>Thu, 30 Oct 2025 11:46:28 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Docker’s ROI and impact on agentic AI, security, and developer productivity. surveyed ~400 IT and AppDev professionals at leading global enterprises to investigate Docker’s ROI and impact on agentic AI development, software supply chain security, and developer productivity.  The industry context is that enterprise developers face mounting pressure to rapidly ship features, build agentic AI applications, and maintain security, all while navigating a fragmented array of development tools and open source code that require engineering cycles and introduce security risks. Docker transformed software development through containers and DevSecOps workflows, and is now doing the same for agentic AI development and software supply chain security.  theCUBE Research quantified Docker’s impact: teams build agentic AI apps faster, achieve near-zero CVEs, remediate vulnerabilities before exploits, ship modern cloud-native applications, save developer hours, and generate financial returns. for key highlights and analysis.  theCUBE Research and to take a deep dive.Agentic AI development streamlined using familiar technologiesDevelopers can build, run, and share agents and compose agentic systems using familiar Docker container workflows. To do this, developers can build agents safely using Docker MCP Gateway, Catalog, and Toolkit; run agents securely with Docker Sandboxes; and run models with Docker Model Runner. These capabilities align with theCUBE Research findings that 87% of organizations reduced AI setup time by over 25% and 80% report accelerating AI time-to-market by at least 26%.  Using Docker’s modern and secure software delivery practices, development teams can implement AI feature experiments faster and in days test agentic AI capabilities that previously took months. Nearly 78% of developers experienced significant improvement in the standardization and streamlining of AI development workflows, enabling better testing and validation of AI models. Docker helps enterprises generate business advantages through deploying new customer experiences that leverage agentic AI applications. This is phenomenal, given the nascent stage of agentic AI development in enterprises.Software supply chain security and innovation can move in lockstepSecurity engineering and vulnerability remediation can slow development to a crawl. Furthermore, checkpoints or controls may be applied too late in the software development cycle, or after dangerous exploits, creating compounded friction between security teams seeking to mitigate vulnerabilities and developers seeking to rapidly ship features. Docker embeds security directly into development workflows through vulnerability analysis and continuously-patched certified container images. theCUBE Research analysis supports these Docker security capabilities: 79% of organizations find Docker extremely or very effective at maintaining security & compliance, while 95% of respondents reported that Docker improved their ability to identify and remediate vulnerabilities. By making it very simple for developers to use secure images as a default, Docker enables engineering teams to plan, build, and deploy securely without sacrificing feature velocity or creating deployment bottlenecks. Security and innovation can move in lockstep because Docker concurrently secures software supply chains and eliminates vulnerabilities.Developer productivity becomes a competitive advantageConsistent container environments eliminate friction, accelerate software delivery cycles, and enable teams to focus on building features rather than overcoming infrastructure challenges. When developers spend less time on environment setup and troubleshooting, they ship more features. Application features that previously took months now reach customers in weeks. The research demonstrates Docker’s ability to increase developer productivity. 72% of organizations reported significant productivity gains in development workflows, while 75% have transformed or adopted DevOps practices when using Docker. Furthermore, when it comes to AI and supply chain security, the findings mentioned above further support how Docker unlocks developer productivity.Financial returns exceed expectationsCFOs demand quantifiable returns for technology investments, and Docker delivers them.  reported substantial , with 43% reporting $50,000-$250,000 in cost reductions from infrastructure efficiency, reduced rework, and faster time-to-market. The ROI story is equally compelling: 69% of organizations report ROI exceeding 101%, with many achieving ROI above 500%. When factoring in faster feature delivery, improved developer satisfaction, and reduced security incidents, the business case for Docker becomes even more tangible. The direct costs of a security breach can surpass $500 million, so mitigating even a fraction of this cost provides a compelling financial justification for enterprises to deploy Docker to every developer.Modernization and cloud native apps remain top of mindFor enterprises who maintain extensive legacy systems, Docker serves as a proven catalyst for cloud-native transformation at scale. Results show that nearly nine in ten (88%) of organizations report Docker has enabled modernization of at least 10% of their applications, with half achieving modernization across 31-60% of workloads and another 20% modernizing over 60%. Docker accelerates the shift from monolithic architectures to modern containerized cloud-native environments while also delivering substantial business value.  For example, 37% of organizations report 26% to >50% faster product time-to-market, and 72% report annual cost savings ranging from $50,000 to over $1 million.Learn more about Docker’s impact on enterprise software developmentDocker has evolved from a containerization suite into a development platform for testing, building, securing, and deploying modern software, including agentic AI applications. Docker enables enterprises to apply proven containerization and DevSecOps practices to agentic AI development and software supply chain security.Download (below) the full report and the ebook from theCUBE Research analysis to learn Docker’s impact on developer productivity, software supply chain security, agentic AI application development, CI/CD and DevSecOps, modernization, cost savings, and ROI.  Learn how enterprises leverage Docker to transform application development and win in markets where speed and innovation determine success.theCUBE Research economic validation of Docker’s development platform]]></content:encoded></item><item><title>Anatomy of an Outage: Our AWS AutoScaling Group “Helping” Hand Pushed us off the Cliff</title><link>https://devops.com/anatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff/</link><author>Muhammad Yawar Malik</author><category>devops</category><pubDate>Thu, 30 Oct 2025 10:32:08 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Exploring Cloud Key Management Options</title><link>https://devops.com/exploring-cloud-key-management-options/</link><author>Alexander Williams</author><category>devops</category><pubDate>Thu, 30 Oct 2025 10:07:45 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GKE 10 Year Anniversary, with Gari Singh</title><link>https://e780d51f-f115-44a6-8252-aed9216bb521.libsyn.com/gke-10-year-anniversary-with-gari-singh</link><author></author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/kpod262.mp3?dest-id=3486674" length="" type=""/><pubDate>Wed, 29 Oct 2025 23:34:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[GKE turned 10 in 2025! In this episode, we talk with GKE PM Gari Singh about GKE's journey from early container orchestration to AI-driven ops. Discover Autopilot, IPPR, and a bold vision for the future of Kubernetes.Do you have something cool to share? Some questions? Let us know:]]></content:encoded></item><item><title>Deaf and Hard of Hearing WG Meeting - October 2025</title><link>https://www.youtube.com/watch?v=Ngjrv07mdDI</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Ngjrv07mdDI?version=3" length="" type=""/><pubDate>Wed, 29 Oct 2025 20:13:05 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Survey Surfaces Rising Tide of Vulnerabilities in Code Generated by AI</title><link>https://devops.com/survey-surfaces-rising-tide-of-vulnerabilities-in-code-generated-by-ai/</link><author>Mike Vizard</author><category>devops</category><pubDate>Wed, 29 Oct 2025 18:26:58 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AppOmni Open Sources Heisenberg Tool to Scan Pull Requests for Dependencies</title><link>https://devops.com/appomni-open-sources-heisenberg-tool-to-scan-pull-requests-for-dependencies/</link><author>Mike Vizard</author><category>devops</category><pubDate>Wed, 29 Oct 2025 16:08:25 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Spring Boot Java App on Azure VM</title><link>https://blog.devops.dev/spring-boot-java-app-on-azure-vm-e36f5ca7ce86?source=rss----33f8b2d9a328---4</link><author>Amita Pal Singh</author><category>devops</category><pubDate>Wed, 29 Oct 2025 14:06:01 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[The classic developer dilemma: Your code is a masterpiece, but it’s trapped on .To make it accessible to the world, you need a robust deployment platform. For critical applications, like core banking systems or high-throughput financial services, deployment isn’t just a technical task; it’s a cornerstone of digital transformation. Large FinTech companies are increasingly turning to cloud infrastructure, even for their complex, legacy monolith applications, to accelerate deployment, ensure scalability, enhance disaster recovery, and deliver insights faster. Azure Virtual Machines (VMs) provide a powerful, flexible solution for hosting these applications with enterprise-grade reliability.This guide serves as your roadmap to deploying a Spring Boot Java application on an Azure VM. We’ll walk through the essential steps to take your compiled application from local development to a production-ready cloud environmentPrerequisites and Environment SetupSign in to the Azure portalEnter virtual machines in the search. Under Services, select Virtual machines.In the Basics tab, under the Project details, select an appropriate subscription. Then choose an existing resource group or create a new one to use.Provide a VM name, select the region nearest to you, and fill in mandatory fields.Select OS Image for LinuxUnder Authentication type, select password, provide user name and password, and save the credentials somewhere. We would need the credentials to login to the VM.Under Inbound port rules > Public inbound ports, choose Allow selected ports and then select SSH (22) and HTTP (80) from the drop-down.Select the Review + create button at the bottom of the page.On the Create a virtual machine page, you can see the details about the VM you are about to create. Select Create.When the deployment is finished, select Go to resource.On the page for your new VM, select the public IP address and copy it.Select BashExecute commandssh <username>@<vm public ip>Enter the password to connect.If you are deploying to an Azure Virtual Machine (VM) running a Linux distribution like Ubuntu, you may need to ensure the correct Java version is installed. The following commands are specific to Debian/Ubuntu-based systems to install .Step 1: Search for Available OpenJDK PackagesThis command verifies the existence of OpenJDK 17 packages in the repository cache.apt-cache search openjdk | grep openjdk-17Step 2: Install OpenJDK 17 JREsudo apt install openjdk-17-jreConfirm that the correct version is set as the default Java runtime. The output should clearly display the installed Java 17 version details.Tomcat server is used for deploying Java-based web applications.Instructions for setup and configuration of Tomcat server are available in articleIn an Azure VM environment, you must also ensure the VM’s Network Security Group (NSG) has an inbound rule to allow traffic on the Tomcat port (default ) from your desired source IP or range.Details on how to create a WAR file for Spring Boot application are available in the articleNavigate to the Tomcat Manager GUI -> “WAR file to deploy” section, and deployCheck for Issues and ExceptionsDeployment is rarely smooth on the first attempt. To troubleshoot, you must monitor the logs at /opt/tomcat/logs directory.Checking Logs using the Command Line:Log files are typically huge in size. To view the end of a log file in real-time or view a specific number of lines, use following Bash commands:View the last 10 lines of the host-manager log/opt/tomcat/logs tail host-manager.*.logView the last N lines of any filetail -n <number of lines> <filename>Continuously stream the main application log (Ctrl+C to stop)The application will be accessible at:http://<public-IP>:8080/your-war-file-name/Where your-war-file-name is the name of your WAR file without the .war extension (e.g. BasicLOS). This is also known as the Context Path.curl http://<privateip>:8080Change the port to 80 (Optional)For improved user experience it is advisable to change the port to 80.]]></content:encoded></item><item><title>Quantum‑Ready Cloud DevOps – Getting ready for Quantum Computing Integration</title><link>https://devops.com/quantum%e2%80%91ready-cloud-devops-getting-ready-for-quantum-computing-integration/</link><author>Joydip Kanjilal</author><category>devops</category><pubDate>Wed, 29 Oct 2025 11:19:48 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>An Experience-Based Guide to Choosing the Right DevOps Provider in 2026</title><link>https://devops.com/an-experience-based-guide-to-choosing-the-right-devops-provider-in-2026/</link><author>Alex Vakulov</author><category>devops</category><pubDate>Wed, 29 Oct 2025 10:09:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Build more accurate AI applications with Amazon Nova Web Grounding</title><link>https://aws.amazon.com/blogs/aws/build-more-accurate-ai-applications-with-amazon-nova-web-grounding/</link><author>Matheus Guimaraes</author><category>devops</category><pubDate>Tue, 28 Oct 2025 23:59:17 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Imagine building AI applications that deliver accurate, current information without the complexity of developing intricate data retrieval systems. Today, we’re excited to announce the general availability of Web Grounding, a new built-in tool for Nova models on Amazon Bedrock.Web Grounding provides developers with a turnkey Retrieval Augmented Generation (RAG) option that allows the Amazon Nova foundation models to intelligently decide when to retrieve and incorporate relevant up-to-date information based on the context of the prompt. This helps to ground the model output by incorporating cited public sources as context, aiming to reduce hallucinations and improve accuracy.Developers should consider using Web Grounding when building applications that require access to current, factual information or need to provide well-cited responses. The capability is particularly valuable across a range of applications, from knowledge-based chat assistants providing up-to-date information about products and services, to content generation tools requiring fact-checking and source verification. It’s also ideal for research assistants that need to synthesize information from multiple current sources, as well as customer support applications where accuracy and verifiability are crucial.Web Grounding is especially useful when you need to reduce hallucinations in your AI applications or when your use case requires transparent source attribution. Because it automatically handles the retrieval and integration of information, it’s an efficient solution for developers who want to focus on building their applications rather than managing complex RAG implementations. Web Grounding seamlessly integrates with supported Amazon Nova models to handle information retrieval and processing during inference. This eliminates the need to build and maintain complex RAG pipelines, while also providing source attributions that verify the origin of information.Let’s see an example of asking a question to Nova Premier using Python to call the Amazon Bedrock Converse API with Web Grounding enabled.First, I created an Amazon Bedrock client using AWS SDK for Python (Boto3) in the usual way. For good practice, I’m using a session, which helps to group configurations and make them reusable. I then create a BedrockRuntimeClient.try:
    session = boto3.Session(region_name='us-east-1')
    client = session.client(
        'bedrock-runtime')I then prepare the Amazon Bedrock Converse API payload. It includes a “role” parameter set to “user”, indicating that the message comes from our application’s user (compared to “assistant” for AI-generated responses).For this demo, I chose the question “What are the current AWS Regions and their locations?” This was selected intentionally because it requires current information, making it useful to demonstrate how Amazon Nova can automatically invoke searches using Web Grounding when it determines that up-to-date knowledge is needed.# Prepare the conversation in the format expected by Bedrock
question = "What are the current AWS regions and their locations?"
conversation = [
   {
     "role": "user",  # Indicates this message is from the user
     "content": [{"text": question}],  # The actual question text
      }
    ]First, let’s see what the output is without Web Grounding. I make a call to Amazon Bedrock Converse API.# Make the API call to Bedrock 
model_id = "us.amazon.nova-premier-v1:0" 
response = client.converse( 
    modelId=model_id, # Which AI model to use 
    messages=conversation, # The conversation history (just our question in this case) 
    )
print(response['output']['message']['content'][0]['text'])I get a list of all the current AWS Regions and their locations.Now let’s use Web Grounding. I make a similar call to the Amazon Bedrock Converse API, but declare  as one of the tools available to the model.model_id = "us.amazon.nova-premier-v1:0" 
response = client.converse( 
    modelId=model_id, 
    messages=conversation, 
    toolConfig= {
          "tools":[ 
              {
                "systemTool": {
                   "name": "nova_grounding" # Enables the model to search real-time information
                 }
              }
          ]
     }
)After processing the response, I can see that the model used Web Grounding to access up-to-date information. The output includes reasoning traces that I can use to follow its thought process and see where it automatically queried external sources. The content of the responses from these external calls appear as  – a standard practice in AI systems that both protects sensitive information and helps manage output size.Additionally, the output also includes  objects containing information about the sources queried by Web Grounding.Finally, I can see the list of AWS Regions. It finishes with a message right at the end stating that “These are the most current and active AWS regions globally.”Web Grounding represents a significant step forward in making AI applications more reliable and current with minimum effort. Whether you’re building customer service chat assistants that need to provide up-to-date accurate information, developing research applications that analyze and synthesize information from multiple sources, or creating travel applications that deliver the latest details about destinations and accommodations, Web Grounding can help you deliver more accurate and relevant responses to your users with a convenient turnkey solution that is straightforward to configure and use.Amazon Nova Web Grounding is available now in US East (N. Virginia), US East (Ohio), and US West (Oregon).Currently, you can only use Web Grounding with Nova Premier but support for other Nova models will be added soon.10/30/25: Updated to all available regions. Original launch only in US East (N. Virginia).Matheus Guimaraes | @codingmatheus]]></content:encoded></item><item><title>TAP Developer Portals in Practice: A Deep Dive into Cloud-Native Productivity with Backstage</title><link>https://blog.devops.dev/tap-developer-portals-in-practice-a-deep-dive-into-cloud-native-productivity-with-backstage-bdb6d20bee32?source=rss----33f8b2d9a328---4</link><author>JIN</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:44 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CORS Busters: Quick Hacks for Local Dev (Chrome, Edge, Firefox, Safari)</title><link>https://blog.devops.dev/cors-busters-quick-hacks-for-local-dev-chrome-edge-firefox-safari-bd06bd93dc3a?source=rss----33f8b2d9a328---4</link><author>Raja Sekar Durairaj</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:34 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Automating Azure Infrastructure and Deployment with Terraform &amp; Ansible: Step-by-Step Guide</title><link>https://blog.devops.dev/automating-azure-infrastructure-and-deployment-with-terraform-ansible-step-by-step-guide-88bbe020418c?source=rss----33f8b2d9a328---4</link><author>Egwu Oko</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:25 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Automation is at the heart of modern DevOps. Manually creating cloud resources and configuring servers is time-consuming, error-prone, and non-repeatable.In this project, I built — a complete automated pipeline to deploy a : provisions the Azure infrastructure. installs and configures Nginx, builds the frontend app, deploys static files, and reloads the service automatically.This project demonstrates Infrastructure as Code (IaC), , and real-world automation practices.Step 1: Prepare the Project FolderCreate the project root directory and change to it.mkdir mini-finance && cd mini-financeCreate the Terraform and Ansible directoriesChange directory to the Terraform folder and create the Terraform filescd terraformtouch main.tf variables.tf outputs.tfChange to the ansible directory and create the ansible filescd ansibletouch inventory.ini site.ymlStep 2: Provision Azure Infrastructure with TerraformCreate a secure VM with networking and firewall rules, ready for deployment.Terraform Implementation StepsDefine main.tf file for resourcesprovider "azurerm" {  features {}resource "azurerm_resource_group" "rg" {  name     = var.resource_group_name  location = var.locationresource "azurerm_virtual_network" "vnet" {  name                = "${var.resource_group_name}-vnet"  address_space       = ["10.0.0.0/16"]  location            = azurerm_resource_group.rg.location  resource_group_name = azurerm_resource_group.rg.name}resource "azurerm_subnet" "subnet" {  name                 = "${var.resource_group_name}-subnet"  resource_group_name  = azurerm_resource_group.rg.name  virtual_network_name = azurerm_virtual_network.vnet.name  address_prefixes     = ["10.0.1.0/24"]}# NSG with rules for SSH and HTTP onlyresource "azurerm_network_security_group" "nsg" {  name                = "${var.resource_group_name}-nsg"  location            = azurerm_resource_group.rg.location  resource_group_name = azurerm_resource_group.rg.name}resource "azurerm_network_security_rule" "ssh" {  name                        = "Allow-SSH"  priority                    = 1001  direction                   = "Inbound"  access                      = "Allow"  protocol                    = "Tcp"  source_port_range           = "*"  destination_port_range      = "22"  source_address_prefix       = "*"  destination_address_prefix  = "*"  resource_group_name         = azurerm_resource_group.rg.name  network_security_group_name = azurerm_network_security_group.nsg.nameresource "azurerm_network_security_rule" "http" {  name                        = "Allow-HTTP"  priority                    = 1010  direction                   = "Inbound"  access                      = "Allow"  protocol                    = "Tcp"  source_port_range           = "*"  destination_port_range      = "80"  source_address_prefix       = "*"  destination_address_prefix  = "*"  resource_group_name         = azurerm_resource_group.rg.name  network_security_group_name = azurerm_network_security_group.nsg.nameresource "azurerm_public_ip" "pip" {  name                = "${var.resource_group_name}-pip"  location            = azurerm_resource_group.rg.location  resource_group_name = azurerm_resource_group.rg.name  allocation_method   = "Static"  #sku                 = "Basic"}resource "azurerm_network_interface" "nic" {  name                = "${var.resource_group_name}-nic"  location            = azurerm_resource_group.rg.location  resource_group_name = azurerm_resource_group.rg.name    name                          = "ipconfig1"    subnet_id                     = azurerm_subnet.subnet.id    private_ip_address_allocation = "Dynamic"    public_ip_address_id          = azurerm_public_ip.pip.id  }resource "azurerm_network_interface_security_group_association" "nsg_association" {  network_interface_id       = azurerm_network_interface.nic.id  network_security_group_id  = azurerm_network_security_group.nsg.id}# Ubuntu VM (azurerm_linux_virtual_machine)resource "azurerm_linux_virtual_machine" "vm" {  name                = "${var.resource_group_name}-vm"  resource_group_name = azurerm_resource_group.rg.name  location            = azurerm_resource_group.rg.location  size                = var.vm_size  admin_username      = var.admin_username  network_interface_ids = [azurerm_network_interface.nic.id]  disable_password_authentication = true    username   = var.admin_username    public_key = file(var.ssh_public_key)  }    caching              = "ReadWrite"    storage_account_type = "Standard_LRS"  }    publisher = "Canonical"    offer     = "0001-com-ubuntu-server-jammy"    sku       = "22_04-lts"  }    project = "mini-finance"}2. Define variables in variables.tfvariable "resource_group_name" {  description = "Name of the Azure Resource Group"}  description = "Azure region for resources"}variable "admin_username" {  description = "Admin username for Linux VMs"}variable "ssh_public_key" {  description = "Path to your SSH public key"}  description = "VM size"}3. Define the output in outputs.tf# Outputsoutput "public_ip" {  value = azurerm_public_ip.pip.ip_address}  value = var.admin_username  description = "SSH commands to access each VM"  value = "ssh ${var.admin_username}@${azurerm_public_ip.pip.ip_address}"}4. Define Terraform variables in terraform.tfvarslocation = "eastus"resource_group_name = "rg-mini-finance"admin_username = "azureuser"vm_size = "Standard_B1s"ssh_public_key = "~/.ssh/id_ed25519.pub"5. Provision the Azure Infrastructure by running Terraform commands:terraform initterraform planterraform apply -auto-approveStep 3: Configure Ansible InventoryPaste the following code into the inventory.ini[web]20.169.254.159 # Use the actual Public IP of your serveransible_user=azureuseransible_ssh_private_key_file=~/.ssh/id_ed25519Test connection to the server- Change directory to the Ansible folder- Run the following command to ping the serveransible all -i inventory -m pingStep 4: Prepare the Ansible multi-PlaybookOpen the site.yml and add the different playbooksInstall and configure nginx playbook---- name: Install and configure Nginx  become: yes    nginx_conf: /etc/nginx/sites-available/default    - name: Update apt cache        update_cache: yes    - name: Install required packages      apt:          - nginx        state: present    - name: Ensure Nginx is enabled and running      service:        state: startedInstalls required packages.Ensures Nginx is enabled and running.Deploy the static mini finance website playbook- name: Deploy static mini finance website  hosts: web  vars:    site_repo: "https://github.com/vincegwu/mini-finance-app.git"    tmp_dir: "/home/azureuser/mini-finance-app"    nginx_root: "/var/www/html"    - name: Remove default Nginx index        path: "{{ nginx_root }}/index.nginx-debian.html"      ignore_errors: yes    - name: Install Node.js and npm      apt:          - nodejs        state: present    - name: Clone static site into temporary directory (user-owned)      git:        dest: "{{ tmp_dir }}"    - name: Install npm dependencies      npm:        production: no    - name: Build the static app      command: npm run build        chdir: "{{ tmp_dir }}"    - name: Deploy built app to Nginx root      copy:        src: "{{ tmp_dir }}/build/"        dest: "{{ nginx_root }}/"        group: www-data        remote_src: yes    - name: Ensure proper ownership for Nginx root      file:        owner: www-data        recurse: yes      service:        state: reloadedRemoves default nginx indexClones static site into temporary directoryInstalls npm dependenciesDeploys built app to Nginx rootEnsures proper ownership for Nginx rootVerify Nginx and Deployment Playbook- name: Verify Nginx and deployment  hosts: web    - name: Check Nginx service status    - name: Display Nginx status      debug:        msg: "Nginx is {{ 'running' if services['nginx'].state == 'running' else 'not running' }}"    - name: Check homepage availability      uri:        return_content: yes    - name: Show homepage preview      debug:        msg: "{{ homepage.content | truncate(200) }}"Checks the Nginx service statusChecks Homepage availabilityRun the automation from the control nodeansible-playbook -i inventory.ini site.ymlVisit http://<VM Public_ip> — the Mini-Finance App home page should render fully.Pull down the infrastructure once done using Terraformcd terraformterraform destroy --auto-approve]]></content:encoded></item><item><title>We Lost Events in Production — Then I Discovered Kafka Transactions</title><link>https://blog.devops.dev/we-lost-events-in-production-then-i-discovered-kafka-transactions-db4851f12684?source=rss----33f8b2d9a328---4</link><author>Gaddam.Naveen</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:22 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A Minimal Feast Tutorial: Turning the California Housing Dataset into Features</title><link>https://blog.devops.dev/a-minimal-feast-tutorial-turning-the-california-housing-dataset-into-features-dbda10a0507b?source=rss----33f8b2d9a328---4</link><author>Okan Yenigün</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:20 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[From Raw Data to Real-Time ML: A Hands-On Guide with FeastFeast (short for Feature Store) is an open-source feature store for machine learning, originally developed by Google Cloud and GO-JEK.It acts as the bridge between models and data.In this post, we’ll briefly explore what Feast is, and later, we’ll use it to build projects.Before reading this post, you may want to check out my previous blog entry, where I introduced the concept of feature stores:To ground the concepts of a feature store without getting lost in infrastructure, let’s build a tiny, local Feast project around the California housing dataset.First, install the library:Let’s pull in the California housing dataset from scikit-learn and take a quick inventory of its shape and columns.from sklearn.datasets import fetch_california_housingdf = fetch_california_housing(as_frame=True).frameprint(f"Shape of the dataset: {df.shape}")print(f"Columns in the dataset: {df.columns.tolist()}")Shape of the dataset: (20640, 9)Columns in the dataset: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']"""Before we can register features in a Feast feature store, each record must be associated with a . Feast relies on these event timestamps to determine when a feature value was observed — crucial for maintaining point-in-time correctness during both training and serving.To prepare our dataset for that, let’s separate predictors and targets, then attach synthetic timestamps to each record.import pandas as pddf_predictors = df.drop(columns=["MedHouseVal"])df_target = df["MedHouseVal"]timestamps = pd.date_range(    end=pd.Timestamp.now(),     freq="D").to_frame(name="event_timestamp", index=False)df_predictors = pd.concat([timestamps, df_predictors], axis=1)df_target = pd.concat([timestamps, df_target], axis=1)To make our dataset fully compatible with Feast, we need an , a column that uniquely identifies each record (or entity) in the feature store.In a production setting, this could be something meaningful like a user_id, customer_id, or property_id. Since our California housing dataset doesn’t include a natural unique identifier, we can generate one ourselves using the DataFrame index.df_predictors.reset_index(drop=False, inplace=True)df_predictors.rename(columns={"index": "id"}, inplace=True)df_target.reset_index(drop=False, inplace=True)df_target.rename(columns={"index": "id"}, inplace=True)At this stage, we’re setting up the Feast feature repository, which will serve as the central configuration and management hub for all our features, entities, and data sources.This command creates a brand-new Feast project directory structure — essentially scaffolding the environment where you’ll define your feature store’s schema and operational logic.Feast automatically generates a folder (here named feature_repo) that contains the core components of a feature repository.feature_store.yaml: This is the global configuration file that tells Feast where to find your data and how to operate.example_repo.py: A starter example containing a sample Entity, FeatureView, and DataSource.data/ folder: A placeholder directory where you can store or point to your offline feature data (e.g., Parquet or CSV files).Feast’s design philosophy separates data engineering (how features are created and stored) from modeling (how features are consumed). The feature repository is where you define that boundary.Now that our Feast repository has been initialized, the next step is to provide it with offline data sources — datasets that define the historical values of our features.df_predictors.to_parquet("./feature_repo/feature_repo/data/predictors.parquet")df_target.to_parquet("./feature_repo/feature_repo/data/target.parquet")Now that our data is organized and stored in Parquet format, the next step is defining feature metadata — the schema, sources, and entities that Feast uses to understand and serve our features.Remove the code inside example_repo.py, and then:# feature_repo/example_repo.py    Project,    ValueType,    Field,)from feast.types import Float64, Int64from datetime import timedeltaproject = Project(name="feature_repo",                  description="A project for house prices prediction")house = Entity(name="id", value_type=ValueType.INT64,               description="house id")predictors_fv = FeatureView(    name="predictors",    ttl=timedelta(seconds=3600 * 1),    entities=[house],        Field(name="MedInc", dtype=Float64),        Field(name="HouseAge", dtype=Float64),        Field(name="AveRooms", dtype=Float64),        Field(name="AveBedrms", dtype=Float64),        Field(name="Population", dtype=Int64),        Field(name="AveOccup", dtype=Float64),        Field(name="Latitude", dtype=Float64),        Field(name="Longitude", dtype=Float64),    ],    source=FileSource(path=r"data/predictors.parquet",                      timestamp_field="event_timestamp"),    tags={"team": "house_price_prediction"},    name="target",    ttl=timedelta(seconds=3600 * 1),    entities=[house],        Field(name="MedHouseVal", dtype=Float64),    source=FileSource(path=r"data/target.parquet",                      timestamp_field="event_timestamp"),    online=True,    tags={"team": "house_price_prediction"},)First, we declared a Feast project — a logical grouping of entities, features, and sources.house = Entity(name="id", value_type=ValueType.INT64, description="house id")An Entity represents the unique key around which features are organized. In our case, each house record has a unique id (we created it earlier using reset_index()). Feast uses this to join feature tables during offline training and online serving.A Feature View tells Feast:What data to treat as featuresHow to link it to entitiesHow long those features remain fresh TTLpredictors_fv = FeatureView(    name="predictors",    ttl=timedelta(seconds=3600 * 1),    entities=[house],        Field(name="MedInc", dtype=Float64),        Field(name="HouseAge", dtype=Float64),        ...    source=FileSource(...),    tags={"team": "house_price_prediction"},schema: explicitly lists each feature and its data type.ttl: sets how long a feature remains valid.online=True: ensures this view can be materialized to an online store for low-latency inference.tags: metadata for discovery.Each FeatureView points to a FileSource, which specifies where Feast should read offline data from.Similarly, we define a smaller view for the target.After saving the file, apply the definitions to register them in Feast:This command takes Python definitions — the Entity, FeatureView, and FileSource objects we declared in example_repo.py — and materializes them into a structured, version-controlled registry.Applying changes for project feature_repo Created project feature_repo Created feature view predictors Created feature view target Created sqlite table feature_repo_predictors Created sqlite table feature_repo_targetThe message we saw means Feast has:Created a new project feature_repo. A logical namespace where all our entities, features, and sources live.Registered the entity id. This entity now acts as the unique key that binds our predictors and targets together across data sources.Created two FeatureViews, predictors and target. Each view has been added to Feast’s internal registry (stored under registry.db).Initialized an SQLite online store. These tables will serve as our online store — a lightweight, fast-access database for serving features during real-time predictions. Since our feature_store.yaml specifies SQLite as the default online store, Feast automatically creates the necessary tables.Now, we’ve reached the practical stage in our Feast workflow: generating and saving a training dataset from our feature store.Up until now, we’ve defined features, entities, and sources and registered them in the Feast registry.from feast import FeatureStorefrom feast.infra.offline_stores.file_source import SavedDatasetFileStoragestore = FeatureStore(repo_path="./feature_repo/feature_repo")entity_df = pd.read_parquet(path="./feature_repo/feature_repo/data/target.parquet")training_data = store.get_historical_features(    entity_df=entity_df,        "predictors:MedInc",        "predictors:AveRooms",        "predictors:Population",        "predictors:Latitude",    ]dataset = store.create_saved_dataset(    from_=training_data,    name="house_price_prediction_dataset",    storage=SavedDatasetFileStorage(path="./data/house_price_prediction_dataset.parquet"),First, we initialize the FeatureStore object, which gives us access to both offline (historical data) and online (real-time) retrieval capabilities.Feast needs an entity_df which acts as a query template that tells it:Which entities (via the id column) we want features for.When (via the event_timestamp column) those features should be valid.Each row provides context for point-in-time feature lookup.Feast will automatically ensure that no future data leakage occurs — it only pulls feature values that existed before the timestamp in each row.training_data = store.get_historical_features(    entity_df=entity_df,    features=[ ... list of predictor features ... ])Our target.parquet includes entity key and timestamp, but also the target value. Fest doesn’t care that entity_df has extra columns. All it needs are:event_timestamp (the event time)So, using the target DataFrame as entity_df is totally valid — it’s a convenient shortcut.What happens behind the scenes:Feast looks up the entity IDs id in the entity_df.It finds all matching feature records in the predictors FeatureView.It ensures each feature value’s event_timestamp is before the entity’s timestamp.It returns a merged dataset ready for model training.The result training_data is a Feast retrieval job object, which you can turn into a DataFrame by calling .to_df().Then, we convert the retrieval job result into a reusable saved dataset managed by Feast.dataset = store.create_saved_dataset(    from_=training_data,    name="house_price_prediction_dataset",    storage=SavedDatasetFileStorage(path="./data/house_price_prediction_dataset.parquet"),This stores it at the specified Parquet path. It also registers metadata about the dataset in the Feast registry.Now, let’s train the model.First, we will reopen the Feast project by pointing to our repo.store = FeatureStore(repo_path="./feature_repo/feature_repo")training_df = store.get_saved_dataset("house_price_prediction_dataset").to_df()That dataset already contains:We split the predictors and the target, and split again for evaluation.y = training_df["MedHouseVal"]X = training_df.drop(columns=["MedHouseVal", "event_timestamp", "id"])from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.2, random_state=42from sklearn.ensemble import RandomForestRegressormodel = RandomForestRegressor()model.fit(X_train, y_train)dump(model, "./feature_repo/feature_repo/data/model.joblib")Lastly, we serialized our model using joblib and saved it.Let’s move from “features on disk” to “features ready for real-time use.” To serve features at low latency, we’ll materialize our offline data (the Parquet files) into Feast’s online store (SQLite in this project).We’ll define a time window so Feast only loads rows whose event_timestamp falls within that range.from datetime import datetimestore = FeatureStore(repo_path="./feature_repo/feature_repo")store.materialize(start_date=datetime(2010, 1, 1), end_date=datetime.now())# Materializing 2 feature views from 2010-01-01 00:00:00+00:00 to 2025-10-21 19:57:54+00:00 into the sqlite online store.# predictors: is actually copying for serving.Feast scans our sources (our two Parquet files), picks all rows whose event_timestamp is between start_date and end_date, and upserts them into the online store. After this, those features are available for low-latency lookup by id.To wrap the workflow with real-time inference, let’s fetch the latest features for a specific house id from Feast’s online store and pass that feature vector to our trained model.feast_features = [    "predictors:MedInc",    "predictors:AveRooms",    "predictors:Population",    "predictors:Latitude",]df_features = store.get_online_features(    features=feast_features,    entity_rows=[{"id": 20637}],).to_df()#       id  Population  MedInc  AveOccup  AveRooms  Longitude  Latitude  \# 0  20637        1007     1.7  2.325635  5.205543    -121.22     39.43#    HouseAge  AveBedrmsentity_rows=[{"id": 20637}] asks Feast for the latest materialized features for that single entity..to_df() returns a tidy, single-row DataFrame that includes the id column plus all requested features.from joblib import loadmodel = load("./feature_repo/feature_repo/data/model.joblib")X = df_features.reindex(columns=model.feature_names_in_)print(preds)model.predict produces one scalar—the estimated median house value—for id=20637.We built a complete mini feature store using Feast to manage features for a house price prediction model — from raw data to real-time predictions.Starting with the California housing dataset, we split predictors and targets, added synthetic timestamps, and an id entity, and saved them as Parquet files. After initializing a Feast repo, we defined an Entity id and two Feature Views predictors and target, then applied them to register our features.Using Feast’s offline store, we created a historical training dataset with point-in-time correctness, trained a Random Forest Regressor, and saved the model.We then materialized the features into the online store and retrieved them by entity ID for real-time inference.The key learnings: every record needs an entity and timestamp, Feast guarantees no data leakage, and the same feature definitions power both training and serving — ensuring consistent, reproducible ML workflows from offline to online.]]></content:encoded></item><item><title>VoxScribe: A platform to test Opensource Speech-to-Text models</title><link>https://blog.devops.dev/voxscribe-a-platform-to-test-opensource-speech-to-text-models-70474a05c513?source=rss----33f8b2d9a328---4</link><author>Fraser sequeira</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:17 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[The views and opinions expressed in this blog post are my own and do not reflect the official position or views of Amazon Web Services (AWS)As a Solutions Architect at AWS, I’ve worked with customers across healthcare, contact centers, and enterprise applications who all share a common challenge:- speech-to-text(STT) at scale is expensive. Whether it’s HealthScribe for medical documentation, analyzing millions of call center recordings, or building voice-enabled applications, proprietary STT solutions quickly become a significant line item in the budget.I’ve watched customers hit cost ceilings with transcription services, because at scale, the per-minute pricing becomes prohibitive. A contact center processing 100,000 hours of calls monthly can easily spend $150,000+ on transcription alone.The open-source STT landscape has matured significantly. Models like Whisper, Voxtral, Parakeet, and Canary-Qwen now rival or exceed proprietary solutions in accuracy. But here’s the problem I kept seeing: customers wanted to evaluate these models for their specific use cases, but each engine has different dependencies, APIs, and setup requirements. Comparing models meant building custom infrastructure, managing version conflicts, and writing integration code, a weeks-long project before you even see results.So here is a lightweight platform 😊 a platform to test out opensource STT models. It’s a FastAPI backend with a lightweight HTML/JS frontend that lets you test multiple open-source STT models through a single interface. Upload your audio, select models, and compare transcriptions side-by-side. The platform handles dependency conflicts (yes, even the transformers version nightmare between Voxtral and NeMo models), manages model caching, and provides a clean API for integration.The Problem: Fragmented STT EcosystemIf you’ve ever worked with multiple speech recognition models, you know the pain:: Different models require conflicting library versions. Mistral’s Voxtral works great with the latest transformers, but then you try to add Parakeet which depends on NeMo that relies of a lower transformers model. Or even the latest Canary-Qwen-2.5B for which you need to directly build NeMo-toolkit from their GIT repo.: Each model has its own interface, making it difficult to compare results or switch between engines.: Installing CUDA drivers, managing Python environments, and debugging version conflicts can take hours or days. I have combined notes from multiple sources to make this process easier😏.Limited Comparison Tools*: Evaluating which model works best for your use case means building custom testing infrastructure.This OpenSource STT platform solves these challenges by managing version conflicts, providing you a playground to test out >5 STT models. Let get straight into deploying this solution. As they say the only way to learn something is to do it yourself. No worries if you aren’t a CUDA freak we’ve got you coveredWe started out with a G6.xlarge EC2 on AWS. This instance comprises of 4 VCPUs/16GB RAM and a single L4 Tensor core GPU with 16GB GPU memory. The latest pricing details for this instance can be found here. Select sufficient GP3 storage, in our case we went ahead with 100 GB GP3 storage.2. This instance should be launched in a public subnet with a security groups that allows inbound access on port 8000 and SSH access on port 22.3. Once you’ve launched your instance and its in running state, lets SSH into the system using the below command or you could use EC2 Instance Connect.The SSH approach: SSH from your local machine using the PEM key used while launching the EC2 instance.ssh -i <your-pem-file> -o "StrictHostKeyChecking=no" ec2-user@<your-ec2-public-ip>The EC2 Instance Connect approach: You could also login via the AWS console. However for the remainder of this blog we shall go ahead with the SSH approach as I’ve found the SSH approach to be more stable especially when our platform downloads larger models from the hub.NVIDIA GRID DRIVERS Installation4. Once within the EC2, we need to install the GRID drivers that gives us access to CUDA. Here are the installation steps the details of which can be found in this AWS documentation (Head to Option3).sudo dnf update -ysudo dnf install gcc make5. Once you hit reboot you will lose connectivity to your instance. Hence you would again need to SSH into the EC2 machine once its rebooted (around 30 seconds later).6. Lets continue with the GRID installation processsudo dnf install -y kernel-devel kernel-modules-extraaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .chmod +x NVIDIA-Linux-x86_64*.runsudo /bin/sh ./NVIDIA-Linux-x86_64*.run7. Verify whether the driver is functional. Here you should see the CUDA version. We are on version 13.8. Conda is an open-source package and environment manager for Python and other software that helps users create isolated environments with different package versions to avoid conflicts. It comes pre-compiled with binaries such as cdifflib required for Nemo on which Parakeet-v2 and Canary-Qwen-2.5B rely. It just makes dependency management way easier compared to PIP.9. Lets install CONDA through the following commandswget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.shAccept the license agreement (type `yes`)Confirm installation location (default is fine)Initialize Conda (type `yes` when prompted)10. Restart your shell(SSH Terminal, just as in Step 3) our source bashrc.11. You should now be able to verify the conda installation by firing the below command12. Now lets create a conda environment with python3.12. Conda will automatically install python3.12 for us. Lets also activate the environmentconda create -n stthub python=3.12conda activate stthub13. Lets install ffmpeg in our conda environment. ffmpeg is implicitly used by STT libraries to convert our audio files into compatible audio formats for the STT engine. It can also efficiently extract the audio track from a video file, isolating it for STT processing.GIT CLONE the STT Platform15. git clone into our VoxScribe projectgit clone https://github.com/Fraser27/VoxScribe.gitcd VoxScribe16. Lets install the project dependenciespip install -r requirements.txt17. Now lets start the VoxScribe app18. The app is now running on port 8000 and can be accessed from your public IPhttp://<your-public-ip>:8000The platform allows you to compare various STT models. The comparisons happen sequentially on the GPU-based instance.Solves our number one problem of cost. This self-hosted solution offers a handle on your costs which is crucial as your STT requirements scale.You can test multiple models in minutes instead of days.Make data-driven choices about which model to use in production.One codebase to update, one set of dependencies to manage.New models can be added without disrupting existing functionality.This is just the beginning. Future enhancements could include:Chunking large audio filesReal-time streaming transcription for live audioSpeaker diarization to identify different voicesLanguage detection and automatic model selectionIntegration with cloud storage (S3, GCS) for seamless workflowsThe platform is open source and ready to test. If you’re working with speech recognition models and want a cleaner way to evaluate them, give it a spin. I’m actively working on it and would value your feedback — especially around edge cases, performance bottlenecks, or additional models you’d like to see supported.Interested in testing it? Clone the repo, follow the setup instructions, and let me know what breaks or what could be better. PRs and issues welcome.Looking for beta testers and contributors. If you have audio samples that challenge STT models or ideas for improving the comparison workflow, I’d love to hear from you.*]]></content:encoded></item><item><title>Complete Guide: GitHub, Git, and Jenkins Integration on Windows</title><link>https://blog.devops.dev/complete-guide-github-git-and-jenkins-integration-on-windows-8822a83d4165?source=rss----33f8b2d9a328---4</link><author>Aravindcsebe</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:11 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[This comprehensive guide will walk you through setting up a complete CI/CD pipeline on Windows, from creating your first GitHub repository to automating builds with Jenkins. By the end of this tutorial, you’ll have a fully functional setup where pushing code to GitHub automatically triggers builds in Jenkins.Part 1: Installing Git on WindowsThe download should start automatically. If not, click on the appropriate version (64-bit or 32-bit)Save the installer file to your computerRun the downloaded .exe fileClick  through the license agreementChoose the installation location (default is usually fine: C:\Program Files\Git)Select components — keep the defaults selected:Windows Explorer integrationChoose the default editor (select your preferred editor or leave as Vim)Adjust your PATH environment — select “Git from the command line and also from 3rd-party software”Choose HTTPS transport backend — select “Use the OpenSSL library”Configure line ending conversions — select “Checkout Windows-style, commit Unix-style line endings”Configure terminal emulator — select Choose default behavior of git pull - select "Default (fast-forward or merge)"Choose credential helper — select Enable file system caching — check this optionStep 3: Verify InstallationOpen Command Prompt or PowerShell and type:You should see output like: git version 2.x.x.windows.xSet up your identity (this will be associated with your commits):git config --global user.name "Your Name"git config --global user.email "your.email@example.com"Verify your configuration:Part 2: Creating a GitHub AccountClick on  in the top right cornerEnter your email address and click Create a strong password and click Enter a username (this will be your GitHub handle) and click Choose whether you want to receive updates and click Solve the verification puzzleCheck your email for the verification codeEnter the 6-digit code sent to your emailStep 2: Personalize Your AccountAnswer the questions about your experience (or skip)Complete your profile by adding a profile picture and bio (optional)Part 3: Creating Your First RepositoryStep 1: Create a New RepositoryLog in to your GitHub accountClick the  icon in the top right cornerFill in the repository details:: my-jenkins-project: “Testing GitHub and Jenkins integration”: Select  (this is important for Jenkins integration without authentication) initialize with README, .gitignore, or license (we’ll add files later)Step 2: Note Your Repository URLAfter creation, you’ll see a page with setup instructions. Copy the repository URL, which looks like:https://github.com/YOUR-USERNAME/my-jenkins-project.gitPart 4: Creating a BranchStep 1: Clone the Repository LocallyOpen Command Prompt or PowerShell and navigate to where you want to store your project:cd C:\Users\YourUsername\Documentsgit clone https://github.com/YOUR-USERNAME/my-jenkins-project.gitStep 2: Create a New BranchStep 3: Switch to the New BranchOr create and switch in one command:git checkout -b developmentStep 4: Verify Your Current BranchThe active branch will be marked with an asterisk (*).Step 5: Push the Branch to GitHubgit push -u origin developmentThis creates the branch on GitHub and sets up tracking.Part 5: Creating a Shell Script (.sh File)Step 1: Create the Script FileIn your project directory, create a new file called build.sh:Open build.sh with any text editor (Notepad, VS Code, etc.) and add the following content:echo "=================================="echo "Starting Build Process"echo "=================================="echo "Build Date: $(date)"echo "Build Number: ${BUILD_NUMBER:-Manual}"echo "=================================="echo "Hello from the build script!"echo "This is version 1.0"echo "=================================="echo "Build completed successfully!"echo "==================================": Even though Windows uses PowerShell/CMD, the .sh file will be executed by Jenkins, which can run bash scripts.Save and close the editor.Part 6: Git Operations — Add, Commit, and PushSee what files have changed:You should see build.sh as an untracked file.Step 2: Add Files to Staginggit commit -m "Add initial build script"The -m flag adds a commit message describing your changes.Push your changes to the remote repository:git push origin developmentThis pushes the development branch to GitHub.Go to your GitHub repository in a browser and switch to the development branch using the branch dropdown. You should see your build.sh file.Part 7: Git Pull OperationsPulling Changes from RemoteWhen others make changes or you make changes from another location, you’ll need to pull updates:git pull origin developmentThis fetches and merges changes from the remote development branch.Before making changes, always pull first:git pull origin development# Make your changesgit commit -m "Your commit message"git push origin developmentPart 8: Installing and Setting Up Jenkins on WindowsUsing the Windows Installer:Run the downloaded .msi fileFollow the installation wizardJenkins will install as a Windows serviceThe installer will show you a path to the initial admin password, typically:(C:\Program Files\Jenkins\secrets\initialAdminPassword)Open this file with NotepadCopy the password and paste it into the web interfaceChoose Install suggested pluginsWait for the installation to complete (this may take several minutes)Step 6: Create First Admin UserFill in the form with your details:Step 7: Configure InstanceKeep the default Jenkins URL (http://localhost:8080)Part 9: Installing Required Jenkins PluginsStep 1: Access Plugin ManagerFrom the Jenkins dashboard, click Click the  tabStep 2: Install Git PluginIn the search box, type Check the box next to  (it might already be installed)Also search for and install:Click  (or Download now and install after restart)Wait for installation to completePart 10: Configuring Git in JenkinsStep 1: Configure Git PathGo to  → Global Tool ConfigurationPath to Git executable: C:\Program Files\Git\bin\git.exePart 11: Linking GitHub and JenkinsSince your repository is , you don’t need to set up credentials. Jenkins can access public repositories directly.Make sure your repository is set to  on GitHub:Under , verify visibility is “Public”Part 12: Creating a Freestyle Job in JenkinsFrom Jenkins dashboard, click Enter job name: GitHub-Build-JobStep 2: Configure Source Code ManagementIn the  section, select In , paste your GitHub repository URL:https://github.com/YOUR-USERNAME/my-jenkins-project.git: Leave as — (since the repo is public): Change from */master to */developmentDo NOT check any build triggers since you want to build manually or trigger manually.Step 4: Configure Build StepsScroll down to  sectionSelect Execute Windows batch commandIn the command box, enter:@echo offecho Running build script...echo.echo ================================echo Executing script contents:echo ================================bash build.shAlternative if Git Bash is in PATH:"C:\Program Files\Git\bin\bash.exe" build.shStep 5: Save ConfigurationClick  at the bottom of the page.Part 13: Testing the PipelineOn your job page, click You’ll see a build appear in the Click on the build number (e.g., )You should see the output from your build.sh scriptStep 2: Make Changes and PushOpen build.sh on your local machineModify the script (e.g., change the version number):echo "=================================="echo "Starting Build Process"echo "=================================="echo "Build Date: $(date)"echo "Build Number: ${BUILD_NUMBER:-Manual}"echo "=================================="echo "Hello from the build script!"echo "This is version 2.0 - NEW UPDATE!"echo "=================================="echo "Build completed successfully!"echo "=================================="git add build.shgit commit -m "Update build script to version 2.0"git push origin developmentStep 3: Trigger New Build in JenkinsClick on your job Click on the new build numberYou should see the NEW output with “version 2.0 — NEW UPDATE!”Commit and push to GitHubThe console output will reflect your latest changes!Part 14: Understanding the Console OutputWhen you check the console output in Jenkins, you’ll see:: Jenkins pulling the latest code from your repository: Your script running: Everything echoed from your build.sh file: Success or failure statusStarted by user adminRunning as SYSTEMBuilding in workspace C:\ProgramData\Jenkins\.jenkins\workspace\GitHub-Build-JobThe recommended git tool is: NONECloning the remote Git repositoryCloning repository https://github.com/YOUR-USERNAME/my-jenkins-project.git > git init C:\ProgramData\Jenkins\.jenkins\workspace\GitHub-Build-Job > git fetch --tags --force --progress -- https://github.com/YOUR-USERNAME/my-jenkins-project.git +refs/heads/*:refs/remotes/origin/* > git config remote.origin.url https://github.com/YOUR-USERNAME/my-jenkins-project.git > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/* > git rev-parse refs/remotes/origin/development^{commit}Checking out Revision abc123... (refs/remotes/origin/development) > git config core.sparsecheckout > git checkout -f abc123...Commit message: "Update build script to version 2.0"[GitHub-Build-Job] $ cmd /c call C:\Users\ADMINI~1\AppData\Local\Temp\jenkins123.batC:\ProgramData\Jenkins\.jenkins\workspace\GitHub-Build-Job>@echo offC:\ProgramData\Jenkins\.jenkins\workspace\GitHub-Build-Job>echo Running build script...Running build script...==================================Starting Build Process==================================Build Date: Wed Oct 15 10:30:45 2025==================================Hello from the build script!This is version 2.0 - NEW UPDATE!==================================Build completed successfully!==================================Part 15: Common Workflows and Tipscd my-jenkins-project # Edit your filesgit add . git commit -m "Descriptive message about changes"git push origin development: Click “Build Now” and check console outputSwitching Between Branches# View all branchesgit branch -a# Switch to a different branchgit checkout branch-name# Create and switch to new branchgit checkout -b new-feature-branch# Update your local repositorygit pull origin developmentgit log# or for a compact view: Git commands not recognized: Ensure Git is added to PATH during installation. Restart Command Prompt after installation.: Permission denied when pushing: Check your Git credentials. Run git config --list to verify.: Cannot access Jenkins at localhost:8080: Check if Jenkins service is running. Go to Services (services.msc) and look for Jenkins.: Build fails with “git command not found”: Configure Git path in Jenkins: Manage Jenkins → Global Tool Configuration → Git: Script doesn’t execute: Verify the bash path in your build step. Use full path: "C:\Program Files\Git\bin\bash.exe": Cannot push to repository: Verify repository URL and that you have permissions. Use git remote -v to check configured remotes.Congratulations! You’ve successfully:✅ Installed Git on Windows ✅ Created a GitHub account and repository ✅ Created and managed branches ✅ Created and edited shell scripts ✅ Performed Git operations (add, commit, push, pull) ✅ Installed and configured Jenkins ✅ Linked GitHub with Jenkins ✅ Created a Freestyle job without triggers ✅ Built your project and viewed console outputEvery time you push changes to your development branch and manually trigger a build in Jenkins, you'll see the updated output in the console. This forms the foundation of CI/CD practices!To enhance your setup, consider:: Learn to merge your development branch to main: Set up automatic builds when you push (requires webhook configuration): Explore Jenkins Pipeline for more complex workflows: Configure email or Slack notifications for build results: Add automated tests to your build processHappy coding and building! 🚀]]></content:encoded></item><item><title>How to Connect Spring Boot with PostgreSQL in Docker</title><link>https://blog.devops.dev/how-to-connect-spring-boot-with-postgresql-in-docker-a654fffdd717?source=rss----33f8b2d9a328---4</link><author>Aravindcsebe</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:51:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Run PostgreSQL in Docker and integrate it seamlessly with a Spring Boot applicationPostgreSQL is one of the most popular open-source databases — and pairing it with Spring Boot is a common choice for modern backend applications.In this guide, I’ll show you how to:Run PostgreSQL using DockerConnect a Spring Boot application (built with Maven) to the containerized databaseSet up everything for quick and clean local development🐳 Step 1: Pull and Run PostgreSQL with DockerWe’ll use the official PostgreSQL image from Docker Hub. If you don’t already have it locally, Docker will  it the first time you run the container.🔧 One-liner to pull and run:docker run --name my-postgres -e POSTGRES_USER=testuser -e POSTGRES_PASSWORD=testpass -e POSTGRES_DB=testdb -p 5433:5432 -d postgresImage pulled automatically: If the postgres image isn’t found locally, Docker will pull the latest version from Docker Hub: Named my-postgres: With user testuser, password testpass, and database testdb: Maps container’s 5432 to your machine's 5433You can verify the container is running:And if you want to pull the image manually beforehand:⚙️ Step 2: Configure Spring Boot (application.properties)Add the following to your src/main/resources/application.properties:spring.application.name=PostgresDockerDemospring.datasource.url=jdbc:postgresql://localhost:5433/testdbspring.datasource.username=testuserspring.datasource.password=testpassspring.datasource.driver-class-name=org.postgresql.Driverspring.jpa.hibernate.ddl-auto=updatespring.jpa.show-sql=truespring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialectThis configures Spring Boot to connect to the PostgreSQL container using the same credentials.📦 Step 3: Add PostgreSQL Driver to MavenIn your pom.xml, include: <dependencies>  <dependency>   <groupId>org.springframework.boot</groupId>   <artifactId>spring-boot-starter-data-jpa</artifactId>  <dependency>   <groupId>org.springframework.boot</groupId>   <artifactId>spring-boot-starter-web</artifactId>  <dependency>   <groupId>org.postgresql</groupId>   <artifactId>postgresql</artifactId>  </dependency>   <groupId>org.projectlombok</groupId>   <artifactId>lombok</artifactId>   <optional>true</optional>  <dependency>   <groupId>org.springframework.boot</groupId>   <artifactId>spring-boot-starter-test</artifactId>  </dependency>🧪 Step 4: Create a Sample Entity and Repository@Entity@Datapublic class User    @Id    @GeneratedValue(strategy = GenerationType.IDENTITY)    private Long id;    private String email;public interface UserRepository extends JpaRepository<User, Long> {}🌐 Step 5: Simple REST Controller@RestController@RequestMapping("/users")public class UserController {  private UserRepository userRepository;  public User create(@RequestBody User user) {    return userRepository.save(user);  }  public List<User> all() {    return userRepository.findAll();  }When you run the spring application you could see the below in console.Hibernate: create table users (id bigint generated by default as identity, email varchar(255), name varchar(255), primary key (id))Access PostgreSQL Inside the Docker Container (Optional)After your PostgreSQL container is up and running, you can connect to the database shell to inspect or insert data manually.🧩 Command to enter the container:docker exec -it my-postgres psql -U testuser -d testdbdocker exec -it runs a command inside your running containermy-postgres is the name of your containerpsql is the PostgreSQL CLI-U testuser connects with the user you set up-d testdb connects to the database you createdlet’s hit the endpoint and check.]]></content:encoded></item><item><title>GitOps Observability: From “It Works on My Machine” to “We Can Prove It Works Everywhere”</title><link>https://blog.devops.dev/gitops-observability-from-it-works-on-my-machine-to-we-can-prove-it-works-everywhere-a7896b8f0e08?source=rss----33f8b2d9a328---4</link><author>Salwan Mohamed</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:50:07 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Host your Helm repo using GitHub</title><link>https://blog.devops.dev/host-your-helm-repo-using-github-212ee44466cd?source=rss----33f8b2d9a328---4</link><author>Dejanu Alex</author><category>devops</category><pubDate>Tue, 28 Oct 2025 19:50:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[When you’re ready to share your charts, the preferred way to do so is by uploading them to a chart repository.TL;DR: A Helm Repository is HTTP server where packaged charts can be stored and shared. The repository consists of packaged charts and a special file called index.yaml (which contains an index of all of the charts in the repository).Create the repository structure in the root directory: mkdir {charts,packages} :GIT_REPO/├── charts/      # For your source chart directories└── packages/    # For packaged .tgz files and index.yaml3. Create a chart i.e. mychart: helm create charts/mychart (helm create creates a chart directory along with the common files and directories used in a chart).4. Package the mychart into a versioned chart archive file (basically creating the .tgz file for the chart under packages directory): helm package charts/mychart -d packageshelm repo indexwill read the packagesdirectory, and generate an index file based on the charts found and write the result to index.yaml (that lists all the charts in your repository along with their metadata).6. Configure your GitHub repo to serve static web pages (via GitHubPages) by configuring it to serve a particular branch (main in this case)To verify everything works, simply:# Add your repo (point to the location of index.yaml file)helm repo add helm_reponame https://your-username.github.io/gitrepo-name/packages# Update to fetch the latest indexhelm repo updatehelm repo list# Search for charts in your repohelm search repo helm_reponame]]></content:encoded></item><item><title>GitHub Adds Platform for Managing AI Agents Embedded in DevOps Workflows</title><link>https://devops.com/github-adds-platform-for-managing-ai-agents-embedded-in-devops-workflows/</link><author>Mike Vizard</author><category>devops</category><pubDate>Tue, 28 Oct 2025 18:56:21 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Securing the AI Era: How Development, Security, and Compliance Must Evolve</title><link>https://devops.com/securing-the-ai-era-how-development-security-and-compliance-must-evolve/</link><author>Sumeet Singh</author><category>devops</category><pubDate>Tue, 28 Oct 2025 18:29:14 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ASL pod</title><link>https://www.youtube.com/watch?v=A8o99is_L-k</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/A8o99is_L-k?version=3" length="" type=""/><pubDate>Tue, 28 Oct 2025 17:49:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Here is the ASL sign for pod.]]></content:encoded></item><item><title>Survey Surfaces Impact AI Coding Tools Are Having on DevOps Workflows</title><link>https://devops.com/survey-surfaces-impact-ai-coding-tools-are-having-on-devops-workflows/</link><author>Mike Vizard</author><category>devops</category><pubDate>Tue, 28 Oct 2025 15:21:04 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Amazon Nova Multimodal Embeddings: State-of-the-art embedding model for agentic RAG and semantic search</title><link>https://aws.amazon.com/blogs/aws/amazon-nova-multimodal-embeddings-now-available-in-amazon-bedrock/</link><author>Danilo Poccia</author><category>devops</category><pubDate>Tue, 28 Oct 2025 15:12:38 +0000</pubDate><source url="https://aws.amazon.com/blogs/aws/">AWS blog</source><content:encoded><![CDATA[Embedding models convert textual, visual, and audio inputs into numerical representations called embeddings. These embeddings capture the meaning of the input in a way that AI systems can compare, search, and analyze, powering use cases such as semantic search and RAG.Organizations are increasingly seeking solutions to unlock insights from the growing volume of unstructured data that is spread across text, image, document, video, and audio content. For example, an organization might have product images, brochures that contain infographics and text, and user-uploaded video clips. Embedding models are able to unlock value from unstructured data, however traditional models are typically specialized to handle one content type. This limitation drives customers to either build complex crossmodal embedding solutions or restrict themselves to use cases focused on a single content type. The problem also applies to mixed-modality content types such as documents with interleaved text and images or video with visual, audio, and textual elements where existing models struggle to capture crossmodal relationships eﬀectively.Nova Multimodal Embeddings supports a unified semantic space for text, documents, images, video, and audio for use cases such as crossmodal search across mixed-modality content, searching with a reference image, and retrieving visual documents.We evaluated the model on a broad range of benchmarks, and it delivers leading accuracy out-of-the-box as described in the following table.Nova Multimodal Embeddings supports a context length of up to 8K tokens, text in up to 200 languages, and accepts inputs via synchronous and asynchronous APIs. Additionally, it supports segmentation (also known as “chunking”) to partition long-form text, video, or audio content into manageable segments, generating embeddings for each portion. Lastly, the model oﬀers four output embedding dimensions, trained using Matryoshka Representation Learning (MRL) that enables low-latency end-to-end retrieval with minimal accuracy changes.Let’s see how the new model can be used in practice. Getting started with Nova Multimodal Embeddings follows the same pattern as other models in Amazon Bedrock. The model accepts text, documents, images, video, or audio as input and returns numerical embeddings that you can use for semantic search, similarity comparison, or RAG.Here’s a practical example using the AWS SDK for Python (Boto3) that shows how to create embeddings from different content types and store them for later retrieval. For simplicity, I’ll use Amazon S3 Vectors, a cost-optimized storage with native support for storing and querying vectors at any scale, to store and search the embeddings.Let’s start with the fundamentals: converting text into embeddings. This example shows how to transform a simple text description into a numerical representation that captures its semantic meaning. These embeddings can later be compared with embeddings from documents, images, videos, or audio to find related content.To make the code easy to follow, I’ll show a section of the script at a time. The full script is included at the end of this walkthrough.import json
import base64
import time
import boto3

MODEL_ID = "amazon.nova-2-multimodal-embeddings-v1:0"
EMBEDDING_DIMENSION = 3072

# Initialize Amazon Bedrock Runtime client
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")

print(f"Generating text embedding with {MODEL_ID} ...")

# Text to embed
text = "Amazon Nova is a multimodal foundation model"

# Create embedding
request_body = {
    "taskType": "SINGLE_EMBEDDING",
    "singleEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "text": {"truncationMode": "END", "value": text},
    },
}

response = bedrock_runtime.invoke_model(
    body=json.dumps(request_body),
    modelId=MODEL_ID,
    contentType="application/json",
)

# Extract embedding
response_body = json.loads(response["body"].read())
embedding = response_body["embeddings"][0]["embedding"]

print(f"Generated embedding with {len(embedding)} dimensions")Now we’ll process visual content using the same embedding space using a  file in the same folder as the script. This demonstrates the power of multimodality: Nova Multimodal Embeddings is able to capture both textual and visual context into a single embedding that provides enhanced understanding of the document.Nova Multimodal Embeddings can generate embeddings that are optimized for how they are being used. When indexing for a search or retrieval use case,  can be set to . For the query step,  can be set depending on the type of item to be retrieved. For example, when retrieving documents,  can be set to .# Read and encode image
print(f"Generating image embedding with {MODEL_ID} ...")

with open("photo.jpg", "rb") as f:
    image_bytes = base64.b64encode(f.read()).decode("utf-8")

# Create embedding
request_body = {
    "taskType": "SINGLE_EMBEDDING",
    "singleEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "image": {
            "format": "jpeg",
            "source": {"bytes": image_bytes}
        },
    },
}

response = bedrock_runtime.invoke_model(
    body=json.dumps(request_body),
    modelId=MODEL_ID,
    contentType="application/json",
)

# Extract embedding
response_body = json.loads(response["body"].read())
embedding = response_body["embeddings"][0]["embedding"]

print(f"Generated embedding with {len(embedding)} dimensions")To process video content, I use the asynchronous API. That’s a requirement for videos that are larger than 25MB when encoded as Base64. First, I upload a local video to an S3 bucket in the same AWS Region.aws s3 cp presentation.mp4 s3://my-video-bucket/videos/This example shows how to extract embeddings from both visual and audio components of a video file. The segmentation feature breaks longer videos into manageable chunks, making it practical to search through hours of content efficiently.# Initialize Amazon S3 client
s3 = boto3.client("s3", region_name="us-east-1")

print(f"Generating video embedding with {MODEL_ID} ...")

# Amazon S3 URIs
S3_VIDEO_URI = "s3://my-video-bucket/videos/presentation.mp4"
S3_EMBEDDING_DESTINATION_URI = "s3://my-embedding-destination-bucket/embeddings-output/"

# Create async embedding job for video with audio
model_input = {
    "taskType": "SEGMENTED_EMBEDDING",
    "segmentedEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "video": {
            "format": "mp4",
            "embeddingMode": "AUDIO_VIDEO_COMBINED",
            "source": {
                "s3Location": {"uri": S3_VIDEO_URI}
            },
            "segmentationConfig": {
                "durationSeconds": 15  # Segment into 15-second chunks
            },
        },
    },
}

response = bedrock_runtime.start_async_invoke(
    modelId=MODEL_ID,
    modelInput=model_input,
    outputDataConfig={
        "s3OutputDataConfig": {
            "s3Uri": S3_EMBEDDING_DESTINATION_URI
        }
    },
)

invocation_arn = response["invocationArn"]
print(f"Async job started: {invocation_arn}")

# Poll until job completes
print("\nPolling for job completion...")
while True:
    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)
    status = job["status"]
    print(f"Status: {status}")

    if status != "InProgress":
        break
    time.sleep(15)

# Check if job completed successfully
if status == "Completed":
    output_s3_uri = job["outputDataConfig"]["s3OutputDataConfig"]["s3Uri"]
    print(f"\nSuccess! Embeddings at: {output_s3_uri}")

    # Parse S3 URI to get bucket and prefix
    s3_uri_parts = output_s3_uri[5:].split("/", 1)  # Remove "s3://" prefix
    bucket = s3_uri_parts[0]
    prefix = s3_uri_parts[1] if len(s3_uri_parts) > 1 else ""

    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl
    # The output_s3_uri already includes the job ID, so just append the filename
    embeddings_key = f"{prefix}/embedding-audio-video.jsonl".lstrip("/")

    print(f"Reading embeddings from: s3://{bucket}/{embeddings_key}")

    # Read and parse JSONL file
    response = s3.get_object(Bucket=bucket, Key=embeddings_key)
    content = response['Body'].read().decode('utf-8')

    embeddings = []
    for line in content.strip().split('\n'):
        if line:
            embeddings.append(json.loads(line))

    print(f"\nFound {len(embeddings)} video segments:")
    for i, segment in enumerate(embeddings):
        print(f"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s")
        print(f"    Embedding dimension: {len(segment.get('embedding', []))}")
else:
    print(f"\nJob failed: {job.get('failureMessage', 'Unknown error')}")With our embeddings generated, we need a place to store and search them efficiently. This example demonstrates setting up a vector store using Amazon S3 Vectors, which provides the infrastructure needed for similarity search at scale. Think of this as creating a searchable index where semantically similar content naturally clusters together. When adding an embedding to the index, I use the metadata to specify the original format and the content being indexed.# Initialize Amazon S3 Vectors client
s3vectors = boto3.client("s3vectors", region_name="us-east-1")

# Configuration
VECTOR_BUCKET = "my-vector-store"
INDEX_NAME = "embeddings"

# Create vector bucket and index (if they don't exist)
try:
    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)
    print(f"Vector bucket {VECTOR_BUCKET} already exists")
except s3vectors.exceptions.NotFoundException:
    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)
    print(f"Created vector bucket: {VECTOR_BUCKET}")

try:
    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)
    print(f"Vector index {INDEX_NAME} already exists")
except s3vectors.exceptions.NotFoundException:
    s3vectors.create_index(
        vectorBucketName=VECTOR_BUCKET,
        indexName=INDEX_NAME,
        dimension=EMBEDDING_DIMENSION,
        dataType="float32",
        distanceMetric="cosine"
    )
    print(f"Created index: {INDEX_NAME}")

texts = [
    "Machine learning on AWS",
    "Amazon Bedrock provides foundation models",
    "S3 Vectors enables semantic search"
]

print(f"\nGenerating embeddings for {len(texts)} texts...")

# Generate embeddings using Amazon Nova for each text
vectors = []
for text in texts:
    response = bedrock_runtime.invoke_model(
        body=json.dumps({
            "taskType": "SINGLE_EMBEDDING",
            "singleEmbeddingParams": {
                "embeddingDimension": EMBEDDING_DIMENSION,
                "text": {"truncationMode": "END", "value": text}
            }
        }),
        modelId=MODEL_ID,
        accept="application/json",
        contentType="application/json"
    )

    response_body = json.loads(response["body"].read())
    embedding = response_body["embeddings"][0]["embedding"]

    vectors.append({
        "key": f"text:{text[:50]}",  # Unique identifier
        "data": {"float32": embedding},
        "metadata": {"type": "text", "content": text}
    })
    print(f"  ✓ Generated embedding for: {text}")

# Add all vectors to store in a single call
s3vectors.put_vectors(
    vectorBucketName=VECTOR_BUCKET,
    indexName=INDEX_NAME,
    vectors=vectors
)

print(f"\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!")This final example demonstrates the capability of searching across different content types with a single query, finding the most similar content regardless of whether it originated from text, images, videos, or audio. The distance scores help you understand how closely related the results are to your original query.# Text to query
query_text = "foundation models"  

print(f"\nGenerating embeddings for query '{query_text}' ...")

# Generate embeddings
response = bedrock_runtime.invoke_model(
    body=json.dumps({
        "taskType": "SINGLE_EMBEDDING",
        "singleEmbeddingParams": {
            "embeddingPurpose": "GENERIC_RETRIEVAL",
            "embeddingDimension": EMBEDDING_DIMENSION,
            "text": {"truncationMode": "END", "value": query_text}
        }
    }),
    modelId=MODEL_ID,
    accept="application/json",
    contentType="application/json"
)

response_body = json.loads(response["body"].read())
query_embedding = response_body["embeddings"][0]["embedding"]

print(f"Searching for similar embeddings...\n")

# Search for top 5 most similar vectors
response = s3vectors.query_vectors(
    vectorBucketName=VECTOR_BUCKET,
    indexName=INDEX_NAME,
    queryVector={"float32": query_embedding},
    topK=5,
    returnDistance=True,
    returnMetadata=True
)

# Display results
print(f"Found {len(response['vectors'])} results:\n")
for i, result in enumerate(response["vectors"], 1):
    print(f"{i}. {result['key']}")
    print(f"   Distance: {result['distance']:.4f}")
    if result.get("metadata"):
        print(f"   Metadata: {result['metadata']}")
    print()Crossmodal search is one of the key advantages of multimodal embeddings. With crossmodal search, you can query with text and find relevant images. You can also search for videos using text descriptions, find audio clips that match certain topics, or discover documents based on their visual and textual content. For your reference, the full script with all previous examples merged together is here:import json
import base64
import time
import boto3

MODEL_ID = "amazon.nova-2-multimodal-embeddings-v1:0"
EMBEDDING_DIMENSION = 3072

# Initialize Amazon Bedrock Runtime client
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")

print(f"Generating text embedding with {MODEL_ID} ...")

# Text to embed
text = "Amazon Nova is a multimodal foundation model"

# Create embedding
request_body = {
    "taskType": "SINGLE_EMBEDDING",
    "singleEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "text": {"truncationMode": "END", "value": text},
    },
}

response = bedrock_runtime.invoke_model(
    body=json.dumps(request_body),
    modelId=MODEL_ID,
    contentType="application/json",
)

# Extract embedding
response_body = json.loads(response["body"].read())
embedding = response_body["embeddings"][0]["embedding"]

print(f"Generated embedding with {len(embedding)} dimensions")
# Read and encode image
print(f"Generating image embedding with {MODEL_ID} ...")

with open("photo.jpg", "rb") as f:
    image_bytes = base64.b64encode(f.read()).decode("utf-8")

# Create embedding
request_body = {
    "taskType": "SINGLE_EMBEDDING",
    "singleEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "image": {
            "format": "jpeg",
            "source": {"bytes": image_bytes}
        },
    },
}

response = bedrock_runtime.invoke_model(
    body=json.dumps(request_body),
    modelId=MODEL_ID,
    contentType="application/json",
)

# Extract embedding
response_body = json.loads(response["body"].read())
embedding = response_body["embeddings"][0]["embedding"]

print(f"Generated embedding with {len(embedding)} dimensions")
# Initialize Amazon S3 client
s3 = boto3.client("s3", region_name="us-east-1")

print(f"Generating video embedding with {MODEL_ID} ...")

# Amazon S3 URIs
S3_VIDEO_URI = "s3://my-video-bucket/videos/presentation.mp4"

# Amazon S3 output bucket and location
S3_EMBEDDING_DESTINATION_URI = "s3://my-video-bucket/embeddings-output/"

# Create async embedding job for video with audio
model_input = {
    "taskType": "SEGMENTED_EMBEDDING",
    "segmentedEmbeddingParams": {
        "embeddingPurpose": "GENERIC_INDEX",
        "embeddingDimension": EMBEDDING_DIMENSION,
        "video": {
            "format": "mp4",
            "embeddingMode": "AUDIO_VIDEO_COMBINED",
            "source": {
                "s3Location": {"uri": S3_VIDEO_URI}
            },
            "segmentationConfig": {
                "durationSeconds": 15  # Segment into 15-second chunks
            },
        },
    },
}

response = bedrock_runtime.start_async_invoke(
    modelId=MODEL_ID,
    modelInput=model_input,
    outputDataConfig={
        "s3OutputDataConfig": {
            "s3Uri": S3_EMBEDDING_DESTINATION_URI
        }
    },
)

invocation_arn = response["invocationArn"]
print(f"Async job started: {invocation_arn}")

# Poll until job completes
print("\nPolling for job completion...")
while True:
    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)
    status = job["status"]
    print(f"Status: {status}")

    if status != "InProgress":
        break
    time.sleep(15)

# Check if job completed successfully
if status == "Completed":
    output_s3_uri = job["outputDataConfig"]["s3OutputDataConfig"]["s3Uri"]
    print(f"\nSuccess! Embeddings at: {output_s3_uri}")

    # Parse S3 URI to get bucket and prefix
    s3_uri_parts = output_s3_uri[5:].split("/", 1)  # Remove "s3://" prefix
    bucket = s3_uri_parts[0]
    prefix = s3_uri_parts[1] if len(s3_uri_parts) > 1 else ""

    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl
    # The output_s3_uri already includes the job ID, so just append the filename
    embeddings_key = f"{prefix}/embedding-audio-video.jsonl".lstrip("/")

    print(f"Reading embeddings from: s3://{bucket}/{embeddings_key}")

    # Read and parse JSONL file
    response = s3.get_object(Bucket=bucket, Key=embeddings_key)
    content = response['Body'].read().decode('utf-8')

    embeddings = []
    for line in content.strip().split('\n'):
        if line:
            embeddings.append(json.loads(line))

    print(f"\nFound {len(embeddings)} video segments:")
    for i, segment in enumerate(embeddings):
        print(f"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s")
        print(f"    Embedding dimension: {len(segment.get('embedding', []))}")
else:
    print(f"\nJob failed: {job.get('failureMessage', 'Unknown error')}")
# Initialize Amazon S3 Vectors client
s3vectors = boto3.client("s3vectors", region_name="us-east-1")

# Configuration
VECTOR_BUCKET = "my-vector-store"
INDEX_NAME = "embeddings"

# Create vector bucket and index (if they don't exist)
try:
    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)
    print(f"Vector bucket {VECTOR_BUCKET} already exists")
except s3vectors.exceptions.NotFoundException:
    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)
    print(f"Created vector bucket: {VECTOR_BUCKET}")

try:
    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)
    print(f"Vector index {INDEX_NAME} already exists")
except s3vectors.exceptions.NotFoundException:
    s3vectors.create_index(
        vectorBucketName=VECTOR_BUCKET,
        indexName=INDEX_NAME,
        dimension=EMBEDDING_DIMENSION,
        dataType="float32",
        distanceMetric="cosine"
    )
    print(f"Created index: {INDEX_NAME}")

texts = [
    "Machine learning on AWS",
    "Amazon Bedrock provides foundation models",
    "S3 Vectors enables semantic search"
]

print(f"\nGenerating embeddings for {len(texts)} texts...")

# Generate embeddings using Amazon Nova for each text
vectors = []
for text in texts:
    response = bedrock_runtime.invoke_model(
        body=json.dumps({
            "taskType": "SINGLE_EMBEDDING",
            "singleEmbeddingParams": {
                "embeddingPurpose": "GENERIC_INDEX",
                "embeddingDimension": EMBEDDING_DIMENSION,
                "text": {"truncationMode": "END", "value": text}
            }
        }),
        modelId=MODEL_ID,
        accept="application/json",
        contentType="application/json"
    )

    response_body = json.loads(response["body"].read())
    embedding = response_body["embeddings"][0]["embedding"]

    vectors.append({
        "key": f"text:{text[:50]}",  # Unique identifier
        "data": {"float32": embedding},
        "metadata": {"type": "text", "content": text}
    })
    print(f"  ✓ Generated embedding for: {text}")

# Add all vectors to store in a single call
s3vectors.put_vectors(
    vectorBucketName=VECTOR_BUCKET,
    indexName=INDEX_NAME,
    vectors=vectors
)

print(f"\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!")
# Text to query
query_text = "foundation models"  

print(f"\nGenerating embeddings for query '{query_text}' ...")

# Generate embeddings
response = bedrock_runtime.invoke_model(
    body=json.dumps({
        "taskType": "SINGLE_EMBEDDING",
        "singleEmbeddingParams": {
            "embeddingPurpose": "GENERIC_RETRIEVAL",
            "embeddingDimension": EMBEDDING_DIMENSION,
            "text": {"truncationMode": "END", "value": query_text}
        }
    }),
    modelId=MODEL_ID,
    accept="application/json",
    contentType="application/json"
)

response_body = json.loads(response["body"].read())
query_embedding = response_body["embeddings"][0]["embedding"]

print(f"Searching for similar embeddings...\n")

# Search for top 5 most similar vectors
response = s3vectors.query_vectors(
    vectorBucketName=VECTOR_BUCKET,
    indexName=INDEX_NAME,
    queryVector={"float32": query_embedding},
    topK=5,
    returnDistance=True,
    returnMetadata=True
)

# Display results
print(f"Found {len(response['vectors'])} results:\n")
for i, result in enumerate(response["vectors"], 1):
    print(f"{i}. {result['key']}")
    print(f"   Distance: {result['distance']:.4f}")
    if result.get("metadata"):
        print(f"   Metadata: {result['metadata']}")
    print()For production applications, embeddings can be stored in any vector database. Amazon OpenSearch Service offers native integration with Nova Multimodal Embeddings at launch, making it straightforward to build scalable search applications. As shown in the examples before, Amazon S3 Vectors provides a simple way to store and query embeddings with your application data. Nova Multimodal Embeddings offers four output dimension options: 3,072, 1,024, 384, and 256. Larger dimensions provide more detailed representations but require more storage and computation. Smaller dimensions offer a practical balance between retrieval performance and resource efficiency. This flexibility helps you optimize for your specific application and cost requirements.The model handles substantial context lengths. For text inputs, it can process up to 8,192 tokens at once. Video and audio inputs support segments of up to 30 seconds, and the model can segment longer files. This segmentation capability is particularly useful when working with large media files—the model splits them into manageable pieces and creates embeddings for each segment.The model includes responsible AI features built into Amazon Bedrock. Content submitted for embedding goes through Amazon Bedrock content safety filters, and the model includes fairness measures to reduce bias.As described in the code examples, the model can be invoked through both synchronous and asynchronous APIs. The synchronous API works well for real-time applications where you need immediate responses, such as processing user queries in a search interface. The asynchronous API handles latency insensitive workloads more efficiently, making it suitable for processing large content such as videos.If you’re using an AI–powered assistant for software development such as Amazon Q Developer or Kiro, you can set up the AWS API MCP Server to help the AI assistants interact with AWS services and resources and the AWS Knowledge MCP Server to provide up-to-date documentation, code samples, knowledge about the regional availability of AWS APIs and CloudFormation resources.Start building multimodal AI-powered applications with Nova Multimodal Embeddings today, and share your feedback through AWS re:Post for Amazon Bedrock or your usual AWS Support contacts.]]></content:encoded></item><item><title>Before learning Kubernetes, understand what a service architecture is.</title><link>https://blog.devops.dev/antes-de-aprender-kubernetes-entenda-o-que-e-uma-arquitetura-de-servicos-f7ad9160f4fc?source=rss----33f8b2d9a328---4</link><author>Prata</author><category>devops</category><pubDate>Tue, 28 Oct 2025 14:12:18 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>7 Proven Benefits of DevOps Implementation in Modern Software Development</title><link>https://devops.com/7-proven-benefits-of-devops-implementation-in-modern-software-development/</link><author>Albert Hilton</author><category>devops</category><pubDate>Tue, 28 Oct 2025 12:09:02 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>End-to-End Visibility: The Role of Observability in Frontend and Backend Systems</title><link>https://devops.com/end-to-end-visibility-observability-in-frontend-backend-systems/</link><author>Neel Shah</author><category>devops</category><pubDate>Mon, 27 Oct 2025 17:48:57 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>