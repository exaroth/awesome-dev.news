<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Early-Exit Levenshtein implementation</title><link>https://www.reddit.com/r/golang/comments/1iknb1u/earlyexit_levenshtein_implementation/</link><author>/u/bitreducer</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 8 Feb 2025 13:31:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I just released Early-Exit Levenshtein, a package for calculating the Levenshtein distance with an optional early-exit threshold. It’s based on agnivade’s implementation and behaves the same if no threshold is provided. When a threshold is set, the library stops as soon as the distance exceeds it, saving CPU time when comparing long strings. In my case, while checking for duplicate posts, I achieved over a 100x speedup. ]]></content:encoded></item><item><title>My experience so far with Rust as a complete Rust newbie</title><link>https://www.reddit.com/r/rust/comments/1ikn7g5/my_experience_so_far_with_rust_as_a_complete_rust/</link><author>/u/IFreakingLoveOranges</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 8 Feb 2025 13:25:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I’ve been a systems programmer for about 6 years, mostly using C/C++ and Java. I always wanted to try something new but kept putting it off. Finally I decided to give Rust a shot to see what all the hype was about.I’m still learning, and there’s definitely a lot more to explore, but after using Rust (casually) for about a month, I wanted to share my thoughts so far. And hopefully maybe get some feedback from more experienced Rust users.Coming from C/C++, having a package manager that "just works" feels amazing. Honestly, this might be my favorite thing about Rust.The first week was rough, I almost gave up. But once things clicked, I started feeling way more confident. And what I mean by "productive" is that feeling when you can just sit down and get shit done.Having a solid C background and a CS degree definitely helped, but I actually didn't struggle much with ownership/borrowing. It’s nice not having to worry about leaks every time I’m working with memory.Rust’s documentation is amazing. Not many languages have this level of high quality learning material. The Rust Book had everything I needed to get started.Things I Don’t Like About RustOkay, maybe this is just me being stuck in my OOP habits (aka skill issues), but Rust feels a little weird in this area. I get that Rust isn’t really an OOP language, but it’s also not fully functional either (at least from my understanding). Since it already has the  keyword, it feels like there was some effort to include OOP features. A lot of modern languages mix OOP and functional programming, and honestly I think having full-fledged classes and inheritance would make Rust more accessible for people like me.I haven’t looked into the details of how the Rust compiler works under the hood, but wow! some of these compile times are super painful, especially for bigger projects. Compared to C, it’s way slower. Would love to know why.All in all, my experience has been positive with Rust for the most parts, and I’m definitely looking forward to getting better at it.]]></content:encoded></item><item><title>How to set necessary permissions to use oidc from github actions to aws eks?</title><link>https://www.reddit.com/r/kubernetes/comments/1ikmlns/how_to_set_necessary_permissions_to_use_oidc_from/</link><author>/u/HumanResult3379</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 8 Feb 2025 12:52:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I want to run ,  and  in github actions workflow to operate kubernetes cluster in AWS EKS.If use AWS' OIDC, create a role for github actions, how many permissions are necessary to set?Also, is it okay just create an OIDC role in AWS? Is it necessary to create a service account in kubernetes to allow the operation from GitHub Actions?Is there a good example about this case?]]></content:encoded></item><item><title>Generating Voronoi Diagrams Using Fortune&apos;s Algorithm (With Odin)</title><link>https://redpenguin101.github.io/html/posts/2025_01_21_voronoi.html</link><author>redpenguin101</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 10:41:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>In a production environment how do you organise nodes?</title><link>https://www.reddit.com/r/kubernetes/comments/1ikkbwh/in_a_production_environment_how_do_you_organise/</link><author>/u/ReverendRou</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 8 Feb 2025 10:21:21 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[With all my learning not a great deal has been discussed with how you would actually allocate your nodes. I understand that concepts of taints/tolerations affinoties and so on. But in a real production environment what would a typical setup look like with nodes and applications. For example, if you have a Postgres Database, I imagine you would want a large node for the primary which is dedicated to this database And perhaps another node dedicated to a hot standby. What is the general guidance then with mixing different applications onto a single node. Is it just a case wanting to put applications onto their own nodes to enforce isolation and separation in the event of failure. For the most part, in my homelab, my only experience with kubernetes, it's just been a case of everything being on two nodes. And letting the scheduler place things ]]></content:encoded></item><item><title>fasterthanlime: The case for sans-io</title><link>https://www.youtube.com/watch?v=RYHYiXMJdZI</link><author>/u/Orange_Tux</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 8 Feb 2025 09:44:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I wrote a command line C compiler that asks ChatGPT to generate x86 assembly. Yes, it&apos;s cursed.</title><link>https://github.com/Sawyer-Powell/chatgcc</link><author>/u/sunmoi</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 04:26:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LINUX is obsolete (1992)</title><link>https://groups.google.com/g/comp.os.minix/c/wlhw16QWltI</link><author>talles</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 04:05:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Sorry, but I just can't resist this thread...:-)>In article <1992Jan29.2...@klaava.Helsinki.FI> torv...@klaava.Helsinki.FI (Linus Benedict Torvalds) writes:>>You use this [being a professor] as an excuse for the limitations of minix? >The limitations of MINIX relate at least partly to my being a professor:>An explicit design goal was to make it run on cheap hardware so students>could afford it.  In particular, for years it ran on a regular 4.77 MHZ PC>with no hard disk.  And an explicit design goal of Linux was to take advantage of the specialfeatures of the 386 architecture.  So what exactly is your point?  Differentdesign goals get you different designs.  You ought to know that.>You could do everything here including modify and recompile>the system.  Just for the record, as of about 1 year ago, there were two>versions, one for the PC (360K diskettes) and one for the 286/386 (1.2M).>The PC version was outselling the 286/386 version by 2 to 1.  I don't have>figures, but my guess is that the fraction of the 60 million existing PCs that>are 386/486 machines as opposed to 8088/286/680x0 etc is small.  Among studentsI find it very interesting that you claim here that Minix was designedprimarily for cheap hardware (in particular, the IBM PC/XT with no harddisk) and yet elsewhere have also mentioned the virtues of being portableacross hardware platforms.  Well, if you insist on designing the thingwith the lowest common denominator as your basis, that's fine, but ofcourse the end result will be less than pretty unless designed *very*>Making software free, but only for folks with enough money>to buy first class hardware is an interesting concept.Except that Linux was designed more for the purposes of the designer thananything else.  If I were writing an OS, I'd design it to suit myself, too.It's just that Linus was nice enough to share his code with the rest of us.>Of course 5 years from now that will be different, but 5 years from now >everyone will be running free GNU on their 200 MIPS, 64M SPARCstation-5.Maybe.  But by then, the 386/486 will probably be where the PC is now:everyone will have one and they'll be dirt cheap.  The timing will beabout right.  In which case Linux will fit right in, wouldn't you say?>>Re 2: your job is being a professor and researcher: That's one hell of a>>good excuse for some of the brain-damages of minix. I can only hope (and>>assume) that Amoeba doesn't suck like minix does.>Amoeba was not designed to run on an 8088 with no hard disk.Here's a question for you: as a general rule, when you go to design anoperating system, do you design it for specific capabilities and then runit on whatever hardware will do the job, or do you design it with thehardware as a target and fit the capabilities to the hardware?  With respectto Minix, it seems you did the latter, but I don't know whether or not youdid that with Amoeba.>>If this was the only criterion for the "goodness" of a kernel, you'd be>>right.  What you don't mention is that minix doesn't do the micro-kernel>>thing very well, and has problems with real multitasking (in the>>kernel).  If I had made an OS that had problems with a multithreading>>filesystem, I wouldn't be so fast to condemn others: in fact, I'd do my>>damndest to make others forget about the fiasco.>A multithreaded file system is only a performance hack.  Bull.  A multithreaded file system has a completely different design thana single-threaded file system and has different design criteria than asingle-threaded file system.>When there is only one job active, the normal case on a small PC, it buys>you nothing and adds complexity to the code.  If there is only going to be one job active anyway then *why bother withmultitasking at all*????If you're going to implement multitasking, then don't do a halfway jobof it.  On the other hand, if you're going to assume that there will beonly one job active anyway, then don't bother with multitasking (afterall, it *does* complicate things :-).>On machines fast enough to>support multiple users, you probably have enough buffer cache to insure a>hit cache hit rate, in which case multithreading also buys you nothing.  Maybe.  Multiple users means multiple things being done simultaneously.  Iwouldn't bet on the buffer cache buying you so much that multithreadingmakes no difference.  It's one thing if the users are doing somethingsimple, like editing a file.  It's another thing if they're compiling,reading news, or other things that touch lots of different files.>It is only a win when there are multiple processes actually doing real disk>I/O.  Which happens a *lot* when you're running multiple users.  Or when you'rea machine hooked up to the net and handling news traffic.>Whether it is worth making the system more complicated for this case is>at least debatable.Oh, come on.  How tough is it to implement a multi-threaded file system?All you need is a decent *buffered* (preferably infinitely so)message-passing system and a way to save your current state when you sendout a request to the device driver(s) to perform some work (and obviouslysome way to restore that state).  Minix has the second via the setjmp()/longjmp() mechanism, but lacks the former in a serious way.>I still maintain the point that designing a monolithic kernel in 1991 is>a fundamental error.  Not if you're trying to implement the system call semantics of Unix in areasonably simple and elegant way.>Be thankful you are not my student.  You would not>get a high grade for such a design :-)Why not?  What's this big thing against monolithic kernels?  There arecertain classes of problems for which a monolithic kernel is a moreappropriate design than a microkernel architecture.  I think implementingUnix semantics with a minimum of fuss is one such problem.Unless you can suggest an elegant way to terminate a system call uponreceipt of a signal from within a microkernel OS?>>The fact is that linux is more portable than minix.  What? I hear you>>say.  It's true - but not in the sense that ast means: I made linux as>>conformant to standards as I knew how (without having any POSIX standard>>in front of me).  Porting things to linux is generally /much/ easier>>than porting them to minix.>MINIX was designed before POSIX, and is now being (slowly) POSIXized as >everyone who follows this newsgroup knows.  Everyone agrees that user-level >standards are a good idea.  As an aside, I congratulate you for being able >to write a POSIX-conformant system without having the POSIX standard in front >of you. I find it difficult enough after studying the standard at great length.>My point is that writing a new operating system that is closely tied to any>particular piece of hardware, especially a weird one like the Intel line,>is basically wrong.  Weird as the Intel line may be, it's *the* most popular line, by severaltimes.  So it's not like it's *that* big a loss.  And Intel hardware isat least relatively cheap to come by, regardless of what your studentsmight tell you (why do you think they all own PCs?)...>An OS itself should be easily portable to new hardware>platforms.  As long as you don't sacrifice too much in the way of performance orarchitectural elegance in order to gain this.  Unfortunately, that's*exactly* what happened with Minix: in attempting to implement it onhardware of the lowest caliber, you ended up having to make designdecisions with respect to the architecture and implementation that havemade vintage Minix unusable as anything more than a personal toy operatingsystem.  For example: why didn't you implement a system call server asa layer between the file system and user programs?  My guess: you didn'thave enough memory on the target machine to do it.Put another way: you hit your original goal right on target, and are tobe applauded for that.  But in doing so, you missed a lot of othertargets that wouldn't have been hard to hit as well, with someconsideration of them.  I think.  But I wasn't there when you were makingthe decisions, so it's real hard for me to say for sure.  I'm speakingfrom hindsight, but you had the tough problem of figuring out what to doNow, *modified* Minix is usable.  Add a bigger buffer cache.  Modify itso that it can take advantage of 386 protected mode.  Fix the tty driverso that it will give you multiple consoles.  Fix the rs232 driver to dealwith DCD/DTR and do the right thing when carrier goes away.  Fix the pipesso that read and write requests don't fail just because they happen to bebigger than the size of a physical pipe.  Add shared text segments so youmaximize the use of your RAM.  Fix the scheduler so that it deals withcharacter I/O bound processes in a reasonable way.>When OS/360 was written in assembler for the IBM 360>25 years ago, they probably could be excused.  When MS-DOS was written>specifically for the 8088 ten years ago, this was less than brilliant, as>IBM and Microsoft now only too painfully realize. Yeah, right.  Just what hardware do you think they'd like to port DOS to,anyway?  I can't think of any.  I don't think IBM or Microsoft areregretting *that* particular aspect of DOS.  Rather, they're probablyregretting the fact that it was written for the address space providedMS-DOS isn't less than brilliant because it was written for one machinearchitecture.  It's less than brilliant because it doesn't do anythingwell, *regardless* of its portability or lack thereof.>Writing a new OS only for the>386 in 1991 gets you your second 'F' for this term.  But if you do real well>on the final exam, you can still pass the course.He made his code freely redistributable.  *You* didn't even do that.  Justfor that move alone, he scores points in my book.  Of course, thedistribution technology available to him is much better than what wasavailable when you did Minix, so it's hard to fault you for that...But I must admit, Minix is still one hell of a bargain, and I would neverhesitate to recommend it to anyone who wants to learn something about Unixand operating systems in general.  As a working operating system (i.e.,one intended for a multi-user environment), however, I'd hesitate torecommend it, except that there really aren't any good alternatives(except Linux, of course, at least tentatively.  I can't say for sure,since I haven't checked out Linux yet), since it doesn't have the performancecapabilities that a working operating system needs.]]></content:encoded></item><item><title>Go for backend, Nextjs for front end</title><link>https://www.reddit.com/r/golang/comments/1ikefo0/go_for_backend_nextjs_for_front_end/</link><author>/u/Terrible_Dimension66</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 8 Feb 2025 03:57:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I’m building an app that sends PDFs to Pinecone and calls OpenAI APIs. Thinking of using Next.js for the frontend and Golang for processing and API calls, but is it worth it, or should I stick with Node.js for simplicity?Also, are there any good tutorials on connecting Next.js with a Go backend? Googled but didn’t find much. Checked older threads here but no clear answer. Appreciate your help!]]></content:encoded></item><item><title>Docker Bake is Now Generally Available</title><link>https://www.docker.com/blog/ga-launch-docker-bake/</link><author>/u/h4l</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 03:43:09 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[We’re excited to announce the General Availability of  with Docker Desktop 4.38! This powerful build orchestration tool takes the hassle out of managing complex builds and offers simplicity, flexibility, and performance for teams of all sizes.Docker Bake is an orchestration tool that streamlines Docker builds, similar to how Compose simplifies managing runtime environments. With Bake, you can define build stages and deployment environments in a declarative file, making complex builds easier to manage. It also leverages BuildKit’s parallelization and optimization features to speed up build times.While Dockerfiles are excellent for defining image build steps, teams often need to build multiple images and execute helper tasks like testing, linting, and code generation. Traditionally, this meant juggling numerous  commands with their own options and arguments – a tedious and error-prone process.Bake changes the game by introducing a declarative file format that encapsulates all options and image dependencies, referred to as . Additionally, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.Challenges with complex Docker Build configuration:Managing long, complex build commands filled with countless flags and environment variables.Tedious workflows for building multiple images.Difficulty declaring builds for specific targets or environments.Requires a script or 3rd-party tool to make things manageableDocker Bake tackles these challenges with a better way to manage complex builds with a simple, declarative approach.Key benefits of Docker Bake: Replace complex chains of Docker build commands and scripts with a single  command while maintaining clear, version-controlled configuration files that are easy to understand and modify.: Express sophisticated build logic through HCL syntax and matrix builds, enabling dynamic configurations that adapt to different environments and requirements while supporting custom functions for advanced use cases.: Maintain standardized build configurations across teams and environments through version-controlled files and inheritance patterns, eliminating environment-specific build issues and reducing configuration drift.: Automatically parallelize independent builds and eliminate redundant operations through context deduplication and intelligent caching, dramatically reducing build times for complex multi-image workflows.: One simple Docker buildx bake command to replace all the flags and environment variables.Use cases for Docker Bake1. Monorepo and Image BakeryDocker Bake can help developers efficiently manage and build multiple related Docker images from a single source repository. Plus, they can leverage shared configurations and automated dependency handling to enforce organizational standards. Teams can maintain consistent build logic across dozens or hundreds of microservices in a single repository, reducing configuration drift and maintenance overhead. Shared base images and contexts are automatically deduplicated, dramatically reducing build times and storage costs. Enforce organizational standards through inherited configurations, ensuring all services follow security, tagging, and testing requirements. A single source of truth for build configurations makes it easier to implement organization-wide changes like base image updates or security patches.Docker Bake provides seamless compatibility with existing docker-compose.yml files, allowing direct use of your current configurations. Existing Compose users are able to get started using Bake with minimal effort. Teams can incrementally adopt advanced build features while still leveraging their existing compose workflows and knowledge. Use the same configuration for both local development (via compose) and production builds (via Bake), eliminating “works on my machine” issues.: Access powerful features like matrix builds and HCL expressions while maintaining compatibility with familiar compose syntax.: Seamlessly integrate with existing CI/CD pipelines that already understand compose files while adding Bake’s advanced build capabilities.3. Complex build configurationsCross-Platform Compatibility: Matrix builds enable teams to efficiently manage builds across multiple architectures, OS versions, and dependency combinations from a single configuration. HCL expressions allow builds to adapt to different environments, git branches, or CI variables without maintaining multiple configurations. Custom functions enable sophisticated logic for things like version calculation, tag generation, and conditional builds based on git history. Variable validation and inheritance ensure consistent configuration across complex build scenarios, reducing errors and maintenance burden. Groups and targets help organize large-scale build systems with dozens or hundreds of permutations, making them manageable and maintainable.With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.Enhanced Docker Build Cloud Performance: Instantly parallelize matrix builds across cloud infrastructure, turning hour-long build pipelines into minutes without managing build infrastructure. Leverage Build Cloud’s distributed caching and deduplication to dramatically reduce bandwidth usage and build times, which is especially valuable for remote teams. Save cost with DBC  Bake’s precise target definitions mean you only consume cloud resources for exactly what needs to be built. Teams can run complex multi-architecture builds without powerful local machines, enabling development from any device while maintaining build performance. Offload resource-intensive builds from CI runners to Build Cloud, reducing CI costs and queue times while improving reliability.What’s New in Bake for GA?Docker Bake has been an experimental feature for several years, allowing us to refine and improve it based on user feedback. So, there is already a strong set of ingredients that users love, such as targets and groups, variables, HCL Expression Support, inheritance capabilities, matrix targets, and additional contexts. With this GA release, Bake is now ready for production use, and we’ve added several enhancements to make it more efficient, secure, and easier to use:Deduplicated Context Transfers Significantly speeds up build pipelines by eliminating redundant file transfers when multiple targets share the same build context. Enhances security and resource management by providing fine-grained control over what capabilities and resources builders can access during the build process. Simplifies configuration management by allowing teams to define reusable attribute sets that can be mixed, matched, and overridden across different targets. Prevents wasted time and resources by catching configuration errors before the actual build process begins.Deduplicate context transfersWhen you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This can significantly impact build time, depending on your build configuration.Previously, the workaround required users to define a named context that loads the context files and then have each target reference the named context. But with Bake, this will be handled automatically now.Bake can automatically deduplicate context transfers from targets sharing the same context. When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This more efficient approach leads to much faster build time. Read more about how to speed up your build time in our docs. Bake now includes entitlements to control access to privileged operations, aligning with Build. This prevents unintended side effects and security risks. If Bake detects a potential issue — like a privileged access request or an attempt to access files outside the current directory — the build will fail unless explicitly allowed.To be consistent, the Bake command now supports the flag to grant access to additional entitlements. The following entitlements are currently supported for Bake.Build equivalents
 Allows executions with host networking.--allow security.insecure Allows executions without sandbox. (i.e. —privileged)File system: Grant filesystem access for builds that need access files outside the working directory. This will impact context, output, cache-from, cache-to, dockerfile, secret Grant read and write access to files outside the working directory. Grant read access to files outside the working directory.--allow fs.write=<path|*>  Grant write access to files outside the working directory.ssh
– Allows exposing SSH agent.Several attributes previously had to be defined in CSV (e.g. ). These were challenging to read and couldn’t be easily overridden. The following can now be defined as structured objects:target "app" {
		attest = [
			{ type = "provenance", mode = "max" },
			{ type = "sbom", disabled = true}
		]

		cache-from = [
			{ type = "registry", ref = "user/app:cache" },
			{ type = "local", src = "path/to/cache"}
		]

		cache-to = [
			{ type = "local", dest = "path/to/cache" },
		]

		output = [
			{ type = "oci", dest = "../out.tar" },
			{ type = "local", dest="../out"}
		]

		secret = [
			{ id = "mysecret", src = "path/to/secret" },
			{ id = "mysecret2", env = "TOKEN" },
		]

		ssh = [
			{ id = "default" },
			{ id = "key", paths = ["path/to/key"] },
		]
}As such, the attributes are now composable. Teams can mix, match, and override attributes across different targets which simplifies configuration management. target "app-dev" {
    attest = [
			{ type = "provenance", mode = "min" },
			{ type = "sbom", disabled = true}
		]
  }

  target "app-prod" {
    inherits = ["app-dev"]

    attest = [
			{ type = "provenance", mode = "max" },
		]
  }
Bake now supports validation for variables similar to Terraform to help developers catch and resolve configuration errors early. The GA for Bake also supports the following use cases.To verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the  block.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
To evaluate more than one condition, define multiple  blocks for the variable. All conditions must be true.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
  validation {
    condition = strlen(FOO) > 4
    error_message = "FOO must be longer than 4 characters."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
Dependency on other variablesYou can reference other Bake variables in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.variable "FOO" {}
variable "BAR" {
  validation {
    condition = FOO != ""
    error_message = "BAR requires FOO to be set."
  }
}

target "default" {
  args = {
    BAR = BAR
  }
}
In addition to updating the Bake configuration, we’ve added a new –list option. Previously, if you were unfamiliar with a project or wanted a reminder of the supported targets and variables, you would have to read through the file. Now, the list option will allow you to quickly query a list of them. It also supports the JSON format option if you need programmatic access.Quickly get a list of the targets available in your Bake configuration.docker buildx bake --list targetsdocker buildx bake --list type=targets,format=jsonGet a list of variables available for your Bake configuration.docker buildx bake --list variablesdocker buildx bake --list type=variables,format=jsonThese improvements build on a powerful feature set, ensuring Bake is both reliable and future-ready.Get started with Docker BakeReady to simplify your builds? Update to Docker Desktop 4.38 today and start using Bake. With its declarative syntax and advanced features, Docker Bake is here to help you build faster, more efficiently, and with less effort.Explore the documentation to learn how to create your first Bake file and experience the benefits of streamlined builds firsthand.Let’s bake something amazing together!]]></content:encoded></item><item><title>AnyOf&lt;L, R&gt; : Neither | Either&lt;L, R&gt; | Both&lt;L, R&gt;</title><link>https://www.reddit.com/r/rust/comments/1ikdp13/anyofl_r_neither_eitherl_r_bothl_r/</link><author>/u/OkResponsibility9677</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 8 Feb 2025 03:17:44 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[My first crate mature enough to talk about:.ℹ️ This library allows you to use the  type, which is a sum type of a product type of two types.ℹ️ It enables you to represent anything in a type-safe manner. It is an algebraic data type (on Wikipedia).✏️ Formally, it can be written as:AnyOf<L, R> = Neither | Either<L, R> | Both<L, R>✏️ The  and  types allow different combinations of types:Either<L, R> = Left(L) | Right(R)✏️ The traits , , , and  provide extensibility to the library.]]></content:encoded></item><item><title>Ghostwriter – use the reMarkable2 as an interface to vision-LLMs</title><link>https://github.com/awwaiid/ghostwriter</link><author>wonger_</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 03:02:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Starlink in the Falkland Islands – A national emergency situation?</title><link>https://www.openfalklands.com/february-2025-starlink-in-the-falkland-islands-a-national-emergency-situation/</link><author>pelagicAustral</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 02:41:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In yesterday’s (Thursday) OpenFalklands blog, I noted that while a week may seem long in politics, even a single day can feel like an eternity when it comes to Starlink developments in the Falklands.And then, ‘Black Friday’ arrived on Friday, 7th of February!This morning, some Falkland Islands Starlink users began experiencing service terminations as their 60-day roaming period expired, following Starlink’s Terms & Conditions.Illegal use of Starlink in the Falkland Islands.Regardless of differing opinions, it is now an established fact that hundreds of Starlink terminals are in use in the Falkland Islands. The underlying reasons for this widespread adoption have been discussed extensively elsewhere and will not be reiterated here.The high level of Starlink usage sparked a successful petition backed by 70% of the island’s population. This petition demanded both a reduction of the £5,400 FIG VSAT licence fee and formal approval for Starlink’s operation in the Falkland Islands.In response, a Starlink Select Committee – comprising all of the island’s MLAs – convened from July to October 2024. The committee formally endorsed the petition’s demands, and the proposal was subsequently forwarded to the Falkland Islands Government (FIG) for implementation. However, the effective date for this approval has now been delayed until April.Because Sure International holds an exclusive monopoly telecommunications licence, Starlink’s use in the islands is currently illegal. Nonetheless, this restriction has not prevented the widespread installation of hundreds of Starlink terminals, which remain unlicensed.At this time, Starlink has not received an official government statement endorsing its use in the Falkland Islands – a prerequisite for adding the territory to Starlink’s approved list of service areas. As a result, using Starlink in the islands continues to be illegal and is considered a criminal offence.The £5,400 FIG VSAT licenceIn addition to the hundreds of unlicensed Starlink terminals operating on the islands, two groups of users are deemed “legal” by FIG – even though Starlink has not yet received formal permission to provide service in the Falkland Islands.Starlink’s low Earth orbit (LEO) satellites are classified as very small aperture terminal (VSAT), a contentious categorisation. This definition was partly adopted to prevent islanders from self-provisioning internet services, thereby protecting Sure International’s telecommunications monopoly. Notably, the original legislation included an exemption for Falkland Islands government entities, allowing them to use Starlink without requiring a VSAT licence.In 2024, several government departments took advantage of this exemption to install Starlink terminals – a move that garnered widespread support. Among the early adopters were FIG’s IT department and the Falkland Islands Development Corporation (FIDC). The KEMH is ‘earmarked’ for Starlink use, but it has yet to be installed.That same year, rising demand from individuals and local businesses prompted the Communications Regulator to issue a limited number of FIG VSAT licences. Although the exact number remains undisclosed outside of government circles, estimates suggest that approximately fourteen licences were granted, including, among others, most of the island’s fishing companies, the inter-island ferry, and International Tours & Travel the local agent for LATAM flights.It is important to emphasize that until FIG formally permits Starlink to operate in the Falkland Islands, any use of Starlink remains officially illegal. Consequently, neither of these two user groups are operating within a fully legal framework no differently than the hundreds of consumer ‘illegal’ users.Questionable FIG-issued VSAT licences effectively permit Starlink usage from a FIG perspective, even though the service is still legally classified as a criminal act.Starlink users who have paid £5,400 for a FIG VSAT licence are not, in any way, in a beneficial position compared to the hundreds of  ‘illegal’ consumer users and cannot be considered to be so.A message from the Communications Regulator was distributed this morning.I’ve had reports this morning that some users have had their Starlink services disabled. If you have got back to me already with the below information, then this has already been passed to a team at Starlink who I understand will work to keep the service on. If you haven’t got back to me with the information below, then please do so asap.   I hope that you haven’t lost access and please be assured I am working to address this. So sorry for any inconvenience caused – which I am sure would be significant!   Many thanks – and do give me a call if you need!”There is currently no ethical or legal basis for Starlink to differentiate between supplying service to users holding FIG VSAT licences and the hundreds of unlicensed users on the Falkland Islands.Requesting such differentiation is highly controversial. It would force Starlink into an ethically and legally precarious position – essentially endorsing a select group of users while its service remains a criminal offence under Falkland Islands law. In this context, holding a FIG VSAT licence does not change the legal status of Starlink usage.An emergency needs emergency actions.The sudden shutdown of Starlink services clearly qualifies as a National Emergency due to the widespread and unforeseen consequences such an action would have. Moreover, the government finds itself in a difficult position concerning holders of a FIG VSAT licence as they are threatened too.For instance, if Sure Falkland Islands Intelsat satellite(s) were damaged by space debris – thereby cutting off the islands’ communications – the situation would undoubtedly be classified as a National Emergency.Allowing the current situation to continue until April seems untenable, especially since many more Starlink users risk losing service as their roaming periods expire in the coming months. This issue could be resolved in a number of days if a National Emergency were declared.During the COVID-19 crisis in 2020, a National Emergency was declared within half a day, demonstrating how quickly decisive action can be taken when circumstances demand it. “Nothing contained in or done under the authority of a law shall be held to be inconsistent with or in contravention of any of the provisions of this Chapter, except for sections 2, 3, 4, 6(2)(a), 6(5), 6(6), 6(7) and 6(…), to the extent that the law authorises the taking of measures during any period of public emergency that are reasonably justifiable for addressing the situation in the Falkland Islands during that period.”It is important to note that the declaration of a National Emergency can only be made by the Falkland Islands Government following an emergency meeting of the EXCO.Surely, it must be to everyone’s benefit to get this situation resolved as soon as possible by permitting Starlink to operate in the Falkland Islands now?Sure’s post this morning.Chris Gare, OpenFalklands February 2025, copyright OpenFalklands]]></content:encoded></item><item><title>LinuxPlay – A Fast, Open-Source Remote Desktop for Linux</title><link>https://www.reddit.com/r/linux/comments/1ikc564/linuxplay_a_fast_opensource_remote_desktop_for/</link><author>/u/Techlm77</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 01:58:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I've been working on LinuxPlay, a low-latency, fully open-source remote desktop solution designed specifically for Linux. Unlike VNC or X2Go, LinuxPlay uses hardware-accelerated video streaming and adaptive bitrate control, making it much smoother and more responsive.Ultra-low latency with UDP multicast streamingFull keyboard and mouse support, including function keys and shortcutsAdaptive bitrate streaming to adjust based on network conditionsNo cloud or accounts required, works entirely over LANClipboard sharing between host and clientCompletely open-source (MIT licensed)Would appreciate feedback from other Linux users. Let me know what you think or if there's anything you’d like to see added. GitHub Would appreciate feedback from other Linux users. Let me know what you think or if there's anything you’d like to see added.If you are interested in how does this software work, feel free to read it at my website.]]></content:encoded></item><item><title>VSCode’s SSH agent is bananas</title><link>https://fly.io/blog/vscode-ssh-wtf/</link><author>zdyxry</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 01:25:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We’re interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they’re using forks of VSCode that generate code with LLMs. ”hallucination” is what we call it when LLMs get code wrong; “engineering” is what we call it when people do.LLM-generated code is useful in the general case if you know what you’re doing. But it’s ultra-useful if you can close the loop between the LLM and the execution environment (with an “Agent” setup). There’s lots to say about this, but for the moment: it’s a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates. So, obviously, the issue here is you don’t want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they’ll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you’d really like to be able to do: run a closed-loop agent-y (“agentic”? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can’t screw you over in any way. You get where we’re going with this.Anyways! I would like to register a concern.Emacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called “Tramp”. If you can hook Tramp up to any kind of interactive environment — usually, an SSH session — where it can run Bourne shell commands, it can extend Emacs to that environment.So, VSCode has a feature like Tramp. Which, neat, right? You’d think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.Unlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node. The agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:Wander around the filesystem
Launch its own shell PTY processes
In security-world, there’s a name for tools that work this way. I won’t say it out loud, because that’s not fair to VSCode, but let’s just say the name is murid in nature.I would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production. It turns out we don’t have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we’ve decided to just be a blog again, so: we had to learn this, and now you do too.]]></content:encoded></item><item><title>Google&apos;s 7-Year Slog To Improve Chrome Extensions Still Hasn&apos;t Satisfied Developers</title><link>https://developers.slashdot.org/story/25/02/07/2246202/googles-7-year-slog-to-improve-chrome-extensions-still-hasnt-satisfied-developers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Sat, 8 Feb 2025 01:25:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[The Register's Thomas Claburn reports: Google's overhaul of Chrome's extension architecture continues to pose problems for developers of ad blockers, content filters, and privacy tools. [...] While Google's desire to improve the security, privacy, and performance of the Chrome extension platform is reasonable, its approach -- which focuses on code and permissions more than human oversight -- remains a work-in-progress that has left extension developers frustrated.
 
Alexei Miagkov, senior staff technology at the Electronic Frontier Foundation, who oversees the organization's Privacy Badger extension, told The Register, "Making extensions under MV3 is much harder than making extensions under MV2. That's just a fact. They made things harder to build and more confusing." Miagkov said with Privacy Badger the problem has been the slowness with which Google addresses gaps in the MV3 platform. "It feels like MV3 is here and the web extensions team at Google is in no rush to fix the frayed ends, to fix what's missing or what's broken still." According to Google's documentation, "There are currently no open issues considered a critical platform gap," and various issues have been addressed through the addition of new API capabilities.
 
Miagkov described an unresolved problem that means Privacy Badger is unable to strip Google tracking redirects on Google sites. "We can't do it the correct way because when Google engineers design the [chrome.declarativeNetRequest API], they fail to think of this scenario," he said. "We can do a redirect to get rid of the tracking, but it ends up being a broken redirect for a lot of URLs. Basically, if the URL has any kind of query string parameters -- the question mark and anything beyond that -- we will break the link." Miagkov said a Chrome developer relations engineer had helped identify a workaround, but it's not great. Miagkov thinks these problems are of Google's own making -- the company changed the rules and has been slow to write the new ones. "It was completely predictable because they moved the ability to fix things from extensions to themselves," he said. "And now they need to fix things and they're not doing it."]]></content:encoded></item><item><title>Television 0.10</title><link>https://www.reddit.com/r/linux/comments/1ik99on/television_010/</link><author>/u/damien__f1</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 23:40:46 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Wanted to share with my super not interesting findings. Both linux and windows have nearly identical power consumption.</title><link>https://www.reddit.com/r/linux/comments/1ik7tg7/wanted_to_share_with_my_super_not_interesting/</link><author>/u/stolasdick</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 22:36:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Calico SNAT - how to specify the source interface</title><link>https://www.reddit.com/r/kubernetes/comments/1ik79kd/calico_snat_how_to_specify_the_source_interface/</link><author>/u/Wrong_username_404</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 7 Feb 2025 22:12:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hey all! I'm struggling to get SNAT setup correctly in a test cluster. I have 3 worker nodes running Alma9 with 2 interfaces each:10.1.1.X bond0 - 1G management network10.1.2.X bond1 - 10G data networkI was able to get the pod-to-pod traffic working correctly by setting the node-ip in the kubelet startup on each host :echo 'KUBELET_EXTRA_ARGS="--node-ip=10.1.2.225"' > /etc/sysconfig/kubeletand patching calico's nodeAddressAutodectionV4:kubectl patch installation.operator.tigera.io default --type merge --patch '{"spec":{"calicoNetwork":{"nodeAddressAutodetectionV4":{"cidr": "10.1.2.0/24"}}}}'kubectl shows each node with the IP from the 10G interface:kube44.ord Ready 19d v1.32.0 10.1.2.224 kube45.ord Ready 115m v1.32.0 10.1.2.225 kube46.ord Ready 15d v1.32.1 10.1.2.226 And ip routes are being set correctly on the host:10.45.115.0/26 via 10.1.2.226 dev tunl0 proto bird onlink 10.45.117.64/26 via 10.1.2.225 dev tunl0 proto bird onlink 10.45.145.64/26 via 10.1.2.224 dev tunl0 proto bird onlink But when I try to ping a resource outside of the cluster, it's grabbing the address on 1G connection:[kube45.grr ~]# tcpdump -i bond0 -n | grep 154.33 14:17:22.059449 IP 10.1.1.225 > 172.16.154.33: ICMP echo request, id 29199, seq 1, length 64 Anyone know what I'm missing? I saw the option for natOutgoingAddress but that doesn't seem to be node-specific. ]]></content:encoded></item><item><title>kubelet did not evict pods under node memory pressure condition</title><link>https://www.reddit.com/r/kubernetes/comments/1ik69cs/kubelet_did_not_evict_pods_under_node_memory/</link><author>/u/0x4ddd</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 7 Feb 2025 21:29:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Under specific conditions one of our pods was undergoing memory leak so it was consuming more and more memory and finally node reached 100% memory usage according to our node level metrics.Due to memory pressure kubelet in the end stopped responding (most likely) as node was reported as NotReady. 100% memory usage rendered node unstable and it wasn't able to come up online back again in reasonable time (we waited 10 minutes) so we needed to manually restart virtual machine.We could and probably should have set container level memory limit so kubelet restarts it sooner but why it didn't kill pod when hard eviction threshold was reached? Do you have any ideas? Maybe default value of 100Mb is too low and kubelet simply stopped responding before being able to evict pod?]]></content:encoded></item><item><title>Show HN: ExpenseOwl – Simple, self-hosted expense tracker</title><link>https://github.com/Tanq16/ExpenseOwl</link><author>import-base64</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 20:56:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[media] cargo run rust projects with vscode&apos;s debugger attached using cargo-debugger!</title><link>https://www.reddit.com/r/rust/comments/1ik5gf6/media_cargo_run_rust_projects_with_vscodes/</link><author>/u/jkelleyrtp</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 7 Feb 2025 20:55:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>UK demands backdoor for encrypted Apple user data...</title><link>https://www.youtube.com/watch?v=ozkg_iW9mNU</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/ozkg_iW9mNU?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 20:20:21 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.

The United Kingdom is demanding Apple build a backdoor to access encrypted iCloud user data. Learn how end-to-end-encryption works and other tools that protect your digital privacy. 

#apple #tech #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Mainstream Source https://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/
Full Cryptography Tutorial https://youtu.be/NuyzuNBFWxQ
Apple Intelligence gone Wild https://youtu.be/7rXgVsIGvGQ
Tails OS in 100 Seconds https://youtu.be/mVKAyw0xqxw

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why does the UK want Apple iCloud data?
- What is a double rachet algorithm?
- How does signal e2ee protocol work?
- How do I keep browsing data private?
- Best Linux distro for privacy]]></content:encoded></item><item><title>Simple log/slog tracing: when OpenTelemetry is too much</title><link>https://www.reddit.com/r/golang/comments/1ik42qn/simple_logslog_tracing_when_opentelemetry_is_too/</link><author>/u/lzap</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 7 Feb 2025 19:57:11 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Simple code instrumentation.Serialization into  (structured logging library).Handler with multiple sub-handlers for further processing.Simple exporting slog handler for callback-based exporters.Adding "trace_id" root field to all logs for easy correlation between logs and traces.Instrumentation is very similar:span, ctx := strc.Start(ctx, "span name") defer span.End() I found OpenTelemetry way too big for the job, the API breaks every so often and we were only interested in tracing capabilities. Also in our use case, we do not do a lot of tracing so there is no need for a proper UI tracing server infrastructure, database, collectors and agents. So I wrote this plus a simple CLI tool for quick inspection.I thought I would share it, might be interesting for those who already integrated log/slog into their application and would like to have a very liitle tracing capability. The idea is that there could be a slog handler for OpenTelemetry if there is a need for that in the future. If anyone finds this useful let me know and I can extract this into a separate github repo, it is part of our team common logging utilities right now.]]></content:encoded></item><item><title>Do-nothing scripting: the key to gradual automation (2019)</title><link>https://blog.danslimmon.com/2019/07/15/do-nothing-scripting-the-key-to-gradual-automation/</link><author>tehnub</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 19:48:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Every ops team has some manual procedures that they haven’t gotten around to automating yet. Toil can never be totally eliminated.Very often, the biggest toil center for a team at a growing company will be its procedure for modifying infrastructure or its procedure for provisioning user accounts. Partial instructions for the latter might look like this:Create an SSH key pair for the user.Commit the public key to Git and push to master.Wait for the build job to finish.Find the user’s email address in the employee directory.Send the user their private key via 1Password.This is a relatively short example. Sometimes there are 20 steps in the process. Sometimes there are branches and special cases to keep track of as you go. Over time, these procedures can become unmanageably large and complex.Procedures like this are frustrating because they’re focus-intensive yet require very little thought. They demand our full attention, but our attention isn’t rewarded with interesting problems or satisfying solutions – just another checkbox checked. I have a word for a procedure like this: a .We know that this procedure is ripe for automation. We can easily see how to automate any given step. And we know that a computer could carry out the instructions with far greater speed and accuracy than we can, and with less tendency toward practical drift.However, automating slogs sometimes feels like an all-or-nothing proposition. Sure, we could write a script to handle step 2, or step 5. But that wouldn’t  make the procedure any less cumbersome. It would lead to a proliferation of single-purpose scripts with different conventions and expectations, and you’d still have to follow a documented multi-step procedure for using those scripts.This perception of futility is the problem we really need to solve in order to escape from these manual slogs. I’ve found an approach that works pretty reliably: .Almost any slog can be turned into a . A do-nothing script is a script that encodes the instructions of a slog, encapsulating each step in a function. For the example procedure above, we could write the following do-nothing script:import sys

def wait_for_enter():
    raw_input("Press Enter to continue: ")

class CreateSSHKeypairStep(object):
    def run(self, context):
        print("Run:")
        print("   ssh-keygen -t rsa -f ~/{0}".format(context["username"]))
        wait_for_enter()

class GitCommitStep(object):
    def run(self, context):
        print("Copy ~/new_key.pub into the `user_keys` Git repository, then run:")
        print("    git commit {0}".format(context["username"]))
        print("    git push")
        wait_for_enter()

class WaitForBuildStep(object):
    build_url = "http://example.com/builds/user_keys"
    def run(self, context):
        print("Wait for the build job at {0} to finish".format(self.build_url))
        wait_for_enter()

class RetrieveUserEmailStep(object):
    dir_url = "http://example.com/directory"
    def run(self, context):
        print("Go to {0}".format(self.dir_url))
        print("Find the email address for user `{0}`".format(context["username"]))
        context["email"] = raw_input("Paste the email address and press enter: ")

class SendPrivateKeyStep(object):
    def run(self, context):
        print("Go to 1Password")
        print("Paste the contents of ~/new_key into a new document")
        print("Share the document with {0}".format(context["email"]))
        wait_for_enter()

if __name__ == "__main__":
    context = {"username": sys.argv[1]}
    procedure = [
        CreateSSHKeypairStep(),
        GitCommitStep(),
        WaitForBuildStep(),
        RetrieveUserEmailStep(),
        SendPrivateKeyStep(),
    ]
    for step in procedure:
        step.run(context)
    print("Done.")
This script doesn’t actually  any of the steps of the procedure. That’s why it’s called a do-nothing script. It feeds the user a step at a time and waits for them to complete each step manually.At first glance, it might not be obvious that this script provides value. Maybe it looks like all we’ve done is make the instructions harder to read. But the value of a do-nothing script is immense:It’s now much less likely that you’ll lose your place and skip a step. This makes it easier to maintain focus and power through the slog.Each step of the procedure is now encapsulated in a function, which makes it possible to replace the text in any given step with code that performs the action automatically.Over time, you’ll develop a library of useful steps, which will make future automation tasks more efficient.A do-nothing script doesn’t save your team any manual effort. It lowers the activation energy for automating tasks, which allows the team to eliminate toil over time.]]></content:encoded></item><item><title>Setting up your own Bluesky feed in Go</title><link>https://dhawos.dev/site/en/articles/bluesky-custom-feeds</link><author>/u/dhawos</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 7 Feb 2025 19:38:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Bluesky is a decentralized alternative to Twitter. It is a platform that praises itself on being more open and putting users at the center of the ecosystem. One of the features that derives from this mantra is the ability for anyone to create their own custom newsfeed. Other users can then discover those feeds and display then on their homepage.This opens up the possibility for anyone to build their own algorithm and select content based on specific topics or based on popularity metrics such as reposts and likes.However, making such a feed is a pretty technical task. The purpose of this blog post is to walk you through the steps required to setup your own custom newsfeed, using the Go programming language. You don’t need to be a Go expert to understand what I’m going to explain, but some level of knowledge of the language will make following through a bit easier. You should also know that the official documentation provides examples in Typescript. There are also community made templates for Python and RubyTo build you own newsfeed, you will have to complete several steps:1.Register the newsfeed on your Bluesky account:
This part is simply about running a script that will make the correct API calls to Bluesky to register your newsfeed. You will need to have a Bluesky account as the feed will appear under your profile. You will also need a domain that you own, that will also be used to host the custom feed server later.2.Index the content created on Bluesky:
Before being able to choose which posts users are going to see. You must first know what posts are available. To do we will record events that occur on the Bluesky network and save them inside our database to later decide which posts a user should see on our feed.
The last task will be building a web server that will listen to Bluesky specific requests (as described in the protocol). The server’s job will be to decide which posts the user should see based on the request and make that reply to Bluesky, who will take care of fetching the posts information and hydrating the view.Let’s see that in more detailsIt is the simplest task in the process. For our newsfeed to be usable, it must first be registered with Bluesky. It will then show up under our profile. To do so, we will use a little script which will use the indigo library which is the Bluesky SDK for Go.Here is what it will look like (I’m omitting a portion of the code here for the sake of clarity). You will need to declare some information that will describe your feed:Then, we will setup our client to talk to Bluesky API. For that, you will need your Bluesky handle (our nickname + domain). In my case this is @dhawos.bsky.social. You will also need an “application password” which you can generate hereIf everything went well, please save the output carefully, you will need it later.
You should now also be able to see your new feed on your profile.2. Indexing content from BlueskyNow that your feed is created on Bluesky’s end, you need to set it up on your side. To suggest posts to your users, you first need to know which posts exist on the network.We’ll use Jetstream for this purpose, an open-source service provided by Bluesky that simplifies retrieving all content produced on the network. Note, however, that this service is not part of Bluesky’s official specifications and may change in the future. For now, it’s more user-friendly than the official(Firehose feed.We’ll also need a database, which we’ll use PostgreSQL. We’ll utilize its full-text search capabilities.To implement this, we’ll create a process that runs continuously to monitor activity on the network and save relevant information to our database. This involves defining a struct that listens for events and saves pertinent data.Now we will instanciate the listener in our main functionWe have the basic structure for our program. Now, we need to implement the function that will listen for events. Before that, let’s understand the foundation of AtProto, which is the protocol BlueSky is built on. Each user has a “repository” similar to Git. Everything you produce as a user results in a new commit within a specific collection. That is what we did when we created our feed earlier. We created a new commit in the  collection with the relevant informationsFor now, we’ll focus on listening to events related to the creation of commits. Other types of events, such as updates or deletions, are not relevant for our current purpose.We still need to write the function that will save the post to the database:We only need to call that function at the end of our :With that set up, and the PostgreSQL running in the background, it should be possible to start the program with the command:If all went well, you should be able to see posts coming to your database. Be warned that it might fill up pretty quickly as there can be a lot of activity on the network. We’ll see later how we can avoid the database growing forever.3. Writing your custom feed serverWith our database filling up, we should be able to build the last piece of the puzzle. To do so we will write a small webserver which will have two features:Validate our identity with BlueskyFulfill feed requests from BlueskyFirst, let’s set up the building blocks for our server. We’ll write a controller that will answer requests cominginThen in our server’s  file :Before sending us a request for a given custom feed, Bluesky will ensure that our identity is valid. There are several methods to assert our identity, namely  and . Here we’ll use the  method as it is the easiest to setup. Be mindful that it has a limitation. It does not support domain migration, so ensure that the domain you chose is the correct one as changing it later on will prove difficult.To assert our identity we just need to produce the correct response which in my case looks like this:Here is the code snippet that generates such a response :We are at the end of our little journey. We now only need to setup the actual custom feed implementation. But first let’s summarize what happens when a user requests a feed from Bluesky.The user load is newsfeedBluesky will fetch the information related to that feedBluesky will check the identity linked to that feed (which we just implemented)Bluesky will send a request to the feed’s domain name. This request will be made against the following endpoint :Here is an example of a valid response for this requestThe cursor field in the request and the response will be used for pagination. This is an opaque field that’s just generated by our server and sent back by Bluesky on the next request. This value has no meaning for Bluesky and its meaning is the responsibility of the custom feed developer. In our case we we’ll use the ID of the last post that we returned. That way, we will be able to send the next batch of posts when we receive a request for the next page.
Since theses IDs are generated sequentially, we know for a fact that we will get the posts from newest to oldestHere is how to generate such a response in our code.Here is the implementation for my custom feed for posts about cycling and commuting on a bicycle:I use  to generate the code that will execute the SQL requests. Here is the request I used (I’m not including the generated boilerplate code as it is not really relevant for this post).This request will fetch any post in a given language, for which the ID is below the cursor. We also use the  function of PostgreSQL to perform a full text search and retrieve posts that contain terms for our query. Finally we limit the number of posts to retrieve with the given limit parameter given by Bluesky.For the pagination to work correctly, we must ensure that the result of this request is stable and does not change over time. This is why we use the ID of the post (which is equivalent to the order in which the posts were indexed).With that done, you should now be able to run this code and setup your domain name to point either to your local machine or to a server on which this code is deployed. Once that is done, you can go on your Bluesky profile to find your feed and navigate to it to see if everything works as expected.Set-up garbage collectionIf everything works well, congratulations. Although there is a last step that you must perform if you don’t want your database to grow infinitely over time. As there is quite a lot of traffic on Bluesky, the database can grow pretty quickly so regular cleanup is important.To do so, we will simply launch a goroutine alongside the indexing process. This goroutine will simply run a cleanup command on a regular interval to delete posts older than 48 hours.Thanks for reading along. I hope those explanations allowed you to see more clearly the tasks needed to set up your own Bluesky custom newsfeed. The complete source code is available at https://gitlab.com/Dhawos/bluesky-custom-feeds. Don’t hesitate to take a lookYou can let me know if you enjoyed this article on my Bluesky: @dhawos.bsky.social. And if it helped you build your own feed, please share it, I would love to see what you build.If you happen to be interested in my feeds, you can find them here : (although they only show posts in French for now)]]></content:encoded></item><item><title>Jujutsu VCS Introduction and Patterns</title><link>https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/</link><author>/u/Alexander_Selkirk</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 19:24:20 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Jujutsu (jj), a new version control system written in Rust, has popped up on my radar a few times over the past year. Looked interesting based on a cursory look, but being actually pretty satisfied with Git, and not having major problems with it, I haven’t checked it out.That is, until last week, when I finally decided to give it a go! I dived into a couple blog posts for a few of hours, and surprisingly (noting that we’re talking about a VCS) I found myself enjoying it a lot, seeing the consistent design, and overall simplicity it managed to achieve. This post is meant to give you a feel for what’s special about jj, and also describe a few patterns that have been working well for me, and which really are the reason I’m enjoying it.Before we dive in, one last thing you should take note of, is that most people use jj with its Git backend. You can use jj with your existing Git repos and reap its benefits in a way that is completely transparent to others you’re collaborating with. Effectively, you can treat it like a Git frontend.Before I get to the meat, something that will surely be very useful for any of your experimentation with jj, and something I would’ve loved to have had in Git when I was learning it - you can undo any jj operation using , and view an operation log using .Changes are the core primitive you’ll be working with in jj. In Git you had commits, in jj you have changes. A jj change is, however, . A change can be modified. It’s ID, however, is immutable and randomly generated - it stays constant while you iterate on the change. A change refers to a  (otherwise called a snapshot) which for our purposes is always a Git commit. As you keep working on and modifying a change, the commit SHA it refers to will change.So to recap - immutable change IDs, mutable changes, mutable revision IDs (Git SHAs), immutable underlying revisions (snapshots - commits). Changes may eventually be marked as immutable - by default this happens (more or less) when they become part of the main branch or a Git tag.While in Git you generally organize your commits in branches, and a commit that’s not part of a branch is scarily called a “detached HEAD”, in jj it’s completely normal to work on changes that are not on branches.  is the main command to view the history and tree of changes, and will default to showing you a very reasonable set of changes that should be relevant to you right now - that is (more or less) any local mutable changes, as well as some additional changes for context (like the tip of your main branch).Changes in jj  be marked with bookmarks (what jj calls branches), but you’d generally do that only for the purpose of pushing to a remote Git server, like GitHub, not local organization.Let’s see a sample of a  invocation:Quick tip: you can use aha to convert colorized shell output to HTML.The IDs on the left side are change IDs, while on the right size you have revisions (commit SHAs). Unique prefixes of those IDs are marked in color, and you can use those prefixes instead of the full form when referring to changes. A change can be reffered to in jj commands by its ID, the underlying commit SHA, or any bookmark referring to it.There is one more, slightly crazy, thing about jj changes. Git has a special feature called the Git index to hold any as of yet uncommitted changes (where you , and then  them). In jj any files you modify are always in the scope of a change. Any modifications you make automatically become part of the current change, which is called the working copy. In the  output, this working copy is indicated by the  symbol. You create a new working change using  (by default as a child of the current change), and give it a description using  (it starts out having an empty message). You can use  to get the metadata of the current change.This much said, here’s a quick demo of changing a file as part of a new change.Editing a file in a jj repo on its own is an alias for .Most jj commands default to operating on the current working change, but let you operate on an arbitrary change (or set of changes!) via the  flag.This whole thing about the working copy being a Change may sound weird, but it brings with it an important feature - you can operate on the working copy like on any other change. Personally, I’m a frequent user of . When I’m working on something, I often want to pause for a moment and work on / try something else, only to later come back to what I was working on (while leaving that other experiment as yet another stash).In jj, I can just , which will start a new change from the parent of my current working copy - leaving my working copy as a normal change with all its file modifications “committed”, so I can later come back to it via .The  after the  means "go one level up". It's basically "take the working copy change, and go to its parent". We'll come back to this briefly later, but suffice it to say that there's a whole sensible expression language here, and  is similarly valid to refer to your grandparent change.Editing Changes in Weird Places#Another cool thing about changes is that you can freely edit any mutable change.  lets you “check out” and edit a given change. You can also squish a new change right after another (between it and its children). You can then keep modifying files in the context of that change, and any descendants will automatically be rebased on top of it (with their underlying commit SHAs likely changing, but change IDs staying the same). That sounds a bit scary if you’re used to Git (and dealing with conflicts in the middle of a rebase) but fear not, it’s actually pretty seamless, and we’ll come back to it later.You can pass  and  to  to indicate that you want to squish a new change after, or before, another change.With this, when I notice a mistake in a change 3 levels back, I can just  (with my working copy remaining there for me to come back to), make a fix, which will auto-rebase all following changes (including my original working change), and then  to go back to my original working change.Editing previous changes with automatic rebaseOccasionally you also need to rebase (move) a set of changes onto some change x, you can do that by using jj rebase -s <change-id-to-rebase> -d <destination-change>. The  will bring all descendants along with the rebased change, and there are other variations of this command for different scenarios. E.g. jj rebase -b <branch> -d <destination-change> will rebase the entire given branch onto a change, and with no arguments it just defaults to , so the current branch. In other words, to rebase the current branch onto main, it’s enough to run .Pattern: Squishing a Fix Before the Current Change#You noticed a mistake in your last change, don’t want to fix it as part of this one, and instead want to squish a fix right before it, but you have already done some “uncommitted” work? Just do . The  means it’s squished before the current working change (referred to via ), and after the previous one (its parent). After you make the fix, you can go back to the original working change via  ( is the opposite of ). You could also make the fix in the original working change and run  to use your favorite diff ui to select what to push down to a separate change placed right before the current one.In jj, instead of branches we have bookmarks. You create them using jj bookmark create <name> (abbreviated to ). You update a bookmark to point the current change using . You track remote bookmarks (which will create local corresponding bookmarks that update on ) by doing .When you add changes on top of a change that a bookmark is attached to, the bookmark won’t automatically move to your new change (like it would with a branch in Git), you have to  it manually. You can push bookmarks attached to the current change via .  will fetch updates to bookmarks from your remote (so generally updates to branches).A common use case with services like GitHub is to split up a big change into multiple PRs, let’s say Multipart 1, Multipart 2 and Multipart 3 (from branches multipart-1, multipart-2, and multipart-3 respectively). Each of them is based on the previous one, so you effectively have the following graph, with bookmark pointers on the way:Now let’s say Multipart 1 got reviewed and you need to fix something. In Git, once you add a commit to it (or modify an existing one), you would have to manually rebase all the other branches. Annoying!How does jj help us here? We can just run  ( is a unique prefix of the broken change id), or , make the fix, and run jj git push -b "glob:multipart-*". Everything will be automatically rebased, and all the bookmarks will be updated and pushed. Effortless!Making fixes in stacked PRsMerges in jj are pretty boring - in a good way. You just create a change with multiple parents. E.g.  will create a change with three parents.One topic I haven’t mentioned yet, and is surely by now giving you an unsettling feeling deep inside about all I’ve said before, are conflicts. With all this rebasing, that’s  to become a pain, right? Well, it doesn’t!As opposed to Git, where conflicts kind of break your workflow, in the sense that you have to resolve them prior to doing anything else, jj handles conflicts in a first-class manner. A change can just “be conflicted”. You can switch away from a conflicted change, you can create a new change on top of an existing conflicted change (and that will in turn also start out conflicted), you can edit a conflicted change, you can do anything.  and  will mention the conflict, but it won’t block you.
├─╮  
│ ○  
│ │  add 'cccc'
○ │  
├─╯  add 'bbbb'
○  
│  add 'aaaa'

Working copy changes:

file.txt    
Working copy : 
Parent commit:  add 'bbbb'
Parent commit:  add 'cccc'
The conflict will be represented in the code via conflict markers:aaaa
<<<<<<< Conflict 1 of 1
%%%%%%% Changes from base to side #1
+bbbb
+++++++ Contents of side #2
cccc
>>>>>>> Conflict 1 of 1 ends
and you can either manually resolve this conflict by editing the code itself, or use  to bring up your favorite three-way-merge tool (e.g. I’ve configured my jj to bring up a visual conflict resolver in Goland) and resolve it there.Once you fix the conflict in a change, all descendants of this change will also cease being conflicted. You could leave a change conflicted and only resolve the conflict in a follow-up child change - that is a completely valid and supported approach.Merge conflict resolutionPattern: Working on Two Things at the Same Time#I’ve mentioned stacked PRs, but another situation you might have is working on two independent things in parallel. Let’s say on bookmarks  and .In order to have both things simultaneosuly active in your codebase, you can create a development change via jj new thing-1 thing-2 -m "dev", that will be a merge between both of them, but will stay local and you’ll never push it ( just lets you give a description to a change when creating it, without a separate  invocation). You will however  this development change to do your work.Then, whenever you want to move some modifications into one of the branches, you can use jj squash --into <target-change-id> <files> to move all modifications to a set of files down into one of the branches. There’s also  where you can use a diff tool to choose modifications to squash into another change, and finally there’s a newer  command which can automate this process in certain scenarios.Editing files in a merged dev-change andselectively squashing the changes into branchesIn conclusion, you can keep working in a local-only merge-change of your branches, and selectively push down any modifications to the relevant branch (This setup would’ve seemed pretty scary before jj, right? I hope it’s a bit less scary now.), and then push just those branches themselves to the remote.jj commands operate on revisions or sets of revisions (revsets). You can refer to those directly, or use a special expression language to describe them. You’ve seen me refer to a change previously via . That was a very simple expression that evaluated to the parent.There is, however, much more. There are functions - like  to get the parents of a change - and operators - like  to refer to the child of x, or  for all descendants of x including x. accepts a revset expression, so you can use it to experiment with them. The default revset it displays is also configurable. Overall, the expression language is powerful and consistent, with simple things being generally easy, and harder things being (presumably, I’ve honestly spent too little time with it) possible.See this article for a much more extensive exploration of the jj expression language, and the jj docs themselves.Pattern: Partial Stashes#Another kind of stash I occasionally like to do is partial, where I temporarily roll back changes to a set of files to verify the before-after (e.g. confirm that a passing test was failing before).In jj the split command works well for this. Just jj split --parallel modifiedFile.txt will move the file into a parallel change. You can do whatever you want to do, and later run jj squash --from parallel_change_id to get the file modifications back into the current change.Partial stash using Setting Up jj with an Existing Git Repo#It’s trivial to start using jj with an existing git repo, though I’d advise cloning it fresh into a new directory.In a directory where you already have a git repo, you can just run . Your  directory will stay in place, and jj will keep it updated, so e.g. your editor won’t be confused what’s happening. It integrates fairly well, with e.g. the working change - even though it’s backed by a commit - being presented as the git index, so your editor can still show files “modified in this change”.The cost of switching is low, as it integrates seamlessly with your existing workflow. It frankly also takes a day tops to get used to, and there’s something to be said for using  things. Sure, you can make excellent tea in any food-safe kettle, but if you have a nice tea kettle, you’ll enjoy it every time you make tea. I use my vcs quite a lot, so why not make that pleasant too?I hope the above gave you an intuition for what Jujutsu, the VCS, is all about, and ideally even encouraged you to take a look at it.If you’d like to do some more readings about jj, I’ve used the below articles and guides when learning it, and a lot of what I wrote above is inspired by parts of them. Check them out!]]></content:encoded></item><item><title>[Media] Cosmic Yudh: a shooting game for the ESP32 with an OLED display</title><link>https://www.reddit.com/r/rust/comments/1ik2b1d/media_cosmic_yudh_a_shooting_game_for_the_esp32/</link><author>/u/AstraKernel</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 7 Feb 2025 18:43:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/AstraKernel ]]></content:encoded></item><item><title>GitHub - perpetual-ml/perpetual: A self-generalizing gradient boosting machine which doesn&apos;t need hyperparameter optimization</title><link>https://github.com/perpetual-ml/perpetual</link><author>/u/mutlu_simsek</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 18:35:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What are some things you would change about Go?</title><link>https://www.reddit.com/r/golang/comments/1ik1zqx/what_are_some_things_you_would_change_about_go/</link><author>/u/Jamlie977</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 7 Feb 2025 18:30:18 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[what are some weird things in Go that you'd like to change?for me, maps default to nil is a footgun]]></content:encoded></item><item><title>Show HN: A website that heatmaps your city based on your housing preferences</title><link>https://theretowhere.com/</link><author>WiggleGuy</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 18:23:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A brief history of code signing at Mozilla</title><link>https://hearsum.ca/posts/history-of-code-signing-at-mozilla/</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 17:51:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Shipping large software to end-user devices is a complicated process. Shipping large software  to end-user devices is even more complicated. Signing the things that ship to end-user devices is one of those complications, and it gets even more complicated when you sign thousands of artifacts per day.Mozilla has been signing Firefox in some form beginning with Firefox 1.0. This began with detached GPG signatures for builds, and progressed to Authenticode signing for Windows installers in Firefox 1.0.1. Since then it has evolved over time to encompass other platforms, other types of files within our products, and other ways that we ship (such as our own update packages). This post will provide a overview of the what, when, why, and how of code signing at Mozilla over the past ~20 years.Early GPG & Authenticode SigningWhen we first began signing, it happened on a Windows machine. Late in the release process, after Windows installers had been built, we would download all of the release artifacts to this machine, sign the Windows installers, and generate detached GPG signatures for those before pushing the artifacts elsewhere.At this time, the private keys and certificates were held on a USB stick that was kept removed from the machine at-rest. A Release Engineer needed to be physically present in Mountain View to perform this step. Once inserted, signing could be done via Remote Desktop rather than at the physical machine (but don't forget to remove the USB stick afterwards!).GPG signing was done with the standard GPG tools, running in cygwin. Authenticode signing was also done with the standard (at the time) Microsoft 'signcode.exe' tool. An annoying fact about that tool, is that it only accepted the necessary passphrase from a GUI dialog. To work around this, we had an AutoIt script running in the background that injected the passphrase into this dialog whenever it popped up. This interesting way of automating the process meant that mouse movements or keyboard interaction at the wrong time could interfere with the signing process.This process was partly scripted, but there was still a series of ~15 commands someone had to run by hand (and not mess up) to get everything done. You can see these commands for yourself in our now-ancient Unified Release Process documentation.Windows internal file signingCareful readers may have noted that early Authenticode signing only covered the Firefox installer itself, not the EXEs and DLLs inside of it. At some point (I haven't gone to the effort of tracking down exactly where...) we started signing these inner files as well. This process seems to have been lost to the sands of time, but I seem to recall it worked very similarly to the installer signing process, but without the GPG parts.Improved signing on WindowsThe first notable improvement we had to this process was to automate most of the copy/pasting that was done from the wiki. This came in the form of a Makefile that with a few mere inputs, would download, sign, and re-upload the signed builds. The main benefit of this was reduced opportunity for human error.Despite the signing process now being very quick once it gets started, it could still sometimes take hours or longer to begin the process. Typically this would happen if our build and repack processes finished at a time when no Release Engineer was around to begin signing. This was solved with what we called "autosign". Rather than require a Release Engineer to be around at the right moment, we adjusted our scripts to allow them to be started ahead of time, and be smart enough to know when all of the files it needs to sign are ready. This work eliminated all wait time between builds being ready and signing running.Signing Windows builds...on Linux!In 2011, signing was rearchitected altogether. In short, the idea was to move signing to a highly secured Linux server, and sign builds through an API as part of the build process. This allowed builds to be signed as they were produced, and reduced the number of times builds had to move from one server to another before they shipped.

An obvious question here is how we would manage to sign Windows binaries on Linux...as it turns out, the mono project had its own version of signcode that ran natively on Linux that we were able to make use of.Shortly after (and perhaps even motivating - I'm not sure at this point) the aforementioned signing server work, we began signing our MAR (Mozilla ARchive) packages that update users from an older version of Firefox to a newer version. Thanks to the earlier work, it was fairly trivial to use the same architecture to sign these files.Unfortunately, there were no tools available at the time to sign macOS builds on anything except a fairly modern macOS machine. For this reason, we had to run additional copies of our signing server on macOS and sign those builds with them. (If you've ever had to run macOS as a server you'll know just how unfortunate this was...)Taskcluster/signingscript/iscriptIn 2018, Mozilla migrated its CI and Release automation from our aging Buildbot systems to Taskcluster. As part of this, signing tasks moved to specialized Taskcluster workers known as "signingscript" and "iscript", used for signing non-macOS and macOS builds respectively. These specialized workers continued to outsource the actual work of signing to the previously discussed signing servers.An important part of this change is the introduction of Chain of Trust, a significant security enhancement that helps ensure that only authentic artifacts are signed to this day.Autograph is Mozilla's modern code signing service. It was built specifically to provide a signing service that allowed us to keep private key material in Hardware Security Modules (HSMs). Migrating release signing to it was a huge improvement over the existing signing server where Release Engineers had direct access to such things. It was initially used for signing XPIs and APKs, but by the end of 2019 we had migrated all non-macOS signing to it and retired the old Linux signing servers.In addition to the security enhancements it brought, we saw great performance wins with it as well, largely in thanks to it's support for  only requiring a hash of the bytes being signed to be sent over the wire. (This requires that the client has some advanced knowledge of the file being signed, but it saves a  amount of network traffic at our scale.)

Notarization with rcodesign

In 2023 we started making use of rcodesign to notarize and staple our macOS builds. While actual macOS code signing itself continues to happen on macOS machines, this allowed us to move at least some of our operations into the cloud and reduce our reliance on mac hardware.

I've mentioned a number of tools and technology that we use as part of signing, but I've purposely glossed over some details in the interest of brevity. The following section is a glossary of sorts, and introduces some more under the hood tools that we use as part of signing. If you're interested in the gory details, the links below should be enough to find them for yourself! Or you can stop by #firefox-ci on Matrix to ask questions!Winsign is a python library for signing and manipulating Authenticode signatures. It relies on osslsigncode for writing signatures, and supports signing directly with a private key, or outsourcing the signing process to a passed in function. The latter is what we use, and it's how we inject a call to Autograph into the signing process.In 2021 we began shipping Firefox as an MSIX package. As part of this we discovered that osslsigncode does not support signing MSIX packages. Luckily for us, Microsoft's MSIX packaging tools are open source and run on Linux, and we found a fork that contained most of what was needed to support signing. With a few additional modifications, we were able to support signing these packages in our existing systems.apple-codesign is a very exciting project from Gregory Szorc which provides 3rd party tools capable of signing, notarizing, and stapling .app bundles and other Apple formats such as .pkg and .dmg. These tools run on Linux, and as noted above, we're already making use of them to notarize and staple our .app bundles.

We're extremely excited about this project, and grateful to Gregory Szorc for all the effort he's bit into it. In the future we're looking forward to migrating our actual code signing to these tools which would (finally) allow us to retire our dedicated macOS signing machines.mardor is a python tool to manage, and most importantly, sign, MAR files. In the days before Autograph it was used to directly sign MAR files. These days we only use it to inject signatures made by Autograph into the files, similar to our usage of osslsigncode.signingscript is the glue between our CI system (Taskcluster) and Autograph. Through a combination of the tools listed above, custom code in signingscript itself, and communication with Autograph it produces signed builds. It is additionally responsible for notarizing and stapling our macOS builds.iscript is essentially a pared down version of signingscript (in fact their code is both derived from our early signing server code), and is responsible for signing our macOS builds. iscript runs on a small cluster of mac minis, which are a huge pain in the butt to manage.


As noted in an earlier section Autograph is our modern code signing service. It has a simple HTTP API that accepts signing requests and returns signed data or files. In addition to signing various artifacts that we ship it also makes Content Signatures on behalf of addons.mozilla.org, aus5.mozilla.org/Balrog (our update server), and some other backend services that Firefox communicates with, helping to ensure the security and integrity of requests made between Firefox and Mozilla-run services.

What a ride it's been over the last 20 years! We've gone from signing nothing to signing nearly everything in some form. Signing started off as a very manual process, and now happens seamlessly thousands of times per day.I don't think it would be possible to name everyone that contributed to this, but it took the ideas and efforts of tens, if not hundreds, of people to get to this point: release engineers, build system experts, security folks, and many others were all critical to getting us where we are today.I've got this post as brief as possible, but if you're interested in more details on any parts here feel free to reach out!]]></content:encoded></item><item><title>Create a Server Driven CLI from your REST API</title><link>https://zuplo.com/blog/2025/02/02/generate-cli-from-api-with-climate</link><author>/u/ZuploAdrian</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 7 Feb 2025 17:24:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This article is written by Rahul Dé, a VP of
Site Reliability Engineering at Citi and creator/maintainer of popular tools
like babashka,
bob, and now
climate. All opinions expressed are
his own.APIs, specifically the REST APIs are everywhere and the
OpenAPI is pretty much a standard.
Accessing them via various means is a fairly regular thing that a lot of us do
often and when it comes to the CLI (Command Line Interface) languages like Go,
Rust etc are quite popular choices when building. These languages are mostly of
statically typed in nature, favouring a closed world approach of knowing all the
types and paths at compile time to be able to produce lean and efficient binary
for ease of deployment and use.Like with every engineering choice, there are trade-offs. The one that's here is
the loss of dynamism, namely we see a lot of bespoke tooling in these languages
doing fundamentally the same thing: make HTTP calls and let users have a better
experience than making those calls themselves. The need to know all the types
and paths beforehand causes these perceived maintenance issues:Spec duplication: the paths, the schemas etc need to be replicated on the
client side again. eg when using the popular Cobra lib
for Go, one must tell it all the possible types beforehand.Tighter coupling of client and server: As we have to know each of the paths
and the methods that a server expects, we need to essentially replicate that
same thing when making the requests making a tight coupling which is
susceptible to breakage when the API changes. API is a product having its own
versioning. eg kubectl only supports
certain versions of
kubernetes. Similarly podman or docker CLIs.Servers can't influence the client: Ironically to the previous point, as we
now have replicated the server spec on the client side we effectively have a
split brain: changes on the server like needing a new parameter etc need to be
copied over to the client.All of this put together increases the maintenance overhead and its specially
true for complex tooling like kubectl.I work primarily on the infra side of this, namely Platform and Site Reliability
Engineering which involves me having other developers as my users and this
cascading effect of an API breakage is quite painful. There are way to work
around this issue and from my experience, being
spec-first
seems to offer the best balance of development and maintenance velocities.I am quite a big fan of being spec-first, mainly for the following reasons:The API spec is the single source of truth: This is what your users see and
not your code. Make this the first class citizen like your users and the code
should use this and not the other way round.This keeps all the servers and clients in sync automatically with less
breakage.This keeps a nice separation between the business logic (the API handler code)
and the infra thereby allowing developers to focus on what's important.Another project of mine Bob can be seen as an
example of spec-first design. All its tooling follow that idea and its
CLI inspired Climate. A lot of Bob uses
Clojure a language that I cherish and who's ideas make
me think better in every other place too.Although codegen is one of the ways to be spec-first, I personally don't
subscribe to the approach of generating code:Introduces another build step adding complexity and more layers of debugging.Makes the build more fragile in keeping up with tooling and language changes.The generated code comes with its own opinions and is often harder to
change/mould to our needs.It is static code at the end, can't do much at runtime.restish: Inspired some of the ideas behind this. This is a
project with different goals of being a fully automatic CLI for an OpenAPI
REST API and is a bit hard to use as a lib.navi: Server side spec-first library I
wrote for Clojure which inspired the handler mechanism in Climate.Keeping all of the above into consideration and the fact that Go is one of the
most widely used CLI languages,
Climate was built to address the
issues.As the name implies, its your mate or sidekick when building CLIs in Go with the
intentions of:Keeping the REST API boilerplate away from you.Keep the CLI code always in sync with the changes on the server.Ability to bootstrap at runtime without any code changes.Decoupling you from API machinery, allowing you to focus on just the handlers,
business logic and things that may not the part of the server calls.It does just enough to take the machinery out and not more like making the
calls for you too; that's business logic.Every OpenAPI3 Schema consists of one or more
Operations
having an . An Operation is a combination of the HTTP path, the
method and some parameters.Overall, Climate works by with these operations at its core. It:Parses these from the YAML or JSON file.Transforms each of these into a corresponding Cobra command by looking at
hints from the server.Transform each of the parameters into a Flag with the type.Build a grouped Cobra command tree and attach it to the root command.Climate allows the server to influence the CLI behaviour by using OpenAPI's
extensions.
This is the secret of Climate's dynamism. Influenced by some of the ideas behind
restish it uses the following extensions as of now:: A list of strings which would be used as the alternate names
for an operation.: A string to allow grouping subcommands together. All operations
in the same group would become subcommands in that group name.: A boolean to hide the operation from the CLI menu. Same
behaviour as a cobra command hide: it's present and expects a handler.: A boolean to tell climate to omit the operation completely.: A string to specify a different name. Applies to operations and
request bodies as of now.As of now, only the primitive types are supported:More support for
types like
collections and composite types are planned. These are subject to limitations of
what Cobra can do out of the box and what makes sense from a CLI perspective.
There are sensible default behaviour like for request bodies its implicity
 which handles most cases. These types are converted to Flags with the
appropriate type checking functions and correctly coerced or the errors reported
when invoked.Checkout Wendy as a proper example of a
project built with Climate.This assumes an installation of Go 1.23+ is
available.Define a cobra root command:Define one or more handler functions of the following signature:As of now, each handler is called with the cobra command it was invoked with,
the args and an extra , more info
hereThis can be used to query the params from the command mostly in a type safe
manner:Define the handlers for the necessary operations. These map to the 
field of each operation:Bootstrap the root command:Continue adding more commands and/or execute:Climate results from my experiences of being at the confluence of many teams
developing various tools and proving the need to keep specifications at the
centre of things. WIth this it hopefully inspires others to adopt such
approaches and with static tooling like Go, its still possible to make flexible
things which keep the users at the forefront.]]></content:encoded></item><item><title>I Automated My Taxes using AutoHotKey</title><link>https://www.preethamrn.com/posts/automating-taxes-autohotkey</link><author>/u/preethamrn</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 16:55:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[7 February 2025 · I hate doing taxes. I don't mind paying them but I'm not a fan of all the effort involved in collecting all the forms and making sure to fill out all the boxes correctly or else risk getting audited. It's especially painful for me because my job gives me RSUs which are automatically sold to cover taxes and at the end of the year, sends me a 1099-B with every single sale of every single stock grant which usually amounts to 90+ sales that I have to report.In theory, I could file a summary of sales and send out a Form 8949 but the first year I did this, I didn't do the process properly which resulted in me getting audited.Ever since that incident, I've diligently copied every single row from my 1099 to TurboTax which usually took 3 hours and sometimes 2 people. This year I decided to change things up. Most of the process is extremely repetitive - you read a row, click on the form, fill the required details, and then move on to the next row. Seemed like the perfect job for autohotkey.Step 1: Figuring out where to clickI could do this using trial and error but it was much simpler to automate this part too. I wrote a script that would tell me my mouse cursor position for each clickF6::
MouseGetPos, xpos, ypos
FileAppend, %xpos%`, %ypos%`n, mouse_positions.txt
return
One thing that I needed to keep in mind was that I wouldn't be able to scroll so I had to shrink the text size to make everything fit. And with that, I had a list of all the places I'd need to click and what to enter at each locationI could probably automate the PDF parsing as well, but it turned out to be much simpler to just copy paste the details into a text file. After cleaning up all the unnecessary details using Sublime's regex find and replace, I ended up with a text file  like this:Step 3: Writing the automation scriptNow for the fun part. I hadn't worked with autohotkey before but I knew that this was the exact kind of thing that it was built for. Turns out I was right and for once in my life the coding went by pretty smoothly.The main structure of the program was like this:#Persistent
#SingleInstance Force
SetTitleMatchMode, 2

; Give time to focus the TurboTax window
Sleep, 2000

dataFile := "data.txt" ; Specify the text file to read from
Loop, Read, %dataFile% {
    line := A_LoopReadLine
    elements := StrSplit(line, " ") ; Split the line by spaces

    ; open dropdown
    MouseClick, left, 252, 419
    Sleep, 100

    ; select from drop down
    MouseClick, left, 259, 462
    Sleep, 100

    ; go to description field
    MouseClick, left, 244, 556
    Sleep, 50

    ; enter description
    Send, % elements[1] . " UBER"

    ....
    ; check for wash sale
    if (elements.Length() >= 7) {
        ... ; enter wash sale adjustment
        ; click submit
        MouseClick, left, 524, 1220
        Sleep 5000
    } else {
        ; click submit
        MouseClick, left, 516, 957
        Sleep 5000
    }
    
    ; add new row
    ; can't use a mouseclick here because the "Add new" button moves down as the table grows
    SetKeyDelay, 50
    Loop, 4
    Send, {rshift down}{tab}{rshift up}
    SendInput, {enter}
    SetKeyDelay, -1

    Sleep, 3000
}

return

; Exit script on Escape key
Esc::ExitApp
A few interesting things that I figured out while testingWhen adding a new row, I needed to cycle through the buttons using tab backwards since the "Add new row" button position changes as the table size grows with each new row.The most time consuming part ended up being tuning the delays. TurboTax starts out fast but after entering a few rows, it slows to a crawl and even a 5 second delay isn't enough. Things never really got into a rhythm and I had to keep adjusting the wait time to give the app time to keep up with the script.TurboTax only allows 25 rows per page so halfway through, I needed to change the script again to account for the new location of the "Add new row" button. Instead of 4 tabs back, I needed to change it to 7 (and then again to 8. and then 9... the table grew pretty large)With the script ready, it was time to run it. And boy did it run. Right away, it finished submitting around 15 rows before I even figured out what I was planning on doing with all my new found free time.The excitement was short lived though because I hit a snag pretty quickly. TurboTax would occasionally take an extra second loading but my script didn't care and would go on clicking all over the place so I'd have to babysit it and make sure to press Escape before it did anything crazy.Whenever I stopped the script, I had to find the last row that was submitted and delete all the data that was processed from the txt file. In total, I had to do this about 25 times.All things considered I'd consider it a success. The script took me about 45 minutes to write and debug and then around another 30 minutes to run/fix any hidden errors. Even if it took me around 1 minute to manually enter each row, it was already almost twice as fast and I can reuse this script next year.Also, whenever I did this manually, I usually made at least one data entry error so it would usually take another 10-15 minutes double checking all my data to figure out where the error was. In this case, once the script completed, I just checked the totals to make sure everything lined up - either I'd get a perfect answer or I'd get something radically off. It worked on the first try.I'm not sure what the moral of the story here is but maybe there are some cases where it actually does make sense to automate things in my life. I'll be looking out for those opportunities more now. #Programming Written by @preethamrn: Software developer at Uber with a degree in CS. Go, Storage, Distributed Systems, Bouldering, Rubik's Cubes. Github]]></content:encoded></item><item><title>I created a little GUI tool that lets you schedule system shutdowns. ClockOut.</title><link>https://www.reddit.com/r/linux/comments/1ijz9ga/i_created_a_little_gui_tool_that_lets_you/</link><author>/u/qinn1996</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 16:39:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/qinn1996 ]]></content:encoded></item><item><title>default/override - An Elegant Schema for User Settings</title><link>https://double.finance/blog/default_override</link><author>/u/Ok-Eye7251</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 16:37:26 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Every app, even basic ones, needs to manage user settings.  It always starts simple, right? "Does the user want the Weekly Email?" But then features grow, and suddenly you're dealing with tons of settings:  themes, notifications, currencies, time zones, the list goes on.  If you're building something complex, like Double – which is for investing –  settings get  important.We have to manage things like fraud settings, drift limits and wash sale prevention settings. Some of these we want to be user controlled, some should only be editable by an admin user. How you store and handle these settings really matters for your app's speed, how well it scales, and how easy it is to maintain.  I've tried a bunch of ways to manage user settings, and some are way better than others.  Let's look at common mistakes first, and then I'll show you a schema that we came up with recently that's been working really well for us.Common Mistakes:  What  To DoOne common mistake to avoid is the wide table trap, where the initial thought for managing new settings is to simply keep adding columns. It may seem easy at first; for example, setting up daily emails might look as simple as adding a  boolean column to your  table with a quick  command. However, this approach quickly leads to columns getting out of control.Before long, you'll need columns for newsletters, app notifications, dark mode, time zones, and other features, causing your  table to become a massive, wide structure. Wide tables can negatively impact performance, as queries take longer because the database has to process huge rows, many of which are sparsely populated. Indexes become more complicated, and backups and restores take significantly longer.Schema changes also become a pain point. Adding another setting involves another risky  operation in a live application. Remembering the purpose of each column becomes a documentation nightmare, and changing or removing old settings becomes a major project.Finally, because most users only modify a few settings, your  table becomes filled with wasted space in the form of numerous NULL values, further slowing down queries and wasting storage.Another approach that may seem appealing after rejecting wide tables is the "grab bag" table, often implemented as a generic  table with columns like , , and .At first glance, this key-value structure appears flexible, because it promises easy addition of new settings without altering the main table. However, bulk updates soon become a significant challenge. For example, adding a new setting to all users could require millions of insert operations, resulting in large, risky database transactions.  This can lead to server instability and data inconsistencies, often necessitating complex error handling, retry mechanisms, and background jobs to ensure data integrity.Data quality also becomes a concern because the  field can accommodate any data type – text, numbers, booleans – making it difficult for the database to enforce consistency.  Consequently, all data validation and type handling logic must be implemented in the application code, increasing complexity and the potential for bugs.  Finally, user signup processes can become slower.Setting default settings for new users may require inserting numerous rows into the  table, adding latency to the signup flow and increasing the chances of failures during this critical operation.3. The Schemaless TemptationAnother tempting approach, especially with the rise of NoSQL databases like MongoDB, is to store settings as JSON documents directly within each user's record.  The schemaless nature of NoSQL databases initially appears advantageous, and using JSON for settings feels like a natural fit.  Adding a new setting seems as simple as adding a new key to the JSON document. However, querying across users for specific settings becomes challenging.While document databases excel at retrieving all data for a single user, asking questions about a particular setting across all users becomes slow and inefficient.  For instance, answering "How many users use dark mode?" could require a full database scan, which does not scale efficiently.Furthermore, the perceived benefit of being "schemaless" diminishes when data consistency is required. Ensuring that every user has a specific setting or validating data types becomes the application's responsibility, as the database provides no inherent mechanisms for these rules.Finally, while NoSQL promises flexibility, settings migrations over time remain complex.  Updating settings across numerous NoSQL documents often necessitates custom scripts and background jobs, mirroring the same challenges encountered with the "grab bag" SQL table approach in terms of complexity and potential issues.A Better SQL Schema: /After struggling with these problems, I've found a SQL schema that actually works well. It balances flexibility, performance, and keeps your data sane.  It uses two key tables:  and . Table:  The Master List of SettingsThink of  as the single place where you define all possible settings in your app.  It's like the blueprint for your settings system.Unique name for the setting (primary key).The default value everyone gets.ENUM('boolean', 'string', 'integer', 'enum')What is the data type of this setting?(Optional, for 'enum' type)  List of valid choices as JSON.['daily', 'weekly', 'monthly'] (for rebalance cadence)Can users change this setting themselves? (for rebalance cadence),  (for internal flags) Table:  User-Specific Changes only stores settings where a user has changed the default.  It's kept lean and efficient.Links to the  table (part of primary key).Links to  (part of primary key).The user's chosen value for the setting.When the user made this change.When the user last updated this setting.This system is designed to be simple and effective in practice.  Adding a new setting is straightforward; you only need to insert a single row into the  table.  Here, you define the setting's name, its default value, the data type, any valid options, and whether users are allowed to edit this setting.  Crucially, this process requires no database schema alterations and avoids mass data modifications.To retrieve a user's setting, the  function (or its equivalent in your ORM) efficiently handles the logic.  It prioritizes checking the  table. If a user-specific setting exists there, that value is used.  Otherwise, the function falls back to the default value defined in the  table.SELECT COALESCE(so.value, sd.value) AS setting_value
FROM default sd
LEFT JOIN override so ON sd.name = so.name AND so.user_id = :user_id
WHERE sd.name = :setting_name;
Retrieving multiple settings for a user simultaneously remains efficient through the use of joins and filters.  Furthermore, data validation is enforced at the SQL level. When a setting is modified, a new row is inserted into the  table.  Before saving this override, validation can be performed based on the  and  columns defined in , allowing the system to reject invalid setting changes proactively.For user settings management, users can easily retrieve all their settings, encompassing both their personalized overrides and the system defaults, or filter specifically for settings they are permitted to modify.  Users can then update their settings via the application's API, which incorporates built-in validation checks and persists valid changes to the  table. This approach offers several advantages. It scales well, allowing for fast addition and retrieval of settings.  Since only user modifications are stored, the database remains efficient. Data consistency is ensured by  which establishes data types and valid options, with database-level checks promoting reliability.Settings management is simplified with  acting as a central list, eliminating ambiguity about setting definitions. The design inherently maintains data consistency, negating the need for supplementary background processes to synchronize settings.The system is clean and organized, clearly separating defaults from user-specific changes, enhancing overall system clarity and usability.   provides control over user modifications, enabling specification of settings users can adjust, which is beneficial for sophisticated applications.Finally, it facilitates efficient bulk operations and the creation of user-facing settings interfaces, making it easy to retrieve numerous settings at once. This system also presents some drawbacks. The primary disadvantage is its complexity compared to simpler methods like wide tables or key-value based approaches.Additionally, gaining easy insight into historical settings is limited. As user settings rely on two tables, tracking a setting's value at a specific past time is not straightforward, requiring periodic snapshots of both tables for reconstruction, which is less than ideal.Lastly, retrieving settings necessitates querying the application's backend to access the database (or use a cache). While slightly slower than direct column access from a  table, the benefits generally outweigh this minor performance difference. Database views can also be implemented to mitigate this speed concern in many scenarios.In Short: Ditch Giant Tables, Use The Default-Override MethodFor apps that need to be reliable and handle lots of users, especially complex ones like investing platforms, how you store settings really matters.  Moving away from simple wide tables and using a structured schema like / has been a great move for us here at Double.  It's cleaner, scales better, easier to maintain, and keeps our data in good shape.What's your experience with handling user settings?  Run into similar problems? Found different solutions?]]></content:encoded></item><item><title>Artem Lajko gives a presentation on Considerations when building an IDP and how you can use Kubernetes + GitOps + vCluster (there&apos;s a demo too)</title><link>https://youtu.be/7p1GdyS7kmA</link><author>/u/mpetersen_loft-sh</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 7 Feb 2025 16:28:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Transductive regular expressions for text editing</title><link>https://github.com/c0stya/trre</link><author>c0nstantine</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 16:18:14 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[An extension of regular expressions for text editing, with a grep-like command-line tool. If you, like me, struggle with group logic in regular expressions, you might find it useful.I wanted to do this for a very long time. It is more of a sketch or prototype. I'd really appreciate your feedback!]]></content:encoded></item><item><title>Calibre 7.25 released (e-book manager)</title><link>https://calibre-ebook.com/whats-new</link><author>/u/gabriel_3</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 16:09:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Your support helps keep calibre development alive. Contacting payment provider, please wait…]]></content:encoded></item><item><title>Best way to work with large strings?</title><link>https://www.reddit.com/r/rust/comments/1ijxwdg/best_way_to_work_with_large_strings/</link><author>/u/exater</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 7 Feb 2025 15:42:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I have a long running web server (axum) that pulls large string feeds from another server. Is there a way to reuse some sort of buffer to avoid reallocating on the heap every time it makes an HTTP call? According to the process memory, it makes pretty significant spike when a burst of HTTP calls are made. I'm concerned that this is causing some kind of memory fragmenting? I recently swapped the memory allocator to jemalloc instead of the default. However, after monitoring for a few days, the memory still seems to be slowly creeping up. I've done extensive memory profiling and cannot really isolate a "leak". However, according to my metrics, when bursts in HTTP calls are made, the memory similarly spikes up and never really seems to recover fully. For example, if the process was sitting at 100mb, bursts up to 150mb, it may go back down to 110mb but not 100mb. And after long periods of time I'm observing a steady increase. Am I sweating over nothing? Is this just standard allocator behavior, holding on to pages of memory to reuse later? The docker containers limit is 512mb, the server started up at around 150mb, and after running for 2 days continuously it is sitting at 220mb. Under the default allocator, this behavior was observed all the way until it reached 512mb and OOMd (took a few weeks). Is jemalloc smarter? Will jemalloc eventually stable out somewhere and not go all the way to 512mb and kill itself?]]></content:encoded></item><item><title>Linus Torvalds&apos; take on the latest Rust-Kernel drama</title><link>https://www.reddit.com/r/linux/comments/1ijxber/linus_torvalds_take_on_the_latest_rustkernel_drama/</link><author>/u/Non-taken-Meursault</author><category>dev</category><category>reddit</category><pubDate>Fri, 7 Feb 2025 15:17:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So at the end it wasn't sabotage. In software development you can't pretend just to change everything at the same time. ]]></content:encoded></item><item><title>Kubernetes Cluster per Developer</title><link>https://www.reddit.com/r/kubernetes/comments/1ijx3n6/kubernetes_cluster_per_developer/</link><author>/u/Born-Organization836</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 7 Feb 2025 15:08:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm working in a team which consists of about 15 developers. Currently we're using only one shared Kubernetes cluster (via Openshift) aside from prod which we call preprod. Obviously this comes with plenty of hardships - our preprod environment is consistently broken and everytime we want to test some code we need to configure plenty of deployments to match prod's deployments, make the changes we need to test our code and pray no one else is going to override our configuration.I've been hearing that the standard today is to create an isolated dev environment for each developer in the team, which, as far as I understand, would require a different Kubernetes cluster/namespace per developer.We don't have enough resources in our cluster to create a namespace per developer, plus we don't have enough resources in our personal computers to run a Kubernetes cluster locally. We do however have enough resources to run a copy of the prod cluster in a VM. So the natural solution, as I see it, would be to run a Kubernetes cluster (pereferably with Openshift) on a different VM for every developer, or alternatively one Kubernetes cluster with a namespace per developer.What tools do you recommend to run a Kubernetes cluster in a VM with good DX when working locally? Also how would you suggest to mimic prod's cluster configuration as good as possible (networking configuration, etc)? I've heard plenty about TIlt and wondered if it'd be applicable here.If you have an alternative suggestion or something you do differently in your company, please share!]]></content:encoded></item><item><title>Asahi Linux Lead Developer Hector Martin Resigns From Linux Kernel</title><link>https://linux.slashdot.org/story/25/02/07/1332241/asahi-linux-lead-developer-hector-martin-resigns-from-linux-kernel?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Fri, 7 Feb 2025 14:40:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Asahi lead developer Hector Martin, writing in an email: I no longer have any faith left in the kernel development process or community management approach. 

Apple/ARM platform development will continue downstream. If I feel like sending some patches upstream in the future myself for whatever subtree I may, or I may not. Anyone who feels like fighting the upstreaming fight themselves is welcome to do so.]]></content:encoded></item><item><title>Asahi Linux lead developer Hector Martin resigns from Linux kernel</title><link>https://lkml.org/lkml/2025/2/7/9</link><author>Mond_</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 13:02:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I no longer have any faith left in the kernel development process orcommunity management approach.Apple/ARM platform development will continue downstream. If I feel likesending some patches upstream in the future myself for whatever subtreeI may, or I may not. Anyone who feels like fighting the upstreamingfight themselves is welcome to do so.Signed-off-by: Hector Martin <marcan@marcan.st>--- 1 file changed, 1 deletion(-)diff --git a/MAINTAINERS b/MAINTAINERSindex 1e930c7a58b13d8bbe6bf133ba7b36aa24c2b5e0..c9623439998709c9d6d6944cbd87e025356422da 100644+++ b/MAINTAINERS@@ -2177,7 +2177,6 @@ F:	sound/soc/codecs/cs42l84.* F:	sound/soc/codecs/ssm3515.c ARM/APPLE MACHINE SUPPORT-M:	Hector Martin <marcan@marcan.st> M:	Sven Peter <sven@svenpeter.dev> R:	Alyssa Rosenzweig <alyssa@rosenzweig.io>---base-commit: 40384c840ea1944d7c5a392e8975ed088ecf0b37change-id: 20250207-rm-maint-af7cccc22871-- Hector Martin <marcan@marcan.st>]]></content:encoded></item><item><title>Apple ordered by UK to create global iCloud encryption backdoor</title><link>https://www.macrumors.com/2025/02/07/uk-government-orders-access-icloud/</link><author>throw0101d</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 12:18:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The British government has secretly demanded that Apple give it blanket access to all encrypted user content uploaded to the cloud, reports .The undisclosed order is said to have been issued last month, and requires that Apple creates a back door that allows UK security officials unencumbered access to encrypted user data worldwide – an unprecedented demand not before seen in any other democratic country.The spying order came by way of a "technical capability notice," a document sent to Apple by the Home Secretary, ordering it to provide access under the sweeping UK Investigatory Powers Act (IPA) of 2016. Critics have labeled the legislation the "Snooper's Charter," as it authorizes law enforcement to compel assistance from companies when needed to collect evidence.An Apple spokesperson declined to comment on the revelation, though the law actually makes it a criminal offense to reveal that the government even made such a demand. Likewise, the Home Office told the publication that its policy was not to discuss any technical demands. "We do not comment on operational matters, including for example confirming or denying the existence of any such notices," a spokesman said.One of the people briefed on the situation, a consultant advising the United States on encryption matters, said Apple would be barred from warning its users that its most advanced encryption no longer provided full security. The person deemed it shocking that the UK government was demanding Apple's help to spy on non-British users without their governments' knowledge. A former White House security adviser confirmed the existence of the British order.Apple May Drop UK iCloud Services Over Secret Encryption Access OrderApple is likely to stop offering encrypted storage in the UK, rather than break the security promises it made to its users, people familiar with the matter told the publication. However, that would not affect the UK order for backdoor access to the service in other countries, including the United States. Apple has previously said it would consider pulling services such as FaceTime and iMessage from the UK rather than compromise future security.The order would compromise Apple's Advanced Data Protection, which the company launched in 2022. The feature gives users the option to end-to-end encrypt many additional iCloud data categories, including Photos, Notes, Voice Memos, Messages backups, device backups, and more, making their data inaccessible to anyone else – including Apple.Google has enforced default encryption for Android phone backups since 2018. When asked by  whether any government had requested a backdoor, Google spokesman Ed Fernandez did not provide a direct answer but suggested none exist: "Google cannot access Android end-to-end encrypted backup data, even with a legal order," he stated.The IPA was updated in 2023 to allow the Home Office to outlaw certain encrypted services using a technical capability notice. Apple at the time called the then proposed amendments "an unprecedented overreach by the government," saying that if the update was enacted, "the UK could attempt to secretly veto new user protections globally preventing us from ever offering them to customers."Apple CEO Tim Cook has consistently insisted that providing back-door access past its encryption for authorities would open the door for "bad guys" to gain access to its users' data. Cyber security experts agree that it would only be a matter of time before bad actors discover such a point of entry. Apple's stance was enhanced in 2016 when it successfully fought a US order to unlock the iPhone of a shooter in San Bernardino, California.US law enforcement's longstanding objections to encryption have recently taken a backseat to concerns over large-scale cyber intrusions attributed to Chinese state-backed hackers. The attackers infiltrated major telecommunications providers, granting them unfettered access to private phone calls. During a December press conference alongside FBI officials, a Department of Homeland Security representative cautioned Americans against assuming traditional phone networks offer privacy, instead advising them to use encrypted communication whenever feasible.That same month, the FBI, National Security Agency, and Cybersecurity and Infrastructure Security Agency issued a joint advisory detailing numerous countermeasures against the Chinese cyber campaign. Among their recommendations: "Ensure that traffic is end-to-end encrypted to the maximum extent possible."In a statement, privacy campaigner Big Brother Watch said: "This misguided attempt at tackling crime and terrorism will not make the UK safer, but it will erode the fundamental rights and civil liberties of the entire population."Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our Political News forum.  All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.]]></content:encoded></item><item><title>Your YouTube Channel to Learn System Design 🔥</title><link>https://newsletter.systemdesign.one/p/system-design-youtube-channel</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/83516d21-e16e-4561-861e-e83b9bf60224_1280x720.gif" length="" type=""/><pubDate>Fri, 7 Feb 2025 11:35:39 +0000</pubDate><source url="https://newsletter.systemdesign.one/">Dev - System Design Newsletter</source><content:encoded><![CDATA[I created a YouTube channel to help you learn system design.• System design fundamentals.• System design interview preparation tips.• Simplified engineering case studies with visuals.• System design deep dives with real-world software architecture.You'll receive a new video every 10 days.I'll make it your main video channel for system design over time.And I want to help you become good at work + ace system design interviews:Please show your support,]]></content:encoded></item><item><title>Meta torrented &amp; seeded 81.7 TB dataset containing copyrighted data</title><link>https://arstechnica.com/tech-policy/2025/02/meta-torrented-over-81-7tb-of-pirated-books-to-train-ai-authors-say/</link><author>gameshot911</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 11:26:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Emails discussing torrenting prove that Meta knew it was "illegal," authors alleged. And Bashlykov's warnings seemingly landed on deaf ears, with authors alleging that evidence showed Meta chose to instead hide its torrenting as best it could while downloading and seeding terabytes of data from multiple shadow libraries as recently as April 2024.Meta allegedly concealed seedingSupposedly, Meta tried to conceal the seeding by not using Facebook servers while downloading the dataset to "avoid" the "risk" of anyone "tracing back the seeder/downloader" from Facebook servers, an internal message from Meta researcher Frank Zhang said, while describing the work as being in "stealth mode." Meta also allegedly modified settings "so that the smallest amount of seeding possible could occur," a Meta executive in charge of project management, Michael Clark, said in a deposition.Now that new information has come to light, authors claim that Meta staff involved in the decision to torrent LibGen must be deposed again because the new facts allegedly "contradict prior deposition testimony."Mark Zuckerberg, for example, claimed to have no involvement in decisions to use LibGen to train AI models. But unredacted messages show the "decision to use LibGen occurred" after "a prior escalation to MZ," authors alleged.Meta did not immediately respond to Ars' request for comment and has maintained throughout the litigation that AI training on LibGen was "fair use."However, Meta has previously addressed its torrenting in a motion to dismiss filed last month, telling the court that "plaintiffs do not plead a single instance in which any part of any book was, in fact, downloaded by a third party from Meta via torrent, much less that Plaintiffs’ books were somehow distributed by Meta."While Meta may be confident in its legal strategy despite the new torrenting wrinkle, the social media company has seemingly complicated its case by allowing authors to expand the distribution theory that's key to winning a direct copyright infringement claim beyond just claiming that Meta's AI outputs unlawfully distributed their works.As limited discovery on Meta's seeding now proceeds, Meta is not fighting the seeding aspect of the direct copyright infringement claim at this time, telling the court that it plans to "set... the record straight and debunk... this meritless allegation on summary judgment."]]></content:encoded></item><item><title>U.K. orders Apple to let it spy on users’ encrypted accounts</title><link>https://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/</link><author>Despegar</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 07:45:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ChatLoopBackOff Episode 45 (CRI-O)</title><link>https://www.youtube.com/watch?v=--eJZu3Zkbw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/--eJZu3Zkbw?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 06:19:59 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[CRI-O, a CNCF Graduated project, is designed to be lightweight, focusing solely on Kubernetes needs’, reducing resource overhead compared to general-purpose container runtimes. 

It improves security by leveraging OCI (Open Container Initiative) standards, integrating seamlessly with tools like seccomp, SELinux, and AppArmor for enhanced isolation and compliance. With robust support for OCI-compliant image formats and runtimes like runc, CRI-O ensures compatibility with a wide range of container images. Join CNCF Ambassador Marvin Beckers test CRI-O’s efficiency, as it minimizes the feature set to what Kubernetes requires, leading to faster startup times and reduced complexity.]]></content:encoded></item><item><title>Mixing Rust and C in Linux Likened To Cancer By Kernel Maintainer</title><link>https://linux.slashdot.org/story/25/02/06/1830233/mixing-rust-and-c-in-linux-likened-to-cancer-by-kernel-maintainer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Thu, 6 Feb 2025 19:22:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[A heated dispute has erupted in the Linux kernel community over the integration of Rust code, with kernel maintainer Christoph Hellwig likening multiple programming languages to "cancer" for the project's maintainability. The conflict centers on a proposed patch enabling Rust-written device drivers to access the kernel's DMA API, which Hellwig strongly opposed. While the dispute isn't about Rust itself, Hellwig argues that maintaining cross-language codebases severely compromises Linux's integrated nature. From a report: "Don't force me to deal with your shiny language of the day," he [Hellwig] wrote. "Maintaining multi-language projects is a pain I have no interest in dealing with. If you want to use something that's not C, be that assembly or Rust, you write to C interfaces and deal with the impedance mismatch yourself as far as I'm concerned." This resistance follows the September departure of Microsoft engineer Wedson Almeida Filho from the Rust for Linux project, citing "nontechnical nonsense."]]></content:encoded></item><item><title>Show HN: An API that takes a URL and returns a file with browser screenshots</title><link>https://github.com/US-Artificial-Intelligence/scraper</link><author>gkamer8</author><category>dev</category><category>hn</category><pubDate>Thu, 6 Feb 2025 18:48:05 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: SQLite disk page explorer</title><link>https://github.com/QuadrupleA/sqlite-page-explorer</link><author>QuadrupleA</author><category>dev</category><category>hn</category><pubDate>Thu, 6 Feb 2025 18:40:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Google finally shipped some fire…</title><link>https://www.youtube.com/watch?v=k9xbh9LUYn0</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/k9xbh9LUYn0?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 18:26:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Deploy your app without complexity using Sevalla http://bit.ly/4hDju8W

Take a first look at Google Gemini 2.0 and see how it compares to models like DeepSeek R1 and OpenAI o3-mini. Learn how to use AI models like Gemini Flash to solve real-world problems. 

#tech #ai #thecodereport

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Gemini for PDF processing https://www.sergey.fyi/articles/gemini-flash-2
OpenAI o3 First Look https://youtu.be/PoeFxGzPpXE
DeepSeek R1 Fallout https://youtu.be/Nl7aCUsWykg

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Is Google Gemini better than OpenAI?
- How to use Gemini Flash
- Comparison of top LLMs
- o3 vs Gemini vs DeepSeek
- Latest trends in Google technology]]></content:encoded></item><item><title>Show HN: Watch fascism unfold in realtime – an AI-powered tracker</title><link>https://www.realtimefascism.com/</link><author>visekr</author><category>dev</category><category>hn</category><pubDate>Thu, 6 Feb 2025 17:02:36 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>