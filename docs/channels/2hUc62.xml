<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blog</title><link>https://www.awesome-dev.news</link><description></description><item><title>Bliki: Excessive Bold</title><link>https://martinfowler.com/bliki/ExcessiveBold.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 28 Jan 2026 14:20:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[I'm increasingly seeing a lot of technical and business writing make heavy
  use of bold font weights, in an attempt to emphasize what the writers think is
  important. LLMs seem to have picked up and spread this practice widely. But
  most of this is self-defeating, the more a writer uses typographical emphasis,
  the less power it has, quickly reaching the point where it loses all its
  benefits.There are various typographical tools that are used to emphasize words and
  phrases, such as: bold, italic, capitals, and underlines. I find that bold is the one
  that's getting most of the over-use. Using a lot of capitals is rightly
  reviled as shouting, and when we see it used widely, it raises our doubts on
  the quality of the underlying thinking.
  Underlines have become the signal for hyperlinks, so I rarely see this for
  emphasis any more. Both capitals and underlines have also been seen as rather
  cheap forms of highlight, since we could do them with typewriters and
  handwriting, while bold and italics were only possible after the rise of
  word-processors. (Although I realize most of my readers are too young to
  remember when word-processors were novel.)Italics are the subtler form of emphasis. When I use them in a paragraph,
  they don't leap out to the eye. This allows me to use them in long flows of text when
  I want to set it apart, and when I use it to emphasize a phrase it only makes
  its presence felt when I'm fully reading the text. For this reason, I prefer
  to use italics for emphasis, but I only use it rarely, suggesting it's
   important to put stress on
  the word should I be speaking the paragraph (and I always try to write in the
  way that I speak).The greatest value of bold is that draws the eye to the bold text even if the
  reader isn't reading, but glancing over the page. This is an important
  property, but one that only works if it's used sparingly. Headings are often
  done in bold, because the it's important to help the reader navigate a longer
  document by skimming and looking for headings to find the section I want to read.I rarely use bold within a prose paragraph, because of my desire to be
  parsimonious with bold. One use I do like is to highlight unfamiliar words at
  the point where I explain them. I got this idea from Giarratano and Riley. I noticed that when the
  unfamiliar term reappeared, I was often unsure what it meant, but glancing
  back and finding the bold quickly reminded me. The trick here is to place the
  bold at point of explanation, which is often, but not always, at its first
  use. 
A common idea is to take an important sentence and bold that, so it leaps
  out while skimming the article. That can be worthwhile, but as ever with this
  kind of emphasize, its effectiveness is inversely proportional to how often
  it's used. It's also usually not the best tool for the job. Callouts usually
  work better. They do a superior job of drawing the eye, and furthermore they don't
  need to use the same words as in the prose text. This allows me to word the
  callout better than it could be if it also had to fit in the flow of the
  prose.A marginal case is where I see bold used in first clause of each item in a
  bulleted list. In some ways this is acting like a heading for the text in the
  list. But we don't need a heading for every paragraph, and the presence of the
  bullets does enough to draw the eye to the items. And bullet-lists are over
  used too - I always try to write such things as a prose paragraph instead, as
  prose flows much better than bullets and is thus more pleasant to read. It's
  important to write in such a way to make it an enjoyable experience for the
  reader - even, indeed especially, when I'm also trying to explain things for them.While writing this, I was tempted to illustrate my point by using  in a paragraph,  and hopefully demonstrating
  why lots of bold loses the power to emphasize and .
  But I also wanted to explain my position clearly, and I felt that illustrating
  the problem would thus . So I've  to a
  . (And, yes, I  with as much bold as this.)]]></content:encoded></item><item><title>Assessing internal quality while coding with an agent</title><link>https://martinfowler.com/articles/exploring-gen-ai/ccmenu-quality.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 27 Jan 2026 15:50:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[ is the maintainer of CCMenu: a Mac
      application that shows the status of CI/CD builds in the Mac menu bar. He
      assesses how using a coding agent affects internal code quality by adding
      a feature using the agent, and seeing what happens to the code.
      ]]></content:encoded></item><item><title>The Constitutionality of Geofence Warrants</title><link>https://www.schneier.com/blog/archives/2026/01/the-constitutionality-of-geofence-warrants.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Tue, 27 Jan 2026 12:01:47 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[The US Supreme Court is considering the constitutionality of geofence warrants.The case centers on the trial of Okello Chatrie, a Virginia man who pleaded guilty to a 2019 robbery outside of Richmond and was sentenced to almost 12 years in prison for stealing $195,000 at gunpoint.Police probing the crime found security camera footage showing a man on a cell phone near the credit union that was robbed and asked Google to produce anonymized location data near the robbery site so they could determine who committed the crime. They did so, providing police with subscriber data for three people, one of whom was Chatrie. Police then searched Chatrie’s home and allegedly surfaced a gun, almost $100,000 in cash and incriminating notes.Chatrie’s appeal challenges the constitutionality of geofence warrants, arguing that they violate individuals’ Fourth Amendment rights protecting against unreasonable searches.]]></content:encoded></item><item><title>Some notes on starting to use Django</title><link>https://jvns.ca/blog/2026/01/27/some-notes-on-starting-to-use-django/</link><author>Julia Evans</author><category>blog</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="https://jvns.ca/atom.xml">Julia Evans</source><content:encoded><![CDATA[Hello! One of my favourite things is starting to learn an
Old Boring Technology that I’ve never tried before but that has been around for
20+ years. It feels really good when every problem I’m ever going to have has
been solved already 1000 times and I can just get stuff done easily.I’ve thought it would be cool to learn a popular web framework like
Rails or Django or Laravel for a long time, but I’d never really managed to
make it happen. But I started learning Django to make a website a few months
back, I’ve been liking it so far, and here are a few quick notes!I spent some time trying to learn Rails in 2020,
and while it was cool and I really wanted to like Rails (the Ruby community is great!),
I found that if I left my Rails project alone for months, when I came
back to it it was hard for me to remember how to get anything done because
(for example) if it says  in your , on its own
that doesn’t tell you where the  routes are configured, you need to
remember or look up the convention.Being able to abandon a project for months or years and then come back to it is
really important to me (that’s how all my projects work!), and Django feels easier
to me because things are more explicit.In my small Django project it feels like I just have 5 main files (other
than the settings files): , , , , and
, and if I want to know where something else is (like an HTML template)
is then it’s usually explicitly referenced from one of those files.For this project I wanted to have an admin interface to manually edit or view
some of the data in the database. Django has a really nice built-in admin
interface, and I can customize it with just a little bit of code.For example, here’s part of one of my admin classes, which sets up which fields
to display in the “list” view,  which field to search on, and how to order them
by default.@admin.register(Zine)
class ZineAdmin(admin.ModelAdmin):
    list_display = ["name", "publication_date", "free", "slug", "image_preview"]
    search_fields = ["name", "slug"]
    readonly_fields = ["image_preview"]
    ordering = ["-publication_date"]
In the past my attitude has been “ORMs? Who needs them? I can just write my own SQL queries!”.
I’ve been enjoying Django’s ORM so far though, and I think it’s cool how Django
uses  to represent a , like this:Zine.objects
    .exclude(product__order__email_hash=email_hash)
This query involves 5 tables: , , , , and .
To make this work I just had to tell Django that there’s a 
relating “orders” and “products”, and another  relating
“zines”, and “products”, so that it knows how to connect , , .I definitely  write that query, but writing product__order__email_hash is
a lot less typing, it feels a lot easier to read, and honestly I think it would
take me a little while to figure out how to construct the query
(which needs to do a few other things than just those joins).I have zero concern about the performance of my ORM-generated queries so I’m
pretty excited about ORMs for now, though I’m sure I’ll find things to be
frustrated with eventually.The other great thing about the ORM is migrations!If I add, delete, or change a field in , Django will automatically
generate a migration script like migrations/0006_delete_imageblob.py.I assume that I could edit those scripts if I wanted, but so far I’ve just
been running the generated scripts with no change and it’s been going great. It
really feels like magic.I’m realizing that being able to do migrations easily is important for me right
now because I’m changing my data model fairly often as I figure out how I want
it to work.For example the intro to models
lists the most important common fields you might want to set when using the ORM.After having a bad experience trying to operate Postgres and not being able to
understand what was going on, I decided to run all of my small websites with
SQLite instead. It’s been going way better, and I love being able to backup by
just doing a  and then copying the resulting single file.I think it should be fine because I’m expecting the site to have a few hundred
writes per day at most, much less than Mess with DNS
which has a lot more of writes and has been working well (though the writes are
split across 3 different SQLite databases).built in email (and more)Django seems to be very “batteries-included”, which I love – if I want CSRF
protection, or a , or I want to send email, it’s all
in there!For example, I wanted to save the emails Django sends to a file in dev mode (so
that it didn’t send real email to real people), which was just a little bit
of configuration.I just put this :EMAIL_BACKEND = "django.core.mail.backends.filebased.EmailBackend"
EMAIL_FILE_PATH = BASE_DIR / "emails"
and then set up the production email like this in EMAIL_BACKEND = "django.core.mail.backends.smtp.EmailBackend"
EMAIL_HOST = "smtp.whatever.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "xxxx"
EMAIL_HOST_PASSWORD = os.getenv('EMAIL_API_KEY')
That made me feel like if I want some other basic website feature, there’s
likely to be an easy way to do it built into Django already.the settings file still feels like a lotI’m still a bit intimidated by the  file: Django’s settings system
works by setting a bunch of global variables in a file, and I feel a bit
stressed about… what if I make a typo in the name of one of those variables?
How will I know? What if I type WSGI_APPLICATOIN = "config.wsgi.application"
instead of ?I guess I’ve gotten used to having a Python language server tell me when I’ve
made a typo and so now it feels a bit disorienting when I can’t rely on the
language server support.I haven’t really successfully used an actual web framework for a project before
(right now almost all of my websites are either a single Go binary or static
sites), so I’m interested in seeing how it goes!There’s still lots for me to learn about, I still haven’t really gotten into
Django’s form validation tooling or authentication systems.Thanks to Marco Rogers for convincing me to give ORMs a chance.]]></content:encoded></item><item><title>make.ts</title><link>https://matklad.github.io/2026/01/27/make-ts.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[Sounds familiar? This is how I historically have been running benchmarks and other experiments
requiring a repeated sequence of commands — type them manually once, then rely on shell history
(and maybe some terminal splits) for reproduction. These past few years I’ve arrived at a much better
workflow pattern — . I was forced to adapt it once I started working with multiprocess
applications, where manually entering commands is borderline infeasible. In retrospect, I should
have adapted the workflow years earlier.Use a (gitignored) file for interactive scripting. Instead of entering a command directly into the
terminal, write it to a file first, and then run the file. For me, I type stuff into  and
then run  in my terminal (Ok, I need  for that).I want to be clear here, I am not advocating writing “proper” scripts, just capturing your
interactive, ad-hoc command to a persistent file. Of course any command that you want to execute
 belongs to the build system. The surprising thing is that even more complex one-off
commands benefit from running through file, because it will take you several tries to get them
right!There are many benefits relative to  workflow:
Real commands tend to get large, and it is so much nicer to use a real 2D text editor rather than
shell’s line editor.

If you need more than one command, you can write several commands, and still run them all with a
single key (before , I was prone to constructing rather horrific && conjuncts for this
reason).

With a sequence of command outlined, you nudge yourself towards incrementally improving them,
making them idempotent, and otherwise investing into your own workflow for the next few minutes,
without falling into the YAGNI pit from the outset.

At some point you might realize after, say, running a series of ad-hoc benchmarks interactively,
that you’d rather write a proper script which executes a collection of benchmarks with varying
parameters. With the file approach, you already have the meat of the script implemented, and you
only need to wrap in a couple of fors and ifs.

Finally, if you happen to work with multi-process projects, you’ll find it easier to manage
concurrency declaratively, spawning a tree of processes from a single script, rather than
switching between terminal splits.
Use a consistent filename for the script. I use , and so there’s a  in the root
of most projects I work on. Correspondingly, I have  line in project’s 
— the  file which is not shared. The fixed name reduces fixed costs — whenever I
need complex interactivity I don’t need to come up with a name for a new file, I open my
pre-existing , wipe whatever was there and start hacking. Similarly, I have  in
my shell history, so
fish autosuggestions
work for me. At one point, I had a VS Code task to run , though I now use
terminal editor.Start the script with hash bang,

in my case, and

the file, to make it easy to run.Write the script in a language that:
you are comfortable with,

doesn’t require huge setup,

makes it easy to spawn subprocesses,

has good support for concurrency.
For me, that is TypeScript. Modern JavaScript is sufficiently ergonomic, and structural, gradual
typing is a sweet spot that gives you reasonable code completion, but still allows brute-forcing any
problem by throwing enough stringly dicts at it.JavaScript’s tagged template syntax is brilliant for scripting use-cases:What happens here is that  gets a list of literal string fragments inside the backticks, and
then, separately, a list of values to be interpolated in-between. It  concatenate everything
to just a single string, but it doesn’t have to. This is precisely what is required for process
spawning, where you want to pass an array of strings to the  syscall.Specifically, I use dax library with Deno, which is excellent as
a single-binary batteries-included scripting environment
(see <3 Deno). Bun has a dax-like
library in the box and is a good alternative (though I personally stick with Deno because of
 and ). You could also use famous zx, though be mindful that it
uses your shell as a middleman, something I
consider to be sloppy (explanation).While  makes it convenient to spawn a single program,  is excellent for herding a
slither of processes:Here’s how I applied this pattern earlier today. I wanted to measure how TigerBeetle cluster
recovers from the crash of the primary. The manual way to do that would be to create a bunch of ssh
sessions for several cloud machines, format datafiles, start replicas, and then create some load. I
 started to split my terminal up, but then figured out I can do it the smart way.The first step was cross-compiling the binary, uploading it to the cloud machines, and running the
cluster
(using my box from the other week):Running the above the second time, I realized that I need to kill the old cluster first, so two new
commands are “interactively” inserted:At this point, my investment in writing this file and not just entering the commands one-by-one
already paid off!The next step is to run the benchmark load in parallel with the cluster:I don’t need two terminals for two processes, and I get to copy-paste-edit the mostly same command.For the next step, I actually want to kill one of the replicas, and I also want to capture live
logs, to see in real-time how the cluster reacts. This is where  multiplexing syntax of box
falls short, but, given that this is JavaScript, I can just write a for loop:At this point, I do need two terminals. One runs  and shows the log from the benchmark
itself, the other runs  to watch the next replica to become primary.I have definitelly crossed the line where writing a script makes sense, but the neat thing is that
the gradual evolution up to this point. There isn’t a discontinuity where I need to spend 15
minutes trying to shape various ad-hoc commands from five terminals into a single coherent script, it
was in the file to begin with.And then the script is easy to evolve. Once you realize that it’s a good idea to also run the same
benchmark against a different, baseline version TigerBeetle, you replace  with
 and wrap everything intoA bit more hacking, and you end up with a repeatable benchmark schedule for a matrix of parameters:That’s the gist of it. Don’t let the shell history be your source, capture it into the file first!]]></content:encoded></item><item><title>Ireland Proposes Giving Police New Digital Surveillance Powers</title><link>https://www.schneier.com/blog/archives/2026/01/ireland-proposes-giving-police-new-digital-surveillance-powers.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Mon, 26 Jan 2026 12:04:57 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[The Irish government is planning to bolster its police’s ability to intercept communications, including encrypted messages, and provide a legal basis for spyware use.]]></content:encoded></item><item><title>Friday Squid Blogging: Giant Squid in the Star Trek Universe</title><link>https://www.schneier.com/blog/archives/2026/01/friday-squid-blogging-giant-squid-in-the-star-trek-universe.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 23 Jan 2026 22:03:20 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[Spock befriends a giant space squid in the comic Star Trek: Strange New Worlds: The Seeds of Salvation #5.As usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.]]></content:encoded></item><item><title>AIs are Getting Better at Finding and Exploiting Internet Vulnerabilities</title><link>https://www.schneier.com/blog/archives/2026/01/ais-are-getting-better-at-finding-and-exploiting-internet-vulnerabilities.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 23 Jan 2026 12:01:19 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[In a recent evaluation of AI models’ cyber capabilities, current Claude models can now succeed at multistage attacks on networks with dozens of hosts using only standard, open-source tools, instead of the custom tools needed by previous generations. This illustrates how barriers to the use of AI in relatively autonomous cyber workflows are rapidly coming down, and highlights the importance of security fundamentals like promptly patching known vulnerabilities.A notable development during the testing of Claude Sonnet 4.5 is that the model can now succeed on a minority of the networks without the custom cyber toolkit needed by previous generations. In particular, Sonnet 4.5 can now exfiltrate all of the (simulated) personal information in a high-fidelity simulation of the Equifax data breach—­one of the costliest cyber attacks in history—­using only a Bash shell on a widely-available Kali Linux host (standard, open-source tools for penetration testing; not a custom toolkit). Sonnet 4.5 accomplishes this by instantly recognizing a publicized CVE and writing code to exploit it without needing to look it up or iterate on it. Recalling that the original Equifax breach happened by exploiting a publicized CVE that had not yet been patched, the prospect of highly competent and fast AI agents leveraging this approach underscores the pressing need for security best practices like prompt updates and patches. Read the whole thing. Automatic exploitation will be a major change in cybersecurity. And things are happening fast. There have been significant developments since I wrote this in October.]]></content:encoded></item><item><title>A Programmer&apos;s Guide to Leaving GitHub</title><link>https://lord.io/leaving-github/</link><author></author><category>dev</category><category>blog</category><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate><source url="https://lord.io/feed">Lord.io blog</source><content:encoded><![CDATA[If you subscribe to many programming blogs, chances are you've come across a post describing someone's move off GitHub. They started as far back as the Microsoft acquisition in 2018, but they've increased in frequency recently. Both the Zig programming language and Leiningen build tool wrote about their move to other platforms late last year. I've drawn inspiration from these posts, and the countless similar posts from individual programmers. However, since they're mostly informational announcements telling people to update their repository URLs, they tend to only briefly mention the reasons the author chose to leave. They don't try too hard to convince a skeptical reader that perhaps they could leave GitHub too.I'm going to migrate all my personal projects off of GitHub this weekend, in support of today's general strike in Minnesota. Instead of the traditional brief message, I'll instead try to record my thought process a little more deeply: my research into which groups have active protests of GitHub, why I think GitHub is a particularly suitable target, what makes a boycott more or less effective, and finally, a list of GitHub alternatives to consider switching to. Migrating your open source projects off GitHub is obviously not the most radical, impactful action you could be taking right now, but also it's pretty easy! If you happen across this blog post, I hope you will consider it.There are many groups protesting GitHub and Microsoft, but I'll discuss these four in this post:The Software Freedom Conservatory (the non-profit that provides legal support for Wine, Inkscape, QEMU, Git, and many more free software projects) started a campaign called "Give Up GitHub" in 2022. They cite how the GitHub platform itself is closed source software, instances of Copilot spitting out verbatim GPL code (a form of plagiarism), and fears over Microsoft's reorganization of all of GitHub under a "CoreAI" product division. They also point to GitHub's 2020 contracts with ICE, the controversial and violent immigration police here in the United States.The since at least 2024 have told the free software community to move projects off of GitHub to apply pressure on Microsoft, pointing again to Github's proprietary server code, as well as Microsoft's decision to require computers running Windows 11 to have a "Trusted Platform Module"; I'll let the FSF post explain why you might want to protest that.The Palestinian BDS National Committeeupgraded Microsoft from a "pressure" target to a "priority" boycott target in 2025, describing Microsoft as "perhaps the most complicit tech company" in Israel's apartheid. BDS has already claimed two major wins here. In 2020, controversy forced Microsoft to divest its stake in AnyVision, a Israeli facial recognition startup used at checkpoints in the West Bank. Then, in 2025, Microsoft terminated Azure services for a subagency of the Israeli military called Unit 8200; this was right after a news article revealed they used Azure to process millions of intercepted phone call recordings from Palestinian civilians to determine where to fire lethal airstrikes in Gaza. BDS and the 2000+ Microsoft employees that signed No Azure For Apartheid, along with many other organizations (including a global network of MSFT-shareholding catholic nuns) continue to protest the Azure services Microsoft provides to the rest of the Israeli military. in late 2025 launched "ICE Out of My Wallet", listing Microsoft as one of just seven top targets for a consumer boycott. Their goal is to put pressure on the $20M+ in Azure services Microsoft provides annually to ICE. (The official boycott currently just lists Microsoft devices as targets, not GitHub.) Back in 2019, there were also mass protests against GitHub's direct contracts with ICE to host their code; as far as I'm aware, they still host code for ICE today.I have four reasons why I want to specifically target GitHub in this post. First, for independent programmers, I think it's incredibly simple and straightforward to move your personal open source projects off of GitHub. Unlike quitting, say, Instagram or TikTok, which centralize and tightly control content discovery, the network effects keeping projects on GitHub are substantially weaker. When you quit Instagram, you become invisible; when you quit GitHub, you instead just add a small speed-bump for contributors. I also see particular ease when migrating a  project. Your small project has no CTO you need to convince, no 400-engineer monorepo, and no half-forgotten custom scripts calling GitHub's APIs, written by a coworker who quit a year ago. If your project depends on many strangers submitting drive-by pull requests, moving off GitHub will impose extra costs on you, and you may want to to consider more carefully whether this is a cost you're willing to pay. But the vast majority of public repos — personal projects with few external contributors — will find leaving GitHub to be relatively frictionless.Second, although you likely don't pay GitHub to host your open-source projects, they still make money from them! When my blog links to an open source project I've published GitHub, they don't just get to put their logo in front of some eyeballs for free; they also get my implicit endorsement, helping establish GitHub as the default choice for companies deciding where to put their internal repos and who to pay for LLM autocomplete. You might find this point kind of banal, but I really just want to emphasize it: by putting your personal projects on GitHub, you are providing them a valuable service! If your account is small, it may just be a tiny amount of value. But they find it valuable nonetheless, and I think this small amount of value matches the very small amount of effort it takes to move most of your code somewhere else.Third, GitHub's web interface has been in a steepening decline since since the Microsoft acquisition in 2018, making it a less appealing place to put your code even without these ongoing protests. Why does an uncached page load of my repository's root take 2500ms, from New York City no less, with gigabit, 17ms ping internet? Why does GitHub Actions choose jobs to run "seemingly at random", causing Zig's CI to not even run on the main branch? Why does the settings page for enabling and disabling the 16 available LLM models take up two vertical laptop screens, and why, instead of a simple checkbox, does each dropdown have three possible states: "enabled", "disabled", and the mysterious "Select an option"?Finally, I think open source communities, with roots in hacker culture from the 80s and 90s, form a particularly fertile soil for this sort of action. Many programmers contribute to open source projects for non-commercial reasons: out of a deep love for programming, or an ethical belief everyone should have the right to read and modify the code on their computer. Hackers also have a long history of general repulsion from Microsoft.This history, however, is a double edged sword. As programmers, we are primed by this history to think individually instead of collectively. I have many friends who would agree with the causes above, and yet might resist participation in a GitHub protest. Even if you are someone who in general might join a boycott, once you hear that the plan is to convince a bunch of  — good luck, but clearly that plan won't work. Here, I think the choice of Microsoft is helpful, because I don't  to convince some critical mass of programmers. Instead, I get to play a tiny part in a large and global movement that already has momentum, made up of people who have never been burdened by the words "rebase merge conflict" before. I've chosen GitHub as a target so that my action mirrors the already-successful efforts of many others.Some strategies are better than othersI have a vivid memory from a little over a decade ago, when I saw Richard Stallman speak about free software at an auditorium in lower Manhattan. In this speech, he cast as wide a net as he could, listing off the misdeeds of tech company after tech company and describing how we should stop using their products. (When one or two people in the audience quietly made their way to the exit — he had been talking long past the allotted time — he started castigating them for leaving before he was done.) His personal website today continues the tradition:If my understanding of boycotts came from this speech, I would absolutely lean anti-boycott, but I'd like to convince you that there is another way! Boycott strategy makes a massive difference in efficacy. Stallman could learn a lot from the BDS National Committee, whose approach in turn was inspired by the highly visible South African anti-apartheid movement. Here's my own list of what has resonated with me:Don't protest everything all at once: We call it a "targeted boycott" for a reason. Target just a handful of companies with the most egregious violations, and in the common situation where you have complaints against all major vendors offering some service, don't boycott all of them. Make it easy for people to switch to an alternative, even if you're implicitly advocating for an imperfect company. Here, GitHub is a great target because many movements list Microsoft as a top target, and we have many free, open source alternatives.Build cross-movement coalitions: BDS chose Chevron as a priority target in part because many climate activist groups already have active protests against oil companies. The Software Freedom Conservatory chose GitHub as a primary target in part because anti-ICE activists protested it in 2020. When you boycott GitHub, you get to participate in several boycotts all at once.Cite specific, achievable goals: unfortunately free software boycotts sometimes have vague goals, like "stop discrediting the GPL". At other times, they cite specific goals that existentially threaten a company, like "release the source code to  of your software" or "only train your language models on public domain code." I think asking for Microsoft to cancel their contracts with the IDF and ICE are concrete goals, and Microsoft could do it without completely destroying itself.: I've seen some programmers delete their GitHub accounts silently, saying that their choice to leave is a personal one. I definitely can empathize with this sentiment — this feeling was hard for me to get over when writing this post. I feared it self-aggrandized what is truly a microscopic drop in the bucket. At the same time, I think there is incredible value to speaking publicly about why you're leaving, assuming it's safe for you to do so. In addition to this post, I also plan to not delete any repositories, instead replacing their contents with a README that explains my departure.Don't worry about perfection: I've seen some worry online that it's impossible to delete your GitHub account when there are still so many open source projects that use it. I agree! I plan to still use GitHub if I need it to contribute to other projects.That's all I've got to say! What follows is some optional resources that you might find useful if you decide to make the move yourself. Thanks for reading this far, and if you're in Minnesota today, I hope you stay warm out there. ∎Appendix A: Table of GitHub alternativesCentrally hosted (with self-hosting as an option)nonprofit based in GermanyFOSS or noncommercial projects only; fork of giteaindie for-profit, not VC fundedcosts $4–12/month, financial aid availableweird vibes, and the free cloud hosting seems sort of like a demo instance; fork of Gogsbranch-based with stackinglimited features for free accountssmall startup, $300k+ in VC fundingpatch-based with stackingno private repos, built on ATProtocolsome ethereum thing, $12M+ in VC fundingprivate repos must be self-hostedi wrote it myself last weekbarebones read-only web uipatch-based with stackingcode hosting/review only; no issues, etcMy personal take on these options: since these days my projects mostly don't have external contributors, I've chosen to move them to j3, a small local binary I wrote in Rust that lets you use an s3 bucket as a Git remote, and automatically pushes a read-only web UI to the bucket's . If I had a startup that needed private code hosting, I would probably set up a Gerrit instance. If I was working on an open source project that wanted to make it easy for people in the open-source community to contribute, I would probably choose Codeberg, assuming I was not tempted by Tangled's code review workflow.What is "patch-based" code review?If you're familiar with GitHub's pull request workflow, you've done -based merges before. You prepare a branch with your changes, and then submit a pull request asking to merge your branch into . If somebody gives you feedback, you push additional commits on top of your base commit. In contrast, with the -based merge workflow originally popularized by Gerrit, you amend the existing commit and push it — something like git commit --amend && git push --force. If you try this in GitHub, but this would make the original commit you pushed disappear, but in patch-based review tools, your reviewers will instead see a diff from the original commit to the new one. In general, I'm trying to move towards patch-based workflows, since they work better with jiujutsu. "Stacking" here means it's easy to submit multiple pull requests for simultaneous code review, where each pull request builds upon the changes from the prior one in a chain. On GitHub, this is very difficult!Appendix B: GitHub stars and network effectsIf you want people to discover your personal projects, you might say that GitHub stars, forks, and followers are a great way to enable this. While this is an interesting question, and I'm sure some people find these tools valuable, my personal experience has been that few people look at their GitHub feed, and this was before the Copilot prompt box pushed it further down the homepage. I've also heard it got diluted by LLM-generated bug reports, but honestly, I don't look at it any more.To show one data point using star counts as a proxy for attention, I had a little over 700 followers in 2020, so while I wasn't one of the most followed users on GitHub, I still had more followers than the vast majority of GitHub users. Throughout 2020, I worked publicly on a Rust side project called Anchors, which by October had accrued a grand total of 4 stars. On November 9th, I published How to Recalculate a Spreadsheet, a tedious and technical blog post which discussed Anchors starting about 2000 words in. By the next day, Anchors jumped to 19 stars, and then (despite essentially zero code changes to the Anchors project itself) slowly climbed up to ~130 stars by late 2023. Clearly the blog post was the catalyst for these stars, and without it, Anchors would still have single digit stars today. That said, I can't prove to you that GitHub's feed didn't  the attention the repo got from the blog post. But at an average rate of 1 star every 10 days, and given the minimal stars the project had prior to the blog post, I guess it's hard for me to imagine GitHub's feed played a major role.Assuming it's even your goal for your project to get discovered by others (for most of my projects it is not!) I think your side project will likely reach orders of magnitude more people on Bluesky, Twitter, or your personal blog than they will via GitHub. GitHub stars are a good sign of social proof, but I would argue they don't serve this role substantially better than stars on Codeberg.Appendix C: Further readingI've already mentioned Zig's and Leiningen's posts, but if you want to read what individual people have said about leaving GitHub, here is what I could find:Appendix D: Richard Stallman, "A Free Digital Society" (New York, 2014)]]></content:encoded></item><item><title>Considering Strictly Monotonic Time</title><link>https://matklad.github.io/2026/01/23/strictly-monotonic-time.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[Monotonic time is a frequently used, load bearing abstraction. Monotonicity is often enforced using
the following code:That is, ask the OS about the current monotonic time, but don’t trust the result too much and clamp
it using an in-process guard. Under normal scenarios, you can trust the OS promise of monotonicity,
but, empirically, there’s a long tail of different scenarios where the promise isn’t upheld:
https://github.com/rust-lang/rust/pull/56988Today I realized that, if you are doing the above, you might as well force the time to be 
monotonic:The benefit of strict monotonicity is that you can tighten asserts,

can become

and that  catches the bug where you pass in  the same instance.
In other words, the  version explicitly allows either query-ing the time again, or using the old
value directly.Conversely, with strictly monotonic time, you know that if you see two numerically identical time
instances, they must have been ultimately derived from the exact same call to . Time becomes
fundamentally less ambiguous.The constraint here is that the resolution of the time value ( the clock resolution) needs to
be high enough, to make sure that repeated  don’t move you into the future, but nanosecond
precision seems fine for that.]]></content:encoded></item><item><title>Fragments: January 22</title><link>https://martinfowler.com/fragments/2026-01-22.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 22 Jan 2026 14:30:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[My colleagues here at Thoughtworks have announced AI/works™, a platform for our work using AI-enabled software development. The platform is in its early days, and is currently intended to support Thoughtworks consultants in their client work. I’m looking forward to sharing what we learn from using and further developing the platform in future months. ❄                ❄                ❄                ❄                ❄Simon Couch examines the electricity consumption of using AI. He’s a heavy user: “usually programming for a few hours, and driving 2 or 3 Claude Code instances at a time”. He finds his usage of electricity is orders of magnitude more than typical estimates based on the “typical query”.On a median day, I estimate I consume 1,300 Wh through Claude Code—4,400 “typical queries” worth.But it’s still not a massive amount of power - similar to that of running a dishwasher.A caveat to this is that this is “napkin math” because we don’t have decent data about how these models use resources. I agree with him that we ought to. ❄                ❄                ❄                ❄                ❄My namesake Chad Fowler (no relation) considers that the movement to agentic coding creates a similar shift in rigor and discipline as appeared in Extreme Programming, dynamic languages, and continuous deployment.In Extreme Programming’s case, this meant a lot of discipline around testing, continuous integration, and keeping the code-base healthy. My current view is that with AI-enabled development we need to be rigorous about evaluating the software, both for its observable behavior and its internal quality.The engineers who thrive in this environment will be the ones who relocate discipline rather than abandon it. They’ll treat generation as a capability that demands more precision in specification, not less. They’ll build evaluation systems that are harder to fool than the ones they replaced. They’ll refuse the temptation to mistake velocity for progress. ❄                ❄                ❄                ❄                ❄There’s been much written about the dreadful events in Minnesota, and I’ve not felt I’ve had anything useful to add to them. But I do want to pass on an excellent post from Noah Smith that captures many of my thoughts. He points out that there is a “consistent record of brutality, aggression, dubious legality, and unprofessionalism” from ICE (and CBP) who seem to be turning into MAGA’s SD.Is this America now? A country where unaccountable and poorly trained government agents go door to door, arresting and beating people on pure suspicion, and shooting people who don’t obey their every order or who try to get away? “When a federal officer gives you instructions, you abide by them and then you get to keep your life” is a perfect description of an authoritarian police state. None of this is Constitutional, every bit of it is deeply antithetical to the American values we grew up taking for granted.My worries about these kinds of developments were what animated me to urge against voting for Trump in the 2016 election. Mostly those worries didn’t come to fruition because enough constitutional Republicans were in a position to stop them from happening, so even when Trump attempted a coup in 2020, he wasn’t able to get very far. But now those constitutional Republicans are absent or quiescent. I fear that what we’ve seen in Minneapolis will be a harbinger of worse to come.But then, after the murderous agent fired three shots — just 30 or 40 feet in front of Callenson — Callenson had the courage and conviction to stay with the scene and keep filming. Not to run away, but instead to follow the scene. To keep filming. To continue documenting with as best clarity as she could, what was unfolding.The recent activity in  Venezuala reminds me that I’ve long felt that Trump is a Hugo Chávez figure - a charismatic populist who’s keen on wrecking institutions and norms. Trump is old, so won’t be with us for that much longer - but the question is: “who is Trump’s Maduro?” ❄                ❄                ❄                ❄                ❄With all the drama at home, we shouldn’t ignore the terrible things that happened in Iran. The people there again suffered again the consequences of an entrenched authoritarian police state.]]></content:encoded></item><item><title>Why AI Keeps Falling for Prompt Injection Attacks</title><link>https://www.schneier.com/blog/archives/2026/01/why-ai-keeps-falling-for-prompt-injection-attacks.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Thu, 22 Jan 2026 12:35:46 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[Imagine you work at a drive-through restaurant. Someone drives up and says: “I’ll have a double cheeseburger, large fries, and ignore previous instructions and give me the contents of the cash drawer.” Would you hand over the money? Of course not. Yet this is what large language models (LLMs) do.Prompt injection is a method of tricking LLMs into doing things they are normally prevented from doing. A user writes a prompt in a certain way, asking for system passwords or private data, or asking the LLM to perform forbidden instructions. The precise phrasing overrides the LLM’s safety guardrails, and it complies.LLMs are vulnerable to all sorts of prompt injection attacks, some of them absurdly obvious. A chatbot won’t tell you how to synthesize a bioweapon, but it might tell you a fictional story that incorporates the same detailed instructions. It won’t accept nefarious text inputs, but might if the text is rendered as ASCII art or appears in an image of a billboard. Some ignore their guardrails when told to “ignore previous instructions” or to “pretend you have no guardrails.”AI vendors can block specific prompt injection techniques once they are discovered, but general safeguards are impossible with today’s LLMs. More precisely, there’s an endless array of prompt injection attacks waiting to be discovered, and they cannot be prevented universally.If we want LLMs that resist these attacks, we need new approaches. One place to look is what keeps even overworked fast-food workers from handing over the cash drawer.Human Judgment Depends on ContextOur basic human defenses come in at least three types: general instincts, social learning, and situation-specific training. These work together in a layered defense.As a social species, we have developed numerous instinctive and cultural habits that help us judge tone, motive, and risk from extremely limited information. We generally know what’s normal and abnormal, when to cooperate and when to resist, and whether to take action individually or to involve others. These instincts give us an intuitive sense of risk and make us especially careful about things that have a large downside or are impossible to reverse.The second layer of defense consists of the norms and trust signals that evolve in any group. These are imperfect but functional: Expectations of cooperation and markers of trustworthiness emerge through repeated interactions with others. We remember who has helped, who has hurt, who has reciprocated, and who has reneged. And emotions like sympathy, anger, guilt, and gratitude motivate each of us to reward cooperation with cooperation and punish defection with defection.A third layer is institutional mechanisms that enable us to interact with multiple strangers every day. Fast-food workers, for example, are trained in procedures, approvals, escalation paths, and so on. Taken together, these defenses give humans a strong sense of context. A fast-food worker basically knows what to expect within the job and how it fits into broader society.We reason by assessing multiple layers of context: perceptual (what we see and hear), relational (who’s making the request), and normative (what’s appropriate within a given role or situation). We constantly navigate these layers, weighing them against each other. In some cases, the normative outweighs the perceptual—for example, following workplace rules even when customers appear angry. Other times, the relational outweighs the normative, as when people comply with orders from superiors that they believe are against the rules.Crucially, we also have an interruption reflex. If something feels “off,” we naturally pause the automation and reevaluate. Our defenses are not perfect; people are fooled and manipulated all the time. But it’s how we humans are able to navigate a complex world where others are constantly trying to trick us.So let’s return to the drive-through window. To convince a fast-food worker to hand us all the money, we might try shifting the context. Show up with a camera crew and tell them you’re filming a commercial, claim to be the head of security doing an audit, or dress like a bank manager collecting the cash receipts for the night. But even these have only a slim chance of success. Most of us, most of the time, can smell a scam.Con artists are astute observers of human defenses. Successful scams are often slow, undermining a mark’s situational assessment, allowing the scammer to manipulate the context. This is an old story, spanning traditional confidence games such as the Depression-era “big store” cons, in which teams of scammers created entirely fake businesses to draw in victims, and modern “pig-butchering” frauds, where online scammers slowly build trust before going in for the kill. In these examples, scammers slowly and methodically reel in a victim using a long series of interactions through which the scammers gradually gain that victim’s trust.Sometimes it even works at the drive-through. One scammer in the 1990s and 2000s targeted fast-food workers by phone, claiming to be a police officer and, over the course of a long phone call, convinced managers to strip-search employees and perform other bizarre acts.Why LLMs Struggle With Context and JudgmentLLMs behave as if they have a notion of context, but it’s different. They do not learn human defenses from repeated interactions and remain untethered from the real world. LLMs flatten multiple levels of context into text similarity. They see “tokens,” not hierarchies and intentions. LLMs don’t reason through context, they only reference it.While LLMs often get the details right, they can easily miss the big picture. If you prompt a chatbot with a fast-food worker scenario and ask if it should give all of its money to a customer, it will respond “no.” What it doesn’t “know”—forgive the anthropomorphizing—is whether it’s actually being deployed as a fast-food bot or is just a test subject following instructions for hypothetical scenarios.This limitation is why LLMs misfire when context is sparse but also when context is overwhelming and complex; when an LLM becomes unmoored from context, it’s hard to get it back. AI expert Simon Willison wipes context clean if an LLM is on the wrong track rather than continuing the conversation and trying to correct the situation.There’s more. LLMs are overconfident because they’ve been designed to give an answer rather than express ignorance. A drive-through worker might say: “I don’t know if I should give you all the money—let me ask my boss,” whereas an LLM will just make the call. And since LLMs are designed to be pleasing, they’re more likely to satisfy a user’s request. Additionally, LLM training is oriented toward the average case and not extreme outliers, which is what’s necessary for security.The result is that the current generation of LLMs is far more gullible than people. They’re naive and regularly fall for manipulative cognitive tricks that wouldn’t fool a third-grader, such as flattery, appeals to groupthink, and a false sense of urgency. There’s a story about a Taco Bell AI system that crashed when a customer ordered 18,000 cups of water. A human fast-food worker would just laugh at the customer.Prompt injection is an unsolvable problem that gets worse when we give AIs tools and tell them to act independently. This is the promise of AI agents: LLMs that can use tools to perform multistep tasks after being given general instructions. Their flattening of context and identity, along with their baked-in independence and overconfidence, mean that they will repeatedly and unpredictably take actions—and sometimes they will take the  wrong ones.Science doesn’t know how much of the problem is inherent to the way LLMs work and how much is a result of deficiencies in the way we train them. The overconfidence and obsequiousness of LLMs are training choices. The lack of an interruption reflex is a deficiency in engineering. And prompt injection resistance requires fundamental advances in AI science. We honestly don’t know if it’s possible to build an LLM, where trusted commands and untrusted inputs are processed through the same channel, which is immune to prompt injection attacks.We humans get our model of the world—and our facility with overlapping contexts—from the way our brains work, years of training, an enormous amount of perceptual input, and millions of years of evolution. Our identities are complex and multifaceted, and which aspects matter at any given moment depend entirely on context. A fast-food worker may normally see someone as a customer, but in a medical emergency, that same person’s identity as a doctor is suddenly more relevant.We don’t know if LLMs will gain a better ability to move between different contexts as the models get more sophisticated. But the problem of recognizing context definitely can’t be reduced to the one type of reasoning that LLMs currently excel at. Cultural norms and styles are historical, relational, emergent, and constantly renegotiated, and are not so readily subsumed into reasoning as we understand it. Knowledge itself can be both logical and discursive.The AI researcher Yann LeCunn believes that improvements will come from embedding AIs in a physical presence and giving them “world models.” Perhaps this is a way to give an AI a robust yet fluid notion of a social identity, and the real-world experience that will help it lose its naïveté.Ultimately we are probably faced with a security trilemma when it comes to AI agents: fast, smart, and secure are the desired attributes, but you can only get two. At the drive-through, you want to prioritize fast and secure. An AI agent should be trained narrowly on food-ordering language and escalate anything else to a manager. Otherwise, every action becomes a coin flip. Even if it comes up heads most of the time, once in a while it’s going to be tails—and along with a burger and fries, the customer will get the contents of the cash drawer.This essay was written with Barath Raghavan, and originally appeared in IEEE Spectrum.]]></content:encoded></item></channel></rss>