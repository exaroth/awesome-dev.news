<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blog</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Conditional types in TypeScript</title><link>https://2ality.com/2025/02/conditional-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In TypeScript, conditional types let us make decisions (think if-then-else expressions) – which is especially useful in generic types. They are also an essential tool for working with union types because they let use “loop” over them. Read on if you want to know how all of that works.]]></content:encoded></item><item><title>Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!</title><link>https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 22:12:10 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[It will be streamed on the following YouTube Live channels:This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.The Async Django ORM: Where Is it?Speed at Scale for Django Web ApplicationsImplementing Agentic AI Solutions in Django from scratchDiving into DSF governance: past, present and futureGetting Knowledge from Django Hits: Using Grafana and PrometheusCulture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open SourceµDjango. The next step in the evolution of asynchronous microservices technology.A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.]]></content:encoded></item><item><title>Eli Bendersky: Decorator JITs - Python as a DSL</title><link>https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 21:49:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the "JIT decorator" pattern is pretty popular. For
example, this JAX snippet:In both cases, the function decorated with  doesn't get executed by the
Python interpreter in the normal sense. Instead, the code inside is more like
a DSL (Domain Specific Language) processed by a special purpose compiler built
into the library (JAX or Triton). Another way to think about it is that Python
is used as a  to describe computations.In this post I will describe some implementation strategies used by libraries to
make this possible.Preface - where we're goingThe goal is to explain how different kinds of  decorators work by using
a simplified, educational example that implements several approaches from
scratch. All the approaches featured in this post will be using this flow: Expr IR --> LLVM IR --> Execution" /> Expr IR --> LLVM IR --> Execution" class="align-center" src="https://eli.thegreenplace.net/images/2025/decjit-python.png" />
These are the steps that happen when a Python function wrapped with
our educational  decorator is called:The function is translated to an "expression IR" - .This expression IR is converted to LLVM IR.Finally, the LLVM IR is JIT-executed.First, let's look at the  IR. Here we'll make a big simplification -
only supporting functions that define a single expression, e.g.:Naturally, this can be easily generalized - after all, LLVM IR can be used to
express fully general computations.Here are the  data structures:To convert an  into LLVM IR and JIT-execute it, we'll use this function:It uses the  class to actually generate LLVM IR from .
This process is straightforward and covered extensively in the resources I
linked to earlier; take a look at the full code here.My goal with this architecture is to make things simple, but .
On one hand - there are several simplifications: only single expressions are
supported, very limited set of operators, etc. It's very easy to extend this!
On the other hand, we could have just trivially evaluated the 
without resorting to LLVM IR; I do want to show a more complete compilation
pipeline, though, to demonstrate that an arbitrary amount of complexity can
be hidden behind these simple interfaces.With these building blocks in hand, we can review the strategies used by
 decorators to convert Python functions into s.Python comes with powerful code reflection and introspection capabilities out
of the box. Here's the  decorator:This is a standard Python decorator. It takes a function and returns another
function that will be used in its place ( ensures that
function attributes like the name and docstring of the wrapper match the
wrapped function).After  is applied to , what  holds is the
wrapper. When  is called, the wrapper is invoked with
.The wrapper obtains the AST of the wrapped function, and then uses
 to convert this AST into an :When  finishes visiting the AST it's given, its
 field will contain the  representing the function's
return value. The wrapper then invokes  with this .Note how our decorator interjects into the regular Python execution process.
When  is called, instead of the standard Python compilation and
execution process (code is compiled into bytecode, which is then executed
by the VM), we translate its code to our own representation and emit LLVM from
it, and then JIT execute the LLVM IR. While it seems kinda pointless in this
artificial example, in reality this means we can execute the function's code
in any way we like.AST JIT case study: TritonThis approach is almost exactly how the Triton language works. The body of a
function decorated with  gets parsed to a Python AST, which then
- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered
to PTX by the
NVPTX LLVM backend.
Then, the code runs on a GPU using a standard CUDA pipeline.Naturally, the subset of Python that can be compiled down to a GPU is limited;
but it's sufficient to run performant kernels, in a language that's much
friendlier than CUDA and - more importantly - lives in the same file with the
"host" part written in regular Python. For example, if you want testing and
debugging, you can run Triton in "interpreter mode" which will just run the
same kernels locally on a CPU.Note that Triton lets us import names from the  package
and use them inside kernels; these serve as the  for the language
- special calls the compiler handles directly.Python is a fairly complicated language with  of features. Therefore,
if our JIT has to support some large portion of Python semantics, it may make
sense to leverage more of Python's own compiler. Concretely, we can have it
compile the wrapped function all the way to bytecode,
and start our translation from there.Here's the  decorator that does just this :The Python VM is a stack machine; so we emulate a stack to convert the
function's bytecode to  IR (a bit like an RPN evaluator).
As before, we then use our  utility function to lower
 to LLVM IR and JIT execute it.Using this JIT is as simple as the previous one - just swap 
for :Bytecode JIT case study: NumbaNumba is a compiler for Python itself. The idea
is that you can speed up specific functions in your code by slapping a
 decorator on them. What happens next is similar in spirit to
our simple , but of course much more complicated because it
supports a very large portion of Python semantics.Numba uses the Python compiler to emit bytecode, just as we did; it then
converts it into its own IR, and then to LLVM using .By starting with the bytecode, Numba makes its life easier (no need to rewrite
the entire Python compiler). On the other hand, it also makes some analyses
, because by the time we're in bytecode, a lot of semantic information
existing in higher-level representations is lost. For example, Numba has to
sweat a bit to recover control flow information from the bytecode (by
running it through a special interpreter first).The two approaches we've seen so far are similar in many ways - both rely on
Python's introspection capabilities to compile the source code of the JIT-ed
function to some extent (one to AST, the other all the way to bytecode), and
then work on this lowered representation.The tracing strategy is very different. It doesn't analyze the source code of
the wrapped function at all - instead, it  its execution by means of
specially-boxed arguments, leveraging overloaded operators and functions, and
then works on the generated trace.The code implementing this for our smile demo is surprisingly compact:Each runtime argument of the wrapped function is assigned a , and
that is placed in a , a placeholder class which lets us
do operator overloading:The remaining key function is :To understand how this works, consider this trivial example:After the decorated function is defined,  holds the wrapper function
defined inside . When  is called, the wrapper runs:For each argument of  itself (that is  and ), it creates
a new  holding a . This denotes a named variable in
the  IR.It then calls the wrapped function, passing it the boxes as runtime
parameters.When (the wrapped)  runs, it invokes . This is caught by the overloaded
 operator of , and it creates a new  with
the s representing  and  as children. This
 is then returned .The wrapper unboxes the returned  and passes it to
 to emit LLVM IR from it and JIT execute it with the
actual runtime arguments of the call: .This might be a little mind-bending at first, because there are two different
executions that happen:The first is calling the wrapped  function itself, letting the Python
interpreter run it as usual, but with special arguments that build up the IR
instead of doing any computations. This is the .The second is lowering this IR our tracing step built into LLVM IR and then
JIT executing it with the actual runtime argument values ; this is
the .This tracing approach has some interesting characteristics. Since we don't
have to analyze the source of the wrapped functions but only trace through
the execution, we can "magically" support a much richer set of programs, e.g.:This  with our basic . Since Python variables are
placeholders (references) for values, our tracing step is oblivious to them - it
follows the flow of values. Another example:This also just works! The created  will be a long chain of 
additions of 's runtime values through the loop, added to the 
for .This last example also leads us to a limitation of the tracing approach; the
loop cannot be  - it cannot depend on the function's arguments,
because the tracing step has no concept of runtime values and wouldn't know
how many iterations to run through; or at least, it doesn't know this unless
we want to perform the tracing run for every runtime execution .Tracing JIT case study: JAXThe JAX ML framework uses a tracing
approach very similar to the one described here. The first code sample in this
post shows the JAX notation. JAX cleverly wraps Numpy with its own version which
is traced (similar to our , but JAX calls these boxes "tracers"),
letting you write regular-feeling Numpy code that can be JIT optimized and
executed on accelerators like GPUs and TPUs via XLA. JAX's tracer builds up an underlying IR (called
jaxpr) which can then be
emitted to XLA ops and passed to XLA for further lowering and execution.For a fairly deep overview of how JAX works, I recommend reading the
autodidax doc.As mentioned earlier, JAX has some limitations
with things like data-dependent control flow in native Python. This won't work,
because there's control flow
that depends on a runtime value ():When  is executed, JAX will throw an exception, saying something
like:
This concrete value was not available in Python because it depends on the
value of the argument count.As a remedy, JAX has its
own built-in intrinsics from the jax.lax package.
Here's the example rewritten in a way that actually works: (and many other built-ins in the  package) is something JAX
can trace through, generating a corresponding XLA operation (XLA has support for
While loops, to which this
 can be lowered).The tracing approach has clear benefits for JAX as well; because it only cares
about the flow of values, it can handle arbitrarily complicated Python code,
as long as the flow of values can be traced. Just like the local variables and
data-independent loops shown earlier, but also things like closures. This makes
meta-programming and templating easy .The full code for this post is available on GitHub.]]></content:encoded></item><item><title>Friday Squid Blogging: Squid the Care Dog</title><link>https://www.schneier.com/blog/archives/2025/02/friday-squid-blogging-squid-the-care-dog.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 17:05:38 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[The Vanderbilt University Medical Center has a pediatric care dog named “Squid.”]]></content:encoded></item><item><title>Upcoming Speaking Engagements</title><link>https://www.schneier.com/blog/archives/2025/02/upcoming-speaking-engagements-43.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 17:01:21 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[This is a current list of where and when I am scheduled to speak:I’m speaking at Boskone 62 in Boston, Massachusetts, USA, which runs from February 14-16, 2025. My talk is at 4:00 PM ET on the 15th.]]></content:encoded></item><item><title>Hugo van Kemenade: Improving licence metadata</title><link>https://hugovk.dev/blog/2025/improving-licence-metadata/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 15:11:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[PEP 639 defines a spec on how to document licences
used in Python projects.Change  as follows.I usually use Hatchling as a build backend, and support was added in 1.27:Replace the freeform  field with a valid SPDX license expression, and add
 which points to the licence files in the repo. There’s often only one,
but if you have more than one, list them all:Optionally delete the deprecated licence classifier:Then make sure to use a PyPI uploader that supports this.pip can also show you the metadata:A lot of work went into this. Thank you to PEP authors
Philippe Ombredanne for creating the first draft in
2019, to C.A.M. Gerlach for the second draft in 2021,
and especially to Karolina Surma for getting the third
draft finish line and helping with the implementation.And many projects were updated to support this, thanks to the maintainers and
contributors of at least:]]></content:encoded></item><item><title>AI and Civil Service Purges</title><link>https://www.schneier.com/blog/archives/2025/02/ai-and-civil-service-purges.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 13:03:22 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Donald Trump and Elon Musk’s chaotic approach to reform is upending government operations. Critical functions have been halted, tens of thousands of federal staffers are being encouraged to resign, and congressional mandates are being disregarded. The next phase: The Department of Government Efficiency reportedly wants to use AI to cut costs. According to , Musk’s group has started to run sensitive data from government systems through AI programs to analyze spending and determine what could be pruned. This may lead to the elimination of human jobs in favor of automation. As one government official who has been tracking Musk’s DOGE team told the, the ultimate aim is to use AI to replace “the human workforce with machines.” (Spokespeople for the White House and DOGE did not respond to requests for comment.)Using AI to make government more efficient is a worthy pursuit, and this is not a new idea. The Biden administration disclosed more than 2,000 AI applications in development across the federal government. For example, FEMA has started using AI to help perform damage assessment in disaster areas. The Centers for Medicare and Medicaid Services has started using AI to look for fraudulent billing. The idea of replacing dedicated and principled civil servants with AI agents, however, new—and complicated.The civil service—the massive cadre of employees who operate government agencies—plays a vital role in translating laws and policy into the operation of society. New presidents can issue sweeping executive orders, but they often have no real effect until they actually change the behavior of public servants. Whether you think of these people as essential and inspiring do-gooders, boring bureaucratic functionaries, or as agents of a “deep state,” their sheer number and continuity act as ballast that resists institutional change.This is why Trump and Musk’s actions are so significant. The more AI decision making is integrated into government, the easier change will be. If human workers are widely replaced with AI, executives will have unilateral authority to instantaneously alter the behavior of the government, profoundly raising the stakes for transitions of power in democracy. Trump’s unprecedented purge of the civil service might be the last time a president needs to replace the human beings in government in order to dictate its new functions. Future leaders may do so at the press of a button.To be clear, the use of AI by the executive branch doesn’t have to be disastrous. In theory, it could allow new leadership to swiftly implement the wishes of its electorate. But this could go very badly in the hands of an authoritarian leader. AI systems concentrate power at the top, so they could allow an executive to effectuate change over sprawling bureaucracies instantaneously. Firing and replacing tens of thousands of human bureaucrats is a huge undertaking. Swapping one AI out for another, or modifying the rules that those AIs operate by, would be much simpler.Social-welfare programs, if automated with AI, could be redirected to systematically benefit one group and disadvantage another with a single prompt change. Immigration-enforcement agencies could prioritize people for investigation and detainment with one instruction. Regulatory-enforcement agencies that monitor corporate behavior for malfeasance could turn their attention to, or away from, any given company on a whim.Even if Congress were motivated to fight back against Trump and Musk, or against a future president seeking to bulldoze the will of the legislature, the absolute power to command AI agents would make it easier to subvert legislative intent. AI has the power to diminish representative politics. Written law is never fully determinative of the actions of government—there is always wiggle room for presidents, appointed leaders, and civil servants to exercise their own judgment. Whether intentional or not, whether charitably or not, each of these actors uses discretion. In human systems, that discretion is widely distributed across many individuals—people who, in the case of career civil servants, usually outlast presidencies.Today, the AI ecosystem is dominated by a small number of corporations that decide how the most widely used AI models are designed, which data they are trained on, and which instructions they follow. Because their work is largely secretive and unaccountable to public interest, these tech companies are capable of making changes to the bias of AI systems—either generally or with aim at specific governmental use cases—that are invisible to the rest of us. And these private actors are both vulnerable to coercion by political leaders and self-interested in appealing to their favor. Musk himself created and funded xAI, now one of the world’s largest AI labs, with an explicitly ideological mandate to generate anti-“woke” AI and steer the wider AI industry in a similar direction.But there’s a second way that AI’s transformation of government could go. AI development could happen inside of transparent and accountable public institutions, alongside its continued development by Big Tech. Applications of AI in democratic governments could be focused on benefitting public servants and the communities they serve by, for example, making it easier for non-English speakers to access government services, making ministerial tasks such as processing routine applications more efficient and reducing backlogs, or helping constituents weigh in on the policies deliberated by their representatives. Such AI integrations should be done gradually and carefully, with public oversight for their design and implementation and monitoring and guardrails to avoid unacceptable bias and harm.Governments around the world are demonstrating how this could be done, though it’s early days. Taiwan has pioneered the use of AI models to facilitate deliberative democracy at an unprecedented scale. Singapore has been a leader in the development of public AI models, built transparently and with public-service use cases in mind. Canada has illustrated the role of disclosure and public input on the consideration of AI use cases in government. Even if you do not trust the current White House to follow any of these examples, U.S. states—which have much greater contact and influence over the daily lives of Americans than the federal government—could lead the way on this kind of responsible development and deployment of AI.As the political theorist David Runciman has written, AI is just another in a long line of artificial “machines” used to govern how people live and act, not unlike corporations and states before it. AI doesn’t replace those older institutions, but it changes how they function. As the Trump administration forges stronger ties to Big Tech and AI developers, we need to recognize the potential of that partnership to steer the future of democratic governance—and act to make sure that it does not enable future authoritarians.This essay was written with Nathan E. Sanders, and originally appeared in The Atlantic.]]></content:encoded></item><item><title>Real Python: The Real Python Podcast – Episode #239: Behavior-Driven vs Test-Driven Development &amp;amp; Using Regex in Python</title><link>https://realpython.com/podcasts/rpp/239/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>Daniel Roy Greenfeld: Building a playing card deck</title><link>https://daniel.feldroy.com/posts/2025-02-deck-of-cards</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 09:50:04 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.]]></content:encoded></item><item><title>Risky Biz Soap Box: Run your own open source IDP with Authentik</title><link>https://risky.biz/soapbox93/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/soapbox93.mp3" length="" type=""/><pubDate>Fri, 14 Feb 2025 00:24:24 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mapped types in TypeScript</title><link>https://2ality.com/2025/02/mapped-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[A mapped type is a loop over keys that produces an object or tuple type and looks as follows:{[]: }
In this blog post, we examine how mapped types work and see examples of using them. Their most importing use cases are transforming objects and mapping tuples.]]></content:encoded></item><item><title>Bojan Mihelac: Prefixed Parameters for Django querystring tag</title><link>http://code.informatikamihelac.com/en/query-string-with-prefixed-parameters/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 21:37:18 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[An overview of Django 5.1's new querystring tag and how to add support for prefixed parameters.]]></content:encoded></item><item><title>Coding Interviews were HARD Until I Learned These 20 Tips</title><link>https://blog.algomaster.io/p/20-coding-interviews-tips</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:30:27 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[I gave my first  in 2016—and failed. I failed the next five interviews as well before finally landing my first job at .Since then, I’ve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .Surprisingly, my success wasn’t due to a dramatic improvement in problem-solving skills. The real game-changer was my approach— and  during the interview.In this article, I’ll share  that made coding interviews significantly easier for me.These tips cover everything you need to know, including:How to systematically approach coding interview problemsKey concepts and patterns you should knowThe type of problems you should practiceHow to choose the right algorithm for a given problemTechniques to optimize your solutionHow to communicate your thought process effectivelyBy applying these strategies, you’ll be able to tackle coding interviews with confidence and massively increase your chances of success.In a coding interview, interviewers want to see how well you , , and  under pressure.Here's a breakdown of what they look for:Understanding the problem: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?: Can you decompose the problem into smaller, manageable parts?: Can you design an optimal solution in terms of time and space complexity?: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?: Can you explain why one approach is better than another?: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?Can you quickly compute the time and space complexity of your solution?Explaining your thought process: Can you clearly articulate your approach and why it works?: Are you receptive to hints and able to adjust your approach accordingly?: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?Improving the initial solution: Can you optimize and refine your first solution when prompted?Are you able to tackle variations of the original problem?Can you manually walk through your code with sample inputs to verify correctness?Most coding interviews last Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.Lets assume you are given one problem, with a follow up in a 45-minute interview. Here’s how you can optimally allocate your time:The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.Understand the Problem (5-10 mins):  Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.Plan the Approach (10-20 mins): Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.Implement the Code (20-30 mins): Write a clean, modular and readable code.Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.Follow-ups and Wrap Up (35-45 mins): Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.If you don't fully understand the question, you might end up solving the Here’s how to ensure you grasp the problem before coding:Read the Problem CarefullyTake a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. Identify the expected input/output format and any hidden constraints.If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren’t explicitly mentioned in the problem statement.Common clarifications include:Are there duplicate values?Can the input be empty? If so, what should the output be?Should the solution handle negative numbers?Should the output maintain the original order of elements?Is the graph directed or undirected?Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?Walk Through Input/Output ExamplesOnce you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.Draw them out if it helps, especially for visual data structures like trees or graphs.Try to take examples that cover different scenarios of the problem. Think about any  that might come up.]]></content:encoded></item><item><title>Peter Bengtsson: get in JavaScript is the same as property in Python</title><link>http://www.peterbe.com/plog/get-in-javascript-is-the-same-as-property-in-python</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 12:41:56 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Prefix a function, in an object or class, with `get` and then that acts as a function call without brackets. Just like Python's `property` decorator.]]></content:encoded></item><item><title>How to add a directory to your PATH</title><link>https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/</link><author>Julia Evans</author><category>blog</category><pubDate>Thu, 13 Feb 2025 12:27:56 +0000</pubDate><source url="https://jvns.ca/atom.xml">Julia Evans</source><content:encoded><![CDATA[I was talking to a friend about how to add a directory to your PATH today. It’s
something that feels “obvious” to me since I’ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn’t find something that explained all of the steps – a lot of them just
said “add this to ”, but what if you’re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?So I wanted to try to write down some more complete directions and mention some
of the gotchas I’ve run into over the years.Here’s a table of contents:step 1: what shell are you using?If you’re not sure what shell you’re using, here’s a way to find out. Run this:if you’re using , it’ll print out if you’re using , it’ll print out if you’re using , it’ll print out an error like “In fish, please use
$fish_pid” ( isn’t valid syntax in fish, but in any case the error
message tells you that you’re using fish, which you probably already knew)Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I’ll only cover bash, zsh, and fish in these directions.step 2: find your shell’s config filein zsh, it’s probably in bash, it might be , but it’s complicated, see the note in the next sectionin fish, it’s probably ~/.config/fish/config.fish (you can run  if you want to be 100% sure)a note on bash’s config fileBash has three possible config files: , , and .If you’re not sure which one your system is set up to use, I’d recommend
testing this way:add  to your If you see “hi there”, that means  is being used! Hooray!Otherwise remove it and try the same thing with You can also try  if the first two options don’t work.(there are a lot of elaborate flow charts out there that explain how bash
decides which config file to use but IMO it’s not worth it and just testing is
the fastest way to be sure)step 3: figure out which directory to addLet’s say that you’re trying to install and run a program called 
and it doesn’t work, like this:$ npm install -g http-server
$ http-server
bash: http-server: command not found
How do you find what directory  is in? Honestly in general this is
not that easy – often the answer is something like “it depends on how npm is
configured”. A few ideas:Often when setting up a new installer (like , , , etc),
when you first set it up it’ll print out some directions about how to update
your PATH. So if you’re paying attention you can get the directions then.Sometimes installers will automatically update your shell’s config file
to update your  for youSometimes just Googling “where does npm install things?” will turn up the
answerSome tools have a subcommand that tells you where they’re configured to
install things, like:
Node/npm:  (then append )Go:  (then append )asdf: asdf info | grep ASDF_DIR (then append  and )step 3.1: double check it’s the right directoryOnce you’ve found a directory you think might be the right one, make sure it’s
actually correct! For example, I found out that on my machine,  is
in . I can make sure that it’s the right directory by trying to
run the program  in that directory like this:$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
It worked! Now that you know what directory you need to add to your ,
let’s move to the next step!step 4: edit your shell configNow we have the 2 critical pieces of information we need:Which directory you’re trying to add to your PATH (like  )Where your shell’s config is (like , , or ~/.config/fish/config.fish)Now what you need to add depends on your shell:Open your shell’s config file, and add a line like this:export PATH=$PATH:~/.npm-global/bin/
(obviously replace  with the actual directory you’re trying to add)You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:path=(
  $path
  ~/.npm-global/bin
)
In fish, the syntax is different:set PATH $PATH ~/.npm-global/bin
(in fish you can also use , some notes on that further down)step 5: restart your shellNow, an extremely important step: updating your shell’s config won’t take
effect if you don’t restart it!open a new terminal (or terminal tab), and maybe close the old one so you don’t get confusedRun  to start a new shell (or  if you’re using zsh, or  if you’re using fish)I’ve found that both of these usually work fine.And you should be done! Try running the program you were trying to run and
hopefully it works now.If not, here are a couple of problems that you might run into:problem 1: it ran the wrong programIf the wrong  of a is program running, you might need to add the
directory to the  of your PATH instead of the end.For example, on my system I have two versions of  installed, which I
can see by running :$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
The one your shell will use is the .If you want to use the Homebrew version, you need to add that directory
() to the  of your PATH instead, by putting this in
your shell’s config file (it’s  instead of the usual )export PATH=/opt/homebrew/bin/:$PATH
set PATH ~/.cargo/bin $PATH
problem 2: the program isn’t being run from your shellAll of these directions only work if you’re running the program . If you’re running the program from an IDE, from a GUI, in a cron job,
or some other way, you’ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.use the full path to the program you’re running, like /home/bork/bin/my-programput the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:….). You can get the full PATH you’re
using in your shell by running .I’m honestly not sure how to handle it in an IDE/GUI because I haven’t run into
that in a long time, will add directions here if someone points me in the right
direction.problem 3: duplicate  entries making it harder to debugIf you edit your path and start a new shell by running  (or , or
), you’ll often end up with duplicate  entries, because the shell
keeps adding new things to your  every time you start your shell.Personally I don’t think I’ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what’s going on with your  if you’re trying to understand its contents.Some ways you could deal with this:If you’re debugging your , open a new terminal to do it in so you get
a “fresh” state. This should avoid the duplication.Deduplicate your  at the end of your shell’s config  (for example in
zsh apparently you can do this with )Check that the directory isn’t already in your  when adding it (for
example in fish I believe you can do this with fish_add_path --path /some/directory)How to deduplicate your  is shell-specific and there isn’t always a
built in way to do it so you’ll need to look up how to accomplish it in your
shell.problem 4: losing your history after updating your Here’s a situation that’s easy to get into in bash or zsh:Run  to reload your configPress the up arrow a couple of times to rerun the failed command (or open a new terminal)The failed command isn’t in your history! Why not?This happens because in bash, by default, history is not saved until you exit
the shell.Some options for fixing this:Instead of running  to reload your config, run  (or
 in zsh). This will reload the config inside your current
session.Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you’re
using bash or zsh, the history options in zsh are a bit complicated and I’m
not exactly sure what the best way is)When you install  (Rust’s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don’t mention a specific
directory at all.This is usually done by running one of the following (note the leading DOT):

. "$HOME/.cargo/env"        	# For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
The idea is that you add that line to your shell’s config, and their script
automatically sets up your  (and potentially other things) for you.This is pretty common (for example Homebrew suggests you eval ), and there are
two ways to approach this:Just do what the tool suggests (like adding  to your shell’s config)Figure out which directories the script they’re telling you to run would add
to your PATH, and then add those manually. Here’s how I’d do that:
Run  in my shell (or the fish version if using fish)Run echo "$PATH" | tr ':' '\n' | grep cargo to figure out which directories it addedSee that it says  and shorten that to Add the directory  to PATH (with the directions in this post)I don’t think there’s anything wrong with doing what the tool suggests (it
might be the “best way”!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I’m changing.fish has a handy function called  that you can run to add a directory to your  like this:fish_add_path /some/directory
This is cool (it’s such a simple command!) but I’ve stopped using it for a couple of reasons:Sometimes  will update the  for every session in the
future (with a “universal variable”) and sometimes it will update the 
just for the current session and it’s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!]]></content:encoded></item><item><title>DOGE as a National Cyberattack</title><link>https://www.schneier.com/blog/archives/2025/02/doge-as-a-national.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Thu, 13 Feb 2025 12:03:26 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[In the span of just weeks, the US government has experienced what may be the most consequential security breach in its history—not through a sophisticated cyberattack or an act of foreign espionage, but through official orders by a billionaire with a poorly defined government role. And the implications for national security are profound.First, it was reported that people associated with the newly created Department of Government Efficiency (DOGE) had accessedtheUSTreasury computer system, giving them the ability to collect data on and potentially control the department’s roughly $5.45 trillion in annual federal payments.Then, we learned that uncleared DOGE personnel had gained access to classified data from the US Agency for International Development, possibly copying it onto their own systems. Next, the Office of Personnel Management—which holds detailed personal data on millions of federal employees, including those with security clearances—wascompromised. After that, Medicaid and Medicare records were compromised.Meanwhile, only partially redacted names of CIA employees were sent over an unclassified email account. DOGE personnel are also reported to be feeding Education Department data into artificial intelligence software, and they have also started working at the Department of Energy.This story is moving very fast. On Feb. 8, a federal judge blocked the DOGE team from accessing the Treasury Department systems any further. But given that DOGE workers have already copied data and possibly installed and modified software, it’s unclear how this fixes anything.In any case, breaches of other critical government systems are likely to follow unless federal employees stand firm on the protocols protecting national security.The systems that DOGE is accessing are not esoteric pieces of our nation’s infrastructure—they are the sinews of government.For example, the Treasury Department systems contain the technical blueprints for how the federal government moves money, while the Office of Personnel Management (OPM) network contains information on who and what organizations the government employs and contracts with.What makes this situation unprecedented isn’t just the scope, but also the method of attack. Foreign adversaries typically spend years attempting to penetrate government systems such as these, using stealth to avoid being seen and carefully hiding any tells or tracks. The Chinese government’s 2015 breach of OPM was a significant US security failure, and it illustrated how personnel data could be used to identify intelligence officers and compromise national security.In this case, external operators with limited experience and minimal oversight are doing their work in plain sight and under massive public scrutiny: gaining the highest levels of administrative access and making changes to the United States’ most sensitive networks, potentially introducing new security vulnerabilities in the process.But the most alarming aspect isn’t just the access being granted. It’s the systematic dismantling of security measures that would detect and prevent misuse—including standard incident response protocols, auditing, and change-tracking mechanisms—by removing the career officials in charge of those security measures and replacing them with inexperienced operators.The Treasury’s computer systems have such an impact on national security that they were designed with the same principle that guides nuclear launch protocols: No single person should have unlimited power. Just as launching a nuclear missile requires two separate officers turning their keys simultaneously, making changes to critical financial systems traditionally requires multiple authorized personnel working in concert.This approach, known as “separation of duties,” isn’t just bureaucratic red tape; it’s a fundamental security principle as old as banking itself. When your local bank processes a large transfer, it requires two different employees to verify the transaction. When a company issues a major financial report, separate teams must review and approve it. These aren’t just formalities—they’re essential safeguards against corruption and error. These measures have been bypassed or ignored. It’s as if someone found a way to rob Fort Knox by simply declaring that the new official policy is to fire all the guards and allow unescorted visits to the vault.The implications for national security are staggering. Sen. Ron Wyden said his office had learned that the attackers gained privileges that allow them to modify core programs in Treasury Department computers that verify federal payments, access encrypted keys that secure financial transactions, and alter audit logs that record system changes. Over at OPM, reports indicate that individuals associated with DOGE connected an unauthorized server into the network. They are also reportedly trainingAI software on all of this sensitive data.This is much more critical than the initial unauthorized access. These new servers have unknown capabilities and configurations, and there’s no evidence that this new code has gone through any rigorous security testing protocols. The AIs being trained are certainly not secure enough for this kind of data. All are ideal targets for any adversary, foreign or domestic, also seeking access to federal data.There’s a reason why every modification—hardware or software—to these systems goes through a complex planning process and includes sophisticated access-control mechanisms. The national security crisis is that these systems are now much more vulnerable to dangerous attacks at the same time that the legitimate system administrators trained to protect them have been locked out.By modifying core systems, the attackers have not only compromised current operations, but have also left behind vulnerabilities that could be exploited in future attacks—giving adversaries such as Russia and China an unprecedentedopportunity. These countries have long targeted these systems. And they don’t just want to gather intelligence—they also want to understand how to disrupt these systems in a crisis.Now, the technical details of how these systems operate, their security protocols, and their vulnerabilities are now potentially exposed to unknown parties without any of the usual safeguards. Instead of having to breach heavily fortified digital walls, these parties  can simply walk through doors that are being propped open—and then erase evidence of their actions.The security implications span three critical areas.First, system manipulation: External operators can now modify operations while also altering audit trails that would track their changes. Second, data exposure: Beyond accessing personal information and transaction records, these operators can copy entire system architectures and security configurations—in one case, the technical blueprint of the country’s federal payment infrastructure. Third, and most critically, is the issue of system control: These operators can alter core systems and authentication mechanisms while disabling the very tools designed to detect such changes. This is more than modifying operations; it is modifying the infrastructure that those operations use.To address these vulnerabilities, three immediate steps are essential. First, unauthorized access must be revoked and proper authentication protocols restored. Next, comprehensive system monitoring and change management must be reinstated—which, given the difficulty of cleaning a compromised system, will likely require a complete system reset. Finally, thorough audits must be conducted of all system changes made during this period.This is beyond politics—this is a matter of national security. Foreign national intelligence organizations will be quick to take advantage of both the chaos and the new insecurities to steal US data and install backdoors to allow for future access.Each day of continued unrestricted access makes the eventual recovery more difficult and increases the risk of irreversible damage to these critical systems. While the full impact may take time to assess, these steps represent the minimum necessary actions to begin restoring system integrity and security protocols.Assuming that anyone in the government still cares.This essay was written with Davi Ottenheimer, and originally appeared in Foreign Policy.]]></content:encoded></item><item><title>GenAI Patterns: Reranker</title><link>https://martinfowler.com/articles/gen-ai-patterns/#reranker</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 13 Feb 2025 10:16:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[LLMs struggle with large amounts of context. Bharani
      Subramaniam and I explain how to mitigate this common RAG
      problem with a Reranker which takes the document
      fragments from the retriever, and ranks them according to their usefulness.]]></content:encoded></item><item><title>EuroPython: EuroPython February 2025 Newsletter</title><link>https://blog.europython.eu/europython-february-2025-newsletter/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 08:36:11 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Hope you&aposre all having a fantastic February. We sure have been busy and got some exciting updates for you as we gear up for EuroPython 2025, which is taking place once again in the beautiful city of Prague. So let&aposs dive right in!EuroPython 2025 is right around the corner and our programme team is hard at work putting together an amazing lineup. But we need your help to shape the conference! We received over 572 fantastic proposals, and now it’s time for Community Voting! 🎉 If you&aposve attended EuroPython before or submitted a proposal this year, you’re eligible to vote.📢 More votes = a stronger, more diverse programme! Spread the word and get your EuroPython friends to cast their votes too.🏃The deadline is , so don’t miss your chance!Want to play a key role in building an incredible conference? Join our review team and help select the best talks for EuroPython 2025! Whether you&aposre a Python expert or an enthusiastic community member, your insights matter.We’d like to also thank the over 100 people who have already signed up to review! For those who haven’t done so yet, please remember to accept your Pretalx link and get your reviews in by You can already start reviewing proposals, and each review takes as little as 5 minutes. We encourage reviewers to go through at least 20-30 proposals, but if you can do more, even better! With almost 600 submissions to pick from, your help ensures we curate a diverse and engaging programme.🏃The deadline is Monday next week, so don’t delay!EuroPython isn’t just present at other Python events—we actively support them too! As a community sponsor, we love helping local PyCons grow and thrive. We love giving back to the community and strengthening Python events across Europe! 🐍💙The EuroPython team had a fantastic time at PyCon + Web in Berlin, meeting fellow Pythonistas, exchanging ideas, and spreading the word about EuroPython 2025. It was great to connect with speakers, organizers, and attendees. Ever wondered how long it takes to walk from Berlin to Prague? A huge thank you to our co-organizers, Cheuk, Artur, and Cristián, for answering that in their fantastic lightning talk about EuroPython!We had some members of the EuroPython team at FOSDEM 2025, connecting with the open-source community and spreading the Python love! 🎉 We enjoyed meeting fellow enthusiasts, sharing insights about the EuroPython Society, and giving away the first EuroPython 2025 stickers. If you stopped by—thank you and we hope to see you in Prague this July.🦒 Speaker Mentorship ProgrammeThe signups for The Speaker Mentorship Programme closed on 22nd January 2025. We’re excited to have matched 43 mentees with 24 mentors from our community. We had an increase in the number of mentees who signed up and that’s amazing! We’re glad to be contributing to the journey of new speakers in the Python community. A massive thank you to our mentors for supporting the mentees and to our mentees; we’re proud of you for taking this step in your journey as a speaker. 26 mentees submitted at least 1 proposal. Out of this number, 13 mentees submitted 1 proposal, 9 mentees submitted 2 proposals, 2 mentees submitted 3 proposals, 1 mentee submitted 4 proposals and lastly, 1 mentee submitted 5 proposals. We wish our mentees the best of luck. We look forward to the acceptance of their proposals.In a few weeks, we will host an online panel session with 2–3 experienced community members who will share their advice with first-time speakers. At the end of the panel, there will be a Q&A session to answer all the participants’ questions.You can watch the recording of the previous year’s workshop here:EuroPython is one of the largest Python conferences in Europe, and it wouldn’t be possible without our sponsors. We are so grateful for the companies who have already expressed interest. If you’re interested in sponsoring EuroPython 2025 as well, please reach out to us at sponsoring@europython.eu.🎤 EuroPython Speakers Share Their ExperiencesWe asked our past speakers to share their experiences speaking at EuroPython. These videos have been published on YouTube as shorts, and we&aposve compiled them into brief clips for you to watch.A big thanks goes to Sebastian Witowski, Jan Smitka, Yuliia Barabash, Jodie Burchell, Max Kahan, and Cheuk Ting Ho for sharing their experiences.Why You Should Submit a Proposal for EuroPython? Part 2Why You Should Submit a Proposal for EuroPython? Part 3📊 EuroPython Society Board Report The EuroPython conference wouldn’t be what it is without the incredible volunteers who make it all happen. 💞 Behind the scenes, there’s also the EuroPython Society—a volunteer-led non-profit that manages the fiscal and legal aspects of running the conference, oversees its organization, and works on a few smaller projects like the grants programme. To keep everyone in the loop and promote transparency, the Board is sharing regular updates on what we’re working on.That&aposs all for now! Keep an eye on your inbox and our website for more news and announcements. We&aposre counting down the days until we can come together in Prague to celebrate our shared love for Python. 🐍❤️Cheers,The EuroPython Team]]></content:encoded></item><item><title>Giampaolo Rodola: psutil: drop Python 2.7 support</title><link>https://gmpy.dev/blog/2025/psutil-drop-python-27-support</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[About dropping Python 2.7 support in psutil, 3 years ago
I stated:Not a chance, for many years to come. [Python 2.7] currently represents 7-10%
of total downloads, meaning around 70k / 100k downloads per day.Only 3 years later, and to my surprise, downloads for Python 2.7 dropped to
0.36%! As such, as of psutil 7.0.0, I finally decided to drop support for
Python 2.7!These are downloads per month:According to pypistats.org Python 2.7 downloads
represents the 0.28% of the total, around 15.000 downloads per day.Maintaining 2.7 support in psutil had become increasingly difficult, but still
possible. E.g. I could still run tests by using old PYPI
backports.
GitHub Actions could still be
tweaked
to run tests and produce 2.7 wheels on Linux and macOS. Not on Windows though,
for which I had to use a separate service (Appveyor). Still, the amount of
hacks in psutil source code necessary to support Python 2.7 piled up over the
years, and became quite big. Some disadvantages that come to mind:Having to maintain a Python compatibility layers like
  psutil/_compat.py.
  This translated in extra extra code and extra imports.The C compatibility layer to differentiate between Python 2 and 3 (#if
  PY_MAJOR_VERSION <= 3, etc.).Dealing with the string vs. unicode differences, both in Python and in C.Inability to use modern language features, especially f-strings.Inability to freely use s, which created a difference on how CONSTANTS
  were exposed in terms of API.Having to install a specific version of  and other (outdated)
  deps.Relying on the third-party Appveyor CI service to run tests and produce 2.7
  wheels.Running 4 extra CI jobs on every commit (Linux, macOS, Windows 32-bit,
  Windows 64-bit) making the CI slower and more subject to failures (we have
  quite a bit of flaky tests).The distribution of 7 wheels specific for Python 2.7. E.g. in the previous
  release I had to upload:psutil-6.1.1-cp27-cp27m-macosx_10_9_x86_64.whl
psutil-6.1.1-cp27-none-win32.whl
psutil-6.1.1-cp27-none-win_amd64.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_x86_64.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_x86_64.whl
The removal was done in
PR-2841, which removed around
1500 lines of code (nice!). . In doing so, in the doc I
still made the promise that the 6.1.* serie will keep supporting Python 2.7
and will receive  (no new features). It will be
maintained in a specific python2
branch. I explicitly kept
the
setup.py
script compatible with Python 2.7 in terms of syntax, so that, when the tarball
is fetched from PYPI, it will emit an informative error message on . The user trying to install psutil on Python 2.7 will see:$pip2installpsutil
Asofversion.0.0psutilnolongersupportsPython.7.
LatestversionsupportingPython.7ispsutil.1.X.
Installitwith:.
As the informative message states, users that are still on Python 2.7 can still
use psutil with:pip2 install psutil==6.1.*
]]></content:encoded></item><item><title>Kay Hayen: Nuitka Release 2.6</title><link>https://nuitka.net/posts/nuitka-release-26.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[ Path normalization to native Windows format was required
in more places for the  variant of .The  function doesn’t normalize to native Win32
paths with MSYS2, instead using forward slashes. This required manual
normalization in additional areas. (Fixed in 2.5.1) Fix, give a proper error when extension modules asked to
include failed to be located. instead of a proper error message.
(Fixed in 2.5.1)Fix, files with illegal module names (containing ) in their
basename were incorrectly considered as potential sub-modules for
. These are now skipped. (Fixed in 2.5.1) Improved stability by preventing crashes when stubgen
encounters code it cannot handle. Exceptions from it are now ignored.
(Fixed in 2.5.1) Addressed a crash that occurred when encountering
assignments to non-variables. (Fixed in 2.5.1) Fixed a regression introduced in 2.5 release that could
lead to segmentation faults in exception handling for generators.
(Fixed in 2.5.2) Corrected an issue where dictionary copies of large
split directories could become corrupted. This primarily affected
instance dictionaries, which are created as copies until updated,
potentially causing problems when adding new keys. (Fixed in 2.5.2) Removed the assumption that module dictionaries
always contain only strings as keys. Some modules, like
 on macOS, use non-string keys. (Fixed in 2.5.2) Ensured that the  option correctly
affects the C compilation process. Previously, only individual
disables were applied. (Fixed in 2.5.2) Fixed a crash that could occur during compilation
when unary operations were used within binary operations. (Fixed in
2.5.3) Corrected the handling of
, which could lead to crashes. (Fixed
in 2.5.4) Resolved a segmentation fault occurring at runtime
when calling  with only keyword arguments.
(Fixed in 2.5.5) Harmless warnings generated for x64 DLLs on arm64 with
newer macOS versions are now ignored. (Fixed in 2.5.5) Addressed a crash in Nuitka’s dictionary code that
occurred when copying dictionaries due to internal changes in Python
3.13. (Fixed in 2.5.6) Improved onefile mode signing by applying
 to the signature of binaries, not just
app bundles. (Fixed in 2.5.6) Corrected an issue where too many paths were added as
extra directories from the Nuitka package configuration. This
primarily affected the  package, which currently relies
on the  import hack. (Fixed in 2.5.6) Prevented crashes on macOS when creating onefile
bundles with Python 2 by handling negative CRC32 values. This issue
may have affected other versions as well. (Fixed in 2.5.6) Restored the functionality of code provided in
, which was no longer being applied due to a
regression. (Fixed in 2.5.6) Suppressed the app bundle mode recommendation when it is
already in use. (Fixed in 2.5.6) Corrected path normalization when the output directory
argument includes “~”. GitHub Actions Python is now correctly identified as a
Homebrew Python to ensure proper DLL resolution. (Fixed in 2.5.7) Fixed a reference leak that could occur with
values sent to generator objects. Asyncgen and coroutines were not
affected. (Fixed in 2.5.7) The  scan now correctly handles
cases where both a package init file and competing Python files
exist, preventing compile-time conflicts. (Fixed in 2.5.7) Resolved an issue where handling string constants in
modules created for Python 3.12 could trigger assertions, and modules
created with 3.12.7 or newer failed to load on older Python 3.12
versions when compiled with Nuitka 2.5.5-2.5.6. (Fixed in 2.5.7) Corrected the tuple code used when calling certain
method descriptors. This issue primarily affected a Python 2
assertion, which was not impacted in practice. (Fixed in 2.5.7) Updated resource readers to accept multiple
arguments for , and correctly handle
 and  as keyword-only arguments. The platform encoding is no longer used to decode
 logs. Instead,  is used, as it is sufficient for
matching filenames across log lines and avoids potential encoding
errors. (Fixed in 2.5.7) Requests to statically link libraries for 
are now ignored, as these libraries do not exist. (Fixed in 2.5.7) Fixed a memory leak affecting the results of
functions called via specs. This primarily impacted overloaded hard
import operations. (Fixed in 2.5.7) When multiple distributions for a package are found,
the one with the most accurate file matching is now selected. This
improves handling of cases where an older version of a package (e.g.,
) is overwritten with a different variant (e.g.,
), ensuring the correct version is used for
Nuitka package configuration and reporting. (Fixed in 2.5.8) Prevented a potential crash during onefile
initialization on Python 2 by passing the directory name directly
from the onefile bootstrap, avoiding the use of  which
may not be fully loaded at that point. (Fixed in 2.5.8) Preserved necessary  environment variables on
Windows for packages that require loading DLLs from those locations.
Only  entries not pointing inside the installation prefix are
removed. (Fixed in 2.5.8) Corrected the  check to function
properly when distribution names and package names differ. (Fixed in
2.5.8) Improved package name resolution for Anaconda
distributions by checking conda metadata when file metadata is
unavailable through the usual methods. (Fixed in 2.5.8) Normalized the downloaded gcc path to use native Windows
slashes, preventing potential compilation failures. (Fixed in 2.5.9) Restored static libpython functionality on Linux by
adapting to a signature change in an unexposed API. (Fixed in 2.5.9) Prevented  from being resurrected when a
finalizer is attached, resolving memory leaks that could occur with
 in the presence of exceptions. (Fixed in 2.5.10) Suppressed the gcc download prompt that could appear during
 output on Windows systems without MSVC or with an
improperly installed gcc.Ensured compatibility with monkey patched  or 
functions, which are used in some testing scenarios. Improved the determinism of the JSON statistics
output by sorting keys, enabling reliable build comparisons. Fixed a memory leak in  with finalizers,
which could lead to significant memory consumption when using
 and encountering exceptions. Optimized empty generators (an optimization result) to
avoid generating unused context code, eliminating C compilation
warnings. Fixed a reference leak affecting the  value
in . While typically , this could lead to
observable reference leaks in certain cases. Improved handling of  and 
resurrection, preventing memory leaks with  and
, and ensuring correct execution of  code in
coroutines. Corrected the handling of  objects
resurrecting during deallocation. While not explicitly demonstrated,
this addresses potential issues similar to those encountered with
coroutines, particularly for old-style coroutines created with the
 decorator. Fixed a potential crash during runtime trace collection by
ensuring timely initialization of the output mechanism.]]></content:encoded></item><item><title>EuroPython Society: Board Report for January 2025</title><link>https://www.europython-society.org/board-report-for-january-2025/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:08:37 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[The top priority for the board in January was finishing the hiring of our event manager. We’re super excited to introduce Anežka Müller! Anežka is a freelance event manager and a longtime member of the Czech Python community. She’s a member of the Pyvec board, co-organizes PyLadies courses, PyCon CZ, Brno Pyvo, and Brno Python Pizza. She’ll be working closely with the board and OPS team, mainly managing communication with service providers. Welcome onboard! Our second priority was onboarding teams. We’re happy that we already have the Programme team in place—they started early and launched the Call for Proposals at the beginning of January. We’ve onboarded a few more teams and are in the process of bringing in the rest.Our third priority was improving our grant programme in order to support more events with our limited budget and to make it more clear and transparent. We went through past data, came up with a new proposal, discussed it, voted on it, and have already published it on our blog. Updating onboarding/offboarding checklists for Volunteers and Board MembersVarious infrastructure updates including new website deployment and self-hosted previews for Pull Requests to the website.Setting up EPS AWS account.Working out the Grant Guidelines update for 2025Attending PyConWeb and FOSDEMReviewing updates to the Sponsors setup and packages for 2025More documentation, sharing know-how and reviewing new proposals.Brand strategy: Analysis of social media posts from previous years and web analytics. Call with a European open-source maintainer and a call with a local events organizer about EP content.Comms & design: Call for proposal announcements, EP 2024 video promotions, speaker mentorship, and newsletter. Video production - gathering videos from speakers, video post-production, and scheduling them on YouTube shorts, and social media.Event management coordination: Calls with the event manager and discussions about previous events.Grants: Work on new grant guidelines and related comms.Team onboarding: Calls with potential comms team members and coordination.PR: Delivering a lightning talk at FOSDEM.Offboarding the old boardOnboarding new team membersAdministrative work on GrantsWorked on the Grants proposalFollow-up with team membersCommunity outreach: FOSDEMWorking on various infrastructure updates, mostly related to the website.Reviewing Pull Requests for the website and the internal botWorking on the infrastructure team proposal.Timeline: Discussion with the Programme Team, and planning to do the same with the other teams.Visa Request letter: Setup and Test Visa Request Automation for the current yearTeam selection discussion with past volunteers]]></content:encoded></item><item><title>Python Morsels: Avoid over-commenting in Python</title><link>https://www.pythonmorsels.com/avoid-comments/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:05:39 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Documenting instead of commentingHere is a comment I would not write in my code:That comment seems to describe what this code does... so why would I  write it?I do like that comment, but I would prefer to write it as a docstring instead:Documentation strings are for conveying the purpose of function, class, or module, typically at a high level.
Unlike comments, they can be read by Python's built-in  function:Docstrings are also read by other documentation-oriented tools, like Sphinx.Non-obvious variables and valuesHere's a potentially helpful comment:]]></content:encoded></item><item><title>Real Python: Python Keywords: An Introduction</title><link>https://realpython.com/python-keywords/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. Currently, Python has thirty-five keywords and four soft keywords. These keywords are always available in Python, which means you don’t need to import them. Understanding how to use them correctly is fundamental for building Python programs.By the end of this tutorial, you’ll understand that:There are  and  in Python.You can get a list of all keywords using  from the  module. in Python act as keywords only in specific contexts. are keywords that have been deprecated and turned into functions in Python 3.In this article, you’ll find a basic introduction to all Python keywords and soft keywords along with other resources that will be helpful for learning more about each keyword. Test your knowledge with our interactive “Python Keywords: An Introduction” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of Python keywords and soft keywords. These reserved words have specific functions and restrictions in Python, and understanding how to use them correctly is fundamental for building Python programs.Python keywords are special reserved words that have specific meanings and purposes and can’t be used for anything but those specific purposes. These keywords are always available—you’ll never have to import them into your code.Python keywords are different from Python’s built-in functions and types. The built-in functions and types are also always available, but they aren’t as restrictive as the keywords in their usage. An example of something you  do with Python keywords is assign something to them. If you try, then you’ll get a . You won’t get a  if you try to assign something to a built-in function or type, but it still isn’t a good idea. For a more in-depth explanation of ways keywords can be misused, check out Invalid Syntax in Python: Common Reasons for SyntaxError.There are thirty-five keywords in Python. Here’s a list of them, each linked to its relevant section in this tutorial:Two keywords have additional uses beyond their initial use cases. The  keyword is also used with loops and with  and  in addition to in conditional statements. The  keyword is most commonly used in  statements, but also used with the  keyword.The list of Python keywords and soft keywords has changed over time. For example, the  and  keywords weren’t added until Python 3.7. Also, both  and  were keywords in Python 2.7 but were turned into built-in functions in Python 3 and no longer appear in the keywords list.As mentioned above, you’ll get an error if you try to assign something to a Python keyword. Soft keywords, on the other hand, aren’t that strict. They syntactically act as keywords only in certain conditions.This new capability was made possible thanks to the introduction of the PEG parser in Python 3.9, which changed how the interpreter reads the source code.Leveraging the PEG parser allowed for the introduction of structural pattern matching in Python. In order to use intuitive syntax, the authors picked , , and  for the pattern matching statements. Notably,  and  are widely used for this purpose in many other programming languages.To prevent conflicts with existing Python code that already used , , and  as variable or function names, Python developers decided to introduce the concept of soft keywords.Currently, there are four  in Python:You can use the links above to jump to the soft keywords you’d like to read about, or you can continue reading for a guided tour.Value Keywords: , , There are three Python keywords that are used as values. These values are singleton values that can be used over and over again and always reference the exact same object. You’ll most likely see and use these values a lot.There are a few terms used in the sections below that may be new to you. They’re defined here, and you should be aware of their meaning before proceeding:]]></content:encoded></item><item><title>EuroPython Society: Changes in the Grants Programme for 2025</title><link>https://www.europython-society.org/changes-in-the-grants-programme-for-2025/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 13:16:30 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[We are increasing transparency and reducing ambiguity in the guidelines.We would like to support more events with our limited budgetWe’ve introduced caps for events in order to make sure all grants are fairly given and we can support more communities.We’ve set aside 10% of our budget for the local community. The EPS introduced a Grant Programme in 2017. Since then, we have granted almost EUR 350k through the programme, partly via EuroPython Finaid and by directly supporting other Python events and projects across Europe. In the last two years, the Grant Programme has grown to EUR 100k per year, with even more requests coming in.With this growth come new challenges in how to distribute funds fairly so that more events can benefit. Looking at data from the past two years, we’ve often been close to or over our budget. The guidelines haven’t been updated in a while. As grant requests become more complex, we’d like to simplify and clarify the process, and better explain it on our website.We would also like to acknowledge that EuroPython, when traveling around Europe, has an additional impact on the host country, and we’d like to set aside part of the budget for the local community.The Grant Programme is also a primary funding source for EuroPython Finaid. To that end, we aim to allocate 30% of the total Grant Programme budget to Finaid, an increase from the previous 25%.We’ve updated the text on our website, and split it into multiple sub-pages to make it easier to navigate. The website now includes a checklist of what we would like to see in a grant application, and a checklist for the Grants Workgroup – so that when you apply for the Grant you already know the steps that it will go through later and when you can expect an answer from us.We looked at the data from previous years, and size and timing of the grant requests. With the growing number and size of the grants, to make it more accessible to smaller conferences and conferences happening later in the year, we decided to introduce max caps per grant and split the budget equally between the first and second half of the year. We would also explicitly split the total budget into three categories – 30% goes to the EuroPython finaid, 10% is reserved for projects in the host country. The remaining 60% of the budget goes to fund other Python Conferences. This is similar to the split in previous years, but more explicit and transparent.Using 2024 data, and the budget available for Community Grants (60% of total), we’ve simulated different budget caps and found a sweet spot at 6000EUR, where we are able to support all the requests with most of the grants being below that limit. For 2025 we expect to receive a similar or bigger number of requests.We are introducing a special 10% pool of money to be used on projects in the host country (in 2025 that’s again Czech Republic). This pool is set aside at the beginning of the year, with one caveat that we would like to deploy it in the first half of the year. Whatever is left unused goes back to the Community Pool to be used in second half of the year.Fairer Funding: By spreading our grants out during the year, conferences that happen later won’t miss out.Easy to Follow: Clear rules and deadlines cut down on confusion about how much you can get and what it’s for.Better Accountability: We ask for simple post-event reports so we can see where the money went and what impact it made.Stronger Community: Funding more events grows our Python network across Europe, helping everyone learn, connect, and collaborate.]]></content:encoded></item><item><title>Delivering Malware Through Abandoned Amazon S3 Buckets</title><link>https://www.schneier.com/blog/archives/2025/02/delivering-malware-through-abandoned-amazon-s3-buckets.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Wed, 12 Feb 2025 12:09:24 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Here’s a supply-chain attack just waiting to happen. A group of researchers searched for, and then registered, abandoned Amazon S3 buckets for about $400. These buckets contained software libraries that are still used. Presumably the projects don’t realize that they have been abandoned, and still ping them for patches, updates, and etc.The TL;DR is that this time, we ended up discovering ~150 Amazon S3 buckets that had previously been used across commercial and open source software products, governments, and infrastructure deployment/update pipelines—and then abandoned.Naturally, we registered them, just to see what would happen—”how many people are really trying to request software updates from S3 buckets that appear to have been abandoned months or even years ago?”, we naively thought to ourselves.Turns out they got eight million requests over two months.Had this been an actual attack, they would have modified the code in those buckets to contain malware and watch as it was incorporated in different software builds around the internet. This is basically the SolarWinds attack, but much more extensive.But there’s a second dimension to this attack. Because these update buckets are abandoned, the developers who are using them also no longer have the power to patch them automatically to protect them. The mechanism they would use to do so is now in the hands of adversaries. Moreover, often—but not always—losing the bucket that they’d use for it also removes the original vendor’s ability to identify the vulnerable software in the first place. That hampers their ability to communicate with vulnerable installations.Software supply-chain security is an absolute mess. And it’s not going to be easy, or cheap, to fix. Which means that it won’t be. Which is an even worse mess.]]></content:encoded></item><item><title>Real Python: Quiz: Python Keywords: An Introduction</title><link>https://realpython.com/quizzes/python-keywords/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. These keywords are always available in Python, which means you don’t need to import them. Understanding how to use them correctly is fundamental for building Python programs.]]></content:encoded></item><item><title>Zato Blog: Modern REST API Tutorial in Python</title><link>https://zato.io/en/blog/modern-rest-api-tutorial-in-python.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[
  2025-02-12, by Dariusz Suchojad
Great APIs don't win theoretical arguments - they just prefer to work reliably and to make developers' lives easier.Here's a tutorial on what building production APIs is really about: creating interfaces that are practical in usage,
while keeping your systems maintainable for years to come.]]></content:encoded></item><item><title>Kushal Das: pass using stateless OpenPGP command line interface</title><link>https://kushaldas.in/posts/pass-using-stateless-openpgp-command-line-interface.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 05:26:13 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Yesterday I wrote about how
I am using a different tool for  signing and verification. Next, I
replaced my  usage. I have a small
patch to use
stateless OpenPGP command line interface (SOP). It is an implementation
agonostic standard for handling OpenPGP messages. You can read the whole SPEC
here.cargo install rsop rsop-oct
And copied the bash script from my repository to the path somewhere.The  binary from  follows the same SOP standard but uses the
card to signing/decryption. I stored my public key in
~/.password-store/.gpg-key file, which is in turn used for encryption.Here nothing changed related my daily  usage, except the number of time I am typing my  :)]]></content:encoded></item><item><title>Risky Business #779 -- DOGE staffer linked to The Com</title><link>https://risky.biz/RB779/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB779.mp3" length="" type=""/><pubDate>Wed, 12 Feb 2025 03:18:48 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GenAI Patterns: Query Rewriting</title><link>https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 11 Feb 2025 20:58:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Users often have difficulty writing the most effective queries.
       and I explain Query Rewriting:
      getting an LLM to formulate alternative queries to send to a RAG's
      retriever. ]]></content:encoded></item><item><title>PyCoder’s Weekly: Issue #668: NumPy, Compiling Python 1.0, BytesIO, and More (Feb. 11, 2025)</title><link>https://pycoders.com/issues/668</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 19:30:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[ In this video course, you’ll learn how to use NumPy by exploring several interesting examples. You’ll read data from a file into an array and analyze structured arrays to perform a reconciliation. You’ll also learn how to quickly chart an analysis & turn a custom function into a vectorized function. This tutorial will help you master Python string splitting. You’ll learn to use , , and  to effectively handle whitespace, custom delimiters, and multiline text, which will level up your data parsing skills. Python developers use Posit Package Manager to mirror public & internally developed repos within their firewalls. Get reporting on known vulnerabilities to proactively address potential threats. High-security environments can even run air-gapped. The author was recently invited with other senior devs to give a lightning talk on their personal development philosophy. This post captures those thoughts.[ Subscribe to 🐍 PyCoder’s Weekly 💌 – Get the best Python news, articles, and tutorials delivered to your inbox once a week >> Click here to learn more ]]]></content:encoded></item><item><title>Python Insider: Python 3.14.0 alpha 5 is out</title><link>https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 16:25:58 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Here comes the antepenultimate alpha.This is an early developer preview of Python
3.14Python 3.14 is still in development. This release, 3.14.0a5, is the
fifth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2025-05-06) and, if necessary, may be modified or
deleted up until the release candidate phase (2025-07-22). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.14 are still being planned and
written. Among the new major new features and changes so far:The next pre-release of Python 3.14 will be the penultimate alpha,
3.14.0a6, currently scheduled for 2025-03-14.2025-01-29 marked the start of a new lunar year, the Year of the
Snake 🐍 (and the Year of Python?).For centuries, π was often approximated as 3 in China. Some time
between the years 1 and 5 CE, astronomer, librarian, mathematician and
politician Liu Xin (劉歆) calculated π as 3.154.Around 130 CE, mathematician, astronomer, and geographer Zhang Heng
(張衡, 78–139) compared the celestial circle with the diameter of the
earth as 736:232 to get 3.1724. He also came up with a formula for the
ratio between a cube and inscribed sphere as 8:5, implying the ratio of
a square’s area to an inscribed circle is √8:√5. From this, he
calculated π as √10 (~3.162).Third century mathematician Liu Hui (刘徽) came up with an algorithm
for calculating π iteratively: calculate the area of a polygon inscribed
in a circle, then as the number of sides of the polygon is increased,
the area becomes closer to that of the circle, from which you can
approximate π.This algorithm is similar to the method used by Archimedes in the 3rd
century BCE and Ludolph van Ceulen in the 16th century CE (see 3.14.0a2
  release notes), but Archimedes only went up to a 96-sided polygon
(96-gon). Liu Hui went up to a 192-gon to approximate π as 157/50 (3.14)
and later a 3072-gon for 3.14159.Liu Hu wrote a commentary on the book The Nine Chapters on the
Mathematical Art which included his π approximations.In the fifth century, astronomer, inventor, mathematician,
politician, and writer Zu Chongzhi (祖沖之, 429–500) used Liu Hui’s
algorithm to inscribe a 12,288-gon to compute π between 3.1415926 and
3.1415927, correct to seven decimal places. This was more accurate than
Hellenistic calculations and wouldn’t be improved upon for 900
years.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a remarkably snowless Helsinki,Your release team, Hugo van KemenadeSteve Dower]]></content:encoded></item><item><title>Real Python: Building a Python Command-Line To-Do App With Typer</title><link>https://realpython.com/courses/build-command-line-todo-app-typer/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Building an application to manage your  can be an interesting project when you’re learning a new programming language or trying to take your skills to the next level. In this video course, you’ll build a functional to-do application for the command line using Python and Typer, which is a relatively young library for creating powerful command-line interface (CLI) applications in almost no time.With a project like this, you’ll apply a wide set of core programming skills while building a real-world application with real features and requirements.In this video course, you’ll learn how to:Build a functional  with a  in PythonUse Typer to add , , and  to your to-do appTest your Python to-do application with Typer’s  and ]]></content:encoded></item><item><title>Trusted Execution Environments</title><link>https://www.schneier.com/blog/archives/2025/02/trusted-encryption-environments.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Tue, 11 Feb 2025 12:08:36 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Really good—and detailed—survey of Trusted Execution Environments (TEEs.)]]></content:encoded></item><item><title>Kushal Das: Using openpgp-card-tool-git with git</title><link>https://kushaldas.in/posts/using-openpgp-card-tool-git-with-git.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 11:12:40 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[One of the power of Unix systems comes from the various small tools and how
they work together. One such new tool I am using for some time is for  &  using OpenPGP and my Yubikey for the actual signing
operation via
openpgp-card-tool-git. I
replaced the standard  for this usecase with the  command from this
project.Installation & configurationcargo install openpgp-card-tool-git
Then you will have to configuration your (in my case the global configuration) git configuration.git config --global gpg.program <path to oct-git>
I am assuming that you already had it configured before for signing, otherwise
you have to run the following two commands too.git config --global commit.gpgsign true
git config --global tag.gpgsign true
Before you start using it, you want to save the pin in your system keyring.Use the following command.That is it, now your  will sign the commits using  tool.In the next blog post I will show how to use the other tools from the 
author for various different OpenPGP oeprations.]]></content:encoded></item><item><title>Stateful vs. Stateless Architecture</title><link>https://blog.algomaster.io/p/stateful-vs-stateless-architecture</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/7e4801c3-e3aa-4ab6-8fe6-759af4a1f91a_1684x1196.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 08:46:26 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[When a client interacts with a server, there are two ways to handle it: The client includes all necessary data in each request, so the server doesn’t store any prior information. The server retains some data from previous requests, making future interactions dependent on past state.In software systems,  refers to any data that persists across requests, such as user sessions, shopping carts, or authentication details.The choice between stateless and stateful architecture can affect scalability, performance, complexity, and cost.In this article, we’ll break down both the approaches, their advantages and trade-offs, and when to use each—with real-world examples.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .In a , the system remembers client or process data () across multiple requests.Once a client connects, the server holds on to certain details—like user preferences, shopping cart contents, or authentication sessions—so the client doesn’t need to resend everything with each request.Stateful systems typically store the state data in a database or in-memory storage. During online shopping, when you add items to your cart, the website remembers your selections. If you navigate away to browse more items and then return to your cart, your items are still there, waiting for you to check out.Common Patterns in Stateful ArchitectureIf you use  session storage (i.e., each app server keeps its own sessions locally), you can configure your load balancer for “sticky sessions.” This means: Once a client is assigned to , all subsequent requests from that client are routed to .: If Server A fails, the user’s session data is lost or the user is forced to re-log in. Sticky sessions are also less flexible when scaling because you can’t seamlessly redistribute user traffic to other servers.2. Centralized Session StoreA more robust approach is to store session data in a  or  store (e.g., Redis). : All servers can access and update session data for any user. Any server can handle any request, because the session data is not tied to a specific server’s memory.: You introduce network overhead and rely on an external storage. If the centralized storage fails, you lose session data unless you have a fallback strategy.Personalized Experiences: Stateful systems can deliver highly tailored interactions, as they remember user preferences and past actions. Users can seamlessly resume activities where they left off, even if they disconnect and reconnect. Certain operations can be faster because the server already possesses necessary data. Maintaining state for a large number of users can become resource-intensive and complex, as each server needs to keep track of specific sessions. Managing and synchronizing state across multiple servers (if needed) introduces additional challenges. If a server holding a user's state fails, their session data might be lost.E-commerce Shopping Carts – Stores cart contents and user preferences across multiple interactions, even if the user navigates away and returns.Video Streaming Services (Netflix, YouTube) – Remembers user watch progress, recommendations, and session data for a seamless experience.Messaging Apps (WhatsApp, Slack) – Maintains active user sessions and message history for real-time communication.In a  architecture, the server does  preserve client-specific data between individual requests.Each request is treated as , with no memory of previous interactions.Every request must include all necessary information for processing.Once the server responds, it discards any temporary data used for that request.: Most  follow a stateless design. For instance, when you request weather data from a public API, you must provide all required details (e.g., location) in each request. The server processes it, sends a response, and forgets the interaction.Common Patterns in Stateless Architecture1. Token-Based Authentication (JWT)A very popular way to implement statelessness is through tokens, particularly  (JSON Web Tokens):Client Authenticates Once: The user logs in using credentials (username/password) for the first time, and the server issues a signed .: The client includes JWT token in each request (e.g., Authorization: Bearer <token> header).: The server validates the token’s signature and any embedded claims (e.g., user ID, expiry time).: The server does  need to store session data; it just verifies the token on each request.Many APIs, including OAuth-based authentication systems, use JWTs to enable stateless, scalable authentication.Stateless architectures benefit from , ensuring that repeated requests produce the same result. This prevents inconsistencies due to network retries or client errors. A  request with the same payload  updates the user’s data but doesn’t create duplicates.Idempotent APIsensures consistency and reliability, especially in distributed systems where requests might be retried automatically. Stateless systems are inherently easier to scale horizontally. New servers can be added effortlessly, as they don't need to maintain any specific user sessions. Since servers don't track state, the architecture is generally simpler and easier to manage. The failure of a single server won't disrupt user sessions, as data isn't tied to specific servers.With no session data stored on the server, you free up memory that would otherwise be reserved for session management.Easier to Cache Responses: Since requests are self-contained, caching layers (like CDNs) can more easily store and serve responses. Stateless systems can't provide the same level of personalization or context awareness as stateful systems without additional effort (like using cookies or tokens).The client must keep track of the authentication token or relevant data. If it loses the token, it must re-authenticate. Every request needs to carry all the required information, potentially leading to larger payloads.Microservices Architecture: Each service handles requests independently, relying on external databases or caches instead of maintaining session data.Public APIs (REST, GraphQL): Clients send tokens with each request, eliminating the need for server-side sessions.Tokens are securely stored on the device and sent with every request to authenticate users.Stateless endpoints make caching easier since responses depend only on request parameters, not stored session data. A CDNcan cache and serve repeated requests, improving performance and reducing backend load.There's no one-size-fits-all answer when choosing between stateful and stateless architectures.The best choice depends on your application’s needs, scalability goals, and user experience expectations.When to Choose Stateful ArchitectureStateful systems are ideal when user context and continuity are critical. Consider a stateful approach if your application:Requires personalization (e.g., user preferences, session history)Needs real-time interactions (e.g., chat applications, multiplayer gaming)Manages multi-step workflows (e.g., online banking transactions, checkout processes)Must retain authentication sessions for security and convenience A shopping cart in an e-commerce app should persist, so users don’t have to re-add items after refreshing the page.When to Choose Stateless ArchitectureStateless systems work best when scalability, simplicity, and resilience are top priorities. Use a stateless approach if your application:Handles a high volume of requests and needs to scale efficientlyDoesn’t require storing client-specific data between requestsNeeds fast, distributed processing without server dependenciesMust ensure reliability and failover readiness A weather API doesn’t need to remember previous requests. Each query includes the location, and the response is processed independently.Hybrid Approaches: The Best of Both WorldsMany modern applications  stateful and stateless components for flexibility.This hybrid approach allows:Stateless APIs for core functionality, ensuring high scalabilityStateful sessions for personalization, improving user experienceExternal session stores (e.g., Redis) to manage state while keeping app servers stateless A video streaming platform (e.g., Netflix) uses a stateless backend for streaming but retains stateful user sessions to track watch history and recommendations.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Django Weblog: DSF member of the month - Lily Foote</title><link>https://www.djangoproject.com/weblog/2025/feb/10/dsf-member-of-the-month-lily-foote/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 04:51:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[For February 2025, we welcome Lily Foote (@lilyf) as our DSF member of the month! ⭐Lily Foote is a contributor to Django core for many years, especially on the ORM. She is currently a member of the Django 6.x Steering Council and she has been a DSF member since March 2021. 
You can learn more about Lily by visiting her GitHub profile.Let’s spend some time getting to know Lily better!Can you tell us a little about yourself (hobbies, education, etc)My name is Lily Foote and I’ve been contributing to Django for most of my career. I’ve also recently got into Rust and I’m excited about using Rust in Python projects. When I’m not programming, I love hiking, climbing and dancing (Ceilidh)! I also really enjoying playing board games and role playing games (e.g. Dungeons and Dragons).How did you start using Django?I’d taught myself Python in my final year at university by doing Project Euler problems and then decided I wanted to learn how to make a website. Django was the first Python web framework I looked at and it worked really well for me.What other framework do you know and if there is anything you would like to have in Django if you had magical powers?I’ve done a small amount with Flask and FastAPI. More than any new features, I think the thing that I’d most like to see is more long-term contributors to spread the work of keeping Django awesome.What projects are you working on now?The side project I’m most excited about is Django Rusty Templates, which is a re-implementation of Django’s templating language in Rust.Which Django libraries are your favorite (core or 3rd party)?What are the top three things in Django that you like?Django Conferences, the mentorship program Djangonaut Space and the whole community!You have been a mentor multiple times with GSoC and Djangonaut Space program, what is required according to you to be a good mentor?I think being willing to invest time is really important. Checking in with your mentees frequently and being an early reviewer of their work. I think this helps keep their motivation up and allows for small corrections early on.Any advice for future contributors?Start small and as you get more familiar with Django and the process of contributing you can take on bigger issues. Also be patient with reviewers – Django has high standards, but is mostly maintained by volunteers with limited time.Yes! It’s a huge honour! Since January, we’ve been meeting weekly and it feels like we’ve hardly scratched the surface of what we want to achieve. The biggest thing we’re trying to tackle is how to improve the contribution experience – especially evaluating new feature ideas – without draining everyone’s time and energy.You have a lot of knowledge in the Django ORM, how did you start to contribute to this part?I added the Greatest and Least expressions in Django 1.9, with the support of one of the core team at the time. After that, I kept showing up (especially at conference sprints) and finding a new thing to tackle.Is there anything else you’d like to say?Thank you for doing the interview, Lily!]]></content:encoded></item><item><title>Quansight Labs Blog: PEP 517 build system popularity</title><link>https://labs.quansight.org/blog/pep-517-build-system-popularity</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Analysis of PEP 517 build backends used in 8000 top PyPI packages]]></content:encoded></item><item><title>Seth Michael Larson: Building software for connection (#2: Consensus)</title><link>https://sethmlarson.dev/building-software-for-connection-consensus?utm_campaign=rss</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[In the previous article we concluded that a persistent always-on internet
connection isn't required for software to elicit feelings of connection between humans.Building on this conclusion: let's explore how Animal Crossing software was able to intercommunicate without requiring
a centralized server and infrastructure and the trade-offs for these design decisions.Distributing digital goods without the internetAnimal Crossing has over 1,000 unique items that need to be collected
for a complete catalog, including furniture, wallpapers, clothing, parasols, and carpets.
Many of these items are quite rare or were only programmed to be accessible
through an official Nintendo-affiliated distribution such as a magazine or online contest.Beyond official distributions, it's clear Animal Crossings' designer, Katsuya Eguchi,
wanted players to  to complete their catalogs.
The game incentivized trading items between towns by assigning
one “native fruit” (Apple, Orange, Cherry, Peach, or Pear) and
randomly making a subset of items harder to find than others depending
on a hidden “item group” variable (either A, B, or C).Items could be exchanged between players when one player visits another town,
but this required physically bringing your memory card to another
players' GameCube. The GameCube might have come with a handle, but the 'cube wasn't exactly a . Sharing a physical space isn't something you can do with everyone or on a regular basis.So what did Katsuya Eguchi design for Animal Crossing? To allow for item distributions from magazines and contests and to make player-to-player item sharing easier Animal Crossing included a feature called “secret codes”.This feature worked by allowing players to exchange 28-character codes with Tom Nook for items. Players could also generate codes for their friends to “send” an item from their own game to a different town. Codes could be shared by writing them on a paper note, instant message, or text message.The forgotten durability of offline software
This Reddit comment thread from the GameCube subreddit was the initial inspiration for this entire series.
The post is about someone's niece who just started playing Animal Crossing for the first time.
The Redditor asked folks to send items to their nieces' town using the secret code system.
This ended up surprising many folks that this system 
 in a game that was over 23 years old!
For reference, Nintendo Wi-Fi Connection and Nintendo Network were only available for 8 and 13 years respectively.
Below are a handful of the comments from the thread:“For real does this still work lol?”It's hard not to take these comments as indicators that something is
 with internet-connected software today. What had to go wrong for a 
system continuing to work to ? Many consumers' 
experience with 
software products
today is that they become useless e-waste after some far-away service is 
discontinued a few years after purchase.My intuition from this is that software that requires centralized servers and infrastructure to function
will have shorter lifetimes than software which is offline or only
opportunistically uses online functionality.I don't think this is particularly insightful,
more dependencies always means less resilience. But if we're building software for human connection then the software
should optimally only be limited by the availability of humans to connect.What is centralization good for?Animal Crossings' secret code system is far from perfect. The system is easily abusable, as the same secret codes can be
reused over-and-over by the same user to duplicate items without ever expiring. The only limit was that 3 codes could be used per day.Not long after Animal Crossing's release
the secret code algorithm was reverse-engineered so secret codes 
for any item could be created for any town and recipient name as if they came from an official Nintendo distribution.
This was possible because the secret code system relied on "security through obscurity".Could  be the answer to preventing these abuses?The most interesting property that a centralized authority approach
provides is : forcing everyone to play by the same rules. By storing
the “single source-of-truth” a central authority is able to prevent abuses
like the ones mentioned above.For example, a centralized “secret code issuing server” could generate
new unique codes per-use and check each code's validity
against a database to prevent users from generating their
own illegitimate codes or codes being re-used multiple times.The problem with
centralized consensus is it tends to be  to cover the entire software state.
A centralized server can generate codes perfectly, but how can that same server
 that the items you're exchanging for codes were obtained legitimately? To know this
the server would also need to track item legitimacy, leading to software which requires
an internet connection to operate.This is optimal from a correctness perspective, but as was noted earlier,
I suspect that if such a server was a mandatory part of the secret code system
in Animal Crossing that the system would likely not be usable today.This seems like a trade-off, which future would you rather have?Redesigning Animal Crossing secret codesIf I were designing Animal Crossings' secret code system with modern hardware, what would it look like?
How can we keep the offline fall-back while providing consensus and being less
abusable, especially for official distributions.I would likely use a public-key cryptographic system for official distributions,
embedding a certificate that could be used to “verify” that specific secret codes
originated from the expected centralized entity. Codes that are accepted would be
recorded to prevent reusing the same code multiple times in the same town.
Using public-key cryptography prevents the
system from being reverse-engineered to distribute arbitrary items until the certificate
private key was cracked.For sharing items between players I would implement a system where each town
generated a public and private key and the public key was shared to other towns
whenever the software was able to, such as when a player visited the other town.
Players would only be able to send items to players that they have visited
(which for Animal Crossing required physical presence, more on this later!)Each sender could store a nonce value for
each potential recipient. Embedding that nonce into the secret code would allow
the recipients' software to verify that the specific code hadn't been used yet.
The nonce wouldn't have to be long to avoid simple reusing of codes.Both above systems would require much more data to be embedded into each “secret
code” compared to the 28-character codes from the GameCube. For this I would
use QR codes to embed over 2KB of data into a single QR code. Funnily enough,
Animal Crossing New Leaf and onwards use QR code technology for players to share design patterns.This design is still abusable if users can modify their software or hardware
but doesn't suffer from the trivial-to-exploit flaws of Animal Crossing's secret code system.Decentralized global consensus?What if we could have the best of both worlds: we want consensus
that is both  and . At least today, we are out of luck.Decentralized global consensus is technologically feasible, but the existing solutions
(mostly blockchains)
are expensive (both in energy and capital) and can't handle throughput on any sort of 
meaningful scale.There are many other decentralized consensus systems that 
are able to form “pockets” of useful peer-to-peer consensus using a fraction of
the resources, such as email, BitTorrent, ActivityPub, and Nostr.
These systems are only possible by adding  or by only guaranteeing .When is global consensus needed?Obviously global consensus is important for certain classes of software like 
financial, civics, and infrastructure, but I wonder how the necessity
of consensus in software changes for software with different risk
profiles.For software which has fewer risks associated with misuse is there as much
need for global consensus?
How can  be designed to reduce risk and require
less consensus to be effective? If global consensus and centralized 
servers become unnecessary, can we expect  to be usable 
on much longer timescales, essentially for as long as there are users?]]></content:encoded></item><item><title>Python Morsels: Newlines and escape sequences in Python</title><link>https://www.pythonmorsels.com/newlines-and-escape-sequences/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Mon, 10 Feb 2025 15:17:29 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[This string contains a newline character:That's what  represents: a newline character.If we print this string, we'll see that  becomes an  newline:Why does Python represent a newline as ?Escape sequences in PythonEvery character in a Python …]]></content:encoded></item><item><title>Pairwise Authentication of Humans</title><link>https://www.schneier.com/blog/archives/2025/02/pairwise-authentication-of-humans.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Mon, 10 Feb 2025 12:00:41 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Here’s an easy system for two humans to remotely authenticate to each other, so they can be sure that neither are digital impersonations.To mitigate that risk, I have developed this simple solution where you can setup a unique time-based one-time passcode (TOTP) between any pair of persons.Two people, Person A and Person B, sit in front of the same computer and open this page;
They input their respective names (e.g. Alice and Bob) onto the same page, and click “Generate”;
The page will generate two TOTP QR codes, one for Alice and one for Bob;
Alice and Bob scan the respective QR code into a TOTP mobile app (such as Authy or Google Authenticator) on their respective mobile phones;
In the future, when Alice speaks with Bob over the phone or over video call, and wants to verify the identity of Bob, Alice asks Bob to provide the 6-digit TOTP code from the mobile app. If the code matches what Alice has on her own phone, then Alice has more confidence that she is speaking with the real Bob.]]></content:encoded></item><item><title>TypeScript: extracting parts of composite types via infer</title><link>https://2ality.com/2025/02/typescript-infer-operator.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this blog post, we explore how we can extract parts of composite types via the  operator.It helps if you are loosely familiar with conditional types. You can check out section “Conditional types” in “Tackling TypeScript” to read up on them.]]></content:encoded></item><item><title>TypeDoc: testing code examples in doc comments</title><link>https://2ality.com/2025/02/testing-typedoc-examples.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 9 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[TypeDoc now lets us refer to parts of other files via . In this blog post, I explain how that works and why it’s useful.]]></content:encoded></item><item><title>UK Is Ordering Apple to Break Its Own Encryption</title><link>https://www.schneier.com/blog/archives/2025/02/uk-is-ordering-apple-to-break-its-own-encryption.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Sat, 8 Feb 2025 15:56:32 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[The  is reporting that the UK government has served Apple with a “technical capability notice” as defined by the 2016 Investigatory Powers Act, requiring it to break the Advanced Data Protection encryption in iCloud for the benefit of law enforcement.This is a big deal, and something we in the security community have worried was coming for a while now.The law, known by critics as the Snoopers’ Charter, makes it a criminal offense to reveal that the government has even made such a demand. An Apple spokesman declined to comment.Apple can appeal the U.K. capability notice to a secret technical panel, which would consider arguments about the expense of the requirement, and to a judge who would weigh whether the request was in proportion to the government’s needs. But the law does not permit Apple to delay complying during an appeal.In March, when the company was on notice that such a requirement might be coming, it told Parliament: “There is no reason why the U.K. [government] should have the authority to decide for citizens of the world whether they can avail themselves of the proven security benefits that flow from end-to-end encryption.”Apple is likely to turn the feature off for UK users rather than break it for everyone worldwide. Of course, UK users will be able to spoof their location. But this might not be enough. According to the law, Apple would not be able to offer the feature to anyone who is in the UK at any point: for example, a visitor from the US.And what happens next? Australia has a law enabling it to ask for the same thing. Will it? Will even more countries follow?]]></content:encoded></item><item><title>TypeScript: the satisfies operator</title><link>https://2ality.com/2025/02/satisfies-operator.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 8 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[TypeScript’s  operator lets us check the type of a value (mostly) without influencing it. In this blog post, we examine how exactly it works and where it’s useful.]]></content:encoded></item><item><title>Seeking Purity</title><link>http://lucumr.pocoo.org/2025/2/8/seeking-purity</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Sat, 8 Feb 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[The concept of purity — historically a guiding principle in social and
moral contexts — is also found in passionate, technical discussions.  By
that I mean that purity in technology translates into adherence to a set
of strict principles, whether it be functional programming, test-driven
development, serverless architectures, or, in the case of Rust, memory
safety.Rust positions itself as a champion of memory safety, treating it as a
non-negotiable foundation of good software engineering.  I love Rust: it's
probably my favorite language.  It probably won't surprise you that I have
no problem with it upholding memory safety as a defining feature.Rust aims to achieve the goal of memory safety via safe abstractions, a
compile time borrow checker and a type system that is in service of those
safe abstractions.  It comes as no surprise that the Rust community is
also pretty active in codifying a new way to reason about pointers.  In many ways,
Rust pioneered completely new technical approaches and it it widely
heralded as an amazing innovation.However, as with many movements rooted in purity, what starts as a
technical pursuit can evolve into something more ideological.  Similar to
how moral purity in political and cultural discourse can become charged,
so does the discourse around Rust, which has been dominated by the pursuit
of memory safety.  Particularly within the core Rust community itself,
discussion has moved beyond technical merits into something akin to
ideological warfare.  The fundamental question of “Is this code memory
safe?”, has shifted to “Was it made memory safe in the  way?”.
This distinction matters because it introduces a purity test that values
methodology over outcomes.  Safe C code, for example, is often dismissed
as impossible, not necessarily because it  impossible, but because it
lacks the strict guarantees that Rust's borrow checker enforces.
Similarly, using Rust’s  blocks is increasingly frowned upon,
despite their intended purpose of enabling low-level optimizations when
necessary.This ideological rigidity creates significant friction when Rust
interfaces with other ecosystems (or gets introduced there), particularly
those that do not share its uncompromising stance.  For instance, the role
of Rust in the Linux kernel has been a hot topic.  The Linux kernel
operates under an entirely different set of priorities.  While memory
safety is important there is insufficient support for adopting Rust in
general.  The kernel is an old project and it aims to remain maintainable
for a long time into the future.  For it to even consider a rather young
programming language should be seen as tremendous success for Rust and
also for how open Linus is to the idea.Yet that introduction is balanced against performance, maintainability,
and decades of accumulated engineering expertise.  Many of the kernel
developers, who have found their own strategies to write safe C for
decades, are not accepting the strongly implied premise that their work is
inherently flawed simply because it does not adhere to Rust's strict
purity rules.Tensions rose when a kernel developer advocating for Rust's inclusion took
to social media to push for changes in the Linux kernel development
process.  The public shaming tactic failed, leading the developer to
conclude:
“If shaming on social media does not work, then tell me what does,
because I'm out of ideas.”It's not just the kernel where Rust's memory safety runs up against the
complexities of the real world.  Very similar feelings creep up in the
gaming industry where people love to do wild stuff with pointers.  You do
not need large disagreements to see the purist approach create some
friction.  A recent post of mine for instance
triggered some discussions about the trade-offs between more dependencies,
and moving unsafe to centralized crates.I really appreciate that Rust code does not crash as much.  That part of
Rust, among many others, makes it very enjoyable to work with.  Yet I am
entirely unconvinced that memory safety should trump everything, at least
at this point in time.What people want in the Rust in Linux situation is for the project leader
to come in to declare support for Rust's call for memory safety above all.
To make the detractors go away.Python's Migration LessonHearing this call and discussion brings back memories.  I have lived
through a purity driven shift in a community before.  The move from Python
2 to Python 3 started out very much the same way.  There was an almost
religious movement in the community to move to Python 3 in a ratcheting
motion.  The idea that you could maintain code bases that support both 2
and 3 were initially very
loudly rejected.  I took a lot of flak at the time (and for years after)
for advocating for a more pragmatic migration which burned me out a lot.
That feedback came both in person and online and it largely pushed me away
from Python for a while.  Not getting behind the Python 3 train was seen
as sabotaging the entire project.  However, a decade later, I feel
somewhat vindicated that it was worth being pragmatic about that
migration.At the root of that discourse was a idealistic view of how Unicode could
work in the language and that you can move an entire ecosystem at once.
Both those things greatly clashed with the lived realities in many
projects and companies.I am a happy user of Python 3 today.  This migration has also taught me
the important lesson not be too stuck on a particular idea.  It would have
been very easy to pick one of the two sides of that debate.  Be stuck on
Python 2 (at the risk of forking), or go all in on Python 3 no questions
asked.  It was the path in between that was quite painful to advocate for,
but it was ultimately the right path.  I wrote about my lessons of that
migration a in 2016 and
I think most of this still rings true.  That was motivated by even years
later people still reaching out to me who did not move to Python 3, hoping
for me to embrace their path.  Yet Python 3 has changed!  Python 3 is a
much better language than it was when it first released.  It is a great
language because it's used by people solving real, messy problems and
because it over time found answers for what to do, if you need to have
both Python 2 and 3 code in the wild.  While the world of Python 2 is
largely gone, we are still in a world where Unicode and bytes mix in
certain contexts.Fully committing to a single worldview can be easier because you stop
questioning everything — you can just go with the flow.  Yet truths often
reside on both sides.  Allowing yourself to walk the careful middle path
enables you to learn from multiple perspectives.  You will face doubts and
open yourself up to vulnerability and uncertainty.  The payoff, however,
is the ability to question deeply held beliefs and push into the unknown
territory where new things can be found.  You can arrive at a solution
that isn't a complete rejection of any side.  There is genuine value in
what Rust offers—just as there was real value in what Python 3 set out to
accomplish.  But the Python 3 of today isn't the Python 3 of those early,
ideological debates; it was shaped by a messy, slow, often contentious,
yet ultimately productive transition process.I am absolutely sure that in 30 years from now we are going to primarily
program in memory safe languages (or the machines will do it for us) in
environments where C and C++ prevail.  That glimpse of a future I can
visualize clearly.  The path to there however?  That's a different story
altogether.  It will be hard, it will be impure.  Maybe the solution will
not even involve Rust at all — who knows.We also have to accept that not everyone is ready for change at the same
pace. Forcing adoption when people aren't prepared only causes the
pendulum to swing back hard.  It's tempting to look for a single authority
to declare “the one true way,” but that won't smooth out the inevitable
complications.  Indeed, those messy, incremental challenges are part of how
real progress happens.  In the long run, these hard-won refinements tend
to produce solutions that benefit all sides—if we’re patient enough to let
them take root.  The painful and messy transition is here to stay, and
that's exactly why, in the end, it works.]]></content:encoded></item><item><title>Friday Squid Blogging: The Colossal Squid</title><link>https://www.schneier.com/blog/archives/2025/02/friday-squid-blogging-the-colossal-squid.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 7 Feb 2025 22:02:37 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The case for sans-io</title><link>https://fasterthanli.me/articles/the-case-for-sans-io</link><author>Amos Wenger</author><category>Faster than time blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 7 Feb 2025 18:53:01 +0000</pubDate><source url="https://fasterthanli.me/index.xml">fasterthanli.me</source><content:encoded><![CDATA[The most popular option to decompress ZIP files from the Rust programming
language is a crate simply named zip — At the time of this writing, it has 48
million downloads. It’s fully-featured, supporting various compression methods,
encryption, and even supports writing zip files.However, that’s not the crate  uses to read ZIP files. Some
applications benefit from using asynchronous I/O, especially if they decompress
archives that they download from the network.]]></content:encoded></item><item><title>Screenshot-Reading Malware</title><link>https://www.schneier.com/blog/archives/2025/02/screenshot-reading-malware.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 7 Feb 2025 15:26:11 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Kaspersky is reporting on a new type of smartphone malware.The malware in question uses optical character recognition (OCR) to review a device’s photo library, seeking screenshots of recovery phrases for crypto wallets. Based on their assessment, infected Google Play apps have been downloaded more than 242,000 times. Kaspersky says: “This is the first known case of an app infected with OCR spyware being found in Apple’s official app marketplace.”That’s a tactic I have not heard of before.]]></content:encoded></item><item><title>From PDFs to Insights: Structured Outputs from PDFs with Gemini 2.0</title><link>https://www.philschmid.de/gemini-pdf-to-data</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Fri, 7 Feb 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[Learn how to extract structured data from PDFs with Gemini 2.0 and Pydantic.]]></content:encoded></item></channel></rss>