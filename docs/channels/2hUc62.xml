<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blog</title><link>https://www.awesome-dev.news</link><description></description><item><title>Links</title><link>https://matklad.github.io/2025/08/23/links.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 23 Aug 2025 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[If you have a blog, consider adding a “links” page to it, which references resources that you find
notable:I’ve started my links page several years ago, mostly because I found myself referring to the same
few links repeatedly in various discussions, and not all the links were easily searchable.Note that the suggestion is different from more typical “monthly links roundup”, which is nice to
maintain Substack engagement/community, but doesn’t contribute to long-term knowledge distilling.It is also different from the exhaustive list of everything I’ve read on the Internet. It is
relatively short, considering its age.]]></content:encoded></item><item><title>Friday Squid Blogging: Bobtail Squid</title><link>https://www.schneier.com/blog/archives/2025/08/friday-squid-blogging-bobtail-squid.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 22 Aug 2025 21:02:39 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[As usual, you can also use this squid post to talk about the security stories in the news that I haven’t covered.]]></content:encoded></item><item><title>The science of loudness</title><link>https://fasterthanli.me/articles/the-science-of-loudness</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 22 Aug 2025 20:30:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[My watch has a “Noise” app: it shows , for decibels.My amp has a volume knob, which also shows decibels, although.. negative ones, this time.And finally, my video editing software has a ton of meters — which are all in decibel or
decibel-adjacent units.How do all these decibels fit together?]]></content:encoded></item><item><title>I’m Spending the Year at the Munk School</title><link>https://www.schneier.com/blog/archives/2025/08/im-spending-the-year-at-the-munk-school.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 22 Aug 2025 19:00:37 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[This academic year, I am taking a sabbatical from the Kennedy School and Harvard University. (It’s not a real sabbatical—I’m just an adjunct—but it’s the same idea.) I will be spending the Fall 2025 and Spring 2026 semesters at the Munk School at the University of Toronto.I will be organizing a reading group on AI security in the fall. I will be teaching my cybersecurity policy class in the Spring. I will be working with Citizen Lab, the Law School, and the Schwartz Reisman Institute. And I will be enjoying all the multicultural offerings of Toronto.It’s all pretty exciting.]]></content:encoded></item><item><title>AI Agents Need Data Integrity</title><link>https://www.schneier.com/blog/archives/2025/08/ai-agents-need-data-integrity.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Fri, 22 Aug 2025 11:04:19 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[Think of the Web as a digital territory with its own social contract. In 2014, Tim Berners-Lee called for a “Magna Carta for the Web” to restore the balance of power between individuals and institutions. This mirrors the original charter’s purpose: ensuring that those who occupy a territory have a meaningful stake in its governance.Web 3.0—the distributed, decentralized Web of tomorrow—is finally poised to change the Internet’s dynamic by returning ownership to data creators. This will change many things about what’s often described as the “CIA triad” of digital security: confidentiality, integrity, and availability. Of those three features, data integrity will become of paramount importance.When we have agency in digital spaces, we naturally maintain their integrity—protecting them from deterioration and shaping them with intention. But in territories controlled by distant platforms, where we’re merely temporary visitors, that connection frays. A disconnect emerges between those who benefit from data and those who bear the consequences of compromised integrity. Like homeowners who care deeply about maintaining the property they own, users in the Web 3.0 paradigm will become stewards of their personal digital spaces.This will be critical in a world where AI agents don’t just answer our questions but act on our behalf. These agents may execute financial transactions, coordinate complex workflows, and autonomously operate critical infrastructure, making decisions that ripple through entire industries. As digital agents become more autonomous and interconnected, the question is no longer whether we will trust AI but what that trust is built upon. In the new age we’re entering, the foundation isn’t intelligence or efficiency—it’s integrity.In information systems, integrity is the guarantee that data will not be modified without authorization, and that all transformations are verifiable throughout the data’s life cycle. While availability ensures that systems are running and confidentiality prevents unauthorized access, integrity focuses on whether information is accurate, unaltered, and consistent across systems and over time.It’s a new idea. The undo button, which prevents accidental data loss, is an integrity feature. So is the reboot process, which returns a computer to a known good state. Checksums are an integrity feature; so are verifications of network transmission. Without integrity, security measures can backfire. Encrypting corrupted data just locks in errors. Systems that score high marks for availability but spread misinformation just become amplifiers of risk.All IT systems require some form of data integrity, but the need for it is especially pronounced in two areas today. First: Internet of Things devices interact directly with the physical world, so corrupted input or output can result in real-world harm. Second: AI systems are only as good as the integrity of the data they’re trained on, and the integrity of their decision-making processes. If that foundation is shaky, the results will be too.Integrity manifests in four key areas. The first,  concerns the quality and authenticity of data entering a system. When this fails, consequences can be severe. In 2021, Facebook’s global outage was triggered by a single mistaken command—an input error missed by automated systems. Protecting input integrity requires robust authentication of data sources, cryptographic signing of sensor data, and diversity in input channels for cross-validation.The second issue is  which ensures that systems transform inputs into outputs correctly. In 2003, the U.S.-Canada blackout affected 55 million people when a control-room process failed to refresh properly, resulting in damages exceeding US $6 billion. Safeguarding processing integrity means formally verifying algorithms, cryptographically protecting models, and monitoring systems for anomalous behavior. covers the correctness of information as it’s stored and communicated. In 2023, the Federal Aviation Administration was forced to halt all U.S. departing flights because of a corrupted database file. Addressing this risk requires cryptographic approaches that make any modification computationally infeasible without detection, distributed storage systems to prevent single points of failure, and rigorous backup procedures.Finally,  addresses the appropriate flow of information according to the norms of its larger context. It’s not enough for data to be accurate; it must also be used in ways that respect expectations and boundaries. For example, if a smart speaker listens in on casual family conversations and uses the data to build advertising profiles, that action would violate the expected boundaries of data collection. Preserving contextual integrity requires clear data-governance policies, principles that limit the use of data to its intended purposes, and mechanisms for enforcing information-flow constraints.As AI systems increasingly make critical decisions with reduced human oversight, all these dimensions of integrity become critical.The Need for Integrity in Web 3.0As the digital landscape has shifted from Web 1.0 to Web 2.0 and now evolves toward Web 3.0, we’ve seen each era bring a different emphasis in the CIA triad of confidentiality, integrity, and availability.Returning to our home metaphor: When simply having shelter is what matters most, availability takes priority—the house must exist and be functional. Once that foundation is secure, confidentiality becomes important—you need locks on your doors to keep others out. Only after these basics are established do you begin to consider integrity, to ensure that what’s inside the house remains trustworthy, unaltered, and consistent over time.Web 1.0 of the 1990s prioritized making information available. Organizations digitized their content, putting it out there for anyone to access. In Web 2.0, the Web of today, platforms for e-commerce, social media, and cloud computing prioritize confidentiality, as personal data has become the Internet’s currency.Somehow, integrity was largely lost along the way. In our current Web architecture, where control is centralized and removed from individual users, the concern for integrity has diminished. The massive social media platforms have created environments where no one feels responsible for the truthfulness or quality of what circulates.Web 3.0 is poised to change this dynamic by returning ownership to the data owners. This is not speculative; it’s already emerging. For example, ActivityPub, the protocol behind decentralized social networks like Mastodon, combines content sharing with built-in attribution. Tim Berners-Lee’s Solid protocol restructures the Web around personal data pods with granular access controls.These technologies prioritize integrity through cryptographic verification that proves authorship, decentralized architectures that eliminate vulnerable central authorities, machine-readable semantics that make meaning explicit—structured data formats that allow computers to understand participants and actions, such as “Alice performed surgery on Bob”—and transparent governance where rules are visible to all. As AI systems become more autonomous, communicating directly with one another via standardized protocols, these integrity controls will be essential for maintaining trust.Why Data Integrity Matters in AIFor AI systems, integrity is crucial in four domains. The first is decision quality. With AI increasingly contributing to decision-making in health care, justice, and finance, the integrity of both data and models’ actions directly impact human welfare. Accountability is the second domain. Understanding the causes of failures requires reliable logging, audit trails, and system records.The third domain is the security relationships between components. Many authentication systems rely on the integrity of identity information and cryptographic keys. If these elements are compromised, malicious agents could impersonate trusted systems, potentially creating cascading failures as AI agents interact and make decisions based on corrupted credentials.Finally, integrity matters in our public definitions of safety. Governments worldwide are introducing rules for AI that focus on data accuracy, transparent algorithms, and verifiable claims about system behavior. Integrity provides the basis for meeting these legal obligations.The importance of integrity only grows as AI systems are entrusted with more critical applications and operate with less human oversight. While people can sometimes detect integrity lapses, autonomous systems may not only miss warning signs—they may exponentially increase the severity of breaches. Without assurances of integrity, organizations will not trust AI systems for important tasks, and we won’t realize the full potential of AI.How to Build AI Systems With IntegrityImagine an AI system as a home we’re building together. The integrity of this home doesn’t rest on a single security feature but on the thoughtful integration of many elements: solid foundations, well-constructed walls, clear pathways between rooms, and shared agreements about how spaces will be used.We begin by laying the cornerstone: cryptographic verification. Digital signatures ensure that data lineage is traceable, much like a title deed proves ownership. Decentralized identifiers act as digital passports, allowing components to prove identity independently. When the front door of our AI home recognizes visitors through their own keys rather than through a vulnerable central doorman, we create resilience in the architecture of trust.Formal verification methods enable us to mathematically prove the structural integrity of critical components, ensuring that systems can withstand pressures placed upon them—especially in high-stakes domains where lives may depend on an AI’s decision.Just as a well-designed home creates separate spaces, trustworthy AI systems are built with thoughtful compartmentalization. We don’t rely on a single barrier but rather layer them to limit how problems in one area might affect others. Just as a kitchen fire is contained by fire doors and independent smoke alarms, training data is separated from the AI’s inferences and output to limit the impact of any single failure or breach.Throughout this AI home, we build transparency into the design: The equivalent of large windows that allow light into every corner is clear pathways from input to output. We install monitoring systems that continuously check for weaknesses, alerting us before small issues become catastrophic failures.But a home isn’t just a physical structure, it’s also the agreements we make about how to live within it. Our governance frameworks act as these shared understandings. Before welcoming new residents, we provide them with certification standards. Just as landlords conduct credit checks, we conduct integrity assessments to evaluate newcomers. And we strive to be good neighbors, aligning our community agreements with broader societal expectations. Perhaps most important, we recognize that our AI home will shelter diverse individuals with varying needs. Our governance structures must reflect this diversity, bringing many stakeholders to the table. A truly trustworthy system cannot be designed only for its builders but must serve anyone authorized to eventually call it home.That’s how we’ll create AI systems worthy of trust: not by blindly believing in their perfection but because we’ve intentionally designed them with integrity controls at every level.Unlike other properties of security, like “available” or “private,” we don’t have a common adjective form for “integrity.” This makes it hard to talk about it. It turns out that there is a word in English: “integrous.” The Oxford English Dictionary recorded the word used in the mid-1600s but now declares it obsolete.We believe that the word needs to be revived. We need the ability to describe a system with integrity. We must be able to talk about integrous systems design.Ensuring integrity in AI presents formidable challenges. As models grow larger and more complex, maintaining integrity without sacrificing performance becomes difficult. Integrity controls often require computational resources that can slow systems down—particularly challenging for real-time applications. Another concern is that emerging technologies like quantum computingthreaten current cryptographic protections. Additionally, the distributed nature of modern AI—which relies on vast ecosystems of libraries, frameworks, and services—presents a large attack surface.Beyond technology, integrity depends heavily on social factors. Companies often prioritize speed to market over robust integrity controls. Development teams may lack specialized knowledge for implementing these controls, and may find it particularly difficult to integrate them into legacy systems. And while some governments have begun establishing regulations for aspects of AI, we need worldwide alignment on governance for AI integrity.Addressing these challenges requires sustained research into verifying and enforcing integrity, as well as recovering from breaches. Priority areas include fault-tolerantalgorithms for distributed learning, verifiable computation on encrypted data, techniques that maintain integrity despite adversarial attacks, and standardized metrics for certification. We also need interfaces that clearly communicate integrity status to human overseers.As AI systems become more powerful and pervasive, the stakes for integrity have never been higher. We are entering an era where machine-to-machine interactions and autonomous agents will operate with reduced human oversight and make decisions with profound impacts.The good news is that the tools for building systems with integrity already exist. What’s needed is a shift in mind-set: from treating integrity as an afterthought to accepting that it’s the core organizing principle of AI security.The next era of technology will be defined not by what AI can do, but by whether we can trust it to know or especially to do what’s right. Integrity—in all its dimensions—will determine the answer.Sidebar: Examples of Integrity FailuresProcessing integrity failure
A 64-bit velocity calculation was converted to a 16-bit output, causing an error called overflow. The corrupted data triggered catastrophic course corrections that forced the US $370 million rocket to self-destruct.Processing integrity failure
Lockheed Martin’s software calculated thrust in pound-seconds, while NASA’s navigation software expected newton-seconds. The failure caused the $328 million spacecraft to burn up in the Mars atmosphere.
Faulty sensor data caused an automated flight-control system to repeatedly push the airplane’s nose down, leading to a fatal crash.Storage integrity failure
Russian hackers compromised the process that SolarWinds used to package its software, injecting malicious code that was distributed to 18,000 customers, including nine federal agencies. The hack remained undetected for 14 months.Storage integrity failure
A bug in OpenAI’s ChatGPT mixed different users’ conversation histories. Users suddenly had other people’s chats appear in their interfaces with no way to prove the conversations weren’t theirs.Contextual integrity failure
Users discovered that the AI image generator often produced biased images of people, such as showing white men as CEOs regardless of the prompt. The AI tool didn’t accurately reflect the context requested by the users.
Attackers embedded hidden prompts in emails, documents, and websites that hijacked AI assistants, causing them to treat malicious instructions as legitimate commands.Processing integrity failure
A faulty software update from CrowdStrike caused 8.5 million Windows computers worldwide to crash—grounding flights, shutting down hospitals, and disrupting banks. The update, which contained a software logic error, hadn’t gone through full testing protocols.Input and processing integrity failure
Scammers used AI-powered voice-cloning tools to mimic the voices of victims’ family members, tricking people into sending money. These scams succeeded because neither phone systems nor victims identified the AI-generated voice as fake.This essay was written with Davi Ottenheimer, and originally appeared in IEEE Spectrum.]]></content:encoded></item><item><title>Jim Sanborn Is Auctioning Off the Solution to Part Four of the Kryptos Sculpture</title><link>https://www.schneier.com/blog/archives/2025/08/jim-sanborn-is-auctioning-off-the-solution-to-part-four-of-the-kryptos-sculpture.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Thu, 21 Aug 2025 11:02:28 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[The auction, which will include other items related to cryptology, will be held Nov. 20. RR Auction, the company arranging the sale, estimates a winning bid between $300,000 and $500,000.Along with the original handwritten plain text of K4 and other papers related to the coding, Mr. Sanborn will also be providing a 12-by-18-inch copper plate that has three lines of alphabetic characters cut through with a jigsaw, which he calls “my proof-of-concept piece” and which he kept on a table for inspiration during the two years he and helpers hand-cut the letters for the project. The process was grueling, exacting and nerve wracking. “You could not make any mistake with 1,800 letters,” he said. “It could not be repaired.”Mr. Sanborn’s ideal winning bidder is someone who will hold on to that secret. He also hopes that person is willing to take over the system of verifying possible solutions and reviewing those unending emails, possibly through an automated system.]]></content:encoded></item><item><title>Subverting AIOps Systems Through Poisoned Input Data</title><link>https://www.schneier.com/blog/archives/2025/08/subverting-aiops-systems-through-poisoned-input-data.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Wed, 20 Aug 2025 11:02:27 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[In this input integrity attack against an AI system, researchers were able to fool AIOps tools:AIOps refers to the use of LLM-based agents to gather and analyze application telemetry, including system logs, performance metrics, traces, and alerts, to detect problems and then suggest or carry out corrective actions. The likes of Cisco have deployed AIops in a conversational interface that admins can use to prompt for information about system performance. Some AIOps tools can respond to such queries by automatically implementing fixes, or suggesting scripts that can address issues.These agents, however, can be tricked by bogus analytics data into taking harmful remedial actions, including downgrading an installed package to a vulnerable version. AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings.In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agent’s decision-making. Our attack methodology, AIOpsDoom, is fully automated—combining reconnaissance, fuzzing, and LLM-driven adversarial input generation—and operates without any prior knowledge of the target system.To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance.Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design.]]></content:encoded></item><item><title>Learning web development: Booleans, comparisons and if statements</title><link>https://2ality.com/2025/08/javascript-booleans-comparisons-if.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this chapter, we learn about tools for only running a piece of code if a condition is met: truth values (booleans), comparisons and  statements.]]></content:encoded></item><item><title>Zero-Day Exploit in WinRAR File</title><link>https://www.schneier.com/blog/archives/2025/08/zero-day-exploit-in-winrar-file.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Tue, 19 Aug 2025 11:07:28 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[A zero-day vulnerability in WinRAR is being exploited by at least two Russian criminal groups:The vulnerability seemed to have super Windows powers. It abused alternate data streams, a Windows feature that allows different ways of representing the same file path. The exploit abused that feature to trigger a previously unknown path traversal flaw that caused WinRAR to plant malicious executables in attacker-chosen file paths %TEMP% and %LOCALAPPDATA%, which Windows normally makes off-limits because of their ability to execute code.More details in the article.]]></content:encoded></item><item><title>Eavesdropping on Phone Conversations Through Vibrations</title><link>https://www.schneier.com/blog/archives/2025/08/eavesdropping-on-phone-conversations-through-vibrations.html</link><author>Bruce Schneier</author><category>infosec</category><category>blog</category><pubDate>Mon, 18 Aug 2025 11:02:55 +0000</pubDate><source url="https://www.schneier.com/">Schneider on Security</source><content:encoded><![CDATA[Researchers have managed to eavesdropon cell phone voice conversations by using radar to detect vibrations. It’s more a proof of concept than anything else. The radar detector is only ten feet away, the setup is stylized, and accuracy is poor. But it’s a start.]]></content:encoded></item><item><title>Bliki: Expansion Joints</title><link>https://martinfowler.com/bliki/ExpansionJoints.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Mon, 18 Aug 2025 04:00:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Back in the days when I did live talks, one of my abilities was to finish
  on time, even if my talk time was cut at the last moment (perhaps due to the
  prior speaker running over). The key to my ability to do this was to use
  Expansion Joints - parts of the talk that I'd
  pre-planned so I could cover them quickly or slowly depending on how much time
  I had.The way I'd do this would be to plan for some topics to be optional. The
  talk would work if I skipped over them, but I could also witter on about them
  for five (or ten) minutes. Ideally, each of these topics would get one slide,
  usually with a bunch of key phrases on it - the headings of what I'd talk
  about should I be talking about it. When I got to the slide, I'd look at how
  time was going with the talk. If (as was usually the case) I was running short
  of time, I could cover the slide in about thirty seconds, saying something
  like: “in doing this, there's a bunch of things you need to consider, but they
  are out of scope for today's talk”.If, however, I did have time, I could then spend some time talking about
  them. The slide would be simple, and not provide much of a Visual Channel, but that wasn't so important, after all this material
  was optional in the first place.The single flex-slide was my favorite Expansion Joint, as it was easy to
  use. Sometimes however my optional topic required a proper visual channel,
  necessitating dedicated slides. My solution here was good control over
  slide handling. Presentation tools include the ability to skip over slides
  while I'm talking, and I made sure I practiced how to use them so I could skip
  a bunch of slides without the audience knowing. It's crucial here that it's
  invisible to the audience, I find it looks sloppy if anyone says “in the
  interests of time I'll skip over these slides”. To do this, however, I do need
  access to my laptop while presenting, venues that only provide a clicker while
  loading the slides on some other machine lack that control. That started to
  happen in my last couple of years, much to my annoyance.When creating talks, I was always worried that I would run out of things to
  say, even though experience told me I reliably crammed more stuff in than I
  could possibly cover. Expansion Joints helped with this, I could aggressively
  trim the core talk to less than I needed, and rely on the Expansion Joints to
  fill the gap. In practice I usually didn't need the Expansion Joints anyway, but
  their presence helped my confidence.Using Expansion Joints was particularly important for me as I never
  rehearsed my talks. I was always someone whose ability to present was driven by
  adrenaline. Talking to a rubber duck just didn't work, the duck was clearly
  every bit as bored as I was. Consequently the first time I gave a talk, I was
  hazy as to how long it would take. Yet with Expansion Joints in place, I was
  able to finish a talk right on time.Expansion Joints enabled me to give the same talk
  to different time slots. Sometimes I'd have thirty minutes, sometimes
  forty-five. With Expansion Joints, I didn't need to change my slides,
  particularly handy if a time cut (or more rarely a time increase) appeared at the
  last moment. (Although in my later years, I handled this by doing a Suite Of Talks.)Talks that encourage audience interaction need these because we can never
  predict how much time the interaction will use up. Sometimes we get a steady
  stream of questions, other times (particularly in Scandinavia, or
  upper-Midwest America) a lack of questions had me blasting through the agenda.
  Any such talk needed a double-dose of this temporal ballast.Expansion Joints are at their most useful in later parts of the talk, as
  it's then that I have the most information on how much time I have. Earlier
  ones can still be handy, particularly if they come after an interactive
  section when I'd like to rebase my timing.The name was coined by Neal Ford, Matthew McCullough, and Nathaniel
    Schutta in their excellent book Presentation Patterns.]]></content:encoded></item><item><title>Learning web development: Arrays in JavaScript</title><link>https://2ality.com/2025/08/javascript-arrays.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this chapter we look at one way of storing more than one value in a variable: .]]></content:encoded></item><item><title>Learning web development: Strings and methods in JavaScript</title><link>https://2ality.com/2025/08/javascript-strings-methods.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In the last chapter, we worked with numbers. In this chapter, we’ll work with text and write our first applications.]]></content:encoded></item></channel></rss>