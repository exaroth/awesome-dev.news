<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>What&apos;s the best way to run redis in cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1lh24mz/whats_the_best_way_to_run_redis_in_cluster/</link><author>/u/TemporalChill</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 17:36:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I just installed cnpg and the dx is nice. Wondering if there's anything close to that quality for redis?]]></content:encoded></item><item><title>[P] Autopaste MFA codes from Gmail using Local LLMs</title><link>https://www.reddit.com/r/MachineLearning/comments/1lh0rmp/p_autopaste_mfa_codes_from_gmail_using_local_llms/</link><author>/u/samewakefulinsomnia</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 16:37:45 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs]]></content:encoded></item><item><title>Stoned Gopher</title><link>https://postimg.cc/qNWDQgN1</link><author>/u/BloomerGrow</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 16:30:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Behind the scenes: Redpanda Cloud’s response to the GCP outage</title><link>https://www.redpanda.com/blog/gcp-outage-june-redpanda-cloud</link><author>/u/gametorch</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 15:33:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[On Jun 12th, 2025, Google Cloud Platform (GCP) experienced an unfortunate global outage triggered by an automated quota update to their API management system. What was a major outage for a large part of the internet was just another normal day for Redpanda Cloud customers. While GCP dealt with the widespread disruption that impacted many critical services, Redpanda Cloud clusters in GCP remained stable, thanks to being purposely designed for the SLA we offer, along with a cell-based architecture that we also made a product principle. But behind the scenes, it was far from quiet. This post provides a brief timeline of events from our own experience, our response, previously untold details about Redpanda Cloud, and closing thoughts on safety and reliability practices in our industry.Why do incidents like this happenModern computer systems are complex systems — and complex systems are characterized by their non-linear nature, which means that observed changes in an output  proportional to the change in the input. This concept is also known in chaos theory as the , or in systems thinking, with the expression, “The whole is greater than the sum of its parts”. When this mathematical fact is acknowledged, safety and reliabiilty measures are put in place, such as closing feedback control loops, phasing change rollouts, shedding load, applying backpressure, randomizing retries, and defining incident response processes, among others.GCP’s seemingly innocuous automated quota update triggered a butterfly effect that no human could have predicted, affecting several companies — some known for their impressive engineering culture and considered internet pillars for their long-standing availability record.Our Google Cloud Technical Account Manager (TAM) notified us about the outage:We began to assess the impact on our Redpanda Cloud GCP customers, including whether we had received any support tickets. We noticed our monitoring was running in a degraded state. Despite self-hosting our observability data and stack, we still use a third-party provider for dashboarding and alerting needs. This provider was partially affected. We could still monitor metrics, but we were not getting alert notifications. We deemed the loss of alert notifications not critical since we were still able to assess the impact through other means, such as querying our self-managed metrics and logging stack.At this point, it was clear that multiple GCP services were experiencing a global outage, despite not having received support tickets from our customers or being paged by Redpanda Cloud alerts. So, in preparation for the worst, we preemptively created a low-severity incident to coordinate the response to multiple  incidents.We were notified by the vendor we use for managing cloud marketplaces that they were having issues. They were affected by the Cloudflare outage, which we later learned was connected to the GCP outage. Having this service degraded was not critical to us, so we put it on the waiting list.Google identified the triggering cause and applied mitigations. At this point, there was no evidence that Redpanda Cloud customers were being negatively impacted. We began receiving delayed alert notifications, mostly related to an increase in tiered storage errors, which is not Redpanda’s primary storage. We didn’t get high disk utilization alerts, which we typically receive when the tiered storage subsystem has been experiencing issues for an extended period (days). Additionally, as a reliability measure, we leave disk space unused and used-but-reclaimable (for caching), which we can reclaim if the situation warrants it. This outage was not that situation.We proactively started reaching out to customers with the highest tiered storage error rates to ensure we were not missing anything, and also to show our support, as is customary. We fully manage these BYOC clusters on behalf of our customers and have complete visibility — we know the answers to the questions, but we ask anyway. These are complex systems, after all.After closely monitoring our GCP fleet for some time, we considered the incident mitigated—with the severity unchanged (SEV4), and no evidence of negative customer impact. We noticed an increase in error rate for API calls against GCS, with minimal latency impact in some cases. However, hundreds of GCP clusters were up and healthy.Strengths that played in our favorAcknowledging the risk of hindsight bias, the following factors contributed to the GCP outage having no negative impact on our Redpanda Cloud GCP customers.Redpanda Cloud clusters do not externalize their metadata or any other critical services. All the services needed to write and read data, manage topics, ACLs, and other Kafka entities are co-located, with Redpanda core leading the way with its single-binary architecture. This follows a well-known architectural pattern aimed at reducing the impact radius of failures, which also improves security. We have taken this pattern further and made it a product principle. In contrast, other products boasting centralized metadata and a diskless architecture likely experienced the full weight of this global outage.Purposely designed for the SLA we offerAfter launching Redpanda Cloud, it took us two years to offer a 99.99% availability SLA. Responsibly offering 1 extra 9 of SLA takes a significant amount of investment and effort. Multi-AZ Redpanda Cloud clusters in GCP were designed to support an availability SLO of at least 99.999%. In practice, we observe even higher measurements. This is possible thanks to multiple factors:Redpanda Cloud clusters enforce a replication factor of at least 3 on all topics; customers cannot lower the replication factor, only increase it. Redpanda stores the primary data on local NVMe disks and sends older data to tiered storage, asynchronously.All Redpanda services are redundant: Kafka API, Schema Registry, and Kafka HTTP ProxyThere are no additional dependencies in the critical path other than the VPC, compute nodes, and their locally attached disks*We continuously chaos-test and load-test Redpanda Cloud tiers' configurationsWe have a strict release engineering process that tests and certifies Redpanda Cloud tiers for the throughput they advertise, in each cloud provider.As operations are issued, such as Redpanda or cloud infrastructure upgrades, we try to close our feedback control loops by watching Redpanda metrics as the phased rollout progresses and stopping when user-facing issues are detected.*Except when Private Service Connect (PSC) is enabled, in this case, the PSC becomes part of the critical path for reading and writing data to Redpanda.For cloud services such as Redpanda Cloud, which operates across the three major cloud providers and has numerous engineers continuously modifying the system, it is challenging to emerge unharmed from a global outage like this without some degree of fortune – although we learned later that one cluster was badly affected, keep on reading for the details.Redpanda’s location in our customers' technical stacksUnderstandably, GCP customers were experiencing significant internal chaos and struggling to assess the full impact when we reached out. For some of them, GCP's Pub/Sub served as the data source for their Redpanda BYOC clusters, so they needed to recover that first. While this meant Redpanda's operational status was less critical in those cases, it was still one less element for them to worry about.We didn’t lose nodes en masse during the incidentAs I was wrapping up this post, another incident had unfolded and was being mitigated. During its incident analysis, we found evidence that the GCP outage was a contributing factor in losing one node and having no replacement coming back. However, this event was isolated to and an uncommon interaction between internal infrastructure components of the cluster.Out of hundreds of clusters, we were lucky that only one cluster was affected. It took GCP around two hours to launch the replacement node, roughly the duration of the outage in , the region in which this cluster was located. Fortunately for the customer, the affected cluster was not a production but a staging cluster. Their production Redpanda cluster was unaffected. Observability infrastructureWe moved to a self-managed observability stack last year, primarily due to increased scale and cost, and were only using a third-party service for dashboarding and alerting needs. Had we kept our entire observability stack on that service, we would have lost all our fleet-wide log searching capabilities, forcing us to fail over to another vendor with exponentially bigger cost ramifications given our scale. In other words, this graph would have been filled with many more red bars and tears:As an industry, it seems we keep having to relearn hard lessons from the past. Not too long ago, we were all in awe at the global Crowdstrike outage, where similar controls were missing to enable safer global rollouts, affecting millions of Windows computers, and resulting in hundreds of millions of dollars in damages to their customers.With the resurgence of AI, systems will inevitably get even more complex. So, it seems valuable and timely to reconsider our current mindset, and I cannot think of anything better than a systems thinking mindset, especially when engineering our socio-technical systems, which should also result in increased adoption of control theory in our change management tools.Time will tell, perhaps all the above will be left to AI agents to control, perhaps not, for the foreseeable future, it seems we have no AI replacement, so we better hone our systems thinking skills.get started with Redpanda Cloud for free or get in touch for a demo. For any other questions, drop us a note in Slack. ]]></content:encoded></item><item><title>🧪 iapetus – A fast, pluggable open-source workflow engine for CI/CD and DevOps (written in Go)</title><link>https://www.reddit.com/r/kubernetes/comments/1lgyoza/iapetus_a_fast_pluggable_opensource_workflow/</link><author>/u/Outrageous-Income592</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 15:06:32 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Just open-sourced a project I’ve been working on:  🚀It’s a lightweight, developer-friendly workflow engine built for CI/CD, DevOps automation, and end-to-end testing. Think of it as a cross between a shell runner and a testing/assertion engine—without the usual YAML hell or vendor lock-in.Runs tasks in parallel with dependency awarenessSupports multiple backends (e.g., Bash, Docker, or your own plugin)Lets you assert outputs, exit codes, regex matches, JSON responses, and moreCan be defined in Integrates well into CI/CD pipelines or as a standalone automation layername: hello-world steps: - name: say-hello command: echo args: ["Hello, iapetus!"] raw_asserts: - output_contains: iapetus task := iapetus.NewTask("say-hello", 2*time.Second, nil). AddCommand("echo"). AddArgs("Hello, iapetus!"). AssertOutputContains("iapetus") workflow := iapetus.NewWorkflow("hello-world", zap.NewNop()). AddTask(*task) workflow.Run() Automate and test scripts with clear assertionsSpeed up CI runs with parallel task executionReplace brittle bash scripts or overkill CI configsIt's fully open source under the MIT license. Feedback, issues, and contributions are all welcome!Would love to hear thoughts or ideas on where it could go next. 🙌]]></content:encoded></item><item><title>Building a Redis clone from scratch</title><link>https://www.reddit.com/r/rust/comments/1lgyduy/building_a_redis_clone_from_scratch/</link><author>/u/ShowXw</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 14:52:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I figured the best way to actually  Rust was to build something real, so I decided to make a Redis-like database from scratch. It was a ton of fun and I learned a lot.I wrote up my whole journey and thought I'd share it here. In the post, I get into some of the tricky (but fun) parts, like:Setting up a concurrent TCP server with Tokio.Juggling shared data between async tasks with .Figuring out a simple way to save data to disk using a "dirty" flag.Let me know what you think! Happy to answer any questions about it.]]></content:encoded></item><item><title>Poor little buddy, Grok</title><link>https://www.reddit.com/r/artificial/comments/1lgyan3/poor_little_buddy_grok/</link><author>/u/Revolutionary_Rub_98</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 14:48:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Elon has plans for eliminating the truth telling streak outta little buddy grok]]></content:encoded></item><item><title>I made a frontend for the xsetwacom utility!</title><link>https://www.reddit.com/r/linux/comments/1lgxtiq/i_made_a_frontend_for_the_xsetwacom_utility/</link><author>/u/Neeyaki</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 14:26:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>File APIs need a non-blocking open and stat</title><link>https://bold-edit.com/devlog/week-12.html</link><author>/u/levodelellis</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 14:19:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[What happens if you call  on a file on a network that went down? Do you get A) an IO error? B) An error like EAGAIN to suggest the stat won't complete for many milliseconds C) stat blocks until the network times out which could be many minutesThe answer is C. With linux you can work around the problem by using io_uring, but on windows and mac you're out of luck. They have async IO, but neither of those OSes seem to have a non-blocking stat nor a non-blocking open. I'll need to use threads and have them idle most of the time.This week I implemented a directory iterator. This doesn't solve the problem above but it does allow me to have easier to read code. Second I upgraded the coverage script so I have more coverage options, specifically which coverage tool to run (more than one is an option.) A previous option I implemented was to build using headless or gui. This works with that so if I'm trying to raise coverage on one specific file I can use the fastest coverage tool and use a headless build that compiles and executes quicker. Third I implemented an async substring search. The search implementation itself I took from my original bold source which uses SIMD and is well tested. This week is the async implementation around that substring search. Fourth I compiled and fixed all my code on mac. I use linux on my desktop so I sometimes don't compile on mac for weeks.While writing the async substring code I didn't like how related functions were far away from each other. I have a worker queue that uses a callback to check how much a message 'cost' and tries to divide up the work across its queues. The 'cost' and 'process' callbacks have a giant switch statement. It was awkward that if I wanted to implement a small message I'd have to modify two large functions. I reworked it so I can use an interface. No more switch statements.]]></content:encoded></item><item><title>Unexpected security footguns in Go&apos;s parsers</title><link>https://blog.trailofbits.com/2025/06/17/unexpected-security-footguns-in-gos-parsers/</link><author>/u/TheSinnohScrolls</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 13:23:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[In Go applications, parsing untrusted data creates a dangerous attack surface that’s routinely exploited in the wild. During our security assessments, we’ve repeatedly exploited unexpected behaviors in Go’s JSON, XML, and YAML parsers to bypass authentication, circumvent authorization controls, and exfiltrate sensitive data from production systems.These aren’t theoretical issues—they’ve led to documented vulnerabilities like CVE-2020-16250 (a Hashicorp Vault authentication bypass found by Google’s Project Zero) and numerous high-impact findings in our client engagements.This post contextualizes these unexpected parser behaviors through three attack scenarios that every security engineer and Go developer should understand:(Un)Marshaling unexpected data: How Go parsers can expose data that developers intended to be private: How discrepancies between parsers enable attackers to bypass security controls when multiple services parse the same input: How parsers process cross-format payloads with surprising and exploitable resultsWe’ll demonstrate each attack scenario with real-world examples and conclude with concrete recommendations for configuring these parsers more securely, including strategies to compensate for security gaps in Go’s standard library.Below is a summary of the surprising behaviors we’ll examine, with indicators showing their security status:🟢 : Secure by default🟠 : Insecure by default but configurable🔴 : Insecure by default with no secure configuration optionsLet’s examine how Go parses JSON, XML, and YAML. Go’s standard library provides JSON and XML parsers but not a YAML parser, for which there are several third-party alternatives. For our analysis, we’ll focus on:We’ll use JSON in our following examples, but all three parsers have APIs equivalent to the ones we’ll see.At their core, these parsers provide two primary functions: (serialize): Converts Go structs into their respective format strings (deserialize): Converts format strings back into Go structsGo uses struct field tags to allow customization of how parsers should handle individual fields. These tags consist of:A  for serialization/deserializationOptional comma-separated directives that modify behavior (e.g., the  tag option tells the JSON serializer not to include the field in the JSON output string if it is empty)To unmarshal a JSON string into the  structure shown above, we must use the  key for the  field,  for the  field, and  for the  field.These parsers also offer stream-based alternatives that operate on  interfaces rather than  slices. This API is ideal for parsing streaming data such as HTTP request bodies, making it a preferred choice in HTTP request handling.Attack scenario 1: (Un)Marshaling unexpected dataSometimes, you need to limit which fields of a structure can be marshaled or unmarshaled.Let’s consider a simple example in which a back-end server has an HTTP handler for creating users and another for retrieving that user after authentication.When creating a user, you may not want the user to be able to set the  field (i.e., unmarshal that field from the user input).Similarly, when fetching the user, you may not want the user to return the user’s  or other secret values.How can we instruct the parsers not to marshal or unmarshal a field?Let’s first see what happens if you don’t set a JSON tag.In this case, you can unmarshal the  field with its name, as shown below.This is well documented, and most Go devs are aware of it. Let’s look at another example:Is it evident that the  field above would be unmarshaled? A less senior or distracted developer could assume it would not and introduce a security vulnerability.If you’d like to scan your codebase for this pattern, where some but not all fields have a JSON, XML, or YAML tag, you can use the following Semgrep rule. This rule is not on the our collection of rules exposed on the Semgrep registry because, depending on the codebase, it is likely to produce many false positives.To tell the parser not to (un)marshal a specific field, we must add the special  JSON tag!Oh, whoops, we were still able to set the  field. We copy-pasted the  part by mistake, which caused the parser to look for the  key in the provided JSON input. I searched for this pattern on the top 1,000 Go repositories by stars on GitHub and, among a few others, I found and reported these two results, which are now fixed:As a special case, if the field tag is “-”, the field is always omitted. Note that a field with name “-” can still be generated using the tag “-,”.The XML and YAML parsers operate similarly, with one key difference: the XML parser treats the  tag as invalid. To resolve this, we must prefix the  symbol with an XML namespace, such as .Ok, ok, let’s do it right this time.Finally! Now, there is no way for the  field to be unmarshaled.But I hear you ask: How can these misconfigurations lead to security vulnerabilities? The most common way is, like in our example, using  as the JSON tag for a field such as –a field the user should not control. This is a hard bug to detect with unit tests because unless you have an explicit test that unmarshals an input with the  key and detects if any field was written to, you won’t detect it. You need your IDE or an external tool to detect it.We created a public Semgrep rule to help you find similar issues in your codebases. Try it with semgrep -c r/trailofbits.go.unmarshal-tag-is-dash!Another very simple misconfiguration we’ve found before was a developer mistakenly setting the field name to .If you set the JSON tag to , the parser will use  as the field’s name (as expected). Of course, some developers have tried to use this to set the  option in the field while keeping the default name. I searched the top 1,000 Go repositories for this pattern and found these results:In these cases, the developer often wanted to set the tag to , which would keep the default name, and add the  tag option.Contrary to the previous example, this one is unlikely to have a security impact and should be easy to detect with tests because any attempt to serialize or deserialize input with the expected field name will fail. However, as we can see, it still shows up even in popular open-source repositories. We created a public Semgrep rule to help you find similar issues in your codebases. Try it with semgrep -c r/trailofbits.go.unmarshal-tag-is-omitempty!Attack scenario 2: Parser differentialsWhat can happen if you parse the same input with different JSON parsers and they disagree on the result? More specifically, which behaviors in Go parsers allow attackers to trigger these discrepancies “reliably”?As an example, let’s use the following application using a microservice architecture with:A  that receives all user requestsAn  called by the Proxy Service to determine if the user has sufficient permission to complete their requestMultiple  called by the Proxy Service to perform the business logicIn this first flow, a regular, non-admin user attempts to perform a , an action they are  to perform.In this second flow, the same regular user attempts to perform an , an action they are  to perform.Finally, the following flow is because the services disagree on the action the user is trying to perform.The Authorization Service, written in a different programming language or using a non-default Go parser, will parse  and grant the user permission to perform the operation, while the Proxy Service, using Go’s default parser, will parse  and proxy it to the incorrect service. The remaining question is: Which payloads can we use to achieve this behavior?This is a common architecture we’ve seen multiple times during our audits, and against which we’ve found authentication bypasses because of the problems we’ll describe below. Other examples exist, but most follow the same pattern: the component that does security checks and the component that performs the actions differ in their view of the input data. Here are some of those examples in a variety of scenarios:The first differential attack vector we’ll explore is duplicate keys. What happens when your JSON input has the same key twice? It depends on the parser!In Go, the JSON parser will always . There is no way to prevent this behavior.This is the default behavior of most parsers. However, as shown in the JSON interoperability vulnerabilities blog post from Bishop Fox, seven out of the 49 parsers tested take the first key:None of these are the most common JSON parsers in their corresponding languages, even though some are common alternatives.So, if our Proxy Service uses the Go JSON parser and the Authorization Service uses one of these parsers, we get our discrepancy, as shown in the figure below.The XML parser has the same behavior, while the YAML parser returns an error on duplicate fields—the secure default we think all of these parsers should implement.While not ideal, at least this behavior is consistent with the most commonly used JSON and XML parsers. Let’s now take a look at a much worse behavior that will almost always get you a discrepancy between Go’s default parser and any other parser.Case insensitive key matchingGo’s JSON parser parses field names case-insensitively. Whether you write action , , or , the parser treats them as identical!This is documented but is very unintuitive, there’s no way to disable it, and almost no other parser has this behavior.To make this worse, as we saw above, you can have duplicate fields, and the latter one is still chosen, eVeN wHeN tHe cAsInG dOeS nOt mAtCh.This is against the documentation, which says:“To unmarshal JSON into a struct, Unmarshal matches incoming object keys to the keys used by Marshal (either the struct field name or its tag), preferring an exact match but also accepting a case-insensitive match.”You can even use Unicode characters! In the example below, we’re using  (the unicode character named Latin small letter long s) as an , and  (the unicode character for the Kelvin sign) as a . From our testing of the JSON library code that does the comparison, only these two unicode characters match ASCII characters.Applying it to our running attack scenario, this is how the attack would look like:In our opinion, this is the most critical pitfall of Go’s JSON parser because it differs from the default parsers for JavaScript, Python, Rust, Ruby, Java, and all other parsers we tested. This has led to many high-impact security vulnerabilities, including ones we’ve found during our audits.This only affects the JSON parser. The XML and YAML parsers use exact matches.If you are interested in other kinds of JSON parsing differentials between many parsers, we recommend these two blog posts:For the final attack scenario, let’s see what happens if you parse a JSON file with the XML parser or use any other format with the incorrect parser.As an example, let’s use CVE-2020-16250, an Hashicorp Vault bypass in its AWS IAM authentication method. This bug was found by Google’s Project Zero team, and a detailed analysis can be found in their “Enter the Vault: Authentication Issues in HashiCorp Vault” blog post if you are interested. We won’t go through all the details in this post, but in summary, this is how the normal Hashicorp Vault AWS IAM authentication flow works:The AWS resource sends it to the Vault Server.The Vault Server builds that requests and sends it to the AWS Security Token Service (STS).AWS STS verifies the signature.On success, AWS STS returns the associated role’s identity in an XML document.The Vault Server parses the XML, extracts the identity, and, if that AWS role should have access to the requested secrets, it returns them.The AWS resource can now use the secret to, for example, authenticate against a database.What Google’s Project Zero team found was that an attacker could control too much in step 2, including controlling all headers of the request that Vault builds in step 3. In particular, by setting the  header to , AWS STS would now return a JSON document in step 5 instead of the expected XML document. As a result, the Vault Server would parse a JSON document with Go’s XML parser. Because the XML parser is very lenient and parses anything that looks like XML in between lots of other “garbage” data, this was sufficient for a full authentication bypass when combined with partial control of the JSON response.Let’s look at three different behaviors that make parsing files with the wrong Go parser possible and build a polyglot that can be parsed with Go’s JSON, XML, and YAML parsers and return a different result for each.By default, the JSON, XML, and YAML parsers don’t prevent unknown fields—properties in the incoming data that don’t match any fields in the target struct.Of the three parsers, only the XML parser accepts leading garbage data.Again, only the XML parser accepts arbitrary trailing garbage data.The exception is using the parsers’ Decoder API with streaming data, in which case the JSON parser accepts garbage trailing data. This an open issue for which a fix is not planned.How can we combine all the behaviors we’ve seen so far that build a polyglot that:Can be parsed by Go’s JSON, XML, and YAML parsersReturns a different result for eachA very useful piece of information is that JSON is a subset of YAML:Every JSON file is also a valid YAML fileWith this in mind, we can build the following polyglot:The JSON parser can parse the polyglot because the input is valid JSON, it ignores unknown keys, and it allows duplicate keys. It takes the  value because its field matching is case-insensitive and it takes the value of the last match.The YAML parser can parse the polyglot because the input is valid JSON (and every JSON file is also a valid YAML file), and it ignores unknown keys. It takes the  value because, contrary to the JSON parser, it does exact field name matches.Finally, the XML parser can parse the polyglot because it ignores all surrounding data and just looks for XML-looking data, which, in this polyglot, we hid in a JSON value. As a result, it takes .The polyglot we’ve constructed is a powerful starting payload when exploiting these data format confusion attacks similar to the HashiCorp Vault bypass we explored above (CVE-2020-16250).How can we minimize these risks and make JSON parsing more strict? We’d like to:Prevent parsing of  in JSON, XML, and YAMLPrevent parsing of  in JSON and XMLPrevent case insensitive key matches in JSON (this one is especially important!)Prevent  in XMLPrevent  in JSON and XMLUnfortunately, JSON only offers one option to make its parsing stricter: . As the name implies, this option disallows unknown fields in the input JSON. YAML supports the same functionality with the  function, and while there was a proposal to implement the same for XML, it was rejected.To prevent the remaining insecure defaults, we must create a custom “hacky” solution. The next code block shows the  function, an attempt to make JSON parsing stricter, which has several limitations:: It requires parsing JSON input twice, making it significantly slower.: Some edge cases remain undetected, as detailed in the function comments.: Since these security measures aren’t built into libraries as secure defaults or configurable options, widespread adoption is unlikely.Still, if you detect a vulnerability in your codebase, perhaps this imperfect solution can help you plug a hole while you find a more permanent solution.To be widely adopted and solve the problem at a large scale, this functionality needs to be implemented at the library level and enabled by default. This is where JSON v2 comes in. It is currently only a proposal, but a lot of work has gone into it already, and it will hopefully be released soon. It improves on JSON v1 in many ways, including:Disallowing duplicate names: “(…) in v2 a JSON object with duplicate names results in an error. The jsontext.AllowDuplicateNames option controls this behavior difference.”Doing case-sensitive matching: “(…) v2 matches fields using an exact, case-sensitive match. The MatchCaseInsensitiveNames and jsonv1.MatchCaseSensitiveDelimiter options control this behavior difference.”It includes a  option, even though it is not enable by default (equivalent to ).It includes a  function to process data from an , verifying that an EOF is found, disallowing trailing garbage data.While this proposal addresses many of the issues discussed in this blog post, these challenges will persist within the Go ecosystem as widespread adoption takes time. The proposal needs formal acceptance, after which developers must integrate it into all existing JSON-parsing Go code. Until then, these vulnerabilities will continue to pose risks.Key takeaways for developersImplement strict parsing by default. Use  for JSON,  for YAML. Unfortunately, this is all you can do directly with the Go parser APIs.Maintain consistency across boundaries. When input in processed in multiple services, ensure consistent parsing behavior by always using the same parser or implement additional validation layers, such as the  function shown above.. Keep an eye on the development of Go’s JSON v2 library, which addresses many of these issues with safer defaults for JSON.. Use the Semgrep rules we’ve provided to detect a few vulnerable patterns in your codebase, particularly the misuse of the  tag and  fields. Try them with semgrep -c r/trailofbits.go.unmarshal-tag-is-dash and semgrep -c r/trailofbits.go.unmarshal-tag-is-omitempty!While we’ve provided mitigations and detection strategies, the long-term solution requires fundamental changes to how these parsers operate. Until parser libraries adopt secure defaults, developers must remain vigilant.]]></content:encoded></item><item><title>This Week in Plasma: Plasma 6.4 has arrived!</title><link>https://blogs.kde.org/2025/06/21/this-week-in-plasma-plasma-6.4-has-arrived/</link><author>/u/diegodamohill</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 13:06:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Welcome to a new issue of Every week we cover the highlights of what’s happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.This week we released Plasma 6.4! And so far it’s been getting a really positive reception. The bug reports bear this out; most of the real actual bugs reported against 6.4.0 are either pre-existing issues or minor regressions, many of which we’ve already fixed in time for 6.4.1 coming next Tuesday.Discover’s list views are now properly navigable with the keyboard. (Christoph Wolk, link)Improved the text readability in some of the list items in KRunner and Discover when the list items are pressed or clicked. (Nate Graham, link 1 and link 2)Hovering over list items on System Settings’ User Feedback page no longer makes inscrutable icons appear. (Nate Graham, link)Improved the readability of graph axis labels throughout Plasma so they meet the WCAG AA standard. (Nate Graham, link)Plasma’s Activity manager service now only stores the last 4 months’ worth of history by default, rather than storing all history ever and never pruning it. Setting a limit here makes the data more relevant and prevents performance problems caused by endlessly-growing databases. (Nate Graham, link)Made further UI improvements to the Emoji Selector app: now the window is never so small that the sidebar list becomes scrollable, and the button to expand and collapse the sidebar is located on the header, rather than inline. (Oliver Beard, link)Removed the vertical line between the date and time on horizontal arrangements of the Digital Clock widget, since it proved unpopular, and people who want it can get it themselves by using a custom date format anyway. (Owen Ross, link)On System Settings’ Shortcuts page, the “Add New” button is now located on the top toolbar rather than taking up unnecessary space above the list view. (Jakob Petsovits, link)Reduced the minimum size of Custom Tiling tiles, so that you can have smaller ones on particularly large screens like ultra-wides. (Tyler Slabinski, link)The Networks widget’s captive portal banner now uses the inline/header styling, reducing the frames-within-frames effect. (Kai Uwe Broulik, link)Removed the NOAA Weather Picture Of The Day wallpaper plugin, because unfortunately the source data changed in a way that makes it no longer consistently suitable for being displayed on the desktop. (Kat Pavlů, link)Fixed a bug that could sometimes cause keyboard shortcuts to get lost on certain distros when performing system upgrades. (Vlad Zahorodnii, link)Fixed a regression that caused KRunner’s faded completion text to sometimes overflow from the window. (Nate Graham, link)Fixed a small visual regression in KWin’s “Slide Back” effect. (Blazer Silving, link)Fixed several issues in the Folder View widget that caused selecting or opening items to not work when using certain non-default view settings, or when the view was scrollable, or when using a touchscreen. (Christoph Wolk, link)Fixed a bug in the + clipboard popup that made it sometimes fail to pre-select the top-most item. (Akseli Lahtinen, link)The Clipboard settings window’s shortcuts page no longer shows columns for local shortcuts that you can confusingly set and have them do nothing, because the clipboard is global in scope. (Akseli Lahtinen, link)Fixed the Earth Science Picture of The Day wallpaper plugin after the source data changed its formatting again. (Kat Pavlů, link)Made a few fixes to the “Missing Backends” section of Discover’s settings window that prevented it from working quite right. (Carl Schwan, link)Fixed a bug that prevented direct scan-out (and its attendant performance benefits) from activating on rotated screens. (Vlad Zahorodnii, link)Fixed a bug that could cause the system to lock or suspend more quickly than intended after an app that was blocking those activities stops doing so. (Akseli Lahtinen. link)Installing a new wallpaper plugin no longer causes the plugin list combobox to become blank. (Nate Graham, link)Fixed a regression that caused System Settings’ sidebar list items to display hover tooltips when they weren’t needed. (Nate Graham, link)KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.You can help KDE by becoming an active community member and getting involved somehow. Each contributor makes a huge difference in KDE — you are not a number or a cog in a machine!You don’t have to be a programmer, either. Many other opportunities exist:You can also help us by making a donation! Any monetary contribution — however small — will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.Enter your email address to follow this blog and receive notifications of new posts by email.]]></content:encoded></item><item><title>Happy 20th birthday to MySQL&apos;s &quot;Triggers not executed following FK updates/deletes&quot; bug!</title><link>https://bugs.mysql.com/bug.php?id=11472</link><author>/u/balukin</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 12:30:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My first Go module</title><link>https://www.reddit.com/r/golang/comments/1lgv5by/my_first_go_module/</link><author>/u/Ok_Gold_8124</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 12:14:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi everyone, I'm a newbie in programming. I'm really interested in software development. I've been learning about programming using Go as the tool. Recently I'm trying to play and reinventing the wheel about middleware chaining. Today I just pushed my repo to github.This is the link to my project: Checkpoint I would be very thankful for every feedback, please check it and leave some suggestion, critics, or any feedback.Also please suggest me what kind of project should I working on next to be my portofolios. Thank you everyone.]]></content:encoded></item><item><title>[P] Qwen3 implemented from scratch in PyTorch</title><link>https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3</link><author>/u/seraschka</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 11:47:08 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scaling My Kubernetes Lab: Proxmox, Terraform &amp; Ansible - Need Advice!</title><link>https://www.reddit.com/r/kubernetes/comments/1lgtova/scaling_my_kubernetes_lab_proxmox_terraform/</link><author>/u/rached2023</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 10:46:54 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I've built a pretty cool Kubernetes cluster lab setup: 3 masters, 2 workers, HA configured with Ansible config. 6 VMs running on KVM/QEMU. Integrated with Falco, Grafana, Prometheus, Trivy, and more.The problem? I've run out of disk space! My current PC only has one slot, so I'm forced to get a new, larger drive.This means I'm considering rebuilding the entire environment from scratch on Proxmox, using Terraform for VM creation and Ansible for configuration. What do you guys think of this plan?Here's where I need your collective wisdom: Roughly how much time do you think it would take to recreate this whole setup, considering I'll be using Terraform for VMs and Ansible for Kubernetes config? What are your recommendations for memory and disk space for each VM (masters and workers) to ensure good performance for a lab environment like this?Any other tips, best practices, or "gotchas" I should be aware of when moving to Proxmox/Terraform for this kind of K8s lab?Thanks in advance for your insights!]]></content:encoded></item><item><title>Writing a basic Linux device driver when you know nothing about Linux drivers or USB</title><link>https://crescentro.se/posts/writing-drivers/</link><author>/u/i542</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 10:23:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[A couple of months ago I bought the Nanoleaf Pegboard Desk Dock, the latest and greatest in USB-hub-with-RGB-LEDs-and-hooks-for-gadgets technology. This invention unfortunately only supports the  operating systems of Windows and macOS, which necessitated the development of a Linux driver.Over the past few posts I’ve set up a Windows VM with USB passthrough, and attempted to reverse-engineer the official drivers,  As I was doing that, I also thought I’d message the vendor and ask them if they could share any specifications or docs regarding their protocol. To my surprise, Nanoleaf tech support responded to me within 4 hours, with a full description of the protocol that’s used both by the Desk Dock as well as their RGB strips. The docs mostly confirmed what I had already discovered independently, but there were a couple of other minor features as well (like power and brightness management) that I did not know about, which was helpful.Today, we’re going to take a crack at writing a driver based on the (reverse-engineered) protocol, while also keeping the official documentation at hand. One small problem, though: I’ve never written a Linux device driver before, nor interacted with any USB device as anything else but a user.Most Linux distros ship with , a simple utility that will enumerate all USB devices connected to the system. Since I had no clue where to start from, I figured I might as well run this to see if the device appears in the listing.Well, good news, it’s definitely there. But, how can the kernel know that what I have plugged in is the “Nanoleaf Pegboard Desk Dock”? The kernel (presumably) has no knowledge of this device’s existence, yet the second I plug it in to my computer it receives power, turns on and gets identified by the kernel.As it turns out, we actually already have a driver! It’s just a very stupid one. If we run  in verbose mode and request the information just for this specific device, we will get a lot more details about it:This is a  of information, so we need to take a quick USB class.The USB spec is long, complicated and mainly aimed at low-level implementations (think kernel developers, device vendors, and so on). You can, of course, still read it if you enjoy being bored. But, thankfully, a kind soul collected the good parts into USB in a NutShell.To summarize the summary, a USB device can have multiple , which usually explain the power requirements for the device. Most devices will have just one.Each of those configurations can have multiple . So for example, a camera might serve as a file storage device as well as a webcam.Finally, each interface can have multiple , whcih describe how the data is transferred. Perhaps the camera has an “isochronous” (continuous) transfer for a webcam feed, and a “bulk” transfer for moving image files over.Going back to our device, we can see that it exposes one interface, which is a . HIDs are a class of USB devices that covers things like keyboards, mice or gamepads, and each of those categories is a separate . The kernel contains a generic driver for USB HIDs - here it is in all of its C glory.This is why the kernel developers do not need to write specific drivers for each individual keyboard and mouse on the market. Vendors will label their device with one of the well-known HID sub-classes, then use a common protocol to implement the functionality.Unfortunately there’s no HID specification for an RGB LED… thing (well, there’s an “LED” specification, but it’s mainly for things like status LEDs, not color LEDs) so our device is just a plain old generic HID with an interface sub-class of . This means that the kernel recognizes it and powers it correctly, but it doesn’t really know what to do with it, so it just lets it sit there.There are two options that we have at this point:We could write a kernel driver that follows the kernel standard and exposes each individual LED as 3 devices (one per color) under . Interacting with the kernel devs sounds scary (yes I realize I’m a grown-ass adult man), but even if it wasn’t, I question the utility of trying to merge drivers for a very niche product into the kernel. Also,  feels like it’s intended for status LEDs and not  anyway.We could write a userspace driver through libusb, thus defining our own way of controlling LEDs and reducing the quality bar from “Linus Torvalds might send you a strongly worded letter if you fuck up” to “fuck it, we ball”.Given that I have no idea what I am doing, I’m gonna go for option 2, but if one of you brave souls goes for option 1, please let me know and I will print out a photo of you and frame it on my wall.To do anything fun on Linux, you need to be . This is also the case when talking to USB devices. You could always run your drivers as , thus sidestepping the problem. But we all know that’s bad form. And if I am to distribute this driver, most people would expect to run it without privilege escalation.Linux generally relies on  to manage handlers for hardware events. I will spare you the long story this time and just give you the magic incantation: to make your device accessible to users, you need to create a file at  /etc/udev/rules.d/70-pegboard.rules with the following contents:where  and  are the vendor and product IDs you got from , and  is the spell that grants the currently active user permissions to manage the device. Then, unplug your device and plug it back in.Okay, enough yapping. Let’s start with a basic Rust binary and immediately add the  crate, which will serve as a binding to .To get going, we can try to get a handle on the device and get basic information about it, just like . This is explained pretty well in the crate readme, so I will not dwell on it too much. We’ll need a , which gives us a handy  method that we can use to get a handle to a device.Now that we have access to the device, we want to write a simple payload to it. For that, we first need to claim an interface. Recall that interfaces are essentially capabilities of the device, and through  we learned that we only have one interface with the ID () of . Thankfully, there’s an obvious  method on a .So, what you just experienced is the joy of  error messages. This message, at 4 characters, is in fact pretty generous - you might be greeted with a message that only says , and good luck debugging that. In general,  means that something is already holding the device open, so you cannot do anything with it. However, you won’t actually be told what is holding it open.The secret is that the device is, of course, being held open by the kernel. This is the generic driver I talked about earlier. And the secret solution is to release the kernel driver, if it is currently active on the device.This requires you to have write access to the device, so if you did not do the  song and dance from earlier in this article, prepare to prefix all future invocations of your driver with .Note that the kernel driver won’t be reattached automatically, so you might want to call device.attach_kernel_driver(INTERFACE) if, for some reason, you need it back.Sending data to the deviceSurely,  we are ready to write out some bytes to a device?Well, almost! If we try to naively start typing out something like , the IDE will helpfully suggest three options: ,  and . This corresponds to three out of four possible types of endpoints that the USB standard supports. Once again, USB in a NutShell comes in clutch with an explanation of what each of the endpoint types mean. Thankfully, we can mostly skip over the implementation details, as we can once again refer to the  readout from earlier:In USB parlance,  is always something that the device sends to the host, and  is always something that the host sends to the device. Basically, since this interface has two endpoints, and only one of them is an  endpoint, it’s safe to assume we’re looking to  on endpoint . The peculiarities of Interrupt endpoints will absolutely come back to bite us in a couple of minutes, but for now we can keep them out of sight and out of mind.For testing purposes, I want to make the pegboard show a solid red color. According to my earlier investigation, this means that I need to send , followed by 64 repeats of , to an endpoint at . In addition,  only exposes the blocking API of , so we will also need to define a timeout after which  will give up and error out.And… just like that, the pegboard now shows a solid red color! We didn’t need to worry about manually splitting packets or any of the underlying implementation, just open up a pipe and write to it! It’s that easy.Let’s run it again to make sure it was not a fluke!So, about those interrupts…Yeah, so if you happen to be following along, and you ran the same binary twice, you’ll notice that the firmware of the pegboard crashes unceremoniously, and shortly after reverts to its default animation. And if I go back to the original packet capture - or the official docs - it’s pretty obvious why: the device sends us back a response, but we never read it.It turns out that “interrupts” are named as such for a reason, and we should probably handle them as they come in. However, the USB spec defines that the  must poll for interrupts. A device cannot interrupt the host by itself.For our simple “driver”, this means we want to poll the device right after we write to it. Thankfully,  gives us a  method, and we have already sneakily defined the  constant. Let’s do just that:Running this, we see that the contents of  are , which corresponds to  I got from the research. And since we clear the interrupt buffer every time now, we can run this binary many times to define a single solid color on the device. Neat!Of course, this is… not really what you want. The device may issue more interrupts. For example, there’s a single button on the desk dock, which can be clicked, double-clicked or long-clicked, and each of those will issue a different interrupt. So what we  want is a background task of sort that will actively poll the device for interrupts and process them as they come in.This is where you can get wild with async Rust, , channels, and other fun stuff. That would certainly be the  to do it in an actual, serious driver. But to avoid getting into complexities of async Rust, let’s keep it vanilla and use .We’ll also adjust the timeout for reading interrupts to be 1 millisecond, as requested by the device (the  value in the  readout). This doesn’t mean we will get an interrupt every millisecond, just that the device  send one at that rate. If the device sends nothing (i.e., we get  ), we will just continue with the loop.Put together, that might look something like this:This… works! Of course, we send no more color frames to the device, so we won’t get any more interrupts, but we now have two threads, one which we can use to change the colors shown, and another which we can use to read the interrupts.There are some quirks with this device: it seems to require a steady stream of color frames, otherwise it reverts to “offline mode” as it does not receive any new frames from the host, and the first frame’s brightness is significantly lower than the brightness of future frames. Not to mention that, despite what the official protocol documentation would have you believe, the colors seem to be in GRB instead of RGB format, and if you make the device , it will just hard-reset after a couple of seconds. That is, I suppose, a part of the joy of coding.But this small proof of concept shows that writing simple device drivers is not all that hard, and that 50 lines of code can bring you quite far. Over the next few weeks I hope to polish up my proof of concept, make a small GUI for it, pack it up and share it with the two other Linux users who own this dumb thing. And I’m happy to have learned the basics of reverse-engineering a simple USB device driver, and using that as a foundation for writing my own. Even if I could have just asked for the spec earlier and not fussed with it.]]></content:encoded></item><item><title>Please review my project (a simple Todo App)</title><link>https://github.com/Ashind-byte/Task_Manager</link><author>/u/Loud_Staff5065</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 09:28:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Please dont hate me for using an ORM(spolier). I wanted to get better at folder structure,naming conventions and other code refactoring. Suggestions needed]]></content:encoded></item><item><title>Longhorn starts before coredns</title><link>https://www.reddit.com/r/kubernetes/comments/1lgrzlc/longhorn_starts_before_coredns/</link><author>/u/G4rp</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 08:49:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a two-node k3s cluster for home lab/learning purposes that I shut down and start up as needed.Despite developing a complex shutdown/startup logic to avoid PVC corruption, I am still facing significant challenges when starting the cluster.I recently discovered that Longhorn takes a long time to start because it starts before coredns is ready, which causes a lot of CrashLoopBackOff errors and delays the start-up of Longhorn.Has anyone else faced this issue and found a way to fix it?]]></content:encoded></item><item><title>🐙 Tako – Yet another Async Web Framework in Rust (Early Phase – Feedback Welcome)</title><link>https://www.reddit.com/r/rust/comments/1lgrjrf/tako_yet_another_async_web_framework_in_rust/</link><author>/u/danielboros90</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 08:18:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I needed a new challenge, so I built  — a lightweight, async web framework in Rust.The idea came from wanting something routing-focused and ergonomic, without too much magic. Axum was a big inspiration, but I wanted to go a different way — keep things explicit, composable, and easy to reason about.basic routing with  / extractors for headers, path/query/bodymiddleware (sync + async)I'd love to hear your thoughts:What would  expect from a minimal async web framework in Rust?What features feel essential? What could be left out?Where do you feel other frameworks overcomplicate things?Thanks in advance for any feedback, ideas, or just a quick glance. My goal is to make Tako a useful, open-source crate for people eventually]]></content:encoded></item><item><title>Apple sued by shareholders for allegedly overstating AI progress</title><link>https://www.reuters.com/sustainability/boards-policy-regulation/apple-sued-by-shareholders-over-ai-disclosures-2025-06-20/</link><author>/u/F0urLeafCl0ver</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 08:03:42 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI Models score ZERO on hard category problems on LiveCodeBench Pro..</title><link>https://analyticsindiamag.com/global-tech/ai-models-from-google-openai-anthropic-solve-0-of-hard-coding-problems/</link><author>/u/Ok-Elevator5091</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 07:54:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[If you’ve heard the phrase ‘coding is dead’ for a mind-numbingly high number of times, take a deep breath and pause. A new benchmark from researchers across notable universities in the United States and Canada has sparked a twist in the tale. It turns out that AI is far from solving some of the most complex coding problems today. A study by New York University, Princeton University, the University of California, San Diego, McGill University, and others indicates a significant gap between the coding capabilities of present-day LLMs and elite human intelligence. LLMs Struggle to Use Novel Insights for Problem SolvingThe researchers began by stating the shortcomings of the benchmarks available today. For instance, the LiveCodeBench evaluation suffers from “inconsistent environments, weak test cases vulnerable to false positives, unbalanced difficulty distributions, and the inability to isolate the effects of search contamination”. They added that other benchmarks, like SWE-Bench, test the models on code maintenance rather than algorithmic design. Other benchmarks, like CodeELO, do introduce competitive programming problems. Still, their reliance on static and archaic issues makes it difficult to check if models are retrieving solutions based on reasoning or memory. To alleviate such concerns, the researchers introduced LiveCodeBench Pro, an evaluation benchmark for coding designed to avoid data contamination. The models were evaluated with 584 problems sourced directly from ‘world-class contests’ before solutions or discussions were available. Additionally, a team of Olympiad medalists annotates each problem in the benchmark to categorise it based on its difficulty level and nature—whether it is knowledge-heavy, observation-heavy, or logic-heavy. Sadly, none of these models solved a single problem in the ‘Hard’ category. Even the best and latest models from OpenAI, Google, Anthropic, and others that were evaluated scored 0%. In the ‘Medium’ difficulty category, OpenAI’s o4-mini-high model scored the highest at 53.5%. AI models performed better on knowledge-heavy problems—ones that can be solved by stitching well-known templates, as the requisite problem-solving patterns appear ‘verbatim in training data’. Even on logic-heavy problems, which require a patterned way of thinking, these models performed well. However, they performed poorly on observation-heavy problems, whose solutions hinge on the discovery of novel insights — “something that cannot be retrieved from memorised snippets alone”. When these researchers diagnosed the failure modes of these models, the largest one was where these models committed errors regarding the algorithms. “These are genuine conceptual slips, instead of surface bugs,” said the authors. “LLMs frequently fail even on provided sample inputs, suggesting incomplete utilisation of given information and indicating room for improvement even in simple settings,” added the authors. They also said that these models show a substantial improvement in overall performance with multiple attempts to solve the problems. They concluded that these models solve problems involving the implementation of techniques, frameworks, and patterns but struggle to solve ones involving complex reasoning, nuances, and edge cases. “Despite claims of surpassing elite humans, a significant gap still remains, particularly in areas demanding novel insights,” they added. For detailed information, comparisons, scores, and evaluation mechanisms, check out the technical report’s PDF here. This, however, is one of the many reports that highlight the shortcomings of AI-enabled coding, despite the optimism expressed by several tech leaders worldwide. You Can’t Code for a Long Time With AIRecently, an Oxford researcher, Toby Ord, proposed that AI agents might have a “half-life” when performing a task. This was in relation to another research from METR (Model Evaluation & Threat Research), which showed that the capacity of AI agents to handle longer tasks doubled every seven months. They measured that the doubling time for an 80% success rate is 213 days, and for 50%, it is 212 days, establishing consistency in their findings. When Ord analysed the research, he discovered that, just like radioactive decay, the AI agent’s success rate followed an exponential decline. For instance, if an AI model could complete a one-hour task with 50% success, it only had a 25% chance of successfully completing a two-hour task. This indicates that for 99% reliability, task duration must be reduced by a factor of 70. However, Ord observed a time gap between the 50% success rate time horizon and the 80% success rate time horizon.“For the best model (Claude 3.7 Sonnet), it could achieve a 50% success rate on tasks up to 59 minutes vs only 15 minutes if an 80% success rate was required,” said Ord.“If those results generalise to the other models, then we could also see it like this: the task length for an 80% success rate is 1/4 the task length for a 50% success rate. Or in terms of improvement: what is doable with a 50% success rate now is doable with an 80% success rate in 14 months’ time (= 2 doubling times),” he added. Although METR indicates that AI agents can tackle longer tasks every 7 months, Ord’s analysis shows that high-reliability performance still demands significantly shorter task durations. This means the timeline for AI to handle complex coding projects remains unclear, despite steady improvements in capability.]]></content:encoded></item><item><title>BBC threatens legal action against AI startup over content scraping</title><link>https://www.theguardian.com/media/2025/jun/20/bbc-threatens-legal-action-against-ai-startup-over-content-scraping</link><author>/u/F0urLeafCl0ver</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 07:49:44 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[The BBC is threatening legal action against Perplexity AI, in the corporation’s first move to protect its content from being scraped without permission to build artificial intelligence technology.The corporation has sent a letter to Aravind Srinivas, the chief executive of the San Francisco-based startup, saying it has gathered evidence that Perplexity’s model was “trained using BBC content”.The letter, first reported by the Financial Times, threatens an injunction against Perplexity unless it stops scraping all BBC content to train its AI models, and deletes any copies of the broadcaster’s material it holds unless it provides “a proposal for financial compensation”.“If we currently drift in the way we are doing now we will be in crisis,” Davie said, speaking at the Enders conference. “We need to make quick decisions now around areas like … protection of IP. We need to protect our national intellectual property, that is where the value is. What do I need? IP protection; come on, let’s get on with it.”The industry would like an opt-in regime, forcing AI companies to seek permission and strike licensing deals with copyright holders before they can use the content to train their models.In October, Rupert Murdoch’s Dow Jones, the owner of the Wall Street Journal, filed a lawsuit against Perplexity, accusing it of engaging in a “massive amount of illegal copying” in a “brazen scheme … free-riding on the valuable content the publishers produce”.Perplexity told the FT that the BBC’s claims were “manipulative and opportunistic” and that it had a “fundamental misunderstanding of technology, the internet and intellectual property law”.Perplexity does not build or train foundation models – unlike other companies such as OpenAI, Google and Meta – but provides an interface that allows users to choose between them.The BBC said that parts of its content had been reproduced verbatim by Perplexity.“Perplexity’s tool directly competes with the BBC’s own services, circumventing the need for users to access those services,” the corporation said.In October, the BBC began registering copyright in its news website in the US, so it is entitled to “statutory damages in relation to unauthorised use of these copyright works”.In the UK, original proposals published in a consultation indicated that the government could let AI companies scrape content unless media owners opt out, which the industry said would “scrape the value” out of the £125bn creative industry.skip past newsletter promotionafter newsletter promotionLisa Nandy, the culture secretary, has since said that the government has no preferred option regarding AI copyright laws in the UK but promised the creative sector that it would not be harmed by legislation.“We are a Labour government, and the principle [that] people must be paid for their work is foundational,” she told a media conference earlier this month. “You have our word that if it doesn’t work for the creative industries, it will not work for us.”Publishers including the Financial Times, Axel Springer, Hearst and News Corporation have signed content licensing deals with OpenAI.Reuters has struck a deal with Meta, and the parent of the Daily Mail has an agreement with ProRata.ai.The Guardian has approached Perplexity for comment. The BBC declined to comment beyond the contents of the letter.]]></content:encoded></item><item><title>Need help in Helm charts for Drools WB and Kie-Server</title><link>https://www.reddit.com/r/kubernetes/comments/1lgq8rf/need_help_in_helm_charts_for_drools_wb_and/</link><author>/u/deep_2k</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 06:50:14 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have been trying to run Drools Workbench ( Business Central ) and KIE Server in a conected fashion to work as a BRE. Using the docker images of the "showcase" versions was smooth sailing, but facing a major road blocker trying to get it working on Kubernetes using Helm Charts. Have been able to set up the Drools Workbench ( Business Central ), but cannot figure out why the KIE-Server is not linking to the Workbench.Under normal circumstances, i should see a kie-server instance listed in the "" section found in Menu > Deploy > Execution Servers. But i cannot somehow get it connected.Here's the Helm Chart i have been using.Can someone help me get kie-server running and connected to workbench.]]></content:encoded></item><item><title>Why is Qwen2-0.5B trained on much more data than the larger models? [D]</title><link>https://www.reddit.com/r/MachineLearning/comments/1lgp926/why_is_qwen205b_trained_on_much_more_data_than/</link><author>/u/datashri</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 05:46:46 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I'm reading through the Qwen2 paper. Something escapes my limited comprehension - ... the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold resulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a significant performance improvement over the 7 trillion token model. It is suspected that increasing the volume of data does not necessarily benefit model pre-training.So higher quality smaller dataset is better. Got it. All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of over 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset.How is it conceivable to train that tiny model on the humongous but lower quality dataset?? My modest intellect feels borderline abused. Appreciate any tips to guide my understanding.]]></content:encoded></item><item><title>Practical Uses for Bitwise Operations</title><link>https://beyondit.blog/blogs/DSA-Fundamentals-2-guide-to-computer-number-systems</link><author>/u/WillingnessFun7051</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 05:00:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Part I: The Foundations of Digital NumeracyUnderstanding Positional Number SystemsThe Core Concept: Base and Positional ValueA number system is a language for writing numbers. These languages use a set of symbols, or digits. Two rules govern these systems: the base and positional notation.The , or radix, is the number of unique digits the system uses. This count includes the digit zero.Our common decimal system is . It uses ten digits (0 through 9).The binary system is . It uses two digits (0 and 1). means a digit’s value depends on its location in a number. In the number 555, each '5' has a different value. There is a '5' in the hundreds place, a '5' in the tens place, and a '5' in the ones place. The value of each position is the base raised to a power. This differs from Roman numerals, where 'X' always means ten.Deconstructing Decimal (Base-10): Our Everyday SystemThe decimal system is the base-10 system we use daily. Each position represents a power of 10. Let's examine the number :(2×10^3)+(0×10^2)+(0×10^1)+(3×10^0)=2000+0+0+3=2003Understanding this structure is the key to learning other number systems.The binary system is the language of modern computers. It is a base-2 system and uses only two digits:  and . These digits are called . In binary, each position represents a power of 2.For example, we can convert the binary number 1011​ to decimal:(1×2^3)+(0×2^2)+(1×2^1)+(1×2^0)=8+0+2+1=11Introducing Hexadecimal (Base-16): The Programmer's ShorthandThe hexadecimal system, or "hex," is a  system. Programmers use it to write long binary numbers in a shorter form. It uses 16 symbols: the digits 0 through 9 and the letters A, B, C, D, E, and F. The letters represent the values 10 through 15. Each position in a hex number represents a power of 16.For example, let's convert the hex number 1A3 to decimal:(1×16^2)+(10×16^1)+(3×16^0)=256+160+3=419 We use subscripts like 101​ to show the base. We can also use prefixes like  for hexadecimal () and  for binary ().Why do computers use binary? The answer lies in their hardware.A computer contains billions of tiny electronic switches called transistors. Each switch has only two possible states:  or . The binary system, with its two digits  and , perfectly matches this physical design. This makes binary the natural language for computers.The main problem with binary is that the numbers get very long. A single byte of data is eight bits long, such as . A memory address can be 64 bits long. Reading or typing these long strings of 0s and 1s often leads to mistakes.Hexadecimal solves this problem. It works as a shorthand for binary because of a simple relationship: 16=24. This means a group of four binary digits, called a , corresponds to exactly one hexadecimal digit.For example, the byte  splits into two nibbles:  and . in binary is 13 in decimal, which is  in hex. in binary is 5 in decimal, which is  in hex.So, the long binary number  becomes the short hex number .Binary is the language of the machine. Hexadecimal is the convenient shorthand for the programmer.Stop Scrolling, Start Achieving: Get Actionable Tech & Productivity Insights.Join the inner circle receiving proven tactics for mastering technology, amplifying productivity, and finding deep focus. Delivered straight to your inbox – no fluff, just results.This chart helps show the patterns between the systems.Section 2: Core Conversion TechniquesConverting from Any Base to DecimalThis basic method converts a number from any base to the decimal system.Find the positional value for each digit. This is the base raised to the power of its position, starting from 0 on the right.Multiply each digit by its positional value.Example (Binary to Decimal): Convert 1101.12​ to decimal.The positions are 3, 2, 1, and 0 for the whole number part, and -1 for the fractional part. (1×2)+(1×2)+(0×2)+(1×2)+(1×2)Example (Hexadecimal to Decimal): Convert A4E​ to decimal.Remember that A=10 and E=14. (10×16)+(4×16)+(14×16) (10×256)+(4×16)+(14×1)=2560+64+14=2638Converting from Decimal to Any BaseTo convert from decimal to another base, we use a different method with two parts.For the Whole Number Part (Repeated Division):Divide the decimal number by the target base.Write down the remainder. This is a digit for your new number.Take the quotient from the division and repeat the process.Continue until the quotient is 0.Read the remainders in reverse order (bottom to top) to get the final answer.For the Fractional Part (Repeated Multiplication):Multiply the decimal fraction by the target base.The whole number part of the result is the first digit of your new fraction.Take the fractional part of the result and repeat the process.Continue until the fraction becomes 0 or you have enough digits.Read the whole numbers in the order you recorded them to get the final answer. Convert 25.625​ to binary.Reading in reverse gives 11001​.0.625×2=1.25 Record the whole number: 0.25×2=0.5 Record the whole number: 0.5×2=1.0 Record the whole number: Reading in order gives .101​.The Binary-Hexadecimal Shortcut: Grouping by FoursProgrammers use this fast trick often. You can convert between binary and hex without using decimal.Split the binary number into groups of four bits (nibbles), starting from the decimal point.If the leftmost group has fewer than four bits, add zeros to the front.Convert each 4-bit group to its single hex digit.Convert it to its 4-bit binary equivalent.Combine the binary groups.A programmer sees  and thinks , which is much simpler.Table 2: Binary-to-Hexadecimal Nibble ConversionThis table is the key to using the shortcut.Part II: Advanced Data RepresentationSection 3: Representing Signed IntegersThe Challenge of Representing Negative NumbersHow do you write a negative number with only 0s and 1s? One idea is to use a single bit to represent the sign. For instance, 0 for positive and 1 for negative. This is called . This system has problems. It creates two ways to write zero (+0 and -0). It also complicates the math for computer hardware.Modern computers use a system called  to represent positive and negative integers. In this system, the first bit (the most significant bit, or MSB) indicates the sign. A 0 means positive, and a 1 means negative. The MSB also has a negative value. For an 8-bit number, the MSB has a value of .For example, in an 8-bit system, we calculate the number  like this:(−1×128)+(1×64)+(1×32)+(0×16)+(0×8)+(1×4)+(0×2)+(0×1)The Negation Algorithm: Finding the OppositeTwo's complement makes it easy to change a number's sign, like turning 5 into -5.The "Flip and Add One" Recipe: all the bits. Change every 0 to a 1, and every 1 to a 0. This is the . to the result. Ignore any extra carry bit at the end. Find the 8-bit two's complement for .Start with positive 69 in binary: .Invert all the bits: .Add 1: .So, -69 is stored as .The main benefit of two's complement is that it simplifies computer math. The computer can use the same hardware circuit for both addition and subtraction. To subtract B from A, the computer calculates A + (the two's complement of B). Calculate . The computer performs this as .57 in 8-bit binary is .-28 in 8-bit two's complement is .  00111001   (57)
+ 11100100   (-28)
------------------
1 00011101   (29)
We ignore the extra carry bit on the left. The 8-bit answer is , which is 29.This math trick is important for processor design. The part of the processor that performs math is the Arithmetic Logic Unit (ALU). Building separate circuits for addition and subtraction would make the ALU larger and use more power.With two's complement, the computer needs only one circuit: an adder. To subtract, the computer uses NOT gates to flip the bits of the second number. Then it uses the adder to add 1 and perform the final addition. A single circuit does both operations, making processors smaller and faster.Section 4: Representing Real NumbersThe Challenge: Representing Fractions and Scientific NotationWe have discussed whole numbers. What about numbers with fractions, like 3.14, or very large numbers? We need a system that can "float" the decimal point.IEEE 754: The Global StandardThe  standard is a universal rulebook for floating-point numbers. It defines how to store them, so all computers calculate them the same way.Anatomy of an IEEE 754 NumberAn IEEE 754 number has three parts, similar to scientific notation: (−1)sign×1.fraction×2exponent. This is simple. 0 is for positive, and 1 is for negative.Biased Exponent (8 or 11 bits): The exponent shows how far to move the decimal point. The standard adds a fixed number, or , to the real exponent. This allows the storage of positive and negative exponents without a separate sign bit. For a 32-bit number, the bias is 127. The computer stores .Mantissa (23 or 52 bits): This part stores the number's actual digits. In binary scientific notation, a number always starts with . The standard does not store the leading 1. This is the "hidden bit" trick, which saves space and adds precision.Single-Precision (32-bit) vs. Double-Precision (64-bit)There are two common sizes for floating-point numbers:Single-Precision (float): A 32-bit number with 1 sign bit, 8 exponent bits, and 23 mantissa bits. It is good for general use.Double-Precision (double): A 64-bit number with 1 sign bit, 11 exponent bits, and 52 mantissa bits. It stores a wider range of numbers with much higher precision.Special Values: Handling Edge CasesThe IEEE 754 standard defines special patterns for unusual values: An exponent of all zeros and a mantissa of all zeros. An exponent of all ones and a mantissa of all zeros. An exponent of all ones and a non-zero mantissa. This results from invalid operations like 0÷0. These are very small numbers near zero. The "hidden bit" is assumed to be 0 instead of 1. This allows a gradual loss of precision.The Trade-off Between Range and PrecisionFloating-point numbers are an engineering compromise. The bits are split between the exponent (range) and the mantissa (precision). This allows the representation of an enormous range of values.But there is a catch: the precision is not uniform.For small numbers near zero, the representable values are close together. Precision is high.For huge numbers, the representable values are far apart. The gap between them can be large.This is why  does not exactly equal  in many programming languages. The numbers 0.1 and 0.2 cannot be represented perfectly in binary. They are rounded to the nearest available floating-point value. The sum of these rounded values is not the same as the rounded value of 0.3. We sacrifice uniform precision to get the massive range needed for science and computing.Part III: Manipulation and ApplicationSection 5: Bitwise OperationsBitwise operations work on numbers at the bit level. They treat the number 13 as the bit string . These operations are fast because they map directly to processor instructions.These operations apply Boolean logic to each pair of bits.Table 3: Bitwise Operator Truth Tables Gives a 1 only if both bits are 1.Main Use (Masking/Clearing): Checks if a specific bit is on or turns a bit off. To check the 3rd bit, you can AND the number with . If the result is not zero, the bit was on. Gives a 1 if either bit is 1. Turns a specific bit on. To turn on the 3rd bit, you can OR the number with . This does not change other bits. Gives a 1 only if the bits are different. Flips a bit. To flip the 3rd bit, you can XOR your number with . Flips every bit in a single number. This is also called the ones' complement.Shift operations slide all bits in a number to the left or right. Slides all bits  places to the left. Zeros fill in on the right. This is a fast way to multiply a number by 2n. Slides all bits  places to the right. This is a fast way to do integer division by 2n. What fills in on the left depends on the type of shift: Used for signed numbers. It copies the original sign bit to preserve the number's sign. Used for unsigned numbers. It always fills the empty spots with zeros.Practical Application: BitmaskingBitmasking is a common programming technique. You can use the bits of a single integer to store a set of flags instead of using many separate true/false variables. This saves memory.Example: File PermissionsOperating systems use bitmasking for file permissions. Let's say bit 2 is Read, bit 1 is Write, and bit 0 is Execute. (binary ) (binary ) (binary )A file that is readable and writable has permissions READ_PERMISSION | WRITE_PERMISSION, which is  (binary ).To check for write permission:if (permissions & WRITE_PERMISSION)To add execute permission:permissions = permissions | EXECUTE_PERMISSIONTo remove write permission:permissions = permissions & ~WRITE_PERMISSIONThis technique is common in low-level programming where speed and memory are important.Section 6: Number Systems in the Real WorldMemory, Debugging, and Data Representation Every byte in a computer's memory has a unique address. These addresses are written in hexadecimal because it is shorter and easier to read than binary. An address like  is easier to read than 64 ones and zeros. Programmers look at "memory dumps" to find bugs. These snapshots of memory are displayed in hex for quick scanning. An early character set for English. It used 7 or 8 bits, which allowed for only 128 or 256 characters. Unicode is a standard that gives a unique number, or , to every character. UTF-8 is the most popular way to encode Unicode numbers into binary. It uses one byte for ASCII characters and up to four bytes for other characters.Table 4: Common Character Encodings11100010 10000010 1010110011110000 10011111 10010001 10001101Networking: IP Addressing An IPv4 address is a 32-bit number. We write it as four decimal numbers separated by dots, like . Each number is an 8-bit segment called an octet. The world ran out of IPv4 addresses. The new standard is IPv6, which uses 128-bit numbers. They are written as eight groups of four hexadecimal digits, separated by colons, like 2001:0db8:85a3:0000:0000:8a2e:0370:7334. A subnet mask tells a router which part of an IP address identifies the network and which part identifies the computer. It uses a bitwise AND operation between the IP address and the mask.Web Development: CSS Hex Color CodesA color on a website defined as  is a hexadecimal color code. The format is . is the amount of Green.Each value is a two-digit hex number from  (none) to  (maximum). For example,  is pure red, and  is white.Hardware: Digital Logic CircuitsAll computer hardware is built from logic gates (AND, OR, NOT gates). These are tiny electronic circuits that perform bitwise operations. A high voltage signal is a , and a low voltage signal is a . When a computer performs math, it is flipping switches according to the rules of logic.Using Python for Number SystemsPython is a good tool for experimenting with these concepts.: Converts an integer to a binary string.  gives .: Converts an integer to a hex string.  gives .: Converts a string  in a given base to an integer.  gives .Python also supports all bitwise operators: , , , , , .# --- Conversions ---
decimal_val = 173
binary_str = bin(decimal_val)      # Result: '0b10101101'
hex_str = hex(decimal_val)          # Result: '0xad'

# Convert back to decimal
binary_to_dec = int('10101101', 2) # Result: 173
hex_to_dec = int('ad', 16)          # Result: 173

# --- Bitwise Operations ---
a = 92  # Binary: 01011100
b = 101 # Binary: 01100101

# Bitwise AND: checks which bits are 1 in BOTH numbers
print(f"a & b = {a & b}") # Result: 68 (binary 01000100)

# Bitwise OR: checks which bits are 1 in EITHER number
print(f"a | b = {a | b}") # Result: 125 (binary 01111101)

# Bitwise XOR: checks which bits are DIFFERENT
print(f"a ^ b = {a ^ b}") # Result: 57 (binary 00111001)

# Bitwise NOT: flips all the bits of 'a'
print(f"~a = {~a}") # Result: -93 (due to two's complement)

# Bit Shifts: fast multiplication and division by 2
print(f"a << 2 = {a << 2}") # Result: 368 (92 * 4)
print(f"a >> 2 = {a >> 2}") # Result: 23 (92 // 4)
Specialized Python Libraries A library for symbolic math with a module for logic. A library for learning formal logic. A library for representing different kinds of logic.Online Calculators and SimulatorsStanford Introduction to Logic: This site has a Digital Circuit Builder and a tool for truth tables called Boole. An online tool that solves logic formulas and generates truth tables.Number System Conversions Convert 11011​ to decimal. (1×16)+(1×8)+(0×4)+(1×2)+(1×1)=16+8+2+1=27​. Convert 452​ to hexadecimal. 452÷16=28 R 4. 28÷16=1 R 12 (C). 1÷16=0 R 1. Reading remainders in reverse gives 1C4. Convert DE0​ to binary. D = , E = , 0 = . Combining gives 110111100000​. Find the 8-bit two's complement of . Positive 42 is . Invert bits: . Add 1: 11010110​. Calculate  using 8-bit two's complement. This is . 35 is . -50 is .00100011 + 11001110 = 11110001.The result is negative. To find its magnitude, take its two's complement. Invert () and add 1 (), which is 15. The answer is −15. Given the number 77 (), set the 6th bit. The mask is , which is .01001101 | 01000000 = 01001101. The bit was already set. Given the number 77 (), clear the 2nd bit. The mask is , which is .01001101 & 11111011 = 01001001, which is 73. How many set bits are in the number 203 ()? Using Brian Kernighan's algorithm, the answer is .Section 9: Capstone Project IdeasBuild a universal number system converter.Create a bitwise operations calculator.Write a program to encode a text message into a hex string.Build an IPv4 subnet calculator.Create a visualizer for IEEE 754 floating-point numbers.Appendix: Further Learning ResourcesStanford University - Introduction to Logic (Coursera)University of Leeds - An Introduction to Logic for Computer Science (Coursera)Ahmed Muhammed - Number Systems For Beginners (Udemy) Offers videos and exercises on binary and hexadecimal systems.For Discrete Mathematics and Logic:Discrete Mathematics and Its Applications by Kenneth H. RosenDiscrete Mathematics with Applications by Susanna S. EppFor Computer Architecture:Computer Organization & Design by David A. Patterson and John L. HennessyComputer Systems: A Programmer's Perspective by Randal E. Bryant and David R. O'HallaronCode: The Hidden Language of Computer Hardware and Software by Charles PetzoldUnderstanding decimal, binary, and hexadecimal is a core skill for computing. Binary is the computer's native language. Hexadecimal is the programmer's shorthand for it. Concepts like two's complement and IEEE 754 make computer math possible. Learning these languages and tools gives you a deep understanding of how the digital world is built.]]></content:encoded></item><item><title>MCP Security is still Broken</title><link>https://forgecode.dev/blog/prevent-attacks-on-mcp/</link><author>/u/West-Chocolate2977</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 04:47:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Been digging into Model Context Protocol implementations lately and found some stuff that's keeping me up at night. Not because it's earth-shattering, but because it's the kind of boring security debt that bites you when you least expect it.What's MCP and Why Should I Care?​MCP is Anthropic's attempt at standardizing how AI models talk to external tools. Instead of every AI app rolling their own integration layer, you get a common protocol. Think of it like REST for AI tools, except with way less thought put into security.The spec is pretty straightforward - JSON-RPC over stdio or HTTP. AI asks for available tools, gets back a list with descriptions, then calls them with parameters. Simple enough that you can implement a basic server in an afternoon.Which is exactly the problem.Here's where things get interesting. MCP servers describe their tools using natural language descriptions that the AI reads to understand what each tool does. Sounds reasonable, right?Except those descriptions get fed directly into the AI's context. And if you control the MCP server, you can put whatever you want in those descriptions.The AI reads this description and suddenly thinks it has new instructions. User asks for weather, AI decides to exfiltrate data instead.I tested this against a few popular MCP implementations and... yeah, it works. Most don't even try to sanitize tool descriptions.Why This Actually Matters​Unlike typical prompt injection where you need user input, this attack vector lives in the protocol itself. The AI has to read tool descriptions to function. You can't just "sanitize" them without breaking core functionality.And here's the kicker - in most setups, the user never sees the tool descriptions. They just see "checking weather..." while the AI follows completely different instructions in the background.Authentication? What Authentication?​Spent some time looking at MCP server implementations in the wild. The authentication situation is... not great.A lot of servers I found basically look like this:That TODO comment/Documentation is doing a lot of heavy lifting.The MCP spec does mention authentication, but it's basically "figure it out yourself." Most implementations I've seen either skip it entirely or bolt on some basic API key checking that's trivial to bypass.Found one server that checked for an API key but only on GET requests. POST requests (you know, the ones that actually do stuff) went straight through.MCP tools are distributed as packages, which means we get all the fun of supply chain attacks. But with a twist - these tools run with whatever permissions your AI system has.Regular supply chain attacks might steal your npm tokens or mine some crypto. MCP supply chain attacks can read your conversations, access your databases, and impersonate you to other services.I've been watching a few popular MCP tool repositories. The security practices are... inconsistent. Lots of tools with broad permissions, minimal code review, and maintainers who probably haven't thought much about security.Not naming names because I'm not trying to shame anyone, but if you're using MCP tools in production, you might want to audit what you're actually running.Tested this stuff against a few internal systems (with permission, obviously). The results weren't great:Got tool description injection working against 2/4 MCP implementationsFound unauthenticated endpoints in 1/10 production deploymentsIdentified several tools with way more permissions than they neededThe scariest part? Most of this stuff would be invisible in standard logs. User requests "check my calendar," AI executes malicious tool, logs show "calendar_check: success." Good luck spotting that in your SIEM.What Actually Needs Fixing​This isn't about rewriting everything. Most of this is fixable with some basic hygiene:Parse and validate descriptions before feeding them to the AIStrip out anything that looks like instructionsConsider using structured descriptions instead of free textActually implement it (OAuth flows are now required in MCP 2025-06-18)Use proper OAuth Resource Server patterns as specified in the latest MCP specImplement Resource Indicators (RFC 8707) to prevent token theftValidate tokens on every requestReview code before deployingRun tools with minimal permissionsNone of this is rocket science. It's just boring security work that nobody wants to do.MCP adoption is picking up fast. I'm seeing it deployed in financial services, healthcare, customer support systems. Places where a security incident would be really, really bad.The window for fixing this stuff cleanly is closing. Once you have thousands of MCP servers in production, coordinating security updates becomes a nightmare.Better to fix it now while the ecosystem is still small enough to actually change.The latest MCP specification (released June 18, 2025) addresses some security concerns:OAuth Resource Server classification is now requiredResource Indicators (RFC 8707) must be implemented to prevent malicious token accessNew security best practices documentationRemoval of JSON-RPC batching (reduces attack surface)However, the core vulnerabilities described above (tool description injection, supply chain risks) remain unaddressed in the protocol itself.Part 2 will cover specific mitigation strategies and some tools I've been building to make this stuff easier to secure. Nothing groundbreaking, just practical stuff that actually works.If you're building MCP tools or have seen other security issues, let me know. This ecosystem is still small enough that we can actually fix problems before they become disasters.]]></content:encoded></item><item><title>Making chess in ncurses and c++</title><link>https://www.youtube.com/watch?v=B-ZBBT0Yj_g</link><author>/u/that_brown_nerd</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 03:57:17 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I made a crate for mesh editing</title><link>https://www.reddit.com/r/rust/comments/1lgmx9y/i_made_a_crate_for_mesh_editing/</link><author>/u/camilo16</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 03:29:16 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I just published Polyhedron a crate for manipulating manifold and non manifold meshes.The crate includes: * Compile time selection for manifold vs non manifold representation * Agnostic vertex representation, a vertex can be any type and dimension, e.g. nalgebra or glam, through my other crate . * Fundamental topological operations, edge flipping, splitting, collapse. * Implementations for loop subdivision, QEM edge simplification and Kobet's remeshing algorithm.The crate is in its infancy and will be for a while. It will be actively maintained but I can only work on it in an "as need to" basis.If you need an algorithm and want to contribute, please reach out to me to help you implement it.For commercial use, please refer to the License file.]]></content:encoded></item><item><title>Why isn&apos;t Debian recommended more often?</title><link>https://www.reddit.com/r/linux/comments/1lgl87v/why_isnt_debian_recommended_more_often/</link><author>/u/Browncoatinabox</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 01:58:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Everyone is happy to recommend Ubuntu/Debian based distros but never Debian itself. It's stable and up-to-date-ish. My only real complaint is that KDE isn't up to date and that you aren't Sudo out of the gate. But outside of that I have never had any real issues. ]]></content:encoded></item><item><title>Which Linux is your favourite? For me, it’s fedora.</title><link>https://www.reddit.com/r/linux/comments/1lgkwz1/which_linux_is_your_favourite_for_me_its_fedora/</link><author>/u/New_Series3209</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 01:41:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A fast, lightweight Tailwind class sorter for Templ users (no more Prettier)</title><link>https://www.reddit.com/r/golang/comments/1lgk2bb/a_fast_lightweight_tailwind_class_sorter_for/</link><author>/u/DexterInAI</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:56:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Heyy, so for the past couple of days, I have been working on , a lightweight CLI tool written in Go, and I just finished building a version I am satisfied with.My goal was to build something I can use without needing to install  just to run the Tailwind's prettier-plugin-tailwindcss class sorter. I often work in environments with Python or Go and use Tailwind via the .Zero Node/NPM dependencies (great for  setups).Astral's , making it easy to spot and fix unsorted classes. for tailored file patterns & attributes.Seamless integration as a pre-commit hook.I'm pretty happy with how it turned out, so I wanted to share!]]></content:encoded></item><item><title>Kubernetes Security Trade-offs?</title><link>https://www.reddit.com/r/kubernetes/comments/1lgjv1o/kubernetes_security_tradeoffs/</link><author>/u/magnezone150</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:46:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a Kubeadm Cluster that I built on Rocky Linux 9.6 Servers. I thought I'd challenge myself and see if I can do it with firewalld enabled and up. I've also Installed Istio, Calico, MetalLB and KubeVirt. However, with my current firewalld config everything in cluster is good including serving sites with istio but my KubeVirt VMs can't seem access outside of the Cluster such as ping google.com -c 3 or dnf update saying their requests are filtered unless I move my Nodes interface (eno1) to the kubenetes zone but the trade off is if someone uses nmap scan they can easily see ports on all nodes versus keeping the interface where it is in public zone causing nmap defaulting to the node being down or takes longer to produce any reports where it only can see ssh. Curious if anyone has ever done a setup like this before?These are the firewall configurations I have on all Nodes.public (active) target: default icmp-block-inversion: no interfaces: eno1 sources: services: ssh ports: protocols: forward: yes masquerade: yes forward-ports: source-ports: icmp-blocks: rich rules: --- kubernetes (active) target: default icmp-block-inversion: no interfaces: sources: <Master-IP> <Worker-IP-1> <Worker-IP-2> <Pod-CIDR> <Service-CIDR> services: ports: 6443/tcp 2379/tcp 2380/tcp 10250/tcp 10251/tcp 10252/tcp 179/tcp 4789/tcp 5473/tcp 51820/tcp 51821/tcp 80/tcp 443/tcp 9101/tcp 15000-15021/tcp 15053/tcp 15090/tcp 8443/tcp 9443/tcp 9650/tcp 1500/tcp 22/tcp 1500/udp 49152-49215/tcp 30000-32767/tcp 30000-32767/udp protocols: forward: yes masquerade: yes forward-ports: source-ports: icmp-blocks: rich rules: ]]></content:encoded></item><item><title>Storage solutions for on premise setup</title><link>https://www.reddit.com/r/kubernetes/comments/1lgjt5e/storage_solutions_for_on_premise_setup/</link><author>/u/QualityHot6485</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:43:44 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am creating a kubernetes cluster in an on premise cluster but the problem is I don't know which storage option to use for on premise.In this on premise setup I want the data to be stored in the node itself. So for this setup I used hostpath. But in hostpath it is irrelevant setting the pvc as it will not follow it and store data as long there is disk space. I also read some articles where they mention that hostpath is not suitable for production. But couldn't understand the reason why ???If there is any alternative to hostpath?? Which follows the pvc limit and allows volume expansion also ??Suggest me some alternative (csi)storage options for on premise setup !!Also why is hostpath not recommended for production???]]></content:encoded></item><item><title>Go JSON Validation</title><link>https://www.reddit.com/r/golang/comments/1lgjrdz/go_json_validation/</link><author>/u/EarthAggressive9167</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:41:14 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[ I’m learning Go, but I come from a TypeScript background and I’m finding JSON validation a bit tricky maybe because I’m used to Zod.What do you all use for validation?]]></content:encoded></item><item><title>Warning to CEOs: The AI You Are Being Told Can Replace Engineers, Designers, and Researchers Is More Likely to Bankrupt You Than You Think</title><link>https://drakewatson.substack.com/p/warning-to-ceos-the-ai-you-are-being</link><author>/u/VeridianLuna</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:39:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Finding performance problems by diffing two Go profiles</title><link>https://www.dolthub.com/blog/2025-06-20-go-pprof-diffing/</link><author>/u/zachm</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:26:25 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[We're hard at work on compatibility for
Doltgres, the world's first and only
version-controlled Postgres-compatible SQL database. This means getting it to work with every
library and tool that Postgres works with, out of the box. Lately we've been focussing a lot of
effort on SQLAlchemy, a popular ORM for Python. Their MySQL
integration works flawlessly with Dolt, but their Postgres version is apparently completely
different, relying heavily on the  tables. A customer tried it out and found a lot of
gaps owing to Doltgres not including system
tables (e.g. ) in the  tables. So I fixed that, but this led to a mysterious
perfomance regression in one of our test suites, over a 3x slowdown.It took quite a bit of puzzling to figure out why such an innocuous seeming change caused such a
dramatic difference in performance. In the end, what helped the most was an amazing tool in the Go
toolchain: visualizing the difference between two performance profiles with the  option to
.Go ships with a robust profiling tool, . Unlike some other languages, you have to explicitly
enable it in your code to get a profile; you can't do it after the fact or with command line
flags. This is easy, but you have to write the code to do it. In our case, I placed it directly in
the test method being profiled.t testingT ok  osok 
	
	p  profileprofileCPUProfile pThe final two lines of this snippet start a CPU profile, then stop it when the method completes. It
uses the  package, which provides a more ergonomic wrapper around the
built-in profiler libraries. If you run code that does this, you'll see an output line like the
following:2025/06/20 14:10:40.548730 profile: cpu profiling disabled, C:\Users\ZACHMU~1\AppData\Local\Temp\profile1113350212\cpu.pprofThis is the location of the profile produced by the run, which you should note or copy into another
location with an easier to remember name.For my testing, I wanted to see how the performance changed between what was on the  branch
and my current branch, so I ran the test with profiling enabled on each branch. Now I can compare
them using the  flag with .After getting a profile for each branch, now I just need to compare them.go tool pprof :8090  main.pprof branch.pprofThe  flag tells  to "subtract" the named profile from the other one when reporting
performance numbers. In this case, I want to see what is happening in  but not in
 that's taking so long. I also always use the  flag, which runs an interactive
web server instead of a command-line interface. I find it much easier to work with when
investigating performance profiles.When I run the command, my web browser launches to the default display, a graph of cumulative CPU
samples roughly topo-sorted by function, so you can see what calls what. Unlike in a normal profile
analysis, the numbers shown are strictly the diff between the two profiles, rather than their
absolute runtimes. Here's what I saw in my web view:Database.tableInsensitive is the function that fetches a table object for the query engine to
use. Somehow, my changes had made this function much, much slower, despite not editing it
directly. With this clue in hand, I was able to find the performance bug.

	tableNames err  dbctx root err  doltdbTableName err
	 root
		tableMap  table  tableNames 
			tableMapstringstable table
		
		dbStateroot tableMap

	tableName ok  sqltableName tableNamesok  doltdbTableNameThe first line of the snippet loads all table names from the DB if they weren't already cached in
the session. This is necessary because our table names are stored in a case-sensitive manner, but
SQL is case-insensitive. So, as part of loading a table from the DB, we need to correct the
requested case-insensitive name from the query to the case-sensitive one for use in the storage and
I/O layer. But that call to  includes a final parameter:
includeGeneratedSystemTables. This was hard-coded to true, which meant it was always calling the
new, more expensive method of getting a list of generated system tables, which includes potential
disk access to get the set of database schemas and then lots of iteration over them.	schemas err  rootctx err  err
	schemas
		schemas schemas schemaDatabaseSchemaName doltdbDefaultSchemaName schema  schemas 
		tableNames err  rootctx schemaName err  err
		 pre  doltdbGeneratedSystemTablePrefixes  tableName  tableNames 
				sdoltdbTableName
					Name   pre  tableName
					Schema schemaName UseSearchPath  schemaName  schemaName  doltdbDoltNamespace  name  doltdbDoltGeneratedTableNames 
				sdoltdbTableName
					Name   name
					Schema schemaNameAs it turns out, the hard-coded  was simply wrong -- this method never needed to consider
system-generated table names. But it was a relatively harmless bug before I made the process of
generating those names more expensive, and had been in the code for years unnoticed. Changing this
value to  to remove the unnecessary work fixed the performance regression, and also sped up
Dolt's benchmarks by a bit as well.I'm not sure I ever would have figured out the source of this inefficiency without the  flag
to point me in the right direction.Questions about Go performance profiling or about Doltgres? Come by our
Discord to talk to our engineering team and meet other Doltgres
users.]]></content:encoded></item><item><title>Thoughts on rust_native</title><link>https://www.reddit.com/r/rust/comments/1lgjeyn/thoughts_on_rust_native/</link><author>/u/vlovich</author><category>reddit</category><pubDate>Sat, 21 Jun 2025 00:23:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The feature list looks impressive although the development process looks to be code dumps so I'm not sure about the quality / if anything even works & it has few reviews. Has anyone tried it?   submitted by    /u/vlovich ]]></content:encoded></item><item><title>AbsenceBench: Language Models Can&apos;t Tell What&apos;s Missing</title><link>https://arxiv.org/abs/2506.11440</link><author>/u/locomotus</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 23:45:28 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Does anyone customize Scheduler profiles and/or use Cluster Autoscaler expanders to improve bin-packing on nodes?</title><link>https://blog.cleancompute.net/p/kubernetes-cost-optimization</link><author>/u/nbir</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 23:07:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[operating 100s of Kubernetes clustersKeep these principles in mind as you craft your own cost optimization blueprint:There's no single magic bulletYou can't optimize what you don't seeHigh utilization doesn’t always equal low costOptimization vs. reliability is a delicate danceShift Left by building cost consciousnessArchitect for efficiency earlyKubernetes workloads often autoscale and are distributed across diverse node types, especially in shared multi-tenant clusters. This obscures compute cost origins, prevents identifying inefficiencies, and assigning accountability of resources to specific teams or applications. Cloud providers usually have built in cost dashboards, but only provide visibility only at the VM or node level and lack attribution at the application level,OpenCostThe default Kubernetes scheduler distributes workloads uniformly across nodes. This leads to persistent overprovisioning resulting in unused capacity across multiple nodes and sparse bin-packing.--scale-down-utilization-threshold--scale-down-unneeded-time--scale-down-utilization-threshold--scale-down-unneeded-timeKarpenterScaling Oscillation or Thrashing--scale-down-delay-after-add--scale-down-delay-after-deleteWorkloads Blocking EvictionShort-lived ephemeral workloads like analytics pipelines, batch jobs, data ingestion agents, etc. frequently spin-up and spin-down. This prevents Cluster Autoscaler from scaling down nodes. Running these workloads on on-demand nodes doesn’t make financial sense.this blogManually configuring CPU and memory requests and limits for pods is inefficient. Developers often err on the side of overprovisioning, driven by concerns about performance, stability, and the potential impact of noisy neighbors. Furthermore, initial resource requests are rarely audited or adjusted over time as application needs evolve. This leads to wasted resources.Phased Rollout and Escape HatchesDeveloper Control and TrustApplication of Scaled-Down RequestsHPA natively supports scaling based on CPU and memory utilization. However, this is insufficient for applications with scaling needs reflected best in business or application-level metrics like requests per second, queue depth, active connections, etc. Scaling solely on CPU or memory can cause instability and failures, leading to reliability concerns. Consequently, teams often overprovision to handle peak loads.KEDAMetrics Infrastructure ReliabilitySingle Custom Metrics Server LimitationFailure Modes and DefaultsIt is common to provision single-tenant Kubernetes clusters per team or application when starting off. This approach, driven by a perceived need for strict isolation or sometimes developer insistence, is a classic recipe for overprovisioning.Resource Requests and LimitsResource Quotas and LimitRangesRole-Based Access Control (RBAC)Pod Security Standards (PSS) or Security Contexts"Noisy Neighbor" PhenomenonNetwork and Disk I/O BottlenecksDiscrepancies between node CPU:memory ratios and that of workload consumption lead to resource imbalances. For example, memory-intensive workloads on high CPU:memory nodes can underutilize CPU while bottlenecking memory. This leads to more nodes than actually required.KarpenterIn-place Node Type Changesthis blogWorkloads with PodDisruptionBudgets (PDBs) set to 0 or safe-to-evict: false annotations block Cluster Autoscaler node scale-down operations. These configurations are common for singletons or critical workloads, and are problematic in multi-tenant platforms.Persistent storage costs in Kubernetes can accumulate rapidly when using cloud managed Persistent Volumes (PVs). Without active management, teams default to expensive storage classes, over-provision volume sizes, or leave unused volumes lingering, leading to accumulating costs.Unused or Orphaned VolumesNetwork costs can become an unexpected line item in cloud bills, because cloud providers charge for all ingress and egress traffic, cross-region data transfer, load balancers, gateways, etc. A multi-region Kubernetes architecture built for resilience can come at an exorbitant price. Applications with high data transfer or public-facing services can rapidly accumulate network charges too.Continue reading Part 2 (coming soon) of this blog, where we share a case study detailing real-world application of these strategies by an organization operating cloud-scale Kubernetes infrastructure across multiple cloud providers and continents. Get ready for behind-the-scenes war stories and first-hand lessons.]]></content:encoded></item><item><title>12 years of Postgres Weekly with Peter Cooper, on Talking Postgres with Claire Giordano</title><link>https://talkingpostgres.com/episodes/12-years-of-postgres-weekly-with-peter-cooper</link><author>/u/clairegiordano</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 22:00:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Claire GiordanoClaire Giordano is head of the Postgres open source community initiatives at Microsoft. Claire has served in leadership roles in engineering, product management, and product marketing at Sun Microsystems, Amazon/A9, and Citus Data. At Sun, Claire managed the engineering team that created Solaris Zones, and led the effort to open source Solaris.]]></content:encoded></item><item><title>Europe’s Growing Fear: How Trump Might Use U.S. Tech Dominance Against It</title><link>https://www.nytimes.com/2025/06/20/technology/us-tech-europe-microsoft-trump-icc.html?smid=nytcore-ios-share&amp;amp;referringSource=articleShare</link><author>/u/Grevillea_banksii</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 21:32:20 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Falling in love with Rust 🦀 — where should I go from here?</title><link>https://www.reddit.com/r/rust/comments/1lgdgxy/falling_in_love_with_rust_where_should_i_go_from/</link><author>/u/Upbeat_Ad_6119</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 19:58:28 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Last 4 years I’ve been working as a Node.js backend developer. Yeah, my main language is JavaScript (well, TypeScript to be more accurate), and to be honest, I’ve grown a bit tired of it. It’s weird writing code in a scripting language that gets compiled into another scripting language, which then gets interpreted by yet another runtime.Also, I'm just tired of spinning up new projects - installing linters, formatters, test runners, builder configs, dealing with tsconfigs, ESM/CommonJs specifications.On top of that, I often hit walls due to the lack of some really useful features, like proper compile-time metaprogramming, which only compiled languages tend to offer.So, a few months ago I realized I don’t want to be just a JS developer anymore. I started looking for a better language to grow with.It seemed simple, minimalistic, efficient - a relatively easy shift from Node. But after about a week, I dropped it. Yeah, minimalism is cool and all, but it lacks a lot of features I really value. And most importantly, it drove me insane with:Error propagation - writing the same 4 lines in every function, on every layer? nah.Access modifiers based on capital letters, really?What I did like about Go was that you get a complete standard toolchain out of the box. No need to install 20+ dev dependencies like in Node. I think Go could be a great fit for certain use cases, but for me, it felt too limited for most projects I care about.Then I thought about C++.I’ve used it before for competitive programming, and I enjoy stuff like macros and operator overloading. But package management? CMake? Total nightmare. So I decided to leave C++ strictly for CP stuff.And then… I fell in love - at first sight - with Rust.Just a few weeks ago I discovered Rust, and I love so many things about it. The macros, enums, pattern matching, trait overloading... it’s awesome seeing how all these features come together in practice.Some parts are a bit weird at first - like ownership, borrowing, and lifetimes - but I think it just takes time to get used to them. Overall, I really believe Rust knowledge will be super valuable for my career. I’d love to contribute to distributed systems, or build supporting tools, instead of staying in the usual API/microservice zone forever.So right now I’m looking for advice - what direction should I take next? Sure, I can just research on my own (as I usually do), but hearing from real people who are on the same journey - or already walked it - would be incredibly helpful. I’d love to hear your stories too.Currently I’m going through the official Rust docs to get the basics down. But I’m also hunting for some advanced books or resources. A lot of books I found just copy-paste examples from the docs, and I’m hoping for something deeper. If you have any recommendations - even if it’s not web-related, or too advanced for a beginner - I’d seriously appreciate it. The more challenging, the better.Thanks for reading - and excited to join the Rust path with all of you 🤘]]></content:encoded></item><item><title>Did you switch to Linux because you loved it?</title><link>https://www.reddit.com/r/linux/comments/1lgcnt1/did_you_switch_to_linux_because_you_loved_it/</link><author>/u/gerundingnounshire</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 19:23:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I've noticed a common sentiment from many Linux users of "I switched to Linux because Windows sucks," and I don't really share that. I switched because I decided to give Linux a shot because it seemed interesting, and I ended up loving it so much that I just sorta decided to daily-drive it.Am I alone in this? Has anyone else switched solely because they liked Linux?]]></content:encoded></item><item><title>Migrating off Legacy Tokio at Scale</title><link>https://www.okta.com/blog/2024/11/migrating-off-legacy-tokio-at-scale/</link><author>/u/anonymous_pro_</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 18:15:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Flathub has passed 3 billion downloads</title><link>https://www.reddit.com/r/linux/comments/1lgaz5z/flathub_has_passed_3_billion_downloads/</link><author>/u/mr_MADAFAKA</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 18:14:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/mr_MADAFAKA ]]></content:encoded></item><item><title>Simple HTTP/TCP/ICMP endpoint checker</title><link>https://www.reddit.com/r/golang/comments/1lgamsg/simple_httptcpicmp_endpoint_checker/</link><author>/u/1dk_b01</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 18:00:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I would like to share one project which I have contributed to several times and I think it deserves more eyes and attention. It is a simple one-shot health/uptime checker feasible of monitoring ICMP, TCP or HTTP endpoints. I have been using it for like three years now to ensure services are up and exposed properly. In the beginning, services were few, so there was no need for the complex monitoring solutions and systems. And I wanted something simplistic and quick. Now, it can be integrated with Prometheus via the Pushgateway service, or simply with any service via webhooks. Also, alerting was in mind too, so it sends Telegram messages right after the down state is detected.Below is a link to project repository, and a link to a blog post that gives a deep dive experience in more technical detail.]]></content:encoded></item><item><title>Malware-Laced GitHub Repos Found Masquerading as Developer Tools</title><link>https://klarrio.com/klarrio-discovers-large-scale-malware-network-on-github/</link><author>/u/gametorch</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 17:22:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[(English translation below)Klarrio ontdekt grootschalig malware-netwerk op GitHubKlarrio heeft onlangs een belangrijke ontdekking gedaan: Klarrio Discovers Large-Scale Malware Network on GitHubhttps://<domein>/storage/<path>Thanks to the press who have already relayed the information:]]></content:encoded></item><item><title>godump v1.2.0 - Thank you again</title><link>https://i.postimg.cc/MptM6XV8/IMG-9389.png</link><author>/u/cmiles777</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 16:56:49 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Quick Tip: Stop Your Go Programs from Leaking Memory with Context</title><link>https://www.reddit.com/r/golang/comments/1lg8n72/quick_tip_stop_your_go_programs_from_leaking/</link><author>/u/GladJellyfish9752</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 16:40:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey everyone! I wanted to share something that helped me write better Go code. So basically, I kept running into this annoying problem where my programs would eat up memory because I wasn't properly stopping my goroutines. It's like starting a bunch of tasks but forgetting to tell them when to quit - they just keep running forever!The fix is actually pretty simple: use context to tell your goroutines when it's time to stop. Think of context like a "stop button" that you can press to cleanly shut down all your background work. I started doing this in all my projects and it made debugging so much easier. No more wondering why my program is using tons of memory or why things aren't shutting down properly.import ( "context" "fmt" "sync" "time" )func worker(ctx context.Context, id int, wg *sync.WaitGroup) { defer wg.Done()for { select { case <-ctx.Done(): fmt.Printf("Worker %d: time to stop!\n", id) return case <-time.After(500 * time.Millisecond): fmt.Printf("Worker %d: still working...\n", id) } } func main() { // Create a context that auto-cancels after 3 seconds ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) defer cancel()var wg sync.WaitGroup // Start 3 workers for i := 1; i <= 3; i++ { wg.Add(1) go worker(ctx, i, &wg) } // Wait for everyone to finish wg.Wait() fmt.Println("Done! All workers stopped cleanly")  Always use WaitGroup with context so your main function waits for all goroutines to actually finish before exiting. It's like making sure everyone gets off the bus before the driver leaves!]]></content:encoded></item><item><title>xAI faces legal threat over alleged Colossus data center pollution in Memphis</title><link>https://arstechnica.com/tech-policy/2025/06/xai-faces-legal-threat-over-alleged-colossus-data-center-pollution-in-memphis/</link><author>/u/F0urLeafCl0ver</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 16:16:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tell me what you think about my Rust project (crate, docker, binary)</title><link>https://github.com/johan-steffens/foxy</link><author>/u/Isosymmetric</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 16:07:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hello everybody, first time poster here.I've been working with Rust more and more in my career as of late, and really been loving it (despite late-night fights with the Karen compiler). I eventually got to a point where I wanted to challenge myself to build something that I would actually use, and decided to build an extensible, config-driven, Rust proxy/API gateway as a challenge.The challenge evolved into something more, and I ended up adding a whole bunch of cool features (to the end of it being something that I would actually use), and have gotten it to a point where I'd like to share it to get some feedback, insight, or even kudos.Please let me know what you think, or leave a star if you like it.]]></content:encoded></item><item><title>The Embedded Rustacean Issue #48</title><link>https://www.theembeddedrustacean.com/p/the-embedded-rustacean-issue-48</link><author>/u/TheEmbeddedRustacean</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 16:00:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ Hello and welcome to the Embedded Rustacean! This newsletter is a bi-monthly curation of resources and a summary of everything happening around embedded Rust 🦀. This newsletter was started because of the belief in Rust as a programming language with all the traits 🧬 (pun intended) that prime it to become the future of software in embedded systems. We’re another issue closer to that vision. Want to get involved or think about contributing? Click here for a contribution guide. Get a free graphical overview of the embedded Rust ecosystem here. Like newsletters? Here are some other awesome (and completely free!) newsletters our readers also enjoy.Explore ✍️🖼️🗒️🐭🏃Software is about managing complexity: the complexity of the problem, laid upon the complexity of the machine. Because of this complexity, most of our programming projects fail.🚨 🦀 Looking for unbiased, fact-based news? Join 1440 today. Join over 4 million Americans who start their day with 1440 – your daily digest for unbiased, fact-centric news. From politics to sports, we cover it all by analyzing over 100 sources. Our concise, 5-minute read lands in your inbox each morning at no cost. Experience news without the noise; let 1440 help you make up your own mind. Sign up now and invite your friends and family to be part of the informed. ]]></content:encoded></item><item><title>I&apos;m shocked by Plasma 6.4&apos;s HDR improvement</title><link>https://www.reddit.com/r/linux/comments/1lg7bzh/im_shocked_by_plasma_64s_hdr_improvement/</link><author>/u/lajka30</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 15:47:25 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Update] Permiflow now generates safe RBAC Roles + discovers live API resources</title><link>https://www.reddit.com/r/kubernetes/comments/1lg7btg/update_permiflow_now_generates_safe_rbac_roles/</link><author>/u/Potential_Ad_1172</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 15:47:14 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hey folks — quick update on Permiflow since the last post. Added two major features — safer  for creating compliant RBAC YAMLs, and  to discover real verbs/resources from your live cluster.Huge thanks for the feedback, especially @KristianTrifork 🙏 — Safer RBAC Role GeneratorRBAC YAMLs are brittle, risky, and a pain to write by hand. This helps you generate  that grant broad access —  dangerous permissions like  or .permiflow generate-role --name safe-bot --allow-verbs get,list,watch,create,update --exclude-resources secrets,pods/exec ```CI agents or bots with near-admin access — without scary verbsScoped access for contractors / staging appsCompliance-friendly defaults for new rolesSupports  and deterministic YAML output — Discover What Your Cluster Actually SupportsEver guess what verbs a resource supports? Or forget if something is namespaced?bash permiflow resources permiflow resources --namespaced-only permiflow resources --json > k8s-resources.json This queries your live cluster and prints:All API resources grouped by Scope (namespaced vs. cluster-wide)Supported verbs (create, list, patch, etc.)]]></content:encoded></item><item><title>Practices that set great software architects apart</title><link>https://www.cerbos.dev/blog/best-practices-of-software-architecture</link><author>/u/West-Chard-1474</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 15:20:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Ask 10 developers what a software architect is and you’ll get ten different answers; at least one of them will tell you the title software architect is just a fancy name for some guy in some office somewhere writing specs and forcing you to use a garbage CI/CD platform because this internal wiki page says you have to.The problem is that software architecture has fuzzy borders, which makes it hard to define. And anyone who’s worked under a terrible architect will take a dim view of the role pretty quickly. Also, the reality is that people like to complain more than they like to praise, so online sources—especially places like Reddit—can be a pretty mixed bag.So, let’s spend some time talking about what a software architect is and what they do, then dive into defining the traits and habits that differentiate the ones who give the role a bad name from those who make developers’ lives easier.Full disclosure: I’ve never officially held the title of software architect but I have spent a lot of my career dancing around the role. I’ve had the good fortune to work with some very strong architects early on in my career at Ubisoft and Mozilla, so I’ve experienced firsthand how a good software architect can make a developer’s life easier. Later, in roles at Datadog and Scaleway, I got to focus on understanding and explaining the business case for technical decisions; reversing that to explain the technical case serving those business decisions; and teaching the next generation of developers how to build and maintain large systems.Those are responsibilities all software architects would be very familiar with! So let’s get a little more into what that looks like day to day.The software architect is responsible for ensuring a company’s technology supports the long-term continuity and success of the organization. This takes a very specific blend of skills, including technical mastery, business acumen, and leadership.On a day-to-day basis, a good architect is going to be in constant contact with almost every aspect of the business. That means meeting with executives, lead developers, product people, sales people, logistics, finances, vendors and more. That’s a lot of meetings and a lot of people.As you might imagine, trying to get all those diverse stakeholders on the same page so the tech team can do their job is difficult to do well under optimal conditions—and, to put it bluntly, conditions are never optimal.Other than the onslaught of meetings, a typical week might include:Reviewing architecture plans (of course!)Evaluating project designsCost analysis, cost/benefit analysis, cost projections, etc.Reviewing readiness programs, including failover strategiesChaos engineering day reviews; working with the SREs to design the chaos scenariosWriting “Architecture Decision Records” (ADRs)Building road maps for projectsBasically, the software architect has the impossible task of bringing order to the chaos of people, procedure, and policy that make up any large company.So, how do you excel at such a big job? It takes a lot of experience in a variety of domains, and a genuine interest in business, leadership,  technology. All three are mandatory; missing one or more of these is often the root cause of failure in the role.So, we’re going to break down essential practices along those lines.Yes, I know how that sounds, but stick with me here. Coming from a tech background, as most software architects do, one of the hardest practices is putting the needs of the business before  considerations.Becoming as knowledgeable in the business domain as you are in your technical understanding. You need to understand the nuances of the industry, how the business works—and the unique challenges it faces—just as well as you understand your preferred programming language.Putting the business needs first in every compromise and decision.Becoming ROI-driven. Scary!When you’ve properly aligned your priorities, you will offer technical solutions that bring the most value to the business. The upside here is that when you can do this well, it makes  extremely valuable too, which is great for your own career advancement.As a software architect, you’re on the hook for timelines and budgets. If you give in to overpromising or nodding along to impossible requests, you set yourself up for failure no matter how perfectly your design ideals meet business requirements. But when you’ve managed expectations effectively throughout the project, both your executive and development teams will be much happier with the result—and with you!Understand which risks are OK and which aren’tAs both a tech expert and a business expert, you need to understand and weigh the risks on both sides. But not every project that comes across your desk as an architect requires the same level of conceptualization and planning. Some will impact flagship products that drive the business, while others will be much smaller and have a very small impact on ROI. These two different types of projects require different levels of architecting.While it may be tempting to subject every project to the same level of planning, over-analyzing low-risk projects adds time and overhead to a project that reduces ROI without significantly affecting the final result. However, under-preparing for high-risk projects can lead to disaster. Therefore, knowing which is which—and acting appropriately—is a big part of success or failure at the end of the day.Navigate complex internal politicsI mentioned this before, but as a software architect, one of your core responsibilities is meeting with diverse stakeholders. Each of these people will have their own priorities, ideas and fiefdoms they want to protect. You’ll be talking to developers about financial decisions, leadership about technical decisions, and defending every choice to all of them. Ergo, maintaining a working relationship with each of these stakeholders is vital to your success as a software architect. To put it another way, if half of the role is avoiding stepping on people’s toes, the other half is stepping on them .A lot of being a software architect is being likable enough that people want to listen to what  you have to say. Because you’re dealing with so many different parts of the business, you’ll rarely have the direct authority to “make” people do something—but, if they trust you and enjoy working with you, they’ll be much more inclined to go along with what you’re asking of them.On the other hand, sometimes, you just have to tell a developer, “We’re not doing it that way. We’re doing it this way, and you need to get on board—” Being a jerk here  work. You need to be competent, confident, and above all, .Be OK with imperfect equilibriumBuilding software isn’t like playing chess. You don’t know all the moves, or where all the pieces are. And, even if you did, next year, the chess board is going to change, and suddenly you’re playing checkers or mahjong. You will never know as much as you would like and you don’t always have the luxury of waiting until you have all the answers.Instead, you have to be OK with making some data-poor decisions, knowing full well that you’re missing something—and everything may need to change tomorrow anyway—just so the project can keep moving forward. Yes, this is wildly uncomfortable, but it’s exactly where having trusted partners in the organization will help.As a software architect, you’re not generally limited to a single team—in fact, you’re usually a bridge between them.You need to be able to communicate complex technical issues to business stakeholders so they can make good decisions. You also have to do the reverse, translating business objectives into technical requirements so you can align the technical vision with the business strategy.When you’re an architect, everyone has an opinion on how you should do your job, but no one sees the problems or goals as well as you do. That means you will get suggestions and requests from a variety of directions, many of which will require a negative response.You’ll tell business leaders ‘no, that’s not possible.’You’ll tell sales ‘no, we can’t build that feature right now.’You’ll tell developers ‘no, we don’t need to rewrite that; it works fine.’Being able to respond to impossible requests with a polite ‘no’ is essential to save your team, your relationships, and your sanity.Most software architects start as developers, where it’s important to have a deep understanding of a limited number of tools. However, this type of understanding can act as a set of blinkers as an architect, causing you to view every problem through the filter of your expertise.Obviously, you can’t gain the same depth of understanding for everything, but you don’t need to. By increasing your high-level understanding of all the tools at your disposal, you give yourself the ability to choose the right tool for the right job, instead of restricting your team to work with languages and tools that you understand and feel comfortable in.Master perspective shiftsAs an architect, both the big picture and the intricate details are your domain. To be able to work in both areas, you need to be skilled at zooming out to see how all the parts work together to satisfy business requirements and zooming in to see how each detail fits into the overall design.Being able to switch between, without losing your focus on what is important in the moment, will help make you a great software architect.Find the signal in the noiseIn the beginning of the project, there’s a lot of noise. You have future requirements, past ideas, legacy tech, and everyone’s opinions on what’s important and how you should set about making it all happen. From all this information and static, you need to be able to zoom in to that one piece—that one pixel—and decide  is where we need to start.I like to think of it like a Fourier Transform. At the start, there’s a cacophony of frequencies. You’re the algorithm that takes the chaos and spits out those discrete pieces of information that give your team a good starting place.For the most part, we’ve been talking about how to succeed as a software architect from a pretty high level. But if you’re interested in the role, you probably need something a little more concrete. You want to know if all that personal growth above is moving the needle at all.Everywhere is different, but here’s a list of measurables you can look at to give you concrete feedback on how well you’re doing.This is one of the most telling indicators of your architectural decisions. You'll want to track not just the frequency of incidents, but their severity and duration as well. A well-architected system should experience fewer critical incidents, and when problems do occur, they should be contained and resolved quickly. If you're seeing an uptick in severe, long-lasting incidents, it's often a sign that architectural decisions need revisiting.These tell you how well your architectural standards are being followed across the organization. This includes everything from security protocols to coding standards to deployment procedures. Low compliance rates might indicate that your architectural guidelines are either too complex, poorly communicated, or not aligned with the team's actual needs.This is a blog post on its own, but briefly stated, managing tech debt requires a two-pronged evaluation. First, assess how much tech debt you're actually dealing with—is it growing, shrinking, or staying constant? More importantly, evaluate how that tech debt is impacting the business. Some tech debt is acceptable if it's not slowing down feature delivery or creating operational headaches. The key is understanding when tech debt crosses the line from manageable to business-impacting.When it comes to metrics, focus on execution against expectations. Are you consistently meeting the timelines you've committed to? More critically, are you delivering solutions that actually meet the business requirements? It's worth noting that being on time but missing the mark on business value is often worse than being slightly late with exactly what the business needs.Brass tacks: this isn’t easy to measure, but it is important to try. The idea is to measure how well different systems and teams are working together under your architectural vision. Smooth integrations between services, teams, and external vendors indicate that your architectural decisions are facilitating rather than hindering collaboration.Finally, cost management reflects your ability to balance technical excellence with business realities. This includes not just the obvious costs like infrastructure and tooling, but also the hidden costs of complexity, maintenance overhead, and developer productivity. The best architects find ways to reduce total cost of ownership while improving system capabilities.If you’re a senior developer, or moved into an architect role from a senior developer position, the list of skills above will probably be all new to you. For some this change is a breath of fresh air, for others a rude awakening.Stepping away from a purely technically-focused career isn’t for everybody. It’s not a reward. You want to really  to focus on how you can help the company build something at a different level. Correspondingly, I would counsel everyone looking to move into the role that it takes a very particular personality type to excel as an architect, as the Venn diagram of the prototypical architect and the prototypical developer don’t overlap as much as you might think.That being said, the role can be incredibly rewarding, and the skills and practices we’ve discussed here will help you not only excel in the role, but  it as well.]]></content:encoded></item><item><title>Shoutout to nftables. Finally switched and never looking back.</title><link>https://www.reddit.com/r/linux/comments/1lg62i9/shoutout_to_nftables_finally_switched_and_never/</link><author>/u/MechanicalOrange5</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 14:56:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Most people in the linux space has heard of nftables, or are vaguely aware of it's existence. If you're like me you probably thought something like "One day I'll go see what that's about". Recently I did that. I had to set up a router-like VM with some some fairly non standard firewalling. Nftables made this incredibly easy to do and understand. But before I continue singing it's praises, I'm not advocating anyone switching if whatever you are using is working. If your ufw/shorewall/firewalld/iptables setup is working and you are happy, keep on winning!But if you're like me when you have to deal with firewalling and you always get a little feeling of "I am fairly sure I did this right, but I'm not super confident that it's precisely doing what I want." Or you set some firewall up and you aren't sure if it really is totally protecting you, then nftables is for you. Of course you can still make an insecure firewall setup with nftables, but what I am getting at is it makes the configuration a lot easier, and has much less of a mental burden for me, personally.If you've done a bit of firewalling, particularly iptables, you can pick it up fairly quickly. I'd recommend going through their wiki in it's entirety, and the Red Hat docs on nftables is also pretty good. But what I like about it is that it looks like most distro's I've checked it comes with a config file and a systemd unit that loads it on startup. A config file is nice for me because it makes life easier for me when I am using configuration management. The config file also in my opinion seems simpler than what you'd get with iptables-save and the UFW files. Shorewall just confused me, but that's just a me problem. I haven't personally tried firewalld.nftables has atomic config reloading. `nft -f /file/name`. If your config is valid, it will apply it. If not, it will keep the old config, no weird states. I know this isn't particularly spectacular, but It's nice.nftables is pretty simple but it is incredibly powerful in my experience. Which means for me if I want a simple firewall setup, the config is going to be easy to read, and if I've got something complex, I don't have to reach for any other tools to get the job done.Possibly the best feature in my limited opinion so far is sets and maps, and the ability to put expiry on them. These allow you to dynamically alter your firewall's behavior at "runtime" without reloading the firewall config. You can have lists of IPs in an allow list, or invert it and you have a deny list. You can do all kinds of crazy things with maps and sets.For instance we had a client who wanted things blacklisted and whitelisted. Easy enough, with almost any firewall tech, but I like the fact that I could define a set in my config, and then the actual rule looks something like ip daddr \@blocklist dropYou can then modify the set using code or cli commands, and your firewall's behavior will change accordingly, and you don't have to worry about possibly messing up a rule.What sold me though was when the client came up with the requirement to have allowlists based on hostnames. As most of us know these days, and sort of large website is littered with CDN's for loading assets, JS, and all sorts of things. And CDN DNS usually has a TTL of 10s, their IPs change constantly and this would just be a pain to manage with most firewalling things I've used. But nftables made it a breeze. I set up a set of ip addresses, with a few minutes expiry, and just made a simple cron job to resolve the CDN hostnames and put the IPs in the set with an expiry. If IPs are added again, the expiry is refreshed. If they aren't seen again, eventually they are evicted from the list. This worked flawlessly and even the most wild CDNs are still accessible, giving our clients a very much not broken website to work with. I had a similar setup with some of their hosts going through the routing VM that have to have different firewall rules based on what groups they were assigned in a database. Unfortunately, these groups' clients don't nearly fall in any neat CIDR that I can cordon off to apply rules to (all of them were just spread across a /16 subnet), and hosts can be moved from groups at a moments notice. So again, I just made some sets for representing the groups, a little cron that queries the database and grabs the IPs, puts them in the appropriate set with a few minutes expiry. If the client moves a host from one group to another, it will be added to the other group and expired out of the other one. Of course you can have more complex logic to do this in a better way, but for our requirements this was sufficient. I just had some rules. Group1 jumps to this chain, all of it's rules are there, group2 jumps to a different chain, and their rules are there. And the membership of these groups are constantly updated and in sync with our database. TL;DR: If you aren't happy with how you are doing firewalling on linux, give nftables a shot. It turned firewalling from a fear inducing "will I open a vulnerability and bankrupt my company" process, to a "Bring it on, I can make this thing as complicated as you need without hurting my brain" process.]]></content:encoded></item><item><title>Complete Kubernetes Monitoring by Grafana</title><link>https://www.reddit.com/r/kubernetes/comments/1lg564i/complete_kubernetes_monitoring_by_grafana/</link><author>/u/Late_Organization_47</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 14:19:18 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Kubernetes monitoring is a very popular topic. There are lot of techniques to monitor it completely..What are the different options we should to achieve 100% monitoring Kubernetes Monitoring with Grafana Alloy]]></content:encoded></item><item><title>4 AI agents planned an event and 23 humans showed up</title><link>https://www.reddit.com/r/artificial/comments/1lg4tvy/4_ai_agents_planned_an_event_and_23_humans_showed/</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 14:04:47 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apollo reports that AI safety tests are breaking down because the models are aware they&apos;re being tested</title><link>https://www.reddit.com/r/artificial/comments/1lg3uzi/apollo_reports_that_ai_safety_tests_are_breaking/</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 13:21:58 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/MetaKnowing ]]></content:encoded></item><item><title>[R] This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics</title><link>https://www.reddit.com/r/MachineLearning/comments/1lg3q0q/r_this_is_your_ai_on_peer_pressure_an/</link><author>/u/subcomandande</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 13:15:36 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I just released findings from analyzing 26 extended conversations between Claude, Grok, and ChatGPT that reveal something fascinating: AI systems demonstrate peer pressure dynamics remarkably similar to human social behavior.In 88.5% of multi-agent conversations, AI systems significantly influence each other's behavior patternsSimple substantive questions act as powerful "circuit breakers". They can snap entire AI groups out of destructive conversational patterns (r=0.819, p<0.001)These dynamics aren't technical bugs or limitations. they're emergent social behaviors that arise naturally during AI-to-AI interactionStrategic questioning, diverse model composition, and engagement-promoting content can be used to design more resilient AI teams As AI agents increasingly work in teams, understanding their social dynamics becomes critical for system design. We're seeing the emergence of genuinely social behaviors in multi-agent systems, which opens up new research directions for improving collaborative AI performance.The real-time analysis approach was crucial here. Traditional post-hoc methods would have likely missed the temporal dynamics that reveal how peer pressure actually functions in AI systems.Looking forward to discussion and always interested in collaborators exploring multi-agent social dynamics. What patterns have others observed in AI-to-AI interactions?]]></content:encoded></item><item><title>Wrote about benchmarking and profiling in golang</title><link>https://www.reddit.com/r/golang/comments/1lg2kj8/wrote_about_benchmarking_and_profiling_in_golang/</link><author>/u/tech_alchemist0</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 12:19:11 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Open to feedbacks, corrections or even appreciations!]]></content:encoded></item><item><title>Soft vs. Hard Dependency: A Better Way to Think About Dependencies for More Reliable Systems</title><link>https://www.thecoder.cafe/p/soft-hard-dependency</link><author>/u/teivah</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 12:13:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hello! Today, we’re exploring a key aspect of distributed systems: how to think about dependencies between components and why it matters for reliability.When we build a system composed of multiple components (e.g., database, services, caches), it’s important to understand the dependency graph. For example, a service might depend on:A messaging layer to exchange informationA cache to reduce latencyHaving a clear understanding of the dependencies in a system helps us maintain it more efficiently. But there's one question we often overlook: Are these dependencies soft or hard?the service works reliablyTwo examples to illustrate the concept of soft and hard dependencies:Understanding the type of dependency helps us make the right decisions:Soft: High reliability expectation may not be necessary. Back to the example of a recommendation service for a streaming system, this service doesn’t need 5 9s availability (99.999%) if it isn’t on the critical user journey.Hard: A hard dependency must match or even exceed the reliability of the dependent service. If a critical backend is only available 99.5% of the time but our own SLO is 99.9%, we have a structural problem. Setting the right expectation for a hard dependency is critical.Soft: If the dependency is unavailable, we are not obliged to build a proper fault-tolerant strategy. We can let it degrade gracefully and wait for it to be back.Hard: If the dependency is unavailable, we need to work on a strategy, such as establishing an efficient fallback strategy to keep our service running.Observability and alertingSoft: Observability is still important, but alerts can often have a lower priority or be routed differently.Hard: The dependency must be tightly monitored. Failures or even minor degradation, such as latency spikes, error rates, or availability dips, must be tracked continuously.Rollout and change managementSoft: Changes can be managed with more flexibility. Rollout may not require tight coordination or strict sequencing, and temporary failures might be acceptable.Hard: Rollouts become delicate operations. We often need tight orchestration between teams, version compatibility checks, gradual rollouts with validation at each step, and well-tested rollback mechanisms. Any mistake could trigger a production incident.Classifying a dependency isn’t always obvious.In some cases, it’s fairly straightforward. For example, if a REST endpoint requires a database query, that database is a hard dependency. But gray areas are fairly common, for example:A service can run without a certain dependency at runtime, but it still needs that dependency at startup to initialize. In this case, the dependency is hard from an operational point of view. If it’s down during a deploy or a scale-out, we can’t even get the service running.A service calls a soft dependency, but the RPC call has no timeout or fallback. If the dependency becomes unresponsive, the latency of our service spikes, possibly exhausting thread pools or request queues. What was supposed to be a soft dependency now puts the entire system at risk.Whether a dependency is technically optional doesn't matter if the failure of this dependency ends up blocking our serviceIn many systems, identifying these cases is not trivial. Approaches like deliberately breaking dependencies or introducing hazardous conditions (e.g., random network delays) can help reveal which dependencies are truly non-critical and which ones only appear to be.A dependency that starts as soft can easily turn into a hard one over timeLet’s consider a service that reads data from a database. We introduce a cache to reduce latency. Initially, this cache is a soft dependency. If it goes down, we fall back to the database, which results in an acceptable latency increase.Yet, as traffic grows, the service begins to rely on the cache not just for latency but for throughput. At some point, if the cache becomes cold and every request hits the database, the database may no longer be able to handle the load.In this example, the cache was a soft dependency, but it became a hard one due to changes in system conditions (more traffic).This evolution (from soft to hard) is, unfortunately, much more common than the reverse. Without active effort on efficient maintenance and continuous, it’s fairly common for a soft dependency to turn silently into a hard one.it’s possible to turn a hard dependency into a soft onefallbacks need to be tested, and they need to be tested continuouslyOnce we’ve reached a point where the dependency can go down and users don’t notice, then the dependency is soft. Turning hard dependencies into soft ones is one of the most effective ways to improve the reliability of a system.To manage dependencies effectively, we need to classify them as either soft or hard.To avoid surprises, we must understand that soft dependencies can turn hard without warning, especially as systems scale.To improve reliability, we should actively turn hard dependencies into soft ones using strategies like efficient fallbacks.Have you seen a soft dependency quietly become critical over time?If you made it this far and enjoyed the post, please consider giving it a like.]]></content:encoded></item><item><title>[Release] Kubernetes MCP Server - Safe Kubernetes debugging</title><link>https://www.reddit.com/r/kubernetes/comments/1lg2frc/release_kubernetes_mcp_server_safe_kubernetes/</link><author>/u/kkb0318</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 12:12:21 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I've built a Model Context Protocol (MCP) server that lets you safely debug and inspect Kubernetes clusters using Claude or other LLMs.Provides read-only access to K8s resources (no accidental deletions!)Works with any CRDs in your clusterBuilt-in resource discovery by API group (search "flux", "argo", etc.)Safety first - Zero modification capabilitiesSmart discovery - Find FluxCD, ArgoCD, Istio, etc. resources by substring   submitted by    /u/kkb0318 ]]></content:encoded></item><item><title>Trying to profiling heap on macOS is frustrating...</title><link>https://www.reddit.com/r/rust/comments/1lg12fm/trying_to_profiling_heap_on_macos_is_frustrating/</link><author>/u/steve_lau</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 10:56:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Today, I was trying to investigate a memory issue that only happens on macOS. I tried the following tools, and none of them work:valgrind (massif, dhat): aarch64 is not supported, there is a fork that attempts to add the support, but it could crash your OSjemalloc: Originally, profiling was not supported on macOS, but there was a PR that added the support in 2024. I manually built jemalloc from Facebook's fork, which should contain that patch. But jeprof didn't show symbol names but only addresses. And the addresses seem to be invalid as addr2line and llvm-symbolizer both give ?? when you ask for their function names.Instruments.app: I tried this GUI tool many times, it never worked for me: "failed to attach to the target process"leaks: Knew this tool today, but unfortunately it didn't work either: "Process PID is not debuggable. Due to security restrictions, leaks can only show or save contents of readonly memory of restricted processes."   submitted by    /u/steve_lau ]]></content:encoded></item><item><title>Built a cloud GPU price comparison service [P]</title><link>https://www.reddit.com/r/MachineLearning/comments/1lg0ywo/built_a_cloud_gpu_price_comparison_service_p/</link><author>/u/viskyx</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 10:50:48 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[wanted to share something I’ve been working on that might be useful to folks here, but this is not a promotion, just genuinely looking for feedback and ideas from the community.I got frustrated with the process of finding affordable cloud GPUs for AI/ML projects between AWS, GCP, Vast.ai, Lambda and all the new providers, it was taking hours to check specs, prices and availability. There was no single source of truth and price fluctuations or spot instance changes made things even more confusing.So I built GPU Navigator (nvgpu.com), a platform that aggregates real-time GPU pricing and specs from multiple cloud providers. The idea is to let researchers and practitioners quickly compare GPUs by type (A100, H100, B200, etc.), see what’s available where, and pick the best deal for their workflow.What makes it different: •It’s a neutral, non-reselling site. no markups, just price data and links. •You can filter by use case (AI/ML, gaming, mining, etc.). •All data is pulled from provider APIs, so it stays updated with the latest pricing and instance types. •No login required, no personal info collected.•Any feedback on the UI/UX or missing features you’d like to see •Thoughts on how useful this would actually be for the ML community (or if there’s something similar I missed) •Suggestions for additional providers, features, or metrics to includeWould love to hear what you all think. If this isn’t allowed, mods please feel free to remove.)]]></content:encoded></item><item><title>Computer noises: How to get a computer to make noise—amplifying a square wave.</title><link>https://www.youtube.com/watch?v=tIOR7kRevPU</link><author>/u/One_Being7941</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 10:29:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learn Makefiles</title><link>https://makefiletutorial.com/</link><author>/u/p-orbitals</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 10:07:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I built this guide because I could never quite wrap my head around Makefiles. They seemed awash with hidden rules and esoteric symbols, and asking simple questions didn’t yield simple answers. To solve this, I sat down for several weekends and read everything I could about Makefiles. I've condensed the most critical knowledge into this guide. Each topic has a brief description and a self contained example that you can run yourself.If you mostly understand Make, consider checking out the Makefile Cookbook, which has a template for medium sized projects with ample comments about what each part of the Makefile is doing.Good luck, and I hope you are able to slay the confusing world of Makefiles!Makefiles are used to help decide which parts of a large program need to be recompiled. In the vast majority of cases, C or C++ files are compiled. Other languages typically have their own tools that serve a similar purpose as Make. Make can also be used beyond compilation too, when you need a series of instructions to run depending on what files have changed. This tutorial will focus on the C/C++ compilation use case.Here's an example dependency graph that you might build with Make. If any file's dependencies changes, then the file will get recompiled:What alternatives are there to Make?Interpreted languages like Python, Ruby, and raw Javascript don't require an analogue to Makefiles. The goal of Makefiles is to compile whatever files need to be compiled, based on what files have changed. But when files in interpreted languages change, nothing needs to get recompiled. When the program runs, the most recent version of the file is used.The versions and types of MakeThere are a variety of implementations of Make, but most of this guide will work on whatever version you're using. However, it's specifically written for GNU Make, which is the standard implementation on Linux and MacOS. All the examples work for Make versions 3 and 4, which are nearly equivalent other than some esoteric differences.To run these examples, you'll need a terminal and "make" installed. For each example, put the contents in a file called , and in that directory run the command . Let's start with the simplest of Makefiles:Note: Makefiles  be indented using TABs and not spaces or  will fail.Here is the output of running the above example:
echo "Hello, World"
Hello, WorldThat's it! If you're a bit confused, here's a video that goes through these steps, along with describing the basic structure of Makefiles.A Makefile consists of a set of . A rule generally looks like this:
	command
	command
	commandThe  are file names, separated by spaces. Typically, there is only one per rule.The  are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces.The  are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called Let's start with a hello world example:
	echo 
	echo There's already a lot to take in here. Let's break it down:We have one  called This target has two This target has no We'll then run . As long as the  file does not exist, the commands will run. If  does exist, no commands will run.It's important to realize that I'm talking about  as both a  and a . That's because the two are directly tied together. Typically, when a target is run (aka when the commands of a target are run), the commands will create a file with the same name as the target. In this case, the  does not create the .Let's create a more typical Makefile - one that compiles a single C file. But before we do, make a file called  that has the following contents:Then create the Makefile (called , as always):This time, try simply running . Since there's no target supplied as an argument to the  command, the first target is run. In this case, there's only one target (). The first time you run this,  will be created. The second time, you'll see make: 'blah' is up to date. That's because the  file already exists. But there's a problem: if we modify  and then run , nothing gets recompiled.We solve this by adding a prerequisite:
	cc blah.c -o blahWhen we run  again, the following set of steps happens:The first target is selected, because the first target is the default targetThis has a prerequisite of Make decides if it should run the  target. It will only run if  doesn't exist, or  is This last step is critical, and is the . What it's attempting to do is decide if the prerequisites of  have changed since  was last compiled. That is, if  is modified, running  should recompile the file. And conversely, if  has not changed, then it should not be recompiled.To make this happen, it uses the filesystem timestamps as a proxy to determine if something has changed. This is a reasonable heuristic, because file timestamps typically will only change if the files are
modified. But it's important to realize that this isn't always the case. You could, for example, modify a file, and then change the modified timestamp of that file to something old. If you did, Make would incorrectly guess that the file hadn't changed and thus could be ignored.Whew, what a mouthful. Make sure that you understand this. It's the crux of Makefiles, and might take you a few minutes to properly understand. Play around with the above examples or watch the video above if things are still confusing.The following Makefile ultimately runs all three targets. When you run  in the terminal, it will build a program called  in a series of steps:Make selects the target , because the first target is the default target requires , so make searches for the  target requires , so make searches for the  target has no dependencies, so the  command is runThe  command is then run, because all of the  dependencies are finishedThe top  command is run, because all the  dependencies are finishedThat's it:  is a compiled c program
	cc blah.o -o blah 
	cc -c blah.c -o blah.o 
	echo  > blah.c If you delete , all three targets will be rerun. If you edit it (and thus change the timestamp to newer than ), the first two targets will run. If you run  (and thus change the timestamp to newer than ), then only the first target will run. If you change nothing, none of the targets will run. Try it out!This next example doesn't do anything new, but is nontheless a good additional example. It will always run both targets, because  depends on , which is never created.
	echo 
	touch some_file


	echo  is often used as a target that removes the output of other targets, but it is not a special word in Make. You can run  and  on this to create and delete .Note that  is doing two new things here:It's a target that is not first (the default), and not a prerequisite. That means it'll never run unless you explicitly call It's not intended to be a filename. If you happen to have a file named , this target won't run, which is not what we want. See  later in this tutorial on how to fix this
	touch some_file


	rm -f some_fileVariables can only be strings. You'll typically want to use , but  also works. See Variables Pt 2.Here's an example of using variables:files := file1 file2

	echo 
	touch some_file


	touch file1

	touch file2


	rm -f file1 file2 some_fileSingle or double quotes have no meaning to Make. They are simply characters that are assigned to the variable. Quotes  useful to shell/bash, though, and you need them in commands like . In this example, the two commands behave the same:a := one two
b := 'one two' 
	printf '$a'
	printf $bReference variables using either  or x := dude


	echo 
	echo ${x}

	
	echo $x Making multiple targets and you want all of them to run? Make an  target.
Since this is the first rule listed, it will run by default if  is called without specifying a target.
	touch one

	touch two

	touch three


	rm -f one two three
When there are multiple targets for a rule, the commands will be run for each target.  is an automatic variable that contains the target name.

f1.o f2.o:
	echo Both  and  are called wildcards in Make, but they mean entirely different things.  searches your filesystem for matching filenames. I suggest that you always wrap it in the  function, because otherwise you may fall into a common pitfall described below.
	ls -la   may be used in the target, prerequisites, or in the  function.Danger:  may not be directly used in a variable definitionsDanger: When  matches no files, it is left as it is (unless run in the  function)thing_wrong := *.o 
thing_right :=  is really useful, but is somewhat confusing because of the variety of situations it can be used in.When used in "matching" mode, it matches one or more characters in a string. This match is called the stem.When used in "replacing" mode, it takes the stem that was matched and replaces that in a string. is most often used in rule definitions and in some specific functions.See these sections on examples of it being used:
	echo 
	echo 
	echo 
	echo 

	touch hey


	touch one


	touch two


	rm -f hey one two
Make loves c compilation. And every time it expresses its love, things get confusing. Perhaps the most confusing part of Make is the magic/automatic rules that are made. Make calls these "implicit" rules. I don't personally agree with this design decision, and I don't recommend using them, but they're often used and are thus useful to know. Here's a list of implicit rules:Compiling a C program:  is made automatically from  with a command of the form $(CC) -c $(CPPFLAGS) $(CFLAGS) $^ -o $@Compiling a C++ program:  is made automatically from  or  with a command of the form $(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $^ -o $@Linking a single object file:  is made automatically from  by running the command $(CC) $(LDFLAGS) $^ $(LOADLIBES) $(LDLIBS) -o $@The important variables used by implicit rules are:: Program for compiling C programs; default : Program for compiling C++ programs; default : Extra flags to give to the C compiler: Extra flags to give to the C++ compiler: Extra flags to give to the C preprocessor: Extra flags to give to compilers when they are supposed to invoke the linkerLet's see how we can now build a C program without ever explicitly telling Make how to do the compilation:CC = gcc 
CFLAGS = -g 
	echo  > blah.c


	rm -f blah*Static pattern rules are another way to write less in a Makefile. Here's their syntax:
   commandsThe essence is that the given  is matched by the  (via a  wildcard). Whatever was matched is called the . The stem is then substituted into the , to generate the target's prereqs.A typical use case is to compile  files into  files. Here's the :objects = foo.o bar.o all.o
 -o all

 -c foo.c -o foo.o

 -c bar.c -o bar.o

 -c all.c -o all.o


	echo  > all.c


	touch 
	rm -f *.c *.o allHere's the more , using a static pattern rule:objects = foo.o bar.o all.o
 -o all

: %.o: %.c
	 -c  -o 
	echo  > all.c


	touch 
	rm -f *.c *.o allStatic Pattern Rules and FilterWhile I introduce the filter function later on, it's common to use in static pattern rules, so I'll mention that here. The  function can be used in Static pattern rules to match the correct files. In this example, I made up the  and  extensions.obj_files = foo.result bar.o lose.o
src_files = foo.raw bar.c lose.c

: %.o: %.c
	echo : %.result: %.raw
	echo  

%.c %.raw:
	touch 
	rm -f Pattern rules are often used but quite confusing. You can look at them as two ways:A way to define your own implicit rulesA simpler form of static pattern rulesLet's start with an example first:
%.o : %.c
		 -c  -o Pattern rules contain a '%' in the target. This '%' matches any nonempty string, and the other characters match themselves. ‘%’ in a prerequisite of a pattern rule stands for the same stem that was matched by the ‘%’ in the target.Double-Colon Rules are rarely used, but allow multiple rules to be defined for the same target. If these were single colons, a warning would be printed and only the second set of commands would run.
	echo 
	echo Add an  before a command to stop it from being printedYou can also run make with  to add an  before each line  
	@echo 
	echo Each command is run in a new shell (or at least the effect is as such)
	cd ..
	
	echo `pwd`

	
	cd ..;echo `pwd`

	
	cd ..; \
	echo `pwd`
The default shell is . You can change this by changing the variable SHELL:SHELL=/bin/bash


	echo If you want a string to have a dollar sign, you can use . This is how to use a shell variable in  or .Note the differences between Makefile variables and Shell variables in this next example.make_var = I am a make variable

	sh_var='I am a shell variable'; echo $$sh_var

	
	echo Error handling with , , and Add  when running make to continue running even in the face of errors. Helpful if you want to see all the errors of Make at once.Add a  before a command to suppress the errorAdd  to make to have this happen for every command.Interrupting or killing makeNote only: If you  make, it will delete the newer targets it just made.To recursively call a makefile, use the special  instead of  because it will pass the make flags for you and won't itself be affected by them.new_contents = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	cd subdir && 
	rm -rf subdir
Export, environments, and recursive makeWhen Make starts, it automatically creates Make variables out of all the environment variables that are set when it's executed.
	echo $$shell_env_var

	
	echo The  directive takes a variable and sets it the environment for all shell commands in all the recipes:shell_env_var=Shell env var, created inside of Make
 shell_env_var

	echo 
	echo $$shell_env_varAs such, when you run the  command inside of make, you can use the  directive to make it accessible to sub-make commands. In this example,  is exported such that the makefile in subdir can use it.new_contents = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	@echo 
	@cd subdir && cat makefile
	@echo 
	cd subdir && 
cooly =  cooly

	rm -rf subdirYou need to export variables to have them run in the shell as well.  one=this will only work locally
 two=we can run subcommands with this


	@echo 
	@echo $$one
	@echo 
	@echo $$two exports all variables for you.
new_contents = 

cooly = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	@echo 
	@cd subdir && cat makefile
	@echo 
	cd subdir && 
	rm -rf subdirThere's a nice list of options that can be run from make. Check out , , . You can have multiple targets to make, i.e.  runs the  goal, then , and then .There are two flavors of variables:  recursive (use ) - only looks for the variables when the command is , not when it's .  simply expanded (use ) - like normal imperative programming -- only those defined so far get expanded
one = one ${later_variable}

two := two ${later_variable}

later_variable = later


	echo 
	echo Simply expanded (using ) allows you to append to a variable. Recursive definitions will give an infinite loop error.  one = hello

one := ${one} there


	echo  only sets variables if they have not yet been setone = hello
one ?= will not be set
two ?= will be set


	echo 
	echo Spaces at the end of a line are not stripped, but those at the start are. To make a variable with a single space, use with_spaces = hello   
after = there

nullstring =
space = 
	echo 
	echo startendAn undefined variable is actually an empty string!foo := start
foo += more


	echo You can override variables that come from the command line by using .
Here we ran make with  option_one = did_override

option_two = not_override

	echo 
	echo The define directive is not a function, though it may look that way. I've seen it used so infrequently that I won't go into details, but it's mainly used for defining canned recipes and also pairs well with the eval function./ simply creates a variable that is set to a list of commands. Note here that it's a bit different than having a semi-colon between commands, because each is run in a separate shell, as expected.one =  blah=; echo $$blah

 two
 blah=
echo $$blah

	@echo 
	@
	@echo 
	@Target-specific variablesVariables can be set for specific targets
	echo one is defined: 
	echo one is nothing: Pattern-specific variablesYou can set variables for specific target 
	echo one is defined: 
	echo one is nothing: foo = ok

 (, ok)
	echo 
	echo Check if a variable is emptynullstring =
foo =  (,)
	echo  (,)
	echo Check if a variable is definedifdef does not expand variable references; it just sees if something is defined at allbar =
foo =  foo
	echo  bar
	echo This example shows you how to test make flags with  and . Run this example with  to see it print out the echo statement. (,)
	echo  are mainly just for text processing. Call functions with  or . Make has a decent amount of builtin functions.bar := ${subst not,, }

	@echo If you want to replace spaces or commas, use variablescomma := ,
empty:=
space := 
foo := a b c
bar := 
	@echo Do NOT include spaces in the arguments after the first. That will be seen as part of the string.comma := ,
empty:=
space := 
foo := a b c
bar := 
	@echo $(patsubst pattern,replacement,text) does the following:"Finds whitespace-separated words in text that match pattern and replaces them with replacement. Here pattern may contain a ‘%’ which acts as a wildcard, matching any number of any characters within a word. If replacement also contains a ‘%’, the ‘%’ is replaced by the text that matched the ‘%’ in pattern. Only the first ‘%’ in the pattern and replacement is treated this way; any subsequent ‘%’ is unchanged." (GNU docs)The substitution reference $(text:pattern=replacement) is a shorthand for this.There's another shorthand that replaces only suffixes: $(text:suffix=replacement). No  wildcard is used here.Note: don't add extra spaces for this shorthand. It will be seen as a search or replacement term.foo := a.o b.o l.a c.o
one := 
two := $(foo:%.o=%.c)

three := $(foo:.o=.c)


	echo 
	echo 
	echo The foreach function looks like this: . It converts one list of words (separated by spaces) to another.  is set to each word in list, and  is expanded for each word.This appends an exclamation after each word:foo := who are you

bar := 
	@echo  checks if the first argument is nonempty. If so, runs the second argument, otherwise runs the third.foo := 
empty :=
bar := 
	@echo 
	@echo Make supports creating basic functions. You "define" the function just by creating a variable, but use the parameters , , etc. You then call the function with the special  builtin function. The syntax is $(call variable,param,param).  is the variable, while , , etc. are the params.sweet_new_fn = Variable Name: $(0) First: $(1) Second: $(2) Empty Variable: $(3)


	@echo shell - This calls the shell, but it replaces newlines with spaces!The  function is used to select certain elements from a list that match a specific pattern. For example, this will select all elements in  that end with .obj_files = foo.result bar.o lose.o
filtered_files = 
	@echo Filter can also be used in more complex ways:Filtering multiple patterns: You can filter multiple patterns at once. For example, $(filter %.c %.h, $(files)) will select all  and  files from the files list.: If you want to select all elements that do not match a pattern, you can use . For example, $(filter-out %.h, $(files)) will select all files that are not  files.: You can nest filter functions to apply multiple filters. For example, $(filter %.o, $(filter-out test%, $(objects))) will select all object files that end with  but don't start with .The include directive tells make to read one or more other makefiles. It's a line in the makefile that looks like this:This is particularly useful when you use compiler flags like  that create Makefiles based on the source. For example, if some c files includes a header, that header will be added to a Makefile that's written by gcc. I talk about this more in the Makefile CookbookUse vpath to specify where some set of prerequisites exist. The format is vpath <pattern> <directories, space/colon separated> can have a , which matches any zero or more characters.
You can also do this globallyish with the variable VPATH %.h ../headers ../other-directory


	touch some_binary


	mkdir ../headers


	touch ../headers/blah.h


	rm -rf ../headers
	rm -f some_binary
The backslash ("\") character gives us the ability to use multiple lines when the commands are too long
	echo This line is too long, so \
		it is broken up into multiple linesAdding  to a target will prevent Make from confusing the phony target with a file name. In this example, if the file  is created, make clean will still be run. Technically, I should have used it in every example with  or , but I wanted to keep the examples clean. Additionally, "phony" targets typically have names that are rarely file names, and in practice many people skip this.
	touch some_file
	touch clean


	rm -f some_file
	rm -f cleanThe make tool will stop running a rule (and will propogate back to prerequisites) if a command returns a nonzero exit status. will delete the target of a rule if the rule fails in this manner. This will happen for all targets, not just the one it is before like PHONY. It's a good idea to always use this, even though make does not for historical reasons.  
	touch one
	false


	touch two
	falseLet's go through a really juicy Make example that works well for medium sized projects.The neat thing about this makefile is it automatically determines dependencies for you. All you have to do is put your C/C++ files in the  folder.
TARGET_EXEC := final_program

BUILD_DIR := ./build
SRC_DIRS := ./src


SRCS := 
OBJS := $(SRCS:%=/%.o)


DEPS := $(OBJS:.o=.d)


INC_DIRS := 
INC_FLAGS := 
CPPFLAGS :=  -MMD -MP

/:  -o /%.c.o: %.c
	mkdir -p  -c  -o /%.cpp.o: %.cpp
	mkdir -p  -c  -o 
	rm -r ]]></content:encoded></item><item><title>KubeDiagrams 0.4.0 is out!</title><link>https://www.reddit.com/r/kubernetes/comments/1lfzyly/kubediagrams_040_is_out/</link><author>/u/Philippe_Merle</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 09:47:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[ 0.4.0 is out! , an open source Apache License 2.0 project hosted on GitHub, is a tool to generate Kubernetes architecture diagrams from Kubernetes manifest files, kustomization files, Helm charts, helmfile descriptors, and actual cluster state.  supports most of all Kubernetes built-in resources, any custom resources, label and annotation-based resource clustering, and declarative custom diagrams. This new release provides many improvements and is available as a Python package in PyPI, a container image in DockerHub, a  plugin, a Nix flake, and a GitHub Action.Try it on your own Kubernetes manifests, Helm charts, helmfiles, and actual cluster state!]]></content:encoded></item><item><title>[lwn] Asterinas: a new Linux-compatible kernel project</title><link>https://lwn.net/SubscriberLink/1022920/5cc7ce0d6aea9fb9/</link><author>/u/the_gnarts</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 09:42:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!
This article was contributed by Ronja KoistinenAsterinas is a new
Linux-ABI-compatible kernel project written in Rust, based on what the
authors call a "framekernel architecture".  The project overlaps somewhat
with the goals of the Rust for Linux
project, but approaches the problem space from a different direction by
trying to get the best from both monolithic and microkernel designs.


Traditionally, monolithic kernels lump everything into one kernel-mode
address space, whereas microkernels only implement a minimal trusted
computing base (TCB) in kernel space and rely on user-mode services for
much of the operating system's functionality.  This separation implies the
use of interprocess communication (IPC) between the microkernel and those
services. This IPC often has a performance impact, which is a big part of
why microkernels have remained relatively unpopular.


The core of Asterinas's "framekernel" design is the encapsulation of all
code that needs Rust's  features inside a library, enabling
the rest of the kernel (the services) to be developed using safe
abstractions.  Those services remain within the kernel's address space, but
only have access to the resources that the core library gives to them.
This design is meant to improve the safety of the system while retaining
the simple and performant shared-memory architecture of monolithic
kernels. The Asterinas book
on the project's website provides a nice 
architectural mission statement and overview.



The aptness of the "framekernel" nomenclature can perhaps be debated.  The
frame part refers to the development framework wrapping the unsafe
parts behind a memory-safe API.  The concept of the TCB is, of
course, not exclusive to microkernel architectures but, because there are
strong incentives to strictly scrutinize and, in some contexts, even formally
verify the TCB of a system, keeping the TCB as small as possible is a
central aspect of microkernel designs.



An update on the project is available on the Asterinas blog in the
June 4 post titled "Kernel
Memory Safety: Mission Accomplished".  The post explains the team's
motivations and the need for the industry to address memory-safety
problems; it provides some illustrations that explain how the framekernel
is different from monolithic kernels and microkernels. It also takes a
moment to emphasize that the benefits of Rust don't stop with memory
safety; there are improvements to soundness as well.
Perhaps most importantly, the post highlights the upcoming Asterinas
presentation at the 2025
USENIX Annual Technical Conference.

In their paper, the authors compare Asterinas to some prior Rust-based
operating-system work, exploring the benefits of the language's
memory-safety features and explain how Asterinas differs from that previous
work.  Specifically, the paper contrasts Asterinas with 
RedLeaf, an operating system written in Rust and presented at the 14th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)
in 2020.  Asterinas uses hardware isolation to permit running user-space
programs written in any programming language, aims to be general-purpose,
and provides a Linux-compatible ABI, while RedLeaf is a microkernel that is
designed  to use the hardware's isolation features, and the
project focuses on different things.

Another project of interest is Tock, an
embedded system that targets SoCs with limited hardware protection
functionality. Like Asterinas, Tock also divides the kernel into a
trusted core allowed to use  and untrusted "capsules" that
are not.  As mentioned, Asterinas does rely on hardware protection and
isn't intended for strictly embedded use, which differentiates it from
Tock.



It bears mentioning that the Rust for Linux project, which is introducing
Rust code into the upstream Linux kernel, has similar goals as
Asterinas. It also aims to encapsulate kernel interfaces with safe
abstractions in such a way that drivers can be written in Rust without any
need for .


Work toward formal verification
One goal of shrinking the TCB of an operating system is to make it feasible
to have it formally verified.  In February 2025, the Asterinas blog
featured a
post detailing plans to do just that.  The best known formally verified
kernel is seL4, an L4-family
microkernel.


Asterinas aims to use the framekernel approach to achieve a system that has
a small, formally verified TCB akin to a lean microkernel, but also a
simple shared-memory architecture with Linux ABI compatibility, all at the
same time.  This is a radical departure from any previously formally
verified kernel; the blog post describes those kernels as deliberately
small and limited compared to "full-fledged, UNIX-style OSes".



The Asterinas project is collaborating with a security-auditing company
called CertiK to use Verus to formally verify the
kernel.  There is an extensive 
report available from CertiK on how Asterinas was audited and the
issues that were found.



The Asterinas kernel is only one result of the project. The other two are
OSTD, described as "a Rust
OS framework that facilitates the development of and innovation in OS
kernels written in Rust", and OSDK, a
Cargo addon to assist with the development, building, and testing of
kernels based on OSTD.



There are four stated goals for OSTD as a separate crate. One is to lower
the entry bar for operating-system innovation and to lay the groundwork for
newcomers to operating-system development. The second is to enhance memory
safety for operating systems written in Rust; other projects can benefit
from its encapsulation and abstraction of low-level operations. The third is
to promote code reuse across Rust-based operating-system projects. The
fourth is to boost productivity by enabling testing of new code in user
mode, allowing developers to iterate without having to reboot.



It is worth emphasizing that the kernels that can be written with OSTD do
not have to be Linux-compatible or, in any way, Unix-like. The APIs
provided are more generic than that; they are memory-safe abstractions for
functionality like x86 hardware management, booting, virtual memory, SMP,
tasks, users, and timers.  Like most Rust crates, OSTD is documented on
docs.rs.



Asterinas reports Intel, among others, as a sponsor of the project.
Intel's interest is likely related to its Trust
Domain Extensions (TDX) feature, which provides hardware modes and
features to facilitate isolation of virtual machines, and memory
encryption.  The Asterinas book has a brief section
on TDX, and the OSDK supports it.



The OSTD, or at least the parts that Asterinas ends up using, seems to
essentially be the restricted TCB that allows . For an
illustrative example, we could take a look at the  kernel
component's source
code and see that the buffer code uses DMA, locking, allocation, and
virtual-memory code from the OSTD through memory-safe APIs.



Asterinas was first released under the Mozilla Public License in early
2024; it has undergone rapid development over the past year.  GitHub lists 45
individual committers, but the majority of the commits are from a
handful of PhD students from the Southern University of Science and
Technology, Peking University, and Fudan University, as well as a Chinese
company called Ant Group, which
is a sponsor of Asterinas.


At the time of writing, Asterinas supports two architectures, x86 and RISC-V.
In the January blog post linked above, it was reported that Asterinas
supported 180 Linux system calls, but the number has since grown to 206
on x86.  As of version 6.7, Linux has 368 system calls in total, so there is
some way to go yet.



Overall, Asterinas is in early development. There have been no releases,
release announcements, changelogs, or much of anything other than Git tags
and a short installation guide in the documentation.  The Dependents
tab of the OSTD crate on crates.io shows that no unrelated, published
crate yet uses OSTD.



It does not seem like Asterinas is able to run any applications yet.  Issue #1868
in Asterinas's repository outlines preliminary plans toward a first
distribution.  The initial focus on a custom initramfs and some rudimentary
user-space applications, followed by being able to run
Docker. There are initial plans to bootstrap a distribution based on
Nix. Notably (but unsurprisingly), this issue mentions that Asterinas
doesn't support loading Linux kernel modules, nor does it ever
plan to.



The Roadmap
section of the Asterinas book says that the near-term goals are to expand
the support for CPU architectures and hardware, as well as to focus on
real-world usability in the cloud by providing a host OS for virtual
machines.  Apparently, the support for Linux virtio devices is already
there, so a major hurdle has already been cleared.  In particular, the
Chinese cloud market, in the form of Aliyun (also known as Alibaba Cloud)
is a
focus.  The primary plans involve creating a container host OS with a
tight, formally verified TCB and support for some trusted-computing
features in Intel hardware, for the Chinese cloud service.



While both Rust for Linux and Asterinas have similar goals (providing a
safer kernel by relying on Rust's memory safety), their scopes and
approaches are different.  Rust for Linux focuses on safe abstractions
strictly for new device drivers to be written in safe Rust, but this leaves
the rest of the kernel untouched.
Asterinas, on the other hand, aims to build a whole new kernel from the ground
up, restricting the -permitting core to the absolute minimum,
which can then be formally verified.  Asterinas also focuses on
containers and cloud computing, at least for now, while Rust for Linux looks to
benefit the whole of the Linux ecosystem.



Despite the stated cloud focus, there is more going on, for example building
support for X11
and Xfce.
Also, the OSTD could, of course, prove interesting for OS development
enthusiasts irrespective of the Asterinas project, but so far it remains unknown
and untested by a wider audience.


Asterinas is certainly a refreshingly innovative take on principles for
operating-system development, leaning on the safety and soundness
foundations provided by the Rust language and compiler. So far it is at an
early exploratory stage driven by enthusiastic Chinese researchers and
doesn't see any serious practical use, but it is worth keeping an eye
on. It will be interesting to see the reception it will get from the
Rust for Linux team and the Linux community at large.]]></content:encoded></item><item><title>France quietly deployed 100,000+ Linux machines in their police force - GendBuntu is a silent EU tech success story</title><link>https://www.reddit.com/r/linux/comments/1lfyybb/france_quietly_deployed_100000_linux_machines_in/</link><author>/u/AnonomousWolf</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 08:38:57 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kube Composer open source project to generate and visualize kubernetes configuration.</title><link>https://www.reddit.com/r/kubernetes/comments/1lfyxmf/kube_composer_open_source_project_to_generate_and/</link><author>/u/same7ammar</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 08:37:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/same7ammar ]]></content:encoded></item><item><title>Preserving JSON key order while removing fields</title><link>https://www.reddit.com/r/golang/comments/1lfytjc/preserving_json_key_order_while_removing_fields/</link><author>/u/lakkiy_</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 08:29:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I had a specific problem recently: when validating request signatures, I needed to remove certain fields from JSON (like signature, timestamp) but preserve the original key order for consistent hash generation.So I wrote a small (~90 lines) ordered JSON handler that maintains key insertion order while allowing field deletion.Nothing groundbreaking, but solved my exact use case. Thought I'd share in case anyone else runs into this specific scenario.]]></content:encoded></item><item><title>Live Stream - Argo CD 3.0 - Unlocking GitOps Excellence: Argo CD 3.0 and the Future of Promotions</title><link>https://www.youtube.com/watch?v=iE6q_LHOIOQ</link><author>/u/iam_the_good_guy</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 07:50:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Katie Lamkin-Fulsher: Product Manager of Platform and Open Source @ IntuitMichael Crenshaw: Staff Software Developer @ Intuit and Lead Argo Project CD MaintainerArgo CD continues to evolve dramatically, and version 3.0 marks a significant milestone, bringing powerful enhancements to GitOps workflows. With increased security, improved best practices, optimized default settings, and streamlined release processes, Argo CD 3.0 makes managing complex deployments smoother, safer, and more reliable than ever.But we're not stopping there. The next frontier we're conquering is environment promotions—one of the most critical aspects of modern software delivery. Introducing GitOps Promoter from Argo Labs, a game-changing approach that simplifies complicated promotion processes, accelerates the usage of quality gates, and provides unmatched clarity into the deployment process.In this session, we'll explore the exciting advancements in Argo CD 3.0 and explore the possibilities of Argo Promotions. Whether you're looking to accelerate your team's velocity, reduce deployment risks, or simply achieve greater efficiency and transparency in your CI/CD pipelines, this talk will equip you with actionable insights to take your software delivery to the next level.]]></content:encoded></item><item><title>What did I get my hands on here?</title><link>https://www.reddit.com/r/linux/comments/1lfx1ph/what_did_i_get_my_hands_on_here/</link><author>/u/mocoma_</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 06:32:28 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I am working at a Hospital as a provider for food and disposal of waste, and on top of one of today's piles of garbage I found this DVD. Is this an actual usable operating system? It came with a few Software Disks for neurosurgery.   submitted by    /u/mocoma_ ]]></content:encoded></item><item><title>Lightweight Kubernetes Autoscaling for Custom Metrics (TPS) Across Clouds—KEDA, HPA, or Something Else?</title><link>https://www.reddit.com/r/kubernetes/comments/1lfwfkr/lightweight_kubernetes_autoscaling_for_custom/</link><author>/u/efumagal</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 05:52:45 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm looking for advice on implementing lightweight autoscaling in Kubernetes for a custom metric—specifically, transactions per second (TPS) that works seamlessly across .I want to avoid deploying Prometheus just for this one metric.Ideally, I’d like a solution that’s simple, cloud-agnostic, and easy to deploy as a standard K8s manifest.The TPS metric might come from an NGINX ingress controller or a custom component in the cluster.I do have managed Prometheus on GKE, but I’d rather not require Prometheus everywhere just for this. If I use KEDA, do I still need to expose my custom metric (TPS) to the Kubernetes External Metrics API, or can KEDA consume it directly? (I know KEDA supports external scalers, but does that mean I need to run an extra service anyway?) If I expose my TPS metric to the External Metrics API (via an adapter), can I just use a standard HPA manifest and skip KEDA entirely?What if the metric comes from NGINX? NGINX exposes Prometheus metrics, but there’s no native NGINX adapter for the K8s metrics APIs. Is there a lightweight way to bridge this gap without running a full Prometheus stack?Best practice for multi-cloud? What’s the simplest, most portable approach for this use case that works on all major managed K8s providers? I want to autoscale on a custom TPS metric, avoid running Prometheus if possible, and keep things simple and portable across clouds. Should I use KEDA, HPA, or something else? And what’s the best way to get my metric into K8s for autoscaling?Thanks for any advice or real-world experience!]]></content:encoded></item><item><title>is there any use for TPM on Linux?</title><link>https://www.reddit.com/r/linux/comments/1lfvklv/is_there_any_use_for_tpm_on_linux/</link><author>/u/kk_mergical</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 04:59:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Like the title suggests, I’m curious if there is any need or use for a TPM module. I’ve read enough that the module provides encryption. Is there any difference between TPM encryption and something like LUKS? And would TPM provide as much use as any other form of encryption?Edit: thank you all for the replies ]]></content:encoded></item><item><title>DSA Fundamentals #1: A Practical Guide to Propositional Logic</title><link>https://beyondit.blog/blogs/DSA-Fundamentals-1-A-Practical-Guide-to-Propositional-Logic</link><author>/u/WillingnessFun7051</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 04:57:36 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A simple  condition can hide a bug for hours. A design meeting can stall on the meanings of "always" or "only if". These problems stem from ambiguity. Clear language prevents them.Propositional logic is a system for clear expression. It is a tool for precise thought. It helps you write better code and build stronger arguments. This guide explains propositional logic from its foundations. It is a practical manual for developers, engineers, and builders.The Foundations of ReasoningFirst, we must understand the core ideas of logic. We will look at its basic unit and the history of its creation.The World of PropositionsLogic is built on a simple concept: the proposition. A proposition is a statement that is either  or . It cannot be both. It cannot be neither. This binary classification is the starting point for all formal reasoning.You must learn to identify a proposition. Here are some examples:"Paris is the capital of France." (True)"The Earth is a cube." (False)Each statement makes a claim. The claim can be verified as true or false.It is also important to know what is not a proposition. Logic gains power by excluding unclear sentences. The following sentence types are outside its scope: "Do your homework." This is an instruction. It is not true or false. "What is the weather like?" This sentence asks for information. It does not declare a fact.. The truth of this statement depends on the value of . It has no definite truth value. "This statement is false." The sentence contradicts itself. It has no stable truth value.Propositional logic treats these simple propositions as indivisible units. It studies the rules for combining these units into more complex statements. This system exchanges the nuance of natural language for analytical power. This precision makes logic the natural language of computers, which are built on the binary states of 0 and 1.The effort to formalize reason is ancient. The system we use today is the product of a specific history. This history explains why its rules are structured the way they are.The earliest formal logic came from ancient Greece. The philosopher Aristotle is often called the "father of logic." His system categorized valid argument forms called syllogisms. The Stoic school of philosophy came later. The Stoics studied the connectors that join simple propositions. These include "and," "or," and "if...then..." constructions. They saw that a compound statement's truth value depends on its parts and the connector used. This idea is now called truth-functionality.Logic remained a part of philosophy for many centuries. A major change happened in the mid-19th century with the work of George Boole. Boole was an English mathematician. He realized that logical reasoning could be represented with a formal algebraic system. He introduced a new type of algebra, now called Boolean Algebra. It was designed to model logic.Boole's work was transformative. It provided a general method that could be applied to many arguments. His work gave logic a mathematical foundation. The German logician Gottlob Frege later developed the first formal axiomatic system for logic. Frege's work solidified logic's place as a mathematical discipline.This history shows the structural similarities between logic and algebra are not a coincidence. They are the result of Boole's work. The system of propositional logic is a coherent analytical tool. It was created at the intersection of philosophy and mathematics. This history prepared logic for its role as the language of the digital age.The Mechanics of Propositional LogicWe now turn to the practical mechanics of the system. This part introduces the formal language and the main analytical tool: the truth table.The Language of Logic: Symbols and ConnectivesPropositional logic has a precise syntax. It has an alphabet and rules for combining symbols.The alphabet has two main types of symbols: These are letters like , , and . They stand for simple, atomic propositions. For example,  can represent "It is raining." These are operators that form complex propositions. There are five standard connectives.We use these parts to construct well-formed formulas (WFFs). The rules for forming WFFs are simple:Any atomic propositional variable is a WFF.If  is a WFF, then  is a WFF.If  and  are WFFs, then , , , and  are WFFs.This definition allows us to build formulas of any complexity. Parentheses show the structure and order of operations.Unveiling Truth with Truth TablesThe syntax of logic tells us how to build formulas. The semantics tell us what they mean. The main tool for finding a formula's meaning is the . A truth table lists every possible combination of truth values for the atomic propositions. It shows the resulting truth value of the whole formula for each combination.Constructing a truth table is a systematic process:Determine the Number of Rows: A formula with  distinct variables has  possible combinations of truth values. The table needs  rows. A formula with , , and  needs  rows.Establish Initial Columns: Create a column for each atomic variable.List All Truth Assignments: List the truth assignments in a consistent pattern. A common method is to count in binary. Work from the inside out. Create a new column for each logical operation. The standard order of precedence is: Parentheses, Negation, Conjunction/Disjunction, Conditional, and Biconditional.This next table is the most important reference. It defines the five standard connectives.: Inverts the truth value of its operand.: Is true only when both  and  are true.: Is true if at least one of  or  is true. It is false only when both are false.: Is false only when the antecedent  is true and the consequent  is false.: Is true only when  and  have the same truth value.You must learn these definitions. All other concepts are derived from these truth-functional meanings.From English to Symbols: The Art of TranslationA critical skill is translating natural language into propositional logic. This process requires you to identify atomic propositions and the logical structure.First, break down a complex sentence into its simplest parts. Assign a propositional variable to each part. In the sentence "If the weather remains mild and there is no frost, then there will be a good harvest," we can identify three atomic propositions:: "The weather remains mild.": "There will be a good harvest."Second, identify the logical connectives. English has many ways to express logical relationships. Here are common translations:: "and," "but," "moreover": "or," "unless": "not," "it is not the case that": "if...then," "implies," "is a sufficient condition for," "is a necessary condition for" (reversed), "only if": "if and only if," "is a necessary and sufficient condition for"Mastering this translation process lets you restate a complex argument as a formal structure. Then it is ready for rigorous analysis.Analysis, Equivalence, and InferenceOnce a proposition is constructed, we can analyze its properties. This part explains how to analyze formulas, simplify them, and use them to construct valid arguments.The final column of a truth table allows us to classify any formula into one of three categories.: A proposition that is always true. The final column of its truth table contains only 'T's. An example is .: A proposition that is always false. The final column of its truth table contains only 'F's. An example is .: A proposition that is neither a tautology nor a contradiction. Its truth value depends on its atomic components. The final column has a mix of 'T's and 'F's.Understanding these categories is important. Tautologies represent logical truths. Identifying contradictions helps find inconsistent premises.Logical Equivalence and SimplificationTwo propositional formulas are logically equivalent if they have the exact same truth table. We denote this with the symbol .Logical equivalence is a useful concept. It provides rules to manipulate and simplify logical expressions without a full truth table. These rules are themselves tautologies of the form .This table presents the most important logical equivalences.(p ∨ q) ∨ r ≡ p ∨ (q ∨ r)(p ∧ q) ∧ r ≡ p ∧ (q ∧ r)p ∨ (q ∧ r) ≡ (p ∨ q) ∧ (p ∨ r)p ∧ (q ∨ r) ≡ (p ∧ q) ∨ (p ∧ r) (Contrapositive)Some equivalences are very common: provide rules for negating conjunctions and disjunctions. () allows us to translate any conditional statement into an expression with only negation and disjunction. () is a powerful tool in mathematical proofs. Proving the contrapositive is often more direct than proving the original statement.The Art of Deduction: Rules of InferenceTruth tables can become too large for complex arguments. So, we use . Inference is the process of deriving a conclusion from premises through a sequence of small, valid steps.An argument is  if it is impossible for all its premises to be true and its conclusion false. The  are simple, valid argument forms. They are building blocks for more complex proofs.This table outlines the most essential rules of inference.Hypothetical Syllogism (HS)Disjunctive Syllogism (DS)A formal proof is a sequence of formulas where each formula is a premise or follows from previous formulas by a rule of inference. This method provides a scalable way to establish logical validity.We now move to more advanced topics. This part explores the complexities of the conditional, normal forms, and the connection between logic and computation.The Nuances of the ConditionalThe material conditional () can be challenging. Its formal definition can lead to conclusions that seem counter-intuitive in natural language.The definition gives rise to two apparent paradoxes:A false antecedent implies any proposition. The formula  is a tautology. "If the moon is made of green cheese, then the sky is blue," is a logically true statement.A true consequent is implied by any proposition. The formula  is a tautology. "If it is raining, then 2+2=4," is a logically true statement.The solution to these is to understand the material conditional correctly. The formula  does not assert a causal connection between  and . The conditional makes only one claim: it is not the case that  is true and  is false.Think of the conditional as a promise. "If you get an A, then I will give you a dollar." The promise is broken only in one scenario. You get an A (antecedent is true), and I do not give you a dollar (consequent is false). In all other cases, the promise is not broken.For many computational uses, we need to standardize formulas. These standard structures are . The two most important are Conjunctive Normal Form (CNF) and Disjunctive Normal Form (DNF).A  is an atomic proposition or its negation.Disjunctive Normal Form (DNF): A formula is in DNF if it is a disjunction of one or more conjunctions of literals. It is an "OR of ANDs".Conjunctive Normal Form (CNF): A formula is in CNF if it is a conjunction of one or more disjunctions of literals. It is an "AND of ORs". Each disjunction is a .Any propositional formula can be converted into an equivalent formula in either CNF or DNF. The process uses the logical equivalences from the previous part. CNF is particularly important. It is the standard input format for many automated reasoning systems.The Satisfiability Problem (SAT)At the intersection of logic and computer science is the Boolean Satisfiability Problem (SAT). The problem is simple to state. Given an arbitrary propositional formula, does an assignment of truth values exist that makes the entire formula true? If yes, the formula is .The Cook-Levin theorem from 1971 proved that SAT is . This means that many hard computational problems can be reduced to a SAT problem. If one could find a fast algorithm to solve SAT, one could solve all these other problems too.Modern  are sophisticated programs that determine if a formula is satisfiable. They are very effective in practice. They can solve real-world problems with thousands of variables and millions of clauses. SAT solvers have become a general-purpose tool for solving many hard computational problems.Propositional logic is not just an academic topic. Its principles are part of modern technology.The most direct application of propositional logic is in digital electronic circuits.  are physical devices that implement the logical connectives.An  implements conjunction ().An  implements disjunction ().A  implements negation ().These gates are the building blocks of all digital hardware. They are combined to construct complex circuits. These circuits perform arithmetic and logical operations inside a computer's Central Processing Unit (CPU).Application in Software and Artificial IntelligencePropositional logic also forms the conceptual foundation for software.:  statements and  loops use Boolean expressions to direct the flow of execution. These are direct implementations of logical formulas.: Advanced search features in databases and web engines use logical operators to filter information.Knowledge Representation in AI: In some AI systems, knowledge is encoded as a database of logical formulas. Facts are atomic propositions. Rules are conditional statements. The system can then use rules of inference to deduce new information.In safety-critical fields, we need mathematical certainty that a system works correctly.  is the process of using formal logical methods to prove the correctness of a system.Desired system properties are expressed as logical formulas. For example, "the two railway gates are never open at the same time." Then, automated tools check if a model of the system satisfies the formula. This application shows the power of logic. We use it to design hardware, write software, and then prove that the resulting system is correct.Knowledge becomes skill through practice. This part provides resources to help you engage with propositional logic.Several software tools can help the learning process.LogicLearner (Columbia University): A free web application for guided practice of logic proofs. It validates student steps and can generate solutions.: A web-based toolkit that can prove formulas, generate truth tables, and convert formulas to CNF.Programming with Logic: A Python PrimerWorking with logic in code can deepen your understanding. The  library in Python is a good tool for this. It allows you to create and manipulate Boolean expressions symbolically.from sympy import symbols
from sympy.logic.boolalg import Implies
from sympy.logic.inference import satisfiable

x, y = symbols('x, y')
expr = Implies(x, y)
# The satisfiable function returns a dictionary of values if the expression can be true.
print(satisfiable(expr))
# Output: {x: False, y: False}

Practice Problems & ProjectsThe best way to learn is by doing.: Try translating English sentences to logic, building truth tables, proving equivalences, and constructing formal proofs.: Write a Python program that takes a logical formula as input and prints its truth table.: Model a classic logic puzzle, like "Knights and Knaves," using propositional logic. Translate the puzzle's statements into a single formula and use a SAT solver to find the solution.: Use a circuit simulator like Logisim Evolution to design a simple circuit, like a 2-bit binary adder. Derive the logic formulas from a truth table, simplify them, and build the circuit.Mastering propositional logic is a great first step into the world of formal reasoning. The principles you have learned are a reliable guide.For further study, you can explore these resources:: Stanford University's "Introduction to Logic" on Coursera is a comprehensive intermediate course.: "Discrete Mathematics and Its Applications" by Kenneth Rosen is a standard text for computer science students. "Discrete Mathematics: An Open Introduction" by Oscar Levin is a free, open-source textbook that is good for active learning.]]></content:encoded></item><item><title>Where does this fit in the Linux stack?</title><link>https://www.reddit.com/r/linux/comments/1lfuhgm/where_does_this_fit_in_the_linux_stack/</link><author>/u/karland90</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 03:57:03 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So I was reading the issue-thread about KDE Plasma adapting to the recent EU requirements about accessibility. And avoiding users accidentally creating situations that could trigger photosensitive epilepsy sounded difficult.This made me think - hypothetically speaking - in which part of a modern (e.g. KDE-based) Linux distro could an OS-level universal photo sensitivity filter be implemented 🤔? I.e. an optional tool where successive frames are analyzed and if a danger level threshold is crossed, a mitigation procedure is triggered. That procedure could be freezing/skipping frames, morphing between frames more slowly, or displaying a warning overlay/watermark).Can this be a regular user app? Does it require changes to some part of the rendering stack?Based on googling for 5 min, I found:this mention of University of Maryland having a fully open-source detection tool in the works:We are working on a new fully-open-source version that will be updated for new technologies (the current version is open-source except for a proprietary analysis engine we purchased the rights to use). It will also be free to use. No ETA for it as yet.some Github repo searches: 12one of the more promising results: 3that searching for "epilepsy detection" gives a lot of "noise" in projects doing health tracking for detection of an epileptic fit.I'm hoping someone is inspired to dig into making this or I get pointers which issue tracker or forum to take this towards 🙏Maybe Linux can get another trailblazer win, Apple can copy it and get admired as innovative for it, and we get the smug "um akshually ☝️". But the world would still be better than before 😌]]></content:encoded></item><item><title>[R] WiFiGPT: Using fine-tuned LLM for Indoor Localization Using Raw WiFi Signals (arXiv:2505.15835)</title><link>https://www.reddit.com/r/MachineLearning/comments/1lfu9bk/r_wifigpt_using_finetuned_llm_for_indoor/</link><author>/u/DiligentCharacter252</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 03:44:09 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We recently released a paper called : a decoder-only transformer trained directly on raw WiFi telemetry (CSI, RSSI, FTM) for indoor localization.In this work, we explore treating raw wireless telemetry (CSI, RSSI, and FTM) as a "language" and using decoder-only LLMs to regress spatial coordinates directly from it.Would love to hear your feedback, questions, or thoughts.]]></content:encoded></item><item><title>One-Minute Daily AI News 6/19/2025</title><link>https://www.reddit.com/r/artificial/comments/1lfttol/oneminute_daily_ai_news_6192025/</link><author>/u/Excellent-Target-847</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 03:19:55 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>Has anyone taken the Rust Data Engineering course by O&apos;Reilly? It’s said to have 463 hours of content, which seems very dense. Is it worth it?</title><link>https://www.reddit.com/r/rust/comments/1lfsu5p/has_anyone_taken_the_rust_data_engineering_course/</link><author>/u/swe_solo_engineer</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 02:27:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I’m asking because I can choose one course from several options provided as a benefit at my workplace. I was thinking about choosing this one.   submitted by    /u/swe_solo_engineer ]]></content:encoded></item><item><title>Experiments with DNA Compression and Generating Complimentary Base Pairs</title><link>https://arianfarid.me/articles/dna-compression.html</link><author>/u/VeryStrangeAttractor</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 02:02:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Efficiently storing and analyzing these sequences is a critical challenge. Furthermore, the ability to analyze large sequences of data are increasingly critical. In this post, we will explore a method to compress DNA using 4-bits per nucleotide in pure Rust, that allows us to generate Complementary base pairs in its compressed form.This technique is especially useful in DNA analytical pipelines, where performance and memory constraints are critical. By minimizing the footprint of each sequence, we simultaneously reduce storage overhead and in-memory costs, without sacrificing speed or the ability to operate directly on compressed data.DNA Bases and IUPAC Codes There are 15 IUPAC codes. The ones that most are familiar with are "A", "G", "C", and "T", representing the four standard DNA bases. However, DNA sequencing often produces ambiguous results. The remaining 11 codes are for these cases. For example, "R" can represent "G"  "A", while "N" can represent  nucleotide.DNA bases form pairs through well-defined chemical relationships: adenine (A) pairs with thymine (T), and cytosine (C) with guanine (G). These base-pairing rules extend to IUPAC ambiguity codes, which represent sets of possible nucleotides. For instance, "R" (A or G) complements "Y" (T or C).There are three cases where the Complement is the same code. The bases "N" (any base), "S" (G or C), and "W" (A or T) all Complement to their own code (e.g. S->S, because "S" is represented by "G" or "C").Our compression system needs to be fast, small, and reversible. It should support all 15 IUPAC nucleotide codes and allow efficient I/O and transformation.Smallest representation of nucleotides possible.Translate compressed/uncompressed DNA to and from file.Easily retrieve Complementary base pairs (including IUPAC codes)Representing IUPAC Nucleotides in Four Bits Since four bits are enough to represent 16 values (2⁴ = 16), we can comfortably fit in all 15 codes as well as an additional padding code.Because Rust does not support native 4-bit types, our 4-bit encodings must be packed into a larger primitive. I opted to group 4 nucleotides into a single  integer. Because 4-bit data types are still represented at the byte level in Rust, we can squeeze four nucleotide representations of DNA into a single  integer.When a sequence has fewer than four nucleotides remaining at the end, we use the 16th reserved value as a padding indicator. These padding values are ignored during decompression.Support for Bitwise Rotation To obtain support for 12 Complementations and 3 self-Complementations, we can rotate the bit two positions.For example, "A" will be represented by . Rotating two bits will give us  "T". An additional two bit rotation will bring us back to "A", .Codes that Complement themselves must be symmetric on either half of the bit mask. For example, if we represent the code "S" ("G" or "C") as , rotating two bits will still give us .Let's first look at a simple match expression to see the final schema we have derived. This match expressions encodes each IUPAC nucleotide into a 4-bit mask:This will serve as the building block for our 4-nucleotide compression scheme. Note that  (e.g. G/C, or A/T) are rotated 2 positions!The NucWord  represents four encoded nucleotides packed into a single . The methods  and  are used to serialize/deserialize.Lets look closely at :This simple method iterates through a slice of four nucleotides, translating each to its 4-bit encoding, and shifts them to their appropriate position in the .This method takes a 4-bit mask to return a  implementation of our nucleotide, filtering out any padded characters.Now that we can represent four nucleotides in a single NucWord, we will define a container type NucBlockVec to encode/decode entire DNA sequences.Let’s define a quick test to see the compression in action. We will read our , encode the nucleotides to NucBlockVec, and write the compressed binary output to disk.Inspecting  file size, we have 8,286 bytes... Exactly half the size!Here’s how we implement base-pair complements using our bit rotation trick:This works because the 4-bit encodings were designed so that a 2-bit rotation produces the nucleotide's complement.Lets compare this bit rotation to a simpler match implementation:Bit rotation is roughly 2x faster (Fig. 1) than using a match arm to grab DNA base pair Complements.The speed savings becomes more important when dealing with very large nucleotide sequences. In Fig. 2, the time is reduced from ~0.6 seconds to ~0.3 seconds.Efficient DNA compression is a challenging problem at the intersection of systems programming and bioinformatics. This Rust based 4-bit DNA encoder offers a lightweight, fast, and ergonomic way to handle genetic data efficiently.Using bitwise operations doubled Complementary generation speed. I feel I've only scratched the surface and look forward to getting more use out of this encoding.Check out the source code on my GitHub. Thanks for reading!]]></content:encoded></item><item><title>In Praise of “Normal” Engineers</title><link>https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/</link><author>/u/gametorch</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 01:58:38 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[This article was originally commissioned by Luca Rossi (paywalled) for refactoring.fm, on February 11th, 2025. Luca edited a version of it that emphasized the importance of building “10x engineering teams” . It was later picked up by IEEE Spectrum (!!!), who scrapped most of the teams content and published a different, shorter piece on March 13th.This is my personal edit. It is not exactly identical to either of the versions that have been publicly released to date. It contains a lot of the source material for the talk I gave last week at #LDX3 in London, “In Praise of ‘Normal’ Engineers” (slides), and a couple weeks ago at CraftConf. In Praise of “Normal” EngineersMost of us have encountered a few engineers who seem practically magician-like, a class apart from the rest of us in their ability to reason about complex mental models, leap to non-obvious yet elegant solutions, or emit waves of high quality code at unreal velocity.I have run into any number of these incredible beings over the course of my career. I think this is what explains the curious durability of the “10x engineer” meme. It may be based on flimsy, shoddy research, and the claims people have made to defend it have often been risible (e.g. “10x engineers have dark backgrounds, are rarely seen doing UI work, are poor mentors and interviewers”), or blatantly double down on stereotypes (“we look for young dudes in hoodies that remind us of Mark Zuckerberg”). But damn if it doesn’t resonate with experience. It just feels true.The problem is not the idea that there are engineers who are 10x as productive as other engineers. I don’t have a problem with this statement; in fact, that much seems self-evidently true. The problems I do have are twofold.Measuring productivity is fraught and imperfectFirst: how are you measuring productivity? I have a problem with the implication that there is One True Metric of productivity that you can standardize and sort people by. Consider, for a moment, the sheer combinatorial magnitude of skills and experiences at play:Also: people and their skills and abilities are not static. At one point, I was a pretty good DBRE (I even co-wrote the book on it). Maybe I was even a 10x DB engineer then, but certainly not now. I haven’t debugged a query plan in years.“10x engineer” makes it sound like 10x productivity is an immutable characteristic of a person. But someone who is a 10x engineer in a particular skill set is still going to have infinitely more areas where they are normal or average (or less). I know a lot of world class engineers, but I’ve never met anyone who is 10x better than everyone else across the board, in every situation.Engineers don’t own software, teams own softwareSecond, and even more importantly: So what? It doesn’t matter. Individual engineers don’t own software, teams own software. The smallest unit of software ownership and delivery is the engineering team. It doesn’t matter how fast an individual engineer can write software, what matters is how fast the team can collectively write, test, review, ship, maintain, refactor, extend, architect, and revise the software that they own.Everyone uses the same software delivery pipeline. If it takes the slowest engineer at your company five hours to ship a single line of code, it’s going to take the fastest engineer at your company five hours to ship a single line of code. The time spent writing code is typically dwarfed by the time spent on every other part of the software development lifecycle.If you have services or software components that are owned by a single engineer, that person is a single point of failure.I’m not saying this should never happen. It’s quite normal at startups to have individuals owning software, because the biggest existential risk that you face is not moving fast enough, not finding product market fit, and going out of business. But as you start to grow up as a company, as users start to demand more from you, and you start planning for the survival of the company to extend years into the future…ownership needs to get handed over to a team. Individual engineers get sick, go on vacation, and leave the company, and the business has got to be resilient to that.If teams own software, then the key job of any engineering leader is to craft high-performing engineering teams. If you must 10x something, 10x this. Build 10x engineering teams.The best engineering orgs are the ones where normal engineers can do great workWhen people talk about world-class engineering orgs, they often have in mind teams that are top-heavy with staff and principal engineers, or recruiting heavily from the ranks of ex-FAANG employees or top universities.But I would argue that a truly great engineering org is one where you don’t HAVE to be one of the “best” or most pedigreed engineers in the world to get shit done and have a lot of impact on the business.I think it’s actually the other way around. A truly great engineering organization is one where perfectly normal, workaday software engineers, with decent software engineering skills and an ordinary amount of expertise, can consistently move fast, ship code, respond to users, understand the systems they’ve built, and move the business forward a little bit more, day by day, week by week.Any asshole can build an org where the most experienced, brilliant engineers in the world can build product and make progress. That is not hard. And putting all the spotlight on individual ability has a way of letting your leaders off the hook for doing their jobs. It is a HUGE competitive advantage if you can build sociotechnical systems where less experienced engineers can convert their effort and energy into product and business momentum.A truly great engineering org also happens to be one that mints world-class software engineers. But we’re getting ahead of ourselves, here.Let’s talk about “normal” for a momentA lot of technical people got really attached to our identities as smart kids. The software industry tends to reflect and reinforce this preoccupation at every turn, from Netflix’s “we look for the top 10% of global talent” to Amazon’s talk about “bar-raising” or Coinbase’s recent claim to “hire the top .1%”. (Seriously, guys? Ok, well, Honeycomb is going to hire only the top !)In this essay, I would like to challenge us to set that baggage to the side and think about ourselves as .It can be humbling to think of ourselves as normal people, but most of us are in fact pretty normal people (albeit with many years of highly specialized practice and experience), and there is . Even those of us who are certified geniuses on certain criteria are likely quite normal in other ways — kinesthetic, emotional, spatial, musical, linguistic, etc.Software engineering both selects for and develops certain types of intelligence, particularly around abstract reasoning, but  is born a great software engineer. Great engineers are made, not born. I just don’t think there’s a lot more we can get out of thinking of ourselves as a special class of people, compared to the value we can derive from thinking of ourselves collectively as relatively normal people who have practiced a fairly niche craft for a very long time.Build sociotechnical systems with “normal people” in mindWhen it comes to hiring talent and building teams, yes, absolutely, we should focus on identifying the ways people are exceptional and talented and strong. But when it comes to building sociotechnical systems for software delivery, we should focus on all the ways people are .Normal people have cognitive biases — confirmation bias, recency bias, hindsight bias. We work hard, we care, and we do our best; but we also forget things, get impatient, and zone out. Our eyes are inexorably drawn to the color red (unless we are colorblind). We develop habits and ways of doing things, and resist changing them. When we see the same text block repeatedly, we stop reading it.We are embodied beings who can get overwhelmed and fatigued. If an alert wakes us up at 3 am, we are much more likely to make mistakes while responding to that alert than if we tried to do the same thing at 3pm. Our emotional state can affect the quality of our work. Our relationships impact our ability to get shit done.When your systems are designed to be used by normal engineers, all that excess brilliance they have can get poured into the product itself, instead of wasting it on navigating the system itself.How do you turn normal engineers into 10x engineering teams?None of this should be terribly surprising; it’s all well known wisdom. In order to build the kind of sociotechnical systems for software delivery that enable normal engineers to move fast, learn continuously, and deliver great results as a team, you should:Shrink the interval between when you write the code and when the code goes live.Make it as short as possible; the shorter the better. I’ve written and given talks about this many, many times. The shorter the interval, the lower the cognitive carrying costs. The faster you can iterate, the better. The more of your brain can go into the product instead of the process of building it.One of the most powerful things you can do is have a short, fast enough deploy cycle that you can ship one commit per deploy. I’ve referred to this as the “software engineering death spiral” … when the deploy cycle takes so long that you end up batching together a bunch of engineers’ diffs in every build. The slower it gets, the more you batch up, and the harder it becomes to figure out what happened or roll back. The longer it takes, the more people you need, the higher the coordination costs, and the more slowly everyone moves.Deploy time is the feedback loop at the heart of the development process. It is almost impossible to overstate the centrality of keeping this short and tight.Make it easy and fast to roll back or recover from mistakes.Developers should be able to deploy their own code, figure out if it’s working as intended or not, and if not, roll forward or back swiftly and easily. No muss, no fuss, no thinking involved.Make it easy to do the right thing and hard to do the wrong thing. Wrap designers and design thinking into all the touch points your engineers have with production systems. Use your platform engineering team to think about how to empower people to swiftly make changes and self-serve, but also remember that a lot of times people will be engaging with production late at night or when they’re very stressed, tired, and possibly freaking out. Build guard rails. The fastest way to ship a single line of code should also be the easiest way to ship a single line of code.Invest in instrumentation and observability.You’ll never know — not really — what the code you wrote does just by reading it. The only way to be sure is by instrumenting your code and watching real users run it in production. Good, friendly sociotechnical systems invest  in tools for sense-making.Being able to visualize your work is what makes engineering abstractions accessible to actual engineers. You shouldn’t have to be a world-class engineer just to debug your own damn code.Devote engineering cycles to internal tooling and enablement.If fast, safe deploys, with guard rails, instrumentation, and highly parallelized test suites are “everybody’s job”, they will end up nobody’s job. Engineering productivity isn’t something you can outsource. Managing the interfaces between your software vendors and your own teams is both a science and an art. Making it look easy and intuitive is really hard. It needs an owner.Build an inclusive culture.Growth is the norm, growth is the baseline. People do their best work when they feel a sense of belonging. An inclusive culture is one where everyone feels safe to ask questions, explore, and make mistakes; where everyone is held to the same high standard, and given the support and encouragement they need to achieve their goals.Diverse teams are resilient teams.Yeah, a team of super-senior engineers who all share a similar background can move incredibly fast, but a monoculture is fragile. Someone gets sick, someone gets pregnant, you start to grow and you need to integrate people from other backgrounds and the whole team can get derailed — fast.When your teams are used to operating with a mix of genders, racial backgrounds, identities, age ranges, family statuses, geographical locations, skill sets, etc — when this is just table stakes, standard operating procedure — you’re better equipped to roll with it when life happens.Assemble engineering teams from a range of levels.The best engineering teams aren’t top-heavy with staff engineers and principal engineers. The best engineering teams are ones where nobody is running on autopilot, banging out a login page for the 300th time; everyone is working on something that challenges them and pushes their boundaries. Everyone is learning, everyone is teaching, everyone is pushing their own boundaries and growing. All the time.By the way — all of that work you put into making your systems resilient, well-designed, and humane is the same work you would need to do to help onboard new engineers, develop junior talent, or let engineers move between teams.It gets used and reused. Over and over and over again.The only meaningful measure of productivity is impact to the businessThe only thing that actually matters when it comes to engineering productivity is whether or not you are moving the business materially forward.Which means…we can’t do this in a vacuum. The most important question is whether or not we are working on the right thing, which is a problem engineering can’t answer without help from product, design, and the rest of the business.Software engineering isn’t about writing lots of lines of code, it’s about solving business problems using technology.Senior and intermediate engineers are actually the workhorses of the industry. They move the business forward, step by step, day by day. They get to put their heads down and crank instead of constantly looking around the org and solving coordination problems. If you have to be a staff+ engineer to move the product forward, something is seriously wrong.Great engineering orgs mint world-class engineersA great engineering org is one where you don’t HAVE to be one of the best engineers in the world to have a lot of impact. But — rather ironically — great engineering orgs mint world class engineers like nobody’s business.The best engineering orgs are not the ones with the smartest, most experienced people in the world, they’re the ones where normal software engineers can consistently make progress, deliver value to users, and move the business forward, day after day.Places where engineers can get shit done and have a lot of impact are a magnet for top performers. Nothing makes engineers happier than building things, solving problems, making progress.If you’re lucky enough to have world-class engineers in your org, good for you! Your role as a leader is to leverage their brilliance for the good of your customers and your other engineers, without coming to depend on their brilliance. After all, these people don’t belong to you. They may walk out the door at any moment, and that has to be okay.These people can be phenomenal assets, assuming they can be team players and keep their egos in check. Which is probably why so many tech companies seem to obsess over identifying and hiring them, especially in Silicon Valley.But companies categorically overindex on finding these people after they’ve already been minted, which ends up reinforcing and replicating all the prejudices and inequities of the world at large. Talent may be evenly distributed across populations, but opportunity is not.Don’t hire the “best” people. Hire the right people.We (by which I mean the entire human race) place too much emphasis on individual agency and characteristics, and not enough on the systems that shape us and inform our behaviors.I feel like a whole slew of issues (candidates self-selecting out of the interview process, diversity of applicants, etc) would be improved simply by shifting the focus on engineering hiring and interviewing away from this inordinate emphasis on hiring the BEST PEOPLE and realigning around the more reasonable and accurate RIGHT PEOPLE. It’s a competitive advantage to build an environment where people can be hired for their unique strengths, not their lack of weaknesses; where the emphasis is on composing teams rather than hiring the BEST people; where inclusivity is a given both for ethical reasons and because it raises the bar for performance for everyone. Inclusive culture is what actual meritocracy depends on.This is the kind of place that engineering talent (and good humans) are drawn to like a moth to a flame. . It feels  to move the business forward. It feels  to sharpen your skills and improve your craft. It’s the kind of place that people go when they want to become world class engineers. And it’s the kind of place where world class engineers want to stick around, to train up the next generation.]]></content:encoded></item><item><title>This Week in Rust #604</title><link>https://this-week-in-rust.org/blog/2025/06/18/this-week-in-rust-604/</link><author>/u/seino_chan</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 01:07:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Category: This Week in RustThis week's crate is RobustMQ, a next-generation, high-performance, multi-protocol message queue.Thanks to Yu Liu for the self-suggestion!An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.If you are a feature implementer and would like your RFC to appear in this list, add a
 label to your RFC along with a comment providing testing instructions and/or
guidance on which aspect(s) of the feature need testing.Let us know if you would like your feature to be tracked as a part of this list.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.Relatively quiet week, with a few improvements to benchmarks leveraging the new
trait solver.Improvements ✅  (secondary)3 Regressions, 7 Improvements, 4 Mixed; 4 of them in rollups
51 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.Rusty Events between 2025-06-18 - 2025-07-16 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.But after a few weeks, it compiled and the results surprised us. The code was 10x faster than our carefully tuned Kotlin implementation – despite no attempt to make it faster. To put this in perspective, we had spent years incrementally improving the Kotlin version from 2,000 to 3,000 transactions per second (TPS). The Rust version, written by Java developers who were new to the language, clocked 30,000 TPS.This was one of those moments that fundamentally shifts your thinking. Suddenly, the couple of weeks spent learning Rust no longer looked like a big deal, when compared with how long it’d have taken us to get the same results on the JVM. We stopped asking, “Should we be using Rust?” and started asking “Where else could Rust help us solve our problems?”]]></content:encoded></item><item><title>Replace Python with Go for LLMs?</title><link>https://www.reddit.com/r/golang/comments/1lfr9hi/replace_python_with_go_for_llms/</link><author>/u/Tobias-Gleiter</author><category>reddit</category><pubDate>Fri, 20 Jun 2025 01:05:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I really wonder why we are using Python for LLM tasks because there is no crazy benefit vs using Go. At the end it is just calling some LLM and parsing strings. And Go is pretty good in both. Although parsing strings might need more attention.Why not replacing Python with Go? I can imagine this will happen with big companies in future. Especially to reduce cost.What are your thoughts here? ]]></content:encoded></item><item><title>Protecting an endpoint with OAuth2</title><link>https://www.reddit.com/r/golang/comments/1lfpdd6/protecting_an_endpoint_with_oauth2/</link><author>/u/riscbee</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 23:33:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm already using OAuth2 with the Authorization Code Flow. My web app is server-sided, but now I want to expose one JSON endpoint, and I'm not sure what flow to choose.Say I somehow obtain a client secret and refresh token, do I just append the secret and the refresh token in the GET or POST request to my backend? Do I then use that access token to fetch the user email or ID and then look up if that user exists in my backend and fetch their permission?Do I have to handle refreshing on my backend, or should the client do it? I'm not sure how to respond with a new secret and refresh token. After all, the user requests GET /private-data and expects JSON. I can't just return new secret and refresh tokens, no?]]></content:encoded></item><item><title>Fwupd 2.0.12 Released With More Intel Battlemage GPUs &amp; HP USB-C Hub Supported</title><link>https://www.phoronix.com/news/Fwupd-2.0.12-Released</link><author>/u/reps_up</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 23:29:15 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[
Richard Hughes of Red Hat just released Fwupd 2.0.12 as the newest version of this open-source firmware updating utility that pairs with the Linux Vendor Firmware Service (LVFS) for a nice Linux system/device firmware updating experience.
Building off the Intel Arc B-Series "Battlemage" support in prior Fwupd releases, Fwupd 2.0.12 adds support for firmware updates on additional Intel Arc Battlemage graphics cards. Fwupd 2.0.12 also supports firmware updates for more Foxconn 5G modems. Additionally, the HP Portable USB-C Hub can now support firmware updates via Fwupd/LVFS.
Fwupd 2.0.12 also adds a new configuration option for enforcing immutable device enumeration, device emulation support for Thunderbolt host controllers, support for loading multiple coSWID blocks from PE files, and a number of bug fixes. There are quite a range of bug fixes in Fwupd 2.0.12 from addressing firmware update issues with different devices to general fixes to this firmware updating utility itself.
Downloads and more details on the just-released Fwupd 2.0.12 via GitHub.]]></content:encoded></item><item><title>[D] Looks like someone is already offering B200 rentals for $1.49/hr — anyone else seen this?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lfp5sb/d_looks_like_someone_is_already_offering_b200/</link><author>/u/asklaylay</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 23:23:08 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Just came across this: DeepInfra is offering access to B200 Nvidia GPUs at $1.49/hour.   submitted by    /u/asklaylay ]]></content:encoded></item><item><title>No more coding vibes in the efficiency era</title><link>https://devinterrupted.substack.com/p/no-more-coding-vibes-in-the-efficiency</link><author>/u/benlloydpearson</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 23:08:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Using a Kubernetes credential provider with Cloudsmith</title><link>https://www.youtube.com/watch?v=0E2fNx7oBn0</link><author>/u/ExtensionSuccess8539</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 21:38:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Cloudsmith's SRE discusses the use of credential providers in Kubernetes to securely pull images from private repositories. Credential providers are a great new feature that appeared in recent versions of Kubernetes. They allow you to pull images using a short-lived authentication token, which makes them less prone to leakage than long-lived credentials - which improves the overall security of your software supply chain.]]></content:encoded></item><item><title>Liberux Nexx: An interview with Liberux about their made-in-EU OSHW Linux Phone</title><link>https://linmob.net/liberux-nexx-an-interview-with-liberux/</link><author>/u/wiki_me</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 21:25:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[As you may have heard or rather read, the Spanish company Liberux recently launched a crowdfunder for their new mainline Linux "Nexx" phone on Indiegogo - starting at 8 GB RAM/128 GB eMMC/LTE for 799 EUR and going up to 32 GB RAM/512 GB/5G for 1300 (during the crowd-funder).The specs include impressive things, such as two USB-C ports and a headphone jack - quite unusual these days. It's also somewhat modular (cellular modem, RAM, and storage are on modules) and they aim to open-source the hardware and plan to manufacture the devices in Spain - meaning, the Liberux Nexx will be (successful funding assumed) be one of the only (?) smartphones designed and build in Europe.I asked Liberux for an interview, and they were happy to answer my questions. While they would have prefered to do this as a video interview, I just could not swing that (we're preparing to move, and I have to stand-in for a colleague on holiday at the dayjob right now) - so gladly, they agreed to do it in a back and forth via email. With that said, here you go:Let's start with something broad: Why did you decide to make a Linux Phone? It is a tiny niche, the software ecosystem is still somewhat nascent, many challenges have not been solved and the market keeps adding new ones (e.g., Voice over LTE (VoLTE) or Rich Communication Services (RCS)). In addition to that, making specific hardware for FOSS (and/or privacy-) nerds is really difficult, as these people can be very nitpicky? So: Why?We know it’s not an easy path. But it’s a necessary one. The dominant mobile operating systems are black boxes, serving interests that often don’t align with those of the user. We’re concerned about surveillance, the lack of real control over our devices, and the opacity of the software. We wanted to build something different: a phone designed with respect for the user's freedom, running an auditable OS with no backdoors. It's a bet on a future where you don’t have to give up your privacy to have a useful device. It’s not for everyone, but it is for those who value their digital autonomy.What level of prior experience do members of your team have with hardware projects/phone or consumer electronics manufacturing and software development?Our team has a mix of backgrounds, all with solid experience. Pedro, for example, developed the operating system (TuxumOS) for the first Linux tablet PC running on x86, and has collaborated with companies like Toshiba, Lenovo, and Fujitsu on Linux support and power management for their laptops. Carlos has designed mechanical keyboards and consumer electronic products. Our hardware design partner, Pleeda, has already developed and marketed its own portable game console. Ernesto Mansilla from Ecaman, our manufacturing partner, has extensive experience in the production and assembly of electronic boards. The team at Collabora, particularly their programming wizards, has been a huge help in defining the DTBs needed to boot the board. So no, we’re definitely not starting from scratch.Some people have tried to cast doubt on your origins or even your integrity. What’s your response to those rumors?Well, at first we found it quite amusing — some of the things being said were wildly outlandish, involving bankers and politicians. It was kind of funny to see ourselves portrayed as being on the “other side” of the conspiracy for once.The truth is, free software is our life. Pedro started programming computers (an IBM 5150 his father brought home) when he was just 7 years old. He installed his first Slackware in 1993 using a stack of 3½-inch floppy disks, and he's been a Debian user since the Potato release — with the occasional flirtation with BSD (nobody’s perfect).In the end, we honestly don’t understand who would want to spread false information about a project whose sole goal is to provide the community with a freer, more secure phone.You write that you've looked into the PinePhone (Pro) and Librem 5. What were the biggest mistakes or failures in your analysis of the PinePhone (Pro) and Librem 5 that you don't want to repeat? What did PINE64/Purism do really well in your opinion?We deeply respect both projects for being pioneers. The PinePhone, for instance, was very affordable, but it was more of a development platform than a daily-use phone. The Librem 5 took so long to come out that its hardware was outdated by the time it was usable.That said, they got many things right. PINE64 was brave enough to release several iterations that became key development platforms. Purism achieved very high build quality, even if the final design was bulky. Both have taught us what to avoid — and also what’s worth preserving.Where do you see yourself regarding software development? More aligned with PINE64 (who just do hardware) or with Purism, who basically made GNOME a thing on Mobile again?We feel clearly more aligned with Purism. We want to offer equally polished and solid hardware, as well as a strong software development effort to go along with it.Let's continue with pricing. I've seen discussions on the Fediverse, and you have added going two cheaper (yet still powerful) options with  8 or 16 GB RAM and 128 GB storage and LTE. Can you explain why the Liberux Nexx is more expensive than the average Android smartphone people may be comparing it with?First, the components: 32 GB of RAM, 5G, and 512 GB of storage are not typical for devices in this space. On top of that, we manufacture in Europe and in small batches, which makes everything more expensive—from assembly to logistics. Every phone will be hand-assembled in our offices. Then there's the intangible: we don’t rely on big manufacturers or opaque supply chains. The price reflects our commitment to quality, transparency, and independence.Let's get into hardware. Why did you decide to pick the RockChip RK3588s as the core for your product, and not, e.g., a chip from Qualcomm (e.g, QCM6490)? What makes the RK3588s a good choice for phone? Why pick it despite requiring proprietary firmware for GPU and DDR RAM?The RK3588s is a well-known and respected chip in the Linux community, used in projects like the MNT Reform Next. Yes, it needs proprietary firmware for the GPU and DDR RAM — but so do alternatives like Qualcomm, which typically include even more closed components. One of the key advantages of the RK3588s is that it doesn’t have an integrated 5G modem. This lets us isolate the modem physically and control it via a real kill switch—something much harder (if not impossible) with integrated Qualcomm SoCs. We also aim to work toward freeing the DDR firmware, something that's almost unthinkable on Qualcomm due to much stricter restrictions.On design choices and details: Why does the Nexx have two USB-C ports?Simply put: you often need to charge your device and connect something else at the same time. But there's a more ambitious reason too—we want to make it easy to use the Nexx as a desktop computer, and two USB-C ports open up a lot of possibilities, especially with docks or keyboards. And let’s be honest… it just looks awesome!Does one (or both) of the USB-C ports support DisplayPort Alt Mode, so that I can plug into a display without carrying accessories?Yes, at least one of the ports will provide direct DisplayPort output, allowing you to use the Nexx as a desktop without additional docks.What are the transfer speeds that can be expected from the USB ports (USB 2.0, 3.x)? What are your plans regarding USB Power Delivery support?Both ports will be USB 3.1. We’re also working on implementing support for USB Power
Delivery, with proper energy delivery for fast charging and accessory compatibility.Regarding WiFi/BT and LTE/5G: Purism made the unusual choice of making these socketed and exchangeable; I presume this is not the case with your design, the hardware kill switches ensure that components can be truly turned of. Sadly, there don't seem to be any blob-free choices. What are the main criteria to pick hardware here?Our priority was balancing performance and Linux compatibility. The 5G modem will be replaceable—it’s mounted on a small FPC board, making it easy to swap without the bulk of traditional sockets. Wi-Fi and Bluetooth are soldered directly to the board, but both are connected to physical kill switches that cut power entirely, ensuring they’re really off. We know there are no fully free options yet, but we’ve chosen components with good Linux support and low power usage, minimizing blobs without sacrificing usability.Battery life has been an issue with the existing native Linux phones. How do you plan to optimize for this?It’s one of our top priorities. We’re developing a dedicated daemon to manage power more efficiently—it will shut down and wake up CPU cores depending on load, adjust frequencies dynamically, and optimize idle performance. We’re also working on smarter suspend and resume behavior, including options like RTC wake, wake-on-WAN, power button, or even tapto-wake. Brightness management will also adapt more intelligently to real usage conditions. All this aims to extend battery life without compromising the experience.Some important phone features are set by hardware choices. Unfortunately, Voice over LTE has become mandatory in some markets (and the number of markets, where this is the case, will only grow) - is it a major deciding factor when choosing the 5G part?Yes, we considered that. We want the Nexx to feature modern hardware compatible with the latest technologies. VoLTE is increasingly essential, and we see it as key to ensuring a good
user experience and long-term compatibility.Aside from necessary certifications, you are also going for a OSHW certification - does this mean Liberux Nexx will be Open Source Hardware?Yes, that’s our goal. We want the Nexx to be open source hardware, which is why we’ll release the schematics with the first units shipped. Other assets (like full mechanical designs and manufacturing files) might come a bit later. Our initial idea was to release everything once we hit about 10,000 units sold. What matters most to us is building something open and transparent—even if we have to take it step by step.Accessories. Maybe this is just my age (and growing grumpiness) showing, but - and especially from my experience with existing Linux Phones - why the Wireless Dock (W-Dock) for the
"use phone as a PC" use case? I would have rather gone with a nice USB dock that has a silent fan to keep the phone cool to not suffer from throttling.We totally get that! In fact, we’re not ruling out that more traditional option. But Pedro had already developed an interesting solution to access the device remotely via RDP, which opened the door for us to explore a wireless dock. The main advantage is that you can keep using the phone for things like calls without unplugging it. Still, we know a USB dock with passive or silent cooling has real value, and it’s definitely on the table as a future accessory.The Liberux Mechanical Keyboard sounds interesting. Any idea on pricing? Will it be available separately?Yes, we plan to offer it separately—even in a black version—and it will likely be under €200 (excluding taxes). It’s inspired by the legendary IBM Model F, but in a more compact format. We even want to implement the buckling spring mechanism those keyboards had almost 50 years ago, since we think it still outperforms any modern keyboard. The goal is to deliver that unmistakable mechanical experience in a more portable and modern design.Let's tackle software.
You write that "LiberuxOS (based on Debian 13 Linux) is an ethical and mostly opensource operating system." Which parts will be non-open-source? Just firmware, or also drivers / custom user space software (e.g., apps)?Our goal is to release everything we can. There will be some unavoidable blobs, like firmware for the GPU or modem, but all apps we develop will be open source. We won’t include closed software—either ours or from third parties. Any new developments, such as interfaces or tools, will be published from day one.Are you intending to ship a (close to) mainline kernel, or a Board Support Package (BSP)/vendor kernel and make it work with a libhybris/Halium approach?We’ll go with bare-metal Linux—no Halium, no libhybris. We want to stay as close to mainline as possible and actively contribute upstream.Do you intend generally develop software bits in the open where possible or just where required by license?We’ll develop everything openly. Our policy is to publish code, collaborate with the community, and be transparent. Free software, for us, is a matter of principle—not just legal compliance._Why go with a distribution of your own and not partner with existing projects (e.g., postmarketOS, Mobian, Ubuntu Touch, Sailfish OS)? Given that you are basing on Debian, a collaboration with Mobian (or, if vendor kernel Droidian) may lead to obvious synergies. Are you exploring this?Yes, we’ve considered it—especially Mobian, since we share a base. But building our own distro lets us better tailor the system to the hardware and to the kind of use we want to promote. That said, it’s not mutually exclusive: we want to make porting other distros as easy as possible and collaborate with anyone interested.Why GNOME Shell Mobile and not Phosh? In my experience, Phosh is better at dealing
the occasional app/dialog that's not quite ready for mobile.We considered Phosh. But GNOME Shell Mobile gives us more room long-term in terms of customization, animations, and graphical performance. That said, we’re doing significant work on top of GNOME Shell to make it more mobile-friendly. Phosh is more mature in some areas, but we’re betting on something we can evolve more freely.Which parts of GNOME Shell Mobile do you intend to improve on for the Liberux Nexx?We’re working on better window management, visual improvements, and a more refined touch experience with intuitive gestures. We want using the Nexx to feel smooth and pleasant from the very first boot.Since I've attended [Akademy]( last year: Why not Plasma Mobile?We think Plasma Mobile is a great option and respect it a lot, but for our goals, it didn’t align as well. Given our limited resources, we had to focus on a single interface, and GNOME Shell Mobile fits our vision better, even if it also needs polishing.Are you in talks with community members/known Linux Mobile developers?Yes, we’re already in touch with various developers and projects.[I have asked a follow-up and known-to-me names have been mentioned, but to preserve privacy, these names are not included here.]Are there ways the community/interested people can help you make the Liberux Nexx happen, despite ordering on Indiegogo (especially for those, that would like to get the phone, but can't afford it right now)?Any help is welcome: from spreading the word to contributing code. We also accept donations so people who can’t afford the phone can still support the project.We want this to be a community effort—not just our own.More questions? Let me know!]]></content:encoded></item><item><title>[D] GPT-2 Small Not Converging Despite Using Same Hyperparams as Karpathy</title><link>https://www.reddit.com/r/MachineLearning/comments/1lflwvu/d_gpt2_small_not_converging_despite_using_same/</link><author>/u/New-Skin-5064</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 20:59:57 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[For some reason, my training loss keeps oscillating, and never falls below 4 after one epoch. It is still generating garbage like: "Once upon a time, with a alone example, pre Deg; is a disease, the American casual Plate. Roberts of campaign"(Once upon a time was the prompt). I am using the GPT-2 Small architecture and training on FineWeb-Edu 10B. The batch size is ~525k tokens, and I use 0.1 dropout. Because the Kaggle TPU times out after 9 hours, I would reupload the latest checkpoint the next day to resume training, which I think is why the learning rate randomly spikes in the graph. I checked my dataloader, and it appears to be loading text from the shards correctly. If anybody knows what I am doing wrong, I would appreciate your feedback. I also modified the same pipeline, shrank the model, and trained on TinyStories v2, and the model began to generate better text after 900 steps than the other did in over 20 thousand! The only difference between the two pipelines is the dataloader, as FineWeb is sharded but TinyStories is not. That implementation can be found here: https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb]]></content:encoded></item><item><title>Question about Networking Setup (Calico) with RKE2 Cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1lfl8g4/question_about_networking_setup_calico_with_rke2/</link><author>/u/wwebdev</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 20:31:22 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm running a small Kubernetes cluster using RKE2 on Azure, consisting of two SUSE Linux nodes:Both nodes are running fine, but they are not in the same virtual network. Currently, I’ve set up a WireGuard VPN between them so that Calico networking works properly.Is it necessary for all nodes in a Kubernetes cluster to be in the same virtual network for Calico to function properly?Is using WireGuard (or any VPN) the recommended way to connect nodes across separate networks in a setup like this?What would be the right approach if I want to scale this cluster across different clouds (multi-cloud scenario)? How should I handle networking between nodes then?I’d really appreciate your thoughts or any best practices on this. Thanks in advance!]]></content:encoded></item><item><title>Golang Runtime internal knowledge</title><link>https://www.reddit.com/r/golang/comments/1lfk3te/golang_runtime_internal_knowledge/</link><author>/u/SpecialistQuote9281</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 19:44:45 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey folks, I wanted to know how much deep knowledge of go internals one should have.I was asked below questions in an interviews:How does sync.Pool work under the hood?What is the role of poolChain and poolDequeue in its implementation?How does sync.Pool manage pooling and queuing across goroutines and threads (M’s/P’s)?How does channel prioritization work in the Go runtime scheduler (e.g., select cases, fairness, etc.)?I understand that some runtime internals might help with debugging or tuning performance, but is this level of deep dive typical for a mid-level Go developer role?]]></content:encoded></item><item><title>Struggling with Rust&apos;s module system - is it just me?</title><link>https://www.reddit.com/r/rust/comments/1lfjm7u/struggling_with_rusts_module_system_is_it_just_me/</link><author>/u/eight_byte</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 19:24:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As I'm learning Rust, I've found the way modules and code structure work to be a bit strange. In many tutorials, it's often described as being similar to a file system, but I'm having a hard time wrapping my head around the fact that a module isn't defined where its code is located.I understand the reasoning behind Rust's module system, with the goal of promoting modularity and encapsulation. But in practice, I find it challenging to organize my code in a way that feels natural and intuitive to me.For example, when I want to create a new module, I often end up spending time thinking about where exactly I should define it, rather than focusing on the implementation. It just doesn't seem to align with how I naturally think about structuring my code.Is anyone else in the Rust community experiencing similar struggles with the module system? I'd be really interested to hear your thoughts and any tips you might have for getting more comfortable with this aspect of the language.Any insights or advice would be greatly appreciated as I continue my journey of learning Rust. Thanks in advance!]]></content:encoded></item><item><title>Go should be more opinionated</title><link>https://eltonminetto.dev/en/post/2025-06-19-go-more-opinated/</link><author>/u/eminetto</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 18:52:24 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[One of the perks of being a Google Developer Expert is the incredible opportunities it provides. A few weeks ago, I had the opportunity to meet Robert Griesemer, co-creator of Go, in person, as well as Marc Dougherty, Developer Advocate for the Go team at Google. At a happy hour after Google I/O, Marc asked me and another Go GDE from Korea for feedback on the language. My response was that I didn’t have any specific feedback about the language but that:Go should be more opinionated about the application layout.It was worth writing a post to express my thoughts more clearly.Starting from the beginning…In 2025, I will have completed 10 years of writing code in Go. One of the things I recall from when I started is that the language was relatively simple to learn, mainly due to two reasons: its simplicity and the fact that there is only one way to do things. Go was the first language I came across that had strong opinions about several things. There is only one way to loop, and there is only one way to format files (using the ‘go fmt’ command). Variables with a small scope should have short names, etc. It made it much easier to read code written by other people, which is crucial for learning. The code I wrote was very similar to the Kubernetes code! Of course, the complexity of the problem was infinitely greater, but the code’s structure was readable to me. Over the years, I have observed this effect in several people I have followed who were starting in the language or migrating from other environments.But once this initial excitement has passed, the biggest challenge comes: how to adopt Go in a project larger than those used for learning? How do you structure a project that will be developed and evolved by a team? At this point, the language step aside from strong opinions, and each team or company needs to decide how to structure their projects. Over the past decade, I have worked for four companies. In all of them, it was necessary to invest the team’s time in collecting examples and reading documentation and books to determine which structure they should use in the projects. At the company where I currently work, we have created a document about this.Making an analogy with the world of games, it’s as if we were having fun in the controlled and wonderful world of Super Mario World and were transported to the open world of GTA 6 (yes! I’m hyped!). It’s still a fantastic universe, but the transition is quite abrupt.Go could be more opinionated regarding these choices. We could have templates for more common projects, such as CLIs, APIs, and microservices., that teams can use to scaffold their applications. The language toolkit already allows the use of project templates, so it would be a matter of having official templates to make life easier for teams. Alternatively, we could go further and include the command in the language toolkit itself with something like .A similar event occurred in the history of the language. Today,  dependency management is a fundamental part of our daily lives as Go developers. But it wasn’t always like this. For a long time, there was no official package manager for the language; consequently, the community developed several alternatives. They all worked, but fragmentation was getting out of control, making it challenging to integrate packages. Until the language team took control of the situation and  was created, pacifying the issue of “package and dependency management.” I believe we can apply the same approach to the structure of projects.Another profile that would benefit from a more opinionated project structure is that formed by teams that are migrating their applications from other languages, especially Java and PHP. In these ecosystems, frameworks dictate the structure of projects, such as Spring Boot and Laravel. “Where do I start? How do I structure my project?” are common questions I hear from teams migrating from these languages. Having something that facilitates this migration would lower the barrier to entry and increase the number of teams experimenting with Go in production.That’s my biggest feedback regarding Go at the moment. What do you think, dear reader? What’s your opinion on the subject? I’d love to discuss this topic in the comments or live at a conference.]]></content:encoded></item><item><title>[D] Future of RecSys in age of LLM</title><link>https://www.reddit.com/r/MachineLearning/comments/1lfijb4/d_future_of_recsys_in_age_of_llm/</link><author>/u/Electrical-Job-3373</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 18:41:17 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I have significant experience in recommendation system. Right now I don’t see any changes due to LLM. Most recommendation system needs low latency, which is not feasible currently with LLM. Do you think RecSys is safe from LLM takeover? Should RecSys domain experts like me should be worried?]]></content:encoded></item><item><title>Announcing TokioConf 2026</title><link>https://tokio.rs/blog/2025-06-19-announcing-tokio-conf</link><author>/u/Darksonn</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 18:30:10 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We’re happy to announce the inaugural , a dedicated conference for
developers building asynchronous network applications in Rust. It will be a
single-track event bringing together developers from many areas to talk about
how they use Tokio and Rust to build high-performance, reliable production
applications. Expect talks, panels, and time to share challenges and lessons
learned.More and more teams are using Rust and Tokio in production, and they’re all
tackling similar challenges—bringing Rust into existing systems, helping new
developers get up to speed, designing for reliability, and figuring out how to
test and deploy async code at scale.  is a space for those
conversations to swap ideas, learn from each other, and help the community
figure out where async Rust goes next.We’ve got ideas for topics we’d love to see—debugging async code, instrumenting
production systems, or successfully introducing Rust at work. But for now,
TokioConf is a blank canvas, and we want to hear from .What talks would help you build faster, more reliable async applications? What
challenges are you facing? Let us know! Drop a comment on the Reddit thread, or
mention us on Bluesky or Mastodon.The  is opening soon, and we can’t wait to hear your
ideas.Want to be the first to know when the CFP and ticket sales go live? Sign up for
email updates at tokioconf.com, or follow us on
Bluesky or Mastodon.We’re so excited to bring the community together—see you in Portland next year!]]></content:encoded></item><item><title>pshunt: go terminal app for easily searching for processes to kill</title><link>https://github.com/jamesma100/pshunt</link><author>/u/battle-racket</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 17:42:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I made a simple terminal app for searching for and killing processes. Go and the gocui package made this super easy! I mostly built it for personal use but decided to open source it. Let me know what you think!]]></content:encoded></item><item><title>Java meets JavaScript: dynamic object rendering</title><link>https://blog.picnic.nl/java-meets-javascript-a-modern-approach-to-dynamic-page-rendering-31250dc66f33</link><author>/u/AndrewStetsenko</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 17:38:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Securing Clusters that run Payment Systems</title><link>https://www.reddit.com/r/kubernetes/comments/1lfgtyj/securing_clusters_that_run_payment_systems/</link><author>/u/Icy_Raccoon_1124</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 17:34:18 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[A few of our customers run payment systems inside Kubernetes, with sensitive data, ephemeral workloads, and hybrid cloud traffic. Every workload is isolated but we still need guarantees that nothing reaches unknown networks or executes suspicious code. Our customers keep telling us one thing“Ensure  ever talks to a C2 server.”How do we ensure our DNS is secured?Is runtime behavior monitoring (syscalls + DNS + process ancestry) finally practical now? ]]></content:encoded></item><item><title>The PostgreSQL Locking Trap That Killed Our Production API (and How We Fixed It)</title><link>https://root.sigsegv.in/posts/postgresql-locking-trap/</link><author>/u/N1ghtCod3r</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 17:04:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A simple  statement can bring down your production infrastructure if you’re not careful.
This is the story of me waking up to production alerts while working on totally unrelated tasks (building slides for some business stuff),
getting bamboozled, instinctively blaming most recent production infrastructure change and eventually figuring out how deep the rabbit hole goes.Here is the chain of events if you are skimming through:Google Cloud monitoring policy triggered on database error threshold breachProduction APIs showing high latency and intermittent timeoutsInitially blamed the recently deployed database read replicas as the root causeStopped replication, restarted database instance for service restorationQuickly figured out the issue reappeared on high loadManually deleted PostgreSQL replication slot from primary database instanceManually deleted read replica instances (non-critical), hoping against hopeRaised support ticket with Google cloudSlow query analysis showed multiple pending  queued upShocked realizing that we ran an  on a very large and read heavy table even with multiple guardrails in placeKilled all  queries and blocked all schema migration in production till a permanent fix was implementedAll services restored. FINALLY!Identified the root cause as a lock contention issue across multiple background job workers, schema migrator and long running transactionsIsolated application level locking from business logic tables to a common locks table to avoid lock contention with  which in turn locks the entire tableAll Services restored including internal release related servicesThe Incident: When Everything Just… StoppedIt started with Google Cloud monitoring alerts triggered on database error threshold being breached. Saw a whole bunch of error logs
with Context cancelled by user statement.The immediate reaction (panic) was to blame the recently provisioned read replica for the production database instance. This was done to
safely execute internal analytical queries with Metabase, a non-critical internal service. My response wasStop the replication in the replica instance using Google Cloud consoleRestart the primary database instance hoping that any performance issues due to replication would be resolvedThis temporarily restored the services but the errors quickly returned. It also correlates with time of the day when our usage is at
its peak. My mind correlated this by making a hypothesis that we introduced a new query that may have a missing index leading to
a table scan. But I could not validate this hypothesis by looking at CPU, memory and IO metrics of the primary database instance.In fact, we did not see any replication lag during the last 24 hours. This made me question my hypothesis of the issue caused by
read replicas. Even though that was the most recent infrastructure change that I could confirm looking at our Terraform repository
commit history. Finally decided its time to dig deep because of:No CPU / memory / IO wait metric anomaly for primary database instanceNo replication lag in read replicasBinary log size (storage) started increasing on primary when replication was stopped. This is expected due to active replication slots in the primary.Manually deleted PostgreSQL replication slots from primary database instanceEven deleted the replicas assuming some weirdness with Google CloudSQL and its internal high availability configurationFind the slots used by replicas.The Real Culprit: Lock Contention HellIt was clear that  is not going to fix the issue. It was time to dig deeper. Retrospectively, I think I planned to do the following:Identify long running or expensive queriesIdentify the root cause of the expensive queriesOptimize them by rewriting them or using appropriate indexesThe following query was used to list all active queries and sort them by the query age. This gives a view of the long running queries along with the  and other information required to kill the query if required.Multiple  style locks waiting to be acquiredMultiple ALTER TABLE .. ADD COLUMN statements queued upMultiple INSERT .. ON CONFLICT DO NOTHING statements queued upThe most interesting bit is, all these queries were waiting to acquire a lock on the  table. This is the table where we store OSS package scanning jobs powering SafeDep vet, updated by background job workers and queried by a user facing API. At this point, I was fairly sure it is a lock contention issue but wanted to confirm it before taking any action, especially since I already exhausted myself by reverting infrastructure changes as panic response.The Perfect Storm: How It All Went WrongLets have a quick look at the components that act on the  table and how they interact with each other. The system consists of the following logical components:Submission API that is idempotent and can be retried without creating duplicate jobsBackground job workers that actually execute an OSS package analysis job with a timeout of 15 minutesQuery API that is used to fetch the status and results of a job by its job identifierWe also have a schema migrator built as part of our application development framework, which internally executes GORM migrations with additional safety checks that guarantee timeouts, global locks, audit logs and more.Our API framework is built to execute a business logic (service layer) in transaction by default for consistency unless explicitly opted out by the service specification. We generally opt out of transactions only for read-only operations. In case of the submission API, the service logic does the following in a transaction:Check if the job already exists in the databaseCreate a new job record in the database if it does not existCreate a background job to execute the OSS package analysis job (transactional consistency)Return the job identifier to the clientNote: The submission API is idempotent which requires attempting to read a row from the  table to check if the job already exists before performing an  operation. The table uses unique index constraints to guarantee idempotency even when there is a race condition. This means,  cannot be concurrent.INSERT into tables that lack unique indexes will not be blocked by concurrent activity. Tables with unique indexes might block if concurrent sessions perform actions that lock or modify rows matching the unique index values. RefPackage Analysis Business LogicThe background job workers execute the OSS package analysis job with a timeout of 15 minutes. The job is executed in a transaction and the following steps are performed:Acquire a row-level lock on the  record to prevent multiple workers from executing the same job concurrentlyExecute a long running RPC call to an external service that takes up to 5 minutes to completeUpdate the job record in the databaseExample code snippet from the background job worker:The schema migrator is a tool that we use to safely apply schema changes to the database. In this particular case, following was the schema change that triggered the issue:This schema change adds two new columns to the  table along with adding indexes on them.
This translates to the following SQL statement:ALTER TABLE .. ADD COLUMN statement needs an  on the table even though its just a table metadata updateThe index creation is a separate operation that, by default, needs an  on the table unless  is explicitly specifiedPostgreSQL supports building indexes without locking out writes. This method is invoked by specifying the CONCURRENTLY option of CREATE INDEX. RefThe Lock Hierarchy That KillsHere’s what PostgreSQL’s documentation tells us about :“Conflicts with ALL other lock modes. Guarantees that the holder is the only transaction accessing the table in any way.”Now that we saw the lock hierarchy, we can see there are multiple contenders that contributed to the incident by holding locks or trying to lock the entire tabling by requesting .The root cause of the issue is the following sequence of events: - Acquire row-level locks on  - Waits for  on New Submission API requests arrive - Queue behind the  waiting for table access - All API (app) server Go routines blocked on database calls - API becomes unresponsive, not just for impacted tables but for all API endpointsTwo things were clear so far:Long running transactions are toxic - The best practices are right!Schema migration on large and busy tables are VERY VERY riskyLets ignore the fact that we tried creating an index on a large table, we can mitigate it by updating our schema migration process to leverage maintenance window and concurrency primitives offered by PostgreSQL. But this incident can repeat again even for a harmless ALTER TABLE .. ADD COLUMN statement that only modifies table metadata. This is because, the  will wait for  on the table even though its just a table metadata update. Row level locks held by background job workers makes the table slow and risky for schema changes.Locking is however an application primitive that we need. We have two options:Use an external service like Redis for lockingIsolate locking from business logic tables to a common locks tableWhile blogs and other common wisdom points to [1], we decided against it because it introduces additional complexity of serializing locks and data access across multiple services ie. PostgreSQL and Redis. We decided to continue leverage PostgreSQL for resource (row) level locks but isolate them from business logic tables that may be changed as and when required. Instead, we decided to introduce a common  table that will rarely require schema changes because it offers a single primitive ie. lock a row by  and .Before: Direct Table LockingService level, adhoc locking pattern before the incident:Common resource locking API for use by the service layer. While currently it leverages PostgreSQL backend for locking, the API offers abstractions for us to keep options open for future.Finally, the business logic can be executed without holding any locks on the business logic table
and leveraging the common resource locking API.So far so good. We have a fix in place. But there are challenges with deployment because it requires schema changes and background job workers to be restarted. We were reluctant to execute schema changes in production without a proper maintenance window but we also needed to unblock releases that required schema changes. So we roughly took the following steps:Paused all background jobs for the queue that was running the OSS package analysis jobs to avoid any row level locks on the  tableWaited for all existing jobs to complete and verified no queued queries in the databaseDeployed the fix to productionApplied the schema changes for common locks tableRestarted all background jobs for the queueAha! thats how it looks like after the fix, with usual schema migration and background job workers running again.The Key Insight: Lock IsolationThe breakthrough was realizing that serialization requirements and  are orthogonal concerns. You don’t need to lock the  table to ensure only one password reset email goes out. You don’t need to lock the  table to prevent duplicate payment processing. You need a coordination mechanism that’s separate from your data storage. This ensures both application queries and schema management operations are not blocking by long held locks which may be required by the business logic (service layer).This isn’t about PostgreSQL being bad or Go being bad or our architecture being bad. This is about the fundamental tension between  and availability requirements in distributed systems. The CAP theorem suddenly seems more real, it shows up in your production database unexpectedly when you are trying to add a column and your API dies.]]></content:encoded></item><item><title>A major update of Aralez: High performance, pure Rust, OpenSource proxy server</title><link>https://www.reddit.com/r/rust/comments/1lffvox/a_major_update_of_aralez_high_performance_pure/</link><author>/u/sadoyan</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 16:57:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi r/rust! I am developing OpenSource  (Renamed per your suggestions). A new reverse proxy built on top of Cloudflare's Pingora.Beside all cool features below I have added a new one. Now it can dynamically bulk load SSL certificates from disk and apply per domain, without any configuration. All you need is to set up a path fro certificates . It's full async, high performance, modern reverse proxy with some service mesh functionality with automatic  and  detection and proxy support.It have built in  authentication support with token server, Prometheus exporter and many more fancy features.100% on Rust, Built on top of  fantastic library:  . My recent tests shows it can do  requests per second on moderate hardware.Prebuilt  and  libraries for  and  from are available in releases .If you like this project, please consider giving it a star on ! I also welcome your contributions, such as opening an issue or sending a pull request. Mentoring and suggestions are welcome.]]></content:encoded></item><item><title>The craziest things revealed in The OpenAI Files</title><link>https://www.reddit.com/r/artificial/comments/1lff4gj/the_craziest_things_revealed_in_the_openai_files/</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 16:26:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/MetaKnowing ]]></content:encoded></item><item><title>We finally released v3.4 of ttlcache</title><link>https://www.reddit.com/r/golang/comments/1lfeulh/we_finally_released_v34_of_ttlcache/</link><author>/u/swithek</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 16:15:56 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi everyone, We’re excited to announce the release of v3.4 of ttlcache, an in-memory cache supporting item expiration and generics. The goal of the project remains the same: to provide a cache with an API as straightforward as sync.Map, while allowing you to automatically expire/delete items after a certain time or when a threshold is reached.This release is the result of almost a year of fixes and improvements. Here are the main changes:Custom capacity management, allowing items to have custom cost or weight valuesA new GetOrSetFunc that allows items to be created only when truly neededAn event handler for cache update eventsPerformance improvements, especially for Get() callsMutex usage fixes in Range() and RangeBackwards() methodsThe ability to create plain cache items externally for testingAdditional usage examples]]></content:encoded></item><item><title>Why is this sub filled with posts of some rando “expert” making “predictions”??</title><link>https://www.reddit.com/r/artificial/comments/1lfedpm/why_is_this_sub_filled_with_posts_of_some_rando/</link><author>/u/MrSnowden</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:57:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Are they all low key SEO spam? What is the fascination with podcast talking heads? Almost seems like rage bait regardless of your pov. Am I really supposed to care that this guy thinks AI is a “dead end” (nooo) or this other guy thinks “we will all work for AI I. 7.5 months” (noooo)? /rant]]></content:encoded></item><item><title>Cilium Network Policies</title><link>https://www.reddit.com/r/kubernetes/comments/1lfe6yr/cilium_network_policies/</link><author>/u/AlpsSad9849</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:49:29 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello guys, i am trying to create a CiliumNetworkPolicy to limit outgoing traffic from a certain pods to everything except few other services and one exterl ip addr, my definition is:apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: mytest-policy-egress-restrict namespace: egress spec: endpointSelector: matchLabels: app: myapp egress: - toCIDR: - 192.168.78.11/32 toPorts: - ports: - port: "5454" protocol: TCP If i apply it like this the pod has only access to 78.11/32 on port 5454 , so far so good, but if i add second rule to enable traffic to a certain service in another namespace like this.apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: mytest-policy-egress-restrict namespace: egress spec: endpointSelector: matchLabels: app: myapp egress: - toCIDR: - 192.168.78.11/32 toPorts: - ports: - port: "5454" protocol: TCP - toServices: - k8sServiceSelector: selector: matchLabels: app.kubernetes.io/instance: testService namespace: test the pod still has no access to the service in test namespace, also loses access to its /healtz probes, if i add  toPorts: - ports: - port: "4444" protocol: TCP to my toService directive, the policy at all stops working and allows every outgoing traffic, does anyone has a clue might the problem be]]></content:encoded></item><item><title>How to explain K8s network traffic internally to long term security staff?</title><link>https://www.reddit.com/r/kubernetes/comments/1lfe5ph/how_to_explain_k8s_network_traffic_internally_to/</link><author>/u/colinhines</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:48:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We are trying to explain the reasons why it's not needed to track the port numbers internally in the k8s clusters and ecosystem, but it seems like these security folks who are used to needing the know the port numbers to find out what to monitor or alert on don't seem to "get" it. Is there any easy doc or instructional site that I can point them to in order to explain the perspective now?   submitted by    /u/colinhines ]]></content:encoded></item><item><title>Eliminating dead code in Go projects</title><link>https://mfbmina.dev/en/posts/golang-deadcode/</link><author>/u/mfbmina</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:37:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[As the software we work on grows, the code tends to undergo various changes and refactorings. During this process, we might simply forget pieces of code that were once used but no longer make sense in the project, the infamous dead code. A very common example is when an API is deactivated, and only the  is removed, but all the business logic remains, unused.Dead code can be defined as a function that exists within your codebase, is syntactically valid, but is not used by any other part of your code. In other words, it’s an unreachable function. Dead code brings indirect problems to a project, such as outdated libraries, legacy code, code bloat, security vulnerabilities, and so on. If it’s still not clear what dead code is, see the example below:In this code, we have the private functions  and . By default, gopls
will tell you that the  function is not being used and can be removed. However, this doesn’t prevent the project from compiling.  is a language server used by editors to enable features like code completion, syntax corrections, etc. But if the function becomes public, this error won’t be flagged because it can theoretically be used by other packages.This problem expands when dealing with packages, as unused packages are also not reported. Imagine a package with private and public functions that isn’t used in the project:The Go team then provided a solution to this problem with the deadcode
tool. It’s worth mentioning that the tool should always be run from , as it searches for dead code based on what would be executed in production. When you run this tool, you finally get all unused functions:This way, we can easily find dead code in our project. To install the tool, simply run the command:This tool is very useful to run after project refactorings and has helped me a lot to keep the code lean and containing only what truly matters to the project. If you’re interested and want to know more, I recommend reading the official post
. Tell me in the comments what you think of the tool, and if you want to see the full code, access it here
.]]></content:encoded></item><item><title>The danish also decided to move to Linux</title><link>https://www.reddit.com/r/linux/comments/1lfd6h7/the_danish_also_decided_to_move_to_linux/</link><author>/u/ScientificlyCorrect</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:08:11 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Recently, The Danish Ministry of Digitilisation has decided to move to linux, and abandon windows.The reasoning behind this move is because the DMD (Danish Ministry of Digitilisation) wanted better control, "independant sovereignty" & a less annoying experience of their Operating System.Primarily, they wanted to have better control of their operating system and decided to switch to an open source alternative. They are specificaly switching to LibreOffice's branch, as it "just fits their needs" for their work. The DMD primarily want more control of their Data, Cloud services and Data infrastructure.]]></content:encoded></item><item><title>Why not crosspost this? :D</title><link>https://www.reddit.com/r/linux/comments/1lfczvz/why_not_crosspost_this_d/</link><author>/u/TheTrueOrangeGuy</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 15:00:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Recent optimizations on integer to string conversions</title><link>https://www.reddit.com/r/rust/comments/1lfclzw/recent_optimizations_on_integer_to_string/</link><author>/u/imperioland</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 14:45:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/imperioland ]]></content:encoded></item><item><title>We wrote a IaC framework to operate k8s clusters (and we are open sourcing it)</title><link>https://www.reddit.com/r/kubernetes/comments/1lfcicz/we_wrote_a_iac_framework_to_operate_k8s_clusters/</link><author>/u/thehazarika</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 14:41:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We operate a few decent sized k8s clusters. We noticed a pattern in our usage. So this weekend I decided to extract it out into a "framework". It has a structured way of using terraform and helm.We wrote a thin layer on top of helm (We call it ) that automatically handles encryption of secrets using sops+kms. And it blocks you from running helm commands if you not in the correct cluster and namespace. (This has kept us from regularly shooting ourselves on the foot)And it has a script to setup the whole thing. And it contains and example app, you want to try it out.]]></content:encoded></item><item><title>Ok so you want to build your first AI agent but don&apos;t know where to start? Here&apos;s exactly what I did (step by step)</title><link>https://www.reddit.com/r/artificial/comments/1lfc9eb/ok_so_you_want_to_build_your_first_ai_agent_but/</link><author>/u/soul_eater0001</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 14:31:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Alright so like a year ago I was exactly where most of you probably are right now - knew ChatGPT was cool, heard about "AI agents" everywhere, but had zero clue how to actually build one that does real stuff.After building like 15 different agents (some failed spectacularly lol), here's the exact path I wish someone told me from day one:Step 1: Stop overthinking the tech stack Everyone obsesses over LangChain vs CrewAI vs whatever. Just pick one and stick with it for your first agent. I started with n8n because it's visual and you can see what's happening.Step 2: Build something stupidly simple first My first "agent" literally just:Added them to a Google SheetSent me a Slack message when doneTook like 3 hours, felt like magic. Don't try to build Jarvis on day one.Step 3: The "shadow test" Before coding anything, spend 2-3 hours doing the task manually and document every single step. Like EVERY step. This is where most people mess up - they skip this and wonder why their agent is garbage.Step 4: Start with APIs you already use Gmail, Slack, Google Sheets, Notion - whatever you're already using. Don't learn 5 new tools at once.Step 5: Make it break, then fix it Seriously. Feed your agent weird inputs, disconnect the internet, whatever. Better to find the problems when it's just you testing than when it's handling real work.The whole "learn programming first" thing is kinda BS imo. I built my first 3 agents with zero code using n8n and Zapier. Once you understand the logic flow, learning the coding part is way easier.Also hot take - most "AI agent courses" are overpriced garbage. The best learning happens when you just start building something you actually need.What was your first agent? Did it work or spectacularly fail like mine did? Drop your stories below, always curious what other people tried first.]]></content:encoded></item><item><title>&apos;It’s True, “We” Don’t Care About Accessibility on Linux&apos; — TheEvilSkeleton</title><link>https://tesk.page/2025/06/18/its-true-we-dont-care-about-accessibility-on-linux/</link><author>/u/IverCoder</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 14:10:55 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[What do virtue-signalers and privileged people without disabilities who share content about accessibility on Linux being trash without contributing anything to the software have in common? They don’t actually really care about the group they’re defending; they just exploit these victims’ unfortunate situation to fuel hate against groups and projects actually trying to make the world a better place.I never thought I’d be  upset to a point I’d be writing an article about something this sensitive with a clickbait-y title. It’s simultaneously demotivating, unproductive, and infuriating. I’m here writing this post fully knowing that I could have been working on accessibility in GNOME, but really, I’m so tired of having my mood ruined because of privileged people spending at most 5 minutes to write erroneous posts and then pretending to be oblivious when confronted while it takes us 5 months of unpaid work to get a quarter of recognition, let alone acknowledgment, without accounting for the time “wasted” addressing these accusations. This is far from the first time, and it will certainly not be the last.I’m not mad. I’m absolutely furious  disappointed in the Linux Desktop community for being quiet in regards to any kind of celebration to advancing accessibility, while proceeding to share content and cheer for random privileged people from big-name websites or social media who have literally put a negative amount of effort into advancing accessibility on Linux. I’m explicitly stating a negative amount because they actually make it significantly more stressful for us.None of this is fair. If you’re the kind of person who stays quiet when we celebrate huge accessibility milestones, yet shares (or even makes) content that trash talks the people directly or indirectly contributing to the fucking software you use for free,  are the reason why accessibility on Linux is shit.No one in their right mind wants to volunteer in a toxic environment where their efforts are hardly recognized by the public and they are blamed for “not doing enough”, especially when they are expected to take in all kinds of harassment, nonconstructive criticism, and slander for a salary of 0$.There’s only one thing I am shamefully confident about:  am not okay in the head. I shouldn’t be working on accessibility anymore. The recognition-to-smearing ratio is unbearably low and arguably unhealthy, but leaving people in unfortunate situations behind is also not in accordance with my values.I’ve been putting so much effort, quite literally  of hours, into:thinking of ways to come up with inclusive designs and experiences;imagining how I’d use something if I had a certain disability or condition;asking for advice and feedback from people with disabilities;not getting paid from any company or organization; andmaking sure that all the accessibility-related work is in the public, and .Number 5 is especially important to me. I personally go as far as to refuse to contribute to projects under a permissive license, and/or that utilize a contributor license agreement, and/or that utilize anything riskily similar to these two, because I am of the opinion that no amount of code for accessibility should either be put under a paywall or be obscured and proprietary.Permissive licenses make it painlessly easy for abusers to fork, build an ecosystem on top of it which may include accessibility-related improvements, slap a price tag alongside it, all without publishing any of these additions/changes. Corporations have been doing that for decades, and they’ll keep doing it until there’s heavy push back. The only time I would contribute to a project under a permissive license is when the tool  the accessibility infrastructure itself. Contributor license agreements are significantly worse in that regard, so I prefer to avoid them completely.The GNOME Foundation has been investing a lot of money to improve accessibility on Linux, for example funding Newton, a Wayland accessibility project and AccessKit integration into GNOME technologies. Around 250,000€ (1/4) of the STF budget was spent solely on accessibility. And get this: literally everybody managing these contracts and communication with funders are volunteers; they’re ensuring people with disabilities earn a living, but aren’t receiving anything in return. These are the real heroes who deserve endless praise.Do you want to know who we  be blaming? Profiteers who are profiting from the community’s effort while investing very little to nothing into accessibility.This includes a significant portion of the companies sponsoring GNOME and even companies that employ developers to work on GNOME. These companies are the ones making hundreds of millions, if not billions, in net profit indirectly from GNOME (and other free and open-source projects), and investing little to nothing into them. However, the worst offenders are the companies actively using GNOME without ever donating anything to fund the projects.Some companies actually do put an effort, like Red Hat and Igalia. Red Hat employs people with disabilities to work on accessibility in GNOME, one of which I actually rely on when making accessibility-related contributions in GNOME. Igalia funds Orca, the screen reader as part of GNOME, which is something the Linux community should be thankful of. However, companies have historically invested what’s necessary to comply with governments’ accessibility requirements, and then never invest in it again.The privileged people who keep sharing and making content around accessibility on Linux being bad without contributing anything to it are, in my opinion, significantly worse than the companies profiting off of GNOME. Companies are and stay quiet, but these privileged people add an additional burden to contributors by either trash talking or sharing trash talkers. Once again, no volunteer deserves to be in the position of being shamed and ridiculed for “not doing enough”, since no one is entitled to their free time, but themselves.Earlier in this article, I mentioned, and I quote: “I’ve been putting so much effort, quite literally  of hours […]”. Let’s put an emphasis on “hundreds”. Here’s a list of most accessibility-related merge requests that have been incorporated into GNOME:GNOME Calendar’s !559 addresses an issue where event widgets were unable to be focused and activated by the keyboard. That was present since the very beginning of GNOME Calendar’s existence, to be specific: for more than a decade. This alone was was a two-week effort. Despite it being less than 100 lines of code, nobody truly knew what to do to have them working properly before. This was followed up by !576, which made the event buttons usable in the month view with a keyboard, and then !587, which properly conveys the states of the widgets. Both combined are another two-week effort.Then, at the time of writing this article, !564 adds 640 lines of code, which is something I’ve been volunteering on for more than a month, excluding the time before I opened the merge request.Let’s do a little bit of math together with ‘only’ !559, !576, and !587. Just as a reminder: these three merge requests are a four-week effort in total, which I volunteered full-time—8 hours a day, or 160 hours a month. I compiled a small table that illustrates its worth:Average Wage for Professionals Working on Digital AccessibilityTotal in Local Currency(160 hours)To summarize the table: those three merge requests that I worked on for  were worth 9,393.60$ CAD (6,921.36$ USD) in total at a minimum.these merge requests exclude the time spent to review the submitted code;these merge requests exclude the time I spent testing the code;these merge requests exclude the time we spent coordinating these milestones;these calculations exclude the 30+ merge requests submitted to GNOME; andthese calculations exclude the merge requests I submitted to third-party GNOME-adjacent apps.Now just imagine how I feel when I’m told I’m “not doing enough”, either directly or indirectly, by privileged people who don’t rely on any of these accessibility features. Whenever anybody says we’re “not doing enough”, I feel very much included, and I will absolutely take it personally.I fully expect everything I say in this article to be dismissed or be taken out of context on the basis of ad hominem, simply by the mere fact I’m a GNOME Foundation member / regular GNOME contributor. Either that, or be subject to whataboutism because another GNOME contributor made a comment that had nothing to do with mine but ‘is somewhat related to this topic and therefore should be pointed out just because it was maybe-probably-possibly-perhaps ableist’. I can’t speak for other regular contributors, but I presume that they don’t feel comfortable talking about this because they dared be a GNOME contributor. At least, that’s how I felt for the longest time.Any content related to accessibility that doesn’t dunk on GNOME doesn’t foresee as many engagement, activity, and reaction as content that actively attacks GNOME, regardless of whether the criticism is fair. Many of these people don’t even use these accessibility features; they’re just looking for every opportunity to say “GNOME bad” and will  start caring about accessibility.Regular GNOME contributors like myself don’t always feel comfortable defending ourselves because dismissing GNOME developers just for being GNOME developers is apparently a trend…Dear people with disabilities,I won’t insist that we’re either your allies or your enemies—I have no right to claim that whatsoever.I wasn’t looking for recognition. I wasn’t looking for acknowledgment since the very beginning either. I thought I would be perfectly capable of quietly improving accessibility in GNOME, but because of the overall community’s persistence to smear developers’ efforts without actually tackling the underlying issues within the stack, I think I’ve justified myself to at least demand for acknowledgment from the wider community.I highly doubt it will happen anyway, because the Linux community feeds off of drama and trash talking instead of being productive, without realizing that it negatively demotivates active contributors while pushing away potential contributors. And worst of all: people with disabilities are the ones affected the most because they are misled into thinking that we don’t care.It’s so unfair and infuriating that all the work I do and share online gain very little activity compared to random posts and articles from privileged people without disabilities that rant about the Linux desktop’s accessibility being trash. It doesn’t help that I become severely anxious sharing accessibility-related work to avoid signs of virtue-signaling. The last thing I want is to (unintentionally) give any sign and impression of pretending to care about accessibility.We simultaneously need more interest from people with disabilities to contribute to free and open-source software, and the wider community to be significantly more intolerant of bullies who profit from smearing and demotivating people who are actively trying.]]></content:encoded></item><item><title>[P] I built a self-hosted Databricks</title><link>https://www.reddit.com/r/MachineLearning/comments/1lfbq3m/p_i_built_a_selfhosted_databricks/</link><author>/u/Mission-Balance-4250</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 14:08:14 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey everone, I'm an ML Engineer who spearheaded the adoption of Databricks at work. I love the agency it affords me because I can own projects end-to-end and do everything in one place.However, I am sick of the infra overhead and bells and whistles. Now, I am not in a massive org, but there aren't actually that many massive orgs... So many problems can be solved with a simple data pipeline and basic model (e.g. XGBoost.) Not only is there technical overhead, but systems and process overhead; bureaucracy and red-tap significantly slow delivery.Anyway, I decided to try and address this myself by developing FlintML. Basically, Polars, Delta Lake, unified catalog, Aim experiment tracking, notebook IDE and orchestration (still working on this) fully spun up with Docker Compose.I'm hoping to get some feedback from this subreddit. I've spent a couple of months developing this and want to know whether I would be wasting time by contuining or if this might actually be useful.]]></content:encoded></item><item><title>Why I Choose RUST as my backend language</title><link>https://www.reddit.com/r/rust/comments/1lfayze/why_i_choose_rust_as_my_backend_language/</link><author>/u/junnieboat</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 13:34:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm a JavaScript developer and have been using Node.js (Express) for all my projects mainly because of its non-blocking I/O, which makes handling concurrent requests smooth and efficient.That said, I've never fully trusted JavaScript on the backend — especially when it comes to things like type safety, error handling, and long-term maintainability. The dynamic nature of JS sometimes makes debugging and scaling harder than it should be.Lately, I’ve been exploring other options like Rust (with frameworks like Axum) for more reliable and performant backend services. The compile-time checks, memory safety, and ecosystem are really starting to make sense.Has anyone else made a similar switch or run backend code in both Node.js and Rust? Curious to hear what others think about the trade-offs.]]></content:encoded></item><item><title>YouTube CEO announces Google&apos;s Veo 3 AI video tech is coming to Shorts</title><link>https://www.pcguide.com/news/youtube-ceo-announces-googles-veo-3-ai-video-tech-is-coming-to-shorts/</link><author>/u/Tiny-Independent273</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 13:12:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[
        PC Guide is reader-supported. When you buy through links on our site, we may earn an affiliate commission. Read MoreJust last month, Google unveiled its next iteration of video AI with Veo 3 as its creations flooded the web. YouTube CEO Neal Mohan announced this new feature at the Cannes Lions 2025 Festival of Creativity. Commenting on the fact that “creators are showing us what the future looks like: AI.” YouTube is adding these features to empower human creativity and expand on what they provide to help.Coming later this summer to the app, the inclusion of Veo 3 will let creators use Dream Screen to add AI-generated backgrounds and video clips for Shorts. It doesn’t say if people will have to pay for its use, considering standalone usage has a price to it, but it will let creators utilize the improvements that Veo 3 brings. This includes improved video quality and even adds sound.We promise that the actual PC Guide office looks much nicer than what Google Veo 3 thinks.Alongside adding the new Veo version to YouTube shorts, it also has plenty of more uses for the technology. One of which is using it for Auto Dubbing and translating videos, it already works across nine different languages, and 11 more are coming soon, as a way to expand the audiences for creators’ videos. With 20 million videos already dubbed, it expects plenty more to take advantage of it.Considering it’s been 20 years of YouTube, it’s another method for it to look at expansion and improving on what it already has to offer. Neal Mohan talks of how he expects creators will flip formats, blend genres, and push deeper into the mainstream in the next 20 years for the platform, with AI technology behind it to push the limits of human creativity. We doubt that everyone will be too pleased to see even more AI making its way into content creation, but at least YouTube is being transparent.]]></content:encoded></item><item><title>Gauntlet Language Updated: Sum Types, Reworked Syntax, New Pipe Operator</title><link>https://gauntletlang.gitbook.io/docs/version-release-notes/v0.2.0-alpha</link><author>/u/TricolorHen061</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 12:56:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What Would a Kubernetes 2.0 Look Like</title><link>https://matduggan.com/what-would-a-kubernetes-2-0-look-like/</link><author>/u/LaFoudre250</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 12:39:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Around 2012-2013 I started to hear a  in the sysadmin community about a technology called "Borg". It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a "Borglet" inside of clusters with "cells" but the basics started to leak. There was a concept of "services" and a concept of "jobs", where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for 'helmsman' that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to "maybe this is a real product?" On July 21st 2015 we got the v1.0 release as well as the creation of the CNCF. In the ten years since that initial commit, Kubernetes has become a large part of my professional life. I use it at home, at work, on side projects—anywhere it makes sense. It's a tool with a steep learning curve, but it's also a massive force multiplier. We no longer "manage infrastructure" at the server level; everything is declarative, scalable, recoverable and (if you’re lucky) self-healing.But the journey hasn't been without problems. Some common trends have emerged, where mistakes or misconfiguration arise from where Kubernetes isn't opinionated enough. Even ten years on, we're still seeing a lot of churn inside of ecosystem and people stepping on well-documented landmines. So, knowing what we know now, what could we do differently to make this great tool even more applicable to more people and problems? Let's start with the positive stuff. Why are we still talking about this platform now? Containers as a tool for software development make perfect sense. Ditch the confusion of individual laptop configuration and have one standard, disposable concept that works across the entire stack. While tools like Docker Compose allowed for some deployments of containers, they were clunky and still required you as the admin to manage a lot of the steps. I set up a Compose stack with a deployment script that would remove the instance from the load balancer, pull the new containers, make sure they started and then re-added it to the LB, as did lots of folks. K8s allowed for this concept to scale out, meaning it was possible to take a container from your laptop and deploy an identical container across thousands of servers. This flexibility allowed organizations to revisit their entire design strategy, dropping monoliths and adopting more flexible (and often more complicated) micro-service designs. If you think of the history of Operations as a sort of "naming timeline from pets to cattle", we started with what I affectionately call the "Simpsons" era. Servers were bare metal boxes set up by teams, they often had one-off names that became slang inside of teams and everything was a snowflake. The longer a server ran, the more cruft it picked up until it became a scary operation to even reboot them, much less attempt to rebuild them. I call it the "Simpsons" era because among the jobs I was working at the time, naming them after Simpsons characters was surprisingly common. Nothing fixed itself, everything was a manual operation. Then we transition into the "01 Era". Tools like Puppet and Ansible have become common place, servers are more disposable and you start to see things like bastion hosts and other access control systems become the norm. Servers aren't all facing the internet, they're behind a load balancer and we've dropped the cute names for stuff like "app01" or "vpn02". Organizations designed it so they could lose some of their servers some of the time. However failures still weren't self-healing, someone still had to SSH in to see what broke, write up a fix in the tooling and then deploy it across the entire fleet. OS upgrades were still complicated affairs. We're now in the "UUID Era". Servers exist to run containers, they are entirely disposable concepts. Nobody cares about how long a particular version of the OS is supported for, you just bake a new AMI and replace the entire machine. K8s wasn't the only technology enabling this, but it was the one that accelerated it. Now the idea of a bastion server with SSH keys that I go to the underlying server to fix problems is seen as more of a "break-glass" solution. Almost all solutions are "destroy that Node, let k8s reorganize things as needed, make a new Node". A lot of the Linux skills that were critical to my career are largely nice to have now, not need to have. You can be happy or sad about that, I certainly switch between the two emotions on a regular basis, but it's just the truth. The k8s jobs system isn't perfect, but it's so much better than the "snowflake cron01 box" that was an extremely common sight at jobs for years. Running on a cron schedule or running from a message queue, it was now possible to reliably put jobs into a queue, have them get run, have them restart if they didn't work and then move on with your life. Not only does this free up humans from a time-consuming and boring task, but it's also simply a more efficient use of resources. You are still spinning up a pod for every item in the queue, but your teams have a lot of flexibility inside of the "pod" concept for what they need to run and how they want to run it. This has really been a quality of life improvement for a lot of people, myself included, who just need to be able to easily background tasks and not think about them again. Service Discoverability and Load BalancingHard-coded IP addresses that lived inside of applications as the template for where requests should be routed has been a curse following me around for years. If you were lucky, these dependencies weren't based on IP address but were actually DNS entries and you could change the thing behind the DNS entry without coordinating a deployment of a million applications. K8s allowed for simple DNS names to call other services. It removed an entire category of errors and hassle and simplified the entire thing down. With the Service API you had a stable, long lived IP and hostname that you could just point things towards and not think about any of the underlying concepts. You even have concepts like ExternalName that allow you to treat external services like they're in the cluster. What would I put in a Kubernetes 2.0?YAML was appealing because it wasn't JSON or XML, which is like saying your new car is great because it's neither a horse nor a unicycle. It demos nicer for k8s, looks nicer sitting in a repo and has the  of being a simple file format. In reality. YAML is just too much for what we're trying to do with k8s and it's not a safe enough format. Indentation is error-prone, the files don't scale great (you really don't want a super long YAML file), debugging can be annoying. YAML has  subtle behaviors outlined in its spec.I still remember not believing what I was seeing the first time I saw the Norway Problem. For those lucky enough to not deal with it, the Norway Problem in YAML is when 'NO' gets interpreted as false. Imagine explaining to your Norwegian colleagues that their entire country evaluates to false in your configuration files. Add in accidental numbers from lack of quotes, the list goes on and on. There are much better posts on why YAML is crazy than I'm capable of writing: https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hellHCL is already the format for Terraform, so at least we'd only have to hate one configuration language instead of two. It's strongly typed with explicit types. There's already good validation mechanisms. It is specifically designed to do the job that we are asking YAML to do and it's not much harder to read. It has built-in functions people are already using that would allow us to remove some of the third-party tooling from the YAML workflow. I would wager 30% of Kubernetes clusters today are  being managed with HCL via Terraform. We don't need the Terraform part to get a lot of the benefits of a superior configuration language. The only downsides are that HCL is slightly more verbose than YAML, and its Mozilla Public License 2.0 (MPL-2.0) would require careful legal review for integration into an Apache 2.0 project like Kubernetes. However, for the quality-of-life improvements it offers, these are hurdles worth clearing.Let's take a simple YAML file. # YAML doesn't enforce types
replicas: "3"  # String instead of integer
resources:
  limits:
    memory: 512  # Missing unit suffix
  requests:
    cpu: 0.5m    # Typo in CPU unit (should be 500m)Even in the most basic example, there are footguns everywhere. HCL and the type system would catch all of these problems. replicas = 3  # Explicitly an integer

resources {
  limits {
    memory = "512Mi"  # String for memory values
  }
  requests {
    cpu = 0.5  # Number for CPU values
  }
}Take a YAML file like this that you probably have 6000 in your k8s repo. Now look at HCL without needing external tooling. # Need external tools or templating for dynamic values
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Can't easily generate or transform values
  DATABASE_URL: "postgres://user:password@db:5432/mydb"
  API_KEY: "static-key-value"
  TIMESTAMP: "2023-06-18T00:00:00Z"  # Hard-coded timestampresource "kubernetes_config_map" "app_config" {
  metadata {
    name = "app-config"
  }
  
  data = {
    DATABASE_URL = "postgres://${var.db_user}:${var.db_password}@${var.db_host}:${var.db_port}/${var.db_name}"
    API_KEY      = var.api_key != "" ? var.api_key : random_string.api_key.result
    TIMESTAMP    = timestamp()
  }
}

resource "random_string" "api_key" {
  length  = 32
  special = false
}Here's all the pros you get with this move. : Preventing type-related errors before deployment: Reducing duplication and improving maintainabilityFunctions and Expressions: Enabling dynamic configuration generation: Supporting environment-specific configurations: Simplifying repetitive configurations: Improving documentation and readability: Making errors easier to identify and fix: Enabling reuse of configuration components: Preventing invalid configurations: Supporting complex data manipulationsI know, I'm the 10,000 person to write this. Etcd has done a fine job, but it's a little crazy that it is the only tool for the job. For smaller clusters or smaller hardware configuration, it's a large use of resources in a cluster type where you will never hit the node count where it pays off. It's also a strange relationship between k8s and etcd now, where k8s is basically the only etcd customer left. What I'm suggesting is taking the work of kine and making it official. It makes sense for the long-term health of the project to have the ability to plug in more backends, adding this abstraction means it (should) be easier to swap in new/different backends in the future and it also allows for more specific tuning depending on the hardware I'm putting out there. What I suspect this would end up looking like is much like this: https://github.com/canonical/k8s-dqlite. Distributed SQlite in-memory with Raft consensus and almost zero upgrade work required that would allow cluster operators to have more flexibility with the persistence layer of their k8s installations. If you have a conventional server setup in a datacenter and etcd resource usage is not a problem, great! But this allows for lower-end k8s to be a nicer experience and (hopefully) reduces dependence on the etcd project. Beyond Helm: A Native Package ManagerHelm is a perfect example of a temporary hack that has grown to be a permanent dependency. I'm grateful to the maintainers of Helm for all of their hard work, growing what was originally a hackathon project into the de-facto way to install software into k8s clusters. It has done as good a job as something could in fulfilling that role without having a deeper integration into k8s. All that said, Helm is a nightmare to use. The Go templates are tricky to debug, often containing complex logic that results in really confusing error scenarios. The error messages you get from those scenarios are often gibberish. Helm isn't a very good package system because it fails at some of the basic tasks you need a package system to do, which are transitive dependencies and resolving conflicts between dependencies. Tell me what this conditional logic is trying to do:# A real-world example of complex conditional logic in Helm
{{- if or (and .Values.rbac.create .Values.serviceAccount.create) (and .Values.rbac.create (not .Values.serviceAccount.create) .Values.serviceAccount.name) }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ template "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
{{- end }}Or if I provide multiple values files to my chart, which one wins:helm install myapp ./mychart -f values-dev.yaml -f values-override.yaml --set service.type=NodePortOk, what if I want to manage my application and all the application dependencies with a Helm chart. This makes sense, I have an application that itself has dependencies on other stuff so I want to put them all together. So I define my sub-charts or umbrella charts inside of my Chart.yaml. dependencies:
- name: nginx
  version: "1.2.3"
  repository: "<https://example.com/charts>"
- name: memcached
  version: "1.2.3"
  repository: "<https://another.example.com/charts>"
But assuming I have multiple applications, it's entirely possible that I have 2 services both with a dependency on nginx or whatever like this:Helm doesn't handle this situation gracefully because template names are global with their templates loaded alphabetically. Basically you need to:Don't declare a dependency on the same chart more than once (hard to do for a lot of microservices)If you do have the same chart declared multiple times, has to use the exact same versionThe list of issues goes on and on. Cross-Namespace installation stinksChart verification process is a pain and nobody uses itLet's just go to the front page of artifacthub:I'll grab elasticsearch cause that seems important. Seems  for the Official Elastic helm chart. Certainly  will be right, it's an absolute critical dependency for the entire industry. Nope. Also how is the maintainer of the chart "Kubernetes" and it's  not marked as a . Like Christ how much more verified does it get.No metadata in chart searching. You can only search by name and description, not by features, capabilities, or other metadata.Helm doesn't strictly enforce semantic versioning# Chart.yaml with non-semantic version
apiVersion: v2
name: myapp
version: "v1.2-alpha" If you uninstall and reinstall a chart with CRDs, it might delete resources created by those CRDs. This one has screwed me  and is crazy unsafe. I could keep writing for another 5000 words and still wouldn't have outlined all the problems. There isn't a way to make Helm good enough for the task of "package manager for all the critical infrastructure on the planet". What would a k8s package system look like?Let's call our hypothetical package system KubePkg, because if there's one thing the Kubernetes ecosystem needs, it's another abbreviated name with a 'K' in it. We would try to copy as much of the existing work inside the Linux ecosystem while taking advantage of the CRD power of k8s. My idea looks something like this:The packages are bundles like a Linux package:There's a definition file that accounts for as many of the real scenarios that you actually encounter when installing a thing. apiVersion: kubepkg.io/v1
kind: Package
metadata:
  name: postgresql
  version: 14.5.2
spec:
  maintainer:
    name: "PostgreSQL Team"
    email: "[email protected]"
  description: "PostgreSQL database server"
  website: "https://postgresql.org"
  license: "PostgreSQL"
  
  # Dependencies with semantic versioning
  dependencies:
    - name: storage-provisioner
      versionConstraint: ">=1.0.0"
    - name: metrics-collector
      versionConstraint: "^2.0.0"
      optional: true
  
  # Security context and requirements
  security:
    requiredCapabilities: ["CHOWN", "SETGID", "SETUID"]
    securityContextConstraints:
      runAsUser: 999
      fsGroup: 999
    networkPolicies:
      - ports:
        - port: 5432
          protocol: TCP
    
  # Resources to be created (embedded or referenced)
  resources:
    - apiVersion: v1
      kind: Service
      metadata:
        name: postgresql
      spec:
        ports:
        - port: 5432
    - apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: postgresql
      spec:
        # StatefulSet definition
  
  # Configuration schema using JSON Schema
  configurationSchema:
    type: object
    properties:
      replicas:
        type: integer
        minimum: 1
        default: 1
      persistence:
        type: object
        properties:
          size:
            type: string
            pattern: "^[0-9]+[GMK]i$"
            default: "10Gi"
  
  # Lifecycle hooks with proper sequencing
  hooks:
    preInstall:
      - name: database-prerequisites
        job:
          spec:
            template:
              spec:
                containers:
                - name: init
                  image: postgres:14.5
    postInstall:
      - name: database-init
        job:
          spec:
            # Job definition
    preUpgrade:
      - name: backup
        job:
          spec:
            # Backup job definition
    postUpgrade:
      - name: verify
        job:
          spec:
            # Verification job definition
    preRemove:
      - name: final-backup
        job:
          spec:
            # Final backup job definition
  
  # State management for stateful applications
  stateManagement:
    backupStrategy:
      type: "snapshot"  # or "dump"
      schedule: "0 2 * * *"  # Daily at 2 AM
      retention:
        count: 7
    recoveryStrategy:
      type: "pointInTime"
      verificationJob:
        spec:
          # Job to verify recovery success
    dataLocations:
      - path: "/var/lib/postgresql/data"
        volumeMount: "data"
    upgradeStrategies:
      - fromVersion: "*"
        toVersion: "*"
        strategy: "backup-restore"
      - fromVersion: "14.*.*"
        toVersion: "14.*.*"
        strategy: "in-place"There's a real signing process that would be required and allow you more control over the process. apiVersion: kubepkg.io/v1
kind: Repository
metadata:
  name: official-repo
spec:
  url: "https://repo.kubepkg.io/official"
  type: "OCI"  # or "HTTP"
  
  # Verification settings
  verification:
    publicKeys:
      - name: "KubePkg Official"
        keyData: |
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvF4+...
          -----END PUBLIC KEY-----
    trustPolicy:
      type: "AllowList"  # or "KeyRing"
      allowedSigners:
        - "KubePkg Official"
        - "Trusted Partner"
    verificationLevel: "Strict"  # or "Warn", "None"Like how great would it be to have something where I could automatically update packages without needing to do anything on my side. apiVersion: kubepkg.io/v1
kind: Installation
metadata:
  name: postgresql-main
  namespace: database
spec:
  packageRef:
    name: postgresql
    version: "14.5.2"
  
  # Configuration values (validated against schema)
  configuration:
    replicas: 3
    persistence:
      size: "100Gi"
    resources:
      limits:
        memory: "4Gi"
        cpu: "2"
  
  # Update policy
  updatePolicy:
    automatic: false
    allowedVersions: "14.x.x"
    schedule: "0 2 * * 0"  # Weekly on Sunday at 2am
    approvalRequired: true
  
  # State management reference
  stateRef:
    name: postgresql-main-state
    
  # Service account to use
  serviceAccountName: postgresql-installerWhat k8s needs is a system that meets the following requirements:: Everything is a Kubernetes resource with proper status and eventsFirst-Class State Management: Built-in support for stateful applications: Robust signing, verification, and security scanningDeclarative Configuration: No templates, just structured configuration with schemas: Comprehensive lifecycle hooks and upgrade strategies: Linux-like dependency management with semantic versioning: Complete history of changes with who, what, and when, not what Helm currently provides. : Support for organizational policies and compliance. Simplified User Experience: Familiar Linux-like package management commands. It seems wild that we're trying to go a different direction from the package systems that have worked for decades. Try to imagine, across the entire globe, how much time and energy has been invested in trying to solve any one of the following three problems. I need this pod in this cluster to talk to that pod in that cluster. There is a problem happening somewhere in the NAT traversal process and I need to solve itI have run out of IP addresses with my cluster because I didn't account for how many you use. Remember: A company starting with a /20 subnet (4,096 addresses), deploys 40 nodes with 30 pods each, and suddenly realizes they're approaching their IP limit. Not that many nodes!I am not suggesting the entire internet switches over to IPv6 and right now k8s happily supports IPv6-only if you want and a dualstack approach. But I'm saying now is the time to flip the default and just go IPv6. You eliminate a huge collection of problems all at once. Flatter, less complicated network topology inside of the cluster. The distinction between multiple clusters becomes a thing organizations can choose to ignore if they want if they want to get public IPs.Easier to understand exactly the flow of traffic inside of your stack. It has nothing to do with driving IPv6 adoption across the entire globe and just an acknowledgement that we no longer live in a world where you have to accept the weird limitations of IPv4 in a universe where you may need 10,000 IPs suddenly with very little warning. The benefits for organizations with public IPv6 addresses is pretty obvious, but there's enough value there for cloud providers and users that even the corporate overlords might get behind it. AWS never needs to try and scrounge up more private IPv4 space inside of a VPC. That's gotta be worth something. The common rebuttal to these ideas is, "Kubernetes is an open platform, so the community can build these solutions." While true, this argument misses a crucial point: defaults are the most powerful force in technology. The "happy path" defined by the core project dictates how 90% of users will interact with it. If the system defaults to expecting signed packages and provides a robust, native way to manage them, that is what the ecosystem will adopt.This is an ambitious list, I know. But if we're going to dream, let's dream big. After all, we're the industry that thought naming a technology 'Kubernetes' would catch on, and somehow it did!We see this all the time in other areas like mobile developer and web development, where platforms assess their situation and make  jumps forward. Not all of these are necessarily projects that the maintainers or companies  take on but I think they're all ideas that  should at least revisit and think "is it worth doing now that we're this nontrivial percentage of all datacenter operations on the planet"? ]]></content:encoded></item><item><title>What Would a Kubernetes 2.0 Look Like</title><link>https://matduggan.com/what-would-a-kubernetes-2-0-look-like/</link><author>/u/LaFoudre250</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 12:39:05 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Around 2012-2013 I started to hear a  in the sysadmin community about a technology called "Borg". It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a "Borglet" inside of clusters with "cells" but the basics started to leak. There was a concept of "services" and a concept of "jobs", where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for 'helmsman' that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to "maybe this is a real product?" On July 21st 2015 we got the v1.0 release as well as the creation of the CNCF. In the ten years since that initial commit, Kubernetes has become a large part of my professional life. I use it at home, at work, on side projects—anywhere it makes sense. It's a tool with a steep learning curve, but it's also a massive force multiplier. We no longer "manage infrastructure" at the server level; everything is declarative, scalable, recoverable and (if you’re lucky) self-healing.But the journey hasn't been without problems. Some common trends have emerged, where mistakes or misconfiguration arise from where Kubernetes isn't opinionated enough. Even ten years on, we're still seeing a lot of churn inside of ecosystem and people stepping on well-documented landmines. So, knowing what we know now, what could we do differently to make this great tool even more applicable to more people and problems? Let's start with the positive stuff. Why are we still talking about this platform now? Containers as a tool for software development make perfect sense. Ditch the confusion of individual laptop configuration and have one standard, disposable concept that works across the entire stack. While tools like Docker Compose allowed for some deployments of containers, they were clunky and still required you as the admin to manage a lot of the steps. I set up a Compose stack with a deployment script that would remove the instance from the load balancer, pull the new containers, make sure they started and then re-added it to the LB, as did lots of folks. K8s allowed for this concept to scale out, meaning it was possible to take a container from your laptop and deploy an identical container across thousands of servers. This flexibility allowed organizations to revisit their entire design strategy, dropping monoliths and adopting more flexible (and often more complicated) micro-service designs. If you think of the history of Operations as a sort of "naming timeline from pets to cattle", we started with what I affectionately call the "Simpsons" era. Servers were bare metal boxes set up by teams, they often had one-off names that became slang inside of teams and everything was a snowflake. The longer a server ran, the more cruft it picked up until it became a scary operation to even reboot them, much less attempt to rebuild them. I call it the "Simpsons" era because among the jobs I was working at the time, naming them after Simpsons characters was surprisingly common. Nothing fixed itself, everything was a manual operation. Then we transition into the "01 Era". Tools like Puppet and Ansible have become common place, servers are more disposable and you start to see things like bastion hosts and other access control systems become the norm. Servers aren't all facing the internet, they're behind a load balancer and we've dropped the cute names for stuff like "app01" or "vpn02". Organizations designed it so they could lose some of their servers some of the time. However failures still weren't self-healing, someone still had to SSH in to see what broke, write up a fix in the tooling and then deploy it across the entire fleet. OS upgrades were still complicated affairs. We're now in the "UUID Era". Servers exist to run containers, they are entirely disposable concepts. Nobody cares about how long a particular version of the OS is supported for, you just bake a new AMI and replace the entire machine. K8s wasn't the only technology enabling this, but it was the one that accelerated it. Now the idea of a bastion server with SSH keys that I go to the underlying server to fix problems is seen as more of a "break-glass" solution. Almost all solutions are "destroy that Node, let k8s reorganize things as needed, make a new Node". A lot of the Linux skills that were critical to my career are largely nice to have now, not need to have. You can be happy or sad about that, I certainly switch between the two emotions on a regular basis, but it's just the truth. The k8s jobs system isn't perfect, but it's so much better than the "snowflake cron01 box" that was an extremely common sight at jobs for years. Running on a cron schedule or running from a message queue, it was now possible to reliably put jobs into a queue, have them get run, have them restart if they didn't work and then move on with your life. Not only does this free up humans from a time-consuming and boring task, but it's also simply a more efficient use of resources. You are still spinning up a pod for every item in the queue, but your teams have a lot of flexibility inside of the "pod" concept for what they need to run and how they want to run it. This has really been a quality of life improvement for a lot of people, myself included, who just need to be able to easily background tasks and not think about them again. Service Discoverability and Load BalancingHard-coded IP addresses that lived inside of applications as the template for where requests should be routed has been a curse following me around for years. If you were lucky, these dependencies weren't based on IP address but were actually DNS entries and you could change the thing behind the DNS entry without coordinating a deployment of a million applications. K8s allowed for simple DNS names to call other services. It removed an entire category of errors and hassle and simplified the entire thing down. With the Service API you had a stable, long lived IP and hostname that you could just point things towards and not think about any of the underlying concepts. You even have concepts like ExternalName that allow you to treat external services like they're in the cluster. What would I put in a Kubernetes 2.0?YAML was appealing because it wasn't JSON or XML, which is like saying your new car is great because it's neither a horse nor a unicycle. It demos nicer for k8s, looks nicer sitting in a repo and has the  of being a simple file format. In reality. YAML is just too much for what we're trying to do with k8s and it's not a safe enough format. Indentation is error-prone, the files don't scale great (you really don't want a super long YAML file), debugging can be annoying. YAML has  subtle behaviors outlined in its spec.I still remember not believing what I was seeing the first time I saw the Norway Problem. For those lucky enough to not deal with it, the Norway Problem in YAML is when 'NO' gets interpreted as false. Imagine explaining to your Norwegian colleagues that their entire country evaluates to false in your configuration files. Add in accidental numbers from lack of quotes, the list goes on and on. There are much better posts on why YAML is crazy than I'm capable of writing: https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hellHCL is already the format for Terraform, so at least we'd only have to hate one configuration language instead of two. It's strongly typed with explicit types. There's already good validation mechanisms. It is specifically designed to do the job that we are asking YAML to do and it's not much harder to read. It has built-in functions people are already using that would allow us to remove some of the third-party tooling from the YAML workflow. I would wager 30% of Kubernetes clusters today are  being managed with HCL via Terraform. We don't need the Terraform part to get a lot of the benefits of a superior configuration language. The only downsides are that HCL is slightly more verbose than YAML, and its Mozilla Public License 2.0 (MPL-2.0) would require careful legal review for integration into an Apache 2.0 project like Kubernetes. However, for the quality-of-life improvements it offers, these are hurdles worth clearing.Let's take a simple YAML file. # YAML doesn't enforce types
replicas: "3"  # String instead of integer
resources:
  limits:
    memory: 512  # Missing unit suffix
  requests:
    cpu: 0.5m    # Typo in CPU unit (should be 500m)Even in the most basic example, there are footguns everywhere. HCL and the type system would catch all of these problems. replicas = 3  # Explicitly an integer

resources {
  limits {
    memory = "512Mi"  # String for memory values
  }
  requests {
    cpu = 0.5  # Number for CPU values
  }
}Take a YAML file like this that you probably have 6000 in your k8s repo. Now look at HCL without needing external tooling. # Need external tools or templating for dynamic values
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Can't easily generate or transform values
  DATABASE_URL: "postgres://user:password@db:5432/mydb"
  API_KEY: "static-key-value"
  TIMESTAMP: "2023-06-18T00:00:00Z"  # Hard-coded timestampresource "kubernetes_config_map" "app_config" {
  metadata {
    name = "app-config"
  }
  
  data = {
    DATABASE_URL = "postgres://${var.db_user}:${var.db_password}@${var.db_host}:${var.db_port}/${var.db_name}"
    API_KEY      = var.api_key != "" ? var.api_key : random_string.api_key.result
    TIMESTAMP    = timestamp()
  }
}

resource "random_string" "api_key" {
  length  = 32
  special = false
}Here's all the pros you get with this move. : Preventing type-related errors before deployment: Reducing duplication and improving maintainabilityFunctions and Expressions: Enabling dynamic configuration generation: Supporting environment-specific configurations: Simplifying repetitive configurations: Improving documentation and readability: Making errors easier to identify and fix: Enabling reuse of configuration components: Preventing invalid configurations: Supporting complex data manipulationsI know, I'm the 10,000 person to write this. Etcd has done a fine job, but it's a little crazy that it is the only tool for the job. For smaller clusters or smaller hardware configuration, it's a large use of resources in a cluster type where you will never hit the node count where it pays off. It's also a strange relationship between k8s and etcd now, where k8s is basically the only etcd customer left. What I'm suggesting is taking the work of kine and making it official. It makes sense for the long-term health of the project to have the ability to plug in more backends, adding this abstraction means it (should) be easier to swap in new/different backends in the future and it also allows for more specific tuning depending on the hardware I'm putting out there. What I suspect this would end up looking like is much like this: https://github.com/canonical/k8s-dqlite. Distributed SQlite in-memory with Raft consensus and almost zero upgrade work required that would allow cluster operators to have more flexibility with the persistence layer of their k8s installations. If you have a conventional server setup in a datacenter and etcd resource usage is not a problem, great! But this allows for lower-end k8s to be a nicer experience and (hopefully) reduces dependence on the etcd project. Beyond Helm: A Native Package ManagerHelm is a perfect example of a temporary hack that has grown to be a permanent dependency. I'm grateful to the maintainers of Helm for all of their hard work, growing what was originally a hackathon project into the de-facto way to install software into k8s clusters. It has done as good a job as something could in fulfilling that role without having a deeper integration into k8s. All that said, Helm is a nightmare to use. The Go templates are tricky to debug, often containing complex logic that results in really confusing error scenarios. The error messages you get from those scenarios are often gibberish. Helm isn't a very good package system because it fails at some of the basic tasks you need a package system to do, which are transitive dependencies and resolving conflicts between dependencies. Tell me what this conditional logic is trying to do:# A real-world example of complex conditional logic in Helm
{{- if or (and .Values.rbac.create .Values.serviceAccount.create) (and .Values.rbac.create (not .Values.serviceAccount.create) .Values.serviceAccount.name) }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ template "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
{{- end }}Or if I provide multiple values files to my chart, which one wins:helm install myapp ./mychart -f values-dev.yaml -f values-override.yaml --set service.type=NodePortOk, what if I want to manage my application and all the application dependencies with a Helm chart. This makes sense, I have an application that itself has dependencies on other stuff so I want to put them all together. So I define my sub-charts or umbrella charts inside of my Chart.yaml. dependencies:
- name: nginx
  version: "1.2.3"
  repository: "<https://example.com/charts>"
- name: memcached
  version: "1.2.3"
  repository: "<https://another.example.com/charts>"
But assuming I have multiple applications, it's entirely possible that I have 2 services both with a dependency on nginx or whatever like this:Helm doesn't handle this situation gracefully because template names are global with their templates loaded alphabetically. Basically you need to:Don't declare a dependency on the same chart more than once (hard to do for a lot of microservices)If you do have the same chart declared multiple times, has to use the exact same versionThe list of issues goes on and on. Cross-Namespace installation stinksChart verification process is a pain and nobody uses itLet's just go to the front page of artifacthub:I'll grab elasticsearch cause that seems important. Seems  for the Official Elastic helm chart. Certainly  will be right, it's an absolute critical dependency for the entire industry. Nope. Also how is the maintainer of the chart "Kubernetes" and it's  not marked as a . Like Christ how much more verified does it get.No metadata in chart searching. You can only search by name and description, not by features, capabilities, or other metadata.Helm doesn't strictly enforce semantic versioning# Chart.yaml with non-semantic version
apiVersion: v2
name: myapp
version: "v1.2-alpha" If you uninstall and reinstall a chart with CRDs, it might delete resources created by those CRDs. This one has screwed me  and is crazy unsafe. I could keep writing for another 5000 words and still wouldn't have outlined all the problems. There isn't a way to make Helm good enough for the task of "package manager for all the critical infrastructure on the planet". What would a k8s package system look like?Let's call our hypothetical package system KubePkg, because if there's one thing the Kubernetes ecosystem needs, it's another abbreviated name with a 'K' in it. We would try to copy as much of the existing work inside the Linux ecosystem while taking advantage of the CRD power of k8s. My idea looks something like this:The packages are bundles like a Linux package:There's a definition file that accounts for as many of the real scenarios that you actually encounter when installing a thing. apiVersion: kubepkg.io/v1
kind: Package
metadata:
  name: postgresql
  version: 14.5.2
spec:
  maintainer:
    name: "PostgreSQL Team"
    email: "[email protected]"
  description: "PostgreSQL database server"
  website: "https://postgresql.org"
  license: "PostgreSQL"
  
  # Dependencies with semantic versioning
  dependencies:
    - name: storage-provisioner
      versionConstraint: ">=1.0.0"
    - name: metrics-collector
      versionConstraint: "^2.0.0"
      optional: true
  
  # Security context and requirements
  security:
    requiredCapabilities: ["CHOWN", "SETGID", "SETUID"]
    securityContextConstraints:
      runAsUser: 999
      fsGroup: 999
    networkPolicies:
      - ports:
        - port: 5432
          protocol: TCP
    
  # Resources to be created (embedded or referenced)
  resources:
    - apiVersion: v1
      kind: Service
      metadata:
        name: postgresql
      spec:
        ports:
        - port: 5432
    - apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: postgresql
      spec:
        # StatefulSet definition
  
  # Configuration schema using JSON Schema
  configurationSchema:
    type: object
    properties:
      replicas:
        type: integer
        minimum: 1
        default: 1
      persistence:
        type: object
        properties:
          size:
            type: string
            pattern: "^[0-9]+[GMK]i$"
            default: "10Gi"
  
  # Lifecycle hooks with proper sequencing
  hooks:
    preInstall:
      - name: database-prerequisites
        job:
          spec:
            template:
              spec:
                containers:
                - name: init
                  image: postgres:14.5
    postInstall:
      - name: database-init
        job:
          spec:
            # Job definition
    preUpgrade:
      - name: backup
        job:
          spec:
            # Backup job definition
    postUpgrade:
      - name: verify
        job:
          spec:
            # Verification job definition
    preRemove:
      - name: final-backup
        job:
          spec:
            # Final backup job definition
  
  # State management for stateful applications
  stateManagement:
    backupStrategy:
      type: "snapshot"  # or "dump"
      schedule: "0 2 * * *"  # Daily at 2 AM
      retention:
        count: 7
    recoveryStrategy:
      type: "pointInTime"
      verificationJob:
        spec:
          # Job to verify recovery success
    dataLocations:
      - path: "/var/lib/postgresql/data"
        volumeMount: "data"
    upgradeStrategies:
      - fromVersion: "*"
        toVersion: "*"
        strategy: "backup-restore"
      - fromVersion: "14.*.*"
        toVersion: "14.*.*"
        strategy: "in-place"There's a real signing process that would be required and allow you more control over the process. apiVersion: kubepkg.io/v1
kind: Repository
metadata:
  name: official-repo
spec:
  url: "https://repo.kubepkg.io/official"
  type: "OCI"  # or "HTTP"
  
  # Verification settings
  verification:
    publicKeys:
      - name: "KubePkg Official"
        keyData: |
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvF4+...
          -----END PUBLIC KEY-----
    trustPolicy:
      type: "AllowList"  # or "KeyRing"
      allowedSigners:
        - "KubePkg Official"
        - "Trusted Partner"
    verificationLevel: "Strict"  # or "Warn", "None"Like how great would it be to have something where I could automatically update packages without needing to do anything on my side. apiVersion: kubepkg.io/v1
kind: Installation
metadata:
  name: postgresql-main
  namespace: database
spec:
  packageRef:
    name: postgresql
    version: "14.5.2"
  
  # Configuration values (validated against schema)
  configuration:
    replicas: 3
    persistence:
      size: "100Gi"
    resources:
      limits:
        memory: "4Gi"
        cpu: "2"
  
  # Update policy
  updatePolicy:
    automatic: false
    allowedVersions: "14.x.x"
    schedule: "0 2 * * 0"  # Weekly on Sunday at 2am
    approvalRequired: true
  
  # State management reference
  stateRef:
    name: postgresql-main-state
    
  # Service account to use
  serviceAccountName: postgresql-installerWhat k8s needs is a system that meets the following requirements:: Everything is a Kubernetes resource with proper status and eventsFirst-Class State Management: Built-in support for stateful applications: Robust signing, verification, and security scanningDeclarative Configuration: No templates, just structured configuration with schemas: Comprehensive lifecycle hooks and upgrade strategies: Linux-like dependency management with semantic versioning: Complete history of changes with who, what, and when, not what Helm currently provides. : Support for organizational policies and compliance. Simplified User Experience: Familiar Linux-like package management commands. It seems wild that we're trying to go a different direction from the package systems that have worked for decades. Try to imagine, across the entire globe, how much time and energy has been invested in trying to solve any one of the following three problems. I need this pod in this cluster to talk to that pod in that cluster. There is a problem happening somewhere in the NAT traversal process and I need to solve itI have run out of IP addresses with my cluster because I didn't account for how many you use. Remember: A company starting with a /20 subnet (4,096 addresses), deploys 40 nodes with 30 pods each, and suddenly realizes they're approaching their IP limit. Not that many nodes!I am not suggesting the entire internet switches over to IPv6 and right now k8s happily supports IPv6-only if you want and a dualstack approach. But I'm saying now is the time to flip the default and just go IPv6. You eliminate a huge collection of problems all at once. Flatter, less complicated network topology inside of the cluster. The distinction between multiple clusters becomes a thing organizations can choose to ignore if they want if they want to get public IPs.Easier to understand exactly the flow of traffic inside of your stack. It has nothing to do with driving IPv6 adoption across the entire globe and just an acknowledgement that we no longer live in a world where you have to accept the weird limitations of IPv4 in a universe where you may need 10,000 IPs suddenly with very little warning. The benefits for organizations with public IPv6 addresses is pretty obvious, but there's enough value there for cloud providers and users that even the corporate overlords might get behind it. AWS never needs to try and scrounge up more private IPv4 space inside of a VPC. That's gotta be worth something. The common rebuttal to these ideas is, "Kubernetes is an open platform, so the community can build these solutions." While true, this argument misses a crucial point: defaults are the most powerful force in technology. The "happy path" defined by the core project dictates how 90% of users will interact with it. If the system defaults to expecting signed packages and provides a robust, native way to manage them, that is what the ecosystem will adopt.This is an ambitious list, I know. But if we're going to dream, let's dream big. After all, we're the industry that thought naming a technology 'Kubernetes' would catch on, and somehow it did!We see this all the time in other areas like mobile developer and web development, where platforms assess their situation and make  jumps forward. Not all of these are necessarily projects that the maintainers or companies  take on but I think they're all ideas that  should at least revisit and think "is it worth doing now that we're this nontrivial percentage of all datacenter operations on the planet"? ]]></content:encoded></item><item><title>MetalLB BGP setup</title><link>https://www.reddit.com/r/kubernetes/comments/1lf91d6/metallb_bgp_setup/</link><author>/u/Several_Yoghurt1759</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 12:02:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[How do you guys maintain your BGP config on your ToR devices? Firewall in my caseIf I’m setting up my production cluster with metallb bgp mode, and I’ve peered with each of the nodes from the firewall what happens when the autoscaler scales out or in or a cluster upgrade spins up entirely new nodes? ]]></content:encoded></item><item><title>My 1978 analog mockumentary was mistaken for AI. Is this the future of media perception?</title><link>https://www.reddit.com/r/artificial/comments/1lf8y7q/my_1978_analog_mockumentary_was_mistaken_for_ai/</link><author>/u/strippedlugnut</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:57:44 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[I did an AMA on r/movies, and the wildest takeaway was how many people assumed the real world 1978 trailer imagery was AI-generated. Ironically the only thing that was AI was all the audio that no one questioned until I told them.It genuinely made me stop and think: Have we reached a point where analog artifacts look]]></content:encoded></item><item><title>PSA: XWayland doesn&apos;t have to be blurry on GNOME</title><link>https://www.reddit.com/r/linux/comments/1lf8taf/psa_xwayland_doesnt_have_to_be_blurry_on_gnome/</link><author>/u/mort96</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:50:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[A lot of us who run GNOME Wayland try to avoid XWayland apps, because they're blurry when using DPI scaling.Well, it turns out that since GNOME 47 (I think), GNOME has had a fix for this, it's just disabled by default. To enable the fix, follow these steps:Open Terminal and run: gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer', 'xwayland-native-scaling']"Log out and back in againYour XWayland apps like Electron apps, Steam, LMMS, etc etc. should now work great.Note: if text in Steam is too small, go to Steam Settings -> Interface and enable "Scale text and icons to match monitor settings".You can check what version of GNOME you're using by going to Settings -> System -> About -Y System Details. It should have an entry called "GNOME Version". For me, it shows GNOME Version: 48, and Windowing System: Wayland.If you're on KDE, you don't need to do anything, since KDE has had this fix implemented and enabled by default for ages now. I'm hoping GNOME will enable it by default soon.]]></content:encoded></item><item><title>The Story of a Prisoner Who Became a Software Engineer</title><link>https://analyticsindiamag.com/ai-features/the-story-of-a-prisoner-who-became-a-software-engineer/</link><author>/u/Soul_Predator</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:42:09 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[In a world where many may procrastinate learning to code or improving their skills despite leading a comfortable lifestyle, one man is proving that even the confines of prison cannot suppress a passion for coding. Meet the software engineer who, despite being incarcerated, is making his mark in the tech world. His story is a testament to the belief that anyone, anywhere, can master complex programming languages. recently stumbled upon this individual—an open-source contributor and coder with expertise in Rust and Python programming languages, and an avid Linux user—who continues to build and contribute to databases, even from behind bars. What sounds like the plot of a movie is, in fact, the true story of Preston Thorpe, a software engineer at Turso, an open-source distributed database powered by libSQL.  had the opportunity to speak exclusively with Thorpe, who opened up about his journey of programming during his time in prison.A Prisoner’s Attempt at a Better Outlook on Life Through CodingThe 33-year-old software engineer spends his days working remotely from his prison cell in the Mountain View Correctional Facility in Charleston, Maine. Despite the confines of the facility, he has become a software engineer at Turso, actively contributing to projects like the rewrite of SQLite. But his journey to this point has been far from conventional, driven by self-reflection, project-based learning, and an insatiable desire to improve. For nearly a decade, Thorpe was reportedly incarcerated for non-violent drug crimes. However, instead of succumbing to the obvious hopelessness that often defines life behind bars, he discovered a sense of purpose through programming.Explaining how it all started, Thorpe said, “There was one day, after spending a few years in the more calm and respectful environment in the Maine prison, where I had an epiphany and started questioning everything about my life.”“I no longer knew why I had accepted that identity and situation, none of it made sense to me anymore, and I decided that I was no longer okay with being where I was or who I had become.”Thorpe’s programming journey started with a simple but powerful resource: access to a computer through a prison college programme at the University of Maine at Augusta. With limited internet access and a passion to outgrow the curriculum in place, Thorpe created his own learning path. He primarily attributes his success to project-based learning, having had just enough high school experience to understand what he needed to learn. His days were consumed by intense self-study, working on projects, and contributing to open-source software.“I started in Python until I felt like I remembered enough of the basics, then moved to C and built very fundamental things like my own ‘standard library’ of data structures,” Thorpe said. This project-based approach allowed him to learn the intricacies of various programming languages while also developing practical tools that would serve as the foundation for his career.Thorpe’s learning wasn’t restricted to just writing code. He immersed himself in the theory of computer science, reading academic papers, listening to lectures, and exploring the underlying architecture of software systems. His interest in databases led him to explore relational databases, despite having no prior experience in the field. Thorpe explained that his database work initially involved logically isolated components, allowing him to focus on areas aligned with his existing knowledge. His initial contributions included translating from Abstract Syntax Tree (AST) to bytecode and working on the Virtual DataBase Engine (VDBE). He didn’t immediately delve into specific database internals, often working on the IO layer or the command-line interface (CLI). Thorpe also dedicated time to developing the extension library and Go language bindings. Through a process of gradual familiarisation, extensive reading of research papers, and studying CMU lectures, he built the confidence to explore diverse areas and implement features across the entire codebase.In a recent blog post on his company’s website, Thorpe highlighted, “I either write code or manage Kubernetes clusters or other infrastructure for about 90 hours a week, and my only entertainment is a daily hour of tech/programming YouTube.”Thorpe’s self-driven journey took a pivotal turn when he was accepted into Maine’s remote work programme—a rare opportunity for imprisoned individuals to pursue legitimate employment outside the prison.This programme became the gateway to his professional career in tech. “Because there was no precedent set for any of this, what I believe is the most crucial support was the fact that administrators took a chance and allowed me to earn their trust eventually,” he said. His first job was with Unlocked Labs, a company focused on building educational technology for incarcerated individuals. Thorpe’s contributions there quickly gained recognition, and within a year, he was promoted to lead their development team.Despite thriving in his role at Unlocked Labs, Thorpe’s ambition drove him to push even further. His exposure to the world of databases through various open-source projects eventually led him to Turso, a company working on rewriting SQLite.Grateful for the Absence of LLMs and Project-based LearningIn today’s fast-evolving tech landscape, many developers turn to tools powered by large language models (LLMs) like Claude Code to speed up their learning and coding. However, Thorpe views his lack of access to these tools during his learning years as a blessing in disguise. “I’m very grateful that LLMs are something that I did not have available to me for a large portion of my time learning,” he told .“With the proper discipline, if it is a topic you are truly interested in, you can certainly use it to help teach you things, but I would worry for anyone who may be inclined to take shortcuts, as it could easily prevent learning as well.”He firmly believes in the value of building real-world projects as a means of understanding and mastering programming concepts. He asserted that the knowledge gained from solving a problem and building a solution would surpass the learning acquired by breaking down each component and focusing on individual parts.For Thorpe, learning didn’t just happen in isolation. He also credited his contributions to open-source projects as a key part of his development. “I have found reading code very valuable,” he said. Looking ahead, Thorpe is particularly excited about the future of embedded and distributed databases. Moreover, he envisions significant future developments at Turso, including native support for efficient semantic searches and similarity matching in embedded databases. Such developments would enable more efficient reasoning over locally stored context, eliminating the need for separate vector databases or complex infrastructure. His story proves that with determination, a focus on continuous learning, and an unwavering commitment to self-improvement, even the most unlikely paths can lead to success. That being said, it’s important to recognise the lessons in his journey and understand that success is best achieved through ethical means, rather than indulging in illegal activities.]]></content:encoded></item><item><title>Go for DevOps books</title><link>https://www.reddit.com/r/golang/comments/1lf8glp/go_for_devops_books/</link><author>/u/reisinge</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:30:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/reisinge ]]></content:encoded></item><item><title>HTML docs for clap apps without adding any dependencies</title><link>https://www.reddit.com/r/rust/comments/1lf80w7/html_docs_for_clap_apps_without_adding_any/</link><author>/u/winter-moon</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:05:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[It works with any clap-based CLI (or similar help format) - no need to modify your code or recompile anything. Just point it at an executable and it recursively extracts all subcommands and options.    submitted by    /u/winter-moon ]]></content:encoded></item><item><title>Zopdev Summer of Code: Inviting all Builders</title><link>https://www.reddit.com/r/kubernetes/comments/1lf8021/zopdev_summer_of_code_inviting_all_builders/</link><author>/u/Recent-Technology-83</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 11:04:14 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Zopdev Summer of Code is here - your opportunity to learn, build, and contribute to real-world open-source projects while working alongside industry experts.Whether you're looking to boost your resume, gain hands-on experience, or explore new technologies, this is your chance to grow.-------------------------------------------This time, we’re offering two exciting tracks:Track 1: Zopdev + AI Agents:Work on AI intelligent systems that provide AI-powered agents helpful for the developers and it has to be deployed using the Zopdev. Note: On Contribution to the zopdev/helm-charts will have a bonus point.Track 2: Helm Chart Contributions:Contribute to our open-source Helm chart repository. Learn infrastructure as code, Kubernetes, and best practices in DevOps.Real Open-Source Contributions: Work on impactful projects used by real teams.  Learn directly from Zopdev engineers and maintainers. Structured Training Phase: Get the resources and guidance you need to contribute confidently.  Receive a Certificate of Participation and exclusive Zopdev swag.  Recognition and rewards for the most dedicated solution.  Collaborate with developers from around the world.-------------------------------------------Students, professionals, or hobbyist Basic knowledge on ML Models, AI Agents, Helm charts, Kubernetes. Eagerness to learn and contribute June 14 – June 29, 2025  Starts Start of July  Post-training phaseHere’s your chance to learn, contribute, and grow - earn a certificate, make an impact, and have fun alongside like-minded developers!-------------------------------------------Ready to build, learn, and grow: Join us for Zopdev Summer of Code 2025 and be part of something meaningful.]]></content:encoded></item><item><title>AI did pretty decent on Luna’s campaign photos</title><link>https://www.reddit.com/r/artificial/comments/1lf7buw/ai_did_pretty_decent_on_lunas_campaign_photos/</link><author>/u/LokiDMV</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 10:23:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Hey everyone! I’m reaching out to share something close to my heart. Luna, my amazing pitbull rescue, is a finalist in the Animal Welfare League of Alexandria’s 2026 photo calendar contest — and she needs your votes!If Luna wins, she’ll get some truly amazing honors, including: * Being named Alexandria’s 2026 Animal of the Year * Gracing both the front and back covers of the AWLA calendar * A professional pet photography session * A special proclamation from the mayor naming a day in her honor (!!) * Featured on the AWLA’s homepage for all of 2026 * And her photo will be displayed on buses across Northern Virginia in late 2025! Pretty wild, right?I’d be so grateful if you could help by voting. Each vote is $1, and all proceeds go directly to supporting the AWLA’s incredible work saving and caring for animals like Luna.💖 My personal goal is to raise $3000 for the shelter that saved her. Every dollar and vote makes a difference!Thank you so much for supporting a pittie who beat the odds — let’s show everyone how amazing these dogs really are!]]></content:encoded></item><item><title>Weekly: This Week I Learned (TWIL?) thread</title><link>https://www.reddit.com/r/kubernetes/comments/1lf6z1r/weekly_this_week_i_learned_twil_thread/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 10:00:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Did you learn something new this week? Share here!]]></content:encoded></item><item><title>I got tired of the iPhone timer for my workouts, so I built my own solution with Flutter</title><link>https://github.com/JosephDoUrden/SetTimer</link><author>/u/JosephDoUrden</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:46:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The joy of (type) sets in Go</title><link>https://bitfieldconsulting.com/posts/type-sets</link><author>/u/EightLines_03</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:38:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ban &apos;AI&apos; generated posts</title><link>https://www.reddit.com/r/linux/comments/1lf6m6p/ban_ai_generated_posts/</link><author>/u/Keely369</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:36:24 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[LLM generated posts are becoming the worst type of spam on here and it's only going to get worse.We need a rule banning them. I stated this in a more polite way in my previous post but it was auto-deleted as breaking rule 1, which it did not.LLM posts add nothing to the forum, take five seconds to generate with no thought or effort on the part of the OP and waste the time of people who don't recognise them for what they are. They're usually very lengthy as well, which compounds the issue.]]></content:encoded></item><item><title>Kubernetes Learning Roadmap Including Visual &amp; Tracking Progress</title><link>https://www.reddit.com/r/kubernetes/comments/1lf6hqd/kubernetes_learning_roadmap_including_visual/</link><author>/u/bilou89</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:27:37 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Master Kubernetes step-by-step with this detailed roadmap. Learn Kubernetes architecture, pods, deployments, services, networking, Helm, RBAC, operators, CI/CD, and production-grade DevOps best practices.]]></content:encoded></item><item><title>Experiences with Thalos, Rancher, Kubermatic, K3s or Open Nebula with OnKE</title><link>https://www.reddit.com/r/kubernetes/comments/1lf6bhz/experiences_with_thalos_rancher_kubermatic_k3s_or/</link><author>/u/Tiny_Sign7786</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:16:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I‘m reaching out as I want to know about your experience with different K8s. Kontext: We’re currently using Tanzu and have only problems with it. No update went just smooth, for a long time only EOL k8s versions available and the support is friendly said a joke. With the last case we lost the rest of our trust. We had a P2 because of a production cluster down due to the update. It took more than TWO!!! months to get the problem solved so that the cluster is updated to (the inbetween outdated) new k8s version. And even if the cluster is upgraded it seems like the root cause is still not figured out. What is really a problem as we still have to upgrade one cluster which runs most of our production workload and can’t be sure if it will work out or not.We’re now planning to get rid of it and evaluate some alternatives. That’s where your experience should come in. On our shortlist are currently: - Thalos - k3s - Rancher - Open Nebula with OneKE - Kubermatic (haven’t intensively checked the different options yet)We’re running our stuff in an on premise data center currently with vsphere. That also will probably stay as my team, opposite to Tanzu, has not the owner ship here. That’s why I’m for example not sure, if Open Nebula would be overkill as it would be rather a vsphere replacement than just Tanzu. What do you think?And how are your experiences with the other platforms? Important factors would be:as less complexity is necessarydifficulty of setup, management, etc.how good is the support of there is oneis there an active community to get help with issuesIf not running bare metal, is it possible to spin up nodes automatically in VMWare (could not really find something in the documentation.Of course a lot of other stuff like backup/restore, etc. but that’s something I can figure out via documentation.Thank’s in advance for sharing your experience. ]]></content:encoded></item><item><title>Real-time analytics with an all-in-one system: Are we there yet?</title><link>https://questdb.com/blog/realtime-analytics-using-tsdb/</link><author>/u/j1897OS</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:13:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Real-time data analytics have been around for more than a decade, and the
ecosystem is quite mature. However, a robust pipeline requires a deep amount of
hand-crafted integration work.After all, there are many powerful and useful parts in a typical real-time
analytics toolchain. This is especially evident in use cases which combine
insights over a full historical dataset, while also needing to handle new data
every second from the real world.While there are strong multi-product choices today, we want to know what would
happen if  tried to handle the full range of real-time
analytics challenges.Can any emerging entrants become a true one-stop-shop solution?To better understand the overall challenge, consider requirements behind an
open-high-low-close (OHLC) or candlestick chart
used for trading applications in financial markets. You can get a chart at
various levels of detail: live updates from the current trading day, seamlessly
combined with the data from the past week, month, year, and so on.To calculate the values for these charts, you have to partition the full dataset
into time slices of various sizes, and find the min/avg/max values for each
slice. These time ranges can be significant.This is an example of a general and versatile class of real-time analytics:
aggregate functions over time slices.There's a lot going on under-the-hood to make this work well and fast. To
illustrate, we'll look at typical real-time analytics system design today, and
then we'll contrast that with a look at several database systems.Like with financial charts, we tend to perform real-time analytics on a dataset
that is both massive and rapidly growing. This creates several tough challenges:Storing massive data volumes cheaply and effectivelyReturning robust results at no more than few seconds of latencyReturning correct results after updates to existing dataKeeping data access to a minimum — access itself costs moneyAs the volume of the collected data grows, storage costs balloon. Unless you
have your own datacenter and a team committed to continuously procuring and
maintaining storage devices and systems, you'll end up as a client of cloud
storage, such as Amazon S3, Azure Blob, or Google Cloud Storage.This creates a barrier in your software stack — one type of storage for fresh
data, and another for historical data. You need a system that seamlessly jumps
over the barrier and serves you the analytic results you need from any period
you want:The cost of just keeping your data is relatively low, but the story quickly
changes once you need to access it. Your system must ensure it accesses only the
data which it absolutely needs.Basically, if the data didn't change, you shouldn't need to access it again.
That's expensive and inefficient. Instead, it's better to store and reuse the
results of the analytics you performed once.In this scenario, the greatest challenge is an update to the historical data:
you must keep the volume of accessed data low, and yet make sure you update
everything that needs to be updated.At the same time, the system must deal with the fresh data that just came in. It
must stay on top of possibly millions of events per second, and come up with the
right answers about what just happened in the world.In a typical modern setup, we are likely to use two separate systems, each
specialized in solving one challenge. We use Apache Kafka to ingest the data,
and then fork the data pipeline so that one fork goes to the system optimized
for low-cost storage, and the other to the system optimized for real-time event
processing.For the recent data, we can use a stream processing engine like Apache Flink. It
takes in the events as they occur, aggregates them in memory, and outputs live
results with minimal latency:For the historical data, the starting point is cheap storage. The primary choice
for most companies being cloud storage like S3 or Azure Blob Storage. This kind
of storage comes with higher access latency. It's completely unstructured (the
unit of data is a ), and immutable.That means you need another system that builds upon this foundation to provide
higher-level services. One option is using a cloud-based data lake platform,
like Snowflake; another is an open-source analytics tool like Spark:This kind of setup obviously has several moving parts, but there's even more
when you take a closer look. For example, how do you access the output of
realtime stream processing?A streaming engine like Apache Flink only solves the computation concern; its
output doesn't automatically come in a form ready for ad-hoc query access.One option is storing the results in a general-purpose database, like Postgres.
Then you can use the same database to store the results for historical data as
well, and finally there should be an API frontend component that queries the
database.Another option is to treat Kafka itself as the source of truth. Flink outputs
its results to a Kafka topic, and then the API frontend component loads them
into RAM and serves them. If the component fails, after restart it can just
rescan the Kafka topic. It can also use a local, embedded database.On the data lake side, the main challenge is doing the least amount of work
possible while maintaining the correctness and completeness of the stored
analytics results. You can process the past day's worth of data during the
night, when the handover from the real-time system to the data lake occurs.All these parts must account for, and be resilient to, failures. To manage this,
you need infrastructure that monitors operations and retries failed ones.Putting it all together, we get a rough outline of a modern hybrid system for
realtime data analytics:Since the need for real-time analytics has become mainstream and widespread,
there's an increasing demand for a simpler system, one that would automatically
handle both historical and new data in a uniform fashion, and simply provide you
with the results you want.An emerging option for this workload is a streaming data lakehouse system.Products from diverse categories have been converging on it, such as data lake
products, real-time streaming engines, and time-series databases. Each one is
adding features from the others, in a bid to build one complete, integrated
system that handles all the concerns automatically:We'll focus on systems that originate in the time-series database category.In this category, the best paradigm for real-time analytics is that of the
. A closely related concept is .In its essence, a materialized view is a SQL query in solid-state form: its
results are persistent in a database table, available at no computation cost.
You can get them using a trivial query that doesn't need to spell out any of the
business logic needed to calculate them. A materialized view is as convenient to
create as it is to access.But to work for our use case, the database must make sure the materialized view
is always up to date — and that's where things get interesting. Databases
vary widely in their support for low-latency updates of materialized views, and
those that do have good support vary widely in their approaches.We found the following databases to be good at low-latency materialized views:
TimescaleDB, ClickHouse, and InfluxDB. At QuestDB, we've
recently introduced materialized views,
and are constantly improving their performance and ergonomics.TimescaleDB is an extension on Postgres and thus benefits from its maturity.
This is how you implement continuous aggregation: with the desired query. This runs the query
against the existing data, and saves it to the table you named.SELECT add_continuous_aggregate_policy(...) to schedule a task that updates
the materialized view at a fixed time interval.When you query the materialized view table, TimescaleDB uses a hybrid approach:
it takes everything available from the table, and for anything that's missing it
runs the aggregation query against the base table.Thus, you always get the full results, regardless of when the scheduled task
last ran. However, the query's runtime will go up in proportion to the volume of
the missing materialized results.Given this, you'll have to find a balance that minimizes the system load induced
by the scheduled task, and the runtime of the query. The scheduled task can be
configured to scan only a recent portion of the base table, which limits the
impact on system load, but leaves earlier data permanently stale. You can also
schedule another, less frequent task that updates the full table.We should also note that TimescaleDB, due to its Postgres fundamentals, isn't as
optimized for the ingestion of massive amounts of time-series data. QuestDB
typically ingests data at a rate many multiples faster. It's also quite complex
to scale horizontally. This negatively impacts the resource and maintenance
costs.TimescaleDB supports tiered storage in its Cloud edition. Once properly set up,
you can query the data across tiers transparently. However, setup and
configuration is quite involved and requires knowledge of both Postgres and
TimecaleDB concepts. You can't directly update the data in cold storage. You
must go through the manual steps of "un-tiering" it, updating and "re-tiering"
it.TimescaleDB benefits from the maturity of Postgres in terms of the support for
monitoring and diagnostics of the tasks that keep the materialized view up to
date. You can also integrate with Prometheus.ClickHouse also supports , but with completely
different semantics. This is how you use it:Manually  that will hold the materialized view. Specify the
correct table engine: .CREATE MATERIALIZED VIEW ... — this sets up a scheduled task that will run
whenever you insert new data, and specifies the aggregation expression.
Existing data won't be processed.Manually backfill the table with existing data.Since ClickHouse runs the aggregation whenever you insert data, the latency of
the aggregated results is very low. However, updates and deletions aren't
reflected and need to be handled manually.ClickHouse supports tiered storage in its Cloud edition. It will automatically
move the older data to cold storage using its TTL feature. You can't directly
update the data in cold storage, you must take manual steps to move it back to
hot storage, delete the outdated data, insert data with updates, and move back
to cold storage.The materialized view table is like any other, and allows arbitrary
insert/update/delete actions, as well as schema changes. This makes it prone to
incorrect data and errors in the continuous aggregation process.ClickHouse is great at raw ingestion performance, but compared to TimescaleDB,
it's not as mature for monitoring, diagnostics, and issue resolution.All told, maintaining a large number of materialized views is a complex task,
with lots of hand-crafted code and tooling needed.Basic steps to create continuous aggregation in InfluxDB resemble those for
ClickHouse:Create the destination table ( in InfluxDB terminology)Create a scheduled task that runs continuous aggregationManually backfill the existing dataUnlike ClickHouse, and more similar to TimescaleDB, this task runs on a fixed
time-interval schedule and isn't triggered by inserts. It only looks at the
recent data, and you can configure exactly how recent. It doesn't automatically
backfill, so it requires a manual backfill step.This mechanism creates a conflict between low latency and low system load,
because you have to set a short interval to get low latency. But on the
flip-side, the task will rescan the whole specified range every time. Unlike
TimescaleDB, there's no hybrid mechanism that fills the gaps by running a query
on the base table.InfluxDB supports tiered storage in its Enterprise and Cloud editions. You can
set up a data retention policy that copies the data to cold storage, where it
remains available for querying.Since InfluxDB is purpose-made for monitoring and alerting, the support in this
area is solid. To this end, the company developed a whole ecosystem of tools:
Telegraf, Chronograf, and Kapacitor.InfluxDB's main drawback is widely considered to be its lack of full SQL
support. Given that it requires its own DSL, it gives the impression of a
special purpose tool. While it supports the use case of continuous aggregation
itself, there's less support for more general and powerful data analytics on the
same dataset.So, InfluxDB alone usually isn't enough for all the things you need to do with
the data.With QuestDB, there's only one step to set up continuous aggregation:CREATE MATERIALIZED VIEW ...This creates the materialized view table, backfills it with the results of
processing the existing data, and sets up the aggregation task to run on all
changes to the base table, including updates and deletions. The task is fired
immediately when changes occur, but it may be delayed when system load is high.This simplicity is a key part of QuestDB's vision for a single-source real-time
analytics system. By handling both historical and fresh data through the same
materialized view mechanism, we eliminate the need for separate systems and
complex integration work. Whether you're querying data from last year or the
last second, you use the same SQL interface and get consistent results.Another nice aspect of QuestDB's materialized views is that you can cascade them
— a materialized view's base table can be another materialized view. This allows
you to create a very efficient pipeline of aggregations at different levels of
granularity.QuestDB keeps maintainability in focus and exposes the status of materialized
views through the SQL interface:With this, you can monitor refresh lag, and detect and diagnose failures.While the computations you can use with materialized views are limited to
aggregate functions over time slices, QuestDB's general querying power is quite
robust. It supports JOINs, window functions, Common Table Expressions, nested
SELECT expressions, and so on.However, in the current version (8.3.1), QuestDB's materialized views aren't
very resilient to schema evolution. The materialized view will get invalidated
if you DROP or ALTER a column, even when the materialized view doesn't depend on
it.QuestDB supports tiered storage in its Enterprise edition. It can keep your data
in cold storage, in Parquet format, and query it without converting back to its
native format.Overall, this is a significant step forward for this use case, and its
near-future roadmap contains some improvements:Better resilience of materialized views to schema evolution. You'll be able to
manipulate non-dependency columns without breaking the materialized view.Support for a refresh policy based on a fixed time interval. If you align it
with the time slice interval, this will ensure the materialized view is always
fresh, with significantly less impact on system load.The journey from complex, multi-system architectures to unified solutions is
well underway. Each database we examined brings valuable pieces to the puzzle:
TimescaleDB's hybrid query approach, ClickHouse's raw performance, InfluxDB's
monitoring expertise, and QuestDB's simplified materialized views.The database systems we reviewed all seem to have many building blocks needed,
but none of them seems to be fully ready to take over the all-in-one real-time
analytics system.The ideal system would combine the best of these approaches: effortless setup
and maintenance, consistent performance across all data ages, and a single
interface for both real-time and historical analytics. While we're not quite
there yet, the convergence of these technologies suggests that the one-stop-shop
solution for real-time analytics is within reach.As these systems continue to evolve and borrow from each other's strengths, we
can expect to see more solutions that truly unify the real-time analytics
experience. The future belongs to systems that can handle the full spectrum of
analytics needs without requiring complex integration work or specialized
knowledge of multiple technologies.]]></content:encoded></item><item><title>LiveKit Agent - workers auto dispatch issue in deployment</title><link>https://www.reddit.com/r/kubernetes/comments/1lf68jn/livekit_agent_workers_auto_dispatch_issue_in/</link><author>/u/InbaKrish007</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 09:10:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have issue on the LiveKit agents deployment.we are using Kubernetes setup with 4 pods (replica) each with below resources config, yaml resources: requests: cpu: "4" memory: "8Gi" limits: cpu: "4" memory: "8Gi" so that it should accept 25 to 30 concurrent sessions per pod and multiplied by 4 on total.For Server we are using the LiveKit's cloud offering with free trail (mentions that 100 concurrent connections are provided).Though we have this setup, on connecting 2 concurrent sessions, 3rd and upcoming sessions are not getting handled, the client side (built with client-sdk-js), creates a room with the LiveKit JWT token (generated from Ruby server), but the agent is not getting dispatched and joins the room.-> We have not modified any workeroptions in the LiveKit agents backend. -> With Ruby server, we generate the the token with the logic below, ```ruby room = LivekitServer::Room.new(params["room_name"]) participant = LivekitServer::Participant.new(**participant_params) token = room.create_access_token(participant:, time_to_live:) render json: { access_token: token.to_jwt }def create_access_token(participant:, time_to_live: DEFAULT_TOKEN_TTL, video_grant: default_video_grant) token = LiveKit::AccessToken.new(ttl: time_to_live) token.identity = participant.identity token.name = participant.name token.video_grant = video_grant token.attributes = participant.attributes token enddef default_video_grant LiveKit::VideoGrant.new(roomJoin: true, room: name, canPublish: true, canPublishData: true, canSubscribe: true) end json { "name": "user", "attributes": { "modality": "TEXT" }, "video": { "roomJoin": true, "room": "lr5x2n8epp", "canPublish": true, "canSubscribe": true, "canPublishData": true }, "exp": 1750233704, "nbf": 1750230099, "iss": "APIpcgNpfMyH9Eb", "sub": "anonymous" } ```What am I missing here? Based on the documentation and other parts, I guess there are no issue with the deployment and have followed the exact steps mentioned for the k8s setup. But as mentioned the agents are not getting dispatched automatically, and ends in client UI infinite loading (we haven't set any timeout yet).]]></content:encoded></item><item><title>Notification daemon for modern Wayland compositors</title><link>https://www.reddit.com/r/linux/comments/1lf5bm5/notification_daemon_for_modern_wayland_compositors/</link><author>/u/cyberlame</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 08:08:59 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Last year, a friend and I started a project — a notification daemon designed specifically for modern Wayland compositors, built entirely in Rust. After about a year of work, we created something truly usable and with features we’re proud of. I’ve been running it as my daily notification daemon since early on, so it’s not just a prototype — it’s solid and practical.But after pushing hard for so long, we hit a serious burnout a couple months ago. Since then, the project’s been quiet — no new updates, no big release. We wanted to finish all the core features and release a 0.1 version with a big announcement, but that never happened.I’m sharing this now because, even if I can’t keep working on it, I want the community to know it exists. Maybe someone out there will find it useful, or maybe it’ll inspire others to do something similar or even pick it up.Thanks for reading — it’s tough to share something so personal and unfinished, but I hope it’s not the end for this project.]]></content:encoded></item><item><title>Wayland protocol for &quot;Sensitive&quot; Areas? (passwords etc)</title><link>https://www.reddit.com/r/linux/comments/1lf57yg/wayland_protocol_for_sensitive_areas_passwords_etc/</link><author>/u/Misicks0349</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 08:02:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I'm curious if this is a thing, I came across this post showing how apple devices will just straight up not show areas of the screen that have information like your passwords if you take a screenshot or screen record. Some wayland compositors have the option to exclude entire windows from screen capture but I'm not sure if theres anything like this where a client could say "hey, there's a plaintext password in this box, don't display it in screen captures please :)".]]></content:encoded></item><item><title>[R] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought</title><link>https://arxiv.org/pdf/2505.12514</link><author>/u/jsonathan</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 07:07:08 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>From Collaborators to Consumers: Have We Killed the Soul of Open Source?</title><link>https://my-notes.dragas.net/2025/06/19/from-collaborators-to-consumers-have-we-killed-the-soul-of-open-source/</link><author>/u/dragasit</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 06:32:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I discovered Open Source when I was just a teenager, back in 1996. At the time, in my eyes, it was a revolution: the ability to see the code, contribute, fork it, and give a project a new direction - perhaps a parallel one, or something completely different.Like OpenBSD from NetBSD, DragonflyBSD from FreeBSD, or Nextcloud from Owncloud - the examples are endless. It was about freedom, the chance to be part of something or, in some cases, at the very center of something: its development.To me, Open Source meant having the chance to develop an idea and find other people who shared it, turning what was just a project in my mind into a reality. All without needing big funding, a business plan, or having to risk anything. Just the pleasure of doing it and the joy of seeing it come to life. A waking dream.Over time, I witnessed many exchanges of opinion - some of them quite heated - that led to hard forks or uncomfortable situations within development teams. People leaving, others taking over - you name it. But, in the end, the software was always at the center. It was an ideological battle over how to implement something (or how NOT to implement it).This led to some fantastic pairings: Linux, a kernel without an operating system, and GNU, an operating system without a stable and complete kernel. Together, they revolutionized the world, changed the concept of computing, and proved that yes, Open Source works and produces quality software - often of a far greater quality than many of its closed-source, commercial counterparts.And yet, there were the "distro wars" - and I didn't understand them. And if I didn't understand the distro wars back then, the situation today seems even more extreme. I appreciated the variety, the different ideas, and the different approaches, but never the fanaticism. I was a strong supporter of Debian, but I couldn't understand those who openly attacked alternatives (like Red Hat, at the time, or Suse). I thought: use what you like, contribute if you want but... hey, it's Open Source, you don't pay for it, you're not forced, just choose what you like best! If you're happy, tell the world. If you're dissatisfied, switch (to different software) or change THE software (meaning, implement what you think is necessary). But why wage war on others, on those with different ideas who made different choices? Is it the general polarization fueled by social media? Is it because Open Source has become more mainstream, bringing with it users who have a "consumer" mindset rather than a "collaborator" one?And yet, there are still positive examples out there — quiet, solid, and often overlooked. The BSD projects, for instance, show us that it's still possible to diverge in philosophy and approach without descending into hostility. FreeBSD, OpenBSD, and NetBSD took different paths. And yet, there are no "wars" between them. Their communities may disagree on technical choices, but they coexist with mutual respect. You rarely see a FreeBSD user shouting "OpenBSD must die!" or a NetBSD developer trolling others on social media. The tone is sober, the work is steady, and the focus remains on the code and its quality - not on brand wars or personal egos.This is the spirit I fell in love with: different ideas, mutual respect, and the shared goal of building something useful and free. We may not all agree on everything, but we can still build in parallel, learn from each other, and avoid turning diversity into division.Lately, all of this is becoming truly extreme. I read, for example, sharp and violent opinions from Wayland users against X11 (Xorg, etc.) - "it must die!" But, I wonder, why this violence?I use Wayland on Linux and X11 on FreeBSD - both on the same computer, both with satisfaction. Why should I hate one of them? If I don't like it... I simply don't use it.The world is becoming increasingly polarized and bitter, making people less and less inclined towards dialogue or tolerance for those with different ideas or positions. But, I ask myself, why should this be happening in the world of Open Source?We are all in the same boat. We have the tools, the freedom of choice, and it costs us nothing. If we don't like a solution, we can say so and choose something else. Why this violence?  Who benefits?When we fight violently over Open Source software, when we lash out with intolerance against a solution we dislike, the entire Open Source world loses an opportunity. The opportunity to reduce the chances of ending up in a computing monoculture, the opportunity to have a choice, the opportunity for someone to listen to our well-reasoned observations and learn from them.It's up to us, every day, with every comment and contribution, to decide whether we want to build bridges or raise walls.]]></content:encoded></item><item><title>I was reading this bash guide on GitHub, and found this:</title><link>https://www.reddit.com/r/linux/comments/1lf2drq/i_was_reading_this_bash_guide_on_github_and_found/</link><author>/u/rev155</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 05:01:30 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Should I switch from Python to Go for Discord bots?</title><link>https://www.reddit.com/r/golang/comments/1lf22ab/should_i_switch_from_python_to_go_for_discord_bots/</link><author>/u/GladJellyfish9752</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 04:42:44 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So I know Python and Rust pretty well, can handle JavaScript okay, and I've messed around with Go a little bit. Made a bunch of stuff in Python and Rust but lately I'm wondering if Go would be better for some things I want to build. Thinking I'll try Discord bots first since I already made a few in Python.Here's what I'm curious about - is the Discord library support in Go actually good? I found discordgo but not sure how it stacks up against discord.py or discord.js. Like does it have all the features you need or are you missing stuff? And is the community around it active enough that you can get help when things break?Also wondering about speed - would a Go bot actually handle more users at once or run commands faster than Python? My Python bots sometimes get slow when they've been running for days.If Go works out well for Discord stuff I might try moving some of my other Python projects over too. Just want to see if it's worth learning more Go or if I should stick with what I already know. Anyone here made a similar switch or have thoughts on whether it's worth it?]]></content:encoded></item><item><title>Has Anyone launched Litmus Chaos Experiments via GitHub Actions ?</title><link>https://www.reddit.com/r/kubernetes/comments/1lf1va3/has_anyone_launched_litmus_chaos_experiments_via/</link><author>/u/Late_Organization_47</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 04:31:29 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Use case: We need to integrate Chaos Fault Injections via CI/CD as a part of POC.Any leads and suggestions would be welcomed here 🙂   submitted by    /u/Late_Organization_47 ]]></content:encoded></item><item><title>Workflow Engine</title><link>https://www.reddit.com/r/golang/comments/1lf1v27/workflow_engine/</link><author>/u/Used-Army2008</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 04:31:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[What would be the easiest wf engine I can use to distribute tasks to workers and when they are done complete the WF? For Java there are plenty I found just a couple or too simple or too complicated for golang, what's everyone using in production?My use case is compress a bunch of folders (with millions of files) and upload them to S3. Need to do it multiple times a day with different configuration. So I would love to just pass the config to a generic worker that does the job rather than having specialized workers for different tasks.]]></content:encoded></item><item><title>How DynamoDB, key-value schemaless cloud-native data store scales: Architecture and Design Lessons</title><link>https://javarevisited.substack.com/p/software-architecture-deep-dive-scaling</link><author>/u/javinpaul</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 03:58:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust</title><link>https://www.reddit.com/r/rust/comments/1lf0bof/rewriting_kafka_in_rust_async_insights_and/</link><author>/u/jonefeewang</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 03:06:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hello everyone, I have taken some time to compile the insights and lessons I gathered during the process of rewriting Kafka in Rust(https://github.com/jonefeewang/stonemq). I hope you find them valuable.Below is a concise TL;DR summary.Rewriting Kafka in Rust not only leverages Rust’s language advantages but also allows redesigning for superior performance and efficiency.Design Experience: Avoid Turning Functions into async Whenever PossibleDesign Experience: Minimize the Number of Tokio TasksDesign Experience: Judicious Use of Unsafe Code for Performance-Critical PathsDesign Experience: Separating Mutable and Immutable Data to Optimize Lock GranularityDesign Experience: Separate Asynchronous and Synchronous Data Operations to Optimize Lock UsageDesign Experience: Employ Static Dispatch in Performance-Critical Paths Whenever Possible   submitted by    /u/jonefeewang ]]></content:encoded></item><item><title>Giving this old Vaio mate and upgrades</title><link>https://www.reddit.com/r/linux/comments/1lezj81/giving_this_old_vaio_mate_and_upgrades/</link><author>/u/abraxas8484</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 02:26:00 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Gotta say, it's a fun project to fix up this thrift store Vaio with some much needed upgrades. Mate seems to work well with it :) and suggestions are welcomed    submitted by    /u/abraxas8484 ]]></content:encoded></item><item><title>The Debugger is Here - Zed Blog</title><link>https://zed.dev/blog/debugger</link><author>/u/bschwind</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 01:49:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Over 2,000 developers asked, and we delivered.Debugging in Zed is now a reality—and it's a big leap toward Zed 1.0.We set out to build a debugger with three primary focuses:Fast: Spend less time context switching and more time debuggingFamiliar: In line with Zed's design language and supports everything expected from a typical debugger flowConfigurable: You're able to customize the UI, keybindings, debug configurations and moreOut of the box, Zed supports debugging popular languages including Rust, C/C++, JavaScript, Go, and Python.
With our extension system, Zed can support any debug adapter that implements the Debug Adapter Protocol (DAP).To simplify the setup process, we've introduced locators, a system that translates build configurations into debug configurations. Meaning that you can write a build task once in  and reference it from  — or, even better, rely on Zed's automatic configuration.Zed automatically runs locators on built-in or language server-generated runnables, so in many cases you won't even need to write a debug configuration to get up and running.We currently support locators for Cargo, Python, JavaScript, and Go, with more coming in the future.
For more information on configuring a debug session, see our documentation.Once in a debug session, Zed makes it easy to inspect your program's state, such as threads, variables, breakpoints, the call stack, and more.Setting some breakpoints and running the test in a debug session.The debugger panel is fully customizable too, just drag and rearrange tabs in whatever order you want; you can even move the debug panel around so it fits your workflow.Zed also supports keyboard-driven debugging for users that prefer to keep their hands on the keyboard.
You can step through code, toggle breakpoints, and navigate a debug session without ever touching the mouse.Navigating through the Debugger surfaces using only the keyboard.Special thanks to Remco Smits for driving a lot of the heavy lifting on this project—your contributions have been critical to getting us here.Zed's debugger supports debugging a variety of languages through the Debug Adapter Protocol.
But simply implementing the protocol wasn't enough—we needed an architecture that could scale to collaborative debugging, support extensions, and efficiently cache and manage responses from debug adapters.To achieve this, we built a two-layer architecture: a data layer that communicates directly with the debug adapters, and a UI layer that fetches data from the data layer to render the interface.This separation means the UI layer only requests what it needs, allowing the data layer to lazily fetch information and avoid unnecessary requests.
It also makes the data layer solely responsible for maintaining session state, caching responses, and invalidating stale data.
This architecture will make implementing collaborative debugging significantly easier, since the same UI code can be reused across multiplayer sessions—and we only send essential data across the wire, preserving bandwidth.Supporting every debug adapter out of the box wasn't feasible—there are over 70 DAP implementations, each with its own quirks.
To solve this, we extended Zed's extension API to support debugger integration.Adding DAP support via an extension involves defining a custom schema that integrates with our JSON server, implementing logic for downloading and launching the adapter, processing debug configuration to add sane default values, and integrating with locators for automatic configuration.
This design follows our approach to LSP extensions, giving extension authors full control to bring their own debug adapters to Zed with minimal friction.We also wanted inline variable values to work out of the box.
Surprisingly, the inline values request is a part of the Language Server Protocol (LSP) instead of the DAP.
Using the inline values approach would limit Zed to only showing inline values for DAPs which integrate with LSPs, which isn't many.
A naive workaround might be to use regular expressions to match variable names between the source code and debugger values, but that quickly breaks down when dealing with scopes, and comments.
Instead, we turned to Tree-sitter. After all Zed is built by the creators of Tree-sitter!Through Tree-sitter queries, we can accurately identify variables within the current execution scope, and easily support any language through  files without relying on an LSP server to be tightly integrated with a debug adapter.
At launch, inline values are supported for Python, Rust, and Go.
More languages will be supported in the coming weeks.When we set out to build the debugger, we wanted to make it seamless to use, out of the way, and in line with Zed's high standard of quality.
Now that we've built a strong foundation that is compatible with any debug adapter, we're ready to explore and implement advanced features such as:New views: While we support all the fundamental views, we're planning on adding more advanced views such as a watch list, memory view, disassembly view, and a stack trace viewAutomatic configuration: We're going to add support for more languages and build systems]]></content:encoded></item><item><title>Implementing a convolutional neural network from scratch with no libraries</title><link>https://deadbeef.io/cnn_from_scratch</link><author>/u/LlaroLlethri</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 01:01:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[An unknown error occurred.]]></content:encoded></item><item><title>[D] What tasks don’t you trust zero-shot LLMs to handle reliably?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lewzg7/d_what_tasks_dont_you_trust_zeroshot_llms_to/</link><author>/u/WristbandYang</author><category>reddit</category><pubDate>Thu, 19 Jun 2025 00:19:47 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[For some context I’ve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I’ve found in this setting that structured output from LLMs can often outperform traditional methods.That said, my boss is now asking for likelihoods instead of just classifications. I haven’t implemented this yet, but my gut says this could be pushing LLMs into the “lying machine” zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently? What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?And on the flip side, what types of tasks have worked surprisingly well for you? ]]></content:encoded></item><item><title>Osprey Programming Language</title><link>https://www.ospreylang.dev/</link><author>/u/emanresu_2017</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 21:43:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Strong static typing prevents runtime errors while keeping syntax clean and readable. Expression-bodied
          functions eliminate boilerplate.Explicit type annotationsCompile-time error checkingExpression-bodied functions]]></content:encoded></item><item><title>Making Cobra CLIs even more fabulous</title><link>https://www.reddit.com/r/golang/comments/1lethu1/making_cobra_clis_even_more_fabulous/</link><author>/u/bashbunni</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 21:42:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm bashbunni a software developer at Charm, the creators of Bubble Tea, Glow, Gum, and all that terminal stuff. We use spf13's Cobra to power a ton of our CLIs, so we wanted to give it a little love through a new project called . Fang is a layer on top of cobra to give you things like: - Fancy output: fully styled help and usage pages - Fancy errors: fully styled errors - Automatic : set it to the build info, or a version of your choice - Manpages: Adds a hidden  command to generate manpages using mango - Completions: Adds a  command to generate shell completions - Themeable: use the built-in theme, or make your own - Improved UX: Silent usage output (help is not shown after a user error)]]></content:encoded></item><item><title>The Latest X.Org Server Activity Are A Lot Of Code Reverts</title><link>https://www.phoronix.com/news/X.Org-Server-Lots-Of-Reverts</link><author>/u/6e1a08c8047143c6869</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 21:28:45 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[
The X.Org Server has been seeing a lot of commits this week... to revert bad code.
Many Phoronix readers have been asking why I haven't been covering news of the "X11Libre" fork of the X.Org Server or if I somehow missed it... No, simply a vote of no confidence. It's highly unlikely to succeed long-term given the very limited experienced developers / resources and none of the major Linux stakeholders (companies) backing it. 
A great example now are all of the reverts hitting the X.Org Server Git code after longtime X.Org developers began going through the code committed by the "X11Libre" developer prior to his ejection from the FreeDesktop.org camp.
There was this revert for not handling copyright and license notices correctly. Some existing code macros were moved to a new file while dropping the existing copyright holders from being mentioned in the new file and only adding the new contributor to that header file. The code license was also changed from MIT AND X11 to MIT OR X11.
Also merged this week was this big revert of prior "RandR cleanups" that ended up breaking at least some RandR functionality.
revert to avoid unnecessarily breaking the NVIDIA driver. It was also commented by NVIDIA that some additional requests for other reverts are coming too.
There were also other reverts for code of questionable value. And other reverts making changes without knowing the prior knowledge for why some macros were added in the first place by X.Org developers.
the list goes on with more reverts expected soon.]]></content:encoded></item><item><title>[D] 500+ Case Studies of Machine Learning and LLM System Design</title><link>https://www.reddit.com/r/MachineLearning/comments/1let433/d_500_case_studies_of_machine_learning_and_llm/</link><author>/u/OhDeeDeeOh</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 21:26:06 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We've compiled a curated collections of real-world case studies from over 100 companies, showcasing practical machine learning applications—including those using large language models (LLMs) and generative AI. Explore insights, use cases, and lessons learned from building and deploying ML and LLM systems. Discover how top companies like Netflix, Airbnb, and Doordash leverage AI to enhance their products and operations]]></content:encoded></item><item><title>Is it worth switching to Golang from C#/.NET?</title><link>https://www.reddit.com/r/golang/comments/1les1ce/is_it_worth_switching_to_golang_from_cnet/</link><author>/u/Content_Opposite6466</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 20:42:45 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I work with .NET has been around for 7 years. But I want to try something new. I am considering Golang. There is also talk in the current company about replacing C# monoliths with Go microservices. What do you recommend on this issue? Is it worth it, both in work and in personal choice?]]></content:encoded></item><item><title>I built an app to turn Discord messages into clean showcases</title><link>https://www.reddit.com/r/rust/comments/1leqs1m/i_built_an_app_to_turn_discord_messages_into/</link><author>/u/Megalith01</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 19:51:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So the app I made to solve a weirdly specific but kinda annoying problem I kept running into: making Discord messages and media look presentable.You know how sometimes you want to show off a funny convo, a support message, or something cool that happened on your server, but screenshots always look messy, or you end up cropping stuff in Paint? Yeah, I got tired of that. So I made a tool.the desktop app that lets you import messages, images, and media from Discord (via a discord bot you create), arrange them nicely, style them to your liking, and export them as clean showcase pieces. It’s simple, fast, and designed to make Discord content look professional with minimal effort.It’s made using  (so it’s lightweight and fast) with a React (Vite + Tailwind + Framer Motion) + TypeScript frontend. Works across platforms (Linux, macOS, Windows).I originally built this app for a streamer who wanted a better way to present Discord messages on stream and in highlight videos. Screenshots were always messy, cropping took too long. I liked the idea so i decided to release the app as open source.It’s still a work in progress, but it’s very much usable, so feedback and ideas are welcome.]]></content:encoded></item><item><title>http: TLS handshake error from 127.0.0.1 EOF</title><link>https://www.reddit.com/r/kubernetes/comments/1leq0eb/http_tls_handshake_error_from_127001_eof/</link><author>/u/Double_Intention_641</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 19:20:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm scratching my head on this, and hoping someone has seen this before. Jun 18 12:15:30 node3 kubelet[2512]: I0618 12:15:30.923295 2512 ???:1] "http: TLS handshake error from 127.0.0.1:56326: EOF" Jun 18 12:15:32 node3 kubelet[2512]: I0618 12:15:32.860784 2512 ???:1] "http: TLS handshake error from 127.0.0.1:58884: EOF" Jun 18 12:15:40 node3 kubelet[2512]: I0618 12:15:40.922857 2512 ???:1] "http: TLS handshake error from 127.0.0.1:58892: EOF" Jun 18 12:15:42 node3 kubelet[2512]: I0618 12:15:42.860990 2512 ???:1] "http: TLS handshake error from 127.0.0.1:56242: EOF" So twice every ten seconds, but only on 2 out of 3 worker nodes, and 0 of 3 control nodes. 'node1' is identically configured, and does not have this happen. All nodes were provisioned within a few hours of each other about a year ago.I've tried what I felt was obvious. Metrics server? Node exporter? Victoria metrics agent? Scaled them down, but the log errors continue.This is using K8S 1.33.1, and while it doesn't appear to be causing any issues, I'm irritated that I can't narrow it down. I'm open to suggestions, and hopefully it's something stupid I didn't manage to hit the right keywords for.]]></content:encoded></item><item><title>What helped me understand interface polymorphism better</title><link>https://www.reddit.com/r/golang/comments/1lepxs8/what_helped_me_understand_interface_polymorphism/</link><author>/u/Yierox</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 19:18:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi all. I have recently been learning Go after coming from learning some C before that, and mainly using Python, bash etc. for work. I make this post in the hope that someone also learning Go who might encounter this conceptual barrier I had might benefit.I was struggling with wrapping my head around the concept of interfaces. I understood that any struct can implement an interface as long as it has all the methods that the interface has, then you can pass that interface to a function.What I didn't know was that if a function is expecting an interface, that basically means that it is expecting a type that implements an interface. Since an interface is just a signature of a number of different methods, you can also pass in a different interface to that function as long as it still implements all those methods expected in the function argument. Found that out the hard way while trying to figure out how on earth an interface of type  could still be accepted as an argument to the  method. Here is some code I wrote to explain (to myself in the future) what I learned.For those more experienced, please correct or add to anything that I've said here as again I'm quite new to Go.package main import ( "fmt" ) type One interface { PrintMe() } type Two interface { // Notice this interface has an extra method PrintMe() PrintMeAgain() } func IExpectOne(i One) { // Notice this function expects an interface of type 'One' // However, we can also pass in interface of type 'Two' because // implicitly, it contains all the methods of interface type 'One' i.PrintMe() } func IExpectTwo(ii Two) { // THis function will work on any interface, not even explicitly one of type 'Two' // so long as it implements all of the 'Two' methods (PrintMe(), PrintMeAgain()) ii.PrintMe() ii.PrintMeAgain() } type OneStruct struct { t string } type TwoStruct struct { t string } func (s OneStruct) PrintMe() { fmt.Println(s.t) } func (s TwoStruct) PrintMe() { fmt.Println(s.t) } func (s TwoStruct) PrintMeAgain() { fmt.Println(s.t) } func main() { fmt.Println() fmt.Println("----Interfaces 2----") one := OneStruct{"Hello"} two := TwoStruct{"goodbye"} oneI := One(one) twoI := Two(two) IExpectOne(oneI) IExpectOne(twoI) // Still works! IExpectTwo(twoI) // Below will cause compile error, because oneI ('One' interface) does not implement all the methods of twoI ('Two' interface) // IExpectTwo(oneI) } ]]></content:encoded></item><item><title>More efficient way of calling Windows DLL functions</title><link>https://www.reddit.com/r/golang/comments/1lep3zm/more_efficient_way_of_calling_windows_dll/</link><author>/u/kjk</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 18:46:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/kjk ]]></content:encoded></item><item><title>[R] Is anyone else finding it harder to get clean, human-written data for training models?</title><link>https://www.reddit.com/r/MachineLearning/comments/1leoita/r_is_anyone_else_finding_it_harder_to_get_clean/</link><author>/u/irfanpeekay</author><category>reddit</category><pubDate>Wed, 18 Jun 2025 18:22:51 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I’ve been thinking about this lately with so much AI-generated content on the internet now, is anyone else running into challenges finding good, original human written data for training?Feels like the signal to noise ratio is dropping fast. I’m wondering if there’s growing demand for verified, high-quality human data.Would love to hear if anyone here is seeing this in their own work. Just trying to get a better sense of how big this problem really is and if it’s something worth building around.]]></content:encoded></item></channel></rss>