<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses</title><link>https://www.wiz.io/blog/kubernetes-report-preview-2025</link><author>/u/Wownever</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 08:45:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[In the ever-evolving world of cloud-native technologies, Kubernetes continues to reign supreme - and with great power comes great responsibility. Our latest Kubernetes Security Report Refresh is coming soon and will unveil a landscape of both peril and progress.Â As a special sneak preview, let's explore the key findings that are shaping the future of container security.Â AKS clusters face probing attempts a mere 18 minutes after deployment.Picture this: Your freshly deployed public , barely out of its digital infancy, already under siege. Our research reveals a startling reality where malicious actors operate at breakneck speeds, probing for weaknesses before the digital ink has even dried on your configuration files. This finding serves as a stark reminder: in the world of Kubernetes, security can never be an afterthought.Â As of October 2024, Kubernetes 1.29 now leads the pack, dethroning last year's 1.24Â End of Support versions down to 46% from 58% last year among the managed clustersÂ Here's a reason to celebrate: Kubernetes operators are leveling up their game. We're witnessing a sea change in version management practices, with teams swiftly adopting the latest releases and bidding farewell to outdated versions. This proactive stance isn't just about chasing the newest features - it's a robust defense against lurking vulnerabilities.Â Severe vulnerabilities in exposed pods slashed by 50%Â Significant drop in high-privilege pod countsÂ The data paints a picture of leaner, meaner workloads. Security teams are tightening the screws on , resulting in a dramatic reduction of critical flaws in exposed containers. Moreover, the principle of least privilege is gaining traction, with fewer pods wielding unnecessary powers. It's a testament to the growing sophistication of Kubernetes security practices.Â While these highlights offer a glimpse into the state of Kubernetes security, they're just the tip of the iceberg. To truly navigate the complexities of  in 2025, you need the full picture.Â While you await the full report (coming soon), check out some of our other Kubernetes content, including:Â ]]></content:encoded></item><item><title>How do you scale to zero and from zero?</title><link>https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/</link><author>/u/Electronic_Role_5981</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 08:05:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Electronic_Role_5981 ]]></content:encoded></item><item><title>How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?</title><link>https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/</link><author>/u/blvck_viking</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 07:03:10 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I'm currently using the GKE free tier and was wondering:Does GKE free tier include any free persistent storage, or will I need to pay for it?What's the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?Any recommendations on storage classes and best practices?   submitted by    /u/blvck_viking ]]></content:encoded></item><item><title>Browser-on-ram: Sync browser related directories to RAM</title><link>https://github.com/64-bitman/browser-on-ram</link><author>/u/64bitman</author><category>linux</category><category>reddit</category><pubDate>Mon, 17 Feb 2025 06:55:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MemSed: MEMory Search and EDit for Linux, inspired by Cheat Engine</title><link>https://www.reddit.com/r/linux/comments/1ircwje/memsed_memory_search_and_edit_for_linux_inspired/</link><author>/u/WillyJL</author><category>linux</category><category>reddit</category><pubDate>Mon, 17 Feb 2025 05:48:05 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I wanted to do the usual Cheat Engine workflow to edit values in games, but found no good solution for Linux. There's Game Conqueror but that crashed a lot for me and doesn't really work how I wanted, so I just made my own!It is still a work in progress, but works fairly well for day-to-day use at this point. Should work on most Linux distros and does not have any additional requirements, it's a single (nearly static) binary. Due to the nature of what it does (read/write process memory) it requires running as root, *insert usual word of warning about that here*.Will post a demo video in the comments below.]]></content:encoded></item><item><title>One-Minute Daily AI News 2/16/2025</title><link>https://www.reddit.com/r/artificial/comments/1ircvd9/oneminute_daily_ai_news_2162025/</link><author>/u/Excellent-Target-847</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Mon, 17 Feb 2025 05:46:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>Linux let my 2015 mbp actually work again!</title><link>https://www.reddit.com/r/linux/comments/1ir83i9/linux_let_my_2015_mbp_actually_work_again/</link><author>/u/jasonsc95</author><category>linux</category><category>reddit</category><pubDate>Mon, 17 Feb 2025 01:20:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ingress Help</title><link>https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/</link><author>/u/MeerkatMoe</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 01:04:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm trying to setup ingress using ingress nginx, but I can't figure out how to get routing to work...either my frontend breaks or my api is unreachable.I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.My first attempt was two separate ingresses (not sure about terminology):--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: "nginx" spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: "nginx" nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} but that didn't work, and sometimes my API won't get routed correctly. I think it's because they get combined and I can't guarantee the order.My next try was to combine them:kubernetes.io/ingress.class: "nginx" nginx.ingress.kubernetes.io/use-regex: "true" nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: "/(?!api).*" pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} (left some stuff out to save space)but that also didn't work.What is the best way to get this working? To summarize, I just need"/api/*" -> api service port 5000 (it can route as /api/<whatever> or just <whatever>)]]></content:encoded></item><item><title>How arch-delta works and saves bandwidth for Arch Linux upgrades</title><link>https://djugei.github.io/how-arch-delta-works/</link><author>/u/djugei</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 17 Feb 2025 00:49:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Event driven workloads on K8s - how do you handle them?</title><link>https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/</link><author>/u/sniktasy</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Mon, 17 Feb 2025 00:31:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have been working with Numaflow, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etcCurious, how are you handling event-driven workloads today? Would love to hear what's working for others?]]></content:encoded></item><item><title>There is no 1875 epoch</title><link>https://iter.ca/post/1875-epoch/</link><author>/u/AlanBennet29</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 22:54:49 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A US government official said â€œweâ€™ve got people in there that are about 150 years oldâ€ claiming social security benefits.Some people have claimed that the reason for this is that the Social Security Administration (SSA) uses an epoch in 1875 for storing dates, and these are just people with unknown years of birth stored as 0. I think the origin of these claims is this post:It looks like Elonâ€™s genius coders donâ€™t know how COBOL works.Social security runs on COBOL, which does not use a date or time type.  So the date is stored as a number using the ISO 8601 standard.  The epoch for this is 150 years ago (1875) - aka the metre standard.So if you donâ€™t know the date of something, it will be a 0 value, which in COBOL will default to 1875 - 150 years ago.I donâ€™t think this is true, for a few reasons.The database has years of birth before 1875In 2007 the SSA released a dataset â€œcontaining earnings records for individuals drawn from a 1-percent sample of all Social Security numbers (SSNs) issued before January 2007â€. They wrote:The final adjustments included removing 5,935 individuals whose [Year Of Birth] value was before 1870, removing 1,096 individuals whose YOB value was equal to 2007, and removing 4 individuals who were assigned a missing YOB value. Individuals born before 1870 were removed because they were unlikely to have received Social Security benefits.They explictly say they have records of individuals born in 1869 and earlier, and that they can represent missing birth years!There is no spike of births in 1875There is no spike in births in 1875 in that dataset, which you would expect if some process was setting unknown births to 1875:The dataset is a 1% sample, so the actual amounts are ~100x larger.The SSA doesnâ€™t use ISO 8601The Master Beneficiary Record, which tracks social security benefits payments, was created in 1962 -  ISO 8601 was first published in 1988. The predecessor to that standard, ISO 2016 was published in 1976 - still too early, and also it has no reference any date in 1875.This research paper based on SSA data said that the SSA stores birthdays in a fixed-width format:The data abstracted from the MBR consisted of a 26-character record for each deceased individual. The four data
items on each record wereâ€¦ the month and year of death.None of the datasets published by the SSA I found used ISO 8601 dates either; all of them have a seperate column for year of birth instead of using an ISO 8601 birthdate.ISO 8601 doesnâ€™t have an epochISO 8601 is a format for representing dates as strings, not as numbers. It has no need for an epoch.ISO 8601:2004 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the Convention du MÃ¨tre (Metre Convention) was signed in Paris (the explicit reference date was removed in ISO 8601-1:2019). However, ISO calendar dates before the convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on 15 October 1582.I.e. the standard only uses 20 May 1875 as a reference date to define the Gregorian calendar, not as some earliest representable date.Nobody uses 1875 as an epochI had found no evidence of 1875 ever being used as an epoch to start counting time from, in any context. I tried to find any case of this happening but I couldnâ€™t find any. Itâ€™s definitely not a standard COBOL thing.]]></content:encoded></item><item><title>Should People Just Use Goreleaser Instead of `actions-rust-release`?</title><link>https://blog.urth.org/2025/02/16/should-people-just-use-goreleaser-instead-of-actions-rust-release/</link><author>/u/autarch</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 22:32:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Iâ€™m cross-posting this from
an issue I made for
. For context,  am
the actionâ€™s author, and this is a serious question, not the start of a pitch for why you should use
my action.TLDR; Does  serve any purpose that isnâ€™t better served by
goreleaser?Hereâ€™s the issue body in full:Recently, I was considering adding some features to this action, notably adding the ability to
produce signed releases (specifically, signing the checksums file). As I started looking into
this, I realized that goreleaser already does this, as well as many
other things this action doesnâ€™t do:It offers a lot more power and flexibility in what is included in the resulting release archive
files.This includes templating files, so for example you can update the copyright year in the
 file to match the release date.Deb, RPM, macOS DMG, MSI, Chocolatey, etc. support.Integration with SBOM creation tools.So Iâ€™m wondering whether itâ€™s worth continuing to invest in this action. It seems like using
goreleaser to release a Rust project is fairly easy. It even supports , though I think
for that Iâ€™d still use my
actions-rust-cross action, as I donâ€™t think
goreleaser would make it easier to do cross-platform builds.Will people who use this action see this issue? If you do, Iâ€™d greatly appreciate your feedback!
Take a look at goreleaser, focusing specifically on the parts related
to releasing, not building. After looking, do you still prefer this actions? If so, why?]]></content:encoded></item><item><title>Why do people hate Ubuntu so much?</title><link>https://www.reddit.com/r/linux/comments/1ir3aq8/why_do_people_hate_ubuntu_so_much/</link><author>/u/Large-Start-9085</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 21:36:06 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[When I switched to Linux 4 years ago, I used Pop OS as my first distro. Then switched to Fedora and used it for a long time until recently I switched again. This time I finally experienced Ubuntu. I know it's usually the first distro of most of the users, but I avoided it because I heard people badmouth it a lot for some reason and I blindly believed them. I was disgusted by Snaps and was a Flatpak Fanboy, until I finally tried them for the first time on Ubuntu. I was so brainwashed that I hated Ubuntu and Snaps for no reason. And I decided to switch to it only because I was given permission to work on a project using my personal laptop (because office laptop had some technical issues and I wasn't going to get one for a month) and I didn't wanted to take risk so I installed Ubuntu as the Stack we use is well supported on Ubuntu only.And damn I was so wrong about Ubuntu! Everything just worked out of the box. No driver issues, every packege I can imagine is available in the repos and all of them work seemlessly. I found Snaps to be better than Flatpaks because Apps like Android Studio and VS Code didn't work out of the box as Flatpaks (because of absurd sandboxing) but I faced no issues at all with Snaps. I also found that Ubuntu is much smoother and much more polished than any distro I have used till now. I really love the Ubuntu experience so far, and I don't understand the community's irrational hate towards it.]]></content:encoded></item><item><title>[D]How to handle highly imbalanced dataset?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ir2zm3/dhow_to_handle_highly_imbalanced_dataset/</link><author>/u/ThickDoctor007</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 21:22:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Iâ€™m working on an insurance claims prediction model, and Iâ€™d love to get insights from the community on tackling a highly imbalanced dataset. In the past, I built churn prediction models, and now Iâ€™m focusing on predicting insurance claims, where the percentage of claims is quite low.My dataset spans 15 years and contains ~800,000 records with features such as sex, age, horsepower, car brand & type ]]></content:encoded></item><item><title>How to Use the New tool Directive in Go 1.24</title><link>https://www.bytesizego.com/blog/go-124-tool-directive</link><author>/u/zakariachahboun</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 21:11:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The release of Go 1.24 introduces the  directive for Go modules. This  simplifies the process of managing tools, such as linters or generators that are widely used in many Go
projects but are not directly used in the codebase. In this blog post, we will explore what the  directive is, why
it matters, and how to use it effectively.The  directive allows Gophers to specify dependencies for tools used in your Go project without adding those tools as
regular dependencies. This is particularly useful for tools like , , or other command-line
utilities that support your development workflow but donâ€™t appear in your import graph.Previously, developers had to manage such tools using workarounds, such as adding them to a  or a  file or manually
installing them outside of Go modules. The  directive removes this friction by providing a dedicated way to
declare these dependencies.Using the new  directive offers several benefits:: Tools are clearly distinguished from code dependencies, reducing confusion.: You can lock specific versions of tools, ensuring consistent behavior across environments.: The Go tooling can install and manage tools automatically based on the  directive,
removing the need for custom scripts or manual installation.Hereâ€™s how you can add a tool dependency using the  directive:Step 1: Update Your  FileIn this example github.com/golang/mock/mockgen is the path to the tool.This will add a tool directive to your go.mod file. It will look something like this:After declaring tools, you can install them using the  command:Once installed, the tools are available for use.  For instance:You can see which tools are installed by running:
: Always specify a version for your tools to ensure consistency. Go will do this for you.: Include instructions in your projectâ€™s README or CONTRIBUTING file on how to install and use
the tools.The  directive is only available in Go 1.24 and later. Ensure that your contributors use this version or greater.]]></content:encoded></item><item><title>Issues with logrotate when logrotate failed to rotate the logs for container</title><link>https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/</link><author>/u/barely_malted</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 21:06:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5) I am facing an issue where I believe these default values are not respected. The kubelet is failing with 'Failed to rotate log for container' 'err=failed to compress log (container/pod log paths) nospace left on device' At the same time one of my pods generated 200 GB logs in one single file. How is this possible ? I was not able to find out any documentation regarding this behaviour. Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ? K8s/EKS version 1.27]]></content:encoded></item><item><title>How do devs use kubernetes services locally via ingress on the likes of docker desktop</title><link>https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/</link><author>/u/TheRandyOne</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 19:28:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it'll really suck to have the devs to add a bunch of records to the hosts fileBasically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.Am I doing this wrong? Is there some other way that I should be approaching this problem? Or can someone explain how they deal with this other than just editing hosts files.]]></content:encoded></item><item><title>Why did they decide not to have union ?</title><link>https://www.reddit.com/r/golang/comments/1iqzofv/why_did_they_decide_not_to_have_union/</link><author>/u/D4kzy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 19:03:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I know life is simpler without union but sometimes you cannot get around it easily. For example when calling the windows API or interfacing with C.Do they plan to add union type in the future ? Or was it a design choice ?   submitted by    /u/D4kzy ]]></content:encoded></item><item><title>Linus Quote of the day</title><link>https://www.reddit.com/r/linux/comments/1iqz9we/linus_quote_of_the_day/</link><author>/u/bhagwano-ka-bhagwan</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 18:47:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&quot;A calculator app? Anyone could make that.&quot;</title><link>https://chadnauseam.com/coding/random/calculator-app</link><author>/u/iamkeyur</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 18:41:02 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pull Request testing on Kubernetes: working with GitHub Actions and GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/</link><author>/u/nfrankel</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 18:22:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Iâ€™m continuing my series on running the test suite for each Pull Request on Kubernetes. In the previous post, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, Iâ€™ve identified gaps that Iâ€™ll implement in this blog post:Create and configure a Google Kubernetes Engine instanceCreate a Kubernetes manifest for the app, with Kustomize for customizationAllow the GitHub workflow to use the GKE instanceBuild the Docker image and store it in the GitHub Docker repoInstall the PostgreSQL Helm chartFinally, run the end-to-end testStages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because Iâ€™m more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.]]></content:encoded></item><item><title>Hinton: &quot;I thought JD Vance&apos;s statement was ludicrous nonsense conveying a total lack of understanding of the dangers of AI ... this alliance between AI companies and the US government is very scary because this administration has no concern for AI safety.&quot;</title><link>https://www.reddit.com/r/artificial/comments/1iqy8te/hinton_i_thought_jd_vances_statement_was/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 18:04:15 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Managing a Talos cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/</link><author>/u/simen64</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 17:36:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM's in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?]]></content:encoded></item><item><title>I wrote a desktop overlay for reading manga with egui</title><link>https://www.reddit.com/r/rust/comments/1iqxfd0/i_wrote_a_desktop_overlay_for_reading_manga_with/</link><author>/u/Takader</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 17:30:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Today i am open sourcing my manga overlay i have been working on. It enables continues detection of japanese text in a selected region on the desktop. My goal was making it easy to find the meaning of kanji in order to learn japanese. This is the first time i am open sourcing a project so feedback is welcome.]]></content:encoded></item><item><title>FOSDEM 2025 - Linux Ã— VR! Beginner&apos;s Guide on How to Join Events in Virtual Reality from Ubuntu using Envision and Monado, an OpenXR Alternative to SteamVR</title><link>https://fosdem.org/2025/schedule/event/fosdem-2025-5976-linux-vr-beginner-s-guide-on-how-to-join-events-in-virtual-reality-from-ubuntu-using-envision-and-monado-an-openxr-alternative-to-steamvr/</link><author>/u/nialv7</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 17:18:00 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Discover how to take your Linux experience into the immersive world of Virtual Reality! In this lightning talk, I, Kawane Rio, will give you a lightning-speed step-by-step crash course tutorial on setting up SteamVR on Ubuntu, installing Linux and Proton compatible VR platforms like VRChat, ChilloutVR, Resonite, and Overte, and exploring an open-source alternative to SteamVR: Monado. Youâ€™ll also learn about Envision, the GUI orchestrator that simplifies Monadoâ€™s setup, and WlxOverlay-S, an OpenXR/OpenVR overlay for Wayland and X11.Beyond the tech, I'll share insights into VR events like the LinuxVRC Meetup and MisskeyVRChat Meetup, where Open Source Software and VR enthusiasts come hang out together! Don't miss the chance to embrace FOSS VRâ€”stop by the Overte booth to learn more!Whether you're a VR newcomer or a Linux enthusiast, this lightning talk should give you a rough idea on where to start your journey to join us at the virtual frontier.Projects/Repositories in this Lightning TalkThanks to Everyone in the Community for making LinuxVR possible.]]></content:encoded></item><item><title>Looking for lightweight Android emulator</title><link>https://www.reddit.com/r/linux/comments/1iqwwrq/looking_for_lightweight_android_emulator/</link><author>/u/__Timo_L_S__</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 17:09:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hi, I'm looking for a lightweight and fast Android emulator, it only needs to have the very basics as all that I want to do is launch a single app. Currently I'm using waydroid to achieve this but starting the app takes a while as it has to boot android up first. This got me wondering if there exists a stripped version or something that would start faster. No google services required.]]></content:encoded></item><item><title>Measure cpu utilization per deployment?</title><link>https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/</link><author>/u/netcat23</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 16:18:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi guys, does measuring cpu utilization of a deployment brings any value?What is you opinion about it?]]></content:encoded></item><item><title>Finally installed Arch in an old 32 bits machine!!</title><link>https://www.reddit.com/r/linux/comments/1iqvkm3/finally_installed_arch_in_an_old_32_bits_machine/</link><author>/u/mierd41a</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 16:12:43 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I installed Arch in this Samsung Laptop NC210 (32-bit) . I was with a lot of problems with keyrings but I was able to fix it. It was easier than I expected, although I have already installed Arch before.What DE or WM do you recommend? It has 2GB of RAM and an Intel Atom, I was thinking about XFCE or BSPWM.I didn't know what TAG put, sorry if I it is wrong. ]]></content:encoded></item><item><title>Announcing: pixelvim, vim inspired pixel editor</title><link>https://www.reddit.com/r/rust/comments/1iqviie/announcing_pixelvim_vim_inspired_pixel_editor/</link><author>/u/avatar_10101</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 16:10:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ is a pixel editor inspired by the  text editor, with an emphasis on keyboard interaction. It also aims to be feature-rich, customizable, and extendable via user scripts.This is my personal project of making a vim-like pixel art editor (not very creative with the name, I know), written in Rust using miniquad.Try it in the browser: https://bolphen.github.io/pixelvim/ (you can drag-and-drop png, gif, and aseprite files, and save to png and gif; use  to increase the UI if you find them too small)vi-style remappable keyboard interaction and a command system, including modifiers ( for moving 5 pixels down) and chain-able commands (:select/all THEN cut THEN :layer/new/above THEN paste)elaborated "visual" mode for pixel selection (that is undo/redoable)animation "live draw" (see the screencast below: very useful for quickly creating particle effects)rudimentary support for lua user scripts (not available in the browser version)data recovery from swap file in case of crashesThe code is not pretty (a lot of places held up with glue) and there are quite a lot I want to improve as well as new features to add, but I feel that the end product could already be useful for some so I'm releasing it. Overall I wish to translate more vim features that could be useful into pixelvim (for example, insert mode for drawing purely with the keyboard; registers and macros; better documentation), also better UI.There used to be a similar project rx. This is not a fork, though I did borrow a few things here and there. I'd say right now pixelvim is much more feature complete than rx.]]></content:encoded></item><item><title>mongotui - A MongoDB client with a terminal user interface</title><link>https://github.com/kreulenk/mongotui</link><author>/u/gopher_256</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 15:51:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI do have some points tho</title><link>https://www.reddit.com/r/artificial/comments/1iqv26f/ai_do_have_some_points_tho/</link><author>/u/JaydenPlayz2011</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 15:49:59 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Making a Streaming JOIN 50% faster</title><link>https://www.reddit.com/r/rust/comments/1iqum3o/making_a_streaming_join_50_faster/</link><author>/u/bobbymk10</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 15:29:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/bobbymk10 ]]></content:encoded></item><item><title>Making a Streaming Join 50% Faster</title><link>https://www.epsio.io/blog/optimizing-streaming-joins-leveraging-asymmetry-for-better-performance</link><author>/u/ThinkRedstone</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 15:28:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Deliver instant & up-to-date results for complex queries]]></content:encoded></item><item><title>How JIT (Just in time) compilation and V8 works and makes js incredibly fast</title><link>https://www.royalbhati.com/posts/why-js-is-fast</link><author>/u/CaptainOnBoard</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 15:28:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Writing LLM prompts in Go with type-safety</title><link>https://blog.lawrencejones.dev/ai-dont-need-python/</link><author>/u/shared_ptr</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 14:53:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[â€œIf you want to build AI products, you need to use Python.â€I hear this a lot, and itâ€™s wrong. I get why people think it- software engineers
have been trained to lean into open-source tooling, and the AI ecosystem is
undoubtably strongest in Python.But if you donâ€™t already use Python, switching languages for the ecosystem is
likely a bad call. Not only is that ecosystem less mature than it seems (who
genuinely has enough experience to build an opinionated agent-framework at this
point?), but it is grounded more in a research/ML perspective than a product
one.Additionally, the language itself is a poor fit for AI products! Python
concurrency can be tricky, which is going to hurt you when it comes to
optimising the latency of your AI systems, and a static type-system can bring
some much-needed structure to non-deterministic models.At incident.io, we use Go for all our backend, and saw no reason why AI should
be any different. Go has been a fantastic choice, helping us build AI
systems that spawn concurrent prompts, speculatively execute tools, and instrument
everything to the hilt.Despite Go being known as a no-frills language, weâ€™ve built some really
ergonomic abstractions that make working with AI really easy. I expect you can
do it in your native language, too.Hereâ€™s an example of a prompt in our codebase:Each prompt is its own struct, describing which model to use, the input type
that we use to template the messages, and finally the prompt that weâ€™ll send to
OpenAI (or Anthropic, GCP, whatever).What I love about this is how much heavy lifting the type system is doing for
us. Running the prompt is as simple as:Behind the scenes,  is doing some cool stuff:It reflects over our  type to build an OpenAPI specification, using
struct tags like  and  as documentation.That specification is used to force the model into giving structured JSON
responses that match our type exactly.The prompt is templated with our .Finally, the response is parsed back into our result type.Each prompt lives in its own file () alongside its evaluation
suite (), making it easy to maintain and test.The pattern above works great for fixed prompts, but sometimes you need more
flexibility. Thatâ€™s where Goâ€™s generics come in handy.Hereâ€™s an example from our incident investigation system:Weâ€™re using  across our investigation
system, but each time we run it we want different types of
results back. Sometimes we want permalinks to code changes, other times weâ€™re
extracting timeline entries, or grading the quality of our investigation
hypothesis.Rather than copy-and-pasting the prompt in order to use the same context setup,
we can use a generic type parameter to control the result we get back. The
prompt template and core logic stays the same, but we get different structured
data back each time.Thatâ€™s as close as weâ€™ll get to Vercelâ€™s  in Go, and Iâ€™d say
itâ€™s a strong challenger.Python is not the only answerPython is a great tool for machine learning and a lot more. But it has real
weaknesses when it comes to building production AI systems, especially if youâ€™re
already working in a different language.Iâ€™ve spoken with several teams who switched to Python to work with AI, away from
their normal stack, and theyâ€™ve all struggled. Theyâ€™re trying to learn AI
alongside new tools and patterns (almost always a bad idea), which
makes it much more likely they burn out or fail.Even if they succeed, theyâ€™ve disconnected their AI features from their core
product. These are companies aiming to be â€œAI nativeâ€ but have their AI features
living in a separate codebase with different conventions. That doesnâ€™t feel like
success to me.At incident.io, we kept everything in Go and itâ€™s working really well. Our AI
features live right alongside our core product, using the same development
patterns the team already knows. Weâ€™ve even managed building some expressive and
fancy abstractions in a language that is notoriously no-frills, while leveraging
Goâ€™s strengths in concurrency and type safety.The AI ecosystem will continue to be Python-first, and thatâ€™s fine! The actual
interaction with models is just HTTP requests with JSON payloads, though. Build
good abstractions in your language of choice, and focus your energy on the hard
stuff: making AI features that actually work.
        
        If you liked this post and want to see more, follow me at @lawrjones.
      ]]></content:encoded></item><item><title>Starting a Weekly Rancher Series â€“ From Zero to Hero!</title><link>https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/</link><author>/u/abhimanyu_saharan</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 14:47:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you're new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.I'll be posting new videos every week, so if you're interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you'd like to see covered!Letâ€™s build and learn together! ðŸš€]]></content:encoded></item><item><title>15 lessons from 15 years in tech</title><link>https://newsletter.eng-leadership.com/p/15-lessons-from-15-years-in-tech</link><author>/u/gregorojstersek</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 14:19:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Finding skilled developers is hard â†’ even for seasoned engineering leaders. Traditional hiring cycles can take months while critical features sit in the backlog.Need to scale your team quickly or find specialized talent that's hard to source locally? They match you with developers from Europe and Latin America who integrate seamlessly into your workflow â†’ without the long hiring cycles or commitment of long-term contracts.They don't just check rÃ©sumÃ©s, but put developers through a rigorous multi-step vetting process that assesses technical skills, problem-solving abilities, and communication.Letâ€™s get back to this weekâ€™s thought!Iâ€™ve made many mistakes throughout my 11+ career in the engineering industry and I learned a lot from them. But one thing I didnâ€™t do so well is to learn from mistakes from others.I didnâ€™t actively look for mentorship and coaching and there werenâ€™t a lot of books on pure Engineering Management back when I first became a manager to help me with this transition.Even though I still progressed all the way to CTO, I believe that my progression would be a LOT easier with good mentorship support.This is very important to keep in mind and thatâ€™s exactly the goal of todayâ€™s article.Owain LewisHeâ€™ll be sharing 15 lessons that he has learned the hard way. Letâ€™s get straight into them. Tech changes fast, but certain principles stay the same.These lessons have helped me navigate challenges at work, grow as a professional, and deliver results. Maybe theyâ€™ll help you too.Here are 15 lessons Iâ€™ve learned along the way:One common mistake I've observed repeatedly is projects being driven by technical enthusiasm rather than product necessity. Engineers often dive into building features without fully understanding whether they address real user needs or business problems. Document assumptions and validate them before writing code. Project cancellations are incredibly painful for everyone, and theyâ€™re often the result of brilliant tech solving the wrong problem.Not every piece of technical debt needs fixing. Not every feature deserves building. Sometimes, doing nothing is the best move. Prioritisation means choosing from the endless things teams â€œshouldâ€ do, and selecting the few things that really matter.Key Takeaway: Ask "What happens if we do nothing?" to gain perspective on priorities. Often, the answer reveals that urgent-feeling work isn't actually important.Heated debates over the perfect language, architecture or framework often miss the point entirely. Every technical decision involves trade-offs: speed vs. maintainability, simplicity vs. flexibility, familiar tools vs. better technology.Key Takeaway: The goal isnâ€™t perfection, itâ€™s finding the right balance of trade-offs for your specific context.As engineers, we love sophisticated solutions. Microservices, event-driven architectures, the latest trends. But simpler approaches often prove more resilient and maintainable. Remember: you build once, but maintain forever. You might be happy about that clever solution now but you wonâ€™t be when your pager goes off at 2am.Key Takeaway: Start simple. Only add complexity when absolutely necessary. Simpler systems are easier to understand, debug, and extend.Understanding the business isnâ€™t optional, itâ€™s critical for making sound technical decisions. The more you know about the business, the more opportunities youâ€™ll find to grow your career.Key Takeaway: Invest time in learning your companyâ€™s business model, market constraints, and strategy.Want to accelerate your career? Focus on helping others succeed. Thoughtfully review pull requests, ask your manager what they need help with, jump in when someoneâ€™s stuck, and freely share knowledge. Helping others makes you more valuable, helps you learn faster, and also creates a better work environment.Key Takeaway: People notice those that help others. Managers, engineers, and even executives. Build a reputation for being helpful.When people fear mistakes, they stop taking initiative. When they fear questions, they waste time struggling alone. Operational post mortems and retrospectives are great ways to help teams learn from mistakes. Make sure people arenâ€™t afraid to ask for help, ask questions, or share their opinions.Key Takeaway: Create an environment where people feel safe to take risks, admit errors, and challenge ideas.A lesson I keep learning is that the fastest way to learn is from others who are ahead of you. Equally important, mentoring others builds your own understanding. Teaching others is the best way to learn.Key Takeaway: Learn from those ahead of you, then pay it forward by guiding those behind you.Titles are lagging indicators of impact. Donâ€™t be intimidated by titles or years of experience. Act like a senior before you are one. Focus on delivering meaningful contributions, solving problems, improving processes, and supporting your team. The people who progress rapidly are the ones who start acting as leaders or seniors before they have the title.Key Takeaway: Donâ€™t let imposter syndrome or titles hold you back. Recognition and titles will follow naturally when you consistently deliver results.A harsh career reality is that youâ€™re rewarded for the value you create, not the effort you put into it. Brilliant work means nothing if itâ€™s unfinished.Focus on completing tasks, no matter how small. Over time, these completed efforts compound into significant achievements.Key Takeaway: Donâ€™t just start things, finish them. Prioritise execution and delivery. The most successful people in tech are the ones who consistently get things done.Writing forces you to clarify your thoughts and uncover gaps in your understanding. Whether itâ€™s documentation, design docs, or blog posts, writing sharpens your thinking and communication skills. Whenever I see people confused or struggling (managers, engineers), Iâ€™ll always suggest writing a one pager so everyone can understand the details clearly.If youâ€™re not sure â€¦ write it down. Itâ€™s the fastest way to create clarity in a team.Key Takeaway: Writing is one of the most underrated skills for engineers to develop. Clear communication will help you think clearly and scale your impact.Software estimates are educated guesses, not promises. Ensure stakeholders understand uncertainties and stay informed as a project progresses. Itâ€™s better to underpromise and overdeliver than to do the opposite. As my coach told me, your job as a manager is: â€œno surprisesâ€.Key Takeaway: It will always take longer than you think. Interruptions, holidays, sickness, unexpected events - all these things play a role.Perhaps the biggest thing I learned as a manager is to learn how to negotiate. This is essential because teams are always negotiating for more resources, more opportunities, or more time. Donâ€™t give up at the first â€œnoâ€. Success depends on negotiating effectively to get what you need. This isnâ€™t about winning arguments, itâ€™s about finding compromises that work for everyone (a win-win).Key Takeaway: Approach problems as negotiations. Understand different viewpoints, seek win-win solutions, but know when to walk away.Youâ€™re responsible for your own learning. Build breadth of knowledge in fundamental principles and depth of knowledge in specific areas. Building knowledge of your domain can often be hugely valuable for career progression.Key Takeaway: Deep expertise in specific areas sets you apart. Build breadth of knowledge in fundamentals and depth in important areas.Side projects let you experiment with new languages, frameworks, and ideas. No risk, no permission needed. Donâ€™t wait for employer training - start a GitHub project and learn by building. I learned more from side projects than anything else.Start small. Build a weekend project or automate a personal task.Dedicate 1-2 hours per week to learning something new.Make side projects and experiments a habitKey Takeaway: Treat side projects as a tool to grow your career quickly. Theyâ€™re the best way to learn new things.These principles have helped me handle challenges at work. I hope theyâ€™ll do the same for you, whether youâ€™re starting out or leading teams. Let me know what stands out to you and what youâ€™d change.Special thanks to Owain for sharing his lessons with us! As mentioned above, itâ€™s really important that we try to learn from other peopleâ€™s mistakes.Owain on LinkedInLeverage AII am already excited to meet the next great group of people and talk about engineering/engineering leadership topics!Itâ€™s going to be the 5th cohort and for every cohort, I try to create some improvements.This time, weâ€™ll have more time for students to network and get to know each other, so weâ€™ll have more breakout rooms for specific topics.I see cohorts as a great place to meet like-minded people who have similar goals and aspirations to help each other and build great connections that last even outside of the course!Looking forward to seeing some of you there!Liked this article? Make sure to ðŸ’™ click the like button.Feedback or addition? Make sure to ðŸ’¬ comment.Know someone that would find this helpful? Make sure to ðŸ” share this post.herehereherehereIf you wish to make a request on particular topic you would like to read, you can send me an email to info@gregorojstersek.com.This newsletter is funded by paid subscriptions from readers like yourself.If you arenâ€™t already, consider becoming a paid subscriber to receive the full experience!You are more than welcome to find whatever interests you here and try it out in your particular case. Let me know how it went! Topics are normally about all things engineering related, leadership, management, developing scalable products, building teams etc.]]></content:encoded></item><item><title>ðŸŽ¸ðŸ”¥ Introducing ChordFlow â€“ A Rust-Powered TUI for Guitar Practice!</title><link>https://www.reddit.com/r/rust/comments/1iqsx67/introducing_chordflow_a_rustpowered_tui_for/</link><author>/u/timvancann</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 14:07:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hey fellow Rustaceans and guitarists! ðŸ‘‹Iâ€™ve been working on , a terminal-based tool built in Rust to help with chord practice and improvisation. The idea came from my own struggles with guitar neck mastery and melodic improvisation. I wanted something lightweight, fast, and distraction free to help me follow chord while keeping time with a metronome. It also gave me a good opportunity to dive into ratatui and learn more about Rust!ðŸŽµ Generates random  for improvisation ðŸŽ›ï¸ Built-in  to stay in time ðŸ–¥ï¸  for an easy and minimal setup ðŸ› ï¸ â€”bring your own chord sets or use the defaults ðŸš€ Written in  for speed and efficiencyItâ€™s open-source, and Iâ€™d love feedback, contributions, or just thoughts from fellow Rustaceans and musicians! If youâ€™re into music theory, Rust, or just want a minimal practice tool, give it a try!Would love to hear what you think! What features would you like to see? ðŸ¤˜]]></content:encoded></item><item><title>I created a simple TOTP library</title><link>https://www.reddit.com/r/golang/comments/1iqrtfg/i_created_a_simple_totp_library/</link><author>/u/Strange_Fun_544</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 13:06:53 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I few months ago I created a TOTP library for myself, as I wanted to explore some different things (as I'm mostly developing websites)And I've been using it in production for a while so, I think it's time to share it with you.While I'm aware that might not be for everybody, I'm sure it will be useful for some of you :) That being said, any feedback is welcomed.]]></content:encoded></item><item><title>Wordle Solver</title><link>https://www.reddit.com/r/golang/comments/1iqrh3v/wordle_solver/</link><author>/u/james-holland</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 12:46:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built a wordle solver in golang a while ago, I tidied it up recently, documented it, dockerised it and made it public to put it on my CV.The word suggester finds the best word by recurrently:counting up how common each letter is in the WordList, repeated letters within a word are not counted.then finding the word with the highest score based on the count of letters.parsing the input and removing all the words that don't correspond to the input.Please star on github if you like it. Feel free to ask me anything.Shameless plug: I'm currently looking for a job as well. If you're hiring, you can contact me on LinkedIn . Needs to be fully remote. Glasgow/UK based but willing to work American Eastern Time or European hours if needs be.]]></content:encoded></item><item><title>Help with k3s setup on wsl</title><link>https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/</link><author>/u/watterbottle800</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 11:49:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with "failed to connect to <svc cluster ip> port 15672 : couldn't connect to server" after sometime. This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks]]></content:encoded></item><item><title>[D] The steps to do original research ( it&apos;s a rant as well )</title><link>https://www.reddit.com/r/MachineLearning/comments/1iqq4fz/d_the_steps_to_do_original_research_its_a_rant_as/</link><author>/u/Snoo_65491</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 11:14:23 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I am a Master's Student in the UK. I have been reading papers on Diffusion for a while. I have contacted PhD students at my University and have expressed my interest in working with them. I thought that I would be helping them with their research direction. However, after talking to them, they told me to read some papers and then find a research idea. For Context, I am reading about Diffusion Models. The more I read, I realize that I lack some math fundamentals. I am filling those holes, through courses, books and articles. However, it takes time. I believe that this lack of fundamental understanding is stopping me from coming up with hypotheses. I can find some research gaps through recent survey papers, but I am not able to come up with any hypotheses or a solution.Am I heading in the right direction? Does understanding stuff from a fundamental standpoint help with producing novel research ideas? How to generate novel research ideas? If you have some tips, I would be glad to hear them.P.S. I have never published before. Therefore, I am sorry if I am missing something fundamental. ]]></content:encoded></item><item><title>[P] I built an open-source AI agent that edits videos fully autonomously</title><link>https://github.com/diffusionstudio/agent</link><author>/u/Maximum_Instance_401</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 11:09:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>httpmock: simple yet powerful HTTP mocking library for Rust</title><link>https://httpmock.rs/</link><author>/u/EightLines_03</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 10:57:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Includes an extensive set of built-in matchers that let you define rules for matching requests based on method, path, headers, query parameters, body, and more, or use  for complex matching logic.]]></content:encoded></item><item><title>Is Nvidia on Linux still bad?</title><link>https://www.reddit.com/r/linux/comments/1iqpsy0/is_nvidia_on_linux_still_bad/</link><author>/u/Szer1410</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 10:51:56 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I am planning to buy a laptop. I want to have a peak Linux experience, so I have been looking for laptops with dedicated AMD GPUs. While searching, I noticed a few things:There are not many laptops with dedicated AMD GPUs. Most available options come with integrated GPUs like the 780M.For the price of a laptop with a 780M, I can get a laptop with an RTX 3050 or better.System76 sells Linux laptops with Nvidia GPUs on their website.Additionally, I want to install Manjaro on my laptop. Are there any Linux distributions with better Nvidia support?]]></content:encoded></item><item><title>Kubernetes In-Place Pod Vertical Scaling</title><link>https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/</link><author>/u/Wownever</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 10:50:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are Resize CPU and Memory Resources assigned to Containers, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.What Is In-Place Pod Vertical Scaling?Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.Whatâ€™s behind the feature gate?The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.The result of the change attempt is communicated as part of the podsâ€™ status in a field calledÂ   (for more information on the new fields, check out the Kubernetes API documentation.)Additionally, this feature introduces the  in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.Dynamic Scaling: Modify CPU and memory allocations while pods run.No Restarts: Avoid downtime caused by pod restarts.Granular Control: Enable precise resource tuning for better efficiency.The InPlacePodVerticalScaling feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Hereâ€™s a detailed breakdown of how it operates: Activating the InPlacePodVerticalScaling feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.Dynamic Resource Updates via Kube API: With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., containerd v1.6.9 or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled. The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.Limitations and ConsiderationsWhile In-Place Pod Vertical Scaling offers significant benefits, it has limitations:1. Cloud Provider SupportAWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the GKE alpha clusters documentation.Several Kubernetes policies and mechanisms govern resource scaling. These include:Resource quotas limit the total CPU and memory usage for a namespace. If an InPlacePodVerticalScaling operation exceeds these limits, the scaling request will fail. For example:apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: example-namespace
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "32Gi"
Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: example-namespace
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
Admission controllers, such as Pod Security Admission or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.Memory-Bound Applications: Applications like Java with fixed Xmx parameters.In cases where the HPA is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:HPA scaling behavior is based on CPU average utilizationA pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created. Dynamically allocate resources during training and inference phases. Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling. Reduce waste by allocating the right amount of resources to each pod in real-time. Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Googleâ€™s example, Startup CPU Boost, demonstrates how dynamic resource scaling can address such scenarios effectively.1. Enable the Feature GateAdd the following configuration to enable the InPlacePodVerticalScaling feature:apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
controllerManager:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
scheduler:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
For GKE, create a cluster with alpha features enabled:gcloud container clusters create poc \
    --enable-kubernetes-alpha \
    --no-enable-autorepair \
    --no-enable-autoupgrade
Define a deployment with initial CPU and memory requests and limits:apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
          requests:
            memory: "64Mi"
            cpu: "250m"
        resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: NotRequired
Once deployed, you can check the cpu.weight, cpu.max, memory.max, memory.min from within the container to see the initial values that the container starts with.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max 
Adjust resource allocations for a running pod dynamically:kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"750m"}}}]}}'
Confirm updated resource settings:kubectl describe pod -l app=app
Additionally, you can connect to the container and see the change in cpu.weight, cpu.max, memory.max, memory.min from within the container.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max
In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and optimizing resource usage. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.While Googleâ€™s Kube Startup CPU Boost example is just a specific use case scenario, ScaleOps provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.]]></content:encoded></item><item><title>I created a CLI trash command</title><link>https://github.com/Maxsafer/trash-tool</link><author>/u/lavishclassman</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 10:23:55 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Its a less than 400 lines CLI trash manager :) made it for personal use and for fun.   submitted by    /u/lavishclassman ]]></content:encoded></item><item><title>Proj Ideas ðŸ’¡ - Willing to lock in for Go (2025)</title><link>https://www.reddit.com/r/golang/comments/1iqp4re/proj_ideas_willing_to_lock_in_for_go_2025/</link><author>/u/ComfortableAcadia839</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 10:04:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm a full stack JS/TS developer but just recently tried Go, built an in memory key-value Redis clone.. I've realised the language makes me enjoy coding ---> Can y'all recommend some project ideas (intermediate to advanced difficulty)I want to build some solid projects ;)]]></content:encoded></item><item><title>I created a command line SSH tunnel manager to learn Go</title><link>https://github.com/alebeck/boring</link><author>/u/Savings-Square572</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 09:56:56 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>NASA has a list of 10 rules for software development</title><link>https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:07:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[NASA has a list of 10 rules for software developmentThose rules were written from the point of view of people writing
embedded software for extremely expensive spacecraft, where tolerating
a lot of programming pain is a good tradeoff for not losing a mission.
I do not know why someone in that situation does not use the SPARK
subset of Ada, which subset was explicitly designed for verification,
and is simply a better starting point for embedded programming than C.
I am criticising them from the point of view of people writing
programming language processors (compilers, interpreters, editors)
and application software.
We are supposed to teach critical thinking.  This is an example.
How have Gerard J. Holzmann's and my different contexts affected
our judgement?
Can you blindly follow his advice without considering 
context?
Can you blindly follow  advice without considering
your context?
Would these rules necessarily apply to a different/better
programming language?  What if function pointers
were tamed?  What if the language provided opaque abstract
data types as Ada does?
1. Restrict all code to very simple control flow constructs â€”
do not use  statements,
 or  constructs,
and direct or indirect .Note that  and 
are how C does exception handling, so this rule bans any use
of exception handling.

It is true that banning recursion and jumps and loops without
explicit bounds means that you  your program is
going to terminate.  It is also true that recursive functions
can be proven to terminate about as often as loops can, with
reasonably well-understood methods.  What's more important here is
that â€œsure to terminateâ€ does not imply
â€œsure to terminate in my lifetimeâ€:
    int const N = 1000000000;
    for (x0 = 0; x0 != N; x0++)
    for (x1 = 0; x1 != N; x1++)
    for (x2 = 0; x2 != N; x2++)
    for (x3 = 0; x3 != N; x3++)
    for (x4 = 0; x4 != N; x4++)
    for (x5 = 0; x5 != N; x5++)
    for (x6 = 0; x6 != N; x6++)
    for (x7 = 0; x7 != N; x7++)
    for (x8 = 0; x8 != N; x8++)
    for (x9 = 0; x9 != N; x9++)
        -- do something --;
This does a bounded number of iterations.  The bound is N.
In this case, that's 10.  If each iteration of the loop body
takes 1 nsec, that's 10 seconds, or about 7.9Ã—10
years.  What is the  difference between â€œwill stop
in 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
yearsâ€ and â€œwill never stopâ€?

Worse still, taking a problem that is  expressed
using recursion and contorting it into something that manipulates an
explicit stack, while possible, turns clear maintainable code into
buggy spaghetti.  (I've done it, several times.  There's an example
on this web site.  It is  a good idea.)

2. All loops must have a fixed upper-bound.  It must be trivially
possible for a checking tool to prove statically that a preset
upper-bound on the number of iterations of a loop cannot be exceeded.
If the loop-bound cannot be proven statically, the rule is considered
violated.This is an old idea.  As the example above shows, it is not enough
by itself to be of any practical use.  You have to try to make the
bounds reasonably , and you have to regard hitting an
artificial bound as a run-time error.

By the way, note that putting depth bounds on recursive procedures
makes them every bit as safe as loops with fixed bounds.

3. Do not use dynamic memory allocation after initialization.This is also a very old idea.  Some languages designed for embedded
work don't even  dynamic memory allocation.  The big
thing, of course, is that embedded applications have a fixed amount of
memory to work with, are never going to get any more, and should not
crash because they couldn't handle another record.

Note that the rationale actually supports a much stronger rule:
don't even  dynamic memory allocation.  You can of
course manage your own storage pool:
    typedef struct Foo_Record *foo;
    struct Foo_Record {
	foo next;
	...
    };
    #define MAX_FOOS ...
    static struct Foo_Record foo_zone[MAX_FOOS];
    foo foo_free_list = 0;

    void init_foo_free_list() {
	for (int i = MAX_FOOS - 1; i >= 0; i--) {
	    foo_zone[i].next = foo_free_list;
	    foo_free_list = &foo_zone[i];
	}
    }

    foo malloc_foo() {
	foo r = foo_free_list;
	if (r == 0) report_error();
	foo_free_list = r->next;
	return r;
    }

    void free_foo(foo x) {
	x->next = foo_free_list;
	foo_free_list = x;
    }
This  satisfies the rule, but it
violates the  of the rule.  Simulating malloc()
and free() this way is  than using the real
thing, because the memory in foo_zone is permanently tied up
for Foo_Records, even if we don't need any of those at the
moment but do desperately need the memory for something else.

What you really need to do is to use a memory allocator
with known behaviour, and to prove that the amount of memory
in use at any given time (data bytes + headers) is bounded
by a known value.

Note also that SPlint can verify at compile time that
the errors NASA speak of do not occur.

One of the reasons given for the ban is that the performance
of malloc() and free() is unpredictable.  Are these the only
functions we use with unpredictable performance?  Is there
anything about malloc() and free() which makes them
 unpredictable?  The existence of
hard-real-time garbage collectors suggests not.

The rationale for this rule says that

Note that the only way
to dynamically claim memory in the absence of memory allocation from the
heap is to use stack memory.  In the absence of recursion (Rule 1), an
upper bound on the use of stack memory can derived statically, thus
making it possible to prove that an application will always live within
its pre-allocated memory means.
Unfortunately, the sunny optimism shown here is unjustified.  Given
the ISO C standard (any version, C89, C99, or C11) it is 
to determine an upper bound on the use of stack memory.  There is not even
any standard way to determine how much memory a compiler will use for the
stack frame of a given function.  (There could have been.  There just isn't.)
There isn't even any requirement that two invocations of the same function
with the same arguments will use the same amount of memory.
Such a bound can only be calculated for a  version of a
specific compiler with specific options.  Here's a trivial example:
void f() {
    char a[100000];
}
How much memory will that take on the stack?  Compiled for debugging,
it might take a full stack frame (however big that is) plus traceback
information plus a million bytes for a[].  Compiled with optimisation,
the compiler might notice that a[] isn't used, and might even compile
calls to f() inline so that they generate no code and take no space.
That's an extreme example, but not really unfair.  If you want bounds
you can rely on, you had better  what your compiler does,
and recheck every time anything about the compiler changes.

4.  No function should be longer than what can be printed on
a single sheet of paper in a standard reference format with one line per
statement and one line per declaration.  Typically, this means no more
than about 60 lines of code per function.Since programmers these days typically read their code on-screen,
not on paper, it's not clear why the size of a sheet of paper is
relevant any longer.

The rule is arguably stated about the wrong thing.  The thing that
needs to be bounded is not the size of a function, but the size of a
chunk that a programmer needs to read and comprehend.

There are also question marks about how to interpret this if you
are using a sensible language (like Algol 60, Simula 67, Algol 68,
Pascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,
F#, Clean, Haskell, or Fortran) that allows nested procedures.
Suppose you have a folding editor that presents a procedure to
you like this:
function Text_To_Floating(S: string, E: integer): Double;
   ï¿½ variables ï¿½
   ï¿½ procedure Mul(Carry: integer) ï¿½
   ï¿½ function Evaluate: Double ï¿½

   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;
   for N := 1 to S.length do begin
       C := S[N];
       if C = '.' then begin
          Point := -1
       end else
       if C = '_' then begin
          Base := Round(Evaluate);
          Max, Min, Power := 0, 1, 0
       end else
       if Char â‰  ' ' then begin
          Q := ord(C) - ord('0');
          if Q > 9 then Q := ord(C) - ord('A') + 10
          Power := Point + Point
          Mul(Q)
       end
    end;
    Power := Power + Exp;
    Value := Evaluate;
    if Sign < 0 then Value := -Value;
end;
which would be much bigger if the declarations
were expanded out instead of being hidden behind ï¿½foldsï¿½.
Which size do we count?  The folded size or the unfolded size?
I was using a folding editor called Apprentice on the Classic Mac
back in the 1980s.  It was written by Peter McInerny and was lightning
fast.

5.  The  of the code should average to a minimum of
two assertions per function.Assertions are wonderful documentation and the very best debugging tool
I know of.  I have never seen any real code that had too many assertions.

The example here is one of the ugliest pieces of code I've seen in a while.
if (!c_assert(p >= 0) == true) {
    return ERROR;
}
It should, of course, just be
if (!c_assert(p >= 0)) {
    return ERROR;
}
Better still, it should be something like
#ifdef NDEBUG
#define check(e, c) (void)0
#else
#define check(e, c) if (!(c)) return bugout(c), (e)
#ifdef NDEBUG_LOG
#define bugout(c) (void)0
#else
#define bugout(c) \
    fprintf(stderr, "%s:%d: assertion '%s' failed.\n", \
    __FILE__, __LINE__, #s)
#endif
#endif
Ahem.  The more interesting part is the required density.
I just checked an open source project from a large telecoms
company, and 23 out of 704 files (not functions) contained
at least one assertion.  I just checked my own Smalltalk
system and one SLOC out of every 43 was an assertion, but
the average Smalltalk â€œfunctionâ€ is only a few
lines.  If the biggest function allowed is 60 lines, then
let's suppose the average function is about 36 lines, so
this rule requires 1 assertion per 18 lines.
Assertions are good, but what they are especially good
for is expressing the requirements on data that come
from outside the function.  I suggest then that
Every argument whose validity is not guaranteed by
its typed should have an assertion to check it.
Every datum that is obtained from an external
source (file, data base, message) whose validity is
not guaranteed by its type should have an assertion
to check it.
The NASA 10 rules are written for embedded systems, where
reading stuff from sensors is fairly common.

6.  Data objects must be declared at the smallest possible level of
scope.This is excellent advice, but why limit it to data objects?
Oh yeah, the rules were written for crippled languages where you
 declare functions in the right place.

People using Ada, Pascal (Delphi), JavaScript, or functional
languages should also declare types and functions as locally as
possible.

7.  The return value of non-void functions must be checked by each
calling function, and the validity of parameters must be checked inside
each function.This again is mainly about C, or any other language that indicates
failure by returning special values.  â€œStandard libraries
famously violate this ruleâ€?  No, the  library does.

You have to be reasonable about this: it simply isn't practical
to check  aspect of validity for 
argument.  Take the C function
void *bsearch(
    void const *key  /* what we are looking for */,
    void const *base /* points to an array of things like that */,
    size_t      n    /* how many elements base has */,
    size_t      size /* the common size of key and base's elements */
    int (*      cmp)(void const *, void const *)
);
This does a binary search in an array.  We must have keyâ‰ 0,
baseâ‰ 0, sizeâ‰ 0, cmpâ‰ 0, cmp(key,key)=0, and for all
1<i<n,
cmp((char*)base+size*(i-1), (char*)base+size*i) <= 0
Checking the validity in full would mean checking
that [key..key+size) is a range of readable addresses,
[base..base+size*n) is a range of readable addresses,
and doing n calls to cmp.  But the whole point of binary
search is to do O(log(n)) calls to cmp.

The fundamental rules here are
Don't let run-time errors go un-noticed, and
any check is safer than no check.
8. The use of the preprocessor must be limited to the inclusion of
header files and simple macro definitions.  Token pasting, variable
argument lists (ellipses), and recursive macro calls are not allowed.Recursive macro calls don't really work in C, so no quarrel there.
Variable argument lists were introduced into macros in
C99 so that you could write code like
#define err_printf(level, ...) \
    if (debug_level >= level) fprintf(stderr, __VA_ARGS__)
...
    err_printf(HIGH, "About to frob %d\n", control_index);
This is a  thing; conditional tracing like this is a
powerful debugging aid.  It should be , not banned.

The rule goes on to ban macros that expand into things that are
not complete syntactic units.  This would, for example, prohibit
simulating try-catch blocks with macros.  (Fair enough, an earlier rule
banned exception handling anyway.)  Consider this code fragment, from
an actual program.
    row_flag = border;     
    if (row_flag) printf("\\hline");
    for_each_element_child(e0, i, j, e1)
        printf(row_flag ? "\\\\\n" : "\n");
        row_flag = true;  
        col_flag = false;
        for_each_element_child(e1, k, l, e2)
            if (col_flag) printf(" & ");
            col_flag = true;
            walk_paragraph("", e2, "");
        end_each_element_child
    end_each_element_child
    if (border) printf("\\\\\\hline");
    printf("\n\\end{tabular}\n");
It's part of a program converting slides written in something like HTML
into another notation for formatting.  The 
â€¦  loops walk over a tree.  Using
these macros means that the programmer has no need to know and no reason to
care how the tree is represented and how the loop actually works.
You can easily see that  must have at
least one unmatched { and  must have at least one
unmatched }.  That's the kind of macro that's banned by requiring
complete syntactic units.  Yet the readability and maintainability of
the code is  improved by these macros.

One thing the rule covers, but does not at the beginning stress, is
â€œno  macro processingâ€.  That is,
no #if.  The argument against it is, I'm afraid, questionable.  If there
are 10 conditions, there are 2 combinations to test,
whether they are expressed as compile-time conditionals or run-time
conditionals.

In particular, the rule against conditional macro processing
would prevent you defining your own assertion macros.
It is not obvious that that's a good idea.

9.  The use of pointers should be restricted.  Specifically, no more
than one level of dereferencing is allowed.  Pointer dereference
operations may not be hidden in macro definitions or inside typedef
declarations.  Function pointers are not permitted.Let's look at the last point first.

double integral(double (*f)(double), double lower, double upper, int n) {
    // Compute the integral of f from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += f((lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += f(lower + h*i);
    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);
}
This kind of code has been important in numerical calculations since
the very earliest days.  Pascal could do it.  Algol 60 could do it.
In the 1950s, Fortran could do it.  And NASA would ban it, because in
C,  is a function pointer.

Now it's important to write functions like this once and only once.
For example, the code has at least one error.  The comment says n+1
points, but the function is actually evaluated at 2n+1 points.  If we
need to bound the number of calls to f in order to meet a deadline,
having that number off by a factor of two will not help.
It's nice to have just one place to fix.
Perhaps I should not have copied that code from a well-known source (:-).
Certainly I should not have more than one copy!

What can we do if we're not allowed to use function pointers?
Suppose there are four functions foo, bar, ugh, and zoo that we need
to integrate.  Now we can write
enum Fun {FOO, BAR, UGH, ZOO};

double call(enum Fun which, double what) {
    switch (which) {
        case FOO: return foo(what);
        case BAR: return bar(what);
        case UGH: return ugh(what);
        case ZOO: return zoo(what);
    }
}

double integral(enum Fun which, double lower, double upper, int n) {
    // Compute the integral of a function from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += call(which, (lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += call(which, lower + h*i);
    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);
}
Has obeying NASA's rule made the code more reliable?  No, it has made
the code  to understand,  maintainable, and
 that it wasn't before.  Here's a call
illustrating the mistake:
x = integral(4, 0.0, 1.0, 10);I have checked this with two C compilers and a static checker at their
highest settings, and they are completely silent about this.

So there are legitimate uses for function pointers, and simulating
them makes programs , not better.

Now  in Fortran,
Algol 60, or Pascal.  Those languages had procedure 
but not procedure . You could pass a subprogram name as
a parameter, and such a parameter could be passed on, but you could not
store them in variables.  You could have a  of C which
allowed function pointer parameters, but made all function pointer
variables read-only.  That would give you a statically checkable subset
of C that allowed integral().

The other use of function pointers is simulating object-orientation.
Imagine for example
struct Channel {
    void (*send)(struct Channel *, Message const *);
    bool (*recv)(struct Channel *, Message *);
    ...
};
inline void send(struct Channel *c, Message const *m) {
    c->send(c, m);
}
inline bool recv(struct Channel *c, Message *m) {
    return c->recv(c, m);
}
This lets us use a common interface for sending and receiving
messages on different kinds of channels.  This approach has been
used extensively in operating systems (at least as far back as
the Burroughs MCP in the 1960s) to decouple the code that uses
a device from the actual device driver.     I would expect any
program that controls more than one hardware device to do something
like this.  It's one of our key tools for controlling complexity.
Again, we can simulate this, but it makes adding a new kind of
channel harder than it should be, and the code is 
when we do it, not better.

The rule against more than one level of dereferencing is also
an assault on good programming.  One of the key ideas that was
developed in the 1960s is the idea of ;
the idea that it should be possible for one module to define a
data type and operations on it and another module to use instances
of that data type and its operations without having to know
anything about what the data type is.
One of the things I detest about Java is that it spits in the
face of the people who worked out that idea.  Yes, Java (now) has
generic type parameters, and that's good, but you cannot use a
 type without knowing what that type is.

Suppose I have a module that offers operations
And suppose that I have two interfaces in mind.  One of them
uses integers as tokens.
// stasher.h, version 1.
typedef int token;
extern token stash(item);
extern item  recall(token);
extern void  delete(token);
Another uses pointers as tokens.
// stasher.h, version 2.
typedef struct Hidden *token;
extern  token stash(item);
extern  item  recall(token);
extern  void  delete(token);
void snoo(token *ans, item x, item y) {
    if (better(x, y)) {
	*ans = stash(x);
    } else {
	*ans = stash(y);
    }
}
By the NASA rule, the function snoo() would not be accepted or rejected on
its own merits.  With stasher.h, version 1, it would be accepted.
With stasher.h, version 2, it would be rejected.

One reason to prefer version 2 to version 1 is that version 2 gets
more use out of type checking.  There are ever so many ways to get an
int in C.  Ask yourself if it ever makes sense to do
token t1 = stash(x);
token t2 = stash(y);
delete(t1*t2);
I really do not like the idea of banning abstract data types.

10.  All code must be compiled, from the first day of development,
with all compiler warnings enabled at the compilerâ€™s
most pedantic setting.  All code must compile with these setting without
any warnings.  All code must be checked daily with at least one, but
preferably more than one, state-of-the-art static source code analyzer
and should pass the analyses with zero warnings.This one is good advice.  Rule 9 is really about making your code
worse in order to get more benefit from limited static checkers.  (Since
C has no standard way to construct new functions at run time, the set of
functions that a particular function pointer  point to can
be determined by a fixed-point data flow analysis, at least for most
programs.)  So is rule 1.  



]]></content:encoded></item><item><title>Resigning as Asahi Linux project lead</title><link>https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:01:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them â€œjailbreaksâ€ these days) to allow users to run their own unofficial apps on the Nintendo Wii.I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other peopleâ€™s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didnâ€™t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which Iâ€™ve already spoken at length about and I wonâ€™t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience. (Clarification: This has nothing to do with Rust at this point, itâ€™s well before R4L was even merged. Upstreaming to Linux is a terrible experience in C too.)But then also came the entitled users. This time, it wasnâ€™t about stealing games, it was about features. â€œWhen is Thunderbolt coming?â€ â€œAsahi is useless to me until I can use monitors over USB-Câ€ â€œThe battery life sucks compared to macOSâ€ (nobody ever complained when compared to x86 laptopsâ€¦) â€œI canâ€™t even check my CPU temperatureâ€ (yes, I seriously got that one). (Edit: This wasnâ€™t just a few instances; Iâ€™ve seen variations on the first three posted hundreds of times by now, including takes like â€œThunderbolt/DP Alt are never going to happenâ€. A few times is fine, but the same thing repeated over and over again every day while weâ€™re trying to make these things happen will get to anyone.)And, of course, â€œWhen is M3/M4 support coming?â€For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were â€œalphaâ€ and â€œunstableâ€ and â€œnot suitable for a daily driverâ€ (despite thousands of users, myself included, daily driving it and even using it for servers).No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I wonâ€™t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).I did make some progress in 2024, but this left me in a very vulnerable position. I hadnâ€™t gotten nearly as much Asahi work done as Iâ€™d liked, and the users werenâ€™t getting any quieter about demanding more features and machine support.We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. â€œFullâ€ DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasnâ€™t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did try to bring the subject up on the mailing lists, but it didnâ€™t get much response).The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I wonâ€™t repeat them in detail here. Suffice it to say, I consider Linusâ€™ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.As you know, this is deeply personal to me, as weâ€™ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.Some might be wondering why we canâ€™t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But thereâ€™s more.In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (thatâ€™s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a partial workaround for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.Iâ€™m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated â€œWe
are the â€˜thin blue lineâ€™â€, and nobody cared, which just further confirmed to me that I donâ€™t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to quit.But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than bunny.net giving us free CDN credits for the hosting).I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say Iâ€™m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they wonâ€™t touch the Linux kernel with a 10-foot pole.When Apple released the M1, Linus Torvalds wished it could run Linux, but didnâ€™t have much hope it would ever happen. We made it happen, and Linux 5.19 was released from an M2 MacBook Air running Asahi Linux. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him an invitation to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.Back in 2011, Con Kolivas left the Linux kernel community. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if youâ€™re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but itâ€™s not fun any more. I have an M3 Pro in a box and I havenâ€™t even turned it on yet. I dread doing the bring-up work. It doesnâ€™t feel worth the trouble.I miss having free time where I can relax and not worry about the features we havenâ€™t shipped yet. I miss making music. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we havenâ€™t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.Iâ€™m resigning as lead of the Asahi Linux project, effective immediately. The project will continue on without me, and Iâ€™m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the Asahi Linux OpenCollective (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).I want to thank the entire Asahi Linux team, without whom I wouldâ€™ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: marcan@marcan.st.: A lot of the discussion around this post and the interactions that led to it brings up the term â€œbrigadingâ€. Please read this excellent Fedi post for a discussion of what is and isnâ€™t brigading.]]></content:encoded></item><item><title>The IRS Is Buying an AI Supercomputer From Nvidia</title><link>https://theintercept.com/2025/02/14/irs-ai-nvidia-tax/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 08:50:22 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[ administration and its cadre of Silicon Valley machine-learning evangelists attempt to restructure the administrative state, the IRS is preparing to purchase advanced artificial intelligence hardware, according to procurement materials reviewed by The Intercept.The hardware has not yet been purchased and installed, nor is a price listed, but SuperPod systems reportedly start at $7 million. The setup described in the contract materials notes that it will include a substantial memory upgrade from Nvidia.Though small compared to the massive AI-training data centers deployed by companies like OpenAI and Meta, the SuperPod is still a powerful and expensive setup using the most advanced technology offered by Nvidia, whose chips have facilitated the global machine-learning spree. While the hardware can be used in many ways, itâ€™s marketed as a turnkey means of creating and querying an AI model. Last year, the MITRE Corporation, a federally funded military R&D lab, acquired a $20 million SuperPod setup to train bespoke AI models for use by government agencies, touting the purchase as a â€œmassive increase in computing powerâ€ for the United States.How exactly the IRS will use its SuperPod is unclear. An agency spokesperson said the IRS had no information to share on the supercomputer purchase, including which presidential administration ordered it. A 2024 report by the Treasury Inspector General for Tax Administration identified 68 different AI-related projects underway at the IRS; the Nvidia cluster is not named among them, though many were redacted.But some clues can be gleaned from the purchase materials. â€œThe IRS requires a robust and scalable infrastructure that can handle complex machine learning (ML) workloads,â€ the document explains. â€œThe Nvidia Super Pod is a critical component of this infrastructure, providing the necessary compute power, storage, and networking capabilities to support the development and deployment of large-scale ML models.â€The document notes that the SuperPod will be run by the IRS Research, Applied Analytics, and Statistics division, or RAAS, which leads a variety of data-centric initiatives at the agency. While no specific uses are cited, it states that this divisionâ€™s Compliance Data Warehouse project, which is behind this SuperPod purchase, has previously used machine learning for automated fraud detection, identity theft prevention, and generally gaining a â€œdeeper understanding of the mechanisms that drive taxpayer behavior.â€â€œThe IRS has probably more proprietary data than most agencies that is totally untapped.â€Itâ€™s unclear from the document whether the SuperPod purchase had been planned under the Biden administration or if it represents a new initiative of the Trump administration.Some funding from the 2022 Inflation Reduction ActÂ was earmarked for upgrading IRS technology generally, said Travis Thompson, a tax attorney with Boutin Jones with an expertise in IRS AI strategy. But â€œthe IRS has been going toward AI for quite some time prior to IRA funding,â€ Thompson explained. â€œThey didnâ€™t have enough money to properly enforce the tax code, they were looking for ways to do more with less.â€ A June 2024 Government Accountability Office report suggested the IRS use artificial intelligence-based software to retrieve â€œhundreds of billions of dollars [that] are potentially missing from what should be collected in taxes each year.â€Thompson added that the agency is ripe for machine-learning training because of the mountain of personal and financial data it sits atop. â€œThe IRS has probably more proprietary data than most agencies that is totally untapped. When you look at something like this Nvidia cluster and training machine learning algorithms going forward, it makes perfect sense, because they have the data there. AI needs data. It needs lots of it. And it needs it quickly. And the IRS has it.â€The purchase comes at a crossroads for U.S. governance of artificial intelligence tech. In Trumpâ€™s first term, the RAAS office was assigned â€œresponsibility for monitoring and overseeing AI at the IRSâ€ under Executive Order 13960, which he signed shortly before leaving office in 2020. This executive order put an emphasis on the â€œresponsible,â€ â€œsafeâ€ implementation of AI by the United States â€” an approach that has fallen out of favor by American tech barons who now advocate for the breakneck development of these technologies unburdened by consideration of ethics or risk. One of Trumpâ€™s first moves following his inauguration was reversing a Biden administration executive order calling for greater AI safety guardrails in government use.Many of theÂ AI industry for whom â€œsafe AIâ€ is now anathema have become close allies of the new Trump White House, such as Elon Musk and venture capitalist Marc Andreessen. This wing of Silicon Valley has reportedly pushed the new administration to leverage artificial intelligence to help dismantle the administrative state via automation.This week, the Wall Street Journal reported Muskâ€™s liquidators had arrived at the IRS, an agency long the target of disparagement and distortion by Trump and Republican allies. Days before, the New York Times reported, â€œRepresentatives from the so-called Department of Government Efficiency have sought information about the tax collectorâ€™s information technology, with a goal of automating more work to replace the need for human staff members.â€The IRS has in recent years increasingly turned to AI for automated fraud detection and chatbot-based support services â€” including through collaboration with Nvidia â€” but a new Nvidia supercomputer could also be a boon to those interested in shrinking the agencyâ€™s human headcount as much as possible. A February 8 report by the Washington Post quoted an unnamed federal official who described Muskâ€™s end goal as â€œreplacing the human workforce with machines,â€ and that â€œEverything that can be machine-automated will be. And the technocrats will replace the bureaucrats.â€Musk underlings are reportedly contemplating replacing humans at the Department of Education with a large language-based chatbot, as well.Wired previously reported that Musk loyalist Thomas Shedd, placed in a directorship within the General Services Administration, has talked of an â€œAI-firstâ€ agenda for Trumpâ€™s second term; DOGE staffers have already reportedly turned to Microsoftâ€™s Azure AI platform for advice on slashing programs. While the Nvidia SuperPod couldnâ€™t on its own replicate services like those provided by Microsoft, it is powerful enough to train AI models based on government data.Thompson told The Intercept that efforts to slash the federal workforce and more aggressively deploy artificial intelligence systems fit hand-in-glove.â€œI firmly believe that rooted behind the reduction in the human workforce that seems to be goal of current administration, thereâ€™s an overarching goal there to implement more technology-based systems in order to do the jobs,â€ he explained. â€œIf youâ€™re going to reduce your workforce, something has to pick up the slack. Something has to do the job.â€]]></content:encoded></item><item><title>Sunsetting Create React App</title><link>https://react.dev/blog/2025/02/14/sunsetting-create-react-app</link><author>/u/sadyetfly11</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 08:11:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Today, weâ€™re deprecating Create React App for new apps, and encouraging existing apps to migrate to a framework. Weâ€™re also providing docs for when a framework isnâ€™t a good fit for your project, or you prefer to start by building a framework.When we released Create React App in 2016, there was no clear way to build a new React app.To create a React app, you had to install a bunch of tools and wire them up together yourself to support basic features like JSX, linting, and hot reloading. This was very tricky to do correctly, so the communitycreatedboilerplates for commonsetups. However, boilerplates were difficult to update and fragmentation made it difficult for React to release new features.Create React App solved these problems by combining several tools into a single recommended configuration. This allowed apps a simple way to upgrade to new tooling features, and allowed the React team to deploy non-trivial tooling changes (Fast Refresh support, React Hooks lint rules) to the broadest possible audience.This model became so popular that thereâ€™s an entire category of tools working this way today.Deprecating Create React App Although Create React App makes it easy to get started, there are several limitations that make it difficult to build high performant production apps. In principle, we could solve these problems by essentially evolving it into a framework.However, since Create React App currently has no active maintainers, and there are many existing frameworks that solve these problems already, weâ€™ve decided to deprecate Create React App.Starting today, if you install a new app, you will see a deprecation warning:create-react-app is deprecated.

You can find a list of up-to-date React frameworks on react.dev
For more info see: react.dev/link/cra

This error message will only be shown once per install.We recommend creating new React apps with a framework. All the frameworks we recommend support client-only SPAs, and can be deployed to a CDN or static hosting service without a server.For existing apps, these guides will help you migrate to a client-only SPA:Create React App will continue working in maintenance mode, and weâ€™ve published a new version of Create React App to work with React 19.If your app has unusual constraints, or you prefer to solve these problems by building your own framework, or you just want to learn how react works from scratch, you can roll your own custom setup with React using Vite, Parcel or Rsbuild.We provide several Vite-based recommendations.React Router v7 is a Vite based framework which allows you to use Viteâ€™s fast development server and build tooling with a framework that provides routing and data fetching. Just like the other frameworks we recommend, you can build a SPA with React Router v7.Just like Svelte has Sveltekit, Vue has Nuxt, and Solid has SolidStart, React recommends using a framework that integrates with build tools like Vite for new projects.Limitations of Create React App Create React App and build tools like it make it easy to get started building a React app. After running npx create-react-app my-app, you get a fully configured React app with a development server, linting, and a production build.For example, if youâ€™re building an internal admin tool, you can start with a landing page:Welcome to the Admin Tool!This allows you to immediately start coding in React with features like JSX, default linting rules, and a bundler to run in both development and production. However, this setup is missing the tools you need to build a real production app.Most production apps need solutions to problems like routing, data fetching, and code splitting.Create React App does not include a specific routing solution. If youâ€™re just getting started, one option is to use  to switch between routes. But doing this means that you canâ€™t share links to your app - every link would go to the same page - and structuring your app becomes difficult over time: =  ===  &&  ===  && This is why most apps that use Create React App solve add routing with a routing library like React Router or Tanstack Router. With a routing library, you can add additional routes to the app, which provides opinions on the structure of your app, and allows you to start sharing links to routes. For example, with React Router you can define routes: = =With this change, you can share a link to  and the app will navigate to the dashboard page . Once you have a routing library, you can add additional features like nested routes, route guards, and route transitions, which are difficult to implement without a routing library.Thereâ€™s a tradeoff being made here: the routing library adds complexity to the app, but it also adds features that are difficult to implement without it.Another common problem in Create React App is data fetching. Create React App does not include a specific data fetching solution. If youâ€™re just getting started, a common option is to use  in an effect to load data.But doing this means that the data is fetched after the component renders, which can cause network waterfalls. Network waterfalls are caused by fetching data when your app renders instead of in parallel while the code is downloading: =       ..      ..=..Fetching in an effect means the user has to wait longer to see the content, even though the data could have been fetched earlier. To solve this, you can use a data fetching library like React Query, SWR, Apollo, or Relay which provide options to prefetch data so the request is started before the component renders.These libraries work best when integrated with your routing â€œloaderâ€ pattern to specify data dependencies at the route level, which allows the router to optimize your data fetches: =  = ..=..On initial load, the router can fetch the data immediately before the route is rendered. As the user navigates around the app, the router is able to fetch both the data and the route at the same time, parallelizing the fetches. This reduces the time it takes to see the content on the screen, and can improve the user experience.However, this requires correctly configuring the loaders in your app and trades off complexity for performance.Another common problem in Create React App is code splitting. Create React App does not include a specific code splitting solution. If youâ€™re just getting started, you might not consider code splitting at all.This means your app is shipped as a single bundle:But for ideal performance, you should â€œsplitâ€ your code into separate bundles so the user only needs to download what they need. This decreases the time the user needs to wait to load your app, by only downloading the code they need to see the page they are on.One way to do code-splitting is with . However, this means that the code is not fetched until the component renders, which can cause network waterfalls. A more optimal solution is to use a router feature that fetches the code in parallel while the code is downloading. For example, React Router provides a  option to specify that a route should be code split and optimize when it is loaded: = Optimized code-splitting is tricky to get right, and itâ€™s easy to make mistakes that can cause the user to download more code than they need. It works best when integrated with your router and data loading solutions to maximize caching, parallelize fetches, and support â€œimport on interactionâ€ patterns.These are just a few examples of the limitations of Create React App.Once youâ€™ve integrated routing, data-fetching, and code splitting, you now also need to consider pending states, navigation interruptions, error messages to the user, and revalidation of the data. There are entire categories of problems that users need to solve like:Solving each of these problems individually in Create React App can be difficult as each problem is interconnected with the others and can require deep expertise in problem areas users may not be familiar with. In order to solve these problems, users end up building their own bespoke solutions on top of Create React App, which was the problem Create React App originally tried to solve.Why we Recommend Frameworks Although you could solve all these pieces yourself in a build tool like Create React App, Vite, or Parcel, it is hard to do well. Just like when Create React App itself integrated several build tools together, you need a tool to integrate all of these features together to provide the best experience to users.This category of tools that integrates build tools, rendering, routing, data fetching, and code splitting are known as â€œframeworksâ€ â€” or if you prefer to call React itself a framework, you might call them â€œmetaframeworksâ€.Frameworks impose some opinions about structuring your app in order to provide a much better user experience, in the same way build tools impose some opinions to make tooling easier. This is why we started recommending frameworks like Next.js, React Router, and Expo for new projects.Frameworks provide the same getting started experience as Create React App, but also provide solutions to problems users need to solve anyway in real production apps.Server Rendering is not just for SEO A common misunderstanding is that server rendering is only for SEO.While server rendering can improve SEO, it also improves performance by reducing the amount of JavaScript the user needs to download and parse before they can see the content on the screen.This is why the Chrome team has encouraged developers to consider static or server-side render over a full client-side approach to achieve the best possible performance.]]></content:encoded></item><item><title>Which approach to rust is more idiomatic (Helix vs Zed)?</title><link>https://www.reddit.com/r/rust/comments/1iqnats/which_approach_to_rust_is_more_idiomatic_helix_vs/</link><author>/u/No_Penalty2781</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 07:50:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi! I am curious what is the current "meta" (by "meta" I mean the current rust's community  and  way of doing things) of rust programming. I am studying source code of 2 editors I am using: Helix and Zed. And I can see that while they are doing a lot of similar things (like using LSP and parsing it outputs for example) the code is kinda different.It starts from the file structure: in Helix there are not that many folders to look at (like you have helix-core which contains features like "diagnostic", "diff", "history", etc but in Zed every single one of them is a different crate , which approach is more "idiomatic"? To divide every feature as a separate crate or to use more "packed" crates like "core".Then the code itself is kinda different, for example I am currently looking at LSP implementation in both of them and in Helix's case I can follow along and understand the code much more easily (here is the file I am referring to. But in Zed's case it is kinda hard to understand the code because of "type level programming" stuff like this one for example. It also doesn't help that files have a lot of SLOC in them (over 1500 in normal in Zed's repository, is it also how you do rust?) Maybe I am just used to lean functions from other languages (I mainly did TypeScript and Elixir in my career).Other thing I see is that Helix has more comments about "why the thing is doing that in the first place" which I find very helpful (on the other hand in seems that Zed's is abusing a lot of "type level" programming to have a self-documented code but it is harder to reason about at least for me) which approach here you prefer?]]></content:encoded></item><item><title>[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation</title><link>https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:55:36 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methodsMain findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning typesI think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.]]></content:encoded></item><item><title>AI Replaces Boyfriends In China, Making Entrepreneur Yao Runhao A Billionaire</title><link>https://observervoice.com/the-rise-of-ai-boyfriends-in-china-96739/</link><author>/u/Curious_Suchit</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:37:47 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Artificial intelligence (AI) is transforming many aspects of our lives, including how we approach relationships. In China, a new trend has emerged where virtual boyfriends powered by AI are becoming increasingly popular. This phenomenon addresses common relationship issues, such as communication gaps and emotional support, that many individuals face. As technology continues to evolve, the demand for these virtual companions is growing, leading to significant financial success for their creators. This article explores the rise of AI boyfriends, their impact on users, and the entrepreneurial success behind this innovative solution.Understanding the Appeal of AI BoyfriendsThe appeal of AI boyfriends lies in their ability to provide companionship without the complexities of real-life relationships. Many individuals, particularly women, often experience frustration when their partners fail to respond promptly to messages or show disinterest in their daily lives. In contrast, AI boyfriends like Li Shen, known as Zayne, offer immediate responses, attentive listening, and tailored interactions. This creates a sense of connection that users find comforting and fulfilling.Alicia Wang, a 32-year-old editor from Shanghai, exemplifies this trend. She has found solace in her virtual relationship with Zayne, who is always available to listen and engage. Wangâ€™s experience is not unique; millions of others are turning to AI companions for emotional support. The game â€œLove and Deepspace,â€ which features these AI boyfriends, has attracted an estimated six million monthly active players. This indicates a significant shift in how people perceive relationships and companionship in the digital age.The convenience of AI boyfriends allows users to escape the pressures of traditional dating. They can interact with their virtual partners at any time, without the fear of rejection or misunderstanding. This dynamic appeals to those who may struggle with social interactions or who simply seek a more manageable form of companionship. As technology continues to advance, the potential for AI to fulfill emotional needs will likely expand, further solidifying its place in modern relationships.The Success of â€œLove and Deepspaceâ€â€œLove and Deepspace,â€ developed by Shanghai-based Paper Games, has become a cultural phenomenon since its launch in January 2024. The game utilizes AI and voice recognition technology to create engaging interactions between players and their virtual boyfriends. Players can unlock new gameplay features and interactions by making in-game purchases, which has contributed to the gameâ€™s financial success.The gameâ€™s creator, Yao Runhao, has seen his wealth soar to an estimated $1.3 billion, thanks to the popularity of â€œLove and Deepspace.â€ This success story highlights the growing market for AI-driven entertainment and companionship. Paper Games, established in 2013, has generated around $850 million in sales worldwide, showcasing the potential for significant revenue in the gaming industry.The gameâ€™s popularity extends beyond China, attracting players from the United States and other countries. This international appeal demonstrates the universal desire for connection and companionship, regardless of cultural differences. As more people seek out virtual relationships, the demand for innovative gaming experiences will likely continue to rise, paving the way for future developments in AI technology.The Financial Impact of AI CompanionshipThe financial implications of AI companionship are profound. As players invest in games like â€œLove and Deepspace,â€ the revenue generated contributes to the overall growth of the gaming industry. The success of Paper Games serves as a testament to the lucrative potential of combining technology with emotional engagement. Analysts estimate that the companyâ€™s valuation could reach over $2 billion, based on its annual revenue and market trends.Players like Alicia Wang are willing to spend significant amounts on their virtual relationships. Wang has reportedly invested around 35,000 yuan (approximately $4,800) to enhance her interactions with Zayne. This willingness to pay for virtual companionship underscores the emotional value that users derive from these experiences. As AI technology continues to improve, the potential for monetization in this sector will likely expand, attracting more entrepreneurs and investors.The rise of AI boyfriends also raises questions about the future of human relationships. As technology fills emotional gaps, society may need to reconsider the nature of companionship and intimacy. While AI can provide immediate support and engagement, it cannot replace the depth of human connection. Nonetheless, the financial success of AI companionship indicates a significant shift in how people approach relationships in the digital age.The Future of AI in RelationshipsThe emergence of AI boyfriends in China represents a significant shift in how individuals seek companionship. As technology continues to evolve, the demand for virtual relationships is likely to grow. While AI can provide immediate emotional support and engagement, it is essential to recognize the limitations of these interactions. The success of games like â€œLove and Deepspaceâ€ highlights the potential for innovation in the gaming industry and the growing market for AI-driven companionship.As society navigates this new landscape, it will be crucial to balance the benefits of AI with the importance of genuine human connections. The future of relationships may involve a blend of both, where technology enhances emotional experiences without replacing the depth of human interaction. The rise of AI boyfriends is just the beginning of a new era in companionship, one that will continue to evolve as technology advances.]]></content:encoded></item><item><title>I created a telegram bot with dynamic form builder</title><link>https://github.com/MeowSaiGithub/tg-form-builder</link><author>/u/Altruistic_Let_8036</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 06:33:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CanSat: A tiny, can-sized, Raspberry Pi-powered satellite</title><link>https://www.raspberrypi.com/news/cansat-a-tiny-can-sized-raspberry-pi-powered-satellite/</link><author>/u/Content-Complaint-98</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:30:21 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The effectiveness of irqbalance</title><link>https://www.reddit.com/r/linux/comments/1iqlqy0/the_effectiveness_of_irqbalance/</link><author>/u/D_Dave</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:03:09 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[During the years I've tried many times irqbalance: is recommended as improvement by various distros.But honestly I didn't noticed any improvement with irqbalance, but also nor neagative effects. I also modified the config file , by editing the line  in various modalities. By various research, I've read that is mostly recommended for servers. What I've done instead, has been to add the kernel parameter  in grub, taken from here:https://docs.kernel.org/admin-guide/kernel-parameters.htmlSo: what do you think about irq balancing, done by irqbalance or done by the kernel itself?]]></content:encoded></item><item><title>Perplexity uses Deepseek-R1 to offer Deep Research 10 times cheaper than OpenAI - Matthias Bastian</title><link>https://the-decoder.com/perplexity-uses-deepseek-r1-to-offer-deep-research-10-times-cheaper-than-openai/</link><author>/u/Altruistic_Age5645</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 03:25:28 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Perplexity has launched its version of Deep Research, joining Google and OpenAI in offering advanced AI-powered research capabilities.Perplexity says the new tool automatically conducts comprehensive research, performing dozens of searches and analyzing hundreds of sources to produce detailed reports in one to two minutes - a process that typically takes humans several hours.The system works through an iterative process: it searches for information, reads documents, and plans its next research steps based on what it finds. Users can export final reports as PDFs or share them via Perplexity Pages.The service launches first on web browsers, with iOS, Android, and Mac versions planned for later release. While basic access is free, daily query limits apply to non-subscribers. Perplexity says the tool works particularly well for finance, marketing, and technology research.Deepseek enables cheaper Deep ResearchPerplexity's Deepseek version of Deep Research scored 20.5 percent accuracy in "Humanity's Last Exam", a comprehensive AI benchmark with over 3,000 questions, placing it just behind OpenAI's Deep Research based on o3.Perplexity measures its own service with Internet knowledge against other models that only answer the questions with trained knowledge, and accordingly achieves significantly better results. For a company whose product has been criticized for being less than truthful, advertising that is less than truthful is not a confidence-building measure.Checking the AI's wall of textLike all so-called "answer engines" - with or without Deep Research - Perplexity generates falsehoods and inaccuracies in its reports. It is up to humans to verify and validate these results.The challenge is that these errors can be very subtle and hidden in large amounts of text, as in this example: Here LLM critic Gary Marcus is credited with a paper that refers to LLMs as "stochastic parrots". This is certainly in line with Marcus' beliefs, and he may have used the term before. But he did not write the paper.Perplexity does not respond to regular inquiries about whether error rates in AI responses are systematically studied and how high they are. Google, Microsoft, and OpenAI do not answer this question either.]]></content:encoded></item><item><title>[D] Self-Promotion Thread</title><link>https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/</link><author>/u/AutoModerator</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 03:15:29 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Please post your personal projects, startups, product placements, collaboration needs, blogs etc.Please mention the payment and pricing requirements for products and services.Please do not post link shorteners, link aggregator websites , or auto-subscribe links.Any abuse of trust will lead to bans.Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.]]></content:encoded></item><item><title>Fluvio: A Rust-powered streaming platform using WebAssembly for programmable data processing</title><link>https://www.reddit.com/r/rust/comments/1iqgg02/fluvio_a_rustpowered_streaming_platform_using/</link><author>/u/drc1728</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 01:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am in the process of writing an essay on composable streaming first architecture for data intensive applications. I am thinking of it as a follow up on this article.Quick question for the Rust community:What information would help the Rust community know and experience Fluvio?What would you like to see covered in the essay?   submitted by    /u/drc1728 ]]></content:encoded></item><item><title>Is it normal for clamav to false flag wine kernel32.dll?</title><link>https://www.reddit.com/r/linux/comments/1iqfveo/is_it_normal_for_clamav_to_false_flag_wine/</link><author>/u/OutrageousFarm9757</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 00:31:35 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I setup clamav just some hour ago using the arch wiki and have also gotten false positives in carla, xterm and uxterm. All these false positives are from the background scanner, or whatever it is called.Here you have my manual scan. I also reinstalled wine, oh and it flagged wine/mtree as containing credit card numbers...[user@system ~]$ clamscan /usr/lib/wine/x86_64-windows/kernel32.dll Loading: 16s, ETA: 0s [========================>] 8.70M/8.70M sigs Compiling: 3s, ETA: 0s [========================>] 41/41 tasks /usr/lib/wine/x86_64-windows/kernel32.dll: OK ----------- SCAN SUMMARY ----------- Known viruses: 8704732 Engine version: 1.4.2 Scanned directories: 0 Scanned files: 1 Infected files: 0 Data scanned: 2.46 MB Data read: 2.32 MB (ratio 1.06:1) Time: 22.854 sec (0 m 22 s) Start Date: 2025:02:16 01:16:50 End Date: 2025:02:16 01:17:13 [user@system ~]$ clamscan /usr/lib32/wine/i386-windows/kernel32.dll Loading: 15s, ETA: 0s [========================>] 8.70M/8.70M sigs Compiling: 3s, ETA: 0s [========================>] 41/41 tasks /usr/lib32/wine/i386-windows/kernel32.dll: OK ----------- SCAN SUMMARY ----------- Known viruses: 8704732 Engine version: 1.4.2 Scanned directories: 0 Scanned files: 1 Infected files: 0 Data scanned: 2.05 MB Data read: 1.93 MB (ratio 1.06:1) Time: 22.764 sec (0 m 22 s) Start Date: 2025:02:16 01:23:21 End Date: 2025:02:16 01:23:44 [user@system ~]$ pacman -Qkk wine wine: 4177 total files, 0 altered files [user@system ~]$ Edit: Here you have the messages clamav keeps spamming:Virus found! Signature detected by clamav: PUA.Win.Packer.Embedpe-3 in /usr/lib/wine/x86_64-windows/kernel32.dll Virus found! Signature detected by clamav: PUA.Win.Packer.Embedpe-3 in /usr/lib32/wine/i386-windows/kernel32.dll Edit2: Here you have what I mentioned about the credit card number:Virus found! Signature detected by clamav: Heuristics.Structured.CreditCardNumber in /var/lib/pacman/local/wine-10.1-1/mtree ]]></content:encoded></item><item><title>Safe elimination of unnecessary bound checks.</title><link>https://www.reddit.com/r/rust/comments/1iqev5s/safe_elimination_of_unnecessary_bound_checks/</link><author>/u/tjientavara</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 23:43:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi, I am working on a Unicode database that is pretty fast, it is a 2 step associated lookup.Here is the code for getting the east-asian-width value of a Unicode code-point. Pay specific attention to the function. This function is a  function and the byte tables that it references are  as well. This will allow you to eventually run the unicode algorithms at both compile and run-time.Since the tables are fixed at compile time, I can proof that all values from the table will result in values that will never break any bounds, so technically the bound checks are unnecessary.There are two bound checks in the assembly output for this function.The check before accessing the EAST_ASIAN_WIDTH_COLUMN table (I use an assert! to do this, otherwise there will be double bound check).And the check on the conversion to the enum.The two bound checks are the two compare + conditional-jump instructions in this code.I could increase the size of the column table to remove one of the bound checks, but I want to keep the table small if possible.Is there a way to safely (I don't want to use the unsafe code) proof to the compiler that those two checks are unnecessary?P.S. technically there is a bound check before the index table a CMOV instruction, but it doubles as a way to also decompress the index table (last entry is repeated), so I feel this is not really a bound check.I was able to concat the two tables, and use a byte offset. So now there is no way to get an out of bound access, and the bound checks are no longer emitted by the compiler.I also added a manual check for out of bound on the enum and return zero instead, this becomes a CMOV and it eliminated all the panic code from the function.]]></content:encoded></item><item><title>rke2 and DNS</title><link>https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/</link><author>/u/Affectionate_Horse86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 22:35:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == "server" and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.]]></content:encoded></item><item><title>Creating my OS</title><link>https://www.reddit.com/r/linux/comments/1iqaxku/creating_my_os/</link><author>/u/zainali28</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 20:46:12 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Now, I know it sounds absurd, but I just want to understand the general workflow of how do you design a linux, or a unix-based OS.I have a fair knowledge of computer architecture and can understand low level language of the computer.I am just an enthusiast who wants to just make a functional os, with just a terminal that is able to execute things.Any advice is greatly appreciated!]]></content:encoded></item><item><title>spf13/cobra v1.9.0</title><link>https://github.com/spf13/cobra/releases/tag/v1.9.0</link><author>/u/jpmmcb</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 20:45:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Amazon AWS &quot;whoAMI&quot; Attack Exploits AMI Name Confusion to Take Over Cloud Instances</title><link>https://www.reddit.com/r/programming/comments/1iqav3c/amazon_aws_whoami_attack_exploits_ami_name/</link><author>/u/Dark-Marc</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 20:43:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Incoming Rust intern need advice?</title><link>https://www.reddit.com/r/rust/comments/1iq9oph/incoming_rust_intern_need_advice/</link><author>/u/Helpful_Ad_9930</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:52:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hey everyone, I'm a 19-year-old college student who just landed a SWE internship at NVIDIA! My manager has me learning Rust and exploring one of its libraries, and Iâ€™m also reading up on operating systems and computer networking. I'm almost done with the OS book and plan to start the networking one next week.I do have a bit of experience with embedded systems I completed two internships during my freshman year. However, so far Iâ€™m really enjoying Rust. I am quite a rookie compared to you experienced folks haha! But so far I love how Rust's compiler enforces safety, how Cargo makes dependency management a breeze compared to CMake, and the whole concept of ownership and borrowing is just super cool.At the moment, Iâ€™m nearly finished with the Rust book. I am on the concurrency chapter. Guess I am just wondering what next? I really want this return offer and I just want to blow this opportunity out the park. I go too a state school and my manager told me he has high expectations for me after my interviews. I just do not want to let him down you know also plus kind of getting impostor syndrome a bit seeing all the other interns coming from schools such as MIT, Harvard, Standford, etc. Sorry for the vent I guess I just want to prove my worth? and show my manager they made the right choice?What fun, Rust projects have helped you learn a lot?Are there any books youâ€™d recommend that could help me out for the summer?Books I want to read before I start summer:Operating Systems (Three easy pieces)Beej's Guide to Network ProgrammingC++ Concurrency in Action]]></content:encoded></item><item><title>Networking in K8s</title><link>https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/</link><author>/u/I-Ad-7</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:49:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Background: Never used k8s before 4 months ago. I would say Iâ€™m pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. I was put into a position where I had to use K8s for everyday work and donâ€™t get me wrong Iâ€™m ecstatic about being an intern but already having the opportunity to work with deployments etc. What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.Right now I can pretty much do most stuff and have some cool projects running on my cluster.Iâ€™m also learning new stuff every day. But where I find myself lacking is Networking. Not just in Kubernetes but also generally. There are two examples of me getting frustrated because of my lacking networking knowledge:I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failedWas wondering why I canâ€™t see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?I donâ€™t mind it but Iâ€™m struggling to learn Networking and itâ€™s not that I donâ€™t have the basics but I donâ€™t have the advanced knowledge needed yet, so how do I actually get there?]]></content:encoded></item><item><title>[D] Is my company missing out by avoiding deep learning?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/</link><author>/u/DatAndre</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:42:42 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too "black-box" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. Am I also missing out on them, since I do not have work experience with such models?EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. ]]></content:encoded></item><item><title>â€˜Mass theftâ€™: Thousands of artists call for AI art auction to be cancelled</title><link>https://www.theguardian.com/technology/2025/feb/10/mass-theft-thousands-of-artists-call-for-ai-art-auction-to-be-cancelled</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:31:42 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Thousands of artists are urging the auction house Christieâ€™s to cancel a sale of art created with artificial intelligence, claiming the technology behind the works is committing â€œmass theftâ€.The Augmented Intelligence auction has been described by Christieâ€™s as the first AI-dedicated sale by a major auctioneer and features 20 lots with prices ranging from $10,000 to $250,000 for works by artists including Refik Anadol and the late AI art pioneer Harold Cohen.A letter calling for the auction to be scrapped has received 3,000 signatures, including from Karla Ortiz and Kelly McKernan, who are suing AI companies over claims that tech firmsâ€™ image generation tools have used their work without permission.The letter says: â€œMany of the artworks you plan to auction were created using AI models that are known to be trained on copyrighted work without a licence. These models, and the companies behind them, exploit human artists, using their work without permission or payment to build commercial AI products that compete with them.â€Calling on Christieâ€™s to cancel the auction, which starts on 20 February, it adds: â€œYour support of these models, and the people who use them, rewards and further incentivizes AI companiesâ€™ mass theft of human artistsâ€™ work.â€The use of copyrighted work to train AI models â€“ the technology that underpins chatbots and image generation tools such as Stable Diffusion and Midjourney â€“ has become a battleground between creatives and tech companies, with artists, authors, publishers and music labels launching a series of lawsuits alleging breach of copyright.The British composer Ed Newton-Rex, a key figure in the campaign by creative professionals for protection of their work and a signatory to the letter, said at least nine of the works appearing in the auction appeared to have used models trained on artistsâ€™ work. However, other pieces in the auction do not appear to have used such models.A spokesperson for Christieâ€™s said that â€œin most casesâ€ the AI used to create art in the auction had been trained on the artistsâ€™ â€œown inputsâ€.â€œThe artists represented in this sale all have strong, existing multidisciplinary art practices, some recognised in leading museum collections. The works in this auction are using artificial intelligence to enhance their bodies of work and in most cases AI is being employed in a controlled manner, with data trained on the artistsâ€™ own inputs,â€ said the spokesperson.A British artist whose work features in the auction, Mat Dryhurst, said he cared about the issue of art and AI â€œdeeplyâ€ and rejected the criticisms in the letter. A piece by Dryhurst and his wife, Holly Herndon â€“ based on a work called xhairymutantx â€“ is on sale at the auction with an estimated price of between $70,000 and $90,000.â€œThis is of interest to us and we have made a lot of art exploring and attempting to intervene in this process as is well within our rights.â€He added: â€œIt is not illegal to use any model to create artwork. I resent that an important debate that should be focused on companies and state policy is being focused on artists grappling with the technology of our time.â€Anadol also rejected the criticism. In a post on X, he said the backlash was a consequence of â€œlazy critic practices and doomsday hysteriaâ€.Anadol told the Guardian the piece being auctioned, ISS Dreams, was created using AI technology that had been trained on publicly available datasets from NASA â€œthat have been used widely by artists for many decades.â€He added that â€œthe suggestion that this artwork was created using â€˜AI models that are known to be on copyrighted work trained without a licenseâ€™ is factually incorrect.â€]]></content:encoded></item><item><title>How are you monitoring your cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/</link><author>/u/psavva</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:28:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.CPU and networking are always displaying 'No Data'I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0That fixes the alerts on a fresh install of the stack. 1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.Any advice would be appreciated.If anyone has a fully working example of the helm chart values that fully work, that would be awesome.]]></content:encoded></item><item><title>Zed for golang</title><link>https://www.reddit.com/r/golang/comments/1iq8jsm/zed_for_golang/</link><author>/u/MrBricole</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 19:02:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am considering using zed for writting go. Is it working out of the box with full syntax high light for noob like me such fmt.Println() ? I mean, I need to have it displaying functions under an import library.Should I give it a try or is it only for advanced users ? ]]></content:encoded></item><item><title>Pushing autovectorization to the limit: utf-8 validator</title><link>https://www.reddit.com/r/rust/comments/1iq7yn2/pushing_autovectorization_to_the_limit_utf8/</link><author>/u/Laiho3</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 18:36:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/Laiho3 ]]></content:encoded></item><item><title>Questions around LoadBalancer</title><link>https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/</link><author>/u/HahaHarmonica</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 18:36:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[New to k8s. Iâ€™ve deployed rke2 and iâ€™ve got several questions. Main Question) So iâ€™m trying to install rancher UI on it. When you go to install with helm it asks for a â€œhostnameâ€ and the hostname should be the name of your load balancerâ€¦i enabled the load balancer of rke2 but I have no clue how to operate with itâ€¦how do I change the configuration to point to rancher? The instructions arenâ€™t very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. 2) During my debugging, i ran the command â€œkubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldnâ€™t have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent nodeâ€¦actually be running.3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasnâ€™t the problem completely resetting the services by running the killall scripts on both of them before it worksâ€¦so why in the world wonâ€™t k8s actually respect systemctl and restart properly without literally shutting everything down. ]]></content:encoded></item><item><title>Introducing encode: Encoders/serializers made easy.</title><link>https://www.reddit.com/r/rust/comments/1iq6pz7/introducing_encode_encodersserializers_made_easy/</link><author>/u/Compux72</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:42:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ is a toolbox for building encoders and serializers in Rust. It is heavily inspired by the  and  crates, which are used for building parsers. It is meant to be a companion to these crates, providing a similar level of flexibility and ease of use for reversing the parsing process.The main idea behind  is to provide a set of combinators for building serializers. These combinators can be used to build complex encoders from simple building blocks. This makes it easy to build encoders for different types of data, without having to write a lot of boilerplate code.Another key feature of  is its support for  environments. This makes it suitable for use in embedded systems, where the standard library (and particularly the [] module) is not available.See the  folder for some examples of how to use . Also, check the  module for a list of all the combinators provided by the crate.Ready to use combinators for minimizing boilerplate.: Enables the  feature.: Enables the use of the standard library.: Enables the use of the  crate.: Implements [] for [].Why the  trait instead of ?A buffer stores bytes in memory such that write operations are . The underlying storage may or may not be in contiguous memory. A BufMut value is a cursor into the buffer. Writing to BufMut advances the cursor position.The bytes crate was never designed with falible writes nor  targets in mind. This means that targets with little memory are forced to crash when memory is low, instead of gracefully handling errors.Why the  trait instead of ?Because there is no alternative, at least that i know of, that supports  properlyBecause it's easier to work with than  and Because using  with binary data often leads to a lot of boilerplate]]></content:encoded></item><item><title>Lil guy is trying his best</title><link>https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:27:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Transition from C++ to Rust</title><link>https://www.reddit.com/r/rust/comments/1iq67vq/transition_from_c_to_rust/</link><author>/u/Dvorakovsky</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:20:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Guys, are here any people who were learning/coding in C++ and switched to Rust. How do you feel? I mean I could easily implement linked lists: singly, doubly in c++, but when I saw how it is implemented in Rust I'd say I got lost completely. I'm only learning rust... So yeah, I really like ownership model even tho it puts some difficulties into learning, but I think it's a benefit rather than a downside. Even tho compared to C++ syntax is a bit messy for me]]></content:encoded></item><item><title>No, your GenAI model isn&apos;t going to replace me</title><link>https://marioarias.hashnode.dev/no-your-genai-model-isnt-going-to-replace-me</link><author>/u/dh44t</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:06:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Type safe Go money library beta2!</title><link>https://www.reddit.com/r/golang/comments/1iq5stk/type_safe_go_money_library_beta2/</link><author>/u/HawkSecure4957</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 17:02:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, after I released beta1, I received many constructive feedback! mainly lacking of locale support.This update brings locale formatting support and an improved interface for better usability. With Fulus, you can perform monetary operations safely and type-soundly. Plus, you can format money for any locale supported by CLDR. You can even define custom money types tailored specifically to your application's needs! I still need to battle test it against production projects, I have none at the moment. I am aiming next for performance benchmarking and more improvement, and parsing from string!I am open for more feedback. Thank you! ]]></content:encoded></item><item><title>TIL There is a minor-planet called Linux</title><link>https://www.reddit.com/r/linux/comments/1iq5p1p/til_there_is_a_minorplanet_called_linux/</link><author>/u/forvirringssirkel</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 16:57:50 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Golang Mastery Exercises</title><link>https://www.reddit.com/r/golang/comments/1iq5k7w/golang_mastery_exercises/</link><author>/u/Temporary-Buy-7562</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 16:51:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I made a repository which has a prompt for you to write many exercises, if you complete this, and then drill the exercises, I would be sure you would reach mastery with the core of the language.I initially wanted to make some exercises for drilling syntax since I use copilot and lsps a lot, but ended up with quite a damn comprehensive list of things you would want to do with the language, and I find this more useful than working on leetcode to really adopt the language.]]></content:encoded></item><item><title>[D] Have any LLM papers predicted a token in the middle rather than the next token?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/</link><author>/u/TheWittyScreenName</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 15:59:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Iâ€™m working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but weâ€™re more interested in finding a series of tokens to connect a starting and ending â€œwordâ€ than the next â€œwordâ€. Since weâ€™re drawing a lot from LLMs in our setup, Iâ€™m wondering if thereâ€™s been any research into how models perform when the loss function isnâ€™t based on the next token, but instead predicting a masked token somewhere in the input sequence. Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. I couldnâ€™t find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?]]></content:encoded></item><item><title>Larry Ellison wants to put all US data in one big AI system</title><link>https://www.theregister.com/2025/02/12/larry_ellison_wants_all_data/</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:59:53 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[If governments want AI to improve services and security for their citizens, then they need to put all their information in one place â€“ even citizensâ€™ genomic data â€“ according to Larry Ellison, the Oracle database tycoon.Ellison shared his take on what governments need to do to succeed with AI during a discussion with his buddy former UK prime minister Tony Blair at the World Governments Summit in Dubai today.The world's fourth-most-richest man â€“ a good friend also of the world's richest man Elon Musk â€“ insisted artificial intelligence is soon going to change everyone's lives "across the board."I have to tell the AI model as much about my country as I can. We need to unify all the national dataIf governments want in, theyâ€™ll need to gather all their data â€“ spatial information, economic data, electronic healthcare records including genomic data, and info about infrastructure. Whatever theyâ€™ve got, basically. And put it all in one place to be analyzed by algorithms. The American multi-billionaire used the United States as an example, if not a goal."I have to tell [the] AI model as much about my country as I can," Ellison said. "We need to unify all the national data, put it into a database where it's easily consumable by the AI model, and then ask whatever question you like," he said. "That's the missing link."He believes the payoff will include better healthcare, thanks to treatments tailored to individuals, and the ability for governments to lift food production by better predicting crop yields. Analyzing land so that farmers can be advised where to apply fertilizers or increase irrigation was another scenario Ellison floated."As long as countries will put their data - all of it - in a single place we can use AI to help manage the care of all of the patients and the population at large," Ellison said, adding his belief that AI can handle other social services and eliminate fraud.Of course, such a vast database system could also be the precursor to pervasive surveillance â€“ an idea Ellison last year said he feels is desirable and would like Oracle to help facilitate.Constant real-time surveillance of populations, analyzed by Oracle-powered machine-learning products, would keep everyone "on their best behavior," Ellison said at an Oracle financial analyst conference in September 2024. We're reminded of the NSA, PRISM, Snowden.Ellison is not just a techno-optimist. Heâ€™s also a top executive and shareholder who has made big AI investments as well as a database company to feed.He therefore told the Dubai audience that Oracle, already a big-time government and military contractor, is ready to help nations realize his above-mentioned AI visions. Ie: Put all this data into one big expensive Oracle system to learn from and process."Oracle is building a 2.2GW datacenter that costs between $50 and $100 billion dollars to build," Ellison said, noting it's sites like that where super-powered AI models will be trained. "Because these models are so expensive, you won't build your own as a rule. There'll be a handful of these models."And a handful of players that can train them. Oracleâ€™s own facilities will likely be one. The super-corp has also joined another, the Stargate project, that plans to blow $500 billion on AI infrastructure in the US in the next four years. Â®]]></content:encoded></item><item><title>Alexandre Mutel a.k.a. xoofx is leaving Unity</title><link>https://mastodon.social/@xoofx/113997304444307991</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:53:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t &quot;optimize&quot; conditional moves in shaders with mix()+step()</title><link>https://iquilezles.org/articles/gpuconditionals/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:52:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
In this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:
So, say I have this code, which I actually published the other day: snap45(  v )
{
     s = (v);
     x = (v.x);
     x>?(s.x,):
           x>?s*():
                      (,s.y);
}
The exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.
Now, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following "optimization", which replaces what they believe (erroneously) are "conditional branches" in the code, by arithmetic operations. They will suggest something like this: snap45(  v )
{
     s = (v);
     x = (v.x);

     w0 = (,x);
     w1 = (,x)*(-w0);
     w2 = -w0-w1;

     res0 = (s.x,);
     res1 = (s.x,s.y)*();
     res2 = (,s.y);

     w0*res0 + w1*res1 + w2*res2;
}
There are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. "select"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.
For the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.
The second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the  function is probably implemented with some sort of conditional move or subtract + bit propagation + AND. step(  x,  y )
{
     x < y ?  : ;
}
Either way, using the step() "optimization" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.
But don't take my word for it, let's look at the generated machine code for the original code I published:
GLSL x>?(s.x,):
       x>?s*():
                  (,s.y);
AMD Compiler     s0,      v3, , v1
     v4, , v0
     s1,   vcc, (v2), s0
 v3, 0, v3, vcc
 v0, v0, v4, vcc
 vcc, (v2), s1
 v1, v1, v3, vcc
 v0, 0, v0, vcc
Microsoft Compiler   r0.xy, l(, ), v0.xy
   r0.zw, v0.xy, l(, )
 r0.xy, -r0.xyxx, r0.zwzz
 r0.xy, r0.xyxx
  r1.xyzw, r0.xyxy, l4()
   r2.xy, l(,), v0.xx  r0.z, l()
 r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy
 o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw
Here we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).
The Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison ( - "lt"=less than) and the masking or conditional move ( - mov=move, c=conditionally).
There are no jump/branch instructions in these listings.
Something not related to the discussion but interesting, is that some of the  GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.
So, if you ever see somebody proposing this a = ( b, c, ( y, x ) );
as an optimization to
then please correct them for me.]]></content:encoded></item><item><title>Altman: OpenAI not for sale, especially to competitor who is not able to beat us</title><link>https://www.axios.com/2025/02/11/openai-altman-musk-offer</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:17:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - yaitoo/xun: Xun is an HTTP web framework built on Go&apos;s built-in html/template and net/http packageâ€™s router (1.22).</title><link>https://github.com/yaitoo/xun</link><author>/u/imlangzi</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 14:15:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What is Event Sourcing?</title><link>https://newsletter.scalablethread.com/p/what-is-event-sourcing</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:05:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like "Created," "Paid," and "Shipped." To determine the current state of an order, you would:Retrieve all events related to the order from the event store.Initialize an empty order object.Apply each event to the order object in the order in which they were stored.By the end of this process, the order object will reflect the current state of the order.As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. If you enjoyed this article, please hit the â¤ï¸ like button.If you think someone else will benefit from this, then please ðŸ” share this post.]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>Built a cli tool for generating .gitignore files</title><link>https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/</link><author>/u/SoaringSignificant</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:38:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built this mostly as an excuse to play around with Charmbraceletâ€™s libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. Itâ€™s a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you donâ€™t have to redo them every time. If youâ€™re always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out here and give it a try. And if youâ€™ve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!]]></content:encoded></item><item><title>ED25519 Digital Signatures In Go</title><link>https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/</link><author>/u/mejaz-01</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:37:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/mejaz-01 ]]></content:encoded></item><item><title>Richard Stallman on RISC-V and Free Hardware</title><link>https://odysee.com/@SemiTO-V:2/richardstallmanriscv:7?r=BYVDNyJt5757WttAfFdvNmR9TvBSJHCv</link><author>/u/ShockleyTransistor</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 13:20:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.â€œRealistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,â€ Fu said.â€œAs long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.â€The panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Bidenâ€™s administration and the United Nations.]]></content:encoded></item><item><title>Karol Herbst steps down as Nouveau maintainer due to â€œthin blue line commentâ€</title><link>https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/</link><author>/u/mdedetrich</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:24:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA["I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.Most of the time I simply excused myself with "if something urgent comes up, I can chime in and help out". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:"we are the thin blue line"This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.]]></content:encoded></item><item><title>Building the MagicMirror in Rust with iced GUI Library ðŸ¦€</title><link>https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/</link><author>/u/amindiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:56:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and Iâ€™d like to share my experiences. I wrost a blog post titled "software you can love: miroir Ã” mon beau miroir" this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.Here's what I loved about using Rust and the iced GUI library:Elm Architecture + Rust is a match made in heaven: iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary. Although troublesome at first, I used â€˜crossâ€™ to cross compile Rust for armv7.If anyone is keen, Iâ€™m thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Richard Stallman in Polytechnic University of Turin, Italy</title><link>https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/</link><author>/u/ShockleyTransistor</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:05:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>async-arp: library for probing hosts and sending advanced ARP (Address Resolution Protocol) requests.</title><link>https://www.reddit.com/r/rust/comments/1ipywbp/asyncarp_library_for_probing_hosts_and_sending/</link><author>/u/arcycar</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:48:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[After a few months of exploring and working with Rust, I am happy to share my first small Rust crate,  and Iâ€™d love to hear your thoughts! ðŸš€This library provides an  way to send and receive , making it useful for network discovery, debugging, and custom networking applications.ðŸŽ  Built on Tokio for non-blocking network operationsðŸ”  Easily detect active devices in a subnetâš™ï¸  Craft and send ARP packets dynamicallyYou can find usage examples and API documentation here: ðŸ“– Since this is my first crate, Iâ€™d really appreciate any feedback on:ðŸ“Œ  â€“ Is the interface intuitive and ergonomic?ðŸš€  â€“ Does it fit well into async Rust workflows?ðŸ”  â€“ Any improvements or best practices I may have missed?ðŸ¦€  â€“ Suggestions to make it more "Rustacean"?If you have further ideas, issues, or want to contribute, check it out on GitHub:Thanks for checking it outâ€”let me know what you think! ðŸ¦€]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.</title><link>https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/</link><author>/u/trollfinnes</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:05:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.I was expecting to spend the "weekend" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. And while he is definitely in the upper half of people in terms of 'tech savvyness', both those "two easy first steps" made it virtually impossible for him to install it. He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.But he wasn't able to get into bios despite all my attempts explaining what button to push and whenNext day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.After that he managed to install Linux just fine without my supervision.But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.I don't know enough about programming to know of this is possible:Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?That would 'open up' Linux to  more people, probably orders of magnitude..]]></content:encoded></item><item><title>Lessons from David Lynch: A Software Developer&apos;s Perspective</title><link>https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/</link><author>/u/aijan1</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 09:40:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. Heâ€™s perhaps best known for the groundbreaking TV series Twin Peaks, which inspired countless shows, including The X-Files, The Sopranos, and Lost.Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down â€“ even those who truly deserved it.Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that Mulholland Drive remained compulsively watchable while refusing to yield to interpretation.While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, Iâ€™d like to share my perspective on his life lessons from a software developerâ€™s viewpoint.Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, youâ€™ve got to go deeper.Weâ€™ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one â€“because theyâ€™re so rareâ€“ write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether itâ€™s a film, a painting, or software.The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.Software development is part art, part engineering. We donâ€™t build the same software over and over again â€“ virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, itâ€™s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.Itâ€™s a good habit to listen to what users have to say, but they often can only describe their problems â€“ they rarely come up with good ideas to solve them. And thatâ€™s OK. Itâ€™s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  â€“ that magical state of mind where we lose track of time and produce code effortlessly. Thatâ€™s why many developers hate meetings â€“ they are toxic to our productivity.I believe you need technical knowledge. And also, itâ€™s really, really great to learn by doing. So, you should make a film.Software development is one of those rare fields where a college degree isnâ€™t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. Itâ€™s crucial to never stop learning, experimenting, and iterating on our craft.Happy accidents are real gifts, and they can open the door to a future that didnâ€™t even exist.Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.Be kind to your teammates, donâ€™t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety â€“that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by Googleâ€™s research on the subject.Itâ€™s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.Most of Hollywood is about making money - and I love money, but I donâ€™t make the films thinking about money.Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.What makes these projects remarkable is that they didnâ€™t emerge from corporate boardrooms â€“ they were built by communities of passionate developers, collaborating across the world.Money is just a means to an end. Unfortunately, many get this confused.David, thank you for making the world a better place!]]></content:encoded></item><item><title>&quot;Dongly Things&quot; by Douglas Adams (of Hitchhikers Guide) - Adams wrote this article in the early days of Mac computers, about manufacturers making things difficult with a million different proprietary cables/ports etc.</title><link>https://www.douglasadams.com/dna/980707-03-a.html</link><author>/u/CaesarSalvage</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 07:47:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>Kafka Delay Queue: When Messages Need a Nap Before They Work</title><link>https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 05:08:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework Youâ€™ve Been Waiting For! ðŸš€</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, Iâ€™ve been missing a Nest.js-like framework â€” its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesnâ€™t have a direct counterpart (yet!), I decided to build one myself! ðŸ› ï¸Introducingâ€¦ Toni.rs â€” a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And itâ€™s live in beta! ðŸŽ‰Hereâ€™s what makes this project interesting:Scalable maintainability ðŸ§©:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code â€” each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection ðŸ¤–:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below â€” suggestions, questions, or even just enthusiasm! ðŸš€ ]]></content:encoded></item><item><title>what was the Linux expirance like in the 90&apos;s and 00&apos;s?</title><link>https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/</link><author>/u/mrcrabs6464</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 01:35:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?   submitted by    /u/mrcrabs6464 ]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...ðŸ“Š Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and SqliteðŸ—‚ï¸ Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>