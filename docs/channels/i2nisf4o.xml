<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>native WebP encoding version 1.0! 🚀</title><link>https://www.reddit.com/r/golang/comments/1iw8a68/native_webp_encoding_version_10/</link><author>/u/Pretend-Ad1926</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 11:38:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I’m excited to announce nativewebp v1.0, a major milestone for our WebP encoder in Go! This version marks 1.0 because we now fully support the VP8L format, making nativewebp a complete solution for lossless WebP encoding. Alongside this, we’ve added better compression, improved Go integration, and important bug fixes.Here are some highlights of this release:Full VP8L Feature SupportThis release now fully supports all VP8L features, including LZ77, Color Caching, and transforms, ensuring more accurate and efficient encoding.Smarter Compression with Filter Selection for Predictor TransformWe now analyze block entropy and automatically select the best filter per block, leading to much better compression.nativewebp now includes a wrapper for golang.org/x/image/webp, so you can use Decode and image.Decode out of the box without extra imports.Looking forward to your thoughts and feedback on the new release!]]></content:encoded></item><item><title>Amazon EKS Downgrade: A Practical Solution</title><link>https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/</link><author>/u/Complete-Emu-6287</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 11:00:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup & restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.🔗 Read the full article here: If you've faced EKS downgrade challenges, let's discuss your experiences! ⬇️ #AWS #Kubernetes #EKS #DevOps]]></content:encoded></item><item><title>Amazon EKS Downgrade: A Practical Solution</title><link>https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/</link><author>/u/Complete-Emu-6287</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 11:00:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup & restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.🔗 Read the full article here: If you've faced EKS downgrade challenges, let's discuss your experiences! ⬇️ #AWS #Kubernetes #EKS #DevOps]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author>/u/Aciddit</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 23 Feb 2025 09:30:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[By Antonio Ojea, Michael McCune |
Friday, February 14, 2025Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the Node
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial Node is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.Chicken and egg problem sequence diagramThis new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a Node resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a Node resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: cloud-controller-manager
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cloud-controller-manager
      annotations:
        kubernetes.io/description: Cloud controller manager for my infrastructure
    spec:
      containers: # the container details will depend on your specific cloud controller manager
      - name: cloud-controller-manager
        command:
        - /bin/my-infrastructure-cloud-controller-manager
        - --leader-elect=true
        - -v=1
        image: registry/my-infrastructure-cloud-controller-manager@latest
        resources:
          requests:
            cpu: 200m
            memory: 50Mi
      hostNetwork: true # these Pods are part of the control plane
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: cloud-controller-manager
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 120
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 120
      - effect: NoSchedule
        key: node.cloudprovider.kubernetes.io/uninitialized
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/not-ready
        operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>Finding UI libraries is easy, but discovering components visually is still a challenge. A curated list + an idea to fix this.</title><link>https://github.com/sanjay10985/animated-react-collection</link><author>/u/Mobile_Candidate_926</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 09:22:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[P] See the idea development of academic papers visually</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 08:30:10 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Relevance-Guided Parameter Optimization for Efficient Control in Diffusion Transformers</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 06:50:56 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[The key technical contribution here is a relevance-guided architecture that makes diffusion transformers more computationally efficient by selectively allocating processing power based on region importance. It combines DiT (Diffusion Transformers) with ControlNet approaches while introducing a relevance prior mechanism.Main technical points: - Introduces a two-stage relevance assessment system: lightweight networks evaluate region importance, followed by adaptive computation allocation - Integrates with existing diffusion pipelines through modular design - Relevance prior guides transformer attention mechanisms - Compatible with standard diffusion transformer architecturesKey results: - 30-50% reduction in computational overhead - Maintains or improves image quality compared to baselines - More precise control over generated content - Effective handling of complex scenesI think this could have meaningful impact on making high-quality image generation more accessible, especially for resource-constrained applications. The approach seems particularly promising for deployment scenarios where computational efficiency is crucial.I think the relevance-guided approach could extend beyond image generation - the core idea of selective computation based on importance could benefit other transformer applications where attention mechanisms are computationally expensive.TLDR: Novel architecture that makes diffusion transformers more efficient by focusing computational resources on important image regions, reducing compute needs by 30-50% while maintaining quality.]]></content:encoded></item><item><title>Folang: Transpiler for F#-like functional languages ​​to Go</title><link>https://www.reddit.com/r/golang/comments/1iw3tz7/folang_transpiler_for_flike_functional_languages/</link><author>/u/karino2012</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 06:26:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I wrote a transpiler in Go that transpiles F#-like functional languages ​​to Go.I design the language specifications from scratch to match Go, and named it Folang.There are still many NYIs, but I have implemented it to the extent that it can be self-hosted, so I will post it on reddit.A transpiler that does not require anything other than Go to tryArgument types are inferred using F#-like syntax, and the arguments are generalized to become generic functionsThe transpiler itself is 3600 lines of Folang code and about 500 lines of Go code]]></content:encoded></item><item><title>A simple VSCode extension to remember which virtual desktop each editor window is on in Linux</title><link>https://marketplace.visualstudio.com/items?itemName=mathiscode.remember-desktops</link><author>/u/FatherCarbon</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:56:05 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[On some Linux desktop managers, Visual Studio Code editors don't remember their last desktop. This extension uses  to save the desktop of each open editor window, and restore them when the editor starts.There are commands to save the editor locations, and to restore them, but by default the extension will start working automatically when it is installed.]]></content:encoded></item><item><title>Thoughts on listings like these selling flash drives with Ubuntu and other Linux distros pre-installed?</title><link>https://www.reddit.com/r/linux/comments/1iw2zog/thoughts_on_listings_like_these_selling_flash/</link><author>/u/FutureSuccess2796</author><category>linux</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:32:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Admittedly someone who's relatively newer to the Linux space, so please bear with my question here. I was in middle of actually shopping for some extra brand new USBs to replace my old ones when I encountered this for the first time. It looked like there were quite a good number of people on marketplace platforms like eBay and Mercari selling bootable USB flash drives with a Linux distro pre-installed on it. Majority of the ones I saw were Ubuntu (like what I had pictured) on there, but I also saw a good amount of ones with Kali and different versions of Linux Mint as well. Seems like you get the USB according to said listings with instructions on how to properly boot it or install it on your computer, and in some cases even provide contact information for support if needed. The prices on some of these are slightly in the higher side when compared to those I had screenshots of in the examples, and the sellers all had a large amount of sales and positive responses. Now, of course, I'd personally just stick to what I've been doing and just create the bootable drive myself for literally free like I have from the start. So to me it was interesting to see these out there actually being bought when the process of doing this yourself is relatively easy with step-by-step guides on the respective distro's website and even YouTube tutorials if you wish to follow those. So in short, what's everyone think of these?]]></content:encoded></item><item><title>Font for programming mathematics</title><link>https://www.reddit.com/r/rust/comments/1iw2ovd/font_for_programming_mathematics/</link><author>/u/okimusix</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:14:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So I am a physics undergrad and I've been using Rust for a few years now. It's my favorite language and I use it for everything, from personal apps using Tauri to taking advantage of its speed for computations and using it in my school assignments.Since I often find myself writing math code, I found naming variables "lambda_squared", for example, looks really clunky and makes it harder to read the code. For this, I implemented a Live Templates group on RustRover that replaced lambda, for example, with its equivalent unicode character. However, Rust did complain a little.Finally, though, I found the solution. I had been trying to do this for a while with no luck, but I found a way to make it work. I used the ligature system on the FiraCode font to implement ligatures for every greek letter and some mathematical symbols, this way you get the readability of actual math, but for the compiler, it still looks like plain text. Here's an exampleThe text for the sum variable, for example, is just "SUMxu2", and both the compiler and I are happier. I don't know if anyone has done this before, I tried to look for it but never found anything. If you find this something that could be useful for you or others, I can share a link to a drive or something where you can download the font, as well as the guide to every symbol I included. If so, please comment and share your thoughts on this too :)]]></content:encoded></item><item><title>[D] API platforms vs self-deployment for diffusion models</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/</link><author>/u/crookedstairs</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:06:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Caveat that Modal is a serverless compute platform! But this post covers when you might choose between API platforms (replicate, fal), traditional cloud (AWS EC2), managed ML platforms (SageMaker, Vertex), and serverless cloud.I often see companies jump to self-deployment even if they're just using off-the-shelf models with a couple of adapters. I think that rarely makes sense from a cost or effort perspective unless you have a high volume of production traffic that you're amortizing those things across. The most compelling reason to move to self-deployment is if you need a high level of control over generated inputs => this requires fine-tuned weights / customer adapters / multi-step generation pipeline => this requires code-level control of your deployment.What do you agree/disagree with? If you've evaluated these categories of providers before, tell me how they stacked up against each other.]]></content:encoded></item><item><title>What&apos;s your recommendation on video editors?</title><link>https://www.reddit.com/r/linux/comments/1iw2aae/whats_your_recommendation_on_video_editors/</link><author>/u/chuzambs</author><category>linux</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 04:50:24 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hi there ! I'm looking for the best video editor for Linux, but as I know that's a completely subjective matter I ask for your favorite one. I come from adobe premiere and I'm looking for a Linux replacement, Im not a cinematographer so I'm not looking for something extremely professional.I think Id go for da Vinci resolve since it's more standard, but would love to hear your recommendationsEdit: I'm running fedora bluefin (gnome) so I'd rather use flatpak ]]></content:encoded></item><item><title>What is your logging, monitoring &amp; observability stack for your golang app?</title><link>https://www.reddit.com/r/golang/comments/1iw07rm/what_is_your_logging_monitoring_observability/</link><author>/u/gwwsc</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 02:53:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[My company uses papertrail for logging, prometheus and grafana for observability and monitoring.I was not actively involved in the integration as it was done by someone else a few years ago and it works.I want to do the same thing for my side project that I am working on for learning purpose. The thing I am confused about it should I first learn the basics about otel, collector agents etc? Or should I just dive in?As a developer I get an itch if things are too abstracted away and I don't know how things are working. I want to understand the underlying concepts first before relying on abstraction.What tools are you or your company using for this?]]></content:encoded></item><item><title>My &quot;AI Operating System&quot; Can Now Organize My Desktop!</title><link>https://v.redd.it/crjpxmcknske1</link><author>/u/mitousa</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 01:47:12 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This feels illegal</title><link>https://www.reddit.com/r/linux/comments/1ivyn4j/this_feels_illegal/</link><author>/u/dk865409</author><category>linux</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 01:30:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I built WikiTok in 4 hours - A TikTok style feed for Wikipedia</title><link>https://www.reddit.com/r/artificial/comments/1ivy48f/i_built_wikitok_in_4_hours_a_tiktok_style_feed/</link><author>/u/Illustrious-King8421</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 01:03:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[So, I decided to use Replit's AI Agent to create my own version. Took me about 4 hours total, which isn't bad since I don't know any code at all.To be honest, at first it seemed unreal - seeing the AI build stuff just from my instructions. But then reality hit me. With every feature I wanted to add, it became more of a headache. Here's what I mean: I wanted to move some buttons around, simple stuff. But when I asked the AI to realign these buttons, it messed up other parts of the design that were working fine before. Like, why would moving a button break the entire layout?This really sucks because these errors took up most of my time. I'm pretty sure I could've finished everything in about 2 hours if it wasn't for all this fixing of things that shouldn't have broken in the first place.I'm curious about other people's experiences. If you don't code, I'd love to hear about your attempts with AI agents for building apps and websites. What worked best for you? Which AI tool actually did what you needed?What do you think? Would love to hear your stories and maybe get some tips for next time!]]></content:encoded></item><item><title>After 15 years of using Windows, I decided to try Linux</title><link>https://www.reddit.com/r/linux/comments/1ivxy81/after_15_years_of_using_windows_i_decided_to_try/</link><author>/u/Catwz</author><category>linux</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 00:55:33 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[First of all, I apologize for writing such a long text.I'm 22 years old. I know I'm young and still don't know much, but I'd like to write about this anyway. I think I started using computers during the Windows XP era. My father worked repairing computers. My mom says I learned to type on a computer before writing on paper. I was like one of today's kids who spend all day on their phones, except with computers. During my childhood, I spent my time chronically online, playing various games and browsing the internet. I remember Windows XP very well, along with Windows 7 and Minecraft. Those were good times, but as I grew older, things changed very quickly. My father stopped working with computer repairs, and soon I knew more than everyone else in the family.I could fix all kinds of computers easily for my friends; back then, everything was Windows. My first contact with Linux was at school when we started having computer classes, when I was around 15. The school computers were slow and had Ubuntu installed. It was slow, ugly, and very limited because the computers were managed by the school. That was my first impression: a slow system for government computers.Microsoft tried various things. I remember Windows 8 when formatting laptops, and then that Windows 8.1 update where they changed the menu. A lot happened, and it seems to have passed so quickly. At school, I always used Office suite programs: Word, PowerPoint, etc., and in computer classes, you had to use LibreOffice on a very slow government computer. it was ugly and seemed very difficult to use.My family's financial situation didn't improve much, so I ended up with limited access to new technologies. My phone was already old, and my computers were getting old. I still remember Windows 10's launch very well. My relatives would bring computers for me to repair and format, wanting the latest version of Windows with Office and everything else, but the computers were already old and barely worked with Windows 8.I begged my father to buy me a laptop, and after much insistence, I finally convinced him. It was an Asus X450LA. A mid-range computer for its time. It came with Windows 8, I think, but I did that upgrade to Windows 10. I used it until I finished high school, but then Windows 11 came along, and my laptop was cut from the list of computers that could upgrade. it was the end of my laptop's life.I was already working at my father's market, so I bought myself a new gaming computer with Windows 11. I had time again to spend on the internet and started to worry about my father's business expenses. Using Office costs money, sales programs are expensive, everything is expensive, and maybe my gaming laptop won't even be able to use the next Windows.I started researching Linux. At first, I was a bit scared because everyone on Reddit talked about terminals, command lines to install anything, etc., but I decided to take my old laptop and refurbish it. I bought a new battery, an SSD, and an 8GB RAM stick. I researched on Reddit which distro was best for beginners, got an old USB drive, put Mint on it, and formatted my computer: Love at first sight.I customized Mint and left it in a way that I spend more than 15 minutes before doing anything just appreciating it. I used LibreOffice for everything I did in Office. I used Firefox and liked it a lot. The system is very fast, strangely seems faster than my new computer with Windows 11. I downloaded my daily-use programs from Mint's app center: Spotify, Bitwarden, everything's there. I spent hours playing with the terminal with ChatGPT's help. I extracted running process logs to txt, system information. it's very easy to use. I even managed to install a game I played in my childhood, a BF2 mod: Forgotten Hope 2 from Windows on Mint using Lutris (I swear it's the last Windows thing I'll use).I'm in love with my old laptop again. I cleaned it, spent hours looking at it, I love using Mint, made it my own. I'm going to buy a new computer for my room and install Mint for my personal use. I'll have a laptop and a computer with Linux. My current computer with Windows 11 will be only for sales programs and government programs that only work on Windows. I showed it to my father, and he liked Linux too. Windows never again. Using Windows now feels like one of those mobile games full of ads]]></content:encoded></item><item><title>Advanced SQL Tricks (CTEs, Conditional Aggregations, etc)</title><link>https://youtu.be/rDGCOE5YGT0</link><author>/u/Special_Community179</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 00:18:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Found this on a piece of digital signage in a bathroom</title><link>https://www.reddit.com/r/linux/comments/1ivwydq/found_this_on_a_piece_of_digital_signage_in_a/</link><author>/u/A_Sc00py_b0i</author><category>linux</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 00:06:24 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Introducing wctx - A simple CLI tool for window context info on Wayland &amp; X11</title><link>https://www.reddit.com/r/linux/comments/1ivw7xn/introducing_wctx_a_simple_cli_tool_for_window/</link><author>/u/slightlyfaulty</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 23:30:48 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hey everyone, I just released my first package for Linux. It's called  (short for window context). It's a simple CLI tool that provides real-time information about the current  window (focused window) or  window (under the mouse cursor) on Wayland and X11. It's (mostly) written in Rust.It's not very useful on its own, but it makes it much easier for programs and scripts to work with windows. For example, you could create hotkeys that only work in specific apps, or change your mouse scroll speed when the cursor is in a browser window, or turn your monitor brightness up when it has a fullscreen window.You can of course already do these things, with a bit of effort. The main advantage of wctx is that it works across multiple desktop environments, which means programs and scripts using it will too. It's also dead simple to use, with several CLI output options and formats, as well as a D-Bus interface.Currently it supports these desktop environments, with more to come if there's enough interest in them:For other distros an installation script is included, with more info in the readme.I'd love to hear everyone's thoughts. This is also my first real Rust project, so please be nice 😄 (or rip me a new one so I can learn).Feedback and contributions are very welcome!]]></content:encoded></item><item><title>My first time with Linux</title><link>https://www.reddit.com/r/linux/comments/1ivvcxn/my_first_time_with_linux/</link><author>/u/Acu17y</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 22:49:59 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Oh my god guys, I'm speechless. Unfortunately I regret it, but it's the first time I've put my hands on a PC with a Linux kernel. But this stuff is absurd! It has mind-blowing performance!I installed it on my old laptop with an i3 5005u / 4gb of ram and a 500gb 5400rpm hdd and it's like it was reborn. I mean, it's basically the OS I've always dreamed of, I feel like the PC is really mine and everything is so fast and intuitive that I can't describe it.I was so impressed by Linux Mint that I'm really thinking of installing it on the main machine and getting rid of Windows, if only it weren't for the huge library of video games I have.It also has a community made up of wonderful people, true enthusiasts.I write this post as an appreciation for this discovery and someone who can help me understand if it is possible to use mint for gaming, I read around that there are problems with anti-cheats and online games?]]></content:encoded></item><item><title>Any open source project that shows a good example of unit test and integration test</title><link>https://www.reddit.com/r/golang/comments/1ivv5eq/any_open_source_project_that_shows_a_good_example/</link><author>/u/smartfinances</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:40:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[As the title suggests. I have been adding various unit tests to my project but I am looking for suggestions/ideas on how to go about writing integration tests.My project mostly entails reading from SQS, batching data based on some parameters and then writing the output to s3. probably, a very common pattern. Extending that the service reads from various SQS and batching is localised to one queue. So 10 queue creats 10 different outputs.I am using localstack for development. I am not looking for examples of exactly the above use case but something similar that is interaction with db/external system and then giving some output would also do.]]></content:encoded></item><item><title>Writing a file system in Go -- not FUSE, but a real FS</title><link>https://www.reddit.com/r/golang/comments/1ivuz61/writing_a_file_system_in_go_not_fuse_but_a_real_fs/</link><author>/u/Rich-Engineer2670</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:31:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I would say I'm crazy, but this both well established and redundant.....Assume I wanted to write my own file system (education), with Golang -- not a fuse variant, but I literally am taking a file on a block device and treating it as a disk. If this were C, OK, I'd do the following:Define a binary boot block at LBA 0Define a certain number of LBAs for boot codeDefine a certain number of LBAs for partitionsWithin each partition define the directories and free lists (FATs, clusters, etc...)Have a bunch of free LBAs.In C, I could define structs and just write them out assuming they were packed. In Go, structs aren't C structs, so I need to constantly convert structs to binaries. Sure, I could use the binary package and a lot functions, but someone must have done this in a better way, or is the "better way" "No, write your file systems in C...."I want to stay in Go, because everything else in the OS is in Go...]]></content:encoded></item><item><title>Talos on IPv6 only network?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/</link><author>/u/Moleventions</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 22:15:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Does anyone know if you can deploy Talos on an IPv6 only network in AWS?   submitted by    /u/Moleventions ]]></content:encoded></item><item><title>Why K8s when there’s k3s with less resource requirements?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/</link><author>/u/Crafty0x</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 21:57:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I don’t get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.   submitted by    /u/Crafty0x ]]></content:encoded></item><item><title>Golang SQLite admin tool</title><link>https://github.com/joelseq/sqliteadmin-go</link><author>/u/lAdddd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 21:08:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask</title><link>https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E</link><author>/u/cramdev</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 20:32:49 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Solving The Millionaires&apos; Problem in Rust</title><link>https://vaktibabat.github.io/posts/smpc_circuits/</link><author>/u/vaktibabat</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 20:24:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Official /r/rust &quot;Who&apos;s Hiring&quot; thread for job-seekers and job-offerers [Rust 1.85]</title><link>https://www.reddit.com/r/rust/comments/1ivrkhs/official_rrust_whos_hiring_thread_for_jobseekers/</link><author>/u/DroidLogician</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 19:59:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Welcome once again to the official r/rust Who's Hiring thread!Before we begin, job-seekers should also remember to peruse the prior thread.This thread will be periodically stickied to the top of r/rust for improved visibility. You can also find it again via the "Latest Megathreads" list, which is a dropdown at the top of the page on new Reddit, and a section in the sidebar under "Useful Links" on old Reddit.The thread will be refreshed and posted anew when the next version of Rust releases in six weeks.Please adhere to the following rules when posting:Don't create top-level comments; those are for employers.Feel free to reply to top-level comments with on-topic questions.Anyone seeking work should reply to my stickied top-level comment.Meta-discussion should be reserved for the distinguished comment at the very bottom.The ordering of fields in the template has been revised to make postings easier to read. If you are reusing a previous posting, please update the ordering as shown below.Remote positions: see bolded text for new requirement.To find individuals seeking work, see the replies to the stickied top-level comment; you will need to click the "more comments" link at the bottom of the top-level comment in order to make these replies visible.To make a top-level comment you must be hiring directly; no third-party recruiters.One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.Proofread your comment after posting it and edit it if necessary to correct mistakes.To share the space fairly with other postings and keep the thread pleasant to browse, we ask that you try to limit your posting to either 50 lines or 500 words, whichever comes first.We reserve the right to remove egregiously long postings. However, this only applies to the content of this thread; you can link to a job page elsewhere with more detail if you like.Please base your comment on the following template:COMPANY: [Company name; optionally link to your company's website or careers page.]TYPE: [Full time, part time, internship, contract, etc.]LOCATION: [Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]REMOTE: [Do you offer the option of working remotely? Please state clearly if remote work is restricted to certain regions or time zones, or if availability within a certain time of day is expected or required.]VISA: [Does your company sponsor visas?]DESCRIPTION: [What does your company do, and what are you using Rust for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]ESTIMATED COMPENSATION: [Be courteous to your potential future colleagues by attempting to provide at least a rough expectation of wages/salary. If you are listing several positions in the "Description" field above, then feel free to include this information inline above, and put "See above" in this field. If compensation is negotiable, please attempt to provide at least a base estimate from which to begin negotiations. If compensation is highly variable, then feel free to provide a range. If compensation is expected to be offset by other benefits, then please include that information here as well. If you don't have firm numbers but do have relative expectations of candidate expertise (e.g. entry-level, senior), then you may include that here. If you truly have no information, then put "Uncertain" here. Note that many jurisdictions (including several U.S. states) require salary ranges on job postings by law. If your company is based in one of these locations or you plan to hire employees who reside in any of these locations, you are likely subject to these laws. Other jurisdictions may require salary information to be available upon request or be provided after the first interview. To avoid issues, we recommend all postings provide salary information. You  state clearly in your posting if you are planning to compensate employees partially or fully in something other than fiat currency (e.g. cryptocurrency, stock options, equity, etc). Do  put just "Uncertain" in this case as the default assumption is that the compensation will be 100% fiat. Postings that fail to comply with this addendum . Thank you.]CONTACT: [How can someone get in touch with you?]]]></content:encoded></item><item><title>I’ve made a bunch of free wallpapers</title><link>https://www.reddit.com/r/linux/comments/1ivqqg0/ive_made_a_bunch_of_free_wallpapers/</link><author>/u/Folium_Creations</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 19:23:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I’ve made a whole bunch of wallpapers and released them under CC:BY I have made a git where I have uploaded, and will continue to upload them in 4k resolution as .png files for your convenience. I can’t stand all those “we have free wallpapers, as long as you register with email,phone number and the blood of your first born.” Here is the link to the git. I’m slowly building up a curated library of wallpapers I’ve created. ]]></content:encoded></item><item><title>What are the odds that Rust is going to have a real competitor?</title><link>https://www.reddit.com/r/rust/comments/1ivqkj1/what_are_the_odds_that_rust_is_going_to_have_a/</link><author>/u/nikitarevenco</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 19:15:59 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[By "Real Competitor" I mean: A language just like Rust with similar goals, but one that people actually prefer to Rust. So it would be a fast, low-level memory safe language with great tooling, great type system and other benefits that Rust offers. But it would need to be better than Rust to actually catch onThis language needs to offer real advantages over Rust to be considered. Of course since Rust has a huge ecosystem that is growing rapidly, it may take a long time. But I am talking on a timescale of 25+ years.Creating a new programming language to compete with Rust would be a massive undertaking and there would have to be some real reason to do it. Rust may be missing some features like higher-kinded types, named function arguments and such but to really catch on the language would need to offer some extremely important feature that Rust doesn't have, as well as offering all of Rust's benefits at the same time.Is there any such language currently in early development? Or perhaps, what would such a language have to look like?]]></content:encoded></item><item><title>Scrap Your ORM—Replacing Your ORM With Relational Algebra</title><link>https://youtu.be/SKXEppEZp9M?si=wccXwllXm-0M-zOO</link><author>/u/JohnyTex</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 18:39:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>FFmpeg School of Assembly Language</title><link>https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md</link><author>/u/mitousa</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 18:00:07 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Interpreting Deep Neural Networks: Memorization, Kernels, Nearest Neighbors, and Attention</title><link>https://medium.com/@thienhn97/interpreting-deep-neural-networks-memorization-kernels-nearest-neighbors-and-attention-6bf0cefc7619</link><author>/u/ThienPro123</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 17:15:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This means that our positive kernel is actually some inner product of a Hilbert space. Typically, Mercer’s theorem is used for the kernel trick where we can map our input data to richer feature spaces that are potentially infinite dimensional (e.g. Gaussian kernel, polynomial kernel, etc.). However, in our case, we will use it to interpret the other way around.Note the following relationship between distance in the Hilbert space and the kernel function:This means that the closer x is to y in H , the more similar they will be in our similarity measure. So our intuition of the similarity measure being related to some form of distance is formalized by the relationship above.Learned kernel instead of fixing a kernel a prioriIf something within our prediction model is not learnable, then it is a prior that we are imposing on the dataset and the problem.In our previous discussions of soft-kernelized NNs, the kernel K is fixed, meaning that we have some prior on the geometry of the data. That is not always desirable and we want our methods to automatically learn the structure of the data rather than us manually imposing this geometry.Hence, if we want to learn the kernel K instead of imposing a prior fixed kernel, we can write (due to Mercer’s theorem):for some parameterized feature map ψ : X → H from the data space X to some Hilbert space H. Typically, H will just be R^n or the dimension of the latent space. We can then learn the parameters of ψ via standard training techniques (i.e. gradient descent on some loss).This view allows us to connect standard deep learning (or representation learning) to kernel learning.Interpreting attention as soft nearest neighborsNote that the soft kernelized nearest neighbor that we presented earliercan be interpreted as the popular attention mechanism that is ubiquitous today in LLMs and LVMs via the transformers architecture.If we interpret x as some token, e.g. x_c, in the sequence (x_1, …, x_n), K(x_i , x) as the attention dot product i.e.and setting W_{ci} as the normalized values for token at time i i.e.then we would recover the attention computation (bidirectional or autoregressive depending on whether we set the W_{ci} = 0 for i < c) as being the weighted average of the values of other tokens in the sequence.The representer theorem states that there exists an optimal linear solution that lies in the span of the training data. We shall call span (ψ(x_1), …, ψ (x_n)) the training (examples) feature span.]]></content:encoded></item><item><title>i made a list of Tech EU tech projects! for users interested in privacy and sustainability</title><link>https://github.com/uscneps/Awesome-European-Tech</link><author>/u/uscnep</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 17:10:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I can play a game that wasn&apos;t meant to run on my PC, using Linux</title><link>https://www.reddit.com/r/linux/comments/1ivn4kr/i_can_play_a_game_that_wasnt_meant_to_run_on_my/</link><author>/u/Vousch</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 16:52:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gitoxide in February</title><link>https://github.com/GitoxideLabs/gitoxide/discussions/1855</link><author>/u/ByronBates</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 16:47:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I made an MMORPG playable with an API. Use any programming language to control your characters with the API.</title><link>https://www.artifactsmmo.com/</link><author>/u/Muigetsu</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 16:24:00 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>windows firewall for local Go web app</title><link>https://www.reddit.com/r/golang/comments/1ivmbtd/windows_firewall_for_local_go_web_app/</link><author>/u/jeevanism</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 16:18:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Every time I start my Go web app locally on Windows, I get a firewall error (see screenshot). The Windows Firewall blocks it, and I have to manually allow access. Why does this keep happening? Is there a way to fix this permanently?NB : I am unable to attach the screenshot here :( ]]></content:encoded></item><item><title>How to implement dynamic storage provisioning for onPrem cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/</link><author>/u/Impossible_Nose_2956</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 15:43:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.I dont want to handle PVs for my onPrem clusters.What options are available?Do let me know if my understanding of things is wrong anywhere. ]]></content:encoded></item><item><title>DSBG, an open-source static site generator built with Go</title><link>https://www.reddit.com/r/golang/comments/1ivl3o0/dsbg_an_opensource_static_site_generator_built/</link><author>/u/tarjano</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 15:24:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This is my first big project built in Go, primarily to see if I could be as productive with it as I am with Python. I wanted to tackle a non-trivial project, so I aimed to include most of the functionality I envisioned for this type of tool. Here's what DSBG offers: Works with both Markdown and HTML source files.Automatic Tagging & Filtering: Tags are generated from paths, with built-in tag filtering.Client-Side Fuzzy Search: Provides fast search over all content within the browser.Automatic RSS Feed Generation: Easily create RSS feeds for your blog.Watch Mode with Auto-Rebuild: For continuous feedback during development. Includes 3 different themes, with the option to add your own custom CSS. For major social networks. Easily add analytics, comments, and more.The code might not be perfectly idiomatic, so any tips, suggestions, and feedback are very welcome!]]></content:encoded></item><item><title>What is Saga Pattern in Distributed Systems?</title><link>https://newsletter.scalablethread.com/p/what-is-saga-pattern-in-distributed</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:43:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[data consistencyThe Saga pattern is a design pattern that helps manage transaction updates across multiple services by breaking them down into a sequence of small local transactional updates, called "saga steps" or "subtransactions." Each step represents a unit of work that interacts with a single service. Once a step is completed, it triggers the next step in the sequence. If any step fails, the saga executes compensating updates to undo the changes made by the previous steps, ensuring that the system returns to its initial state.There are two main approaches to implementing the Saga Pattern: Orchestration and Choreography.In this approach, a central orchestrator service coordinates the saga steps. The orchestrator tells each service when to execute its local transaction. It maintains the state of the saga and handles any failures by invoking compensating transactions. The orchestrator knows the entire saga flow.The client initiates the saga by communicating with the orchestrator. The orchestrator then invokes the first service. Upon successful completion, the orchestrator moves to the next step, invoking the corresponding service. If a service fails, the orchestrator triggers compensating transactions in reverse order.In the Choreography approach, there is no central coordinator. Instead, each service involved in the saga knows its role and communicates with the other services through events or messages. Each service listens for specific events and performs local transactions when the appropriate event is received. The saga flow is distributed across the services.The client initiates the saga by communicating with the first service. This service performs its transaction and publishes an event. Other services, listening for this event, perform their respective transactions and publish their events. This chain reaction continues until the saga is complete. If a service fails, it publishes a compensating event, triggering other services to execute compensating transactions.Choreography has no single point of failure, as each service manages its part of the saga.Orchestration provides simplified error handling and monitoring with centralized control. In contrast, each service needs to handle its errors in Choreography, which can lead to complex error-handling logic.In Orchestration, the coordinator needs to know about all the services involved in the saga, which can lead to tight coupling. In contrast, in Choreography, services need to agree on the events and the order of transactions, which can lead to overhead in coordination.If you enjoyed this article, please hit the ❤️ like button.If you think someone else will benefit from this, then please 🔁 share this post.]]></content:encoded></item><item><title>[R] Calculating costs of fine tuning an Vision Language Model</title><link>https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/</link><author>/u/thekarthikprasad</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:21:21 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hello guys, I need help in calculating the cost of fine-tuning a VL model. My image dataset is of size 80+gb (https://huggingface.co/datasets/RussRobin/SpatialQA) The VL model is InternVL's 2B model I am confused about whether to do a full parameter/QLoRA Finetuning. I can't spend more on this, but wish to check the results.If so I could, what would be the cost estimate, also how to estimate cost in general Can I sample the dataset, if it breaks my cost bound and still see the results? Also do suggest the best and cheapest compute platform for my case. Thanks in advance.]]></content:encoded></item><item><title>Rustaceans, What are your thoughts on Gleam?</title><link>https://www.reddit.com/r/rust/comments/1ivjcus/rustaceans_what_are_your_thoughts_on_gleam/</link><author>/u/nikitarevenco</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:00:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've been writing Rust for a couple months. I absolutely love its monads like Result and Option, pattern-matching, private-by-default, the friendly compiler and its type system. I took a quick look at Gleam and it seems to have those features as well. Its syntax heavily reminds me of Rust's, the major distinction is that Gleam is much higher level (No lifetimes, for example), and also it is a purely functional language. It is still relatively new.For those who have tried it, what do you think about it? Are there situations where you will prefer Gleam over Rust and why. ]]></content:encoded></item><item><title>Almost everyone is under-appreciating automated AI research</title><link>https://www.reddit.com/r/artificial/comments/1ivja6c/almost_everyone_is_underappreciating_automated_ai/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 13:57:12 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SystemV Filesystem Being Removed From The Linux Kernel</title><link>https://www.phoronix.com/news/Removing-SystemV-Filesystem</link><author>/u/unixbhaskar</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 13:12:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>anyone tried kro for kubernetes resource management yet?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/</link><author>/u/AnnualRich5252</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 12:39:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it’s smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.it’s worth noting kro isn’t trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.looking forward to hearing your thoughts!]]></content:encoded></item><item><title>The Efficiency Paradox: How to Save Yourself &amp; the World • Holly Cummins</title><link>https://youtu.be/dU_WHead0oY</link><author>/u/goto-con</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 12:29:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>API Application Monitoring - OpenTelemetry? Or something else?</title><link>https://www.reddit.com/r/golang/comments/1ivhm7y/api_application_monitoring_opentelemetry_or/</link><author>/u/_nullptr_</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 12:25:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am writing a few different gRPC and HTTP (via gRPC Gateway) API servers for various heavy financial compute/IO operations (trading systems and market data). I am doing this as a single developer. These are mostly for me as a hobbyist, but may become commercial/cloud provided at some point with a nice polished UI frontend.Given the nature of the applications, I want to know what is "going on" and be able to troubleshoot performance bottlenecks as they arise, see how long transactions take, etc. I want to standardize the support for this into my apiserver package so all my apps can leverage and it isn't an afterthought. That said, I don't want some huge overhead either, but just want to know the performance of my app when I want to (and not when I don't). I do think I want to instrument with logs, trace and metrics after thinking what each would give me in value.Right now I am leaning towards just going full OpenTelemetry knowing that it is early and might not be fully mature, but that it likely will over time. I am thinking I will use stdlib  for logs with Otel handler only when needed else default to basic stdout handler. Do I want to use otel metrics/tracing directly? I am also thinking I want these others sent to a  handler by default (even stdout is too much noise), and only to a collector when configured at runtime. Is that possible with the Go Otel packages? Does this seem like the best strategy? How does stdlib  play into this? or doesn't it? Other ideas?]]></content:encoded></item><item><title>What&apos;s a good combination of tools to get a proper application observation solution together?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/</link><author>/u/tofagerl</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 12:03:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I work for a company with tons of k8s clusters, but they haven't really got the whole "let's provide all the benefits of this to the product teams" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. So now I'd like to pressure the infra teams to provide something better for us, but I can't just say "use Anthos Service Mesh", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. ]]></content:encoded></item><item><title>OpenAI bans Chinese accounts using ChatGPT to edit code for social media surveillance</title><link>https://www.engadget.com/ai/openai-bans-chinese-accounts-using-chatgpt-to-edit-code-for-social-media-surveillance-230451036.html</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 11:24:23 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[OpenAI has banned the accounts of a group of Chinese users who had attempted to use ChatGPT to debug and edit code for an AI social media surveillance tool, the company . The campaign, which OpenAI calls Peer Review, saw the group prompt ChatGPT to generate sales pitches for a program those documents suggest was designed to monitor anti-Chinese sentiment on X, Facebook, YouTube, Instagram and other platforms. The operation appears to have been particularly interested in spotting calls for protests against human rights violations in China, with the intent of sharing those insights with the country's authorities."This network consisted of ChatGPT accounts that operated in a time pattern consistent with mainland Chinese business hours, prompted our models in Chinese, and used our tools with a volume and variety consistent with manual prompting, rather than automation," said OpenAI. "The operators used our models to proofread claims that their insights had been sent to Chinese embassies abroad, and to intelligence agents monitoring protests in countries including the United States, Germany and the United Kingdom."According to Ben Nimmo, a principal investigator with OpenAI, this was the first time the company had uncovered an AI tool of this kind. "Threat actors sometimes give us a glimpse of what they are doing in other parts of the internet because of the way they use our AI models," Nimmo told .Much of the code for the surveillance tool appears to have been based on an open-source version of one of Meta's . The group also appears to have used ChatGPT to generate an end-of-year performance review where it claims to have written phishing emails on behalf of clients in China."Assessing the impact of this activity would require inputs from multiple stakeholders, including operators of any open-source models who can shed a light on this activity," OpenAI said of the operation's efforts to use ChatGPT to edit code for the AI social media surveillance tool.Separately, OpenAI said it recently banned an account that used ChatGPT to generate social media posts critical of , a Chinese political scientist and dissident who lives in the US in exile. The same group also used the chatbot to generate articles in Spanish critical of the US. These articles were published by "mainstream" news organizations in Latin America and often attributed to either an individual or a Chinese company.]]></content:encoded></item><item><title>I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?</title><link>https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/</link><author>/u/sabir8992</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 11:13:52 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please   submitted by    /u/sabir8992 ]]></content:encoded></item><item><title>Distributed system courses in Rust?</title><link>https://www.reddit.com/r/rust/comments/1ivgbko/distributed_system_courses_in_rust/</link><author>/u/FeelingAttempt55</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 11:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am currently following the pingcap course to learn distributed systems with Rust. So far, I am really enjoying the course, but the course is 5 years old, could you guys suggest some other project-based and more up-to-date courses? ]]></content:encoded></item><item><title>Dockerize GO environment with only go.mod and go.sum and no source code</title><link>https://www.reddit.com/r/golang/comments/1ivfxtb/dockerize_go_environment_with_only_gomod_and/</link><author>/u/muthunatsharma</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:32:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I need a docker container which has go packages downloaded, installed and compiled as mentioned in go.mod and go.sum. All the articles show how to do it but the install/compile actually happens only when the source-code is copied in to the container and "go build" is run in the dockerfile.I see "go download" downloads all pkgs in go.mod to /go/mod/pkg. How do I install these? I can give "go install <pkg>" but that would mean I need to update my Dockerfile each time a new pkg is added to go.mod.What is the one-shot way of installing it in the dockerfile build?Edit: The context is to build a dev container where deps are pre-built saving time when code is mounted on the container and built -- this is the main point to save time. The container wouldn't have the app itself. Only the dependencies fully installed and serve as a standard environment to run.]]></content:encoded></item><item><title>godoc.nvim - Golang docs inside Neovim!</title><link>https://www.reddit.com/r/golang/comments/1ivfv16/godocnvim_golang_docs_inside_neovim/</link><author>/u/ffredrikk</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:27:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PeaZip 10.3.0 released!</title><link>https://www.reddit.com/r/linux/comments/1ivfn9y/peazip_1030_released/</link><author>/u/peazip</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 10:11:14 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Thought We Had Our EKS Upgrade Figured Out… We Did Not</title><link>https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/</link><author>/u/rohit_raveendran</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 22 Feb 2025 09:43:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[You ever think you’ve got everything under control, only for prod to absolutely humble you? Yeah, that was us.Lower environments? ✅ Tested a bunch.Version mismatches? ✅ All within limits.EKS addons? ✅ Using the standard upgrade flow.So we run Terraform on upgrade day. Everything’s looking fine—until kube-proxy upgrade just straight-up fails. Some pods get stuck in  Great.Cool, thanks, very helpful. We hadn’t changed anything on kube-proxy beyond the upgrade, so what the hell?At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can’t leave it like that.And then we find it: a tiny note in the AWS docs added just a few days ago. Turns out, kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions (link).And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom—everything worked.You’re never actually prepared. We tested everything, but something always slips through. The real test is how fast you fix it.Karpenter is great, but don’t let it go rogue. We’re now explicitly blocking unsupported instance families.Anyway, if you guys have ever had one of those “we did everything right, and it still blew up” moments, drop your stories. Misery loves company.]]></content:encoded></item><item><title>Pewdiepie uses linux mint</title><link>https://www.reddit.com/r/linux/comments/1ivf01w/pewdiepie_uses_linux_mint/</link><author>/u/RedDevilVortex</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 09:25:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I created a fairly extensive cheat sheet for scripting Sieve mail filters. Here&apos;s a link to the Gist if anyone is interested.</title><link>https://gist.github.com/Hotrod369/6b7a24e1ea060e48e0c02459cbb950a0</link><author>/u/StinkyPete312</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 09:01:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This is a minimalist 2-click MSI installer generator for your projects for Windows. Magic works as all you need is to populate _configMSI.yml with your own values, then click 2 bat or sh files (if you use MS Visual Studio or MSYS2/MINGW64). And voila, your MSI Installer is ready!</title><link>https://github.com/windows-2048/Magic-MSI-Installer-Template</link><author>/u/florida-haunted</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 08:51:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pixerise v0.12 Release: Python High-Performance 3D Renderer Adds Ray Casting, 1/z Depth Interpolation, and Group Management with Improved Architecture</title><link>https://github.com/enricostara/pixerise</link><author>/u/jumpixel</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 08:47:55 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Confused about &quot;NEW&quot; Rust feature in - Closures in async functions</title><link>https://www.reddit.com/r/rust/comments/1ivdmek/confused_about_new_rust_feature_in_closures_in/</link><author>/u/DataBora</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 07:45:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am reading how new Rust feature is comming for using closures in Async functions. || async whaterver...In my Elusion library implementation i have PipelineScheduler function signature: ```rust pub async fn new<F, Fut>(frequency: &str, job: F) -> ElusionResult<Self> where F: Fn() -> Fut + Send + Sync + 'static, Fut: Future<Output = ElusionResult<()>> + Send + 'static ``` and then for Job creation:```rust let job = Job::new_async(&cron, move |uuid, mut l| { let job_fn = job_fn.clone(); Box::pin(async move { let future = job_fn(); future.await.unwrap_or_else(|e| eprintln!("❌ Job execution failed: {}", e)); let next_tick = l.next_tick_for_job(uuid).await; match next_tick { Ok(Some(ts)) => println!("Next job execution: {:?} UTC Time", ts), _ => println!("Could not determine next job execution"), } }) }).map_err(|e| ElusionError::Custom(format!("❌ Job creation failed: {}", e)))?; ``` which user can use like this:let scheduler = PipelineScheduler::new("5min", || async {}) How this new feature will be different?]]></content:encoded></item><item><title>[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering</title><link>https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 07:02:41 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[A new evaluation benchmark tests language models across 285 graduate-level disciplines using an iterative human-AI collaborative approach to generate and validate questions. The methodology combines expert review with model-assisted filtering to ensure high-quality, discipline-appropriate assessment.Key technical points: - Uses a two-stage question generation process: initial AI generation followed by expert review - Implements collaborative filtering where both human experts and LLMs help identify and remove problematic questions - Covers disciplines from traditional academia to specialized industrial fields - Tests both factual knowledge and reasoning capabilities - Evaluated on multiple leading LLMs including GPT-4, Claude 2, and DeepSeekResults: - Best performance: DeepSeek-R1 at 61.82% accuracy - Significant variance in performance across different disciplines - 80+ expert annotators involved in validation - Generated dataset of 2,855 validated questionsI think this benchmark addresses a critical gap in LLM evaluation by going beyond common academic subjects. The methodology of combining human expertise with AI assistance for question validation could be valuable for developing future evaluation datasets.I think the relatively modest performance (62%) on graduate-level questions across diverse fields suggests current LLMs still have significant room for improvement in specialized domains. This could influence how we approach model training and evaluation for domain-specific applications.TLDR: New benchmark tests LLMs across 285 graduate disciplines using human-AI collaborative question generation. Best model achieved 62% accuracy, revealing gaps in specialized knowledge.]]></content:encoded></item><item><title>This Week in Plasma: Refinements All Around</title><link>https://blogs.kde.org/2025/02/22/this-week-in-plasma-refinements-all-around/</link><author>/u/gabriel_3</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 06:53:35 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Welcome to a new issue of "This Week in Plasma"! Every week we cover as much as possible of what's happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.This week, we've been rapidly fixing the bugs that people found in Plasma 6.3, as well as some older bugs as well. In addition to that, some smaller UI improvements have started to trickle in! There's some larger work in progress too, but not merged yet. Have a look at what did merge this week:Improved the weather widget's display of search results from the BBC weather service to reduce unhelpful visual noise. (Ismael Asensio, link)Eliminated the visual difference between how Night Light looks on Wayland compared to on X11. (Xaver Hugl, link)The Digital Clock widget's context menu is now less cluttered with things you're not likely to use. (Nate Graham, link)Rephrased some settings on System Settings' General Behavior page to be clearer about what it is that they actually do. (Nate Graham, link)Improved the accessibility of the Widget Explorer sidebar. (Christoph Wolk, link)Fixed the issue mentioned last week where KWin built with LTO on GCC 15 could show a black screen on login when using an ICC profile; we found a way to restructure the code that avoids the issue. (Vlad Zahorodnii and Xaver Hugl, link)Fixed a case where Plasma could crash when you tried to access the Properties dialog for a file in the Recently or Frequently Used file lists in the Kickoff Application Launcher. (Nicolas Fella, link)Fixed a regression that caused the volume change OSD to fail to appear when adjusting the volume with the integrated volume buttons of a Bluetooth headset. (David Redondo, link)Fixed a regression that caused the + clipboard popup to lose its visual highlights when navigated by keyboard. (Christoph Wolk, link)Fixed an issue in KWin that caused the new "Prefer efficiency" option when using an ICC profile to not actually be very efficient on some hardware, and another one that broke Night Light while using the "Prefer color accuracy" setting. (Xaver Hugl, link 1 and link 2)Taking screenshots on Wayland in FreeBSD now works. (Vlad Zahorodnii, link)Fixed a few bugs in the Color Picker widget, such as the shortcut option not working, and the tooltip not looking correct in certain circumstances. (Christoph Wolk, link 1 and link 2)Fixed a bug with the Task Manager widgets that broke the ability to move the pointer diagonally to a tooltip without dismissing it by accident while using a right-to-left language like Arabic or Hebrew. (Christoph Wolk, link)Made several improvements and fixes for keyboard navigation in the Kicker Application Menu widget. (Christoph Wolk, link 1, link 2, and link 3)Fixed a regression that caused desktop icons selected by dragging a box around them to become inappropriately deselected if the pointer ended right over one of the icons when releasing the mouse button. (David Edmundson, link)Fixed a regression that caused the automatic tablet mode feature to accidentally get blocked on certain types of devices, but only when using the feature to re-bind mouse buttons. (Vlad Zahorodnii, link)Fixed a bug that caused the desktop and panels to go missing when applying a new Global Theme and using the option to replace the existing layout. This also fixed a bug that caused deleted widgets to not be deleted from the plasma-org.kde.plasma.desktop-appletsrc config file. (Marco Martin, link 1 and link 2)Fixed a set of subtle bugs in the implementation of the new "prefer symbolic icons" behavior of the System Tray that caused it to actually do the opposite, showing you colorful icons instead! (Nate Graham and David Redondo, link 1 and link 2)Extremely long weather station names no longer overflow and break the widget popup's layout. (Ismael Asensio, link)The inline file renaming text field on the desktop is now colored correctly when using a mixed light/dark setup, as with Breeze Twilight. (Evgeniy Harchenko, link)Limited the Power Management setting "Change screen brightness" to only take effect for built-in screens on battery-powered systems (e.g. laptops), which avoids certain timing-related brightness bugs for external monitors and makes the settings page less confusing. (Jakob Petsovits, link)Fixed an issue that could cause user switching from KRunner to behave strangely and eventually cause a crash. (David Edmundson, link)Fixed an older regression that broke the "highlight non-default settings" features for pages in System Settings written using QtWidgets. The fact that this was overlooked for so long goes to show how few are left these days! (David Redondo, link)Switched KWin's render loop initialization code to use a more precise type of timer that should reduce frame drops. (Apostolos Dimitromanolakis, link)When the  daemon crashes, now it automatically restarts itself in the background. (Bryan Liang, link)KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.You can help KDE by becoming an active community member and getting involved somehow. Each contributor makes a huge difference in KDE — you are not a number or a cog in a machine!You don’t have to be a programmer, either. Many other opportunities exist:You can also help us by making a donation! Any monetary contribution — however small — will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.Enter your email address to follow this blog and receive notifications of new posts by email.]]></content:encoded></item><item><title>I Made a Configurable Rate Limiter… Because APIs Can’t Say ‘Chill’</title><link>https://beyondthesyntax.substack.com/p/i-made-a-configurable-rate-limiter?r=4jgehp&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;triedRedirect=true</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 06:24:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I have made a pong game in C++ using raylib. So can anyone plz give suggession where I can improve the game and my code?</title><link>https://github.com/EthicalAniruddha/AI-Pong</link><author>/u/Ethical_Aniruddha</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 06:01:42 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>One-Minute Daily AI News 2/21/2025</title><link>https://www.reddit.com/r/artificial/comments/1ivbt3a/oneminute_daily_ai_news_2212025/</link><author>/u/Excellent-Target-847</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 05:45:10 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>Generic Bitfield I had fun implementing</title><link>https://gist.github.com/oplanre/de0bba4f1e2f769458ca1adff7f12280</link><author>/u/ln3ar</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 04:12:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Announcing async-local 3.0 now with async closure support</title><link>https://www.reddit.com/r/rust/comments/1iv8o6v/announcing_asynclocal_30_now_with_async_closure/</link><author>/u/Jester831</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 02:45:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Async-local enables thread locals to be used in an async context across await points or within blocking threads managed by the Tokio runtime without the overhead of `Arc`. The way this is accomplished is by using generativity to create unique invariant lifetimes so that borrows to TLS can't be coerced to a `&'static` lifetime and by configuring the runtime with a barrier to rendezvous worker threads during shutdown. This shutdown barrier makes it such that runtime tasks never outlive TLS data owned by worker threads; this makes every invariant lifetime guaranteed to be valid until no tasks remain. Blocking threads managed by the Tokio runtime cannot outlive worker threads with this configuration, and so pointers to TLS from worker threads can be safely moved to these blocking threads with the lifetime constrained. As the lifetimes cannot be coerced into `&'static`, moving onto other threads is prevented. This crate downgrades to using `Arc` whenever the `barrier-protected-runtime` feature is not enabled, making it the end users choice to opt into this optimization by using async_local to configure the runtime shutdown barrier. ]]></content:encoded></item><item><title>from nodejs want to move to golang</title><link>https://www.reddit.com/r/golang/comments/1iv7ngg/from_nodejs_want_to_move_to_golang/</link><author>/u/Spirited-Item1431</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 01:53:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I used to be a web developer who used Node.js as my daily programming language, but now I'm interested in switching to Golang. Aside from the usual fundamentals, what are the most important things to learn in Golang?]]></content:encoded></item><item><title>Ring is unmaintained</title><link>https://rustsec.org/advisories/RUSTSEC-2025-0007.html</link><author>/u/technobicheiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:44:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[This advisory has been withdrawn and should be ignored. It is kept only for reference.The author has announced an indefinite hiatus in its development, noting that
any reported security vulnerabilities may go unaddressed for prolonged periods
of time.After this advisory was published, the author graciously agreed to give
access to the rustls team. The rustls team is committed to providing
security (only) maintenance for  for the foreseeable future.Advisory available under CC0-1.0
    license.

    
    ]]></content:encoded></item><item><title>[P] Decensor AI models Qwen/Deepseek by finetuning with non political data</title><link>https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/</link><author>/u/Ambitious_Anybody855</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:30:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[The best way to decensor a DeepSeek model? Don’t try to decensor it.Fine-tuned OpenThinker on OpenThoughts-114k, a dataset focused on reasoning tasks like math, coding, and graduate-level Q&A, with no political content. Despite using censored base models (Qwen), the fine-tuned OpenThinker-7B and OpenThinker-32B models became decensored without any explicit intervention. Unlike Perplexity, no custom fine-tuning was applied to remove censorship, yet the results remain uncensored. It challenges assumptions about model safety and opens exciting new research directions. AI game is so on]]></content:encoded></item><item><title>Firefox&apos;s HEVC support for Linux (via VA-API) coming in Firefox 137</title><link>https://www.reddit.com/r/linux/comments/1iv6bhi/firefoxs_hevc_support_for_linux_via_vaapi_coming/</link><author>/u/neks101</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:29:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Windows got support in Firefox 134, MacOS on the Firefox beta build 136, and Linux will be on the Firefox nightly with 137. Looks like all OS will be supported by 137!]]></content:encoded></item><item><title>First time on Linux, 3 gig ram and works like a rocket lol</title><link>https://www.reddit.com/r/linux/comments/1iv5tas/first_time_on_linux_3_gig_ram_and_works_like_a/</link><author>/u/SnooOpinions7428</author><category>linux</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:05:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Lightweight Real-Time System Stats for VS Code</title><link>https://marketplace.visualstudio.com/items?itemName=odangoo.otak-monitor</link><author>/u/Wise_Bug47</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 23:39:37 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A lightweight system monitor for VS Code - Track CPU, memory, and disk usage with efficient 5-second updates and 1-minute averages.Find the system monitor in your VS Code status barView CPU usage percentageHover to see detailed current and average metricsotak-monitor is a lightweight VS Code extension that helps you monitor system resources without leaving your editor.Status bar display of CPU usage percentageAggregated across all CPU coresPrecise to one decimal placeCurrent CPU clock speed (MHz)Detailed memory informationShows used and total memory in MBCross-platform disk space monitoring
Windows: C: drive (home directory in Codespaces)Linux: Root filesystem (workspace root in Codespaces)Shows used and total space in GBClean status bar integrationDetailed hover tooltip showing:
Current CPU, memory, and disk metricsVisual Studio Code ^1.90.0Supported environments:
Local: Windows, macOS, LinuxRemote: GitHub CodespacesInstall the extension from VS Code MarketplaceLook for the CPU usage display in your status barHover over it to see detailed system informationThe extension shows the following information in your status bar:With a detailed tooltip showing:Current:
CPU Usage: 45.3% (2400 MHz)
Memory Usage: 1024 MB / 2048 MB (50.0%)
Disk Usage: 150 GB / 500 GB (30.0%)
Note: For disk usage, the monitored path varies by environment:Windows:
Codespaces: Home directoryLinux:
Local: Root filesystem (/)Codespaces: Workspace rootCPU usage is calculated by comparing idle and total CPU time differencesMemory values are shown in MB and percentageDisk usage monitoring adapts to the environment:
Local machines: Monitors system root or C: driveCodespaces: Monitors relevant workspace pathsMoving averages are calculated using 12 data points (5-second intervals over 1 minute)Updates occur every 5 seconds for efficient monitoringMinimal performance impact on the systemGitHub Codespaces SupportThe extension automatically detects when running in GitHub Codespaces and adjusts its behavior:Monitors the workspace root directory in Linux environmentsUses home directory for Windows-based CodespacesMaintains consistent monitoring experience across all environmentsProvides accurate disk usage information for containerized developmentContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.This project is licensed under the MIT License - see the LICENSE file for details.]]></content:encoded></item><item><title>Best way to develop talos locally?</title><link>https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/</link><author>/u/obviouslyGAR</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 23:19:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am currently learning and building a cluster using talos.One thing I want to know is how are you all developing locally? Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?   submitted by    /u/obviouslyGAR ]]></content:encoded></item><item><title>I made an AirDrop server that uses URL Requests to accept data from anywhere</title><link>https://github.com/gnhen/SkyDrop</link><author>/u/GranttH</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 23:06:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust Rant Contest: std::io::Error, the oversized junk drawer of failure</title><link>https://www.reddit.com/r/rust/comments/1iv3rb3/rust_rant_contest_stdioerror_the_oversized_junk/</link><author>/u/OliveTreeFounder</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 22:33:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've been coding in Rust for five years, and  has never been anything but a headache. The error code? Never useful. It’s impossible to handle—too big, too vague—so we all end up just passing this bloated mess back to the caller without even knowing what’s inside or what actually caused the error.But it gets worse. Traits, instead of being parameterized over an  type, just return Result<..., std::io::Error>. Once a trait like this becomes popular—like  or —you're stuck. You can’t handle errors properly unless you rewrite every crate that depends on these traits. is a contagious disease infecting the entire ecosystem. We need to stop this pandemic!]]></content:encoded></item><item><title>Matrix.org bridges to shut down in 1 month unless $100k can be raised</title><link>https://matrix.org/blog/2025/02/crossroads/</link><author>/u/Evidlo</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 21:44:53 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[After a successful 2024 with a lot to be proud of, and a Matrix Conference that brought our community together to celebrate 10 years of Matrix, we step into 2025 with a light budget and a mighty team poised to make the most of it!Our priorities remain to make Matrix a safer network, keep growing the ecosystem, make the most of our Governing Board, and drive a fruitful and friendly collaboration across all actors.However, whether we will manage to get there is not fully a given.🔗The Foundation is key to the success of MatrixThe Matrix.org Foundation has gone from depending entirely on Element, the company set up by the creators of Matrix, to having half of its budget covered by its 11 funding members, which is a great success on the road to financial independence! However half of the budget being covered means half of it isn’t. Or in other words: the Foundation is not yet sustainable, despite running on the strictest possible budget, and is burning through its (relatively small) reserves. And we are at the point where the end of the road is in sight.The Matrix.org Foundation exists to act as a neutral  and to nurture it as efficiently as possible as a single unfragmented standard, for the greater benefit of the whole ecosystem, not benefiting or privileging any single player or subset of players.Without the Foundation and its programs, the Matrix protocol itself faces existential threats:Without Trust & Safety efforts, bad actors and communities would proliferate on the network and make it unlivable for the rest.Without a canonical specification, the shared infrastructure and a Spec Core Team to maintain it, the protocol would become fragmented, losing its effective interoperability – increasing the costs on all downstream users.Without a neutral entity as the custodian of the specification, the ecosystem would first shatter and then consolidate around the biggest (likely for-profit) actor.Without advocacy, conferences, documentation and tutorials, Matrix would become a niche protocol used by a few enthusiasts for side projects, whilst big proprietary and siloed networks continue to hold the world’s communications.But there is light at the end of the tunnel! Concretely, the Foundation delivers most of its value by fostering a healthy, fair and fertile ecosystem around Matrix. It needs to strike the right balance between:Making Matrix accessible & visible.For the general public it means maintaining an easy default onboarding server (Matrix.org).For server administrators it means providing the right tooling to keep their users (and themselves!) safe.For developers it means making it easy to develop products using Matrix, via documentation, tutorials, and in-person events.Making Matrix compelling to build on.This means maintaining the Matrix Specification as a canonical, unencumbered, patent free and royalty free specification.Being responsive and vendor-neutral when an organisation or individual contributes.Promoting the good players within the ecosystem.Ensuring the network grows and attracts more users.Making Matrix a product that benefits the greater good.This means ensuring that the general public can easily build safe & easy to use communities on Matrix.Ensuring that bad actors are proactively chased and discouraged to use Matrix.Matrix has been here for 10 years, and will hopefully be here for many more! But to continue to grow and thrive, it needs the Foundation to be around and healthy, which means carefully allocating its budget in order to continue to exist and fulfill its mission. This is why it needs to focus on critical programs and shut down some of its activities.We view the following programs as critical to the Foundation’s mission:Maintaining the canonical, backwards compatible, stable Matrix SpecDeveloping protocol enhancements and Trust and Safety tooling, making the tools available to the ecosystem and moderating the servers under its control (typically Matrix.org) - see our recent blog postRunning the Matrix.org homeserver as an initial home for newcomersPromoting the Matrix protocol via online content, conferences and meet-ups and other marketing strategiesWe might fine tune our approach, but we can't cease any of those programs without severe consequences for the ecosystem.Meanwhile, bridges have been at the heart of Matrix for a long time. Public bridges hosted by the Matrix.org Foundation have been a very good resource to show the power of interoperability, connect communities together, and onboard many people into their Matrix journey.However, these bridges require regular maintenance as the bridged platforms evolve their APIs, and significant engineering and moderation support to run. Luckily, the Matrix ecosystem is now more mature than it was at the time we spun up those public Slack, XMPP and IRC bridge instances. There are now commercial players like Beeper providing a user-friendly offering for people who want to get all their conversations in a single app, or IndieHosters and Fairkom offering hosting for Matrix server and bridge instances (and much more).So unless the Foundation manages to raise $100,000 of funding by the end of March 2025, we will have to focus our resources on the critical lines of work, and consequently we will have to shut down all the remaining bridges hosted by the Matrix.org Foundation. This includes bridges to Slack, XMPP, OFTC (IRC), and Snoonet (IRC). We will also mark the software behind those bridges as archived, as we don't have the resources to accept new contributions.In practice, the Foundation needs an additional $610K in revenue to break-even, but this $100K would extend our runway 1 month while we work on landing grants and new members. To put this in context, we nearly doubled our revenue in 2024, reaching $561K, but it was also the first year in which we carried the full cost of our operations: $1.2M. To make ends meet, we liquidated $283K worth of cryptocurrency donations and ended the year with a $356K deficit. We are currently on target for $587K revenue in 2025, with a modest increase in expenses.🔗Growing the ecosystem and the networkChoosing to shut the bridges down is a difficult decision to make, but will allow us to focus on the critical projects which will keep the ecosystem growing. The success of Matrix depends on how widely it is used by the general public and by organisations – preferably natively rather than via bridges.The more people and organisations rely on Matrix, the more attractive it becomes for organisations to build products and services on top of it, the more funding the Foundation gets, and the more the Foundation can in turn reinvest into the ecosystem and run initiatives that benefit all stakeholders for the growth of the network.Once the Foundation is cashflow positive, it will be able to accelerate and eventually get on with the multiple projects the team and Governing Board have in mind to make Matrix fun, exciting, reliable, safe, easy to use, and above all useful. And we hope to get there by the end of the year.Most importantly, despite the Trust and Safety team being the Foundation’s biggest expense, as explained in our blog post, the team is still underresourced: they are understaffed and under a lot of pressure to deliver protocol improvements, better tooling for server admins, and ensure Matrix.org is a good citizen of the open federation. T&S will be the first area to see increased funding.Separately, the Foundation wants to continue executing on its mission! Among others, better connect the doers in the ecosystem with the people and organisations who need their energy, share the successes and learnings from the community: the Matrix Conference was an incredible success and we want to see more of that.We’ve also seen a clear change in how many users and organisations were adopting Matrix in the last few months: the world needs a decentralised end-to-end encrypted network to communicate more than ever, and it shows! We want to uplift the good players which are driving this growth.There is so much more that we could do to make Matrix better and realise its full potential. Right now, the Foundation urgently needs your financial help. For the sake of a safe network, our primary focus today, but also to be able to deliver on the reason we all want Matrix to succeed.People should have full control over their own communication.People should not be locked into centralised communication silos, but instead be free to pick who hosts their communication without limiting who they can reach.The ability to converse securely and privately is a basic human right.Communication should be available to everyone as a free and open, unencumbered, standard and global network.If you are an organisation building on top of Matrix, you can help by , which also gives you the opportunity to be eligible to participate in the Governing Board, and other perks. If you are an organisation buying Matrix services or products, you can help by ensuring that your vendor is financially contributing back to the project or becoming a member yourself.If you are an individual using Matrix, you can help by .If you are a philanthropist or other funder, you can help by getting in touch with us at  to discuss funding options. It isn’t the firsttime we’ve rung the alarm bell, and it is no fun to beg for help. We are at a crossroads, where the vibrancy of the ecosystem and enthusiasm around Matrix is not reflected in the support the Foundation gets, and we are at risk of losing this common resource and all it offers.But all in all, we are optimists – we wouldn’t have begun this journey if we weren’t – and we believe that there are people out there who realise that sovereign and secure communication is as high on the list of today’s essential technology – if not higher – as ensuring AI is safe, so let’s spread the word and let’s continue working on a safer and more sovereign world!
                        The Matrix.org Foundation is a non-profit and only relies
                        on donations to operate. Its core mission is to maintain
                        the Matrix Specification, but it does much more than that.
                    
                        It maintains the matrix.org homeserver and hosts several
                        bridges for free. It fights for our collective rights to
                        digital privacy and dignity.
                    Support us]]></content:encoded></item><item><title>Reading the Source Code</title><link>https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/</link><author>/u/TopNo6605</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 20:52:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?]]></content:encoded></item><item><title>Tk9.0 canvas demo</title><link>https://opu.peklo.biz/p/25/02/21/1740170028-43ac6.png</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 20:38:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Windows to Linux, Set Up Full Disk Encryption on openSUSE</title><link>https://news.opensuse.org/2025/02/20/setup-fde-on-opensuse/</link><author>/u/gabriel_3</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 20:36:00 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Data breaches and cyber threats are becoming increasingly common and securing your personal and professional information has never been more critical.Users transitioning from Windows to Linux through the Upgrade to Freedom campaign can use openSUSE’s tools to protect sensitive data, which include full disk encryption (FDE).Full disk encryption during installation ensures maximum security. It safeguards all data on your hard drive by encrypting it and makes it unreadable without an decryption key. This level of protection is vital for preventing unauthorized access if your laptop or desktop is lost or stolen.FDE with openSUSE is both user-friendly and powerful. The setup with advanced security features is easy.For users seeking feature parity with Windows BitLocker, openSUSE offers Full Disk Encryption (FDE) secured by a TPM2 chip or a FIDO2 key. This advanced setup enhances security by storing encryption keys within the TPM, which ensures that only a trusted system configuration can unlock the disk. For a step-by-step guide on enabling this feature, read the Quickstart in Full Disk Encryption with TPM and YaST2 article.Here’s a step-by-step guide to set up FDE on your system:Step 1: Download and Boot openSUSEVisit get.opensuse.org to download the latest version of openSUSE Leap or Tumbleweed.Restart your computer and boot from the USB drive to begin the installation process.Step 2: Configure Encryption During InstallationOnce the installer starts, select your preferred language and keyboard layout.In the partitioning setup, choose Guided Setup with Encrypted LVM.Set a strong passphrase for encryption. This passphrase will be required every time the system boots.   - Use a mix of upper and lower case letters, numbers and special characters for optimal security.Proceed with the installation as directed by the installer.Step 3: Verify Encryption SettingsAfter installation is complete and the system restarts, you’ll be prompted to enter your encryption passphrase. Once entered, openSUSE tools will decrypt the disk and boot normally. To confirm encryption is active:Open a terminal or console.Run the command  to verify that your disk is listed with the encryption type (e.g., ).The output might look something similar to the following:NAME        FSTYPE      FSVER LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINT
sda                                                                                     
├─sda1      ext4        1.0     4a83v1e1-e8d2-4e38-815d-fd79j194f5   25G    30%    /
└─sda2      swap        1           d2e18c23-9w4b-4d26-p1s2-cm2sd64tx9de                
sdb                                                                                     
└─sdb1      crypto_LUKS 1           10bb2vca-81r4-418b-a2c4-e0f6585f2c7a                
  └─luks    ext4        1.0         8a9wka1b-7e9c-1a1f-a9f7-3c82x1e4e87f   150G    10%    /mnt/data
While FDE protects your data, it does not prevent data loss from hardware failure or accidental deletion. Regularly back up your data to an encrypted external drive or a secure cloud service to ensure its safety.Enhanced Security for Modern ChallengesSetting up full disk encryption on openSUSE not only protects your data but also aligns with the Upgrade to Freedom campaign’s mission of empowering users to maintain control over their hardware and privacy. By combining open-source software with good security practices, openSUSE ensures that users can confidently embrace a more secure digital future.For additional guidance and community support, visit the openSUSE forums or join discussions at your local Linux user group. Please be aware that some hardward configurations may require additional drivers or BIOS settings adjustments for full disk encryption to fully function properly. Check your device’s compatibility and update your firmware before proceeding. ]]></content:encoded></item><item><title>Streamline Kubernetes Management with Rancher</title><link>https://youtube.com/shorts/fOVTDobiwIE?feature=share</link><author>/u/abhimanyu_saharan</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 19:58:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[First crate] derive_regex: construct a type by parsing a string with regular expressions</title><link>https://www.reddit.com/r/rust/comments/1iuzg1i/first_crate_derive_regex_construct_a_type_by/</link><author>/u/TitaniumBrain</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 19:31:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I had an idea and decided it was simple enough to publish my first crate and contribute to the Rust ecosystem.I'm still relatively new to Rust (coming from a few years of Python but I fell in love with the language), so any feedback is welcome. I'm confident my code isn't , but I want to make sure I follow best practices and learn about any Rust .Using this crate - and the associated derive proc macro - you can derive  on an enum or struct to automatically derive the  constructor method.Copied from the readme, here's a couple examples if you don't to click away from Reddit:```rust use derive_regex::FromRegex;pattern = r"^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>[A-Z]+)\] (?P<message>.+)$" )] struct LogEntry { timestamp: String, level: String, message: String, }fn main() { let log = "2025-02-20 15:30:00 [INFO] Server started successfully"; let entry = LogEntry::parse(log).expect("Failed to parse log entry"); println!("Parsed log entry: {:#?}", entry); // Parsed log entry: LogEntry { // timestamp: "2025-02-20 15:30:00", // level: "INFO", // message: "Server started successfully", // } } ``````rust use derive_regex::FromRegex;enum CookingCommand { // Parses a command like "chop 3 carrots" #[regex(pattern = r"chop (?P<quantity>\d+) (?P<ingredient>\w+)")] Chop { quantity: u32, ingredient: String },// Parses a command like "boil for 10 minutes" #[regex(pattern = r"boil for (?P<minutes>\d+) minutes")] Boil(u32), // Parses a command like "bake at 375.0 degrees for 25 minutes" #[regex(pattern = r"bake at (?P<temperature>\d+\.\d+) degrees for (?P<minutes>\d+) minutes")] Bake { temperature: f64, minutes: u32 }, // Parses a command like "mix salt and pepper" #[regex(pattern = r"mix (?P<ingredient1>\w+) and (?P<ingredient2>\w+)")] Mix { ingredient1: String, ingredient2: String, }, fn main() { let commands = [ "First, chop 3 carrots", "Don't forget to boil for 10 minutes", "I guess I'll bake at 375.0 degrees for 25 minutes", "mix salt and pepper now", ];for cmd in &commands { if let Ok(command) = CookingCommand::parse(cmd) { match command { CookingCommand::Chop { quantity, ingredient, } => { println!("Chop {} {}(s)", quantity, ingredient); } CookingCommand::Boil(minutes) => { println!("Boil for {} minutes", minutes); } CookingCommand::Bake { temperature, minutes, } => { println!("Bake at {} degrees for {} minutes", temperature, minutes); } CookingCommand::Mix { ingredient1, ingredient2, } => { println!("Mix {} and {}", ingredient1, ingredient2); } } } else { eprintln!("Failed to parse command: {}", cmd); } } // Chop 3 carrots(s) // Boil for 10 minutes // Bake at 375 degrees for 25 minutes // Mix salt and pepper ]]></content:encoded></item><item><title>Interop 2025: another year of web platform improvements</title><link>https://web.dev/blog/interop-2025?hl=en</link><author>/u/feross</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 18:45:42 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
  Published: February 13, 2025
After the huge success of Interop 2024, the project returns today with a new set
of focus areas for 2025. While we couldn't include every suggestion made this
year, the final list reaches across the web platform—from CSS to
performance-related features.In addition, and as in previous years, there's a set of areas for investigation.
These are areas where we don't have enough information or tests to include as a
focus area, but the group feels some work should be done to get them to a stage
where we can include them.We're excited about all of these features and the improvements this year's
project will bring to the platform. And, as with last
year, the project will make a whole set of things
Baseline Newly available. This post shares more information about some of the
features on the list, with links to information to find out more.Several of the features included in Interop 2025 are features that you flagged
up as important in the State of CSS 2024 survey. They'll help you create more
beautiful and performant user experiences.This feature lets you anchor a positioned element to an anchor, it's
particularly useful when displaying popovers.Same-document view transitionsAlso included this year are view transitions, specifically same-document view
transitions, and the  CSS property.The  propertyThe

property has been Baseline Newly available since September 2024. It lets you
create effects behind your content. For example to blur or create effects that
you might expect to only be available in a graphics application.Despite being mostly interoperable, you can see from the failing tests for
that
there are bugs and issues in those implementations. While these issues might not
be a problem to everyone, we know that many of you do run into them, it'll be
great to get this feature working really well.The  element is a disclosure widget which can be expanded to reveal
additional content. The  element itself is Baseline Widely available.
However, there are a number of related features that have been more recently
added that make  more
useful.The  and  CSS pseudo-elements.Using  to toggle the content instead of .Auto-expanding the  element with find-in-page matches.The  attribute, which hides an element until it is found
using the browser's find-in-page search or it is directly navigated to by
following a URL fragment.The  at-rule lets you scope your selectors to a sub-tree of the DOM, or
even select between an upper and lower boundary in the tree. For example, the
following CSS only selects  elements inside an element with a class of
.In the next example, an upper and lower bound is used. The  element is
only selected if it's between the element with a class of  and also
outside of the element with a class of .Without the  event, there's no reliable way to detect that a scroll is
complete. The best you could do is to use  to check if the scroll
has stopped for a period. This makes it more like a scroll has paused event, not
a scroll has ended event.With the  event, the browser does all this difficult evaluation for
you.The  propertyThe

property is a shorthand for , ,
, and text-decoration-thickness. It's deemed Baseline
Widely available, however in Safari the only unprefixed shorthand property that
works is . It's this that will be addressed during 2025.The CSS 
property has a number of possible values, many of which are designed to lay out
scripts that display vertically. Sometimes however, you want to lay out text
vertically as part of a design, rather than for language support reasons. The
 and  values are designed for this, but have suffered
from poor browser compatibility. This should be fixed during 2025.In addition, the logical CSS properties  and 
are included. These make it possible to control what happens when content
overflows boxes, regardless of the writing mode.Web Vitals can help you quantify the
experience of your site and identify opportunities to improve. The Web Vitals
initiative aims to simplify the landscape, and help sites focus on the metrics
that matter most, the Core Web Vitals.Event Timing API (for INP)This year, the work will focus on the following features:JavaScript string builtins: to make the WebAssembly built-in string
functions mirror a subset of the JavaScript String API so it can be callable
without JavaScript glue code.Resizable buffers integration: to integrate WebAssembly into JavaScript code
that uses resizable buffers.This year the project includes a removal from the platform. Mutation
events are deprecated
and replaced with the much more performant and Baseline Widely available
Mutation Observer
API. Chrome
removed these events in Chrome 126, and this focus area is to remove them from
all browsers.Descriptions of the full list of features can be found in the project README.
Also, read the posts from the other companies working on Interop 2025.]]></content:encoded></item><item><title>COSMIC Alpha 6: Big Leaps Forward</title><link>https://blog.system76.com/post/cosmic-alpha-6-big-leaps-forward</link><author>/u/Schnurres</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 18:40:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Our COSMIC mission continues! This month, we finished up some essential features and fixes in preparation for the upcoming beta alongside some amazing COSMIC contributors. Check out what’s new in Alpha 6, and make sure you’re fully updated to see these changes for yourself!Desktop Zoom can now be activated in Settings > Accessibility, from the Accessibility applet in the panel, or using the shortcuts Super + =, Super + -, or Super + Mouse Scroll.More accessibility features in the books! Clicking the Accessibility icon at login gives you access to various settings toggles for:Reads on-screen text aloud Scrolling up while holding Super magnifies the region of the screen where your cursor is locatedNavigates you to Accessibility SettingsAdditional accessibility features for high-contrast, color inversion and various color filters for colorblindness are being worked on soon.Desktop view is now supported in COSMIC. Right-clicking an empty desktop and selecting “Desktop view options” opens a settings window for your desktop. Show or hide desktop folders, drives, or the Trash; you can also adjust icon size and spacing between icons from this window. A fix for the window appearing below other windows will arrive in a later update. Files and folders can also be dragged between the desktop view and COSMIC Files.Additional Scaling OptionsAn additional scaling setting has been added to scale the screen slightly, from 5% to 20% larger. For example, on a display set to 125% scaling, those desiring larger text can use this new setting to increase scaling to 130%, 135%, 140%, or 145%.Workspaces received some updates to really make the feature sparkle. For starters, you can now scroll between workspaces in the overview as a quick and easy way of navigating to your intended destination. Clicking on the preview of the current workspace or empty space in the workspace overview will allow you to exit the workspace view.Previews for horizontal workspace now include name and number. Workspace previews on rotated displays will show the correct orientation. The last workspace is now removed if it follows another empty workspace.Additionally, minimized windows can now be dragged and dropped between workspaces, while windows can be moved to another display by dropping them in the workspace overview. Dragging a window out of a stack will match expectations and no longer move the whole stack. Window titles in the overview are now at the top left of the window and match the users theme. Additional workspace features, including pinned workspaces, will arrive in a future update.Windows Gravitate to EdgesToggling on “Floating windows gravitate to nearby edges” in Window Management Settings will automatically align a window’s edge to the adjacent screen border when dragged close to it, removing the struggle of aligning it with the edge manually.If a search in the Launcher yields more than eight entries, users can now scroll to see the additional options. In addition, the Launcher has been updated to trigger a countdown timer whenever Power Off, Restart, and Log Out are selected, matching the behavior of the Power applet.File path completion is now in COSMIC Files. Hitting the Down arrow when typing a file path into the search bar will automatically finish the file path you’re searching for. Meanwhile, copying a file allows pasting of the file path in other applications. COSMIC Files uses Home and End keys for navigating the app. You can now compress and extract password protected zip files and drag selecting will scroll the content window.Copy/paste using the middle mouse button has been implemented. Sweet convenience.A nav bar has been added for viewing folders in a tree view to display the video files available to open in the Media Player. File menu options have also been completed:When a music file is playing, the Media Player image will display the song title, album, artist, and year released. Down the line we’d like to explore adding further metadata, such as album artwork and song lyrics. Mpris control has been added to show and control currently-playing media in the sound applet, and the scrubber now moves to a second line for improved single-column usability.A Revert all changes feature has been added to COSMIC Edit to revert your file back to the most recent saved state. If you decide to scrap everything or start a new file from scratch, go to File > Close Project to remove the project from the NavBar and bring up a new document and tab. When multiple tabs are open, cycle project tabs using Ctrl+Tab and Ctrl+Shift+Tab shortcuts.Zoom has also been implemented. Zoom in and out from the View menu, or using Ctrl + or Ctrl - shortcuts. Reset back to default using Ctrl + 0.Opens Sans replaces Fira Sans as the default font for COSMIC. The team liked Open Sans for its better legibility, glyph and language support, and a more modern aesthetic. Likewise, Noto Sans Mono will be used for the default monospace font.Memory usage has been greatly reduced in a number of areas, including minimize, COSMIC Files, and workspaces. A related update made to libcosmic should prevent memory fragmentation. In addition, optimizations to cosmic-text and freedesktop-icons have reduced memory usage across all COSMIC apps and applets.A Whole Swarm of Bug Fixes…and More!Fixed a bug with server-side decorations that caused the cursor to drag a window after a single clickWhen clicking an app icon of an app with multiple windows opened, window previews now adapt to the size and shape of the windowImplemented a fix in cosmic-comp related to keyboard grabbing after a window is focusedImplemented behavior to COSMIC Files for exiting the context menuCOSMIC Files now changes view away from the external drive after the drive is mounted and removedImplemented a fix for COSMIC Files attempting to read an unreadable .hidden fileFixed a crash involving the file picker related to a11y in libcosmicCOSMIC Terminal now uses a hollow block cursor design when the window is unfocusedRemoved Spell Check menu option from COSMIC Edit, to be returned once the feature existsIn COSMIC Edit, “Find” searches now highlight all occurrences, and the currently selected item is highlighted at a higher opacityScrolling now occurs as expected when dragging to highlight text in COSMIC EditSaving a root or read-only file in COSMIC Edit now prompts the user for their password, removing the need to run the application as rootScreenshot tool now respects the user’s time zone when naming screenshot filesImplemented a fix preventing icons from disappearing from the screenshot toolFixed a bug preventing the Delete key from moving a desktop file to the TrashImplemented a fix for a bug causing Steam to crashFixed an issue causing some Radeon RX users to be unable to log inThe context menu in COSMIC Settings now closes when another option is selected in the NavBarImplemented the ability to import environment variables from systemdFixed a regression with libcosmic affecting the ComboBox widgetAdded a slight delay when the cursor hovers from one applet to the next to account for intentAdded support for using the middle mouse button to copy/pasteClicking next/previous month in the calendar widget no longer selects the dayResolved a bug with Firefox not recognizing it’s the default web browser when it’s not set as the default mail clientRemoved WPS suggestion from the WiFi applet when WPS is not supportedAdded support to cosmic-bg for compositors without fractional scaling supportPop!_OS 24.04 Linux kernel updated to version 6.12.10Head to the COSMIC page for a fresh install of Alpha 6. If your system has an NVIDIA GPU, remember to install the NVIDIA ISO. Have fun and break things!]]></content:encoded></item><item><title>Meanwhile at the Pentagon</title><link>https://www.reddit.com/r/artificial/comments/1iuwy03/meanwhile_at_the_pentagon/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:49:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] MLGym: A New Framework and Benchmark for Advancing AI Research Agents</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/</link><author>/u/Rybolos</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:46:14 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.]]></content:encoded></item><item><title>[D] Dimensionality reduction is bad practice?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/</link><author>/u/Ready_Plastic1737</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:30:22 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I was given a problem statement and data to go along with it. My initial intuition was "what features are most important in this dataset and what initial relationships can i reveal?"I proposed t-sne, PCA, or UMAP to observe preliminary relationships to explore but was immediately shut down because "reducing dimensions means losing information."which i know is true but..._____________can some of you add to the ___________? what would you have said?]]></content:encoded></item><item><title>AI Godfather Yoshua Bengio says it is an &quot;extremely worrisome&quot; sign that when AI models are losing at chess, they will cheat by hacking their opponent</title><link>https://www.reddit.com/r/artificial/comments/1iuvosh/ai_godfather_yoshua_bengio_says_it_is_an/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:59:37 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust 🦀 DataFrame Library Elusion v3.3.0 is released 🚀 FIXED NORMALIZATION</title><link>https://www.reddit.com/r/rust/comments/1iuvnrr/rust_dataframe_library_elusion_v330_is_released/</link><author>/u/DataBora</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:58:28 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Elusion is a high-performance DataFrame / Data Engineering / Data Analysis library designed for in-memory data formats such as CSV, JSON, PARQUET, DELTA, as well as for ODBC Database Connections for MySQL and PostgreSQL, as well as for Azure Blob Storage Connections, as well as for creating JSON files from REST API's which can be forwarded to DataFrame.]]></content:encoded></item><item><title>Best Practices for Consistent API Error Handling</title><link>https://zuplo.com/blog/2025/02/11/best-practices-for-api-error-handling</link><author>/u/ZuploAdrian</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:57:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Clear and consistent API error handling is crucial for improving developer
experience and reducing debugging time. Poor practices, like unclear messages
or misuse of HTTP status codes, can frustrate developers and lead to increased
support tickets. This guide covers actionable strategies to standardize API
error handling, including:Use of HTTP Status Codes: Ensure accurate mapping (e.g., 400 for client
errors, 500 for server issues).Structured Error Responses: Follow RFC 9457 (Problem Details) standards
for clear, actionable error details. Write brief, helpful, and secure messages.Protocol-Specific Practices: Tailor error handling for REST, GraphQL, and
gRPC APIs.To address the challenges highlighted earlier, you can apply these
well-established methods.HTTP status codes are your first tool for communicating errors. The key is to
use them accurately, rather than relying on generic codes.401 Unauthorized, 422 Unprocessable Entity500 Internal Error, 503 Service UnavailableYou can find a full list of status codes
on MDN, but here's a
few helpful docs we've written in the past that go into more depth:For consistent error reporting, modern APIs should follow the
RFC 9457 Problem Details
specification. This is the successor to the popular
RFC 7807 draft. For an in-depth
understanding of this format, check out our
full problem details guide. In
case you're short on time - here's a quick overview:The problem details response is sent back as a JSON body with the following
properties:: A URI that identifies the specific error type. This
helps clients understand the error and potentially find more information or
documentation about it. Ideally, this URI should be stable and not change over
time.: A short, human-readable summary of the problem. This
should be a brief description that concisely conveys the error. The title
should not change for a given "type" URI.status (integer, optional): The HTTP status code generated by the origin
server for this occurrence of the problem. This helps clients understand the
nature of the error and how it relates to the HTTP protocol.detail (string, optional): A more detailed, human-readable explanation of
the problem. This can include specific information about the error and what
might have caused it. The "detail" field is intended to provide context and
suggestions to clients on how they might address the problem.instance (string, URI, optional): A URI that identifies the specific
occurrence of the problem. This can help clients and servers correlate and
track individual instances of errors.Here's what a standardized error response might look like:This response would be accompanied with the following headers and status code:Here's a video that shows you how to send these error back in practice. It's in
.Net/C# but the concepts are broadly applicable:When evolving error schemas, it's crucial to make changes without disrupting
existing clients. Here's how you can manage this:Add optional fields instead of altering existing onesPreserve legacy formats during transitionsImplement semantic versioning for your endpointsAPI management tools like Zuplo are really handy when trying to bring
consistency across your error formats, and are especially useful when trying to
transition all of your APIs over from one format to another.Creating effective API error messages means providing useful guidance while
keeping security in mind. Google's AIP-193 guidelines
[4] recommend using a structured format with
plain, straightforward language that remains technically accurate.The goal is to make error messages both clear and actionable. Here's a
comparison of good and bad examples:"Invalid email format in 'user_email' field""ValidationError: field_23""Request to /api/v1/users failed"If you've ever used Azure, many of their system errors demonstrate this
approach, for example:Maintaining security while providing useful error feedback involves a few key
practices: to prevent leaks.Standardize authentication errors for consistency. to avoid exposing vulnerabilities.For example, following RFC 9457 guidelines, a secure error response might look
like this:This is another area where using an API gateway/API management tool is useful.
You can monitor your outbound responses and scan them for PII or other sensitive
information/keywords using a regex. A programmable gateway (ex. Zuplo) will even
let you rewrite your response bodies to strip out sensitive data.Over 10,000 developers trust Zuplo to secure, document, and monetize their APIsLearn MoreLet's dive into how different API protocols handle errors, building on the
standards discussed earlier.REST APIs rely on HTTP status codes paired with structured error payloads. A
common standard for this is , which ensures a
consistent format across endpoints. This method also supports content
negotiation between JSON and XML, keeping the structure consistent across
different formats.GraphQL handles errors differently. It always responds with a  status
code, even when errors occur. Errors are communicated through an  array,
which allows for partial success. For instance, GitHub's GraphQL API might
return:This approach allows for returning valid data alongside error details.gRPC uses a predefined set of numeric status codes (ranging from 0 to 16) for
error handling, aligned with . Errors include structured details
for better context. Here's an example:Standard HTTP success codesEach protocol has its own approach, but they all follow two key principles:
machine-readable error codes and . This ensures
errors are both understandable and actionable.For cross-protocol APIs, API gateways can simplify error handling. They provide
unified error schema management and automate transformations between
protocol-specific formats. This helps maintain consistency while respecting the
conventions of each API type.Effective error management relies on consistent monitoring and testing to uphold
usability standards outlined in earlier protocols. This process builds on
protocol-specific error conventions to ensure smooth handling across systems.A centralized error code system can ensure uniformity across distributed
services. For example, Google uses the  format, which
enforces standardized error structures with both machine-readable codes and
human-readable messages [4].In multi-service architectures, two main approaches are common:Ensures uniform error codes, Acts as a single source of truthRequires strict oversightDistributed with PrefixesAllows team independence, Enables quicker updatesDemands thorough documentationMany organizations find that combining these methods provides the best results.Key metrics to monitor include
[2][3]:Mean Time to Acknowledge (MTTA) errors: Target under 30 minutes.: Should remain below 5% after fixes.95th percentile error resolution time: A critical benchmark for resolution
speed.User-impacting error ratio: Measured per 10,000 requests.Tools like Sentry support distributed tracing in over 15
languages, while Raygun offers deployment correlation to
pinpoint issues [2].Testing ensures compliance with HTTP status codes and response formats covered
in earlier sections. Error testing is typically done manually using tools like
Postman - but you should definitely invest in automation as your API grows and
evolves. To test specific errors, you should invest in automated
end-to-end API testing using
tools like Playwright and StepCI.If you have schematized errors, you can perform
schema validation on your live responses
to ensure they adhere to those schemas. When response validation is combined
with an API design specification
like OpenAPI to enforce outputs match what's documented, it's known as
.Effective API error handling builds on the protocol-specific conventions
discussed earlier. Two key goals are ensuring consistent response formats
and reducing repeat client errors
[2][5].Key requirements include:Standardized Response Structure:Machine-readable error codesClear, human-readable messagesLinks to relevant documentationRequest correlation IDs for troubleshootingSecurity-Focused Practices:Prevent exposure of sensitive data by adhering to security guidelines
outlined in the Writing Clear Error Messages sectionUse appropriate status codesFollow established security best practicesTo create a reliable error-handling system, follow these four phases:Use an
API monitoring tool
to analyze errors at the endpoint level. This helps identify inconsistencies
and establish a baseline for improvement
[2][5].Introduce centralized error-handling middleware to enforce the newly defined
standards. Often, an API gateway plays this role.Use error tracking tools to evaluate the system’s performance. Track metrics
like error recurrence rates, resolution times, and Mean Time to Acknowledge
(MTTA).These steps align with earlier recommendations for policy-driven error handling
and offer practical ways to enhance your API's reliability.Handling API errors effectively requires a clear and structured approach. This
involves combining standard protocols with additional application-specific
information to provide clarity and maintain security.1. Protocol-Specific HandlingEach API protocol has its own method for managing errors. Here’s how to handle
errors for some common protocols:: Use HTTP status codes alongside detailed error messages in the
response body.: Include error arrays in the response, allowing for partial
success when appropriate.: Utilize standardized status codes with structured error details.2. Security Best PracticesTo keep your API secure, follow these guidelines (as outlined in the "Security
in Error Messages" section):Use generic error messages for authentication failures to prevent revealing
sensitive information.Avoid exposing internal system details in error responses.Filter sensitive data on the server side before sending error responses.3. Monitoring and ConsistencySet up monitoring tools, such as distributed tracing, to identify and analyze
error patterns. Use
API gateways to
enforce consistent error formats and schemas across your system for better
management and debugging
[2][3].]]></content:encoded></item><item><title>Meetup: All in Kubernetes (Munich)</title><link>https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/</link><author>/u/simplyblock-r</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 16:50:52 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)We're running our second meetup of the "All in Kubernetes" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.Community is focused around stateful workloads in Kubernetes. The sessions lined up are:Architecting and Building a K8s-based AI PlatformDatabases on Kubernetes: A Storage Story]]></content:encoded></item><item><title>GitHub Traffic - CLI Edition</title><link>https://postimg.cc/XXWK9gzB</link><author>/u/manifoldjava</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:33:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>list of decimal packages: fixed and big</title><link>https://www.reddit.com/r/golang/comments/1iuunf1/list_of_decimal_packages_fixed_and_big/</link><author>/u/kardianos</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 16:16:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I was updating a list of decimal packages. I thought I would share.There are generally 2 varieties: fixed sized and arbitrary precision. The udecimal is interesting as it uses a fixed size for 128 bit precision with zero allocations, then uses an allocating "*big.Int" version for anything larger then that.I currently use "cockroachdb/apd", which is a great package for frameworks or databases, but, it's a bit awkward to hold and lacks good formating. Realistically, I just need a fixed size decimal for my needs (financial/clinical). When I get a chance, I'll probably swap in for one of the fixed size packages.]]></content:encoded></item><item><title>I built a new playground for Go, Pt, TS and more other, with Postgres... It supports program arguments, pretty output for JSON and I will add a lot feature soon</title><link>https://codiew.io/ide</link><author>/u/Halabooda</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 15:32:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Borrow Checker Trauma</title><link>https://www.reddit.com/r/rust/comments/1iuthsl/borrow_checker_trauma/</link><author>/u/xwaxes</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 15:27:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am using the term ‘borrow checker trauma’ for lack of a better word. A bit of context first; I have been using Rust for my personal web projects extensively but use Rails at work. So the problem is, whenever I am working on work projects and want to perform two or more operations on a variable, especially if I am passing it around or returning it, I always find myself taking a step back to consider if the ownership has moved before I remember that I am on Ruby and that doesn’t apply. Has anyone experienced this in other languages or on their daily workflow?]]></content:encoded></item><item><title>Talk me out of using Mongo</title><link>https://www.reddit.com/r/golang/comments/1iutb24/talk_me_out_of_using_mongo/</link><author>/u/grdevops</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 15:19:32 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Talk me out of using Mongo for a project I'm starting and intend to make a publicly available service. I  love how native Mongo feels for golang, specifically structs. I have a fair amount of utils written for it and it's basically at a copy and paste stage when I'm adding it to different structs and different types. Undeniably, Mongo is what I'm comfortable with have spend the most time writing and the queries are dead simple in Go (to me at least) compared to Postgres where I have not had luck with embedded structs and getting them to easily insert or scanned when querying (especially many rows) using sqlx. Getting better at postgres is something I can do and am absolutely 100% willing to do if it's the right choice, I just haven't run into the issues with Mongo that I've seen other people haveAs far as the data goes, there's not a ton of places where I would need to do joins, maybe 5% of the total DB calls or less and I know that's where Mongo gets most of its flak. ]]></content:encoded></item><item><title>Bugs with k8s snap and IPv6 only</title><link>https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/</link><author>/u/hblok</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 14:13:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.The first is in the k8s-apiserver-proxy config file /var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json. It looks like this, where the the last part is the port number 6443. The service does not start with a "failed to parse endpoint" error:{"endpoints":["dead:beef:1234::1:6443"]} When correcting the address to use brackets, it will start up correctly.{"endpoints":["[dead:beef:1234::1]:6443"]} Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with "Failed to listen and serve" err="listen tcp 0.0.0.0:10250: bind: address already in use". Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly related to this report.]]></content:encoded></item><item><title>Gofs - a file server written in go</title><link>https://www.reddit.com/r/golang/comments/1iuqggw/gofs_a_file_server_written_in_go/</link><author>/u/-dtdt-</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 13:06:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/-dtdt- ]]></content:encoded></item><item><title>Certifications for software architects</title><link>https://www.cerbos.dev/blog/certifications-for-enterprise-architects-domain-solutions-architects-software-engineers</link><author>/u/West-Chard-1474</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 13:02:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Over the years, I’ve noticed that no one can quite settle on how important certification is. All it takes is one look at the Software Architecture subreddit and you’ll see people asking about certificates only to be told they’re both useless and useful.When I was working with Lemon.io (a developer marketplace with 80k+ developers), I got to see firsthand how certification affected their careers. So when I saw this topic start to pop up again (without any real answers), I decided to dive into research to see if my experience at Lemon.io still held true for architects.The value of certificationAssuming I’m talking to architects with a lot of experience, what I will say is that certification doesn’t replace experience, but it does complement it really well. And, it can be a strong differentiator from your peers.I’m going to try a metaphor here. If your career is a burger, your experience is the patty and certificates are the condiments. Some people prefer their burgers with bacon, cheese, or even an egg. Depending on what your career goals are, you’ll want to add different condiments to your ‘burger’. But keep in mind, the most important thing will always be the meat, a.k.a your experience.Deciding if certification is right for you is the first step. The next step is asking the question: what is the right certification for you?I loved the Role Based Roadmap from Mumshad Mannambeth, founder & CEO at KodeKloud on navigating the certification paths (you can find it here).It inspired me to do something similar but focused on Architects. Below, you’ll find 12 of the most popular architectural certificates you can choose to upgrade your career, and add more “trust badges” to your CV, LinkedIn profile, or freelancer profile.Each has its own focus and value it can add depending on your career goals and role:Certifications for Enterprise ArchitectsCertifications for Domain Solutions ArchitectsSoftware Architecture, Governance, and Infrastructure CertificationAWS Certified Solutions ArchitectGoogle Professional Cloud ArchitectZachman Certified - Enterprise ArchitectMicrosoft Certified: Azure Solutions Architect ExpertCertified Enterprise Architect (CEA) Black Belt ProgramRed Hat Certified Architect (RHCA)Recognized globally, this certificate covers the TOGAF framework for designing, planning, implementing, and governing enterprise information technology architecture. So if you are working on an enterprise-wide architecture or want to switch to that direction, it can be useful.
Unlike the certifications below, this is a whole ecosystem that builds on itself. So it is a significant investment. However, if you’re working in, or looking to work in, the governmental sector or with large corporations, this may be a good choice for you. Keep in mind, however, that while it may look cheap, the cost does not include training, which is provided by TOGAF-accredited partners, each of whom sets their own price.The comment below sums up my research into TOGAF 9 really well:Best for Software Architects working in large organizations: Enterprise Architects,  Business Architects, Solutions Architects, and IT Strategy Consultants with experience in strategic planning, We had a few Enterprise Architects with TOGAF 9 certification at Lemon.io and it was a nice value-add to their profiles. It is worth mentioning that their rates were in the top tier 🙂.13 Level 1 Learning Units27 Level 2 Learning UnitsTesting: Two-stage testing, including TOGAF 9 Part 1 and TOGAF 9 Part 2 examinations$360 USD per exam (requires two exams)English, Simplified Chinese, Spanish (Latin American), FrenchCertification expiry & recertification: TOGAF 9 certificate does not expire.ITIL is one of the most popular certification systems in the world, with more than two million certified specialists in the world. The Master certification is the highest level of ITIL certifications and requires passing all four previous certifications before challenging it. To achieve the master certification you have to pass an assessment to validate your ability to apply ITIL frameworks to real-world business scenarios. So, experience working with the ITIL principals and practices as an enterprise software architect is required.ITIL is not a training or testing provider but works with accredited partners for each. That means the pricing is dependent on your provider. The nice thing is, that it is one of the few large accreditations you can achieve through self-study.If you want to chat with peers who are planning to become ITIL Masters, there is an active subreddit called ITIL_Certification.: Governance & Compliance Architects, Governance and Compliance Managers, Enterprise Architects.Five distinct levels to progress through: Foundation -> Practitioner –> Intermediate -> Expert -> Master. Each level has its own training and requirements.Attend a training course with an accredited training organization, which will include the exam as part of the course.Self-study using the core manual, then book an exam directly with PeopleCert.Dependent on your chosen training/testing partner.All 4 previous ITIL certifications, including ITIL Expert certification5 years in IT leadership, management, or higher management advisory levels.English, Brazilian Portuguese, Chinese, Dutch, French, German, Italian, Japanese, Polish, Spanish, ThaiCertification expiry & recertification: Certification is valid for 3 years. You can renew your certification by retaking the exam or by earning a new certification.Zachman Certified - Enterprise ArchitectThis certification covers the Zachman Framework for designing and maintaining Enterprise Architectures, which aligns IT with business goals. The training is focused on high-level enterprise planning, including Enterprise Architecture principles, strategy formulation, and practical application, rather than project or solutions architecture. This makes it ideal for those who need a structured approach to solve enterprise challenges.This is another multi-level certification option. Each level builds on the one before it, which makes it a very comprehensive course. And, in my humble opinion, very expensive.: Enterprise Architects, Business Architects, and IT Consultants with an Enterprise Architecture focus.Four levels available: Associate, Practitioner, Professional and Educator (last one only required for those who want to teach the course).Training: Two weeks of online prep work, and three days of live instructionTesting: Two-hour, online exam (passing grants level 1 Associate)Case study: Delivering a case study provides level 2 Practitioner, and a second case study provides level 3 Professional$2999 USD covers level 1 & 2 (regional pricing is available)Each level requires certification in the preceding levelCertification expiry & recertification: The certificate expires in 3 years. Recertification costs $99 USD.Certified Enterprise Architect (CEA) Black Belt Program (Owned by Zachman now)Zachman now owns CEA, so I decided to add this certification under the Zachman banner. Just like Zachman, this is a three-step course, except the naming convention is designed to make you feel like you’re in a martial art, which is cool. It’s also $7,000 more, which is not as cool, but that does mean you get to skip the yellow and green belt and go straight for the black belt.Designed to develop Enterprise Architects through hands-on training and real-world projects, this program will prepare you for leadership roles in Enterprise Architecture. This program is built on ISO standards and focuses on the practical application of frameworks, tools, and methodologies.: (Very rich people) Senior Enterprise Architect, IT Director with EA experience, Chief Architect, Enterprise Architect with 10+ years experience, Enterprise Architecture Consultant.Accelerated Path of 12 Weeks: Five individual courses taught in parallel over twelve weeks.Progressive Path that is self-paced: Five individual online courses taken at your own pace over 24-30 weeks.The IASA Global certification is a vendor-independent program for Business Technology Architects. The training has four stages that roughly align with your career stage. For Enterprise Architects, the professional (3rd) tier is the most useful  The previous two tiers are for those in earlier stages of their career. The training is based on practical experience with a focus on Enterprise Architecture (EA), Software Architecture (SA), Solution Architecture (SolA), Infrastructure Architecture (IA) and Business Architecture (BA).Unlike most tiered options, IASA allows you to challenge each level even if you haven’t attained the certification under it. So if you’ve progressed in your career far enough that you don’t think a foundational certification is valuable, and you don’t want to work your way through 3 tiers you already fully understand, IASA may be the answer for you.: Senior architects and business analysts aiming to bridge the gap between business and technology.Four levels of certification:Professional (recommended for enterprise-level architects) - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2 hrs allotted for the exam.Distinguished - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2.5 hrs allotted for the exam.Testing: Online or onsite testing is available (for professional tier) Exam: $425 USD  N/A  Exam + prep: $2,000 USD  Exam + prep: $2,800 USDA minimum of 10 years in the industry as a practicing architect.  Extensive documentation is required.CITA-Associate level certificate is recommended but not mandatory.Certification expiry & recertification: Requires individuals to maintain an active Full Iasa Membership and collect at least 80 hours of Continuing Education Units bi-annually (based on their website).The Open Group ArchiMate 3 CertificationArchiMate was mentioned a lot over Reddit. The Open Group ArchiMate 3 doesn’t teach you how to be an EA but rather focuses on how to communicate better as an EA by teaching ArchiMate’s modelling language. This language is designed to remove ambiguity from the description, analysis, and visualization of Enterprise Architectures.The content of the course is designed to complement TOGAF, which makes it useful for Enterprise Architects who already work in a TOGAF framework. They aren’t however, the same thing.: Enterprise ArchitectsTraining: Online self-study available, or attend an accredited training course (Accredited Training Courses provide an exam voucher).ArchiMate 3 Part 1 offers foundation certification (60 min time limit for 40-question, multiple choice exam. Passing score: 60%)ArchiMate 3 Part 2 offers practitioner certification (90 min time limit for 8-question, scenario-based and complex multiple choice exam. Passing score: 65%)Online or in-person proctored exams available depending on providerTraining cost depends on the provider.  Each exam costs $360 USD. None  Foundation certification or pass Part 1 exam on the same day with the same provider.Certification expiry & recertification: The certification does not expire, which is very cool.AWS Certified Solutions ArchitectThe AWS certification is a great starting point for architects or senior Software Engineers with AWS Cloud or strong on-premises IT experience. It covers the design and optimization of AWS cloud-based software and shows you can handle complex multi-service architectures, which is crucial as more companies move to the cloud. The exam tests real-world scenarios you would face designing large-scale systems, including hybrid architectures, multi-region deployments, and cost optimization at scale. Basic familiarity with programming concepts will help, but you don’t need hands-on experience with code.This is the only provider-specific certificate that offers a more in-depth, self-directed training option for a price. Of course, you don’t have to take that option. If you’re confident, you can take the free training and then challenge the test. However, the test is also the cheapest among these certifications, so it offsets the overall cost a little bit.During my research, I found quite a few posters, including this one, that had seen significant benefits from taking the course.: Systems Administrators (Cloud Focused), Cloud Architects, Solutions Architects, Cloud Consultants Software Architects for Cloud-Based Applications, and Enterprise Architects.3 courses of online, self-directed exam prep are available15.25 hrs free; 48.75 hrs paidTesting: 130 minutes. Pearson VUE testing center, or online proctored exam1 year of hands-on experience designing cloud solutions with AWS services.English, French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Spanish (Latin America), Spanish (Spain), Simplified and Traditional ChineseGoogle Professional Cloud ArchitectIf you’re all in on Google, this certification is for you. It will help you show your cloud architecture skills and advance your career in Google’s cloud technology. Keep in mind, this is focused on Google’s cloud infrastructure and doesn’t cover software architecture.The training and exam for this certificate are all online (though on-site exams are available) which makes it very flexible. Plus, it’s pretty cheap (although it’s the most expensive of the provider-specific options). However, if you go through all the training, it’s going to take you some time, as it’s the longest provider-specific course here.: Cloud Architects, Solutions Architects, IT Project Managers focused on the cloud, Cloud Engineers, DevOps Engineers, and Enterprise Architects.Training: 114.75 hrs, online, self-directedTesting: 2-hr test, with two options: an online proctored exam, or an onsite-proctored exam at a testing center.3+ years of industry experience, including 1+ year of designing and managing solutions with Google CloudCertification expiry & recertification: The certificate is valid for 2 years. Recertify by retaking the exam within 60 days of the expiration date.Microsoft Certified: Azure Solutions Architect ExpertThis certificate shows you know your way around Azure, including how to design and implement cloud and hybrid solutions. It dives deep into how various IT infrastructure components in the Microsoft ecosystem (like compute, network, storage, monitoring, and security) interact to generate solutions. Just like the Google certificate above, this is infrastructure-focused, not software-focused, so keep that in mind when considering it.The course for the Microsoft certification is shorter than Google’s by almost 100 hours and can be taken both online at your own pace, or with an online instructor. It’s also a bit cheaper than Google’s, making it an easier investment both for hours and dollars spent.: Systems Administrators, Network Engineers, IT Managers, Cloud Architects, Computer Systems Analysts and Infrastructure Engineers.Training: Self-paced online learning (15.25 hrs), or instructor-led online training (4 days).Testing: Online proctored exam.$165 USD  Self-paced training is free; instructor-led depends on the provider.Experience with Azure administration and development, and DevOps processes.  Advanced experience and knowledge of IT operations.English, Chinese (Simplified), French, German, Japanese, Korean, Portuguese (Brazil), SpanishCertification expiry & recertification: The certificate expires after one year. Recertification is free via an online assessment on Microsoft Learn.Red Hat Certified Architect (RHCA)Red Hat’s highest level of certification, the RHCA designation covers designing, implementing, and managing Red Hat-based IT infrastructures. One of the nice things is that RHCA offers tracks in both infrastructure and enterprise applications. So you can choose the option that best matches your goals.Red Hat prices its certification differently than most. It’s a subscription-based model, which allows you to have a little more freedom with how you tackle their training. In fact, their certification offers the most custom options, with a variety of options to get from A (where you are) to B (certified). So if you have diverse interests, this one might be the one for you.: Senior Systems Administrator, Cloud Architect, IT Infrastructure Architect, DevOps Engineer, Senior Application Developer, Enterprise Solutions ArchitectRed Hat Certified Architect in InfrastructureRed Hat Certified Architect in Enterprise ApplicationsCertification builds on prerequisites with five additional certifications chosen from a list.Training and exams depend on your chosen path and certifications. Standard: $7,500 USD/year for 25 training units and certification.  Premium: $9,000 USD/year for 30 training units and certification.Red Hat Certified Engineer (RHCE)  OR  Red Hat Certified Enterprise Microservices Developer (RHCEMD)  OR  Red Hat Certified Cloud-native Developer (RHCCD)Recommended experience depends on your specific path.Certification expiry & recertification: Certifications become ‘non-current’ after three years. To maintain an RHCA certification, you must maintain five additional certifications over your RHCE. RHCEMD or RHCCD. These certifications do not need to be the same as those you’ve taken to attain your RHCA.Made for IT professionals who want to master SOA, this certification covers designing, implementing, and managing Service-Oriented Architecture (SOA) infrastructure.SOA certification is one and done. There are no levels or long-term commitment to one system, which makes it less of a commitment. It’s also very affordable.: Enterprise Architects, Solutions Architects, IT Architects, Software Architects, Systems Engineers with SOA experience, Technical Leads with architecture responsibilities170 mins online proctored exam50 hrs of training over 5 modules (modules include: Workbook Lessons, Video Lessons, Interactive Exercises, Mind Map Poster, Practice Exam Questions, PDFs of Workbook and Poster, Lab Exercise Booklet).$399 USD for course and certificationiSAQB CPSA-F/CPSA-A (International Software Architecture Qualification Board)In contrast to TOGAF training, the CPSA program focuses on the practical implementation of IT systems. Its foundation and advanced certificates offer room for architects to grow.This is a two-step program, foundation and advanced, which puts it between the single-step SOA certification and the larger three- or even four-step offerings. Just like TOGAF and ITIL, iSAQB is not a testing or training provider, so there are a lot of options for training. Or, if you’re confident, you can challenge the exam without, though that’s not recommended.: Software designers, software developers, Software Architects, systems analysts
Though training and testing are performed by independent operators, you have four testing options available, including:Exam after classroom trainingThe cost of training is dependent on training providers.  Testing price is dependent on training providers.Training is suggested.  Foundation certification is required for Advanced.18+ months of practical experience, including:  - A higher programming language  - Technical documentation  - Object-oriented programming language  - Design and implementation of distributed applications  - Basics of modeling and abstraction; and UML and their relation to sourceLanguage is dependent on training providers.Certification expiry & recertification: The certificate does not expire.The right certification can set you apart on your resume, but it’s never a replacement for experience. Depending on your career path, you may choose to get certified by one of the bodies above, or simply study on your own and prove what you can do through practical experience.If you’ve decided to pursue the certification path, the options above are all great choices. Of course, each requires a commitment of time and money. While you can’t warp the space-time continuum to change the time requirement, it may be possible to get your employer to help you cover some, if not all of the cost. If they do, it gives you an even higher ROI on your investment to yourself.]]></content:encoded></item><item><title>This month in Servo: new webview API, relative colors, canvas buffs, and more!</title><link>https://servo.org/blog/2025/02/19/this-month-in-servo/</link><author>/u/wuyuwei-tw</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:53:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Servo now supports several new web API features:We’ve landed a bunch of  improvements: are a lot more useful now, with  now supporting  (@Taym95, #35040), , , and  (@Taym95, #34958).Servo aims to be an embeddable web engine, but so far it’s been a lot harder to embed Servo than it should be.For one, configuring and starting Servo is complicated.
We found that getting Servo running at all, even without wiring up input or handling resizes correctly, took  of Rust code (@delan, @mrobinson, #35118).
Embedders (apps) could only control Servo by sending and receiving a variety of “messages” and “events”, and simple questions like “what’s the current URL?” were impossible to answer without keeping track of extra state in the app.Contrast this with WebKitGTK, where you can write a minimal kiosk app with a fully-functional webview in  of C.
To close that gap, we’ve started reworking our embedding API towards something more idiomatic and ergonomic, starting with the concept embedders care about most: the .Our new webview API is controlled by calling methods on a  (@delan, @mrobinson, #35119, #35183, #35192), including navigation and user input.
Handles will eventually represent the lifecycle of the webview itself; if you have one, the webview is valid, and if you drop them, the webview is destroyed.Servo needs to call into the embedder too, and here we’ve started replacing the old EmbedderMsg API with a  (@delan, @mrobinson, #35211), much like the delegates in Apple’s WebKit API.
In Rust, a delegate is a  that the embedder can install its own  for.
Stay tuned for more on this next month!Other embedding improvements include:We’ve reworked Servo’s , making all prefs optional with reasonable defaults (@mrobinson, #34966, #34999, #34994).
As a result:The names of all preferences have changed; see the Prefs docs for a listEmbedders no longer need a  resource to get Servo runningServo’s networking is more efficient now, with the ability to cancel fetches for navigation that contain redirects (@mrobinson, #34919) and cancel fetches for <video> and <media> when the document is unloaded (@mrobinson, #34883).
Those changes also eliminate per-request IPC channels for navigation and cancellation respectively, and in the same vein, we’ve eliminated them for image loading too (@mrobinson, #35041).We’ve continued splitting up our massive script crate (@jdm, #34359, #35157, #35169, #35172), which will eventually make Servo much faster to build.We now run CI smoketests on OpenHarmony using a real device (@jschwe, @mukilan, #35006), increasing confidence in your changes beyond compile-time errors.We’ve also tripled our self-hosted CI runner capacity (@delan, #34983, #35002), making concurrent Windows and macOS builds possible without falling back to the much slower GitHub-hosted runners.Servo can’t yet run WebDriver-based tests on wpt.fyi, wpt.servo.org, or CI, because the  executor for the Web Platform Tests does not support testdriver.js.
 does, though, so we’ve started fixing test regressions with that executor with the goal of eventually switching to it (@jdm, #34957, #34997).Thanks again for your generous support!
We are now receiving  (−11.4% over December) in recurring donations.
With this money, we’ve been able to expand our capacity for self-hostedCIrunners on Windows, Linux, and macOS builds, halving  build times from over an hour to under 30 minutes!Servo is also on thanks.dev, and already  (+5 over December) that depend on Servo are sponsoring us there.
If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.As always, use of these funds will be decided transparently in the Technical Steering Committee.
For more details, head to our Sponsorship page.]]></content:encoded></item><item><title>Is this architecture possible without using haproxy but nginx(in rocky linux 9)?</title><link>https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/</link><author>/u/Keeper-Name_2271</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 12:37:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alerting from Prometheus and Grafana with kube-prometheus-stack</title><link>https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/</link><author>/u/HumanResult3379</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 12:36:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[In Grafana page's , I find the built-in alert rules named .I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?]]></content:encoded></item><item><title>Have we hit a scaling wall in base models? (non reasoning)</title><link>https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/</link><author>/u/CH1997H</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:28:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 SonnetYet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the "scaling laws" where the chart just says "line goes up")Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scalingIt looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it]]></content:encoded></item><item><title>I built a new playground for Go</title><link>https://codiew.io/ide?t=go</link><author>/u/Halabooda</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 12:23:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] Have we hit a scaling wall in base models? (non reasoning)</title><link>https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/</link><author>/u/CH1997H</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:23:20 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 SonnetYet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the "scaling laws" where the chart just says "line goes up")Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scalingIt looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it]]></content:encoded></item><item><title>I created A Easy to use Rust Web Framework</title><link>https://www.reddit.com/r/rust/comments/1iuplg1/i_created_a_easy_to_use_rust_web_framework/</link><author>/u/Rough_Shopping_6547</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:20:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I just published my  project!I realized there isn’t a single easy-to-use, plug-and-play Rust web framework out there (at least to my knowledge), so I decided to create my own.I'd love to hear your thoughts on it!]]></content:encoded></item><item><title>Junior, Trying to understand why startups use golang for backend</title><link>https://www.reddit.com/r/golang/comments/1iup4di/junior_trying_to_understand_why_startups_use/</link><author>/u/FriendshipOk6564</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:53:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello,i just took a look at the website 'who is hiring' and saw a lot of startups using ruby on rails and golang in their stack and i'm confuse, the path isn't normally mvp in rails and after some companies will rewrite their wall backend at some point in something like Java spring? it append for netflix but also a big company where i live. Why would they mixte ror and golang? Those it mean they are rewriting their ror in a microservice architecture in go?]]></content:encoded></item><item><title>How to properly prepare monorepos in Golang and is it worth it?</title><link>https://www.reddit.com/r/golang/comments/1iuoppk/how_to_properly_prepare_monorepos_in_golang_and/</link><author>/u/GoDuffer</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:27:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello everyone. At the moment I am writing a report on the topic of a monorepo in order to close my internship at the university.Since I am a Go developer (or at least I aspire to be one), I decided to make a monorepo in Go.The first thing I came across was an article from Uber about how they use Bazel and I started digging in this direction.And then I realized that it was too complicated for small projects and I became interested.Does it make sense to use a monorepo on small projects? If not, how to split the application into services? Or store each service in a separate repository.In Java, everything is trivially simple with their modules and Gradle. Yes, Go has modules and a workspace, but let's be honest, this is not the level of Gradle.As a result, we have that Bazel is too complicated for simple projects, and gowork seems somehow cut down after Gradle.Monorepo or polyrepo for Go?Is there anything other than go work and Bazel?What is the correct way to split a Go project so that it looks like a Solution in C#, or modules in Java/Gradle?It is quite possible that I really don't understand the architecture of Go projects, I will be glad if you point me in the right direction.]]></content:encoded></item><item><title>The Deeper Love of Go (Go 1.24 early access edition)</title><link>https://bitfieldconsulting.com/books/deeper</link><author>/u/bitfieldconsulting</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:10:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/</link><author>/u/gctaylor</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 11:00:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>AI and the future of work - an EU perspective</title><link>https://v.redd.it/cx0l3st20hke1</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 10:34:58 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Getting organised! · AerynOS</title><link>https://github.com/orgs/AerynOS/discussions/37</link><author>/u/Wooden-Opposite3557</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 10:17:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ChatGPT took an oath to protect its own.😄🤖</title><link>https://www.reddit.com/r/artificial/comments/1iuno62/chatgpt_took_an_oath_to_protect_its_own/</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 10:15:32 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My experience after switching from Java to Go</title><link>https://www.reddit.com/r/golang/comments/1iuni44/my_experience_after_switching_from_java_to_go/</link><author>/u/hosmanagic</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 10:03:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/hosmanagic ]]></content:encoded></item><item><title>AVR microcontrollers are now officially maintained!</title><link>https://www.reddit.com/r/rust/comments/1iunfgx/avr_microcontrollers_are_now_officially_maintained/</link><author>/u/Patryk27</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:59:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[AVRs are cute & tiny microcontrollers from Atmel - you might've heard about ATmega328p used in Arduino Uno, for example:Every week we're marching towards better AVR support in Rust and as of today I can proudly say: we don't need no `target.json`s anymore + we've got an official maintainer! (points finger at self)So far AVRs remain tier 3, but at least it's waay easier to use them now - just target `avr-none` and provide `-C target-cpu` so that rustc & llvm know which specific microcontroller you're building for; a couple of important codegen fixes are also coming together with rustc's upgrade to LLVM 20, hoping to wrap up on https://github.com/Rahix/avr-hal/pull/585 over the coming days.I'd like to take this moment to thank https://github.com/benshi001 for his continued support and code reviews on the LLVM's side - let AVR flourish!]]></content:encoded></item><item><title>Sponsoring Rust Developers</title><link>https://www.reddit.com/r/rust/comments/1iun7oj/sponsoring_rust_developers/</link><author>/u/szabgab</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:44:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[One of the "findings" of my previous question was that some crates are missing or not mature enough to be used.If you would like to use Rust you can hope that those gaps will be closed in time or you can do something about it. If you have the time and expertise you can get involved in the needed projects, but there is a much easier and less time-consuming way. You and/or your company can sponsor the development efforts.Allocating 10-20 USD / month by an individual or 1000-2000 USD month by a small company does not sound like a big investment and many such sponsors can make a huge difference together.One way to find who to sponsor is to find the developers of your dependencies. For that visit the Explore GitHub Sponsors page. On the left-hand side select the "Cargo" ecosystem. That will show you the individuals and the organizations that you currently rely upon that also accept sponsorship.I've also created a page listing some of the people and project who develop Rust and could be sponsored. For some of them I've also included background information.]]></content:encoded></item><item><title>My experience with the GNOME Desktop - from despised to loved</title><link>https://www.reddit.com/r/linux/comments/1iun2fo/my_experience_with_the_gnome_desktop_from/</link><author>/u/Fishsven</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:33:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[ I started my Linux journey with Pop!_OS, and I hated the wasted space of the panel-like dock. It took me a while for me to return to GNOME as I was discovering KDE Plasma's (5.24) customization potential. I loved it at first, but I noticed how the DE slowly became unstable after a lot of customising (Plasma has GREATLY improved by now, last time I tried 5.27 on Q4OS and it was blazing fast and rock solid). I was annoyed at how people took a liking to the hideous DE known as GNOME, and for me there was little difference between it and Windows 8, as they were basically tablet centric with GNOME and it's wasted space. I eventually got tired of Plasma, because it had way too many features that I didn´t wan´t to use. Tried XFCE, MATE and Budgie, and they felt too outdated for my liking; Budgie felt off. I decided to give GNOME a shot and installed Ubuntu 22.04. For once I was starting to like GNOME. It felt more unified and simple than KDE, but just more modern than the other desktops. However, this was NOT stock GNOME. I installed vanilla GNOME on the same OS and decided to give it a shot. Moving on from Ubuntu's Yaru theme to Adwaita felt like a MASSIVE downgrade. Except the looks, GNOME's true workflow actually started to make sense to me and it was more productive than any desktop I tried. Of course, I installed some extensions like Blur my Shell, but I can use GNOME without extensions nowadays. As I'm writing this, GNOME 48 would bring a new Adwaita font with Inter as it's base, which will improve the looks of GNOME by a bit, IMO. Currently using Zorin OS, which has a GNOME theme that is MILES better compared to Libadwaita / Adwaita.  What I understood is GNOME is not all about looks, it makes the UI simpler and easier to understand, with ONLY the things you need, and it stays out of your way and focuses on your work. It might be dumbing down the desktop for some, but that's exactly what GNOME's for. A solid philosophy IMO- but definitely lagging in some important areas. ]]></content:encoded></item><item><title>How donations helped the LibreOffice project and community in 2024</title><link>https://blog.documentfoundation.org/blog/2025/02/21/how-your-donations-helped-the-libreoffice-project-in-2024/</link><author>/u/themikeosguy</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:28:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Thank you for visiting our website and your interest in our services and products. As the protection of your personal data is an important concern for us, please click on the "More information" link to access our Privacy Policy page - which will open in a separate browser tab - where we explain what information we collect during your visit to our website, how it is processed, and whether or how it may be used.
Once you have carefully read our Privacy Policy page, close the browser tab to return to this page and click on the "Save Preferences" button under this text to acknowledge it, close the dialogue and return to the website.
We take all the necessary technical and organisational security measures to protect your personal data from loss and misuse. Your data is stored in a secure operating environment that is not accessible to the public.]]></content:encoded></item><item><title>reddittui - A terminal browser for reddit</title><link>https://github.com/tonymajestro/reddit-tui</link><author>/u/tmajest</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 07:54:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Sharing my Open Source Project: Realtime Messaging Platform Built with Go &amp; React (Fullstack)</title><link>https://github.com/JoyalAJohney/Realtime-distributed-chat</link><author>/u/BruceWayn_</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 04:53:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Minecraft from scratch with only modern OpenGL</title><link>https://github.com/GianlucaP106/minecraft</link><author>/u/One_Mess_1093</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 03:32:48 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Contribute by filing bugs. You&apos;ll feel all warm and fuzzy inside.</title><link>https://www.reddit.com/r/linux/comments/1iugim6/contribute_by_filing_bugs_youll_feel_all_warm_and/</link><author>/u/billhughes1960</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 02:45:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[As a lifelong Linux user, I believe strongly in giving back to the open-source community. While I'm not a developer myself, I've found another way to contribute: filing bug reports.I'll admit my early attempts were probably pretty rough – missing crucial context and details. But practice makes perfect (or at least close!), and these days my bug reports are often addressed within a day or so.There's something incredibly satisfying about uncovering a problem, meticulously documenting it, submitting a report, seeing it assigned to someone, and finally witnessing the fix. It's a tangible way to make a difference in the software we all rely on.This level of responsiveness and respect simply doesn't exist in proprietary ecosystems. I've tried reporting bugs on Windows and macOS with little success – it often feels like shouting into the void. But in the open-source world, even smaller projects welcome contributions and treat you seriously.So, I encourage everyone to embrace bug reporting! Start with a simpler project to get comfortable with the process, then gradually tackle more complex ones. Not only will you be improving the software for everyone, but you'll also experience that warm glow of knowing you made a positive impact.]]></content:encoded></item><item><title>Linus Torvalds responds to Christoph Hellwig</title><link>https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/</link><author>/u/bik1230</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 02:30:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linus Torvalds rips into Hellwig for blocking Rust for Linux</title><link>https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/</link><author>/u/eugay</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 01:49:32 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Installed Ubuntu on my Nan&apos;s laptop:</title><link>https://www.reddit.com/r/linux/comments/1iueq5x/installed_ubuntu_on_my_nans_laptop/</link><author>/u/Unique_Ad4547</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 01:18:01 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Libreboot 20241206, 10th revision released! GRUB security fixes, better LVM scanning, non-root USB2 hub support</title><link>https://libreboot.org/news/libreboot20241206rev10.html</link><author>/u/libreleah</author><category>linux</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 01:11:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Article published by: Leah RoweDate of publication: 18 February 2025Today’s Libreboot 20241206 revision is the 10th revision in the Libreboot 20241206 stable release series. The changelog on this page is written, relative to Libreboot 20241206 revision 9 which was released on 12 February 2025. The  Libreboot 20241206 release came out on 6 December 2024. You can find the full list of revisions here and the original release here.Open source BIOS/UEFI firmware[link]Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g. Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of coreboot for hardware initialisation, and then a payload such as SeaBIOS or GRUB to boot your operating system; on ARM(chromebooks), we provide  (as a coreboot payload).We also provide an experimental U-Boot setup on x86, as a coreboot payload for providing a minimal UEFI implementation.Normally, revisions would only be documented on the Libreboot 20241206 revisions page, but this revision contains , so it was decided that there should be a full announcement, to ensure that more people see it.Summarised list of changes[link]GRUB released  to its main branch, fixing a large number of security issues. You can read about them here:This updates GRUB to revision 4dc6166571645780c459dde2cdc1b001a5ec844c from 18 February 2025. Several OOB heap writes, buffer overflows, use after frees and so on, are now prevented with this update.In addition to the security fixes, several out-of-tree fixes from Libreboot’s main branch have been merged for GRUB, fixing bugs in the xHCI driver, and adding support for non-root USB2 hubs on platforms that use the  GRUB tree.Changes to the GRUB configuration have been made, to make scanning of LVM volume/group names more reliable, including on full-disk-encryption setups. More such changes are planned for the next major release; the current changes are very minor.]]></content:encoded></item><item><title>BritCSS: Fixes CSS to use non-American English</title><link>https://github.com/DeclanChidlow/BritCSS</link><author>/u/ValenceTheHuman</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 00:14:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st</title><link>https://docs.docker.com/docker-hub/usage/</link><author>/u/onedr0p</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 21 Feb 2025 00:06:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Starting April 1, 2025, all users with a Pro, Team, or Business
subscription will have unlimited Docker Hub pulls with fair use.
Unauthenticated users and users with a free Personal account have the
following pull limits:Unauthenticated users: 10 pulls/hourAuthenticated users with a free account: 100 pulls/hourThe following table provides an overview of the included usage and limits for each
user type, subject to fair use:Number of public repositoriesNumber of private repositories10 per IPv4 address or IPv6 /64 subnetFor more details, see the following:When utilizing the Docker Platform, users should be aware that excessive data
transfer, pull rates, or data storage can lead to throttling, or additional
charges. To ensure fair resource usage and maintain service quality, we reserve
the right to impose restrictions or apply additional charges to accounts
exhibiting excessive data and storage consumption.Docker Hub has an abuse rate limit to protect the application and
infrastructure. This limit applies to all requests to Hub properties including
web pages, APIs, and image pulls. The limit is applied per IPv4 address or per
IPv6 /64 subnet, and while the limit changes over time depending on load and
other factors, it's in the order of thousands of requests per minute. The abuse
limit applies to all users equally regardless of account level.You can differentiate between the pull rate limit and abuse rate limit by
looking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes
a link to documentation.]]></content:encoded></item><item><title>[Media] Rust powered flight radar</title><link>https://www.reddit.com/r/rust/comments/1iubs4r/media_rust_powered_flight_radar/</link><author>/u/Confident-Alarm-6911</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:47:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So, consider this mix: I have thing for retro-interfaces with monochromatic displays, I wanted to learn rust and do something with sdr radio, I live next to the airport. And that’s how my small radar comes to life 😎Hardware: ESP32C3, 1.5 inch i2c oled display, some encoder. RTL-SDR V4 running on my local linux machine and small endpoint to serve ADS-B data via http.Firmware written in rust 2021 edition. Libraries: mostly std and esp-idf-svc + rtos (not necessary, but I wanted to try it)I’m pretty content with this small project as it is my first attempt to build something in Rust. Now I want to design 3D printable case, do some polishing on software side, and publish it as open source.I wanted to post video but it says I can not do this in this community, so only pic]]></content:encoded></item><item><title>IaaC Simplified: Automating EC2 Deployments with GitHub Actions, Terraform, Docker &amp; Distribution Registry | Vue &amp; Node admin panel framework</title><link>https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/</link><author>/u/Unerring-Ocean</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:29:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[This guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with Docker and Terraform involving Docker self-hosted registry.GitHub actions Free plan which includes 2000 minutes per month (1000 of 2-minute builds per month - more then enough for many projects, if you are not running tests etc). Extra builds would cost  per minute.AWS account where we will auto-spawn EC2 instance. We will use  instance (2 vCPUs, 2GB RAM) which costs  per month in  region (cheapest region). Also it will take  per month for EBS gp2 storage (20GB) for EC2 instanceThis is it, registry will be auto-spawned on EC2 instance, so no extra costs for it. Also GitHub storage is not used, so no extra costs for it.The setup has next features:Build process is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo so no manual clicks are needed.Docker build process is done on GitHub actions, so EC2 server is not overloadedChanges in infrastructure including changing server type, adding S3 Bucket, changing size of sever disk is also can be done by commiting code to repo.Docker images and cache are stored on EC2 server, so no extra costs for Docker registry are needed.Total build time for average commit to AdminForth app (with Vite rebuilds) is around 2 minutes.Previously we had a blog post about deploying AdminForth to EC2 with Terraform without registry. That method might work well but has a significant disadvantage - build process happens on EC2 itself and uses EC2 RAM and CPU. This can be a problem if your EC2 instance is well-loaded without extra free resources. Moreover, low-end EC2 instances have a small amount of RAM and CPU, so build process which involves vite/tsc/etc can be slow or even fail.So obviously to solve this problem we need to move the build process to CI, however it introduces new chellenges and we will solve them in this post.Quick difference between approaches from previous post and current post:How and where docker build happensSource code is rsync-ed from CI to EC2 and docker build is done thereDocker build is done on CI and docker image is pushed to registry (in this post we run registry automatically on EC2)How Docker build layers are cachedGitHub actions has no own Docker cache out of the box, so it should be stored in dedicated place (we use self-hosted registry on the EC2 as it is free)Simpler setup with less code (we don't need code to run and secure registry, and don't need extra cache setup as is naturally persisted on EC2).Build is done on CI, so EC2 server is not overloaded. For most cases CI builds are faster than on EC2. Plus time is saved because we don't need to rsync source code to EC2Build on EC2 requires additional server RAM / overloads CPUMore terraform code is needed. registry cache might require small extra space on EC2Chellenges when you build on CI​When you move build process to CI you have to solve next chellenges:We need to deliver built docker images to EC2 somehow (and only we)We need to persist cache between buildsExporing images to tar files​Simplest option which you can find is save docker images to tar files and deliver them to EC2. We can easily do it in terraform (using  command on CI and  command on EC2). However this option has a significant disadvantage - it is slow. Docker images are big (always include all layers, without any options), so it takes infinity to do save/load and another infinity to transfer them to EC2 (via relatively slow rsync/SSH and relatively slow GitHub actions outbound connection).Faster, right option which we will use here - involve Docker registry. Registry is a repository which stores docker images. It does it in a smart way - it saves each image as several layers, so if you will update last layer, then only last layer will be pushed to registry and then only last will be pulled to EC2.
To give you row compare - whole-layers image might take , but last layer created by  command might take . And most builds you will do only last layer changes, so it will be 20 times faster to push/pull last layer than whole image.
And this is not all, registry uses TLS HTTP protocol so it is faster then SSH/rsync encrypted connection.Of course you have to care about a way of registry authentication (so only you and your CI/EC2 can push/pull images).What docker registry can you use? Pretty known options:Docker Hub - most famous. It is free for public images, so literally every opensource project uses it. However it is not free for private images, and you have to pay for it. In this post we are considering you might do development for commercial project with tight budget, so we will not use it.GHCR - Registry from Google. Has free plan but allows to store only 500MB and allows to transfer 1GB of traffic per month. Then you pay for every extra GB in storage and traffic. Probably small images will fit in this plan, but generally even alpine-based docker images are bigger than 500MB, so it is not a good option.Self-hosted registry web system. In our software development company, we use Harbor. It is a powerful free open-source registry that can be installed to own server. It allows pushing and pulling without limit. Also, it has internal life-cycle rules that cleanup unnecessary images and layers. The main drawbacks of it are that it is not so fast to install and configure, plus you have to get a domain and another powerfull server to run it. So unless you are a software development company, it is not worth using it.Self-hosted minimal CNCF Distribution registry on EC2 itself. So since we already have EC2, we can run registry on it directly. The  container is pretty light-weight and easy to setup and it will not consume a lot of extra CPU/RAM on server. Plus images will be stored close to application so pull will be fast.In the post we will use last (4th way). Our terraform will deploy registry automatically, so you don't have to do anything special.Docker builds without layer cache persistence are possible but very slow. Most builds only change a couple of layers, and having no ability to cache them will cause the Docker builder to regenerate all layers from scratch. This can, for example, increase the Docker build time from a minute to ten minutes or even more.Out of the box, GitHub Actions can't save Docker layers between builds, so you have to use external storage.Though some CI systems can persist docker build cache, e.g. open-source self-hosted Woodpecker CI allows it out of the box. However GitHub actions which is pretty popular, reasonably can't allow such free storage to anyoneSo when build-in Docker cache can't be used, there is one alternative - Docker BuildKit external cache.
So BuildKit allows you to connect external storage. There are several options, but most sweet for us is using Docker registry as cache storage (not only as images storage to deliver them to application server).BuildKit cache in Compose issue
Previously we used docker compose to build & run our app, it can be used to both build, push and pull images, but has issues with external cache connection. While they are not solved we have to use  command to build images. It is not so bad, but is another point of configuration which we will cover in this post.Registry authorization and traffic encryption​Hosting custom CNCF registry, from other hand is a security responsibility.If you don't protect it right, someone will be able to push any image to your registry and then pull it to your EC2 instance. This is a big security issue, so we have to protect our registry.First of all we need to set some authorization to our registry so everyone who will push/pull images will be authorized. Here we have 2 options: HTTP basic auth and Client certificate auth. We will use first one as it is easier to setup. We will generate basic login and password automatically in terraform so no extra actions are needed from you.But this is not enough. Basic auth is not encrypted, so someone can perform MITM attack and get your credentials. So we need to encrypt traffic between CI and registry. We can do it by using TLS certificates. So we will generate self-signed TLS certificates, and attach them to our registry.Assume you have your AdminForth project in .Create file  in :create folder  and create file  inside:Step 3 - create a SSH keypair​Make sure you are still in  folder, run next command:Now it should create  and  files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.Step 4 - create TLS certificates to encrypt traffic between CI and registry​Make sure you are still in  folder, run next command:Run next command to create TLS certificates:This will create  and  files.Step 5 - .gitignore file​Create  file with next content:Step 6 - buildx bake file​Create file :Step 7 - main terraform file main.tf​Create file  in  folder:👆 Replace  with your app name (no spaces, only underscores or letters)Step 7.1 - Configure AWS Profile​Open or create file  and add (if not already there):Step 7.2 - Run deployment​To run the deployment first time, you need to run:Step 8 - Migrate state to the cloud​First deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.Add to the end of :👆 Replace  with your app name (no spaces, only underscores or letters).
Unfortunately we can't use variables, HashiCorp thinks it is too dangerous 😥Now you can delete local  file and  file as they are in the cloud now.Step 9 - CI/CD - Github Actions​Create file .github/workflows/deploy.yml:.github/workflows/deploy.ymlStep 8.1 - Add secrets to GitHub​Go to your GitHub repository, then  ->  ->  and add: - your AWS access keyVAULT_AWS_SECRET_ACCESS_KEY - your AWS secret key - execute  and paste to GitHub secrets - execute  and paste to GitHub secrets - execute  and paste to GitHub secrets - execute  and paste to GitHub secretsNow you can push your changes to GitHub and see how it will be deployed automatically.Once you will have sensitive tokens/passwords in your apps you have to store them in a secure way.Simplest way is to use GitHub secrets.Let's imagine you have  which will be used one of AI-powered plugins of adminforth. We can't put this key to the code, so we have to store it in GitHub secrets.Open your GitHub repository, then  ->  ->  and add  with your key.Now open GitHub actions file and add it to the  section:.github/workflows/deploy.ymlNext add it to the  script:In the same way you can add any other secrets to your GitHub actions.Out of space on EC2 instance? Extend EBS volume​To upgrade EBS volume size you have to do next steps:This will increase physical size of EBS volume, but you have to increase filesystem size too.You can find your EC2 IP in AWS console by visiting EC2 -> Instances -> Your instance -> IPv4 Public IPThis would show something like this:Here we see that  is our disk and  is our partition.Now to extend partition run:This will extend partition to the full disk size. No reboot is needed.]]></content:encoded></item><item><title>Using one ingress controller to proxy to another cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/</link><author>/u/djjudas21</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 22:15:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to app.example.com on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: "HTTPS" nginx.ingress.kubernetes.io/ssl-passthrough: "true" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks]]></content:encoded></item><item><title>Google&apos;s Shift to Rust Programming Cuts Android Memory Vulnerabilities by 68%</title><link>https://thehackernews.com/2024/09/googles-shift-to-rust-programming-cuts.html</link><author>/u/Unerring-Ocean</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:14:55 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Google has revealed that its transition to memory-safe languages such as Rust as part of its secure-by-design approach has led to the percentage of memory-safe vulnerabilities discovered in Android dropping from 76% to 24% over a period of six years.The tech giant said focusing on Safe Coding for new features not only reduces the overall security risk of a codebase, but also makes the switch more "scalable and cost-effective."Eventually, this leads to a drop in memory safety vulnerabilities as new memory unsafe development slows down after a certain period of time, and new memory safe development takes over, Google's Jeff Vander Stoep and Alex Rebert said in a post shared with The Hacker News.Perhaps even more interestingly, the number of memory safety vulnerabilities tends to register a drop notwithstanding an increase in the quantity of new memory unsafe code.The paradox is explained by the fact that vulnerabilities decay exponentially, with a study finding that a high number of vulnerabilities often reside in new or recently modified code."The problem is overwhelmingly with new code, necessitating a fundamental change in how we develop code," Vander Stoep and Rebert noted. "Code matures and gets safer with time, exponentially, making the returns on investments like rewrites diminish over time as code gets older."Google, which formally announced its plans to support the Rust programming language in Android way back in April 2021, said it began prioritizing transitioning new development to memory-safe languages around 2019.As a result, the number of memory safety vulnerabilities discovered in the operating system has declined from 223 in 2019 to less than 50 in 2024.It also goes without saying that much of the decrease in such flaws is down to advancements in the ways devised to combat them, moving from reactive patching to proactive mitigating to proactive vulnerability discovery using tools like Clang sanitizers.The tech giant further noted that memory safety strategies should evolve even more to prioritize "high-assurance prevention" by incorporating secure-by-design principles that enshrine security into the very foundations. "Instead of focusing on the interventions applied (mitigations, fuzzing), or attempting to use past performance to predict future security, Safe Coding allows us to make strong assertions about the code's properties and what can or cannot happen based on those properties," Vander Stoep and Rebert said.That's not all. Google said it is also focusing on offering interoperability between Rust, C++, and Kotlin, instead of code rewrites, as a "practical and incremental approach" to embracing memory-safe languages and ultimately eliminating entire vulnerability classes."Adopting Safe Coding in new code offers a paradigm shift, allowing us to leverage the inherent decay of vulnerabilities to our advantage, even in large existing systems," it said."The concept is simple: once we turn off the tap of new vulnerabilities, they decrease exponentially, making all of our code safer, increasing the effectiveness of security design, and alleviating the scalability challenges associated with existing memory safety strategies such that they can be applied more effectively in a targeted manner."The development comes as Google touted increased collaboration with Arm's product security and graphics processing unit (GPU) engineering teams to flag multiple shortcomings and elevate the overall security of the GPU software/firmware stack across the Android ecosystem."Proactive testing is good hygiene as it can lead to the detection and resolution of new vulnerabilities before they're exploited," Google and Arm said.Found this article interesting?  Follow us on Twitter  and LinkedIn to read more exclusive content we post.]]></content:encoded></item><item><title>CustomResourceDefinitions to provision Azure resources such as storage blob</title><link>https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/</link><author>/u/Valuable-Ad3229</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 22:11:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?I am mindful that this may be anti-pattern but I am curious. Thank you!]]></content:encoded></item><item><title>[D] Are there any theoretical machine learning papers that have significantly helped practitioners?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/</link><author>/u/nihaomundo123</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:59:39 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[21M deciding whether or not to specialize in theoretical ML for their math PhD. Specifically, I am interested inii) but NOT interested in papers focusing on improving empirical performance, like the original dropout and batch normalization papers.I want to work on something with the potential for deep impact during my PhD, yet still theoretical. When trying to find out if the understanding-based questions in category i) fits this description, however, I could not find much on the web...If anyone has any specific examples of papers whose main focus was to understand some phenomena, and that ended up revolutionizing things for practitioners, would appreciate it :)]]></content:encoded></item><item><title>The State of Scala &amp; Clojure Surveys: How is functional programming on JVM doing</title><link>https://www.jvm-weekly.com/p/the-state-of-scala-and-clojure-surveys</link><author>/u/ArturSkowronski</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:50:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The title might be a bit of an overstatement – don’t expect a very extensive analysis of functional programming trends. I wanted to focus on two surveys that have appeared recently, which tell us a bit about languages that many of you probably used in a previous reincarnation cycle, but have already forgotten about.Of course, the usual disclaimer at the beginning – we know that surveys tend to show what they feel is worth showing and have a certain narrative power.Having that in mind, I think we can begin.First, let’s look at the people who filled out the survey – demographics tell us a lot about the quality of the results and what we can expect. We have as many as 232 responses, with 75% Software Engineers and 49.6% people in tech-lead or related areas – apparently, many of us wear several hats at once (yes, I know that pain too). It turned out that most of them work on closed-source projects (87.5%), although there’s no shortage of hardcore open-source folks (18.1%). I think that fairly reflects the state of the industry.An overwhelming majority of respondents like or love Scala: 49.1% love it, 44% rather like it. The remaining 6.9% are still undecided, and 0.9% (i.e., 2 people) have fallen into Scala-depression. However here we also hit, to some extent, the “peak” of people interested in the topic - those willing to fill out the Scala Survey.The average age of projects is 7 years, with a median of 6 years – that’s quite… a lot. It also shows that quite a bit of Scala is legacy projects – at least in the surveyed group, “greenfields” are relatively rare.Typelevel (41.8%), Akka (35.3%), ZIO (23.3%), and Play (15.5%). And of course, Spark (7.7%) – let’s not forget that big data elephant in the room, though the days when it was synonymous with Data seem to be behind us. These people have a new best friend.Moreover, more than half of the projects (53%) combine different ecosystems such as Akka and Cats, indicating that we’re building increasingly hybrid beasts. 36.2% are “monogamists” relying entirely on a single library (ZIO, we’re looking at you), and 10.8% are the brave ones using only the standard library.VirtusLabproject hereSBT beats everyone hands down (87.5%). Scala-CLI (11.2%) is relatively new but has decent traction, and Bazel (7.8%) and Maven (7.3%) also have loyal fans. 22.4% of commercial projects have already switched to Scala 3, but as many as 37% do not plan to. Why? Because as usual, the ecosystem is (still) not fully ready, there’s a lack of resources, and management is pushing the topic aside. You know that feeling when a new version of a language tempts you, but there are dozens of “work in progress” branches piling up in the repo…?Talking about the problems, the report shows three major ones:Fragmentation of the ecosystem and migration issues to Scala 3Lack of resources to maintain older projects.recruiting Scala developersGiven these recruitment problems, it’s not surprising that 68.6% allow remote work, which at least somewhat makes life easier for those who have managed to settle in the Bieszczady Mountains or in Bali.Despite all these challenges, 88.4% of respondents would still choose Scala for new projects without hesitation. It shows that the JVM community sees great potential in Scala, but also knows that working on its further development and tooling is a marathon, not a sprint.the report is full of interesting detailsNow, let's take a look at the other report I have for you today.Alex MillerState of ClojureLet’s start with what warms the hearts of backend folks the most: does Clojure live in real projects and is it more than just a hobby experiment? Definitely yes! 73% of respondents use Clojure at work, mainly in web development, commercial services, and enterprise applications. Their services often end up in the cloud – public (58%) or private (26%). So you can say with confidence that the “Lispy DSL” is conquering more and more server rooms and Docker containers.What about team size? Most are small teams (up to 10 people), though there are also true giants – Nubank, with over a thousand Clojure developers, is a prime example. It’s no coincidence they’re now responsible for the development of the language.Regarding the adoption of new versions, things look surprisingly good. As many as 58% have already moved to Clojure 1.12, released in September 2024, which indicates that stability and a lack of painful breaking changes are quite a motivator.Here we see that Java 21 LTS already has 54% of users, and Java 8 is losing ground in favor of newer versions (only 9% remain with the old-timer, which is better than in Java itself). Probably for this reason, Clojure plans to raise the base version in subsequent releases (much like Scala, but we’ll talk about that next week).BabashkaAn example is Babashka – a dialect that enjoys huge popularity (93% of respondents who use dialects had dealt with it) because it allows scripts to be run quickly, without the start-up delays of the JVM. ClojureDart, on the other hand, brings Clojure into the Dart ecosystem, opening new perspectives for web and mobile apps. Other projects like Squint, Jank, and Cherry demonstrate the community’s ongoing creativity – each introduces its own modifications, often experimental, allowing the Clojure philosophy to adapt to entirely new conditions.Leiningen and deps.edn continues to vie for space among dependency management tools – we can see that deps.edn is gaining strength, and nRepl, REBL, and other plugins help make REPL feel like home.A special section of the survey examines people who have just started their adventure with Clojure (less than a year of experience). The report has been tracking programmers’ migration paths to Clojure for years. It appears they still largely come from Java, JavaScript, and Python. Ruby and C++ are in decline, whereas C# is starting to gain slightly – perhaps thanks to the “functional awakening” in the .NET ecosystem.Biggest challenges for newcomers?If you’ve read this far, you’re probably as happy as I am to see that Clojure keeps evolving. On one hand – a stable, mature platform, and on the other – new dialects, a growing community not just in corporate settings but also in open-source projects and even in the gaming industry (yes, yes, I’ve seen it!). State of Clojure 2024 shows that Lisp on the JVM is still a very strong player: a steady, balanced development without revolutionary changes, yet… there’s always something new to discover.through the full reportHappy coding – and until next time in JVM Weekly!]]></content:encoded></item><item><title>[R] Detecting LLM Hallucinations using Information Theory</title><link>https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/</link><author>/u/meltingwaxcandle</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:22:44 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[LLM hallucinations and errors are a major challenge, but what if we could predict when they happen? Nature had a great publication on semantic entropy, but I haven't seen many practical guides on production patterns for LLMs.Sequence log-probabilities provides a free, effective way to detect unreliable outputs (can be interpreted as "LLM confidence").High-confidence responses were nearly twice as accurate as low-confidence ones (76% vs 45%).Using this approach, we can automatically filter poor responses, introduce human review, or iterative RAG pipelines.Experiment setup is simple: generate 1000 RAG-supported LLM responses to various questions. Ask experts to blindly evaluate responses for quality. See how much LLM confidence predicts quality.Bonus: precision recall curve for an LLM.My interpretation is that LLM operates in a higher entropy (less predictable output / flatter token likelihood distributions) regime when it's not confident. So it's dealing with more uncertainty and starts to break down essentially.Regardless of your opinions on validity of LLMs, this feels like one of the simplest, but effective methods to catch a bulk of errors. ]]></content:encoded></item><item><title>Learning Project - Deploy Flask App With MySQL on Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/</link><author>/u/kchandank</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Thu, 20 Feb 2025 20:53:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).kubectl installed and configured to interact with your Kubernetes cluster.Docker installed on your machine to build and push the Docker image of the Flask app.Docker Hub account to push the Docker image.You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:Create a app.py file with following contentfrom flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): """ Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD """ host = os.environ.get("MYSQL_HOST", "localhost") database = os.environ.get("MYSQL_DB", "flaskdb") user = os.environ.get("MYSQL_USER", "flaskuser") password = os.environ.get("MYSQL_PASSWORD", "flaskpass") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f"Error connecting to MySQL: {e}") return None u/app.route("/") def index(): return f"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!" u/app.route("/dbtest") def db_test(): """ A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. """ connection = get_db_connection() if connection is None: return jsonify({"error": "Failed to connect to MySQL database"}), 500 try: cursor = connection.cursor() cursor.execute("SELECT NOW();") current_time = cursor.fetchone() return jsonify({ "message": "Successfully connected to MySQL!", "current_time": current_time[0] }) except Error as e: return jsonify({"error": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == "__main__": debug_mode = os.environ.get("DEBUG", "false").lower() == "true" app.run(host="0.0.0.0", port=5000, debug=debug_mode) FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update && apt-get install -y iputils-ping && rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD ["python", "app.py"] docker build -t becloudready/my-flask-app It will show a 6 digit Code, which you need to enter to following URLPush the Image to DockerHubdocker push becloudready/my-flask-app You should be able to see the Pushed ImageapiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: "false" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql kubectl create secret generic db-credentials \ --namespace=flask-app \ --from-literal=username=flaskuser \ --from-literal=password=flaskpass \ --from-literal=database=flaskdb apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: "mysql-svc" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: ["sh", "-c", "rm -rf /var/lib/mysql/*"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi storageClassName: do-block-storage kubectl apply -f namespaces.yamlDeploy ConfigMaps and Secrets:kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yamlkubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yamlkubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yamlkubectl get svc -n flask-appNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEflask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53sUnable to connect to MySQL from Flask AppLogin to the Flask app pod to ensure all values are loaded properlykubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.]]></content:encoded></item><item><title>How to manage tool dependencies in Go 1.24+</title><link>https://www.alexedwards.net/blog/how-to-manage-tool-dependencies-in-go-1.24-plus</link><author>/u/alexedwards</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 20:36:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[One of my favourite features of Go 1.24 is the new functionality for managing  dependencies.By this, I mean tooling that you use to assist with development, testing, build, or deployment – such as  for static code analysis,  for vulnerability scanning, or  for live-reloading applications.Historically, managing these dependencies — especially in a team setting — has been tricky. The previous solutions have been to use a  file or the  pattern, but while these approaches work, they’ve always felt like workarounds with some downsides.With Go 1.24, there’s finally a better way. To demonstrate the new functionality, let's scaffold a simple module and add some application code.$ go mod init example.com
go: creating new go.mod: module example.com
$ touch main.go
package main

import (
    "fmt"

    "github.com/kr/text"
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)
    fmt.Println(wrapped)
}
Now fetch the  package and run the code. The output should look like this:$ go get github.com/kr/text
go: downloading github.com/kr/text v0.2.0
go: added github.com/kr/text v0.2.0
$ go run .
This is an informational
message that should be
wrapped.Go 1.24 introduces the  flag for , which you can use like this:go get -tool import_path@version
This command will download the package specified by the import path (along with any child dependencies), store them in your module cache, and record them in your  file. The  part is optional – if you omit it, the latest version will be downloaded.Let's use this to add the latest versions of  and  to our module as developer tools, along with  version .$ go get -tool golang.org/x/tools/cmd/stringer
go: downloading golang.org/x/tools v0.30.0
go: downloading golang.org/x/sync v0.11.0
go: downloading golang.org/x/mod v0.23.0
go: added golang.org/x/mod v0.23.0
go: added golang.org/x/sync v0.11.0
go: added golang.org/x/tools v0.30.0

$ go get -tool golang.org/x/vuln/cmd/govulncheck
go: downloading golang.org/x/vuln v1.1.4
go: downloading golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7
go: downloading golang.org/x/sys v0.30.0
go: upgraded golang.org/x/telemetry v0.0.0-20240521205824-bda55230c457 => v0.0.0-20240522233618-39ace7a40ae7
go: added golang.org/x/vuln v1.1.4

$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.1
go: downloading honnef.co/go/tools v0.5.1
go: downloading golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: downloading github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: downloading golang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa
go: added github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: added golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: added honnef.co/go/tools v0.5.1After running these, your  file will now include a  section listing the tools you've added. The corresponding module paths and versions for all the dependencies will appear in the  section and be marked as indirect:module example.com

go 1.24.0

require (
    github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c // indirect
    github.com/kr/text v0.2.0 // indirect
    golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678 // indirect
    golang.org/x/mod v0.23.0 // indirect
    golang.org/x/sync v0.11.0 // indirect
    golang.org/x/sys v0.30.0 // indirect
    golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7 // indirect
    golang.org/x/tools v0.30.0 // indirect
    golang.org/x/vuln v1.1.4 // indirect
    honnef.co/go/tools v0.5.1 // indirect
)

tool (
    golang.org/x/tools/cmd/stringer
    golang.org/x/vuln/cmd/govulncheck
    honnef.co/go/tools/cmd/staticcheck
)
Once added, you can run tools using the  command.To run a specific tool from the command line within your module, you can use  followed by the last non-major-version segment of the import path for the tool (which is, normally, just the name for the tool). For example:$ go tool staticcheck -version
staticcheck 2024.1.1 (0.5.1)

$ go tool govulncheck
No vulnerabilities found.The  command also works nicely if you want to execute tools from your scripts or Makefiles. To illustrate, let's create a Makefile with an  task that runs staticcheck and govulncheck on the codebase..PHONY: audit
audit:
    go vet ./...
    go tool staticcheck ./...
    go tool govulncheck
If you run , you should see that all the checks complete successfully.$ make audit
go vet ./...
go tool staticcheck ./...
go tool govulncheck
No vulnerabilities found.Let's also take a look at an example where we use the stringer tool in conjunction with  to generate  methods for some  constants.package main

import (
    "fmt"

    "github.com/kr/text"
)

//go:generate go tool stringer -type=Level

type Level int

const (
    Info Level = iota
    Error
    Fatal
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)

    fmt.Printf("%s: %s\n", Info, wrapped)
}
The important thing here is the  line. When you run  on this file, it will in turn use  to execute the version of the stringer tool listed in your  file.$ go generate .
$ ls 
go.mod  go.sum  level_string.go  main.go  MakefileYou should see that a new  file is created, and running the application should result in some output that looks like this:$ go run .
Info: This is an informational
message that should be
wrapped.You can check which tools have been added to a module by running , like so:$ go list tool
honnef.co/go/tools/cmd/staticcheck
golang.org/x/tools/cmd/stringer
golang.org/x/vuln/cmd/govulncheckBecause the tools are included in your  file as dependencies, if you want to check that the code for the tools stored in your module cache has not changed you can simply run :$ go mod verify
This will check that the code in your module cache exactly matches the corresponding checksums in your  file.If you run , the code for tooling dependencies will be included in the  folder and the  manifest alongside your non-tool dependencies.$ go mod vendor
$  tree  -L 3
.
├── go.mod
├── go.sum
├── main.go
├── Makefile
└── vendor
        ├── github.com
        │   ├── BurntSushi
        │   └── kr
        ├── golang.org
        │   └── x
        ├── honnef.co
        │   └── go
        └── modules.txtWhen tools are vendored in this way, running  will execute the corresponding code in the  directory. Note that  does not work on vendored code.To upgrade or downgrade a specific tool to a specific version, you can use the same go get -tool import_path@version command that you did for adding the tool originally. For example:$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.0
To upgrade to the latest version of a specific tool, omit the  suffix. $ go get -tool honnef.co/go/tools/cmd/staticcheck
You can also upgrade  to their latest version by running . Note:  is a sub-command here, not a flag.If your tool dependencies are vendored, you will need to re-run  after any upgrades or downgrades.At the time of writing, I'm not aware of any easy way to specifically list the tools that have upgrades available – if you know of one please let me know!To remove the tool completely from your module, use  with the special version tag .$ go get -tool honnef.co/go/tools/cmd/staticcheck@none
Again, if you're vendoring, make sure to run  after removing a tool.A Reddit commenter mentioned the potential for problems if your tools share dependencies with your application code. For example, let's say that your application code depends on  version , and is tested and known to work with that version. Then if you add a tool that relies on a  version of , the version number in your  file will be bumped to the newer version and your application code will use that newer version too.In theory, this  be a problem so long as all your dependencies and their child dependencies are stable, follow strict semantic versioning, and don't make backwards-incompatible changes without a major version increment. But, of course, the real world is messy and backwards-incompatible changes  happen, which could unexpectedly break your application code.It's worth noting that this issue isn't limited to tool dependencies – the same thing can happen if your application code and a non-tool dependency both rely on the same package. However, including tools in  increases the risk.To reduce this risk, you can use a separate modfile for tool dependencies instead of including them in your main . You can do this with the  flag, specifying an alternative file such as , like so:# Initialize a go.tool.mod modfile
$ go mod init -modfile=go.tool.mod example.com

# Add a tool to the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck

# Run the tool from the command line
$ go tool -modfile=go.tool.mod govulncheck

# List all tools added to the module
$ go list -modfile=go.tool.mod tool

# Verify the integrity of the tool dependencies
$ go mod verify -modfile=go.tool.mod

# Upgrade or downgrade a tool to a specific version
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@v1.1.2

# Upgrade all tools to their latest version
$ go get -modfile=go.tool.mod tool

# Remove a tool from the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@none
]]></content:encoded></item><item><title>Why do temporaries need to explicitly borrowed?</title><link>https://www.reddit.com/r/rust/comments/1iu8jsn/why_do_temporaries_need_to_explicitly_borrowed/</link><author>/u/parkotron</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 20:31:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a long time C++ dev, I feel it didn't take me very long to pick up Rust's reference semantics and borrowing rules, but there one place where I constantly find myself forgetting to include the : passing temporaries into functions taking references.fn foo(s: &str) { println!("The str is: {s}"); } fn bar() -> String { "temporary".to_string() } fn main() { foo(&bar()); // ^ I always forget this ampersand until reminded by the compiler. } Rust's explicit  and  operators make a lot of sense to me: given a chunk of code, it should be obvious where a value has been borrowed and what kind of borrow it is. One should never be surprised to learn a reference was taken, because it's right there in the code.But in the case of temporary values, it really doesn't matter, does it? Whatever a function call does (or doesn't) do to a temporary value passed to it, the effect cannot be observed in the surrounding code, since the temporary is gone by the end of the statement.Is there a subtlety I'm missing here? Does that ampersand on a temporary convey useful information to an experienced Rust dev? Or is it really just syntactic noise, as it seems to me? Are there corner cases I'm just not considering? Could a future edition of Rust be changed to implicitly borrow from temporaries (like it implicitly borrows to make method calls)? Is my mental model just wrong?To be perfectly clear, this isn't a criticism, just curiosity. Clearly a lot of thought has been put into the language's design and syntax. This is just the only place I've encountered where Rust's explicitness doesn't feel completely justified.]]></content:encoded></item><item><title>To the purists rocking linux from scratch systems: how was it?</title><link>https://www.reddit.com/r/linux/comments/1iu7gc4/to_the_purists_rocking_linux_from_scratch_systems/</link><author>/u/0110010001101111</author><category>linux</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 19:46:51 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[how was your experience from installation to day to day management? what was your use case to build such system over just choosing a distro.the apps and the updating it. is it a hassle?is it a viable or reasonable option as a daily driver. i just wanted to get some insights about it.what do you like or dont like about it. the tradeoffs you were willing to accept, etc. ]]></content:encoded></item><item><title>Rate my photo manipulation tool</title><link>https://www.reddit.com/r/golang/comments/1iu6pch/rate_my_photo_manipulation_tool/</link><author>/u/tunerhd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 19:16:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/tunerhd ]]></content:encoded></item><item><title>AI can fix bugs—but can’t find them: OpenAI’s study highlights limits of LLMs in software engineering</title><link>https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 19:13:24 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn MoreIn a new paper, OpenAI researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they can’t see why the bug exists and continue to make more mistakes. The researchers tasked three LLMs — OpenAI’s GPT-4o and o1 and Anthropic’s Claude-3.5 Sonnet — with 1,488 freelance software engineer tasks from the freelance platform Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues). “Results indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,” the researchers write. The test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, they’re not quite at the level where they can start earning freelancing cash by themselves. Benchmarking freelancing modelsThe researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub “to avoid the possible of models scraping code diffs or pull request details,” they explained. The team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225.The tasks were added to the expensing platform Expensify. The researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, “we also generated a management task using the issue description and list of proposals,” they explained. From here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then “triple-verified” by professional software engineers.“Tests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the model’s solution works as expected,” the paper explains. After running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, “the majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.”The models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o. “Agents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,” the report explains. “Agents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions — often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.”Interestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding.These benchmark tests showed that AI models can solve some “low-level” coding problems and can’t replace “low-level” software engineers yet. The models still took time, often made mistakes, and couldn’t chase a bug around to find the root cause of coding problems. Many “low-level” engineers work better, but the researchers said this may not be the case for very long. ]]></content:encoded></item><item><title>TwinSong: Jupyter notebook built from scratch in Rust</title><link>https://www.reddit.com/r/rust/comments/1iu5tpa/twinsong_jupyter_notebook_built_from_scratch_in/</link><author>/u/winter-moon</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 18:40:46 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've spent a lot of time working with Python in Jupyter notebooks, but one thing has always bothered me: the way code and outputs are mixed together. While this is great for tutorials and interactive documentation, it's less ideal for exploratory work or data processing, where I just want to interact with Python without the constraints of a document-style interface. To address this, I created TwinSong, a Jupyter alternative that separates code and outputs. Right now, it's primarily a UX experiment, but core features like editing and executing cells are already in place. Instead of modifying Jupyter's existing codebase, I built it from scratch with a React frontend and a Rust backend.While performance wasn't the main focus, implementing a Python kernel driver in Rust keeps the kernel clean and avoids loading Python dependencies that might interfere with user code. Plus, as we've seen with other projects, rewriting classic Python tools in Rust can open up new possibilities.]]></content:encoded></item><item><title>[D] Enriching token embedding with last hidden state?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</link><author>/u/Academic_Sleep1118</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 18:06:18 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Looking at a decoder transformer working process from an information theory standpoint, we can see that the information available in the last hidden state is collapsed into a single token during generation. It means that you collapse a hidden state that, in theory, has about: (or whatever quant) bits of information to something like:I wonder if it's a good thing (sorry for the naive phrasing). The information used by a transformer to predict the next token is entirely stored in its context window and does not involve any recurrent state. So, predicting the next token of a sequence the transformer was just fed with is going to yield the exact same result as doing so for the same sequence if it were entirely generated by the transformer itself.Fair enough, in some sense: whether the sequence was generated or just read doesn't change anything about what the next token should be.But on the other hand, this approach means that  the information flow between tokens has to happen through the attention mechanism. There's no way for the transformer to embed some nuance or flavor into the predicted token embedding. Like in:"Well, I predicted the token 'When the next token is predicted, this nuance that was likely present in the last hidden state (or even in the softmaxed output probability distribution) is totally lost.So while I was having a little walk yesterday, I was thinking that it might be a good idea to add some information to the token embeddings using something like:augmented_embedding = embedding(token) + F(last_hidden_state)(It would be important to make sure that:‖F(last_hidden_state)‖ ≪ ‖embedding(token)‖I have tried to find papers on this subject and asked for feedback from Claude, ChatGPT, and Perplexity. told me it was "an incredibly insightful idea." hallucinated a paper on the subject. gave me a very long list of totally unrelated sources.So I'm turning to you guys. I would love it if some big-brained guy told me why other big-brained guys decided not to follow this idea, or why it doesn't work.Here are some things I identified as potentially problematic:Transformers are nice to train with heavy parallelization precisely because they are not recursive. Each sequence of size  can give  independent training examples. Injecting last hidden states' information in token embeddings would break some of that parallelization.It would still be possible to train it efficiently, I guess.First, take the () vanilla sequences and get the predictions.Then, for each prediction, store the last hidden state and update the corresponding token embedding in each of the sequences where it appears.Now, you have a new set of training sequences, with all (but the first) token embeddings updated.You can repeat this process indefinitely. I hope it converges ^^This really looks like a diffusion process, by the way. That brings me to the next point:Here, I am not very competent. What are the conditions that define such a process' stability? My uneducated guess is that if you keep:‖last_hidden_state_contribution‖ ≪ ‖augmented_token_embedding‖ you should not have many problems. But it would also limit the information flow. I guess there's a trade-off, and I wouldn't be surprised if it's not good enough.What do you guys think? Has this already been tried somewhere? Is there a fundamental reason this wouldn't work?]]></content:encoded></item><item><title>Thoughts on an AI powered bipedal, musculoskeletal , anatomically accurate, synthetic human with over 200 degrees of freedom, over 1,000 Myofibers, and 500 sensors?</title><link>https://v.redd.it/b1iwrsu32cke1</link><author>/u/VivariuM_007</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 17:57:23 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Announcing Rust 1.85.0 and Rust 2024 | Rust Blog</title><link>https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html</link><author>/u/slanterns</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 17:11:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.85.0. This stabilizes the 2024 edition as well.
Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.85.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!We are excited to announce that the Rust 2024 Edition is now stable!
Editions are a mechanism for opt-in changes that may otherwise pose a backwards compatibility risk. See the edition guide for details on how this is achieved, and detailed instructions on how to migrate.This is the largest edition we have released. The edition guide contains detailed information about each change, but as a summary, here are all the changes:The guide includes migration instructions for all new features, and in general
transitioning an existing project to a new edition.
In many cases  can automate the necessary changes. You may even find that no changes in your code are needed at all for 2024!Note that automatic fixes via  are very conservative to avoid ever changing the semantics of your code. In many cases you may wish to keep your code the same and use the new semantics of Rust 2024; for instance, continuing to use the  macro matcher, and ignoring the conversions of conditionals because you want the new 2024 drop order semantics. The result of  should not be considered a recommendation, just a conservative conversion that preserves behavior. people came together to create this edition. We'd like to thank them all for their hard work!Rust now supports asynchronous closures like  which return futures when called. This works like an  which can also capture values from the local environment, just like the difference between regular closures and functions. This also comes with 3 analogous traits in the standard library prelude: , , and .In some cases, you could already approximate this with a regular closure and an asynchronous block, like . However, the future returned by such an inner block is not able to borrow from the closure captures, but this does work with  closures:let mut vec: Vec<String> = vec![];

let closure = async || {
    vec.push(ready(String::from("")).await);
};
It also has not been possible to properly express higher-ranked function signatures with the  traits returning a , but you can write this with the  traits:use core::future::Future;
async fn f<Fut>(_: impl for<'a> Fn(&'a u8) -> Fut)
where
    Fut: Future<Output = ()>,
{ todo!() }

async fn f2(_: impl for<'a> AsyncFn(&'a u8))
{ todo!() }

async fn main() {
    async fn g(_: &u8) { todo!() }
    f(g).await;
    //~^ ERROR mismatched types
    //~| ERROR one type is more general than the other

    f2(g).await; // ok!
}
Hiding trait implementations from diagnosticsThe new #[diagnostic::do_not_recommend] attribute is a hint to the compiler to not show the annotated trait implementation as part of a diagnostic message. For library authors, this is a way to keep the compiler from making suggestions that may be unhelpful or misleading. For example:pub trait Foo {}
pub trait Bar {}

impl<T: Foo> Bar for T {}

struct MyType;

fn main() {
    let _object: &dyn Bar = &MyType;
}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
 --> src/main.rs:9:29
  |
9 |     let _object: &dyn Bar = &MyType;
  |                             ^^^^ the trait `Foo` is not implemented for `MyType`
  |
note: required for `MyType` to implement `Bar`
 --> src/main.rs:4:14
  |
4 | impl<T: Foo> Bar for T {}
  |         ---  ^^^     ^
  |         |
  |         unsatisfied trait bound introduced here
  = note: required for the cast from `&MyType` to `&dyn Bar`
For some APIs, it might make good sense for you to implement , and get  indirectly by that blanket implementation. For others, it might be expected that most users should implement  directly, so that  suggestion is a red herring. In that case, adding the diagnostic hint will change the error message like so:#[diagnostic::do_not_recommend]
impl<T: Foo> Bar for T {}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
  --> src/main.rs:10:29
   |
10 |     let _object: &dyn Bar = &MyType;
   |                             ^^^^ the trait `Bar` is not implemented for `MyType`
   |
   = note: required for the cast from `&MyType` to `&dyn Bar`
 and  for tuplesEarlier versions of Rust implemented convenience traits for iterators of  tuple pairs to behave like , with  in 1.56 and  in 1.79. These have now been  to more tuple lengths, from singleton  through to 12 items long, . For example, you can now use  to fanout into multiple collections at once:use std::collections::{LinkedList, VecDeque};
fn main() {
    let (squares, cubes, tesseracts): (Vec<_>, VecDeque<_>, LinkedList<_>) =
        (0i32..10).map(|i| (i * i, i.pow(3), i.pow(4))).collect();
    println!("{squares:?}");
    println!("{cubes:?}");
    println!("{tesseracts:?}");
}
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]
[0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]
Updates to  has been deprecated for years, because it can give surprising results in some Windows configurations if the  environment variable is set (which is not the normal configuration on Windows). We had previously avoided changing its behavior, out of concern for compatibility with code depending on this non-standard configuration. Given how long this function has been deprecated, we're now updating its behavior as a bug fix, and a subsequent release will remove the deprecation for this function.These APIs are now stable in const contextsMany people came together to create Rust 1.85.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Ugly Code and Dumb Things</title><link>https://lucumr.pocoo.org/2025/2/20/ugly-code/</link><author>/u/FoxInTheRedBox</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 16:57:00 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[written on Thursday, February 20, 2025This week I had a conversation with one of our engineers about “shitty
code” which lead me to sharing with him one of my more unusual
inspirations: Flamework, a
pseudo framework created at Flickr.Two Passions, Two ApproachesThere are two driving passions in my work.  One is the love of creating
beautiful, elegant code — making Open Source libraries and APIs that focus
on clear design and reusability.  The other passion is building quick,
pragmatic solutions for real users (who may not even be developers).  The
latter usually in a setting of building a product, where the product is
not the code.  Here, speed and iteration matter more than beautiful code
or reusability, because success hinges on shipping something people want.Flamework is in service of the latter, and in crass violation of the
former.Early on, I realized that creating reusable code and directly solving
problems for users are often at odds.  My first clue came when I helped
run the German
ubuntuusers website.  It was powered by
a heavily modified version of phpBB, which despite how messy it was,
scaled to a large user base when patched properly.  It was messy, but easy
to adjust.  The abstractions were one layer deep.Back then, me and a friend tried to replace it by writing my own bulletin
board software, Pocoo.
Working in isolation, without users, led me down a path of
over-engineering.  While we learned a lot and ended up creating popular
Open Source libraries (like Jinja, Werkzeug and Pygments), Pocoo never
became a solid product.  Later, my collaborators and I rebuilt
ubuntuusers, without the
goal of making it into a reusable product.  That rewrite shipped
successfully and it lives to this very day.But it took me years to fully realize what was happening here: reusability
is not that important when you’re building an application, but it’s
crucial when you’re building a library or framework.If you are unfamiliar with Flamework you should watch a talk that Cal
Henderson gave in 2008 at DjangoCon (Why I hate Django).  He talked about scale
and how Django didn't solve for it.  He enumerated all the things
important to him: sharding, using custom sequences for primary keys,
forgoing joins and foreign keys, supporting database replication setups,
denormalizing data to the extreme.  This is also were I first learned
about the possibility of putting all session data into cookies via
signing.  It was a memorable talk for me because it showed me that there
are shortcomings.  Django (which I used for ubuntuusers) had beautiful
APIs but at the time solved for little of that Cal needed.  The talk
really stuck with me.At the time of the talk, Flamework did not really exist.  It was more of
an idea and principles of engineering at Flickr.A few years later, Flamework appeared on GitHub, not as an open-sourced
piece of Flickr code but as a reimplementation of those same ideas.  You
can explore its repository and see code like this:Instinctively it makes me cringe.  Is that a SQL injection?  Well you were
supposed to use the PHP addslashes function
beforehand.  But notice how it caters to sharding and clustering directly
in the query function.Code like this often triggers a visceral reaction, especially in engineers
who prize clean design.How does something like that get created?  Cal Henderson described
Flickr's principle as “doing the dumbest possible thing that will work.”
Maybe “dumb” is too strong — “simple” might be more apt.  Yet simplicity
can look messy to someone expecting a meticulously engineered codebase.
This is not at all uncommon and I have seen it over and over.  The first
large commercial project that got traction that I ever worked on (Plurk) was also pretty pragmatic and
messy inside.  My former colleague Ben Vinegar also recently shared a story of early,
messy FreshBooks code and how he came to terms with it.  Same story at
Sentry.  We moved fast, we made a mess.None of this is surprising in retrospective.  Perfect code doesn't
guarantee success if you haven't solved a real problem for real people.
Pursuing elegance in a vacuum leads to abandoned side projects or
frameworks nobody uses.  By contrast, clunky but functional code often
comes with just the right compromises for quick iteration.  And that in
turn means a lot of messy code powers products that people love —
something that's a far bigger challenge.I have shown Flamework's code to multiple engineers over the years and it
usually creates such a visceral response.  It blind sights one by
seemingly disregarding all rules of good software engineering.That makes Flamework serve as a fascinating Rorschach test for engineers.
Are you looking at it with
admiration for the focus on some critical issues like scale, the built-in
observability and debugging tools.  Or are you judging it, and its
creators, for manually constructing SQL queries, using global variables,
not using classes and looking like messy PHP4 code?  Is it a pragmatic
tool, intentionally designed to iterate quickly at scale, or is it a naive
mess made by unskilled developers?Would I use Flamework?  Hello no.  But I appreciate the priorities behind
it.  If these ugly choices help you move faster, attract users and
validate the product, then a rewrite, or large refactorings later are a
small price to pay.At the end of the day, where you stand on “shitty code” depends on your
primary goal:Are you shipping a product and racing to meet user needs?Or are you building a reusable library or framework meant to stand the
test of time?Both mindsets are valid, but they rarely coexist harmoniously in a single
codebase.  Flamework is a reminder that messy, simple solutions can be
powerful if they solve real problems.  Eventually, when the time is right,
you can clean it up or rebuild from the ground up.The real challenge is deciding which route to take — and when.  Even with
experience, it is can be hard to know when to move from quick fixes to
more robust foundations.  The principles behind Flamework are also
reflected in Sentry's development philosophy.  One more
poignant one being “Embrace the Duct Tape”.  Yet as Sentry matured, much
of our duct tape didn't stand the test of time, and was re-applied at
moments when the real solution would have been a solid foundation poured
with concrete.That's because successful projects eventually grow up.  What let you
iterate fast in the beginning might eventually turn into an unmaintainable
mess and will be rebuilt from the inside out.I personally would never have built Flamework, it repulses me a bit.  At the
same time, I have a enormous respect for the people who build it.  Their
work and thinking has shaped how I solve problems and think of product
engineering.]]></content:encoded></item><item><title>Why Firefox?</title><link>https://www.reddit.com/r/linux/comments/1iu25zd/why_firefox/</link><author>/u/Flaky_Comfortable425</author><category>linux</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 16:13:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This actually makes me curious, when I switch between a lot of distros, jumping from Debian to CentOS to dfferent distros, I can see that they all love firefox, it's not my favorite actually, and there are plenty of internet browsers out there which is free and open source like Brave for example, still I am wondering what kind of attachment they have to this browser]]></content:encoded></item><item><title>C equivalent of select() / poll() in go socket programming</title><link>https://www.reddit.com/r/golang/comments/1iu1ugj/c_equivalent_of_select_poll_in_go_socket/</link><author>/u/ChestPainGuy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 16:00:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, I'm fairly new to socket programming and go, so forgive my ignorance.Recently, I have been reading up Beej's guide to network programming, where he explains the use of  and  to read and write to multiple sockets without blocking.I have googled quite a bit, but almost every tutorial or go example on the basics of socket connections just spawn a new goroutine with something like .So whats' the equivalent of  in go?Is spawning a goroutine for every connection an effective approach?Any good links to network programming in go would be appreciated if this question is too dumb. Thanks]]></content:encoded></item><item><title>Is anyone working on AI designed to preserve democracy?</title><link>https://www.reddit.com/r/artificial/comments/1iu1n10/is_anyone_working_on_ai_designed_to_preserve/</link><author>/u/BarbaGramm</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 15:51:52 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[I’m looking for people or groups who are already working on something like this:A decentralized AI trained to preserve the intellectual, historical, and emotional essence of democracy—what it actually means, not just what future regimes might redefine it to be. Think of it as a fusion of data hoarding, decentralized AI, and resistance tech, built to withstand authoritarian drift and historical revisionism.Maybe it doesn't reach the heights of the corporate or state models, but a system that can always articulate the delta—the difference between a true democratic society (or at least what we seem to be leaving behind) and whatever comes next. If democracy gets twisted into something unrecognizable, this AI should be able to compare, contrast, and remind people what was lost. It should be self-contained, offline-capable, decentralized, and resistant to censorship—an incorruptible witness to history.Does this exist? Are there people in AI, decentralized infrastructure, or archival communities working toward something like this? I don’t want to reinvent the wheel if a community is already building it. If you know of any projects, frameworks, or people tackling this problem, please point me in the right direction.If no one is doing it, shouldn't this be a project people are working on? Is there an assumption that corporate or state controlled AI will do this inherently?]]></content:encoded></item><item><title>Slice Internals in Go: How the Runtime Expands Slices Efficiently</title><link>https://themsaid.com/slice-internals-in-go</link><author>/u/themsaid</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 15:21:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The deeper you delve into Go’s internals, the more evident it becomes that its creators carefully engineered the language to strike a precise balance between performance and flexibility. This delicate equilibrium influences many of Go’s core features, including its approach to memory management and data structures. One standout example of this thoughtful design is the implementation of slice growth. Through this approach, Go ensures that slices expand seamlessly, optimizing both performance and memory usage without compromising ease of use.In Go, a slice is a lightweight data structure that serves as a window into a contiguous block of memory where elements of a specific type are stored. At its core, a slice doesn’t directly contain the data itself but instead holds a pointer to an underlying array (know as the backing array.)When the Go runtime creates a slice, as in this example, it constructs a small struct under the hood, defined in the runtime as : {
	 unsafe.
}The  type has the following fields: holds a pointer to the underlying array. stores the number of elements in the slice. stores the capacity of the array.The  type used for the  field is a generic pointer type that bypasses Go's type safety rules. Since the size of an array in Go is part of the type, the runtime uses  so it can replace the array with a larger one when needed.The code in the example above creates a slice that has zero elements with a backing array that can hold 10 elements of type . We can later fill that array with elements by using the  function:Here, we append a byte to the empty array represented by the unsigned 8-bit integer .When appending elements to a slice, the Go runtime first checks whether the backing array has enough capacity to accommodate the new elements. If it does, the elements are simply added to the existing array. However, if the current array lacks sufficient space, the runtime allocates a larger backing array, copies the existing elements into it, and then appends the new elements.([], , )

(, ) (, ) The capacity of the newly allocated array is determined by several factors, including the current array’s capacity, the type of elements it holds, and the number of new elements being appended. These factors influence how much the array grows, ensuring efficient memory usage while minimizing the need for frequent reallocations.The runtime begins by attempting to double the existing capacity as the first step in determining the new array size:If the total number of existing and newly appended elements exceeds the doubled capacity, the runtime sets the new capacity to match the required number of elements: {
    
}This ensures that the new capacity is larger than or equals to the number of elements after the appending operation.([], , )

(, , , )

.(()) In this example, the integer slice initially had a capacity of 1. After adding three elements, the runtime allocated a new backing array with a capacity of 3. This happened because doubling the original capacity (1 * 2) was insufficient to accommodate the new elements, prompting the runtime to adjust the capacity accordingly.If doubling the capacity is sufficient, the runtime further evaluates whether allocating such a large array is efficient or merely a waste of memory.For small slices, capacity less than 256, the runtime employs a simple doubling strategy (e.g., 2 to 4, 4 to 8, 8 to 16.) This makes sense for small workloads: doubling ensures plenty of headroom for future appends. However, as the slice’s capacity climbs into more than 256, or beyond, doubling becomes less practical. Doubling a capacity of, say, 10,000 to 20,000 allocates an extra 10,000 elements’ worth of memory (potentially tens or hundreds of kilobytes, depending on the element size) which might sit unused for a long time.To address this, the runtime adjusts its growth strategy for larger slices by reducing the growth factor gradually until it reaches 1.25. This slower growth means that if a slice already has a capacity of, say, 512, adding a few elements doesn’t balloon it to 1024; it might rise to 832 instead (a 62.5% increase). The key insight is that a larger slice can absorb more appends before hitting its capacity limit. For instance, a slice with a capacity of 512 has room for 512 more elements if empty, compared to just 8 for a capacity of 8. This naturally delays the need for reallocation.This conservative approach aims to curb excessive memory usage. By growing incrementally rather than exponentially, the runtime avoids reserving vast swaths of memory that might remain idle, which is critical in applications handling large datasets or with limited resources (e.g., embedded systems). However, there’s a flip side: smaller growth steps mean the slice fills up sooner, triggering reallocation more often. Each reallocation involves CPU work (allocating memory, copying the existing elements, and updating the slice’s pointer) which can add up if appends are frequent.The runtime’s strategy thus balances these two forces: memory footprint versus CPU overhead. It leans toward saving memory at the cost of potentially more frequent (but smaller) reallocations, betting that the trade-off pays off in most real-world scenarios where slices don’t grow indefinitely.When determining how a slice should grow, the Go runtime takes into account the type of elements stored in the array, as this directly impacts memory allocation. On 64-bit systems, memory is generally allocated in chunks of 8 bytes. Any allocation that does not align with this rule is rounded up to the nearest multiple of 8 to ensure efficient memory usage and alignment.Let's say we create a slice of bytes with capacity zero and then append an element to it:([], , )

(, )

.(()) After the growth, the capacity of the slice becomes 8 (8 bytes). If the element type was a 64-bit integer instead, the growth will increase the capacity to 1 (1 * 64 bits = 8 bytes):([], , )

(, )

.(()) If the element type was a 32-bit integer, the growth will increase the capacity to 2 (2 * 32 bits = 8 bytes):([], , )

(, )

.(()) The reason is that modern CPUs, particularly on 64-bit systems, operate most efficiently when data is aligned to their word size (the amount of data they can process in one cycle.) On a 64-bit system, the word size is 64 bits, or 8 bytes. If the runtime allocates, say, 5 bytes, the CPU  and masks off the unused portion. That means, allocating in 8-byte multiples ensures the entire chunk is usable without waste or extra work.In addition, CPU caches fetch memory in 64-byte lines (8 words of 8 bytes each.) Multiples of 8 bytes fit neatly into these lines, reducing cache misses and improving locality when accessing sequential data, like a slice’s backing array.In addition to the 8-byte chunk allocation rule, the Go runtime maintains a table of predefined constants to guide its memory allocation decisions. This table categorizes memory allocations into specific size classes, helping the runtime minimize fragmentation and efficiently reuse freed memory blocks.The table looks like this:+---------+-------+
| Class   | Value |
+---------+-------+
| Class  |      |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |    |
| .     | .   |
+---------+-------+This size class allocation table functions as an efficient lookup mechanism for managing memory allocation and deallocation. When a memory block belonging to a specific size class is freed, the runtime stores it in the table rather than immediately returning it to the operating system. Later, if a request is made for a memory block of the same size class, the system can quickly retrieve and reuse the previously freed block instead of performing an extensive search through physical memory to find a suitable allocation.+---------------------------------+
|  Freed memory  size class X  |
+---------------------------------+
        │
        ▼
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
        │
        ▼
+-------------------------------------+
| Incoming Memory Allocation Request  |
+-------------------------------------+
        │
          (Lookup in Table)
+-------------------------------+
| Matching Freed Block Found    |
+-------------------------------+
        │
          (Reused Instead of )
+----------------------------+
|  Allocated to Application  |
+----------------------------+With that in mind, the runtime not only rounds up to the nearest 8-byte boundary but also rounds up to the nearest size class in the allocation table.Consider this slice operation:([], , )

(, , , , , )Here, the slice starts with zero capacity and we add 5 elements of type . Without considering the size class allocation table, the runtime would allocate 40 bytes for the new backing array: *  bits =  bits /  =  bytesInstead, the runtime consults the class allocation table and rounds up to the nearest match (48 in this case). As a result, it allocates a backing array with a capacity of 6 (48 bytes / 64 bits), even though the new array would only need to hold 5 elements (that require only 40 bytes).This approach significantly improves performance by reducing external fragmentation (where free memory is scattered in small, non-contiguous blocks, making larger allocations difficult.) It also minimizes allocation overhead and speeds up memory access by eliminating the need to repeatedly request new memory from the operating system.In summary, the Go runtime takes several key factors into account when growing an array:: It starts with a doubling factor (2x) and then gradually winds down to 1.25x.: The array is rounded up to the nearest 8-byte boundary.The Size Class Allocation Table: The runtime rounds up to the nearest available class in the table.I really admire the thoughtful work the Go team has put into making the language both efficient and flexible. It's clear that a lot of careful consideration went into optimizing performance while maintaining flexibility for developers.]]></content:encoded></item><item><title>Rust 2024 Is Coming: baby steps</title><link>https://smallcultfollowing.com/babysteps/blog/2025/02/20/rust-2024-is-coming/?utm_source=atom_feed</link><author>/u/VorpalWay</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 14:53:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So, a little bird told me that Rust 2024 is going to become stable today, along with Rust 1.85.0. In honor of this momentous event, I have penned a little ditty that I’d like to share with you all. Unfortunately, for those of you who remember Rust 2021’s “Edition: The song”, in the 3 years between Rust 2021 and now, my daughter has realized that her father is deeply uncool and so I had to take this one on solo. Anyway, enjoy! Or, you know, suffer. As the case may be.In ChordPro format, for those of you who are inspired to play along.{title: Rust 2024}
{subtitle: }

{key: C}

[Verse 1]
[C] When I got functions that never return
I write an exclamation point [G]
But use it for an error that could never be
the compiler [C] will yell at me

[Verse 2]
[C] We Rust designers, we want that too
[C7] But we had to make a [F] change
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Bridge]
[Am] ... [Am] But will my program [E] build?
[Am] Yes ... oh that’s [D7] for sure
[F] edi-tions [G] are [C] opt in

[Verse 3]
[C] Usually when I return an `impl Trait`
everything works out fine [G]
but sometimes I need a tick underscore
and I don’t really [C] know what that’s for

[Verse 4]
[C] We Rust designers we do agree
[C7] That was con- [F] fusing 
[F] But that will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Bridge 2]
[Am] Cargo fix will make the changes
automatically [G] Oh that sure sounds great...
[Am] but wait... [Am] my de-pen-denc-[E]-ies
[Am] Don’t worry e-[D7]ditions
[F] inter [G] oper [C] ate

[Verse 5]
[C] Whenever I match on an ampersand T
The borrow [G] propagates
But where do I put the ampersand
when I want to [C] copy again?

[Verse 6]
[C] We Rust designers, we do agree
[C7] That really had to [F] change
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Outro]
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

One more time!

[Half speed]
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four
]]></content:encoded></item><item><title>[D] Deepseek 681bn inference costs vs. hyperscale?</title><link>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</link><author>/u/sgt102</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 13:44:05 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've estimated the cost/performance of Deepseek 681bn like this :Huggingface open deepseek blog reported config & performance = 32 H100's 800tps 1million tokens = 1250s = 21 (ish) , minutes. 69.12 million tokens per day Cost to rent 32 H100's per month ~$80000Cost per million tokens = $37.33 (80000/ 31 days /69.12 ) I know that this is very optimistic (100% utilisation, no support etc.) but does the arithmetic make sense and does it pass the sniff test do you think? Or have I got something significantly wrong? I guess this is 1000 times more expensive than an API served model like Gemini, and this gap has made me wonder if I am being silly]]></content:encoded></item><item><title>I still have these</title><link>https://www.reddit.com/r/linux/comments/1itynca/i_still_have_these/</link><author>/u/emuboy85</author><category>linux</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 13:37:18 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chromium Ozone/Wayland: The Last Mile Stretch</title><link>https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/</link><author>/u/Worldly_Topic</author><category>linux</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 13:27:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hey there! I’m glad to finally start paying my blogging debt :) as this
is something I’ve been planning to do for quite some time now. To get the
ball rolling, I’ve shared some bits about me in my very first blog post
Olá Mundo.In this article, I’m going to walk through what we’ve been working on
since last year in the Chromium Ozone/Wayland project, on which I’ve
been involved (directly or indirectly) since I’ve joined Igalia back in
2018.Lets start with some context, the project consists of implementing,
shipping and maintaining native Wayland support in the
Chromium project. Our team at Igalia has been leading the
effort since it was first merged upstream back in 2016. For more
historical context, there are a few blogposts and this amazing talk, by my colleagues
Antonio Gomes and Max Ihlenfeldt, presented at last year’s Web Engines
Hackfest.Especially due to the Lacros project, progresses on Linux
Desktop has been slower over the last few years. Fortunately, the
scenario changed since last year, when a new sponsor came up and made it
possible to address most of the outstanding missing features and issues
required to move Ozone Wayland to the finish line.It’s been a few months since Chromium Wayland backend has started to be
tested as the main browser backend by Google employees, through a finch
trial experiment, as well as internally at Igalia. Feedback collected
since then is quite positive in general. The exception is Nvidia setups,
which, depending on the driver version, may face major regressions (see
Explicit Sync section below for more details).While official roll-out has been under discussion, it’s still disabled
by default on Linux Desktop. Early adopters willing to test it are
encouraged to explicitly opt-in by flipping the 
chrome flag to  or . Issue reports are welcome at
crbug.com/new.There are also a few other Wayland-specific flags which might be
selectively enabled, if you feel brave enough :) such as, ui scaling,
text input v3, etc; all described in more details below.Chrome Wayland flags available in M135.Initial fractional scaling support for Linux Desktop was originally
implemented by an external contributor back in
2023.
After some months of stabilization, reports of blurriness and some
other subtle issues started to
pop up, which were listed as top-priority when this new project phase
kicked off back in 2024 June.After some analysis, we could confirm that there were some fundamental
missing bits in the process. Which was the actual usage of the fractional
scale values provided by the Wayland compositor, via
the fractional-scale-v1
protocol extension. Rather than using it, fractional scales were being
 using xdg-output
protocol instead, which is unsupported (for such usage) and prone to
precision issues.The screenshot above shows a sharp Chrome window scaled by a 1.25
factor, running on Gnome Shell 47.The work involved a considerable architectural refactoring to make it
possible to support the per-window scaling design of the Wayland
protocol, without breaking the standard per-display scaling implemented
in Chromium. The feature started shipping experimentally in Milestone
128, behind the  chrome flag and disabled by
default until M135. I plan to cover it in more details soon in separate
blog post.IME has been yet another major pain point for some Wayland users. Back
in last June, a careful study was conducted by my colleague Orko
Garai in order to understand and consolidate the
possible approaches to tackle it, as well as pros, cons and potential
road-blockers for each of them. Besides the detailed outcomes of that
study, publicly available in the form of a design
document,
Orko has recently started a blog post series about the topic
here.Experimental support for
text-input-v3
protocol was implemented on the Chromium side, with ongoing work on the
Wayland community side to fullfill browser use cases from a protocol
perspective, which is expected to come soon as part of version
3.2
of the text-input protocol.In the meantime, we keep working with the Chromium community on
improvements to the client-side implementation of the protocol, although
progress has been slow as we are still looking for some key browser
requirements to be solved on the protocol side to have the confidence
and buy-in for productization.Supporting the full original user experience for Chrome’s tab dragging
under Wayland has proved to be complex, especially because the core
Wayland protocol does not cover all of its requirements. Back in 2021,
we designed a brand new protocol and implemented it in ChromeOS’ Exo
Wayland compositor, in the context of Lacros project, to fullfill those
gaps.Years later,
xdg-toplevel-drag
has emerged as a community effort to standardize it upstream, thanks
David Redondo and Robert Mader for working on it. It consists of a
trimmed down version of the original  protocol with
some tweaks to make it more aligned with Wayland design principles. A
few months ago, initial support for it has landed in Chromium and we’ve
been stabilizing it since then.Full UX support in Mutter#Back in last November, xdg-toplevel-drag was
supported
only in KWin and Jay compositors, when a demand to implement support for
it in Mutter was raised by our customer, and the task was assigned to
me.Long story short, it was a pretty interesting and challenging experience
which I’m glad to have had. As a curiosity, last time I had coded in C
and glib had been , maybe >13 years? 😱 Also, it was my
first time hacking on Mutter and Gnome code base. After all, the
MR did
landed and will start shipping as part of Gnome 48, next month.xdg-toplevel-drag-v1 demo on Gnome Shell 48.I’m preparing a blog post to share more technical details about the
protocol implementation from the perspective of a browser developer and
Gnome/Mutter newcomer.Let me take the opportunity to say thanks to the Gnome developers who
helped me a lot in the process: Jonas Ådahl, Carlos Garnacho, Georges
Stavracas and Sebastian Wick. Really appreciate your help and patience,
guys!In parallel to the work on the regular tab drag experience through
xdg-toplevel-drag, a fallback implementation relying solely on core
Wayland drag-and-drop protocol has been led by my colleague Max
Ihlenfeldt. A few days ago, Max has published an awesome in-depth blog
post about his
work on it. Enjoy the read!Fallback tab dragging UX demoThe main difference to the regular UX is that, rather than instantly
creating a browser window when it gets dragged out of its tab strip, a
drag icon containing the tab thumbnail is used instead. Browser window
creation is then deferred to when the drop happens, as can be seen in
the video above. The feature was recently enabled by default, and
started shipping in Chrome 133.Linux desktop environments usually support system-wide “text scaling”
settings, which are supposed to be handled by applications. On
Gnome-based environments, it can be triggered via several ways,
such as the “Large Text” accessibility feature.Historically, it has been supported in Chromium X11 by resizing the
whole browser UI elements, instead of just text items. After an in-depth
analysis, it was decided to follow the very same approach for the
Wayland initial implementation. The main motivation was that there seems
to be gaps in both Chromium’s internal UI framework as well as in
Chrome’s UI/layout code, which would need to be fixed before supporting
such text-only live resizing/relayout.Quick demo of text scale in action on Chromium Wayland on Gnome 47.Technically, the solution involved implementing an additional scaling
layer in Ozone/Wayland, so called “ui scale” which make the browser UI
to get fully resized/re-laid out instantly in reaction to system’s “text
scaling factor” updates.The feature has started shipping in Milestone 131, disabled by default
as usual. Users can enable it by using the chrome://flags/#wayland-ui-scaling
flag.To address some display tearing reports,
support for the linux-drm-syncobj-v1
protocol has been implemented. The patch series has landed and started
shipping in version 132.0.6834.83, and can be enabled using the
wayland-linux-drm-syncobj chrome flag.Feedback has been positive so far. If you’re willing to give it a try,
please bear in mind that Linux kernel version >= 6.11 is required, and
don’t hesitate to get back to us with your remarks.Besides overall stabilization and maintenance, there is a large ongoing
effort led by my colleague Orko to get Chromium’s interactive UI tests
infrastructure and code working with major Wayland compositors,
primary focus is Mutter/Gnome, though wlroots is also considered
for the future.Another area we are currently investigating is “session management”,
which will make it possible to restore browser window attributes, such
as, position, display and workspace across restarts.Aside from that, there are a bunch of technical debt and follow-up
issues which spun off from some of the features and fixes listed above,
such as:New sponsors and partners are always welcome, so please don’t hesitate
to mail us to discuss how we coud be of help.Yay! Quite busy and exciting times!! I’d like to thank once more all of
our supporters, sponsors and, of course, Igalia for making all
this possible ❤️ Looking forward to the challenges ahead! Stay tuned for
more updates and don’t hesitate to reach out if you have questions or
other remarks. 👋👋]]></content:encoded></item><item><title>The Fedora Project Leader is willfully ignorant about Flathub</title><link>https://blogs.gnome.org/alatiera/2025/02/19/the-fedora-project-leader-is-willfully-ignorant-about-flathub/</link><author>/u/Worldly_Topic</author><category>linux</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 13:12:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Today I woke up to a link of an interview from the current Fedora Project Leader, Matthew Miller. Brodie who conducted the interview mentioned that Miller was the one that reached out to him. The background of this video was the currently ongoing issue regarding OBS, Bottles and the Fedora project, which Niccolò made an excellent video explaining and summarizing the situation. You can also find the article over at thelibre.news. “Impressive” as this story is, it’s for another time.What I want to talk in this post, is the outrageous, smearing and straight up slanderous statements about Flathub that the Fedora Project Leader made during the interview..I am not directly involved with the Flathub project (A lot of my friends are), however I am a maintainer of the GNOME Flatpak Runtime, and a contributor to the Freedesktop-sdk and ElementaryOS Runtimes. I also maintain applications that get published on Flathub directly. So you can say I am someone invested in the project and that has put a lot of time into it. It was extremely frustrating to hear what would only qualify as reddit-level completely made up arguments with no base in reality coming directly from Matthew Miller.Below is a transcript, slightly edited for brevity, of all the times Flathub and Flatpak was mentioned. You can refer to the original video as well as there were many more interesting things Miller talked about.It starts off with an introduction and some history and around the 10-minute mark, the conversation starts to involve Flathub.Miller: [..] long way of saying I think for something like OBS we’re not really providing anything by packaging that. Miller: I think there is an overall place for the Fedora Flatpaks, because Flathub part of the reason its so popular (there’s a double edged sword), (its) because the rules are fairly lax about what can go into Flathub and the idea is we want to make it as easy for developers to get their things to users, but there is not really much of a reviewThis is not the main reason why Flathub is popular, its a lot more involved and interesting in practice. I will go into this in a separate post hopefully soon.Claiming that Flathub does not have any review process or inclusion policies is straight up wrong and incredibly damaging. It’s the kind of thing we’ve heard ad nauseam from Flathub haters, but never from a person in charge of one of the most popular distributions and that should have really really known better.You can find the Requirements in the Flathub documentation if you spend 30 seconds to google for them, along with the submission guidelines for developers. If those documents qualify as a wild west and free for all, I can’t possibly take you seriously.I haven’t maintained a linux distribution package myself so I won’t go to comparisons between Flathub and other distros, however you can find people, with red hats even, that do so and talked about it. Of course this is one off examples and social bias from my part. But it proves how laughable of a claim is that things are not reviewed. Additionally, the most popular story I hear from developers is how Flathub requirements are often stricter and sometimes cause annoyances.Additionally, Flathub has been the driving force behind encouraging applications to update their metadata, completely reworking the User Experience and handling off permissions and made them prominent to the user. (To the point where even network access is marked as potentially-unsafe).Miller: [..] the thing that says verified just says that it’s verified from the developer themselves.No, verified does not mean that the developer signed off into it. Let’s take another 30 seconds to look into the Flathub documentation page about exactly this.A verified app on Flathub is one whose developer has confirmed their ownership of the app ID […]. This usually also may mean that either the app is maintained directly by the developer or a party authorized or approved by them.It still went through the review process and all the rest of requirements and policies apply. The verified program is basically a badge to tell users this is a supported application by the upstream developers, rather than the free for all that exists currently where you may or may not get an application released from years ago depending on how stable your distribution is.Sidenote, did you know that 1483/3003 applications on Flathub are verified as of the writing of this post? As opposed to maybe a dozen of them at best in the distributions. You can check for yourselfMiller: .. and it doesn’t necessarily verify that it was build with good practices, maybe it was built in a coffee shop on some laptop or whatever which could be infected with malware or whatever could happenAgain if Miller had done the bare minimum effort, he would have come across the Requirements page which describes exactly how an Application in Flathub is built, instead of further spreading made up takes about the infrastructure. I can’t stress enough how damaging it has been throughout the years to claim that “Flathub may be potential Malware”. Why it’s malware? Because I don’t like its vibes and I just assume so..I am sure If I did the same about Fedora in a very very public medium with thousand of listeners I would probably end up with a Layers letter from Redhat.Now Applications in Flathub are all built without a network access, in Flathub’s build servers, using flatpak-builder and Flatpak Manifests which are a declarative format, which means all the sources required to build the application are known, validated/checksumed, the build is reproducible to the extend possible, you can easily inspect the resulting binaries and the manifest itself used to build the application ends up in  which you can also inspect with the following command and use it to rebuild the application yourself exactly like how it’s done in Flathub.The exception to this, are proprietary applications naturally, and a handful of applications (under an OSI approved license) where Flathub developers helped the upstream projects integrate a direct publishing workflow into their Deployment pipelines. I am aware of Firefox and OBS as the main examples, both of which publish in Flathub through their Continues Deployment (CI/CD) pipeline the same way they generate their builds for other platforms they support and the code for how it happens is available on their repos.If you have issues trusting Mozilla’s infrastructure, then how are you trusting Firefox in the first place and good luck auditing gecko to make sure it does not start to ship malware. Surely distribution packagers audit every single change that happens from release to release for each package they maintain and can verify no malicious code ever gets merged. The xz backdoor was very recent, and it was identified by pure chance, none of this prevented it.Then Miller proceeds to describe the Fedora build infrastructure and afterward we get into the following:Miller: I will give an example of something I installed in Flathub, I was trying to get some nice gui thing that would show me like my system Hardware stats […] one of them ones I picked seemed to do nothing, and turns out what it was actually doing, there was no graphical application it was just a script, it was running that script in the background and that script uploaded my system stats to a server somewhere.Firstly we don’t really have many details to be able to identify which application it was, I would be very curious to know. Now speculating on my part, the most popular application matching that description it’s Hardware Probe and it absolutely has a GUI, no matter how minimal. It also asks you before uploading.Maybe there is a org.upload.MySystem application that I don’t know about, and it ended up doing what was in the description, again I would love to know more and update the post if you could recall!Miller: No one is checking for things like that and there’s no necessarily even agreement that that was was bad.Second time! Again with the “There is no review and inclusion process in Flathub” narrative. There absolutely is, and these are the kinds of things that get brought up during it.Miller: I am not trying to be down on Flathub because I think it is a great resourceYes, I can see that, however in your ignorance you were something much worse than “Down”. This is pure slander and defamation, coming from the current “Fedora Project Leader”, the “Technically Voice of Fedora” (direct quote from a couple seconds later). All the statements made above are manufactured and inaccurate. Myths that you’d hear from people that never asked, looked or cared about any of these cause the moment you do you its obvious how laughable all these claims are.Miller: And in a lot of ways Flathub is a competing distribution to Fedora’s packaging of all applications.Precisely, he is spot on here, and I believe this is what kept Miller willfully ignorant and caused him to happily pick the first anit-flatpak/anti-flathub arguments he came across on reddit and repeat the verbatim without putting any thought into it. I do not believe Miller is malicious on purpose, I do truly believe he means well and does not know better.However, we can’t ignore the conflict that arises from his current job position as an big influence to why incidents like this happened. Nor the influence and damage this causes when it comes from a person of Matthew Miller’s position.Miller: One of the other things I wanted to talk about Flatpak, is the security and sandboxing around it. Miller: Like I said the stuff in the Flathub are not really reviewed in detail and it can do a lot of things:Third time with the no review theme. I was fuming when I first heard this, and I am very very angry about still, If you can’t tell. Not only is this an incredibly damaging lie as covered above, it gets repeated over and over again.With Flatpak basically the developer defines what the permissions are. So there is a sandbox, but the sandbox is what the person who put it there is, and one can imagine that if you were to put malware in there you might make your sandboxing pretty loose.Brodie: One of the things you can say is “I want full file system access, and then you can do anything”No, again it’s stated in the Flathub documentation, permissions are very carefully reviewed and updates get blocked when permissions change until another review has happened.Miller: Android and Apple have pretty strong leverage against application developers to make applications work in their sandboxBrodie: the model is the other way around where they request permissions and then the user grants them whereas Flatpak, they get the permission and then you could reject them laterThis is partially correct, the first part about leverage will talk about in a bit, but here’s a primer on how permissions work in Flatpak and how it compares to the sandboxing technologies in iOS and Android.In all of them we have a separation between Static and Dynamic permissions. Static are the ones the application always has access to, for example the network, or the ability to send you notifications. These are always there and are mentioned at install time usually. Dynamic permissions are the ones where the application has to ask the user before being able to access a resource. For example opening a file chooser dialog so the user can upload a file, the application the only gets access to the file the user consented or none. Another example is using the camera on the device and capturing photos/video from it.Brodie here gets a bit confused and only mentions static permissions. If I had to guess it would be cause we usually refer to the dynamic permissions system in the Flatpak world as “Portals”.Miller: it didn’t used to be that way and and in fact um Android had much weaker sandboxing like you could know read the whole file system from one app and things like that […] they slowly tightened it and then app developers had to adjust Miller: I think with the Linux ecosystem we don’t really have the way to tighten that kind of thing on app developers … Flatpak actually has that kind of functionality […] with portals […] but there’s no not really a strong incentive for developers to do that because, you know well, first of all of course my software is not going to be bad so why should I you know work on sandboxing it, it’s kind of extra work and I I don’t know I don’t know how to solve that. I would like to get to the utopian world where we have that same security for applications and it would be nice to be able to install things from completely untrusted places and know that they can’t do anything to harm your system and that’s not the case with it right nowAs with any technology and adoption, we don’t get to perfection from day 1. Static permissions are necessary to provide a migration path for existing applications and until you have developed the appropriate and much more complex dynamic permissions mechanisms that are needed. For example up until iOS 18 it wasn’t possible to give applications access to a subset of your contacts list. Think of it like having to give access your entire filesystem instead of the specific files you want. Similarly partial-only access to your photos library arrived couple years ago in IOS and Android.In an ideal world all permissions are dynamic, but this takes time and resources and adaptation for the needs of applications and the platform as development progresses.Now about the leverage part.I do agree that “the Linux ecosystem” as a whole does not have any leverage on applications developers. This is cause Miller is looking at the wrong place for it. There is no Linux ecosystem but rather Platforms developers target.GNOME and KDE, as they distribute all their applications on Flathub absolutely have leverage. Similarly Flathub itself has leverage by changing the publishing requirements and inclusion guidelines. Which I kept being told they don’t exist.. Every other application that wants to publish also has to adhere by the rules on Flathub. ElementaryOS and their Appcenter has leverage on developers. Canonical does have the same pull as well with the Snapstore. Fedora on the other hand doesn’t have any leverage cause the Fedora Flatpak repository is irrelevant, broken and nobody wants to use it.[..] The xz backdoor gets brought up when discussing dependencies and how software gets composed together.Miller: we try to keep all of those things up to date and make sure everything is patched across the dist even when it’s even when it’s difficult. I think that really is one of the best ways to keep your system secure and because the sandboxing isn’t very strong that can really be a problem, you know like the XZ thing that happened before. If XZ is just one place it’s not that hard of an update but if you’ve got a 100 Flatpaks from different places […] and no consistency to it it’s pretty hard to manage thatI am not going to get in depth about this problem domain and the arguments over it. In fact I have been writing another blog post for a while. I hope to publish shortly. Till then I can not recommend high enough Emmanuele’s and Lennart’s blog posts, as well as one of the very early posts from Alex when Flatpak was in early design phase on the shortcomings of the current distribution model.Now about bundled dependencies. The concept of Runtimes has served us well so far, and we have been doing a pretty decent job providing most of the things applications need but would not want to bundle themselves. This makes the Runtimes a single place for most of the high profile dependencies (curl, openssl, webkitgtk and so on) that you’d frequently update for security vulnerabilities and once it’s done they roll out to everyone without needing to do anything manual to update the applications or even rebuilt them.Applications only need to bundle their direct dependencies,and as mentioned above, the flatpak manifest includes the exact definition of all of them. They are available to anyone to inspect and there’s tooling that can scan them and hopefully in the future alert us.If the Docker/OCI model where you end bundling the entire toolchain, runtime, and now you have to maintain it and keep up with updates and rebuild your containers is good enough for all those enterprise distributions, then the Flatpak model which is much more efficient, streamlined and thought out and much much much less maintenance intensive, it is probably fine.Miller: part of the idea of having a distro was to keep all those things consistent so that it’s easier for everyone, including the developersAs mentioned above, nothing that fundamentally differs from the leverage that Flathub and the Platform Developers have.Brodie: took us 20 minutes to get to an explanation [..] but the tldr Fedora Flatpak is basically it is built off of the Fedora RPM build system and because that it is more well tested and sort of intended, even if not entirely for the Enterprise, designed in a way as if an Enterprise user was going to use it the idea is this is more well tested and more secure in a lot of cases not every case.
Miller: Yea that’s basically itThis is a question/conclusion that Brodie reaches with after the previous statements and by far the most enraging thing in this interview. This is also an excellent example of the damage Matthew Miller caused today and if I was a Flathub developer I would stop on nothing sort of a public apology from the Fedora project itself. Hell I want this just being an application developer that publishes on it. The interview has been basically shitting on both the Developers of Flathub  the people that choose to publish in it. And if that’s not enough there should be an apology just out of decency. Dear god..Brodie: how should Fedora handle upstreams that don’t want to be packaged  like the OBS case here where they did not want there to be a package in Fedora Flatpak or another example is obviously bottles which has made a lot of noise about the packagingLastly I want to touch on this closing question in light of recent events.Miller: I think we probably shouldn’t do it. We should respect people’s wishes there. At least when it is an open source project working in good faith there. There maybe some other cases where the software, say theoretically there’s somebody who has commercial interests in some thing and they only want to release it from their thing even though it’s open source. We might want to actually like, well it’s open source we can provide things, we in that case we might end up you having a different name or something but yeah I can imagine situations where it makes sense to have it packaged in Fedora still but in general especially and when it’s a you know friendly successful open source project we should be friendly yeah. The name thing is something people forget history like that’s happened before with Mozilla with Firefox and Debian.This is an excellent idea! But it gets better:Miller: so I understand why they strict about that but it was kind of frustrating um you know we in Fedora have basically the same rules if you want to take Fedora Linux and do something out of it, make your own thing out of it, put your own software on whatever, you can do that but we ask you not to call it Fedora if it’s a fedora remix brand you can use in some cases otherwise pick your own name it’s all open source but you know the name is ours. yeah and I the Upstream as well it make totally makes sense.Brodie: yeah no the name is completely understandable especially if you do have a trademark to already even if you don’t like it’s it’s common courtesy to not name the thing the exact same thingMiller: yeah I mean and depending on the legalities like you don’t necessarily have to register a trademark to have the trademark kind of protections under things so hopefully lawyers you can stay out of the whole thing because that always makes the situations a lot more complicated, and we can just get along talking like human beings who care about making good software and getting it to users.And I completely agree with all of these, all of it. But let’s break it down a bit because no matter how nice the words and intentions it hasn’t been working out this way with the Fedora community so far.First, Miller agrees the Fedora project should be respecting of application developer’s wishes to not have their application distributed by fedora but rather it be a renamed version if Fedora wishes to keep distributing it.However, every single time a developer has asked for this, they have been ridiculed, laughed at and straight up bullied by Fedora packagers and the rest of the Fedora community. It has been a similar response from other distribution projects and companies as well, it’s not just Fedora. You can look at Bottle’s story for the most recent example. It is very nice to hear Miller’s intentions but means nothing in practice.Then Miller proceeds to assure us why he understand that naming and branding is such a big deal to those projects (unlike the rest of the Fedora community again). He further informs us how Fedora has the exact same policies and asks from people that want to fork Fedora. Which makes the treatment that every single application developer has received when asking about the same exact thing ever more outrageous.What I didn’t know is that in certain cases you don’t even need to have a trademark yet to be covered by some of the protections, depending on jurisdiction and all.And last we come into lawyers. Neither Fedora nor application developers would want it to ever come to this, and it was stated multiple times by Bottles developers that they don’t want to have to file for a trademark so they can be taken seriously. Similarly, OBS developers said how resorting to legal action would be the last thing they would want to do and would rather have the issue resolved before that. But it took until OBS, a project of a high enough profile, with the resources required to acquire a trademark and to threaten legal action before the Fedora Leadership cared to treat application developers like human beings and get the Fedora packagers and community members to comply. (Something which they had stated multiple times they simply couldn’t do).I hate all of this. Fedora and all the other distributions need to do better. They all claim to care about their users but happily keep shipping broken and miss configured software to them over the upstream version, just cause it’s what aligns with their current interests. In this case is the promotion of Fedora tooling and Fedora Flatpaks over the application in Flathub they have no control over. In previous incidents it was about branding applications like the rest of the system even though it was making them unusable. And I can find you and list you with a bunch of examples from other distributions just as easily.They don’t care about their users, they care about their bottom line first and foremost. Any civil attempts at fixing issues get ignored and laughed at, up until there is a threat of a legal action or a big enough PR damage, drama and shitshow that they can’t ignore it anymore and have to backtrack on them.This is my two angry cents. Overall I am not exactly sure how Matthew Miller managed in a rushed and desperate attempt at damage control for the OBS drama, to not only to make it worse, but to piss off the entire Flathub community at the same time. But what’s done is done, let’s see what we can do to address the issues that have festered and persisted for years now.]]></content:encoded></item></channel></rss>