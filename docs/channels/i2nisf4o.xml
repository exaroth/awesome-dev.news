<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>Help with k3s setup on wsl</title><link>https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/</link><author>/u/watterbottle800</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 11:49:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with "failed to connect to <svc cluster ip> port 15672 : couldn't connect to server" after sometime. This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks]]></content:encoded></item><item><title>Is Nvidia on Linux still bad?</title><link>https://www.reddit.com/r/linux/comments/1iqpsy0/is_nvidia_on_linux_still_bad/</link><author>/u/Szer1410</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 10:51:56 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I am planning to buy a laptop. I want to have a peak Linux experience, so I have been looking for laptops with dedicated AMD GPUs. While searching, I noticed a few things:There are not many laptops with dedicated AMD GPUs. Most available options come with integrated GPUs like the 780M.For the price of a laptop with a 780M, I can get a laptop with an RTX 3050 or better.System76 sells Linux laptops with Nvidia GPUs on their website.Additionally, I want to install Manjaro on my laptop. Are there any Linux distributions with better Nvidia support?]]></content:encoded></item><item><title>Kubernetes In-Place Pod Vertical Scaling</title><link>https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/</link><author>/u/Wownever</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sun, 16 Feb 2025 10:50:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are Resize CPU and Memory Resources assigned to Containers, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.What Is In-Place Pod Vertical Scaling?Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.What‚Äôs behind the feature gate?The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.The result of the change attempt is communicated as part of the pods‚Äô status in a field called¬†  (for more information on the new fields, check out the Kubernetes API documentation.)Additionally, this feature introduces the  in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.Dynamic Scaling: Modify CPU and memory allocations while pods run.No Restarts: Avoid downtime caused by pod restarts.Granular Control: Enable precise resource tuning for better efficiency.The InPlacePodVerticalScaling feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Here‚Äôs a detailed breakdown of how it operates: Activating the InPlacePodVerticalScaling feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.Dynamic Resource Updates via Kube API: With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., containerd v1.6.9 or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled. The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.Limitations and ConsiderationsWhile In-Place Pod Vertical Scaling offers significant benefits, it has limitations:1. Cloud Provider SupportAWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the GKE alpha clusters documentation.Several Kubernetes policies and mechanisms govern resource scaling. These include:Resource quotas limit the total CPU and memory usage for a namespace. If an InPlacePodVerticalScaling operation exceeds these limits, the scaling request will fail. For example:apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: example-namespace
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "32Gi"
Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: example-namespace
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
Admission controllers, such as Pod Security Admission or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.Memory-Bound Applications: Applications like Java with fixed Xmx parameters.In cases where the HPA is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:HPA scaling behavior is based on CPU average utilizationA pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created. Dynamically allocate resources during training and inference phases. Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling. Reduce waste by allocating the right amount of resources to each pod in real-time. Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Google‚Äôs example, Startup CPU Boost, demonstrates how dynamic resource scaling can address such scenarios effectively.1. Enable the Feature GateAdd the following configuration to enable the InPlacePodVerticalScaling feature:apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
controllerManager:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
scheduler:
  extraArgs:
    feature-gates: InPlacePodVerticalScaling=true
For GKE, create a cluster with alpha features enabled:gcloud container clusters create poc \
    --enable-kubernetes-alpha \
    --no-enable-autorepair \
    --no-enable-autoupgrade
Define a deployment with initial CPU and memory requests and limits:apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
          requests:
            memory: "64Mi"
            cpu: "250m"
        resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: NotRequired
Once deployed, you can check the cpu.weight, cpu.max, memory.max, memory.min from within the container to see the initial values that the container starts with.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max 
Adjust resource allocations for a running pod dynamically:kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"750m"}}}]}}'
Confirm updated resource settings:kubectl describe pod -l app=app
Additionally, you can connect to the container and see the change in cpu.weight, cpu.max, memory.max, memory.min from within the container.kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min

kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max
In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and optimizing resource usage. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.While Google‚Äôs Kube Startup CPU Boost example is just a specific use case scenario, ScaleOps provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.]]></content:encoded></item><item><title>I created a CLI trash command</title><link>https://github.com/Maxsafer/trash-tool</link><author>/u/lavishclassman</author><category>linux</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 10:23:55 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Its a less than 400 lines CLI trash manager :) made it for personal use and for fun.   submitted by    /u/lavishclassman ]]></content:encoded></item><item><title>Proj Ideas üí° - Willing to lock in for Go (2025)</title><link>https://www.reddit.com/r/golang/comments/1iqp4re/proj_ideas_willing_to_lock_in_for_go_2025/</link><author>/u/ComfortableAcadia839</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 10:04:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm a full stack JS/TS developer but just recently tried Go, built an in memory key-value Redis clone.. I've realised the language makes me enjoy coding ---> Can y'all recommend some project ideas (intermediate to advanced difficulty)I want to build some solid projects ;)]]></content:encoded></item><item><title>NASA has a list of 10 rules for software development</title><link>https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:07:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[NASA has a list of 10 rules for software developmentThose rules were written from the point of view of people writing
embedded software for extremely expensive spacecraft, where tolerating
a lot of programming pain is a good tradeoff for not losing a mission.
I do not know why someone in that situation does not use the SPARK
subset of Ada, which subset was explicitly designed for verification,
and is simply a better starting point for embedded programming than C.
I am criticising them from the point of view of people writing
programming language processors (compilers, interpreters, editors)
and application software.
We are supposed to teach critical thinking.  This is an example.
How have Gerard J. Holzmann's and my different contexts affected
our judgement?
Can you blindly follow his advice without considering 
context?
Can you blindly follow  advice without considering
your context?
Would these rules necessarily apply to a different/better
programming language?  What if function pointers
were tamed?  What if the language provided opaque abstract
data types as Ada does?
1. Restrict all code to very simple control flow constructs ‚Äî
do not use  statements,
 or  constructs,
and direct or indirect .Note that  and 
are how C does exception handling, so this rule bans any use
of exception handling.

It is true that banning recursion and jumps and loops without
explicit bounds means that you  your program is
going to terminate.  It is also true that recursive functions
can be proven to terminate about as often as loops can, with
reasonably well-understood methods.  What's more important here is
that ‚Äúsure to terminate‚Äù does not imply
‚Äúsure to terminate in my lifetime‚Äù:
    int const N = 1000000000;
    for (x0 = 0; x0 != N; x0++)
    for (x1 = 0; x1 != N; x1++)
    for (x2 = 0; x2 != N; x2++)
    for (x3 = 0; x3 != N; x3++)
    for (x4 = 0; x4 != N; x4++)
    for (x5 = 0; x5 != N; x5++)
    for (x6 = 0; x6 != N; x6++)
    for (x7 = 0; x7 != N; x7++)
    for (x8 = 0; x8 != N; x8++)
    for (x9 = 0; x9 != N; x9++)
        -- do something --;
This does a bounded number of iterations.  The bound is N.
In this case, that's 10.  If each iteration of the loop body
takes 1 nsec, that's 10 seconds, or about 7.9√ó10
years.  What is the  difference between ‚Äúwill stop
in 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
years‚Äù and ‚Äúwill never stop‚Äù?

Worse still, taking a problem that is  expressed
using recursion and contorting it into something that manipulates an
explicit stack, while possible, turns clear maintainable code into
buggy spaghetti.  (I've done it, several times.  There's an example
on this web site.  It is  a good idea.)

2. All loops must have a fixed upper-bound.  It must be trivially
possible for a checking tool to prove statically that a preset
upper-bound on the number of iterations of a loop cannot be exceeded.
If the loop-bound cannot be proven statically, the rule is considered
violated.This is an old idea.  As the example above shows, it is not enough
by itself to be of any practical use.  You have to try to make the
bounds reasonably , and you have to regard hitting an
artificial bound as a run-time error.

By the way, note that putting depth bounds on recursive procedures
makes them every bit as safe as loops with fixed bounds.

3. Do not use dynamic memory allocation after initialization.This is also a very old idea.  Some languages designed for embedded
work don't even  dynamic memory allocation.  The big
thing, of course, is that embedded applications have a fixed amount of
memory to work with, are never going to get any more, and should not
crash because they couldn't handle another record.

Note that the rationale actually supports a much stronger rule:
don't even  dynamic memory allocation.  You can of
course manage your own storage pool:
    typedef struct Foo_Record *foo;
    struct Foo_Record {
	foo next;
	...
    };
    #define MAX_FOOS ...
    static struct Foo_Record foo_zone[MAX_FOOS];
    foo foo_free_list = 0;

    void init_foo_free_list() {
	for (int i = MAX_FOOS - 1; i >= 0; i--) {
	    foo_zone[i].next = foo_free_list;
	    foo_free_list = &foo_zone[i];
	}
    }

    foo malloc_foo() {
	foo r = foo_free_list;
	if (r == 0) report_error();
	foo_free_list = r->next;
	return r;
    }

    void free_foo(foo x) {
	x->next = foo_free_list;
	foo_free_list = x;
    }
This  satisfies the rule, but it
violates the  of the rule.  Simulating malloc()
and free() this way is  than using the real
thing, because the memory in foo_zone is permanently tied up
for Foo_Records, even if we don't need any of those at the
moment but do desperately need the memory for something else.

What you really need to do is to use a memory allocator
with known behaviour, and to prove that the amount of memory
in use at any given time (data bytes + headers) is bounded
by a known value.

Note also that SPlint can verify at compile time that
the errors NASA speak of do not occur.

One of the reasons given for the ban is that the performance
of malloc() and free() is unpredictable.  Are these the only
functions we use with unpredictable performance?  Is there
anything about malloc() and free() which makes them
 unpredictable?  The existence of
hard-real-time garbage collectors suggests not.

The rationale for this rule says that

Note that the only way
to dynamically claim memory in the absence of memory allocation from the
heap is to use stack memory.  In the absence of recursion (Rule 1), an
upper bound on the use of stack memory can derived statically, thus
making it possible to prove that an application will always live within
its pre-allocated memory means.
Unfortunately, the sunny optimism shown here is unjustified.  Given
the ISO C standard (any version, C89, C99, or C11) it is 
to determine an upper bound on the use of stack memory.  There is not even
any standard way to determine how much memory a compiler will use for the
stack frame of a given function.  (There could have been.  There just isn't.)
There isn't even any requirement that two invocations of the same function
with the same arguments will use the same amount of memory.
Such a bound can only be calculated for a  version of a
specific compiler with specific options.  Here's a trivial example:
void f() {
    char a[100000];
}
How much memory will that take on the stack?  Compiled for debugging,
it might take a full stack frame (however big that is) plus traceback
information plus a million bytes for a[].  Compiled with optimisation,
the compiler might notice that a[] isn't used, and might even compile
calls to f() inline so that they generate no code and take no space.
That's an extreme example, but not really unfair.  If you want bounds
you can rely on, you had better  what your compiler does,
and recheck every time anything about the compiler changes.

4.  No function should be longer than what can be printed on
a single sheet of paper in a standard reference format with one line per
statement and one line per declaration.  Typically, this means no more
than about 60 lines of code per function.Since programmers these days typically read their code on-screen,
not on paper, it's not clear why the size of a sheet of paper is
relevant any longer.

The rule is arguably stated about the wrong thing.  The thing that
needs to be bounded is not the size of a function, but the size of a
chunk that a programmer needs to read and comprehend.

There are also question marks about how to interpret this if you
are using a sensible language (like Algol 60, Simula 67, Algol 68,
Pascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,
F#, Clean, Haskell, or Fortran) that allows nested procedures.
Suppose you have a folding editor that presents a procedure to
you like this:
function Text_To_Floating(S: string, E: integer): Double;
   ÔøΩ variables ÔøΩ
   ÔøΩ procedure Mul(Carry: integer) ÔøΩ
   ÔøΩ function Evaluate: Double ÔøΩ

   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;
   for N := 1 to S.length do begin
       C := S[N];
       if C = '.' then begin
          Point := -1
       end else
       if C = '_' then begin
          Base := Round(Evaluate);
          Max, Min, Power := 0, 1, 0
       end else
       if Char ‚â† ' ' then begin
          Q := ord(C) - ord('0');
          if Q > 9 then Q := ord(C) - ord('A') + 10
          Power := Point + Point
          Mul(Q)
       end
    end;
    Power := Power + Exp;
    Value := Evaluate;
    if Sign < 0 then Value := -Value;
end;
which would be much bigger if the declarations
were expanded out instead of being hidden behind ÔøΩfoldsÔøΩ.
Which size do we count?  The folded size or the unfolded size?
I was using a folding editor called Apprentice on the Classic Mac
back in the 1980s.  It was written by Peter McInerny and was lightning
fast.

5.  The  of the code should average to a minimum of
two assertions per function.Assertions are wonderful documentation and the very best debugging tool
I know of.  I have never seen any real code that had too many assertions.

The example here is one of the ugliest pieces of code I've seen in a while.
if (!c_assert(p >= 0) == true) {
    return ERROR;
}
It should, of course, just be
if (!c_assert(p >= 0)) {
    return ERROR;
}
Better still, it should be something like
#ifdef NDEBUG
#define check(e, c) (void)0
#else
#define check(e, c) if (!(c)) return bugout(c), (e)
#ifdef NDEBUG_LOG
#define bugout(c) (void)0
#else
#define bugout(c) \
    fprintf(stderr, "%s:%d: assertion '%s' failed.\n", \
    __FILE__, __LINE__, #s)
#endif
#endif
Ahem.  The more interesting part is the required density.
I just checked an open source project from a large telecoms
company, and 23 out of 704 files (not functions) contained
at least one assertion.  I just checked my own Smalltalk
system and one SLOC out of every 43 was an assertion, but
the average Smalltalk ‚Äúfunction‚Äù is only a few
lines.  If the biggest function allowed is 60 lines, then
let's suppose the average function is about 36 lines, so
this rule requires 1 assertion per 18 lines.
Assertions are good, but what they are especially good
for is expressing the requirements on data that come
from outside the function.  I suggest then that
Every argument whose validity is not guaranteed by
its typed should have an assertion to check it.
Every datum that is obtained from an external
source (file, data base, message) whose validity is
not guaranteed by its type should have an assertion
to check it.
The NASA 10 rules are written for embedded systems, where
reading stuff from sensors is fairly common.

6.  Data objects must be declared at the smallest possible level of
scope.This is excellent advice, but why limit it to data objects?
Oh yeah, the rules were written for crippled languages where you
 declare functions in the right place.

People using Ada, Pascal (Delphi), JavaScript, or functional
languages should also declare types and functions as locally as
possible.

7.  The return value of non-void functions must be checked by each
calling function, and the validity of parameters must be checked inside
each function.This again is mainly about C, or any other language that indicates
failure by returning special values.  ‚ÄúStandard libraries
famously violate this rule‚Äù?  No, the  library does.

You have to be reasonable about this: it simply isn't practical
to check  aspect of validity for 
argument.  Take the C function
void *bsearch(
    void const *key  /* what we are looking for */,
    void const *base /* points to an array of things like that */,
    size_t      n    /* how many elements base has */,
    size_t      size /* the common size of key and base's elements */
    int (*      cmp)(void const *, void const *)
);
This does a binary search in an array.  We must have key‚â†0,
base‚â†0, size‚â†0, cmp‚â†0, cmp(key,key)=0, and for all
1<i<n,
cmp((char*)base+size*(i-1), (char*)base+size*i) <= 0
Checking the validity in full would mean checking
that [key..key+size) is a range of readable addresses,
[base..base+size*n) is a range of readable addresses,
and doing n calls to cmp.  But the whole point of binary
search is to do O(log(n)) calls to cmp.

The fundamental rules here are
Don't let run-time errors go un-noticed, and
any check is safer than no check.
8. The use of the preprocessor must be limited to the inclusion of
header files and simple macro definitions.  Token pasting, variable
argument lists (ellipses), and recursive macro calls are not allowed.Recursive macro calls don't really work in C, so no quarrel there.
Variable argument lists were introduced into macros in
C99 so that you could write code like
#define err_printf(level, ...) \
    if (debug_level >= level) fprintf(stderr, __VA_ARGS__)
...
    err_printf(HIGH, "About to frob %d\n", control_index);
This is a  thing; conditional tracing like this is a
powerful debugging aid.  It should be , not banned.

The rule goes on to ban macros that expand into things that are
not complete syntactic units.  This would, for example, prohibit
simulating try-catch blocks with macros.  (Fair enough, an earlier rule
banned exception handling anyway.)  Consider this code fragment, from
an actual program.
    row_flag = border;     
    if (row_flag) printf("\\hline");
    for_each_element_child(e0, i, j, e1)
        printf(row_flag ? "\\\\\n" : "\n");
        row_flag = true;  
        col_flag = false;
        for_each_element_child(e1, k, l, e2)
            if (col_flag) printf(" & ");
            col_flag = true;
            walk_paragraph("", e2, "");
        end_each_element_child
    end_each_element_child
    if (border) printf("\\\\\\hline");
    printf("\n\\end{tabular}\n");
It's part of a program converting slides written in something like HTML
into another notation for formatting.  The 
‚Ä¶  loops walk over a tree.  Using
these macros means that the programmer has no need to know and no reason to
care how the tree is represented and how the loop actually works.
You can easily see that  must have at
least one unmatched { and  must have at least one
unmatched }.  That's the kind of macro that's banned by requiring
complete syntactic units.  Yet the readability and maintainability of
the code is  improved by these macros.

One thing the rule covers, but does not at the beginning stress, is
‚Äúno  macro processing‚Äù.  That is,
no #if.  The argument against it is, I'm afraid, questionable.  If there
are 10 conditions, there are 2 combinations to test,
whether they are expressed as compile-time conditionals or run-time
conditionals.

In particular, the rule against conditional macro processing
would prevent you defining your own assertion macros.
It is not obvious that that's a good idea.

9.  The use of pointers should be restricted.  Specifically, no more
than one level of dereferencing is allowed.  Pointer dereference
operations may not be hidden in macro definitions or inside typedef
declarations.  Function pointers are not permitted.Let's look at the last point first.

double integral(double (*f)(double), double lower, double upper, int n) {
    // Compute the integral of f from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += f((lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += f(lower + h*i);
    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);
}
This kind of code has been important in numerical calculations since
the very earliest days.  Pascal could do it.  Algol 60 could do it.
In the 1950s, Fortran could do it.  And NASA would ban it, because in
C,  is a function pointer.

Now it's important to write functions like this once and only once.
For example, the code has at least one error.  The comment says n+1
points, but the function is actually evaluated at 2n+1 points.  If we
need to bound the number of calls to f in order to meet a deadline,
having that number off by a factor of two will not help.
It's nice to have just one place to fix.
Perhaps I should not have copied that code from a well-known source (:-).
Certainly I should not have more than one copy!

What can we do if we're not allowed to use function pointers?
Suppose there are four functions foo, bar, ugh, and zoo that we need
to integrate.  Now we can write
enum Fun {FOO, BAR, UGH, ZOO};

double call(enum Fun which, double what) {
    switch (which) {
        case FOO: return foo(what);
        case BAR: return bar(what);
        case UGH: return ugh(what);
        case ZOO: return zoo(what);
    }
}

double integral(enum Fun which, double lower, double upper, int n) {
    // Compute the integral of a function from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += call(which, (lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += call(which, lower + h*i);
    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);
}
Has obeying NASA's rule made the code more reliable?  No, it has made
the code  to understand,  maintainable, and
 that it wasn't before.  Here's a call
illustrating the mistake:
x = integral(4, 0.0, 1.0, 10);I have checked this with two C compilers and a static checker at their
highest settings, and they are completely silent about this.

So there are legitimate uses for function pointers, and simulating
them makes programs , not better.

Now  in Fortran,
Algol 60, or Pascal.  Those languages had procedure 
but not procedure . You could pass a subprogram name as
a parameter, and such a parameter could be passed on, but you could not
store them in variables.  You could have a  of C which
allowed function pointer parameters, but made all function pointer
variables read-only.  That would give you a statically checkable subset
of C that allowed integral().

The other use of function pointers is simulating object-orientation.
Imagine for example
struct Channel {
    void (*send)(struct Channel *, Message const *);
    bool (*recv)(struct Channel *, Message *);
    ...
};
inline void send(struct Channel *c, Message const *m) {
    c->send(c, m);
}
inline bool recv(struct Channel *c, Message *m) {
    return c->recv(c, m);
}
This lets us use a common interface for sending and receiving
messages on different kinds of channels.  This approach has been
used extensively in operating systems (at least as far back as
the Burroughs MCP in the 1960s) to decouple the code that uses
a device from the actual device driver.     I would expect any
program that controls more than one hardware device to do something
like this.  It's one of our key tools for controlling complexity.
Again, we can simulate this, but it makes adding a new kind of
channel harder than it should be, and the code is 
when we do it, not better.

The rule against more than one level of dereferencing is also
an assault on good programming.  One of the key ideas that was
developed in the 1960s is the idea of ;
the idea that it should be possible for one module to define a
data type and operations on it and another module to use instances
of that data type and its operations without having to know
anything about what the data type is.
One of the things I detest about Java is that it spits in the
face of the people who worked out that idea.  Yes, Java (now) has
generic type parameters, and that's good, but you cannot use a
 type without knowing what that type is.

Suppose I have a module that offers operations
And suppose that I have two interfaces in mind.  One of them
uses integers as tokens.
// stasher.h, version 1.
typedef int token;
extern token stash(item);
extern item  recall(token);
extern void  delete(token);
Another uses pointers as tokens.
// stasher.h, version 2.
typedef struct Hidden *token;
extern  token stash(item);
extern  item  recall(token);
extern  void  delete(token);
void snoo(token *ans, item x, item y) {
    if (better(x, y)) {
	*ans = stash(x);
    } else {
	*ans = stash(y);
    }
}
By the NASA rule, the function snoo() would not be accepted or rejected on
its own merits.  With stasher.h, version 1, it would be accepted.
With stasher.h, version 2, it would be rejected.

One reason to prefer version 2 to version 1 is that version 2 gets
more use out of type checking.  There are ever so many ways to get an
int in C.  Ask yourself if it ever makes sense to do
token t1 = stash(x);
token t2 = stash(y);
delete(t1*t2);
I really do not like the idea of banning abstract data types.

10.  All code must be compiled, from the first day of development,
with all compiler warnings enabled at the compiler‚Äôs
most pedantic setting.  All code must compile with these setting without
any warnings.  All code must be checked daily with at least one, but
preferably more than one, state-of-the-art static source code analyzer
and should pass the analyses with zero warnings.This one is good advice.  Rule 9 is really about making your code
worse in order to get more benefit from limited static checkers.  (Since
C has no standard way to construct new functions at run time, the set of
functions that a particular function pointer  point to can
be determined by a fixed-point data flow analysis, at least for most
programs.)  So is rule 1.  



]]></content:encoded></item><item><title>Resigning as Asahi Linux project lead</title><link>https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:01:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them ‚Äújailbreaks‚Äù these days) to allow users to run their own unofficial apps on the Nintendo Wii.I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people‚Äôs work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn‚Äôt attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I‚Äôve already spoken at length about and I won‚Äôt repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience. (Clarification: This has nothing to do with Rust at this point, it‚Äôs well before R4L was even merged. Upstreaming to Linux is a terrible experience in C too.)But then also came the entitled users. This time, it wasn‚Äôt about stealing games, it was about features. ‚ÄúWhen is Thunderbolt coming?‚Äù ‚ÄúAsahi is useless to me until I can use monitors over USB-C‚Äù ‚ÄúThe battery life sucks compared to macOS‚Äù (nobody ever complained when compared to x86 laptops‚Ä¶) ‚ÄúI can‚Äôt even check my CPU temperature‚Äù (yes, I seriously got that one). (Edit: This wasn‚Äôt just a few instances; I‚Äôve seen variations on the first three posted hundreds of times by now, including takes like ‚ÄúThunderbolt/DP Alt are never going to happen‚Äù. A few times is fine, but the same thing repeated over and over again every day while we‚Äôre trying to make these things happen will get to anyone.)And, of course, ‚ÄúWhen is M3/M4 support coming?‚ÄùFor a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were ‚Äúalpha‚Äù and ‚Äúunstable‚Äù and ‚Äúnot suitable for a daily driver‚Äù (despite thousands of users, myself included, daily driving it and even using it for servers).No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won‚Äôt go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).I did make some progress in 2024, but this left me in a very vulnerable position. I hadn‚Äôt gotten nearly as much Asahi work done as I‚Äôd liked, and the users weren‚Äôt getting any quieter about demanding more features and machine support.We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. ‚ÄúFull‚Äù DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn‚Äôt the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did try to bring the subject up on the mailing lists, but it didn‚Äôt get much response).The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won‚Äôt repeat them in detail here. Suffice it to say, I consider Linus‚Äô handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.As you know, this is deeply personal to me, as we‚Äôve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.Some might be wondering why we can‚Äôt just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there‚Äôs more.In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that‚Äôs how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a partial workaround for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.I‚Äôm not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated ‚ÄúWe
are the ‚Äòthin blue line‚Äô‚Äù, and nobody cared, which just further confirmed to me that I don‚Äôt want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to quit.But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than bunny.net giving us free CDN credits for the hosting).I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I‚Äôm the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won‚Äôt touch the Linux kernel with a 10-foot pole.When Apple released the M1, Linus Torvalds wished it could run Linux, but didn‚Äôt have much hope it would ever happen. We made it happen, and Linux 5.19 was released from an M2 MacBook Air running Asahi Linux. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him an invitation to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.Back in 2011, Con Kolivas left the Linux kernel community. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you‚Äôre not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it‚Äôs not fun any more. I have an M3 Pro in a box and I haven‚Äôt even turned it on yet. I dread doing the bring-up work. It doesn‚Äôt feel worth the trouble.I miss having free time where I can relax and not worry about the features we haven‚Äôt shipped yet. I miss making music. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven‚Äôt upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.I‚Äôm resigning as lead of the Asahi Linux project, effective immediately. The project will continue on without me, and I‚Äôm working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the Asahi Linux OpenCollective (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).I want to thank the entire Asahi Linux team, without whom I would‚Äôve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: marcan@marcan.st.: A lot of the discussion around this post and the interactions that led to it brings up the term ‚Äúbrigading‚Äù. Please read this excellent Fedi post for a discussion of what is and isn‚Äôt brigading.]]></content:encoded></item><item><title>Which approach to rust is more idiomatic (Helix vs Zed)?</title><link>https://www.reddit.com/r/rust/comments/1iqnats/which_approach_to_rust_is_more_idiomatic_helix_vs/</link><author>/u/No_Penalty2781</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 07:50:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi! I am curious what is the current "meta" (by "meta" I mean the current rust's community  and  way of doing things) of rust programming. I am studying source code of 2 editors I am using: Helix and Zed. And I can see that while they are doing a lot of similar things (like using LSP and parsing it outputs for example) the code is kinda different.It starts from the file structure: in Helix there are not that many folders to look at (like you have helix-core which contains features like "diagnostic", "diff", "history", etc but in Zed every single one of them is a different crate , which approach is more "idiomatic"? To divide every feature as a separate crate or to use more "packed" crates like "core".Then the code itself is kinda different, for example I am currently looking at LSP implementation in both of them and in Helix's case I can follow along and understand the code much more easily (here is the file I am referring to. But in Zed's case it is kinda hard to understand the code because of "type level programming" stuff like this one for example. It also doesn't help that files have a lot of SLOC in them (over 1500 in normal in Zed's repository, is it also how you do rust?) Maybe I am just used to lean functions from other languages (I mainly did TypeScript and Elixir in my career).Other thing I see is that Helix has more comments about "why the thing is doing that in the first place" which I find very helpful (on the other hand in seems that Zed's is abusing a lot of "type level" programming to have a self-documented code but it is harder to reason about at least for me) which approach here you prefer?]]></content:encoded></item><item><title>[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation</title><link>https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:55:36 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methodsMain findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning typesI think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.]]></content:encoded></item><item><title>Fluvio: A Rust-powered streaming platform using WebAssembly for programmable data processing</title><link>https://www.reddit.com/r/rust/comments/1iqgg02/fluvio_a_rustpowered_streaming_platform_using/</link><author>/u/drc1728</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 01:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am in the process of writing an essay on composable streaming first architecture for data intensive applications. I am thinking of it as a follow up on this article.Quick question for the Rust community:What information would help the Rust community know and experience Fluvio?What would you like to see covered in the essay?   submitted by    /u/drc1728 ]]></content:encoded></item><item><title>Safe elimination of unnecessary bound checks.</title><link>https://www.reddit.com/r/rust/comments/1iqev5s/safe_elimination_of_unnecessary_bound_checks/</link><author>/u/tjientavara</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 23:43:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi, I am working on a Unicode database that is pretty fast, it is a 2 step associated lookup.Here is the code for getting the east-asian-width value of a Unicode code-point. Pay specific attention to the function. This function is a  function and the byte tables that it references are  as well. This will allow you to eventually run the unicode algorithms at both compile and run-time.Since the tables are fixed at compile time, I can proof that all values from the table will result in values that will never break any bounds, so technically the bound checks are unnecessary.There are two bound checks in the assembly output for this function.The check before accessing the EAST_ASIAN_WIDTH_COLUMN table (I use an assert! to do this, otherwise there will be double bound check).And the check on the conversion to the enum.The two bound checks are the two compare + conditional-jump instructions in this code.I could increase the size of the column table to remove one of the bound checks, but I want to keep the table small if possible.Is there a way to safely (I don't want to use the unsafe code) proof to the compiler that those two checks are unnecessary?P.S. technically there is a bound check before the index table a CMOV instruction, but it doubles as a way to also decompress the index table (last entry is repeated), so I feel this is not really a bound check.I was able to concat the two tables, and use a byte offset. So now there is no way to get an out of bound access, and the bound checks are no longer emitted by the compiler.I also added a manual check for out of bound on the enum and return zero instead, this becomes a CMOV and it eliminated all the panic code from the function.]]></content:encoded></item><item><title>rke2 and DNS</title><link>https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/</link><author>/u/Affectionate_Horse86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 22:35:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == "server" and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.]]></content:encoded></item><item><title>Creating my OS</title><link>https://www.reddit.com/r/linux/comments/1iqaxku/creating_my_os/</link><author>/u/zainali28</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 20:46:12 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Now, I know it sounds absurd, but I just want to understand the general workflow of how do you design a linux, or a unix-based OS.I have a fair knowledge of computer architecture and can understand low level language of the computer.I am just an enthusiast who wants to just make a functional os, with just a terminal that is able to execute things.Any advice is greatly appreciated!]]></content:encoded></item><item><title>Amazon AWS &quot;whoAMI&quot; Attack Exploits AMI Name Confusion to Take Over Cloud Instances</title><link>https://www.reddit.com/r/programming/comments/1iqav3c/amazon_aws_whoami_attack_exploits_ami_name/</link><author>/u/Dark-Marc</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 20:43:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Incoming Rust intern need advice?</title><link>https://www.reddit.com/r/rust/comments/1iq9oph/incoming_rust_intern_need_advice/</link><author>/u/Helpful_Ad_9930</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:52:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hey everyone, I'm a 19-year-old college student who just landed a SWE internship at NVIDIA! My manager has me learning Rust and exploring one of its libraries, and I‚Äôm also reading up on operating systems and computer networking. I'm almost done with the OS book and plan to start the networking one next week.I do have a bit of experience with embedded systems I completed two internships during my freshman year. However, so far I‚Äôm really enjoying Rust. I am quite a rookie compared to you experienced folks haha! But so far I love how Rust's compiler enforces safety, how Cargo makes dependency management a breeze compared to CMake, and the whole concept of ownership and borrowing is just super cool.At the moment, I‚Äôm nearly finished with the Rust book. I am on the concurrency chapter. Guess I am just wondering what next? I really want this return offer and I just want to blow this opportunity out the park. I go too a state school and my manager told me he has high expectations for me after my interviews. I just do not want to let him down you know also plus kind of getting impostor syndrome a bit seeing all the other interns coming from schools such as MIT, Harvard, Standford, etc. Sorry for the vent I guess I just want to prove my worth? and show my manager they made the right choice?What fun, Rust projects have helped you learn a lot?Are there any books you‚Äôd recommend that could help me out for the summer?Books I want to read before I start summer:Operating Systems (Three easy pieces)Beej's Guide to Network ProgrammingC++ Concurrency in Action]]></content:encoded></item><item><title>Networking in K8s</title><link>https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/</link><author>/u/I-Ad-7</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:49:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Background: Never used k8s before 4 months ago. I would say I‚Äôm pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. I was put into a position where I had to use K8s for everyday work and don‚Äôt get me wrong I‚Äôm ecstatic about being an intern but already having the opportunity to work with deployments etc. What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.Right now I can pretty much do most stuff and have some cool projects running on my cluster.I‚Äôm also learning new stuff every day. But where I find myself lacking is Networking. Not just in Kubernetes but also generally. There are two examples of me getting frustrated because of my lacking networking knowledge:I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failedWas wondering why I can‚Äôt see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?I don‚Äôt mind it but I‚Äôm struggling to learn Networking and it‚Äôs not that I don‚Äôt have the basics but I don‚Äôt have the advanced knowledge needed yet, so how do I actually get there?]]></content:encoded></item><item><title>[D] Is my company missing out by avoiding deep learning?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/</link><author>/u/DatAndre</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:42:42 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too "black-box" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. Am I also missing out on them, since I do not have work experience with such models?EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. ]]></content:encoded></item><item><title>How are you monitoring your cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/</link><author>/u/psavva</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 19:28:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.CPU and networking are always displaying 'No Data'I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0That fixes the alerts on a fresh install of the stack. 1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.Any advice would be appreciated.If anyone has a fully working example of the helm chart values that fully work, that would be awesome.]]></content:encoded></item><item><title>Zed for golang</title><link>https://www.reddit.com/r/golang/comments/1iq8jsm/zed_for_golang/</link><author>/u/MrBricole</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 19:02:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am considering using zed for writting go. Is it working out of the box with full syntax high light for noob like me such fmt.Println() ? I mean, I need to have it displaying functions under an import library.Should I give it a try or is it only for advanced users ? ]]></content:encoded></item><item><title>Pushing autovectorization to the limit: utf-8 validator</title><link>https://www.reddit.com/r/rust/comments/1iq7yn2/pushing_autovectorization_to_the_limit_utf8/</link><author>/u/Laiho3</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 18:36:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/Laiho3 ]]></content:encoded></item><item><title>Questions around LoadBalancer</title><link>https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/</link><author>/u/HahaHarmonica</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 18:36:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[New to k8s. I‚Äôve deployed rke2 and i‚Äôve got several questions. Main Question) So i‚Äôm trying to install rancher UI on it. When you go to install with helm it asks for a ‚Äúhostname‚Äù and the hostname should be the name of your load balancer‚Ä¶i enabled the load balancer of rke2 but I have no clue how to operate with it‚Ä¶how do I change the configuration to point to rancher? The instructions aren‚Äôt very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. 2) During my debugging, i ran the command ‚Äúkubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldn‚Äôt have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent node‚Ä¶actually be running.3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasn‚Äôt the problem completely resetting the services by running the killall scripts on both of them before it works‚Ä¶so why in the world won‚Äôt k8s actually respect systemctl and restart properly without literally shutting everything down. ]]></content:encoded></item><item><title>Introducing encode: Encoders/serializers made easy.</title><link>https://www.reddit.com/r/rust/comments/1iq6pz7/introducing_encode_encodersserializers_made_easy/</link><author>/u/Compux72</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:42:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ is a toolbox for building encoders and serializers in Rust. It is heavily inspired by the  and  crates, which are used for building parsers. It is meant to be a companion to these crates, providing a similar level of flexibility and ease of use for reversing the parsing process.The main idea behind  is to provide a set of combinators for building serializers. These combinators can be used to build complex encoders from simple building blocks. This makes it easy to build encoders for different types of data, without having to write a lot of boilerplate code.Another key feature of  is its support for  environments. This makes it suitable for use in embedded systems, where the standard library (and particularly the [] module) is not available.See the  folder for some examples of how to use . Also, check the  module for a list of all the combinators provided by the crate.Ready to use combinators for minimizing boilerplate.: Enables the  feature.: Enables the use of the standard library.: Enables the use of the  crate.: Implements [] for [].Why the  trait instead of ?A buffer stores bytes in memory such that write operations are . The underlying storage may or may not be in contiguous memory. A BufMut value is a cursor into the buffer. Writing to BufMut advances the cursor position.The bytes crate was never designed with falible writes nor  targets in mind. This means that targets with little memory are forced to crash when memory is low, instead of gracefully handling errors.Why the  trait instead of ?Because there is no alternative, at least that i know of, that supports  properlyBecause it's easier to work with than  and Because using  with binary data often leads to a lot of boilerplate]]></content:encoded></item><item><title>Lil guy is trying his best</title><link>https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:27:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Transition from C++ to Rust</title><link>https://www.reddit.com/r/rust/comments/1iq67vq/transition_from_c_to_rust/</link><author>/u/Dvorakovsky</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:20:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Guys, are here any people who were learning/coding in C++ and switched to Rust. How do you feel? I mean I could easily implement linked lists: singly, doubly in c++, but when I saw how it is implemented in Rust I'd say I got lost completely. I'm only learning rust... So yeah, I really like ownership model even tho it puts some difficulties into learning, but I think it's a benefit rather than a downside. Even tho compared to C++ syntax is a bit messy for me]]></content:encoded></item><item><title>No, your GenAI model isn&apos;t going to replace me</title><link>https://marioarias.hashnode.dev/no-your-genai-model-isnt-going-to-replace-me</link><author>/u/dh44t</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:06:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Type safe Go money library beta2!</title><link>https://www.reddit.com/r/golang/comments/1iq5stk/type_safe_go_money_library_beta2/</link><author>/u/HawkSecure4957</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 17:02:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, after I released beta1, I received many constructive feedback! mainly lacking of locale support.This update brings locale formatting support and an improved interface for better usability. With Fulus, you can perform monetary operations safely and type-soundly. Plus, you can format money for any locale supported by CLDR. You can even define custom money types tailored specifically to your application's needs! I still need to battle test it against production projects, I have none at the moment. I am aiming next for performance benchmarking and more improvement, and parsing from string!I am open for more feedback. Thank you! ]]></content:encoded></item><item><title>TIL There is a minor-planet called Linux</title><link>https://www.reddit.com/r/linux/comments/1iq5p1p/til_there_is_a_minorplanet_called_linux/</link><author>/u/forvirringssirkel</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 16:57:50 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Golang Mastery Exercises</title><link>https://www.reddit.com/r/golang/comments/1iq5k7w/golang_mastery_exercises/</link><author>/u/Temporary-Buy-7562</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 16:51:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I made a repository which has a prompt for you to write many exercises, if you complete this, and then drill the exercises, I would be sure you would reach mastery with the core of the language.I initially wanted to make some exercises for drilling syntax since I use copilot and lsps a lot, but ended up with quite a damn comprehensive list of things you would want to do with the language, and I find this more useful than working on leetcode to really adopt the language.]]></content:encoded></item><item><title>[D] Have any LLM papers predicted a token in the middle rather than the next token?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/</link><author>/u/TheWittyScreenName</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 15:59:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I‚Äôm working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but we‚Äôre more interested in finding a series of tokens to connect a starting and ending ‚Äúword‚Äù than the next ‚Äúword‚Äù. Since we‚Äôre drawing a lot from LLMs in our setup, I‚Äôm wondering if there‚Äôs been any research into how models perform when the loss function isn‚Äôt based on the next token, but instead predicting a masked token somewhere in the input sequence. Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. I couldn‚Äôt find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?]]></content:encoded></item><item><title>Alexandre Mutel a.k.a. xoofx is leaving Unity</title><link>https://mastodon.social/@xoofx/113997304444307991</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:53:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t &quot;optimize&quot; conditional moves in shaders with mix()+step()</title><link>https://iquilezles.org/articles/gpuconditionals/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:52:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
In this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:
So, say I have this code, which I actually published the other day: snap45(  v )
{
     s = (v);
     x = (v.x);
     x>?(s.x,):
           x>?s*():
                      (,s.y);
}
The exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.
Now, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following "optimization", which replaces what they believe (erroneously) are "conditional branches" in the code, by arithmetic operations. They will suggest something like this: snap45(  v )
{
     s = (v);
     x = (v.x);

     w0 = (,x);
     w1 = (,x)*(-w0);
     w2 = -w0-w1;

     res0 = (s.x,);
     res1 = (s.x,s.y)*();
     res2 = (,s.y);

     w0*res0 + w1*res1 + w2*res2;
}
There are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. "select"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.
For the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.
The second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the  function is probably implemented with some sort of conditional move or subtract + bit propagation + AND. step(  x,  y )
{
     x < y ?  : ;
}
Either way, using the step() "optimization" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.
But don't take my word for it, let's look at the generated machine code for the original code I published:
GLSL x>?(s.x,):
       x>?s*():
                  (,s.y);
AMD Compiler     s0,      v3, , v1
     v4, , v0
     s1,   vcc, (v2), s0
 v3, 0, v3, vcc
 v0, v0, v4, vcc
 vcc, (v2), s1
 v1, v1, v3, vcc
 v0, 0, v0, vcc
Microsoft Compiler   r0.xy, l(, ), v0.xy
   r0.zw, v0.xy, l(, )
 r0.xy, -r0.xyxx, r0.zwzz
 r0.xy, r0.xyxx
  r1.xyzw, r0.xyxy, l4()
   r2.xy, l(,), v0.xx  r0.z, l()
 r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy
 o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw
Here we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).
The Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison ( - "lt"=less than) and the masking or conditional move ( - mov=move, c=conditionally).
There are no jump/branch instructions in these listings.
Something not related to the discussion but interesting, is that some of the  GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.
So, if you ever see somebody proposing this a = ( b, c, ( y, x ) );
as an optimization to
then please correct them for me.]]></content:encoded></item><item><title>Altman: OpenAI not for sale, especially to competitor who is not able to beat us</title><link>https://www.axios.com/2025/02/11/openai-altman-musk-offer</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:17:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - yaitoo/xun: Xun is an HTTP web framework built on Go&apos;s built-in html/template and net/http package‚Äôs router (1.22).</title><link>https://github.com/yaitoo/xun</link><author>/u/imlangzi</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 14:15:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What is Event Sourcing?</title><link>https://newsletter.scalablethread.com/p/what-is-event-sourcing</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:05:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like "Created," "Paid," and "Shipped." To determine the current state of an order, you would:Retrieve all events related to the order from the event store.Initialize an empty order object.Apply each event to the order object in the order in which they were stored.By the end of this process, the order object will reflect the current state of the order.As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. If you enjoyed this article, please hit the ‚ù§Ô∏è like button.If you think someone else will benefit from this, then please üîÅ share this post.]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>Built a cli tool for generating .gitignore files</title><link>https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/</link><author>/u/SoaringSignificant</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:38:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built this mostly as an excuse to play around with Charmbracelet‚Äôs libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It‚Äôs a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don‚Äôt have to redo them every time. If you‚Äôre always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out here and give it a try. And if you‚Äôve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!]]></content:encoded></item><item><title>ED25519 Digital Signatures In Go</title><link>https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/</link><author>/u/mejaz-01</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:37:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/mejaz-01 ]]></content:encoded></item><item><title>Richard Stallman on RISC-V and Free Hardware</title><link>https://odysee.com/@SemiTO-V:2/richardstallmanriscv:7?r=BYVDNyJt5757WttAfFdvNmR9TvBSJHCv</link><author>/u/ShockleyTransistor</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 13:20:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.‚ÄúRealistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,‚Äù Fu said.‚ÄúAs long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.‚ÄùThe panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden‚Äôs administration and the United Nations.]]></content:encoded></item><item><title>Karol Herbst steps down as Nouveau maintainer due to ‚Äúthin blue line comment‚Äù</title><link>https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/</link><author>/u/mdedetrich</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:24:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA["I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.Most of the time I simply excused myself with "if something urgent comes up, I can chime in and help out". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:"we are the thin blue line"This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.]]></content:encoded></item><item><title>Building the MagicMirror in Rust with iced GUI Library ü¶Ä</title><link>https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/</link><author>/u/amindiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:56:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and I‚Äôd like to share my experiences. I wrost a blog post titled "software you can love: miroir √î mon beau miroir" this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.Here's what I loved about using Rust and the iced GUI library:Elm Architecture + Rust is a match made in heaven: iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary. Although troublesome at first, I used ‚Äòcross‚Äô to cross compile Rust for armv7.If anyone is keen, I‚Äôm thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Richard Stallman in Polytechnic University of Turin, Italy</title><link>https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/</link><author>/u/ShockleyTransistor</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:05:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>async-arp: library for probing hosts and sending advanced ARP (Address Resolution Protocol) requests.</title><link>https://www.reddit.com/r/rust/comments/1ipywbp/asyncarp_library_for_probing_hosts_and_sending/</link><author>/u/arcycar</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:48:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[After a few months of exploring and working with Rust, I am happy to share my first small Rust crate,  and I‚Äôd love to hear your thoughts! üöÄThis library provides an  way to send and receive , making it useful for network discovery, debugging, and custom networking applications.üèé  Built on Tokio for non-blocking network operationsüîç  Easily detect active devices in a subnet‚öôÔ∏è  Craft and send ARP packets dynamicallyYou can find usage examples and API documentation here: üìñ Since this is my first crate, I‚Äôd really appreciate any feedback on:üìå  ‚Äì Is the interface intuitive and ergonomic?üöÄ  ‚Äì Does it fit well into async Rust workflows?üîç  ‚Äì Any improvements or best practices I may have missed?ü¶Ä  ‚Äì Suggestions to make it more "Rustacean"?If you have further ideas, issues, or want to contribute, check it out on GitHub:Thanks for checking it out‚Äîlet me know what you think! ü¶Ä]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.</title><link>https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/</link><author>/u/trollfinnes</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:05:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.I was expecting to spend the "weekend" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. And while he is definitely in the upper half of people in terms of 'tech savvyness', both those "two easy first steps" made it virtually impossible for him to install it. He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.But he wasn't able to get into bios despite all my attempts explaining what button to push and whenNext day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.After that he managed to install Linux just fine without my supervision.But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.I don't know enough about programming to know of this is possible:Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?That would 'open up' Linux to  more people, probably orders of magnitude..]]></content:encoded></item><item><title>Lessons from David Lynch: A Software Developer&apos;s Perspective</title><link>https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/</link><author>/u/aijan1</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 09:40:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He‚Äôs perhaps best known for the groundbreaking TV series Twin Peaks, which inspired countless shows, including The X-Files, The Sopranos, and Lost.Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down ‚Äì even those who truly deserved it.Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that Mulholland Drive remained compulsively watchable while refusing to yield to interpretation.While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I‚Äôd like to share my perspective on his life lessons from a software developer‚Äôs viewpoint.Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you‚Äôve got to go deeper.We‚Äôve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one ‚Äìbecause they‚Äôre so rare‚Äì write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it‚Äôs a film, a painting, or software.The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.Software development is part art, part engineering. We don‚Äôt build the same software over and over again ‚Äì virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it‚Äôs very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.It‚Äôs a good habit to listen to what users have to say, but they often can only describe their problems ‚Äì they rarely come up with good ideas to solve them. And that‚Äôs OK. It‚Äôs our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  ‚Äì that magical state of mind where we lose track of time and produce code effortlessly. That‚Äôs why many developers hate meetings ‚Äì they are toxic to our productivity.I believe you need technical knowledge. And also, it‚Äôs really, really great to learn by doing. So, you should make a film.Software development is one of those rare fields where a college degree isn‚Äôt required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It‚Äôs crucial to never stop learning, experimenting, and iterating on our craft.Happy accidents are real gifts, and they can open the door to a future that didn‚Äôt even exist.Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.Be kind to your teammates, don‚Äôt embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety ‚Äìthat is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by Google‚Äôs research on the subject.It‚Äôs OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.Most of Hollywood is about making money - and I love money, but I don‚Äôt make the films thinking about money.Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.What makes these projects remarkable is that they didn‚Äôt emerge from corporate boardrooms ‚Äì they were built by communities of passionate developers, collaborating across the world.Money is just a means to an end. Unfortunately, many get this confused.David, thank you for making the world a better place!]]></content:encoded></item><item><title>&quot;Dongly Things&quot; by Douglas Adams (of Hitchhikers Guide) - Adams wrote this article in the early days of Mac computers, about manufacturers making things difficult with a million different proprietary cables/ports etc.</title><link>https://www.douglasadams.com/dna/980707-03-a.html</link><author>/u/CaesarSalvage</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 07:47:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>Kafka Delay Queue: When Messages Need a Nap Before They Work</title><link>https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 05:08:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework You‚Äôve Been Waiting For! üöÄ</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, I‚Äôve been missing a Nest.js-like framework ‚Äî its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn‚Äôt have a direct counterpart (yet!), I decided to build one myself! üõ†Ô∏èIntroducing‚Ä¶ Toni.rs ‚Äî a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it‚Äôs live in beta! üéâHere‚Äôs what makes this project interesting:Scalable maintainability üß©:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code ‚Äî each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection ü§ñ:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below ‚Äî suggestions, questions, or even just enthusiasm! üöÄ ]]></content:encoded></item><item><title>what was the Linux expirance like in the 90&apos;s and 00&apos;s?</title><link>https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/</link><author>/u/mrcrabs6464</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 01:35:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?   submitted by    /u/mrcrabs6464 ]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...üìä Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and SqliteüóÇÔ∏è Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>