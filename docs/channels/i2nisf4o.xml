<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>Compute Freedom: Scale Your K8s GPU Cluster to &apos;Infinity&apos; with Tailscale</title><link>https://www.reddit.com/r/kubernetes/comments/1lpqmtw/compute_freedom_scale_your_k8s_gpu_cluster_to/</link><author>/u/qingdi</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 08:38:07 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/qingdi ]]></content:encoded></item><item><title>grep isn&apos;t what you think it means...</title><link>https://youtu.be/iQZ81MbjKpU</link><author>/u/MatchingTurret</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 08:31:25 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Exploiting the IKKO Activebuds &quot;AI powered&quot; earbuds, running DOOM, stealing their OpenAI API key and customer data</title><link>https://blog.mgdproductions.com/ikko-activebuds/</link><author>/u/Kok_Nikol</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 08:27:17 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[So my journey with these earbuds started after I saw them on this Mrwhosetheboss video about pointless tech. This device seems to be also popular on TikTok. My suspicions were confirmed, this runs android. So of course i went ahead and bought them.245 euros later... and they finally arrived!Before we dive further into this, unlike with rabbit, this issue has been properly reported and patched. This is also my first real blog post/disclosure so feedback is appreciated.I like how they strapped a USB-c cable to the outside of the box while there is also a smaller one inside the box. They ran out of box space it seems...I also wonder if they are legally allowed to use this OpenAI logo (probably not lol)Anyways, we aren't here for fancy boxes, lets get to the main point. The device itself boots up to a screen with the time and ChatGPT front and center.There are some other AI features available too like translations. But this isn't a review of the device, you can watch other YouTube videos about that. The ChatGPT animation looks way too similar to the actual app and OpenAI could probably get them in legal trouble for stealing their brand identity. I will also mention that the audio quality is absolute shit if you use their EQ profiles but can be upped to a usable level by tweaking the EQ curves yourself.There are also some apps available in the IKKO store, the reason that there is no google play store available is because these apps are modified specifically for the screen on the ActiveBuds, at least, that is what the CEO says about them. We will check that out in a bit. These apps include some music apps like Spotify, but also some gaming apps like, oh god, SUBWAY SURFERS BAYBEEEEEOf course all of them unbearable to navigate due to the small screen. However we can now confirm that it most definitely runs android.There is sadly no browser available to directly download other apps. And while you can open the native android settings app, clicking the build number 7 times does not enable developer mode. So i couldn't enable adb it seems. Is it locked that well? heh nope.Let's just plug it into a pc and see what happens....What the fuck, they left ADB enabled. Well, this makes it a lot easier. After sideloading the obligatory DOOM, i began checking out how the ChatGPT integration works on the backend. I first started HTTP inspecting the device, however since i couldn't enable the proper system certificates without rooting the device, i couldn't see exactly to what URL it communicated. Fortunately that wasn't really needed.Holy shit, holy shit, holy shit, it communicates DIRECTLY TO OPENAI. This means that a ChatGPT key must be present on the device!I know that this device can be rooted to get the proper certificates installed because a tool exists on all Spreadtrum/Unisoc devices which can be used to unlock the bootloader as long as companies use the default signing keys. This was indeed the case here too. However, i couldn't get past the confirmation screen as the device does not have a volume up key to confirm the unlock. I think you are able to sign your own partitions to make it flash them without an unlocked bootloader but that's a bit too advanced for my own liking.So, i went back to the drawing board and just dumped all of the apps from it with an APK extractor tool. After popping the launcher app into JADX, things immediately became concerning.The device can communicate to either of these domains.api.openai.com
Obvious, the OpenAI APIchat1.chat.iamjoy.cn
Seems to be the API for the entire device, including features not related to ChatGPT like the app store. Loading it up in a browser gives a login page.chat2.chat.iamjoy.cn
Same thing as chat1, possibly a backup server?openspeech.bytedance.com
No idea, might be a speech recogniser backup instead of whisper, haven't seen communication to this from the device.www.airdimple.cn
Seems like an OpenAI API mirror or proxy?Knowing this i went hunting for api endpoints and keys. I found a file called SecurityStringsAPI which contained encrypted endpoints and authentication keys. You might think, hey that's just base64 idiot, the most basic encoding known to mankind. And well, yeah, it is.However, there is a second stage which is handled by a native library which is obfuscated to hell. I am not going to even try to read that. Fortunately i didn't have to. I just sideloaded the app on a different device which was rooted, and well, just like the rabbit apk, it just works!Yup, that's an OpenAI key.Now, while having this access, we can also expose their (pretty funny) system prompt.The device also has another few modes, which are Angry Dan and In-Love Dan. For the angry one you need to confirm you are 18+ because it actually swears a lot.The system prompts for these are a bit more boring.I also noticed that it logs the chat to another endpoint on the chat1 domain. This is probably just to keep a log of messages since the ChatGPT API does not allow that. Possibly for some Chinese espionage? Well, possibly but not entirely, we will get to that.The headers for this request include the message, model, response and the device IMEI as the device id.I also sideloaded the store app and found out that the apps seem to be mostly ripped straight from apkpure.comAfter discovering this information, i sent an email to the security department of IKKObuds.While waiting for their response i started to investigate their companion app. Wait i forgot to tell you about that? Yeah, these earbuds have a companion app with which you can also directly interface with ChatGPT and see your previous chats from the device. So that's what the logging endpoint is used for! You bind the app by scanning a QR code from the device in the "Membership" menu.So, let's HTTP inspect this app and check out where it gets this information from.Alright so it queries this API with your account token and your device id and returns all the chats you have ever had with the device. However, after removing the account token, the request still worked? So this api has no authentication apart from the device id. I feared the worst.I found a frame in the tutorial video in which the device id wasn't properly blurred and plugged that into the api.YUP, i now had their entire demo device chat history. And as the IMEI has a certain range, you would be able to figure out the chat history of all customers, which may include sensitive details.I also added this new discovery to the email chain.While that email was waiting for a reply i checked if i could fabricate a linking QR code from a known IMEI to bind the device. (The QR code is not the IMEI itself but something encrypted) I found the API endpoint by looking at the same SecurityStringsAPI, which was less secure than i initially thought because the variable names literally expose the encrypted api endpoints (lol)Plugging in the getBindDevQrCode api in postman, i could fabricate a base64 image of the QR code with any IMEI.However, using this QR code to try and bind the device to my app resulted in an error, saying that the device has already been bound to someone else. So that has been the only good security implementation up until now.However, i lied, this is still a security/privacy issue. Why, you may ask? This exposes the username you set when creating the account for the app. However, there is no username field when creating your account. Only first and last name.I created an account with the first name as "Cheese2" and the second name as "Delight2". Turns out that the username is equal to First name + Last name. When trying to bind that device to an app after it has already been bound to another app, the response includes the name "Cheese2Delight2". Great. Doxed.So what we can do now is guess IMEI -> generate QR code -> Bind the device if not bound already, or get your full name when the device is already bound. -> Get all your chat history either way if the device is bound or not.There is an unbind_dev endpoint????Unfortunately that one actually checks account token and does not allow to unbind a random device IMEI. Phew.Hey, do you remember that logging endpoint that actually sent your chats you made with ChatGPT to their servers? This one?Yeah, that also only used the device id as authentication, so we can send arbitrary text to the companion app of anyone....I tried to send some HTML and JS through it to try and exploit the companion app, fortunately they use vue for their app and that has default HTML and JS injection security built in. But we can still send scams or something to any user.Oh hey a reply to my email!First of all, from a gmail address? Come on, actually try to have at least some professionalism. Second, OK they are actually doing something about it. (The YouTube channel mentioned is because i said that i will be making a video about this. I have all the footage for it but i hate my voice with a passion so here we are on this blog post :))Shortly after this email, they locked down the app and put out an announcement stating that the app will be in maintenance for a week.They also wanted to become a sponsor of my empty YouTube channel? What? I don't think that they understood that i would be talking about their horrible security. Anyways.The API was now non functional and displayed a maintenance message. After the service period they put out both an app update and a device update. What changed? The endpoint to get the chat history now needs a "signature" header. Which is composed of your account token, your device id, language and the current time encoded with a public/private key + a password. Anyways, it is now impossible to fetch the chats without having a valid account token. Still doesn't fix the fact that i can generate a QR code with the guessable IMEI and bind the device to an app if it hasn't been bound already. That circumvents this all. The device update broke the ChatGPT functionality from functioning on a device which is not the IkkoBuds itself. The keys remain on device and have not been rotated. So if anyone is able to figure out the broken app on another device or the key encryption system, you can still get your very own free OpenAI API key.However i just gave up at this moment, also because they never replied with anything after my last email criticizing them for leaving the keys on device. This is now a month and a half ago.So, that is it. You can still inject messages into apps of others, link devices that are not already bound to another companion app, thus leaking chat history. And leak first and last names of devices which are bound.I am giving up, but if anyone else wants this company to fix this, be my guest.Also if you liked this deep dive, consider supporting me so i will be able to convince myself that buying more strange android devices is worth it lolhttps://ko-fi.com/mgdproductionsI got this device rooted with help from @haro7zThey are now checking the device's imei before it is able to use the chatgpt integration and are now using a proxy api instead of calling directly to openai. However this proxy api doesn't require any auth and only requires the User-Agent to be set to okhttp/4.9.0 LOLThey have also FINALLY rotated their old chatgpt api key!]]></content:encoded></item><item><title>Kubernetes RKE Cluster Recovery</title><link>https://www.reddit.com/r/kubernetes/comments/1lppqq0/kubernetes_rke_cluster_recovery/</link><author>/u/Always_smile_student</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 07:35:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[There is an RKE cluster with 6 nodes: 3 master nodes and 3 worker nodes.Docker containers with RKE components were removed from one of the worker nodes.How can they be restored?kubectl get nodes -o widedaf5a99691bf rancher/hyperkube:v1.26.6-rancher1 kube-proxydaf3eb9dbc00 rancher/rke-tools:v0.1.89 nginx-proxy2e99fa30d31b rancher/mirrored-pause:3.7 k8s_POD_coredns5f63df24b87e rancher/mirrored-pause:3.7 k8s_POD_metrics-server9825bada1a0b rancher/mirrored-pause:3.7 k8s_POD_rancher93121bfde17d rancher/mirrored-pause:3.7 k8s_POD_fleet-controller2834a48cd9d5 rancher/mirrored-pause:3.7 k8s_POD_fleet-agentc8f0e21b3b6f rancher/nginx-ingress-controller k8s_controller_nginx-ingress-controller-wpwnk_ingress-nginxa5161e1e39bd rancher/mirrored-flannel-flannel k8s_kube-flannel_canal-f586q_kube-system36c4bfe8eb0e rancher/mirrored-pause:3.7 k8s_POD_nginx-ingress-controller-wpwnk_ingress-nginxcdb2863fcb95 08616d26b8e7 k8s_calico-node_canal-f586q_kube-system90c914dc9438 rancher/mirrored-pause:3.7 k8s_POD_canal-f586q_kube-systemc65b5ebc5771 rancher/hyperkube:v1.26.6-rancher1 kube-proxyf8607c05b5ef rancher/hyperkube:v1.26.6-rancher1 kubelet28f19464c733 rancher/rke-tools:v0.1.89 nginx-proxy]]></content:encoded></item><item><title>K3s or full Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1lposyz/k3s_or_full_kubernetes/</link><author>/u/ReticularTen82</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 06:34:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[So I just build a system on a supermicro x10dri. And I need help. Do I run K3S or full enterprise kubernetes?]]></content:encoded></item><item><title>Built Elasti – a dead simple, open source low-latency way to scale K8s services to zero 🚀</title><link>https://www.reddit.com/r/kubernetes/comments/1lpluou/built_elasti_a_dead_simple_open_source_lowlatency/</link><author>/u/ramantehlan</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 03:39:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We recently built  — a Kubernetes-native controller that gives your  true , without requiring major rewrites or platform buy-in.If you’ve ever felt the pain of idle pods consuming CPU, memory, or even licensing costs — and your HPA or KEDA only scales down to 1 replica — this is built for you.Elasti adds a lightweight proxy + operator combo to your cluster. When traffic hits a scaled-down service, the proxy:Forwards the request once the pod is ready.And when the pod is already running? The proxy just passes through —  in the warm path.It’s designed to be minimal, fast, and transparent. Bursty or periodic workloads: APIs that spike during work hours, idle overnight.: Tear everything down to zero and auto-spin-up on demand.: Decrease infra costs by scaling unused tenants fully to zero.We did a deep dive comparing it with tools like Knative, KEDA, OpenFaaS, and Fission. Here's what stood out:Works with any K8s ServiceWe kept things simple and focused: for now (TCP/gRPC planned). metrics for triggers.Deployment & Argo Rollouts only (extending support to other scalable objects). CRD → defines how the service scalesElasti Proxy → intercepts HTTP and buffers if neededResolver → scales up and rewrites routingWorks with Kubernetes ≥ 1.20, Prometheus, and optional KEDA for hybrid autoscalingMore technical details in our blog: — proxy just forwards.: Helm + CRD, no big stack. — use your existing Deployments.If you're exploring serverless for existing Kubernetes services (not just functions), I’d love your thoughts:Does this solve something real for your team?What limitations do you see today?Anything you'd want supported next?Happy to chat, debate, and take ideas back into the roadmap.— One of the engineers behind Elasti]]></content:encoded></item><item><title>How loosely coupled should I make my code???</title><link>https://www.reddit.com/r/golang/comments/1lpirr4/how_loosely_coupled_should_i_make_my_code/</link><author>/u/ShookethThySpear</author><category>reddit</category><pubDate>Wed, 2 Jul 2025 01:02:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am a relatively new Go developer so I'm still working my way around Go coding and best practices in Go development. I am currently creating a microservice for personal use now my question is that how loosely coupled do you guys make your code? I am currently using multiple external libraries one of which is widely used in my microservice. I used it widely due to the fact that the struct included in the package is massive and it contains many more nested structs of everything I need. I was thinking of decoupling code from 3rd party packages and also trying out dependency injection manually through interfaces and main() instantiation, but my worry is if I were to create an interface that my services can depend on, I have to create my own struct similar to the one provided by that 3rd party package just for the sake of abstraction.]]></content:encoded></item><item><title>[D] Request for Career Advice – ML PhD non hot topic</title><link>https://www.reddit.com/r/MachineLearning/comments/1lphfhf/d_request_for_career_advice_ml_phd_non_hot_topic/</link><author>/u/Hope999991</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 23:57:13 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I’m currently a PhD student in Machine Learning, working on a research topic that isn’t considered “hot” in the current academic or industrial landscape. Despite this, I’ve managed to publish as the lead author at ICML, NeurIPS. And twice at ECML. I also have two co-authored publications at ECAI.I’ve noticed that many PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market—especially given the wave of layoffs and increasing demand for very specialized expertise in industry.That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren’t the direct focus of my PhD research).Given all of this, I’m trying to realistically assess: • What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? • What could I do now to improve those chances? • Goal is FANNG.I’d greatly appreciate any feedback.Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.]]></content:encoded></item><item><title>6 months ago didn&apos;t know how to code, now I launched my first app that actually has users</title><link>https://www.reddit.com/r/artificial/comments/1lph92p/6_months_ago_didnt_know_how_to_code_now_i/</link><author>/u/Sad_Mathematician95</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 23:49:11 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Kinda wild to see how far you can take the use of AIA fully functional Photo restoration app that has a Gallery feature with sorting tools like folders and tags, Family tree builder and more!If anyone is curious to try it's free!]]></content:encoded></item><item><title>How do you ship go?</title><link>https://www.reddit.com/r/golang/comments/1lpgkn2/how_do_you_ship_go/</link><author>/u/itsabdur_rahman</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 23:18:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I created a todo list app to learn go web development. I'm currently using templ, htmx, alpine and tailwind. Building the app was a breeze once I got used to the go sytanx and it's been fun.After completing the app I decided to make a docker container for it, So it can run anywhere without hassle. Now the problem starts. I made a container as folows:FROM golang:1.24.4 WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . # Install tools RUN curl -L -o /usr/local/bin/tailwindcss https://github.com/tailwindlabs/tailwindcss/releases/latest/download/tailwindcss-linux-x64 && chmod +x /usr/local/bin/tailwindcss RUN go install github.com/a-h/templ/cmd/templ@latest RUN go install github.com/sqlc-dev/sqlc/cmd/sqlc@latest # Produce Binary RUN tailwindcss -i ./static/css/input.css -o ./static/css/style.min.css RUN templ generate RUN sqlc --file ./internal/db/config/sqlc.yaml generate RUN go build -o /usr/local/bin/app ./cmd CMD [ "app" ] The problem I see here is that the build times are a lot longer none of the intall tool commands are cached (There is probably a way but I don't know yet). The produced go binary comes out to be just about 15 mb but we can see here that the containers are too big for such a small task$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE todo-app latest 92322069832a 2 minutes ago 2.42GB postgres 16-alpine d60bd50d7e2d 3 weeks ago 276MB I was considering shipping just the binary but that requires postgres so I bundle both postgres and my app to run using docker compose. There has to be a way to build and ship faster. Hence why I'm here. I know go-alpine has a smaller size that still wouldn't justify a binary as small as 15 mbHow do you guys ship go web applications. Whether it is just static sties of with the gothh stack.]]></content:encoded></item><item><title>RFK Jr. Says AI Will Approve New Drugs at FDA &apos;Very, Very Quickly. &quot;We need to stop trusting the experts,&quot; Kennedy told Tucker Carlson.</title><link>https://gizmodo.com/rfk-jr-says-ai-will-approve-new-drugs-at-fda-very-very-quickly-2000622778</link><author>/u/esporx</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 22:51:26 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Robert F. Kennedy Jr. appeared on the latest episode of Tucker Carlson’s podcast on Monday and it’s filled with the ramblings of a man completely detached from reality. Kennedy falsely suggested vaccines cause autism, more or less endorsed the idea that Anthony Fauci should go to prison, and says that AI will allow the FDA to approve new drugs very quickly. It’s quite a mess.These absolutely unhinged ideas wouldn’t be such a problem if this were any other fringe lunatic appearing on the podcast of a racist former Fox News host. But Kennedy happens to be the Secretary of Health and Human Services, a man who’s been given enormous power over America’s entire healthcare system thanks to President Donald Trump.One of the most troubling moments in the new interview comes when Kennedy discusses the role that artificial intelligence is going to play in replacing or altering the VAERS system, which stands for Vaccine Adverse Event Reporting System. VAERS allows doctors to report incidents when they believe a patient has been harmed by vaccines, but Kennedy isn’t happy with it. The secretary insists it was “designed to fail,” suggesting it’s not registering enough people who in his mind have been harmed by vaccines over the years.“We’re going to absolutely change VAERS and we’re going to make it, we’re going to create either within VAERS or supplementary to VAERS, a system that actually works,” Kennedy said. “And, you know, right now, even that system is antiquated because we have access to AI.”Kennedy told Carlson he was creating an “AI revolution” at the Department of Health and Human Services and was attracting the top people from Silicon Valley who “walked away from billion dollar businesses.” But Kennedy says these people don’t want prestige or power, they just want to make the healthcare system better.“We are at the cutting edge of AI,” Kennedy said. “We’re implementing it in all of our departments. At FDA, we’re accelerating drug approvals so that you don’t need to use primates or even animal models. You can do the drug approvals very, very quickly with AI.”Kennedy has previously talked about using AI to increase efficiency at FDA but hasn’t provided details about what AI tools will be used and how they would be used to approve new drugs. But given generative AI’s instability and propensity for failing at some of the most basic tasks, the idea of putting drug approvals in the hands of robots is pretty terrifying.Kennedy, who was the founder of an anti-vaccine group called the Children’s Defense Fund, says repeatedly during the interview that vaccines have never been properly studied, which is just a flat-out lie. But he now has the power to demand investigations into vaccines that will get him the results he wants, no matter how much he insists his own opinion doesn’t matter.“We need to stop trusting the experts, right?” Kennedy told Carlson. “We were told at the beginning of COVID, don’t look at any data yourself, don’t do any investigation yourself, just trust the experts. And trusting the experts is not a feature of science, it’s not a feature of democracy, it’s a feature of religion, and it’s a feature of totalitarianism.”Kennedy went on to insist that it was important for everyone to “do your own research,” a common refrain among those in the so-called Make America Healthy Again movement. But Kennedy is intentionally misrepresenting the role of experts in an informed society. Listening to experts isn’t about abandoning all critical thinking. It’s about recognizing that there are areas where you may not have expertise and taking the opinions of medical professionals more seriously than random people on shows like Joe Rogan and Tucker Carlson who are just self-proclaimed experts.Kennedy was asked several leading questions from Carlson, including whether the covid-19 vaccine has killed more people than it saved. And Kennedy is skilled enough as a communicator (his father was Attorney General during his uncle’s presidency, as he frequently mentions) that he can avoid directly answering in the affirmative while subtly telling you that he believes it’s the case.Notice, for instance, how Kennedy initially responds to Carlson’s question while eventually working his way to sowing doubt about trust in vaccines. Do you think overall the COVID vaccine killed more than it saved? My opinion about that is irrelevant. What we’re going to try to do is make that science available so the public can look at the science. And I would not say one way or the other. And the truth is, I don’t know. And the reason I don’t know is because the studies that were done by my agency were substandard. And they were not designed to answer that question. And there’s been a lot of obfuscation about covering up, as you know, about suppressing any kind of discussion of vaccine injuries.Kennedy is often effective at manipulating an audience, but also says things that don’t make any sense, even if you agree with his worldview. At one point during his interview with Carlson he said that when Pfizer’s covid-19 vaccine was studied there were two people who died in the control group and one person who died in the vaccine group.“You remember they were saying the vaccine is 100% effective? Well, that’s why they were saying it because there was… there was… two is 100% of one,” Kennedy said.That’s not how anyone is measuring the efficacy of vaccines. Yes, some of the early studies were admittedly too rosy in their projections, especially those in early 2021 as the vaccines were first released. But nobody was claiming that two people dying in a control group and one person dying in the vaccine group showed the vaccine was 100% effective. That math isn’t anything that was actually presented in any study Gizmodo is aware of.Kennedy was also asked about whether Anthony Fauci, the nation’s most visible public health expert during the covid-19 pandemic, would be prosecuted for some unspecified crimes. Again, the secretary danced around a bit with his language but then heavily suggested Fauci should be tried for criminal acts. Kennedy said there should be some kind of “truth commission” for covid-19 vaccines like the truth and reconciliation commissions in South Africa and Central America in the 20th century under repressive governments.“Anybody who comes and volunteers to testify truthfully is then given immunity from prosecution. And, but, so that at least the public knows who did what,” Kennedy said. “And people who are called and don’t take that deal and purge themselves, they then can be, they can be prosecuted criminally.”Kennedy believes that Fauci was involved in some kind of weaponization of covid-19 and in cahoots with the Chinese government. “I think he had a lot of liability on creating coronavirus,” Kennedy said. “You know, he was funding precisely that research at the Wuhan lab. And he was giving them the technology.”When Kennedy notes that Fauci no longer has protection from the Secret Service since President Trump withdrew it, Carlson responds “good.” Fauci received countless death threats from lunatics over the years.Kennedy didn’t really get into the spiritual side of his MAHA movement during his latest interview, something that’s previously been top of mind. In fact, Kennedy was very focused on the role of a higher power when he last appeared on Carlson’s show back in August 2024, shortly after abandoning his own bid for president. Casey Means, Kennedy’s pick to be Surgeon General, has also appeared on podcasts like Joe Rogan to spout many of the same crazy talking points and emphasize how important spirituality is for health. But it remains to be seen whether Means will be confirmed by the U.S. Senate. Kennedy recently said he’s going to push for all Americans to get a wearable device to monitor their health, and as luck would have it, Means sells a wearable for monitoring glucose. The device is targeted at consumers who aren’t even diabetic, the people who do actually need glucose monitoring.The entire episode of Tucker Carlson is available on YouTube but it’s a frustrating thing to sit through for any halfway intelligent person. At one point, Kennedy insists Trump is a smart guy, calling him “immensely knowledgeable” and “encyclopedic in certain areas.” Kennedy even referred to Trump as “one of the most empathetic people that I’ve ever met.” The only point where Kennedy seems to disagree with Trump is on tariffs, with the secretary saying that “businesses are hurting because of the tariffs.” But it’s the kind of quick dissent that will likely go unnoticed given how Kennedy praises the fascist president incessantly throughout.You’ve been warned. Listen at the risk of your own sanity.]]></content:encoded></item><item><title>Any deaf Linux users here?</title><link>https://www.reddit.com/r/linux/comments/1lpfpf5/any_deaf_linux_users_here/</link><author>/u/Macdaddyaz_24</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 22:39:08 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Who here is Deaf? Been wanting to create a deaf only Linux user subreddit. Please comment here if you’re deaf and use linux, plus interested in creating a deaf Linux subreddit. This way we can work with like minded users :)]]></content:encoded></item><item><title>Linux breaks through 5% share in USA desktop OS market (Statcounter)</title><link>https://www.reddit.com/r/linux/comments/1lpepvq/linux_breaks_through_5_share_in_usa_desktop_os/</link><author>/u/MrHighStreetRoad</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 21:57:08 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/MrHighStreetRoad ]]></content:encoded></item><item><title>I&apos;ve been writing Rust for 5 years and I still just .clone() everything until it compiles</title><link>https://www.reddit.com/r/rust/comments/1lpc85o/ive_been_writing_rust_for_5_years_and_i_still/</link><author>/u/kruseragnar</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 20:17:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[That's it. That's the post. Then I go back and fix it later. Sometimes I don't.]]></content:encoded></item><item><title>Libreboot 25.06 released</title><link>https://libreboot.org/news/libreboot2506.html</link><author>/u/libreleah</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 19:42:39 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Article published by: Leah RoweDate of publication: 30 June 2025There  a Libreboot 25.04 release in April 2025, but that is retroactively regarded as an RC of 25.06. The original 25.06 release announcement showed changes since 25.04, but the changelog is now relative to December 2024. This reflects the revised release schedule. It means that the changelog is much bigger, and also includes the changes that went in Libreboot 25.04.Today’s Libreboot 25.06 revision is a , whereas the previous stable release was Libreboot 20241206. This revised release log lists all changes as of today, 30 June 2025, since the Libreboot 20241206 release of December 2024.Open source BIOS/UEFI firmware[link]Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g. Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of coreboot for hardware initialisation, and then a payload such as SeaBIOS or GRUB to boot your operating system; on ARM(chromebooks) and certain x86 mainboards, we provide  (as a coreboot payload), which provides a lightweight UEFI implementation..Summarised list of changes[link]The priority for the first half of 2025 has been on further auditing the Libreboot build system, so fewer board ports were added. More board ports will be added instead in the December 2025 release (a lot more).The following boards have been added since the Libreboot 20241206 release:Acer Q45T-AM support added (similar to G43T-AM3 mainboard)Dell Precision T1700 SFF and MTBoard ports were low priority for this release; now it shall be the focus, between June 2025 and October 2025, ready for the 25.12 release cycle leading into December 2025.Dell Precision T1700 is essentially the OptiPlex 9020 but with a slightly different, code-compatible PCH that also supports ECC memory features when an Intel Xeon processor is installed.In descending order from latest changes to earliest changes:GRUB: Update to revision 73d1c959e (14 March 2025)Bump SeaBIOS to to rev 9029a010, 4 March 2025Updated Untitled to newer LBSSG repository.Bump flashprog to rev e060018 (1 March 2025)Bump U-Boot on ARM64 boards to U-Boot v2025.04. Patching courtesy of Alper Nebi Yasak.Bump uefitool to rev a072527, 26 Apr 2025 to fix CMake compatibility issue since CMake 4 no longer supports version 3.5, whereas the old uefitool had an earlier version as the minimum supported. This fixed a minor build error.Merged coreboot/next with coreboot/defaultBump coreboot/next to rev c247f62749b as of 20 April 2025Bump coreboot/default to rev c247f62749b as of 20 April 2025Bump flashprog to revision eb2c041 (14 Nov 2024).The GRUB revision includes a number of critical CVE fixes, and regression fixes, that were also included in Libreboot 20241206 rev11. Some later fixes are also present, such as wiping LUKS keys from memory after successfully booting Linux (Linux handles LUKS itself, and starts the process again).The NASM version was updated to version 2.16.03 on coreboot/fam15h, to prevent build errors, instead of fixing the old NASM 2.14.02. Tested on Debian Sid Experimental, with GCC15-based toolchain, and on Fedora 42.PICO support: Reverted to the old pico serprog/sdk repositories used in Libreboot 20240612. This is temporary, because pico2 support is currently broken, so this release only has pico1 support, when dealing with Rpi Pico devices. Upstream pico-serprog works fine on pico2, so this will be fixed in and re-updated again in a future revision release. The pico2 update images were retroactively removed from the 20241206 release on rsync.A patch from upstream was backported to the old pico-sdk version, so that it builds correctly on newer GCC15 (tested on Debian Sid with “Experimental” packages enabled).Added SPDX license headers to almost every configuration file in lbmk.These can be considered bug fixes, but these are special fixes that are of massive concern to users.This GRUB change was merged, in the aforementioned revision update: dbc0eb5bd disk/cryptodisk: Wipe the passphrase from memory - this wipes the LUKS key from memory, after GRUB exits, where one was created by GRUB while unlocking a given volume.Merged  critical CVE fixes into the GNU GRUB source code, from upstream.Stricter use of pledge and unveil in the nvmutil source code. safer . It used to be that the tarballs were extracted and files inserted into the extracted images, but the tarballs were left unmodified; many users thought then that they should extract the tarball and flash that, which lead to bricks. And it was easy to flash uninjected images, where files (e.g. Intel ME) are needed, so now ROM images are padded by one byte, to prevent flashing, and the user is strongly reminded to inject files first; upon running the  commands, these images are then safe to flash.Fix globbing issues in lbmk by double-quoting variables everywhere, and generally making sure that certain characters are escaped properly when necessary. To reduce the chance of bad commands being run by mistake or intentionally.Removed auto-confirm on  commands, to mitigate the risk of a buggy package manager on the user’s distro possibly removing many packages. Now the user must confirm their choice, e.g. when a conflict occurs, instead of the package manager already deciding for the user.ThinkPad T480 / OptiPlex 3050: Disable HyperThreading/SMT by default, for security, to reduce the attack vector of certain speculative execution-based exploits.In descending order from latest changes to earliest changes:: looser  validation; correct it on child instances, if it’s not set, or set incorrectly.: use subshells on  functions, wrapped in an error handler so as to provide more verbose output under fault conditions. This makes it easier to debug when a download fails.: Re-implement redundant git downloads, more reliably than before; all repositories are now cached, reliably, including submodules, even when upstream repo links differ wildly. This reduces the amount of internet bandwidth used, when handling multiple builds.: build in tmp directory first, leaving old files behind under fault conditions, for further analysis: re-add mac address confirmation, for user-friendliness, when running the inject commands.: Resolve  via readlink: Use  in , with realpath only as fallback. This makes the function more redundant, working on more systems by default.: support any command on  (later renamed); this is a generic function, that implements a while loop for a given set of files, based on the output a command that generates those paths. This is operated on by a function, defined when calling find_exec. This unifies all use of while loops on lists of files and directories, throughout xbmk, rather than re-implementing the for/while loops each time.: simplify kconfig scanning by using the  with a new function, . This new function checks  coreboot configs for a given target, whereas the old behaviour only resulted in the  config being checked. In practise, this causes no real behaviour changes.: Print the rom image path being generated: Add warning if x_ is called without args: More verbose error info, on non-zero exits.: Within each 4KB part, only handle 4KB, even if the block size is bigger. This means using less memory, and modification of anything past 4KB is not required.: Support 16KB and 128KB GbE files, in addition to the usual 8KB files. The size is based on the block size of the flash you use.Added non-root USB3 hub support to GRUB on the xHCI implementation, courtesy of a patch from Nitrokey.GRUB: Scan LUKS inside  LVM, to support the uncommon use case where LUKS is inside LVM, instead of LVM inside LUKS. It is theoretically possible, even if ill advised.GRUB: Scan  LVM device, where available, as a fallback at boot time when all else fails.Release ROMs prefixed with a “DO NOT FLASH” warning and padded by one byte, where vendor files are required. The  commands remove this prefix/padding, after vendor files are inserted and checksums verified.Better detecting of whether vendor files are needed, and confirmation to the user while running  commands.Allow restoring the default MAC address on  commands, by using the  arguments.Randomise the MAC address by default, where applicable, when running the  commands, because lots of users previously flashed without changing it, so lots of users had generic MAC addresses. The  argument prevents this from happening, where desired.: More user-friendly debug messages, for the user to know what’s going on.: Add uninstall command to the Makefile: Add distclean command to the Makefile: Nicer hexdump display, similar to .Support a  argument in  Fedora commands, for re-installation of packages as desired.Support  in the  command, when the user wants to re-install dependencies.Put temporary  directory in the normal  directory, and clear it whenever a new parent instance of the build system is executed. This is used for the GCC/GNAT matched symlinks, for example, or the python symlink created at startup.Pico 2 support briefly added, but was a bit buggy for now, so it’s removed in this release, and was retroactively removed in rsync for the Libreboot 20241206 release; this will be re-added in a future release.Added GRUB-first payload setups as an option, but not enabled by default. The user can add  in the  file for a given mainboard.Support automatically downloading Lenovo ThunderBolt firmware for the ThinkPad T480, automatically padding it for installation. This update fixes a charging bug that affected some earlier launch models.Insert GRUB backgrounds in CBFS instead of GRUB memdisk, which makes GRUB background images easier to replace.In descending order from the latest changes to the earliest changes:ifd/hp8300usdt: set the HAP bit by default; it was previously not set, but the  config was nonetheless used, and ME Soft Temporary Disable was also used. As a result, this change is basically redundant, but otherwise technically correct (more so than the previous behaviour).coreboot: Remove unused vboot tests (futility tests), to shrink the size of release tarballs.coreboot/default: Remove unneeded FSP modules when downloading, because only the Kabylake version is needed at this time. This is done, using the  function via  files. This shrinks the size of release tarballs.: add HP 820 G2: Use fam15h cbfstool tree for refcode; this avoids the need to clutter the source code with an entire additional coreboot tree, thus reducing the size of releases.A GRUB configuration change was made, fixing auto-scanning of LVMs when doing cryptomount.T480/3050micro: Removed the  targets, because we only need the  targets.Added  to Fedora 41 dependencies.Added  to Arch dependencies, needed for the  utility.Added  to Arch dependencies, because it’s needed for certain commands e.g. git commands.GRUB: Use the codeberg mirror first, to mitigate GNU mirrors often being slow or rate limited, e.g. for gnulib downloads.fedora41/dependencies: add libuuid-develAdded  to fedora41 dependenciesflashprog: Disable  to prevent minor warnings being treated as errors.This combines both build system fixes, and changes to upstream sources (e.g. coreboot and various payloads like SeaBIOS/GRUB, utilities like flashprog, and so on).The following bug fixes have been merged (in descending order from the latest changes to the earliest changes):: add sha512 error for . Handle errors in  and ; also check that  exists and error out if it doesn’t, when checking a given project hash. We know that the project hash file should always exist, and always be read; technically, find might not yield results, but then an empty file would be produced. the empty file edge-case scenario would already have resulted in an error exit inside , so that’s already covered.: add error checking in , when reading the  variable; we need to error out where a read error occurs. such an error is extremely unlikely, so this fix is largely theoretical and preventative.: more reliable clean in ; don’t do a no-op if it fails, instead fall back to the  method, and throw an error if  fails. The no-op existed because not all projects have distclean, but we always intend for them to be cleaned. This therefore prevents further unhandled error conditions, in such edge cases.put coreboot utils in , to prevent old binaries from still being used when a code change is made.: use printf to create version files, instead of copying the version files, because they don’t exist in some cases, so this prevents an error condition.: error out if .git/ is a symlink; this is a preventative bug fix, to prevent future unknown bugs in such a scenario.: Properly error out if  fails, where it previously failed to throw an error under certain fault conditions.: Don’t auto-run make-oldconfig; it now must be applied permanently, via e.g.  commands. Otherwise, undesirable changes can sometimes be made during build time, especially on projects that don’t use scons quite as reliably, as in the U-Boot build system.: re-generate remotes every time, on cached Git repositories, so that configuration changes in  are automatically applied when dealing with multiple versions of a given upstream project.: copy version files to  (release source directory), otherwise an  version number is erroneously created. This fixes a regression caused by previous optimisation to xbmk: add fake config makefile args to , and , to prevent  (without additional arguments) from erroneously exiting with error status. otherwise, an error can occur in such conditions if a Makefile has not yet been created.: skip running  on dry builds, otherwise running  without argument will cause an error.: Don’t run make-clean on dry runs (), to prevent error conditions while building GRUB, if  is passed without additional argument, since the latter requiires running autoconf to get a Makefile in the GRUB build system.: add missing check in ; we were checking the main URL on a download, but not the backup URL.: stricter URL check in ; throw an error if a URL is empty, rather than skipping to the next. If a URL is set but fails, then falling back to the next is OK (or throw an error if the backup is set, and also failed).: Make  always throw an error upon exiting the loop check; it was previously throwing an error if the for loop returned with zero status. Depending on the sh implementation, or changes made in the future, this could cause unpredictable buggy behaviour. Therefore, the error exit is much stricter now, and less ambiguous, to prevent future bugs, because it is imperative that execution must never continue under fault conditions. If a file or repository is successfully handled, a return (zero) occurs, otherwise the loop exits and a non-zero exit occurs.: fix up , or specifically fix a bad  loop, because shorthand conditionals are used and the way they were used can be buggy on some sh implementations, so they are terminated more explicitly.xbmk: stricter handling of files on while loops, to prevent instances where execution continues under fault conditions. This prevents other, less predictable bugs in the future.: Hardcode  for integrity; this is a bug fix, because there’s too much that can be wrong with this being configurable, so now it is hardcoded at runtime. It was never intended to be configurable anyway.: check/validate version/versiondate once read, in child instances of xbmk, to further verify that they were previously set, and set correctly. This is theoretically a preventative bug fix.: force an error condition if the xbmk version was not read. This prevents further erroneous state within xbmk.: check the  file BEFORE , to prevent erroneous initialisation while another xbmk parent instance is running.: return from xbmk child instances in  instead. This is easier than the previous check, preventing the initialisation of a git repo and/or recreation of xbmktmp and xbmklocal by erroneoues parent executions of xbmk while another parent is running - the latter of which could have caused a massively unpredictable build failure, so this is also a preemptive bug fix, fixing and preventing all kinds of weird unknown bugs.: Remove  if it’s bad; this complements a bug fix, in the bug fix section above, that caches the extracted files and hashes them. On a subsequent run where the given file is needed, it is  if the final file exists. This mitigates the possibility that corruption may have occured, under unhandled fault conditions. Therefore, this is a preventative bug fix.: don’t move  to  inside release archives, because otherwise  will fail, inside release archives.: Properly verify SHA512SUM on extraction. This is performed on the actual extracted files, alongside the existing check on downloaded files. This mitigates against future fault conditions in the extraction process, thus fixing a major design flaw. This change also caches those files, thus speeding up extractions when they’re done multiple times. submodules: Don’t delete files recursively. Use  instead of , on files.: Only create destination repo on success; don’t leave a broken cache laying around, which would otherwise break the build system under certain conditions.: removed an unnecessary  variable: delete tmp/cache from release tarballs: Remove confusing path on tar creation; that is, don’t print said path, because temporary paths are printed during this, when creating tarballs. In this file, the correct path is printed at the end of the process, when handling an images tarball.: only create elfdir in , to prevent empty directories being created where a project provides , but where no actual configs are being built on a given target name.: operate on refcode in tmp area first, to prevent bad files from being saved to the final destination under fault conditions. This pertains to the change made at build time that enables GbE devices from the refcode.: use subshell to speed up  (this is a bug fix, because slowness is a bug): add missing error handli for  (when doing releases): hard fail if git am fails (regression fix): Hard fail if reset fails; allowing re-try when cloning fails, but the reset-fail scenario didn’t cause any exit at all. This is fixed now.: Only check  if it exists: fix trying to boot all logical volumes after unlocking an encrypted volume; this makes booting LVMs more reliable, on encrypted boot setups.: also allow  or , not just  and , because some people use uppercase here. This is considered a bug fix, but could just as easily have been in the features section.: check  is a directory instead of a file.: run , to prevent a future situation where the version is not set correctly. In general, the version should always be set as early as poessible when running xbmk.: clean up tmp me file before extract; this is a preventative fix, to ensure that cross-flashing does not occur.: re-add missing break in fe/fx_, that caused improper exits (or non exits) in some cases.: use , not ; this is a less strict test, to prevent certain errors under specific edge-case conditions.: Safer ; don’t insert special files like GRUB keymaps AFTER copying the system ROM to the final destination; do it BEFORE, instead, to ensure that bad images aren’t left in place under fault conditions.: specifically check keymaps in ; it previously checked whether a setup is  seauboot, which was valid, but future conditionals would break this check. the code has been changed in advance, to prevent bugs in a future revision of xbmk.: Fix bad error handling for ; I accidentally mixed and/or in a shorthand conditional statement, which leads to buggy behaviour in various implementations of sh.GRUB: Mark E820 reserved on coreboot memory, to fix cbmem when running with strict  access; otherwise, restrictions on access to memory below 1MB will cause an error when trying to access the cbmem console.: set  in  in case they were set  in other parts of xbmk.: Silence the output of git config –global: Run git name/email check before init; otherwise, it returns if init is already done, which could lead to an error later when building coreboot.: stricter  check in : simplify err-not-set handling err: add missing redirect to stderrxbmk: MUCH safer  function; make it an actual function, instead of a variable. Initially, this function was made to then check a variable, that refers to a function, and a fallback was provided for non-zero exit in case the pointed function didn’t, but it was later made to be just a simple function that exits with a message. Code equals bugs, so fewer lines of code will yield fewer bugs.: Make x_ err if first arg is empty; this is a preventative bug fix, to make the build system still exit under such conditions, but it would result in an empty error message.: Make err_ always exit no matter what; this is a preventative bug fix, just making the exit stricter in all cases.: re-make gnupath/ after handling crossgcc, rather than deleting files within. This makes the creation of it more reliable.: re-make gnupath/ for each cross compiler, to ensure that no stagnant build artifacts are re-used: Stricter TBFW handling; don’t copy it until it has been properly padded to the correct size.:  tmpdirs on parent instance, to ensure that they are not cluttered with old files that might cause weird bugs in the future; this is a preventative bug fix.: Always create xbmklocal, to prevent errors in the case when it isn’t created automatically in certain child instances, like when running a  copy of the build system, during release builds.: Fix bad touch command: always re-build nvmutil, so that changes to it are automatically re-applied when running the build system again. (and only build it once, for a given instance of xbmk): use , not , for random characters, while still supporting  for backwards compatibility. This is because ZSH errors out when providing the old characters, in some setups. Use of  is more reliable, across several implementations of sh, e.g.  would be a full random MAC address. find_ex: explicitly create the tmp file, to prevent errors, which were nonetheless unlikely to begin with.: Explicitly create the xbmktmp directory (make sure to do this when creating this which is a temporary directory).: add fe_ which is fx_ but err on findxbmk: unified execution on  commands. Handle it with a new special function that is common across the build system.: Download vendorfiles before building release, to mitigate intermittent internet connectivity during release builds, otherwise a release build could fail. This way, all downloads are done simultaneously, since downloads are the fastest part, even on a crap internet connection.Revert AHCI reset patch for SeaBIOS, which caused AHCI not to work in SeaBIOS on the 25.04 release; the latter was also revised, to fix this. SeaBIOS has since added a new release, which includes a fix that delays AHCI reset, to mitigate in cases where the controller isn’t ready sooner. However, this release simply reverts the AHCI reset patch for now. The AHCI reset plus delay will be present in Libreboot’s next release, after 25.06.lenovo/t420: Add missing text-mode configurationcoreboot (all trees): Added patch fixing GMP build errors on modern GCC15 hostcc.coreboot (all trees): Fixed building of crossgcc with newer GCC15. Patches courtesy of Alper Nebi Yasak.coreboot (all trees): Added a patch to fix building coreboot utils with newer GCC15.dependencies/debian: Fixed the libusb package name for newer Debian releases, courtesy of Alper Nebi Yasak.SeaBIOS: Fixed  function pointers in the  patch, courtesy of Alper Nebi Yasak. Fix build errors on GCC 15.: Force use of System Python e.g. , when a python venv is detected. This prevents the build system from hanging.coreboot : Fixed the  path.Alper Nebi Yasak fixed the Python 2/3 detection in some edge cases when the  command is python2. (later ): Do root check , right after the dependencies check, whereas it previously did the python check before checking for root user.lbmk: Don’t use TMPDIR directly, use another variable containing its value, and make sure it doesn’t get changed wrongly. This reduces the possibility of accidentally leaving old tmp files laying around.:  commands now return an exit with error, if a fault occurs, whereas it didn’t before, due to piped output. This is done using the  wrapper on tar commands, to provide error exits.: function  now returns an error, if the sha512sum command fails. It previously didn’t, due to piped outputs. It’s now mitigated by using  on piped commands, for error exits.Forking of lbmk parent instance to child instance isno longer handled by variables. It’s been simplified, to only be based on whether TMPDIR is set, and it’s generally more robust now in this release. The old code sometimes broke under certain edge cases. (later renaming to ): General code cleanup, about 100 sloc removed without reducing features.lbmk: Initialise  to a standard string if not set, on the parent instance of lbmk.lbmk: Use  instead of the  variable, resetting the latter safely as lbmk runs. This prevents lbmk from changing directory to an erroneous system path, if  wasn’t properly set for some reason. This is a preventative bug fix, because no actual issue ever occured in practise.Much safer Python version check at lbmk startup, using data structures that are provided universally by all Python implementations, instead of relying on the output of .Fixed T480 backlight controls, courtesy of a patch from Mate Kukri.Set up Python in  when lbmk starts, to ensure that it is always version 3. This is checked at startup.: Prevent double-nuke, where a given tarball already had vendor files removed prior to release.: Allow setting a MAC address even if vendor files aren’t needed.: Download utils even if  is not set, in case the user is also setting a MAC address.: Honour the  variable, if set by the user, otherwise it is set to  by default.: Don’t do  when running .: Proper DESTDIR/PREFIX handling, whereas it was not handled properly at all before.: Only set CC/CFLAGS if unset, and use sensible defaults.Fixed various shellcheck errors in lbmk.HP EliteBook 820 G2: Fixed vendor file insertion and set . The insertion of Intel MRC and refcode previously didn’t pass checksum validation.ThinkPad T480 / OptiPlex 3050: Force power-off state upon recovery from power loss, otherwise the system always turns on as soon as a charger is plugged in. This is configured by hardcoding, due to a current lack of any option table on the T480.Debian dependencies: replace liblz4-tool with lz4 and liblz4-dev. The latter is also available in Debian Trixie and Sid, at this time, in addition to Debian Bookworm, so it works on all of them.U-Boot (x86): Fixed a bug since Swig 4.3.0 changed the syntax for its language-specific AppendOut functions. A patch from upstream was backported, and the patch is also compatible with older versions of Swig.In lbmk scripts, use  instead of , to find the locations of certain binaries. This is a bug fix, since  is non-standard and so could break on some setups.Crossgcc: when building it for coreboot, fix mismatching GCC/GNAT versions so that they match, if multiple versions are present. This was done because Debain Trixie initially had GCC 14 and GNAT 13, whereas we need GNAT to build the Intel video init code on many mainboards.T480/T480: Disable TPM2 to mitigate a hang in SeaBIOS due to buggy drivers.: Fix the  package, renamed it to , which works on bookworm  newer, but the former did not.: don’t initialise the  variable globally, reset it per target instead, to prevent some repositories from being wrongly re-cloned.Thinkpad T480 / Dell OptiPlex 3050: Handle FSP insertion post-release, rather than providing FSP images directly in release images. It is now handled by the  command, copying the reference image from coreboot and splitting it upp and rebasing it, to mitigate certain technicalities of Intel’s FSP license, which otherwise permits free redistribution.Safer, more reliable exit when handling vendor files, because in some cases lbmk was leaving the  file in place (erroneously).Safer exit when running the  commands, so that lbmk is more likely to exit, because it was theoretically possible that it might not under certain edge cases.Disable nvme hotplug on Dell OptiPlex 3050 Micro, to prevent replugging in Linux, which would otherwise lead to possible data corruption.T480: Fix coreboot SPD size to 512 instead of 256 (it was already auto-corrected to 512 at build time, but the original configs were 256 which is wrong).Add tarballs and gpg signatures to Another bug focus in this release was to clean up the logic of Libreboot’s build system, and fix several bugs, especially those relating to error handling.A lot of cleanup was done on the init functions used by the build system, to initialise common variables, such as environmental variables, and temporary files and/or directories; such logic was moved to a new script called .In descending order from the latest changes to the earliest changes:: simplify : simplify : simplify : simplify : tidy up : simplify general cleanup in  and xbmk: rename / variables (shorten them): consolidate printf statements: remove redundant printf in : remove superfluous command in : simplify : simplify : rename , which actually handles general init tasks, including the processing of vendor files where appropriate.: simplify ccache handling for coreboot; make-oldconfig wasn’t needed at all, when cooking configs to enable ccache, so the  function became much smaller and was then merged with : simplify u-boot payload handling, by using a single variable name that defines the type of U-Boot tree. This allows several other U-Boot-related checks to be greatly simplified, as they were.: add a colon at the end of a  loop: make  easier to understand, by not using shorthand conditional statements in the for loop handling a repository or file download.: merge  with : set pyver from  instead of the main function.: merge  with : only update version files on parent, to speed up xbmk: simplify unknown version creation, where none was created and no Git metadata exists.: only set xbmk version on parent instance; we only need to read what was set, on child instances. In other words, apply the principle of least privelege.: initialise variables AFTER path, to avoid unnecessary work inside child instances of xbmk.: merge  with : Set python version only on parent instances of xbmk, to speed up operation of the xbmk build system.:  to : move  creation to : move PATH init to : shorten the  variable name: simplify : simplify : use  on find command for , so as to remove the need for a more complicated while loop inside said function.: move  to  and only run it on releases; don’t do it on normal xbmk Git. It’s only needed in the former context, because that has to do with distribution by the project, and this makes development easier. Therefore, files are only purged within the release archives, but not during development.: simplify : merge  with : simplify : simplify  by using  for the file loop: simplify : simplify  config check: simplify  by using  for everything, instead of implementing redundant logic in the build system.: reduce indendation in ; simplify the for loop by replacing it with a call to  instead.: simplify : clean up the  after release: removed an unnecessary  command: split up  into smaller functions: remove the unnecessary  function: move  to : move  to : split up  into smaller functions: move  to : remove the  variable: define  here instead: remove : simplify : simplify : split up  into smaller functions: only compile nvmutil if needed: simplified serprog check: tidy up variables: split up  into smaller functions: further cleanup for , such that all vendor-download functions are only defined in ; this means that the Canoeboot version of the file can remain in much closer sync, with fewer differences.: simplified srcdir check on make-clean: split download functions to a new file, : split up the inject functions into smaller functions for each specific task.xbmk: use  instead of , where appropriate, because it handles globbing perfectly these days, and  is cleaner in most cases.: fix outdated info in a comment: use direct comparison for metmp, to speed up checking so many files.: remove unnecessary line break: re-split tree logic to new file, : move release functions to : use  for fail variables: remove useless export; variables that are y/n can just be reset to  if not set to , for simplicity.: export  in  instead of .: simplify : simplified MAC address handling: Simplify : Remove useless command in : rename  and  functions (make the names shorter).: Simplified  and removed ; fe didn’t prefix  to a given command, but fx did. Now, it is prefix manually, for greater control, on commands that need stricter error handling, while it can be avoided on commands where strict error handling is unfeasible.: Create serprog tarballs here instead;  was simplified to use mkhelp when building actual images.build serprog images using , to tidy up xbmk: build serprog images with , rather than implementing a specific for loop.: insanely optimise the me bruteforce, by operating on files recursively via the  function, instead of manually implementing a recursive file search, when bruteforce-extracting  images.: Simplify git am handling by using the new  or  function, instead of making a specific while loop.: remove an unused function: New function  to execute path files; this is used instead of for loops all around xbmk, to simplify operations where the output of a file search is used as argument to a function.: Further simplified FSP extraction: Write sort errors to : Remove warning of empty args; it’s really not required, since it’s obvious anyway in the resulting final error message.xbmk: Replace  with much simpler implementation, for reliability and bug prevention.: simplify : simplify : move  to : simplify extract_intel_me_bruteforce(): Remove unnecessary check: reduce indentation: Move FSP extraction only to , since that’s the only place where it’s needed.: tidy up intel me handling: tidy up the deguard command: single-quote xbmklock in : define lock file in a variable instead; this makes it more flexible, because the path can be checked and then re-used nicely.: tidy up ; make the command style more consistent: rename errx to xmsg: tidy up TBFW handling: remove useless comment block: tidy up the python version check: move non-init functions to : simplify dependencies handling: tidy up : tidy up xgccargs handling: generally removed dead code: tidy up pathdir creation: tidy up : reduce indentation in : Allow use of x_ on prefix functions: tidy up  sha512sum check: simplify : general code cleanup: simplify : simplified fsp extraction: Remove redundant code in copy_tbfwxbmk: Unified local ./tmp handling: redirect find errors to  to prevent clutter on the user’s terminal: unified handling of ./tmp: include rom.sh directly: support multiple arguments in remkdir(): simplify remkdir(): move setvars/err_ to lib.sh: Generally modularised it, moving separate tasks into separate functions, rathher than having it be one big monolith. was renamed to , so that future changes can be in better sync between lbmk and cbmk on this file, because the cbmk version has the MAC address changer (but no vendorfile handling). In the future, this will be split so that  exists again, containing only the vendorfile handling, and  will only handle MAC addresses.: Several variables were moved out of this file and elsewwhere in lbmk.Moved the  function to  instead of Moved the  function from  to .: Use a more top-down function order, more clear, and it was split into an extra file  that does the most basic lbmk initialisation at startup, whereas what remains in  really are generic library functions used throughout lbmk.: Removed unused crossgcc linking feature, because we don’t use it anymore (coreboot trees have their own crossgcc and never link to another these days). Libreboot used to have many more coreboot trees, some of which re-used crossgcc from another tree. Similarly, the accompanying variable  is no longer handled. The  variable is still handled, because projects like U-Boot use that to configure crossgcc.include/vendor.sh: Removed unnecessary check against the ROM image size. Generally simplified the processing of release images.include/git.sh`: Removed many redundant functions, merging several of them.: Fixed a bad print, making proper use of a string inside a printf statement.Simplified many file checks in lbmk, by using the  function.Removed a bunch of useless  commands in general, throughout lbmk, making the code much cleaner.lbmk: the  function is now used much more aggressively, for error handling, simplifying error handling in lbmk overall. main script: Merged the  script with it, so now it’s all one script. The  script is now the only executable script in lbmk. (main script): The  command is removed (legacy / obsolete).The version/versiondate files are now dotfiles, to hide during operation.: Hardcoded projectname/projectsite variables, instead of storing them in a file. script: Unified handling of flags (same string used in error output), to ensure that error(usage) messages always match. script (later merged into ): Removed a lot of old bloat.: Make the checksum word position a define. Generally cleaned up a lot of code to make it clearer for the reader. Added more verbose messages to the user, confirming things such as how much was read or written on the user’s file system. Various miscallaneous bug fixes (edge cases that were unlikely to ever be triggered).: More efficient use of memory when handling files.: Much cleaner handling of user input.`util/nvmutil: More granular MAC address parsing errors, easy for debugging.: Make the Gbe Checksum a define, for readibility.: Obey the 79-character-per-line limit, as per lbmk coding style.: Tidied up several pledge callsRemoved use of several unnecessary subshells and  statements in lbmk.: Later, the GCC/GNAT matching feature was rewritten to work both ways, where an older GCC was matched to GNAT and vice versa, whereas it previously only went one way.  and  are manipulated in  to ensure that the user has a consistent version of both. later merged into the  script (which later merged into the main  script). This  is what contained the first implementation of the GNAT/GCC version matching feature.: Remove unnecessary shebang, and the same on other  scripts. NOTE:  was later merged into , which then became split into  in later changes (see above).Removed legacy build system commands e.g.  and ; now only the newer  commands are supported. This and the change below was briefly reverted, for the 20241206 revisions, but then re-introduced.Removed the deprecated  command; now only  commands are used. The  commands are used, for downloading vendor files.Removed unused patch that was for the original deguard implementation, prior to Mate Kukri’s re-write of it.This log shows all changes in today’s release, from 30 June 2025, ever since the Libreboot 20241206 release of 6 December 2025:* c46a71138c7 Libreboot 25.06 release 
* b1ef562b767 tree.sh: add sha512 error for check_project_hashes 
* 04bee3834d0 tree.sh: add error check in check_project_hashes() 
* 677dfc4d103 tree.sh: more reliable clean in run_make_command 
* 267d4c90341 inject.sh: add missing semicolons 
* 974bdbb3815 vendor.sh: fix bad cbfstool path 
* dc6996252a0 put coreboot utils in elf/coreboot/TREE 
* b77154640de release.sh: use printf to create version files 
* dee6997d0cc lib.sh: simplify setvars() 
* 79ded40f3d0 lib.sh: simplify chkvars() 
* 5036a0bc501 mk: simplify main() 
* 41308ee9244 get.sh: simplify fetch_project() 
* b5867be214d get.sh: simplify try_copy() 
* 495098d6a71 get.sh: tidy up bad_checksum() 
* 671e3aa27b4 get.sh: simplify fetch_targets() 
* 09b6e91803d general cleanup in get.sh and vendor.sh 
* 18dacd4c22b xbmk: rename xbmklocal/xbmktmp variables 
* e981132c829 get.sh: consolidate printf statements 
* afc36754b13 get.sh: remove redundant printf in fetch_project 
* ffe387ac6b9 get.sh: remove superfluous command in try_git() 
* ba7c49c090b vendor.sh: simplify fetch() 
* 30bc3732c39 init.sh: error out if .git/ is a symlink 
* 2493203ee53 get.sh: Properly error out if tmpclone fails 
* ad333ae2481 tree.sh: Don't auto-run make-oldconfig 
* 97ce531c341 rom.sh: simplify mkcoreboottar() 
* a47e9811723 rom.sh: rename mkvendorfiles 
* d2e148fdd9d rom.sh: simplify ccache handling for coreboot 
* 8c3f10ba402 rom.sh: simplify u-boot payload handling 
* 3e28873532b ifd/hp8300usdt: set the HAP bit by default 
* 452aeb6001a coreboot: Remove unused vboot tests 
* 64cc91bca33 coreboot/default: Remove unneeded FSP modules 
* 0216a3104a5 get.sh: Always update git remotes 
* 419733d3073 get.sh: re-generate remotes every time 
* 231b320e63b release.sh: copy version files to rsrc 
* fc0720184d9 xbmk: add fake config makefile args to flashprog 
* f9266601b8c vendor.sh: add colon at the end of a for loop 
* 8e0c6059d15 rom.sh: skip copyps1bios on dry builds 
* a3250d14474 tree.sh: Don't run make-clean on dry runs 
* 24b8e633e03 GRUB: Update to revision 73d1c959e (14 March 2025) 
* f6b77822835 Revert "vendor.sh: optimise find_me()" 
* fb7aaa78bb0 vendor.sh: optimise find_me() 
* 903f78bf080 get.sh: add missing check in fetch_project() 
* f15bb8153a3 get.sh: stricter URL check in xbmkget() 
* cdc0fb49e1c get.sh: make xbmkget() easier to understand 
* 620c1dd6fae get.sh: Make xbmkget err on exiting the loop check 
* 900da04efa9 tree.sh: fix up copy_elf(), bad for loop 
* 8aaf404ddea lib.sh: Use while, not for, to process arguments 
* d9c64b26754 xbmk: stricter handling of files on while loops 
* b25a4876434 init.sh: looser XBMK_THREADS validation 
* 769a97aed5a init.sh: Hardcode XBMK_CACHE for integrity 
* 265ec0b7673 dependencies/debian: add libx86 
* 2702a43a86d init.sh: merge xbmk_lock() with xbmk_set_env() 
* fc4006ce877 init.sh: move xbmk_set_version 
* 962902a1c4a init.sh: set pyver from set_env 
* 158c56072c0 init.sh: merge xbmk_mkdirs with set_env 
* 5f022acbf47 init.sh: check version/versiondate once read 
* 485a60e2f6a init.sh: error if version not read 
* 99f09f25ef3 init.sh: only update version files on parent 
* 94437278dc7 init.sh: simplify unknown version creation 
* 6b603b9fbf4 init.sh: only set xbmk version on parent instance 
* ac36ea7f950 init.sh: initialise variables AFTER path 
* 484afcb9196 init.sh: merge create_pathdirs with set_pyver 
* d0bee6b4ebb init.sh: Set python version only on parent 
* 4aa69a7d1f0 init.sh: remove useless command 
* 36ffe6ef501 init.sh: remove useless comment 
* 0343081d905 init.sh: xbmk_create_tmpdir to xbmk_mkdirs 
* c75bc0449d0 init.sh: move gnupath creation to create_tmpdir 
* 253aa81a3f9 init.sh: move PATH init to set_env 
* e05a18d3513 init.sh: check the lock file BEFORE git init 
* cde3b7051e4 init.sh: return from child in set_env instead 
* 7ec9ee42283 inject.sh: shorten the nukemode variable name 
* b48eb161e49 vendor.sh: simplify mksha512sum() 
* ac609d5aae4 vendor.sh: Remove _dest if it's bad 
* a3e1ed9823d release.sh: rename relsrc to rsrc 
* 44df3b2bff8 release.sh: tidy up nuke() 
* 3c58181f69e get.sh: remove useless message 
* 01a0217c1e3 get.sh: simplify bad_checksum() 
* 4ca57943d70 release.sh: simplify nuke() EVEN MORE, yet again 
* 47a3982bbea release.sh: use x_ on find command for nuke() 
* 6dc71cc0246 release.sh: simplify nuke() EVEN MORE 
* 05c07f7401b get.sh: move nuke() to release.sh 
* 587d245cafa release.sh: simplify prep_release_bin() 
* 136bd66c280 mrc.sh: merge extract_mrc with extract_shellball 
* dbe109d7b54 release.sh: don't move src/docs/ 
* 840d6a1d277 get.sh: FURTHER simplify nuke() 
* d2564fd9457 get.sh: simplify tmpclone() 
* 6dea381614d get.sh: fix bad mkdir command 
* 6a2ed9428b7 vendor.sh: Fix broken KBC1126 insertion 
* 4313b474a59 vendor.sh: additional safety check 
* d668f3a3529 vendor.sh: Properly verify SHA512SUM on extraction 
* a191d22bd6d get.sh: add missing eval to dx_ in nuke() 
* c8813c9a144 properly exit 1 when calling fx_ 
* 208dfc89bd5 get.sh: simplify nuke() 
* 46f42291d3c get.sh: fix broken printf statement 
* f29aa9c8d59 get.sh: use subshells on try_ functions 
* e62886dedae get.sh: simplify try_copy() 
* d9ed03f9ea5 get.sh submodules: Don't delete files recursively 
* 8d5475ed5b5 get.sh: simplify fetch_submodules() config check 
* 21867b7d805 get.sh: simplify fetch_submodules() 
* e9fe5a74a2e get.sh: fix caching of crossgcc tarballs 
* 6089716f07c release.sh: Don't run prep_release with fx_ 
* b04c86e5740 git.sh: rename to get.sh 
* 3c23ff4fa18 git.sh: Only create destination repo on success 
* ed8a33d6fb1 git.sh: cleanup 
* 1ca26c5d238 git.sh: Re-implement redundant git downloads 
* e38805a9448 rom.sh: reduce indendation in check_coreboot_utils 
* 6bf24221e60 release.sh: simplify release() 
* 66f7ecdb2d7 release.sh: clean up the vdir after release 
* d4c0479093a release.sh: remove src_dirname variable 
* 6d3a6347c3e release.sh: build in tmp directory first 
* a0105e1ab44 release.sh: remove unnecessary mkdir command 
* f4871da9bca release.sh: split up build_release() 
* c85aff5c54e release.sh: delete tmp/cache from the tarball 
* 92954eeb38f lib.sh: remove rmgit() 
* 05b5914b354 lib.sh: remove mk() 
* c9696e23338 lib.sh: move xbmkget() to git.sh 
* 23913bb8d2a lib.sh: move mksha512sum() to vendor.sh 
* 80f0562e8d1 lib.sh: split up try_file() 
* 89cd828e87c lib.sh: move _ua to try_file() 
* 308a9ab1e17 mrc.sh: minor cleanup 
* 40163dcfa4e mrc.sh: update copyright year to include 2025 
* ef800b652c8 inject.sh: remove the hashfiles variable 
* 311ae2f8df2 inject.sh: define xchanged here instead 
* 76f81697e6e vendor.sh: remove check_vcfg() 
* 97d4d020d97 vendor.sh: simplify getvfile() 
* 57f896ac016 vendor.sh: simplify setvfile() 
* 3879f6c4d8f lib.sh: use fx_ in rmgit() 
* 0911a5a5aed lib.sh: split up xbmkget() 
* a449afb287f inject.sh: only compile nvmutil if needed 
* 2bbf2ae80b7 inject.sh: simplified serprog check 
* 9c27b7437cf vendor.sh: tidy up variables 
* 0cc816167bb vendor.sh: split up setvfile() 
* 7d90d434252 remove another confusing message 
* a0c436ad4ba inject.sh: Remove confusing path on tar creation 
* dcfd3e632e2 inject.sh: re-add mac address confirmation 
* e5af201060e inject.sh: further cleanup for vendor.sh 
* 0aa99f4bf8b tree.sh: only create elfdir in copy_elf() 
* a8e374020c0 tree.sh: simplified srcdir check on make-clean 
* 0f931b508a8 inject.sh: split to vendor.sh the download parts 
* 3554b5aad9c inject.sh: split up the inject functions 
* 81dbde7e09f lbmk: use x_ instead of err, where appropriate 
* 14d46abceda mrc.sh: operate on refcode in tmp area first 
* 6e521c2e1ea mrc.sh: fix outdated info in the comment 
* 23486abef3a inject.sh: use direct comparison for metmp 
* 91220ce1833 inject.sh: use subshell to speed up find_me() 
* ff33ec3352b mk: use zero exit instead, to run trees 
* c2b627dc6d0 remove useless comment 
* 066402b7e7a mk: remove unnecessary line break 
* 7012c00ed11 mk: re-split tree logic to include/tree.sh 
* 50ce1ac9b22 mk: move release functions to idnclude/release.sh 
* 1ce3e7a3d39 mk: add missing error handli for mk -f 
* 0d876622fcb git.sh: re-write tmpclone without caching 
* 454f11bdd7b git.sh: use setvars for fail variables 
* 6bdb15fd329 git.sh: hard fail if git am fails 
* 93d4eca04ae git.sh: Hard fail if reset fails 
* a3ba8acface init.sh: Only check XBMK_CACHE if it exists 
* 021e7615c84 HP 820 G2: Use fam15h cbfstool tree for refcode 
* fe926052441 also fix the other grub trees 
*   a8594762d27 Merge pull request 'fix trying to boot all logical volumes after unlocking an encrypted volume' (#330) from cqst/lbmk:master into master 
|\  
| * e084b06dc76 fix trying to boot all logical volumes after unlocking an encrypted volume 
|/  
* 2cea8517f3b init.sh: remove useless export 
* 1b0afdcea22 init.sh: also allow XBMK_RELEASE=Y or N 
* 570f1417a80 init.sh: Resolve XBMK_CACHE via readlink 
* e1af1055ed1 init.sh: check XBMK_CACHE is a directory instead 
* e1628ad8f3e init.sh: export LOCALVERSION in set_env 
* 40a944118f2 init.sh: run set_version before set_env 
* cba04aa74b8 init.sh: Use readlink in pybin() 
* a94bd3c0939 inject.sh: simplify extract_kbc1126ec() 
* e3098c61f43 inject.sh: simplified MAC address handling 
* d530e68594d inject.sh: Simplify patch_release_roms() 
* 7f71328f0e2 lib.sh: Remove useless command in err() 
* 394b4ea7a59 inject.sh: rename copytb and preprom functions 
* ec5c954337b lib.sh: Simplified fx_() and removed fe_() 
* 1390f7f8007 mk: Create serprog tarballs here instead 
* 0ef77e65832 build serprog using fe_ *defined inside mkhelper* 
* d2e6f989d7e rom.sh: build serprog images with fe_ 
* 0faef899469 lib.sh: support any command on find_exec() 
* 2b7f6b7d7ce inject.sh: Simplify extract_intel_me_bruteforce() 
* 485d785d331 inject.sh: clean up tmp me file before extract 
* fac99aa2d44 lib.sh: re-add missing break in fe/fx_ 
* 03300766d14 inject.sh: tidy up extract_intel_me_bruteforce 
* 4781dbd2a05 inject.sh: fix oversight in me bruteforce 
* cf78583a6d8 inject.sh: remove unnecessary check 
* 5657cc1afb3 inject.sh: don't use subshell for me bruteforce 
* 5686f35e0f1 inject.sh: insanely optimise the me bruteforce 
* e8be3fd1d41 git.sh: Simplify git am handling 
* 4c1de1ad126 inject.sh: remove unused function 
* 282b939d9da init.sh: New function dx_ to execute path files 
* 73074dedee3 inject.sh: Further simplified FSP extraction 
* 7585336b914 inject.sh: simplify kconfig scanning 
* ef38333f8b0 lib.sh find_ex: Write sort errors to /dev/null 
* c275f35e7e2 lib.sh x_(): Remove warning of empty args 
* 17d826d3a96 lbmk: Replace err with much simpler implementation 
* f98e34a24dd singletree/elfcheck: use fx_, not fe_ 
* 8ca06463ebc rom.sh: Print the rom image path being generated 
* dc9fe517cb0 rom.sh: Safer cprom() 
* 2be8d1c7982 rom.sh: specifically check keymaps in cprom() 
* 89a8cd4936a rom.sh: simplify mkseagrub() 
* c2182d82193 mk: simplify elfcheck() 
* 437ac2454c1 lib.sh: simplify singletree() 
* 62ec3dac075 git.sh: move singletree() to lib.sh 
* 6b247c93e25 mk: Fix bad error handling for gnu_setver 
* ee8bb28ba21 GRUB: Mark E820 reserved on coreboot memory 
* 61ec396ef6d inject.sh: simplify extract_intel_me_bruteforce() 
* e4edc2194d3 inject.sh: Remove unnecessary check 
* f4057d7daab inject.sh extract_intel_me(): reduce indentation 
* b7ca59debe6 inject.sh: Move FSP extraction only to extract_fsp 
* eb882de94cb inject.sh: tidy up intel me handling 
* 153dd76a82e inject.sh: tidy up the deguard command 
* 428c46ca2b1 lib.sh: set -u -e in err() 
* 20c87308587 lib.sh: Provide error message where none is given 
* 35265731c5b init.sh: Silence the output of git config --global 
* 5e3aaa1eb8b init.sh: Run git name/email check before init 
* a3b5626f53d lib.sh: stricter xbmk_err check in err() 
* 51b2a1159d0 lib.sh: simplify err-not-set handling 
* 61e5fd1a0b2 lib.sh: Add warning if x_ is called without args 
* 4020fb43280 lib.sh: simplify err() 
* b51846da6de init.sh: single-quote xbmklock in xbmk_lock() 
* 8b7bd992f66 init.sh: define lock file in a variable instead 
* 9611c19e7ed init.sh: tidy up xbmk_child_exec() 
* 37ca0c90e1c lib.sh err: add missing redirect to stderr 
* 54291ebb720 lbmk: MUCH safer err function 
* 3f7dc2a55f5 lib.sh: rename errx to xmsg 
* 59c94664e3e lib.sh: Make x_ err if first arg is empty 
* 91bb6cbede0 lib.sh: Make err_ always exit no matter what 
* b19c4f8f674 inject.sh: tidy up TBFW handling 
* 439020fbda5 inject.sh: remove useless comment block 
* 6e447876cca init.sh: tidy up the python version check 
* 7392f6fc8ec init.sh: move non-init functions to lib.sh 
* 7acec7a3a1d init.sh: simplify dependencies handling 
* 93ba36ae456 rom.sh: tidy up copyps1bios() 
* fc71e52fdfc mk: tidy up xgccargs handling 
* 184871bc17c mk: remove useless code 
* b6a2dc4ea3c init.sh: tidy up pathdir creation 
* f5b2bdb8868 mk: re-make gnupath/ after handling crossgcc 
* 1b7a9fd637d mk: tidy up check_cross_compiler 
* 488d52e784f mk: re-make gnupath/ for each cross compiler 
* c33467df1e6 mk: reduce indentation in check_cross_compiler() 
* aa4083443b1 mk: Allow use of x_ on prefix functions 
* 8f828e6cd35 mk: tidy up check_project_hashes() sha512sum check 
* 7a2f33264d7 mk: simplify check_gnu_path() 
* 46b968a6e85 inject.sh: minor code cleanup 
* 5499ae66bd8 inject.sh: simplify extract_archive() 
* 72f4412a52d inject.sh: simplified fsp extraction 
* bf569d2b4dc inject.sh: Remove redundant code in copy_tbfw 
* 8de0ed811fb inject.sh: Stricter TBFW handling 
* 530e4109a2b init.sh: *Re-create* tmpdirs on parent instance 
* 498f5a26cc8 init.sh: Always create xbmklocal 
* 00d22f20829 lbmk: Unified local ./tmp handling 
* 0f7b3691aba lib.sh: redirect find errors to /dev/null 
* 7fadb17fd9e lib.sh: Fix bad touch command 
* 0b09d970732 inject.sh: Only build nvmutil once 
* 308df9ca406 inject.sh: always re-build nvmutil 
* 44a1cc9ef85 util/nvmutil:  use x, not ?, for random characters 
* a17875c3459 lib.sh find_ex: explicitly create the tmp file 
* 0ffaf5c7331 init.sh: Explicitly create the xbmktmp directory 
* fcc52b986e7 init.sh: unified handling of ./tmp 
* 47762c84ad0 lib.sh: add fe_ which is fx_ but err on find 
* d18d1c2cae2 lbmk: unified execution on find commands 
* 773d2deaca0 NEW MAINBOARD: Dell Precision T1700 SFF and MT 
* 9b11e93686c mk: include rom.sh directly 
* 1f7e4b35cb2 mk: Download vendorfiles before building release 
* acb0ea202f2 lib.sh: Simplify rmgit() 
* 15b76bc202f lib.sh: support multiple arguments in remkdir() 
* f3ae3dbbbe4 lib.sh: simplify remkdir() 
* 6c4d88f2686 move x_() to lib.sh 
* 2ae565ba93a init.sh: move setvars/err_ to lib.sh 
* c073ee9d4fc Restore SeaBIOS 9029a010 update, but with AHCI fix 
* 8245f0b3211 Revert "seabios: bump to rev 9029a010, 4 March 2025" 
* 4c50157234d coreboot/t420_8mb: add missing txtmode config 
* f21749da8b1 Libreboot 25.04 Corny Calamity 
* bb5f5cd5763 add pico-sdk backport patch fixing gcc 14.x 
* 4f77125066d coreboot/fam15h: update submodule for nasm 
* 0f2202554ab coreboot/fam15h: update nasm to 2.16.03 
* 2009c26f0aa serprog: Remove pico2 support for the time being 
* a08b8d94fc5 seabios: bump to rev 9029a010, 4 March 2025 
* 342eca6f3d1 update untitled 
* b0a6d4711a3 coreboot413: add alper's fix to cbfstool for gcc15 
* 628ae867c9a flashprog: bump to rev e060018 (1 March 2025) 
* 5e96db5a2b4 further gcc-15 fix for gmp on -std=23 
* 9a9cd26b2d5 coreboot/default and fam15h: gmp fix, gcc15 hostcc 
* 80007223c85 lib.sh: Provide printf for mktarball 
*   a16c483e5fd Merge pull request 'coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler' (#318) from alpernebbi/lbmk:coreboot-fam15h-gcc15 into master 
|\  
| * 685685ab0e4 coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler 
|/  
*   02110f2bc1d Merge pull request 'coreboot: Add patch to fix build with GCC 15 as host compiler' (#317) from alpernebbi/lbmk:coreboot-gcc15-nonstring into master 
|\  
| * 5ad1de3931a coreboot: Add patch to fix build with GCC 15 as host compiler 
|/  
*   9e7bceb7fa9 Merge pull request 'seabios: Fix malloc_fn function pointer in romfile patch' (#313) from alpernebbi/lbmk:seabios-romfile-malloc-fptr into master 
|\  
| * 35c853f8b33 seabios: Fix malloc_fn function pointer in romfile patch 
* |   686e136f150 Merge pull request 'dependencies/debian: Fix libusb package name' (#315) from alpernebbi/lbmk:debian-libusb-dependency into master 
|\ \  
| * | 6f120f01588 dependencies/debian: Fix libusb package name 
| |/  
* / d8b0e749983 init.sh: fix yet another double quote for dotfiles 
|/  
*   780844112ae Merge pull request 'Update U-Boot to v2025.10' (#305) from alpernebbi/lbmk:uboot-v2025.04 into master 
|\  
| * 1265927ca38 u-boot: gru: Disable INIT_SP_RELATIVE 
| * 5bea1fade9a u-boot: arm64: Expand our modified defconfigs to full configs 
| * fd56d8ada13 u-boot: arm64: Merge our modifications into new defconfigs 
| * ed9ddd7415f u-boot: arm64: Add new upstream defconfigs 
| * b1fa44858cb u-boot: arm64: Rebase to v2025.04 
| * 976fc6890ae u-boot: arm64: Save our modifications to the upstream defconfigs 
| * 418570a6172 u-boot: arm64: Turn configs into defconfigs 
|/  
* 093a86d9c09 init.sh: don't use eval to read version files 
* 3045079947b init.sh: use backslash for dotfiles in eval 
* da108d1c045 mk: Don't run mkhelpers if mode is set 
* 71a58a38ab4 mk: condense main() again 
* f3882b9bf21 init.sh: make git name/email error more useful 
* 9cebda333d5 init.sh: move git name/mail check to xbmk_git_init 
* ea081adc4ca init.sh: tidy up the git name/email check 
* 3292bded692 mk: make main() more readable 
* 97a5e3d15ed mk: move git check to init.sh xbmk_set_version 
* 11cd952060d init.sh: tidy up xbmk_init() 
* f6c5c8d396d mk: move git_init to init.sh 
* ec1c92238cc init.sh: minor cleanup 
* e009f09e7fa init.sh: clean up setvars 
* 9ec72153408 init.sh setvars: make err a printf for eval 
* 18ad654a1f7 init.sh: merge xbmk_child_init with xbmk_init 
* 15268202478 init.sh: split xbmk_child_init into functions 
* 0280cd4c0e7 init.sh: move parent fork to new function 
* a0e1d42ff74 init.sh: Provide more complete error info 
* a8f0623efbb update uefitool to rev a072527, 26 Apr 2025 
* c698972130f rename include/vendor.sh to inject.sh 
* 24e488aae56 lib.sh: move _ua to the xbmkget function 
* 6779d3f9915 move variables out of init.sh to others 
* 848159fa0eb lib.sh: rename vendor_checksum 
* 1de77c6558c lib.sh: move singletree() to git.sh 
* 703fe444312 lib.sh: move cbfs() to rom.sh 
* b57952e90d2 re-split include/init.sh to lib.sh 
* 8ecb62c6628 rename include/lib.sh to init.sh 
* ce4381169fa lib.sh: introduce more top-down function order 
* 15b64cfebe8 mk/git.sh: remove tree_depend variable 
* 9b8179c0e5d git.sh: remove unused xgcc linking feature 
* 4624c6e536c mk: remove unused variables (ser/xp) 
* aba5b3a3532 mk: simplify main() 
* 0ab7c6ff9cf lib.sh: use realpath to get sys python on venv 
* 8edea026c58 lib.sh: Force use of System Python to prevent hang 
* b1b964fa5c3 lib.sh: further condense the python check 
* 9543a325acb lib.sh: further simplify the python check 
* 9baabed7186 lib.sh: condense the python check 
* 0c5c5ffc873 lib.sh: simplify mk() 
* 83022b6ba83 lib.sh: simplify cbfs() 
* 13ad839691d lib.sh: simplify the python check 
* b1ea4165754 mk: remove mkhelp() and use x_() instead 
* 4cf64e59ed0 mk: simplify handling of trees() 
* d0581914c74 coreboot/hp8300cmt: purge xhci_overcurrent_mapping 
* cb52fc4ba82 Fix VBT path on HP Elite desktops 
* 2bee87cfc26 lib.sh: add missing copyright year 
* 4b7ab403c65 ifd/q45t_am: unlock regions by default 
* 564155277ea coreboot/g43t_am3: use ifd-based setup 
* 0ddd1963751 coreboot/q45t_am3: use ifd-based setup 
* 3b2d933842a coreboot/default: add missing submodules 
* a10d81399c7 NEW MAINBOARD: Acer Q45T-AM (G43T-AM3 variant) 
* d114e0a765c mk: don't print confirmation of git pkg.cfg 
* f59c24f12aa coreboot/g43t_am3: fix data.vbt path 
* 21020fa319a add missing config/data/coreboot/0 
*   2b4629d790b Merge pull request 'lib.sh: Fix python3 detection when 'python' is python2' (#290) from alpernebbi/lbmk:python3-detection-fix into master 
|\  
| * a18d287a81e lib.sh: Fix python3 detection when 'python' is python2 
|/  
* c7569a67145 coreboot/next: merge with coreboot/default 
* 762c7ff43eb coreboot/default: Update, c247f62749b (8 Feb 2025) 
* 86e7aa80c51 Update the GRUB revisions 
* 8d57bf6009e Revert "git.sh: minor cleanup" 
* a2898771f6e lib.sh: perform root check even earlier 
* 779f6003421 lib.sh: tidy up opening logic (put it together) 
* bac4be99c20 lib.sh: do root check before python check 
* e63d8dd20d9 git.sh: minor cleanup 
* 11078508a25 lib.sh: simplify mktarball() 
* 087bbedc5f8 vendor.sh: tidy up vendor_download() 
* e11fd52d958 mk: tidy up check_gnu_path() 
* 3442f4278ed mk: simplify check_project_hashes() 
* 6b6a0fa607c lib.sh: fix missing s/TMPDIR/xbmktmp 
* e07a2adb130 lbmk: don't handle TMPDIR directly 
* 9d3b52cd1d2 rom.sh: minor cleanup 
* b4402c54258 vendor.sh: yet even more code cleanup 
* fe5bdc7633d vendor.sh: even more cleanup 
* fcedb17a9a1 vendor.sh: more cleanup 
* 4e2b59ed3ff vendor.sh: minor cleanup 
* a3acf4c3f95 vendor.sh: simplify process_release_roms 
* 30213a96883 vendor.sh: remove unnecessary check 
* 38df7275f12 git.sh: remove unnecessary comment 
* f5891fb6991 git.sh: remove link_crossgcc() 
* a685654b90f git.sh: remove move_repo() 
* e4aa62f79a8 git.sh: remove prep_submodule() 
* 2839feb9e43 git.sh: make git_prep command clearer 
* 410fa702c9c mrc.sh: Make proper use of variable inside printf 
* 075902c3ea7 simplify a few file checks 
* b2255425eba rom.sh: remove unnecessary check 
* 39640d76a75 lbmk: minor cleanup 
* c8dc701f3eb lib.sh mktarball: stricter tar error handling 
* 58a53d7046f vendor.sh: don't err on bruteforce me extract 
* 958fa34832a mk check_project_hashes: handle error on sha512sum 
* 8b4b069e3f6 vendor.sh: remove unnecessary xchanged="y" 
* 166dbb04c92 vendor.sh: set need_files="n" if skipping patch 
* e90657cc734 vendor.sh: Don't handle vendor files if not needed 
* 2e10a45fa36 Revert "lib.sh: use eval for the command in x_" 
* 738d4bb6b6d lib.sh: fix bad eval writing resized file 
* eb9e5d2d5d4 lib.sh: fix bad eval writing version/versiondate 
* 3bfdecdc75b lib.sh: use eval for the command in x_ 
* 4fa3bb9e5b1 mk: use eval to run mkhelp commands 
* 9b3635718a8 mk: tidy up the switch/case block in main() 
* 0c381028abc mk: tidier error handling 
* 023f9cf0498 lib.sh: tidy up the error handling 
* cb3253befb9 rom.sh: tidy up error handling 
* 7af46721bcb vendor.sh: tidy up error handling 
* 04ebb3b91a0 vendor.sh: tidy up decat_fspfd() 
* 0c87fdf96ad git.sh: clean up fetch_project() 
* 9eb8856b3c5 mk: Remove unnecessary argument checks on trees() 
* 52f3d54116f vendor.sh: properly call err_ in fail_inject 
* c4c6692b761 remove xbmk_parent, handle forking in lib.sh 
* fd5431db05d lib.sh: define x_ right after err_ 
* 972681a127b mk: minor cleanup 
* b41cd39b686 lib.sh: minor cleanup 
* 49939502648 mrc.sh: minor cleanup 
* c158d82298b rom.sh: minor cleanup 
* cb36248c8c0 vendor.sh: tidy up check_release() 
* 409cab39c56 vendor.sh: tidy up vendor_inject() 
* 12b1623e473 vendor.sh: tidy up readcfg() 
* 0d85f061e2e vendor.sh: tidy up patch_release_roms() 
* 61f20141028 vendor.sh: tidy up process_release_roms() 
* 5901f36e49d vendor.sh: tidy up patch_rom() 
* 082930ce0e7 vendor.sh: tidy up inject() 
* e1f91f30372 vendor.sh: tidy up modify_mac_addresses() 
* 3181ac50126 script/trees: merge with mk and delete script/ 
* 3d03dd1a507 mk: remove the legacy "roms" command 
* f0c629dcc6c lib.sh: write version/versiondate to dotfiles 
* 23b942c83e9 lib.sh: hardcode projectname/projectsite 
* a03bb793aea remove update/vendor symlinks 
* d7f80ebe71e move build to mk 
* 57d58527fd0 trees: unify the execution of mkhelper commands 
* e5262da4be7 trees: tidy up configure_project() 
* 51798278397 build: make coreboot building an else in "roms" 
* c189257888a trees: don't build dependencies if dry=":" 
* 115a66fddd3 trees: unified handling of flags 
* 3ea633cc791 trees: simplified handling of badhash/do_make 
* 9be40e94a2b trees: don't set mode on ./mk -b 
* 67ad7c2635c trees: don't set mod on ./mk -d 
* 24448948419 trees: don't initialise mode to "all" 
* 97c50a39a60 trees: clean up some comments 
* cfb14fd8dd8 vendor.sh: simplified readkconfig() 
* 5b697b93a2d lib.sh: double-quote pwd to prevent globbing 
* 5a0a24f5559 lbmk: unified PWD handling (work directory) 
* a25a29cfbb7 lib.sh: initialise PATH if it's unset 
* 1022abf6991 move XBMKPATH to include/lib.sh 
* 0764c969a29 lbmk: use pwd util, not PWD environmental variable 
* f98b9b01107 clean up a few semicolons in the build system 
* 8ccb61cc718 trees: err if first argument is not a flag 
* 947c3e1a176 trees: err if no argument given 
* edbbde0b12d trees: set dry=":" on ./mk -f 
* 33bb0ecf764 trees: clean up initialisation of the dry variable 
* c7636ff1dfc trees: initialise mode to "all", not "" 
* d0bd12631a6 trees: don't abuse the mode variable on -f 
* c4cd876c609 trees: Add missing flag to error output 
* 5ebcae5235f lbmk: minor code formatting cleanup 
* 70cef71dbab grub/xhci: Remove unused patch 
* 3f14a470a2e remove _fsp targets (keep _vfsp) 
* d7312260e7e util/nvmutil: remove excessive comments 
* e348ea0381a Bump GRUB revision to add 73 security patches 
*   4b228c11f9f Merge pull request 'Update pico-serprog revision' (#271) from Riku_V/lbmk:master into master 
|\  
| * a8359e30b27 Update pico-serprog revision 
|/  
* d2cb954933b util/nvmutil: Fix bad error messages on R/W 
* e1e515bd22a util/nvmutil: hardened pledge on help output 
*   ada057a865c Merge pull request 'Simplify the README' (#269) from runxiyu/lbmk:readme-simplification into master 
|\  
| * 9ced146b47c README.html: Use newlines instead of bulleted list for docs/support links 
| * 266122592cd README.html: Use the EFF's page on Right to Repair 
| * e36aa8c5a5c README.html: Vastly simplify it 
| * c17f4381ce5 README.html: Mention SeaBIOS and U-Boot instead of Tianocore as payloads 
|/  
*   47eb049cb47 Merge pull request 'deps/arch: genisoimage belongs to cdrtools' (#267) from runxiyu/lbmk:master into master 
|\  
| * fa9a0df2458 deps/arch: genisoimage belongs to cdrtools 
|/  
* a98490573be util/nvmutil: only set mac_updated at the end 
* 6b9cf09ca21 restore old x230 gbe file 
* 8a435355135 util/nvmutil: Fix bad comparison 
* a65a0c2f963 util/nvmutil: allow ./nvm gbe MAC 
* 96356ce94f6 util/nvmutil: move "e" to swap() 
* b1d8975959d util/nvmutil: Only read up to 4KB on larger gbe 
* 6821659bcb2 util/nvmutil: fix minor mistake (line break) 
* 3bb7520f6d9 util/nvmutil: do setmac if only filename given 
* d94b274fd9f vendor.sh: don't error if grep -v fails 
* 6ebdd3c72ba vendor.sh: Don't show gbe filename on inject 
* a08748a9eda util/nvmutil: don't say write not needed if errno 
* 6841a351ebc util/nvmutil: print dump *after* modification 
* da0a6c216cf util/nvmutil: verbosely print the written MAC 
* db5879c6b5a util/nvmutil: minor cleanup in cmd_dump 
* bd7215d1eb7 util/nvmutil: show nvm words written on writeGbe 
* c70117c79c4 util/nvmutil: clean up readonly check on writeGbe 
* cf5a63e65ca util/nvmutil: Remove useless gbeFileChanged var 
* 83601aa524b util/nvmutil: reset errno if any MAC updated 
* 3e86bf5ce25 util/nvmutil: reset errno when writing a MAC 
* bcf53cc2cc0 util/nvmutil: show total number of bytes read 
* c91cc329cf8 util/nvmutil: rename tbw/bw to tnw/nw 
* 90607108330 util/nvmutil: err if bytes read lower than nf 
* c72f699d368 util/nvmutil: err if fewer bytes written 
* d666f67ebe5 util/nvmutil: Show bytes written in writeGbe 
* b2d6393ed5f util/nvmutil swap(): ensure that no overflow occurs 
* 063fef14d34 util/nvmutil: make swap() a bit clearer 
* fd1bbdc96cb util/nvmutil: make 0x3f checksum position a define 
* 5ddf7f251d6 util/nvmutil: make 128 (nvm area) a define 
* 8850acc7da6 util/nvmutil swap(): Only handle the nvm area 
* 49506a88328 util/nvmutil: move write checks to writeGbe 
* 948377b0e7e util/nvmutil: make cmd_swap its own function again 
* 6e134c9f4bf util/nvmutil: minor cleanup 
* 98e105ac4f1 util/nvmutil: allocate less memory for setchecksum 
* 52e8ea57f7b util/nvmutil: Further reduce memory usage 
* 7a7d356824e util/nvmutil: Remove unnecessary buf16 variable 
* cdf23975bc1 util/nvmutil: Only allocate needed memory for file 
* ed45da9cae5 util/nvmutil: Remove unnecessary buffer 
* ec3148dc3b5 util/nvmutil: Show specific error for bad cmd argc 
* 073420d3056 util/nvmutil: cleaner argument handling 
* a6c18734e70 util/nvmutil: extreme pledge/unveil hardening 
* deb307eaf63 util/nvmutil: more minor cleanup 
* c14eccaf153 util/nvmutil: more granular MAC parsing errors 
* 88fb9cc90ea util/nvmutil: more cleanup 
* 5aaf27f80c3 remove errant comment in nvmutil 
* c829b45c17c util/nvmutil: support 16kb and 128kb gbe files 
* a98ca5bf65c util/nvmutil: Prevent unveil allowing dir access 
* 68c32034a00 typo: nvme should say nvm in nvmutil.c 
* c944c2bbac7 util/nvmutil: General code cleanup 
* 8c65e64e398 snip 
* f666652fe15 snip 
* 64d3c7b5150 grub/xhci: Add xHCI non-root-hub fixes from Nitrokey 
* 7bf0d4c2ed5 add gnults-devel to fedora 41 dependencies 
* 66d084e7f7c grub.cfg: scan luks *inside lvm* 
* 5a3b0dab966 grub.cfg: Scan *every* LVM device 
* 3c9f4be76f6 Libreboot 20241206, 8th revision 
* d4cc94d6b44 rom.sh: don't run mkpicotool on dry builds 
* de6d2f556f1 pico-sdk: Import picotool as a dependency 
* 4210ee68ea2 lib.sh: Much safer python version check 
* 8c7ba6131cc coreboot/next uprev: Fix T480 backlight keys 
* 411fb697dfc set up python in PATH, ensuring that it is python3 
* e8336bcc3ca vendor.sh: Proper semantics on prefix file names 
* 63f45782638 vendor.sh: Confirm if need_files=n 
* 13b06ae130f vendor.sh: Allow restoring the default GbE file 
* ab8feff92e0 vendor.sh: set random MAC address *by default* 
* 0ceaa01d45d vendor.sh: add clarification to nogbe warning 
* 4d5caf1dcfc vendor.sh: check that the vcfg file exists 
* fc4ee88e167 vendor.sh: error out if nuking failed 
* 8819a93d89b add line break, part 3 
* 8ce1a00f517 add line break, part 2 
* bc2c14e76a8 add line break 
* c762850311a vendor.sh: prevent double-nuke 
* 68299ad05ca vendor.sh: much more verbose errors/confirmation 
* b8e6d12f3d9 add libx86 to arch dependencies 
* cf8ad497b4e vendor.sh: Remove unnecessary return 
* c858099b359 vendor.sh: Download utils even if vcfg unset 
* ce16856a242 vendor.sh: Allow setmac if vendorfiles not needed 
* 4b51787d078 add less to arch dependencies 
* 8bd028ec153 lib.sh: Set python after dependencies 
* 44b6df7c24c update my copyright years on modified scripts 
* 818f3d630c2 vendor.sh: Don't error if vcfg is unset 
* 432a1a5bca7 lib.sh: Fix unescaped quotes in chkvars() 
* a73b0fd910a Revert "fix more unescaped quotes in eval" 
* ec6bcc1fba5 fix more unescaped quotes in eval 
* 5284f20b981 fix ./mk dependencies build issue 
* d825f9a9683 rom.sh: Remove errant GRUB modules check 
* 4149f3dc81a submodule/grub: use codeberg for 1st gnulib mirror 
* 0305975e705 util/nvmutil: Update AUTHORS and COPYING files 
* 20b192e13bd util/nvmutil: Describe nvmutil in help output 
* d1ca21628cb util/nvmutil: Remove the correct binary on uninstall 
* e63fe256dfc util/spkmodem-recv: More correct Makefile 
* efd50ee548b util/nvmutil: Honour the INSTALL variable 
* 8008838abbc util/nvmutil: Don't clean when doing uninstall 
* 982f257f58a util/nvmutil: Proper DESTDIR/PREFIX handling 
* 3f85ae5f853 util/nvmutil: Set CC and CFLAGS only if unset 
* 2c7b9fb9412 util/nvmutil: Capitalise BABA 
* 57f9906f6d1 util/nvmutil: Add uninstall to Makefile 
* 4defe2c6085 util/nvmutil: Add distclean to Makefile 
* 033e4cd9d50 util/nvmutil: Make the GbE checksum a define 
* 874317c4e59 util/nvmutil: nicer hexdump display 
* a338e585eed util/nvmutil: show the correct hexdump order 
* b032e483ef1 lib.sh mktarball: cleaner if statement 
* 0cf58c22734 fix lbmk shellcheck errors 
* 8276560cc99 lib.sh and rom.sh: update my header 
* 08e86d2218c vendor.sh inject: reset err upon return 
* 41275d699ca vendor.sh: MUCH, MUCH, MUCH safer ./mk inject 
* ed7293494e3 util/nvmutil: Obey the 79-character per line limit 
* 637b5e36fd2 util/nvmutil: Tidy up copyright header 
* cd28db883e2 vendor.sh: fix comment 
* 57971ceb227 util/nvmutil: Fix another straggler 
* 15b37b2a1ab util/nvmutil: Tidy up pledge calls 
* e8799310db2 hp820g2: fix vendorfile inject and set release=y 
* f9ab082ec19 fedora41/dependencies: add libuuid-devel 
* 661591f9f0b add uuid-devel to fedora41 dependencies 
* 1a46c047386 support ./mk dependencies fedora reinstall 
* d58d63569f1 fix missing semicolon in grub nvme patch 
* 95ea3293df5 bump seabios to rev 1602647f1 (7 November 2024) 
* 6d7e6c361b3 Bump GRUB revision to 6811f6f09 (26 November 2024) 
* 09a01477df6 t480/3050micro: force power off post power failure 
* d344cd95eac flashprog: Disable -Werror 
* dc95e912bfe bump flashprog to revision eb2c041 (14 Nov 2024) 
* 27c8c1c16ba replace liblz4-tool with lz4 and liblz4-dev 
* d3a732a64db lib.sh dependencies: support --reinstall argument 
* 466ada423dd move xbmkpath to XBMK_CACHE/ 
* b0a23840327 Revert "Remove legacy update/vendor commands" 
* 3d7dd4aa9fe Fix U-Boot build issue with Swig 4.3.0 
* 0c810747469 use command -v instead of which 
* 6c7e3ce2d6e trees: remove unnecessary subshell 
* ad137eae89d trees: only symlink host gcc/gnat to build xgcc 
* cfb6de94c33 trees: correction on check_gnu_path 
* ec2f0716662 trees: match gcc/gnat versions both ways 
* f64b5996279 Merge path.sh into script/trees 
* 295463d281e path.sh: Further cleanup 
* 5b24e0a5a96 path.sh: More thorough gcc/gnat version check 
* 7849a075886 path.sh: minor cleanup 
* 17168a87dbf path.sh: remove unnecessary shebang 
* e565df94fd7 Fix globbing issue in lbmk 
* c80cc0a00b6 remove auto-confirm on distro dependencies 
* 01fc65a0a9d Mitigate Debian Trixie/Sid GCC/GNAT version mismatch 
* 424b0c7103b t480/3050micro: disable hyperthreading 
* 603105f3b4e t480/t480s: Disable TPM2 to mitigate SeaBIOS lag 
* 754bd1e6ca3 rom.sh: Name pico directory serprog_pico 
* db22308eba5 add 2024 to Riku's copyright header on rom.sh 
*   4fa5f696db8 Merge pull request 'rp2530' (#258) from Riku_V/lbmk:rp2530 into master 
|\  
| * a5e0360992d pico-sdk: update to 2.1.0 
| * e2f8cc7f3ee pico-serprog: enable building for multiple pico chips 
|/  
* ccc2b4d589f add spdx headers to dependencies configs 
* a3969701e6b dependencies/debian: fix debian sid 
* 8f370cb60d9 add spdx headers to various config files 
* d591ea4c5dc git.sh: don't initialise livepull globally 
* b5da9feba3b vendor.sh: Print useful message on ./mk inject 
* 12c6259cb2f vendor.sh: Handle FSP insertion post-release 
* 78132051462 Remove legacy update/vendor commands 
* 07037561bd6 lbmk: remove use of deprecated ./vendor command 
* 5d1f1823067 vendor.sh: Safer exit when vendorfiles not needed 
* a18175a5df9 data/deguard: Remove unused patch 
* ee8f53b96ff lib.sh: Safer exit from ./mk dependencies 
* a8b35c88cf1 remove geteltorito and mtools from lbmk 
* 1dd32ea5487 rom.sh: support grub-first setups 
* f7801ef4770 vendor.sh: delete old tb.bin first, just in case 
* 02cbf8a729d vendor.sh: make TBFW pad size configurable 
* 9884e5ed1b0 T480/T480S: Support fetching ThunderBolt firmware 
* 36b42dd1c11 also de-rainbow the u-boot menu 
* eafc82028a4 Revert "use rainbow deer on the grub background" 
* 44969c73bd2 rom.sh: insert grub background in cbfs not memdisk 
* 401efb24b22 use rainbow deer on the grub background 
* dc27cb91784 add some scripts to .gitignore 
* 3b6b283eabe disable 3050micro nvme hotplug 
* c2023921893 fix t480 spd size (512, not 256) 
* da527459b68 add tarballs and signatures to gitignore 
* b910424b5df fix another very stupid mistake 
* e3b77b132e6 fix the stupidest bug ever This is about 650 changes.When certain bugs are found, releases may be re-built and re-uploaded. When this happens, the original release is replaced with a .Revisions are numbered; for example, the first post-release revision is .No revisions, thus far. The original 25.06 release is the current revision, so it could be considered  (revision zero).This release was built on the latest Debian 12.10 Bookworm release, as of this day. It was also build-tested successfully on the latest Arch Linux updates as of 26 June 2025.]]></content:encoded></item><item><title>A Pro-Russia Disinformation Campaign Is Using Free AI Tools to Fuel a ‘Content Explosion’</title><link>https://www.wired.com/story/pro-russia-disinformation-campaign-free-ai-tools/</link><author>/u/wiredmagazine</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 19:32:01 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[ campaign is leveraging consumer artificial intelligence tools to fuel a “content explosion” focused on exacerbating existing tensions around global elections, Ukraine, and immigration, among other controversial issues, according to new research published last week.The campaign, known by many names including Operation Overload and Matryoshka (other researchers have also tied it to Storm-1679), has been operating since 2023 and has been aligned with the Russian government by multiple groups, including Microsoft and the Institute for Strategic Dialogue. The campaign disseminates false narratives by impersonating media outlets with the apparent aim of sowing division in democratic countries. While the campaign targets audiences around the world, including in the US, its main target has been Ukraine. Hundreds of AI-manipulated videos from the campaign have tried to fuel pro-Russian narratives.The report outlines how, between September 2024 and May 2025, the amount of content being produced by those running the campaign has increased dramatically and is receiving millions of views around the world.In their report, the researchers identified 230 unique pieces of content promoted by the campaign between July 2023 and June 2024, including pictures, videos, QR codes, and fake websites. Over the last eight months, however, Operation Overload churned out a total of 587 unique pieces of content, with the majority of them being created with the help of AI tools, researchers said.The researchers said the spike in content was driven by consumer-grade AI tools that are available for free online. This easy access helped fuel the campaign’s tactic of “content amalgamation,” where those running the operation were able to produce multiple pieces of content pushing the same story thanks to AI tools.“This marks a shift toward more scalable, multilingual, and increasingly sophisticated propaganda tactics,” researchers from Reset Tech, a London-based nonprofit that tracks disinformation campaigns, and Check First, a Finnish software company, wrote in the report. “The campaign has substantially amped up the production of new content in the past eight months, signalling a shift toward faster, more scalable content creation methods.”Researchers were also stunned by the variety of tools and types of content the campaign was pursuing. "What came as a surprise to me was the diversity of the content, the different types of content that they started using,” Aleksandra Atanasova, lead open-source intelligence researcher at Reset Tech, tells WIRED. “It's like they have diversified their palette to catch as many like different angles of those stories. They're layering up different types of content, one after another.”Atanasova added that the campaign did not appear to be using any custom AI tools to achieve their goals, but were using AI-powered voice and image generators that are accessible to everyone.While it was difficult to identify all the tools the campaign operatives were using, the researchers were able to narrow down to one tool in particular: Flux AI.Flux AI is a text-to-image generator developed by Black Forest Labs, a German-based company founded by former employees of Stability AI. Using the SightEngine image analysis tool, the researchers found a 99 percent likelihood that a number of the fake images shared by the Overload campaign—some of which claimed to show Muslim migrants rioting and setting fires in Berlin and Paris—were created using image generation from Flux AI.]]></content:encoded></item><item><title>How to safely change StorageClass reclaimPolicy from Delete to Retain without losing existing PVC data?</title><link>https://www.reddit.com/r/kubernetes/comments/1lpavii/how_to_safely_change_storageclass_reclaimpolicy/</link><author>/u/kiroxops</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 19:24:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi everyone, I have a StorageClass in my Kubernetes cluster that uses reclaimPolicy: Delete by default. I’d like to change it to Retain to avoid losing persistent volume data when PVCs are deleted.However, I want to make sure I don’t lose any existing data in the PVCs that are already using this StorageClass.]]></content:encoded></item><item><title>A virtual pet site written in Rust, inspired by Neopets - 2 years later!</title><link>https://www.reddit.com/r/rust/comments/1lp9f6g/a_virtual_pet_site_written_in_rust_inspired_by/</link><author>/u/lemphi</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 18:29:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Just about two years ago I posted here about how I was looking to get better with Rust and remembered how much I liked Neopets as a kid, so I ended up making a virtual pet site in Rust as a fun little project! Well, I've still been working on it daily ever since then, and it's not quite so little anymore, so I thought I'd provide an update here in case anyone was curious about how everything's going.It uses Rust on the backend and TypeScript on the frontend. The only frontend dependencies are TypeScript, Solid JS, and mutative. The backend server runs on a $5 / month monolithic server and mostly uses axum, sqlx, Postgres, strum, tokio, tungstenite, rand, and oauth2.I've also really optimized the code since then. Previously, user requests would use JSON, but I've since migrated to binary websockets. So now most requests and responses are only a few bytes each (variable length binary encoding).I also wrote some derive macro crates that convert Rust data types to TypeScript. So I can annotate my Rust structs / enums and it generates interface definitions in TypeScript land for them, along with functions to decode the binary into the generated interface types. So Rust is my single source of truth (as opposed to having proto files or something). Some simple encoding / decoding crates then serialize my Rust data structures into compact binary (so much faster and smaller than JSON). Most of the data sent (like item IDs, quantities, etc.) are all small positive integers, and with this encoding protocol each is only 1 byte (variable length encoding).So now I can write something like this:#[derive(BinPack, FromRow, ToTS, DecodeTS)] pub struct ResponseProfile { pub person_id: PersonID, pub handle: String, pub created_at: UnixTimestamp, pub fishing_casts: i32 } and the following TypeScript is automatically generated:export interface ResponseProfile { person_id: PersonID; handle: string; created_at: UnixTimestamp; fishing_casts: number; } export function decodeResponseProfile(dv: MyDecoder): ResponseProfile { return { person_id: decodePersonID(dv), handle: dv.getStr(), created_at: decodeUnixTimestamp(dv), fishing_casts: dv.getInt(), }; } Another design change was that previously I used a lot of  for many things in the web server (like everyone's current luck, activity feed, rate limiters, etc.) I never liked this and after a lot of thinking I finally switched towards an approach where each player is a separate actor, and channels are used to send messages to them, and they own their own state in their own tokio task. So each player actor now owns their activity feed, game states, current luck, arena battle state, etc. This has led to a much cleaner (and much more performant!) architecture and I was able to delete a ton of mutexes / rwlocks, and new features are much easier to add now.With these changes, I was able to be much more productive and added a ton of new locations, activities, items, etc. I added new puzzles, games, dark mode, etc. And during all of this time, the Rust server has still never crashed in the entire 3 years it's been running (compared to my old Node JS days this provides me  much peace of mind). The entire codebase (frontend + backend) has grown to be around 130,000 lines of code, but the code is still so simple and adding new features is still really trivial. And whenever I refactor anything, the Rust compiler tells me everything I need to change. It's so nice because I never have to worry about breaking anything because the compiler always tells me anything I need to update. If I had to do everything over again I would still choose Rust 100% because it's been nothing but a pleasure.But yeah, everything is still going great and it's so much fun coming up with new stuff to add all the time. Here's some screenshots and a trailer I made recently if you want to see what it looks like (also, almost every asset is an SVG since I wanted all the characters and locations to look beautiful at every browser zoom level). Also, I'd love to hear any feedback, critique, thoughts, or ideas if you have any!]]></content:encoded></item><item><title>Websites used to be simple</title><link>https://simplesite.ayra.ch/</link><author>/u/AyrA_ch</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 17:50:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
				This website is a trip down memory lane.
				I'm not trying to tell you to stop modern web development.
				This website uses technologies not available at the time the content here is about.
				It works on mobile (tested in Firefox for Android) but you miss out on the background image.
			
			I created my first website somewhere in the early 2000s,
			and like most websites back then, it was very simple.
			Not surprising, considering most people (including me) were likely using notepad to create those websites,
			which puts a limit on their complexity.
			It was either that or  editors that would chain you to themselves
			because there was no chance the generated HTML would be maintainable at all without the tool,
			and if you did manual edits it could outright break your editor.
			There were no iPhones, there was barely any SEO, and JavaScript really was optional, and so was CSS.
			The color representation on early LCD screens was bad, so you better picked a color scheme with high contrast.
		
			The resolution of choice was 1024×768 (or 1280×1024 if you could afford it),
			and yet with the window frame and the toolbars we had back then your website should better also work on 800×600.
			You didn't want to have content right up to the edge of the left or right screen border anyways.
		
			For reference, this image shows the resolutions 1920×1080 (HD), 1280×1024, 1024×768,
			and 800×600 in relation to each other.
			The simplest sites were just plain HTML without anything beyond the basic formatting.
			Sometimes a  and  if we felt fancy.
			People that cared would change the font to a sans-serif type,
			often using a  tag that went around the entire page.
			There weren't a lot of safe fonts out there, and your choices were as follows:
		:
				Serif fonts being the default made this look like a mistake or low effort,
				but in the right places could add some nostalgia.
				And it does look professional, considering it's the "newspaper font".
				Being the default, it is also what you got if the font of choice
				by the website owner was not installed on your system.
			:
				The font of choice for when you don't know what else to pick.
				A sensible option, suitable for pretty much all applications.
			:
				Nerds. Nerds picked this font.
				The choice for when text had to be green on black background.
			:
				Now likely the most hated font, back then a good choice for your personal website.
				It may look goofy but its readability is not to be underestimated.
				It's a windows-only font, but most people were using Windows back then.
			
			Animated gif images were always an option, but they eat up valuable bandwidth.
			For simple move or blink animations,
			the  and  tags had you covered.
			The marquee is deprecated but still works. Blink was removed
			but a bit of JS .
			In that regards, please enjoy the most important animation ever in pure HTML.
		
			In absence of most CSS features we now take for granted,
			tiny images were common to achieve things like color gradient, rounded border edges
			and fancy looking buttons. It wasn't uncommon for a site to be made up of 10 or more images for this purpose.
		
			We also occasionally added background images.
			These were usually very small and would tile across the page.
			You could steal them from an existing website, or make one yourself.
			In fact, I proudly present the literal "wall" paper on this page, created in mspaint in two minutes.
			It's around 130 bytes. Not as fancy as an animated image but better than a solid color.
			If the wallpaper made text unreadable we would either go into mspaint and floodfill it with a different color,
			or just increase the font size.
			We weren't in a rush, and didn't need to cram as much information as possible into the viewport either.
		
			The fancy people would make the background image stay put when scrolling the content, just like on this page.
		Partitioning your website
			Modern websites have it easy.
			If you want your header or a menu to stay visible you can just use 
			and voilà, your element will stay inside of the viewing area when scrolling.
		
			We didn't had this luxury, but what we had were framesets.
			A frameset was (actually still is) a way of partitioning the browser window,
			and displaying individual pages inside of those partitions.
			Many websites would have the following layout:
		
			Unless blocked with an attribute,
			users could drag the frame border around to change the partition ratio.
			The border could also be made invisible, creating a seamless experience.
			If the background color of each frame was the same you could usually not tell that frames were there,
			unless you were looking closely during the page load.
		
			The frames could interact with each other.
			You could give them an identifier using the  attribute,
			and links could change the URL of a given frame using the  attribute.
			My first background music changer worked this way.
			A frame could contain a website, or another frameset, allowing to partition the window multiple times.
		
			Yes this technology still works. It is marked as deprecated but
			it will still render and behave like it did over 20 years ago.
		
			Tables were pretty much the only universal means of creating a responsive web layout.
			If you fixed the width of all but one column the unfixed column would adjust in width automatically
			to the current window size if the table width was set to 100%.
		
			Tables were also the first universal way of vertically centering content
			using the  attribute.
			The frameset layout shown above would occasionally be made with tables
			rather than frames. People would shove entire websites into single table cells.
		
			Responsive meant it would adjust to the screen size of a computer.
			Nobody cared about the modern doomscrolling rectangles because they didn't exist yet.
			We did not have flexbox, but float:left gets you quite far.
		
			The early solution to mobile devices was a completely separate website,
			optimized for small screens. People would be redirected based on the user agent string.
		
			JavaScript is a language designed to be just barely good enough to make animated gif images
			dance around on the page and for the content around your mouse cursor to sparkle
			when you move it.
			And this is exactly how we used it. This and drop down menus.
			Anything beyond that would get you into the realm of browser compatibility problems.
			Those problems are what made jQuery popular because it detected your browser
			and abstracted all the differences it had with other browsers away,
			but for a very long time, conditional HTML comments were the norm to make IE behave.
			Practically every page had a <!--[if IE lt 7]>...<![endif]--> section.
			For most small scripts, you would go to a site like  (defunct),
			search for the script you wanted, and copy paste it into your website.
			This behavior hasn't significantly changed although now you go to stackoverflow or ask an AI.
			Back then copy pasting scripts was your best bet
			because JS debugging tools were virtually non-existent at that time.
			Script loading would (and still does) block the page from rendering anything below it,
			so script tags were traditionally put at the bottom of the page rather than the header,
			and small scripts were inlined.
			Today we have the  attribute.
		
			A script at the bottom of course meant users with slow connections
			could try to interact with your page before it fully loaded in.
			This was often solved by making the main page element hidden initially
			and displaying a "Loading" text (or gif if you were fancy) instead.
			A small script that was jammed into the  attribute of the body
			would then hide the loading banner and show the main content.
		
			If you ever need to make a page interactable before it is fully loaded,
			use a mutation observer on the document root element that monitors added nodes.
			Then simply add the events to the relevant elements as they're streamed in.
			We didn't had that back then. Instead we would register a setInterval function
			that frequently added events, and unregistered said function in the onload event.
		
			Regardless of the amount of scripts on a page,
			serious websites would work to some extent without it,
			simply because JS was a security problem, and so was disabled in many corporate environments.
			The  tag can be used to render any content (including  tags)
			if scripts are disabled. This is usually used to inform the user that JS is necessary,
			or to provide a link to a less interactive version of the site.
			However JS is now considered a base requirement for most websites.
			Sites that use JS based UI rendering will just remain blank if JS is disabled.
		Dynamic Server Side Content
			A super fancy page would show dynamic content from the server and update it.
			The simplest solution that worked in all browsers was an iframe with a meta refresh tag inside.
			It would unconditionally reload the iframe. By making the border invisible people wouldn't even be able to tell.
			This of course is kinda bad because you reload the page regardless of whether there is new content to show or not.
		
			Long polling must be among the dirtiest, nastiest tricks we used to get content to dynamically update from the server.
			When you load a website, your browser actually renders the HTML elements as they're streamed from the network.
			This is why on some slow sites the width of various elements changes as the site loads.
			HTTP was strictly a client to server initiated protocol.
			We figured out however that you can abuse the HTML streaming behavior
			to implement a server to client initiated protocol.
			You would do this by loading a page in an iframe that was purposefully designed to never stop loading.
			It would occasionally send an invisible HTML comment to keep the connection open,
			but would otherwise remain silent until it was necessary to push new content to the client.
			You would then simply send a  tag with new JavaScript instructions inside,
			or if the content was purely for display purposes,
			a  with the content inside, plus a piece of CSS code to hide the previous div.
			Websites would grow indefinitely with this but you could simply solve this with a meta refresh
			that triggered when the connection ended.
			It was crude but it was dynamic content without JS.
			We built entire live chat systems around this.
		
			The first true way to replace long polling are websockets.
			HTTP 2 and 3 have the ability to push events to the client without waiting for a client request
			in what is known as "server push" but I've never seen it in the wild.
		
			Ajax stands for "Asynchronous javascript and XML".
			It was invented by Microsoft to streamline communication between web browsers (only Internet Explorer actually)
			and the Exchange Server web interface. The technology is actually quite old.
			Internet Explorer 5 was the first browser to ship with this, but others were quickly to adopt it too.
			Microsoft products intensively use XML, which is why XML is contained in "Ajax",
			why the JS object to make requests is named ,
			and why there's a dedicated  property in it.
		
			Anything not covered by web standards could be extended using ActiveX.
			This was basically a way to load and call functions from native DLLs that registered themselves as an ActiveX component.
			This was necessary to play video. It was also needed for audio if you desired any control over the playback whatsoever
			because  would not allow you to control playback
			beyond replacing its value with "about:blank" using JS to stop it entirely.
			Now deprecated,  was replaced with .
			And thankfully, fully automated audio playback on page load is not permitted by modern browsers
			because that is certainly something I don't want back.
		
			Back then, ActiveX was also a way to bypass some system restrictions.
			At school they would block the remote desktop client, but that block was only for the executable.
			I would just load the MSTSCAX library into a html file and then use the web browser to connect to my home computer
			whenever I found a website being blocked.
		
			The entire system was a nightmare but Microsoft could basically do whatever they desired
			because of the massive market share that Internet Explorer had.
			Every browser implemented this differently, and you had to update these components all the time.
		
			Flash (see: "Vulnerability as a Service") allowed you to do many things that initially weren't possible without,
			including but not limited to video and audio playback without depending on locally installed codecs,
			live streaming, file uploads with a progress bar, and networked multiplayer games.
			At some point it was basically mandatory to have it installed
			because most websites would to some extent depend on it.
		
			Bandwidth was obviously at a premium.
			And while times were slower back then,
			we would not want to wait for ages either.
			DSL was just getting popular,
			and I was lucky enough to start out with a 5000/500 kbps connection.
			A good website however would load in an acceptable manner on a dialup connection.
			These were commonly known as "56k" because that was their speed under ideal conditions,
			56 kilobits per second. This amounts to 7 kilobytes per second.
			You often would get a bit more because the connection supported compression,
			and HTML is fairly simple to compress.
		
			Images were considered high Bandwidth media back then, and you would not fill your page with them
			without some serious modifications.
			The image further below is a new version of the Windows XP default wallpaper known as "bliss"
			that Microsoft recently released. The colors in their version are more muted than the original,
			but I found this "corrected" version that's more in spirit with the original.
			The image is 2'345'199 bytes in size.
			This would load 5.5 minutes given a 56k connection.
			To improve load speeds we first drop the resolution.
			If our site should work with 800×600 displays
			there is no need to have this image in its original 4089×2726 size,
			and because we likely don't have the full size of the screen available due to the frameset menu,
			we can scale it down to fit 640×480 (the OG VGA resolution).
		
			Next is the quality. By setting the jpeg to 75% quality we can further reduce the size.
			This quality value is good for noisy pictures like this but will show artifacts around hard borders,
			of which there are none.
			With those limits the image will load in about 6 seconds.
		
			We can simulate a faster image load by saving it as a progressive image.
			Progressive images don't store the pixels in the normal order, but rather store them in groups.(This is a simplification)
			The first group will only store every 8th pixel horizontally and vertically.
			The next group will store every 4th pixel (minus those already contained in the first group),
			and so on until all pixels are represented.
			Depending on how good your eyes are you may not even notice the last pass.
			Images stored this way will slightly increase in size,
			but rather than having to wait for it to slowly appear line by line,
			we can fairly quickly render an initial (blurry) version that progressively (hence the name) gets better.
			This is also possible with PNG images.
		
			The image below has been saved with all the given constraints and settings mentioned above.
			It is now about 35 KB. To see it load progressively,
			refresh the page or open the image in a new tab.
		
			If you did not want to compromise on the quality you would
			create a thumbnail that when clicked, would navigate the user to the raw image.
		
			Here's the PHP code that converted this image
		$bliss = ImageCreateFromJpeg('bliss.jpg');
// "-1" means to calculate height to keep aspect ratio
$scaled = ImageScale($bliss, 640, -1, IMG_BICUBIC);
ImageDestroy($bliss);
// Enable progressive scan
ImageInterlace($scaled, TRUE);
// Save with 75% quality
ImageJpeg($scaled, 'bliss_scaled.jpg', 75);
ImageDestroy($scaled);
			If the client had JavaScript enabled,
			you could load the image using JS only when the user scrolls the image into view.
			This could save substantial bandwidth on image heavy sites.
			You can still do this, but the browser does it automatically if you add the  attribute.
			This also stops the image from blocking the page load event.
		
			Images could be made interactive to some extent.
			If the image was inside of an  tag, the attribute 
			could be added to the image. When clicking, the browser would navigate to the URL in the link,
			but adds the X and Y coordinate as  query string to the URL.
			This allows a server to check where in the image the user clicked. This allowed for image based menus,
			or a "Where's Waldo" (or "Wally" in some countries) style game.
			Later came client side image maps, which are invisible clickable regions you could overlay over an image.
		
			Both of these technologies still work,
			none of them are marked as deprecated either.
		
			Yolo driven development was the norm back then for personal sites and small company pages.
			You changed a few lines, then uploaded your changes to your webserver with an FTP client,
			likely not even encrypted.
			Your choices were PHP on an Apache web server and ASP on an IIS.
			Staging environment? What staging environment?
		
			It was crude, but oh boy was it fast to get something going.
			PHP is easy to get started, more forgiving than other languages, available practically everywhere,
			and still actively maintained.
			At this point it has probably gained cockroach status and will be around for ages.
		
			This is also how this website is deployed. Except instead of FTP I use syncthing
			because I want it to upload automatically when I change something,
			but straight to prod it goes.
			It doesn't needs any form of compilation or build process whatsoever.
		
			Simpler times. Not necessarily better, but simpler.
			We achieved a lot with less. We optimized our media, and used very little scripting.
			Now we don't. Nobody cares anymore if your website is 10 or 20 megabytes.
			Do I miss creating websites in a plain text editor? No. I want to keep my IDE with syntax highlighting,
			syntax checking, and code completion.
			I also want to keep the libraries that make my life easier.
			I haven't written an SQL statement in my backend code for a long time now.
			SQL ORM mappers are great.
			Do I wish we would create simpler pages again? Yes.
			I believe that although websites back then were simpler and technology more limited,
			it was not necessarily worse than today, but the average website now feels bloated and overengineered.
			The internet is no longer a place of creation; it is a place of consumption.
			I want the world wide web wild west back (call it W5 or whatever).
			I want search engines to find wacky websites again.
			I don't want to please algorithms of our corporate overlords.
			Writing this page, I just found out that in 2021, Warner Bros has taken down the original Space Jam website from 1996.
			I would have liked to put it here,
			but I guess the web archive is your only hope now.
			If you have time, check out how the main menu on that site was made.
		
			In any case, my tribute to those times is a wordle clone.
			If you have Internet Explorer 1.5 or later, or any modern browser (I heard this Mozilla thing is taking off),
			you can play a game of wordle here.
			I recommend you use at least IE 2.0 because 1.5 lacks color support, which makes the clues less visible.
		
			If you want to go super oldschool and can get a copy of NCSA Mosaic running (the first ever web browser),
			you can play the game here instead.
			I can confirm the browser does run on Windows XP, and possibly later versions of Windows,
			but likely only on 32 bit versions.
			Note: The WWW was designed to share crudely formatted scientific documents and link them,
			which explains why it's so hostile towards making screen oriented applications
			but has a plethora of features what work well for print media.
			This early feature set is exactly what Mosaic 1.0 provides.
			It lacks color support and doesn't even has means for text input,
			but I managed to cobble together something for it anyways.
			Its HTTP support is best described as "somewhat there", which explains why this wordle version runs on its own port.
		
			You're experiencing it right now.
			This website is looped through a RS-232 serial connection at 56k baud rate
			(actually a little bit extra to handle protocol overhead).
			I disabled the server cache so you can experience the scrollbar shrinking as content slowly loads in.
			But some things are worth the time. People may not even notice that your website is slow
			if you give them a bit of content to read before shoving an AI generated title image down their throat.
			You don't need a serial loopback for this,
			a tool like SlowPipe in front of your server does the same.
		]]></content:encoded></item><item><title>The Senate Just Put Clean Energy for AI in the Crosshairs</title><link>https://www.wired.com/story/the-senate-just-put-clean-energy-for-ai-in-the-crosshairs/</link><author>/u/wiredmagazine</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 17:26:44 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Even without the industry-ending excise tax, experts still say that the forced retirement of the tax credits blows up valuable investment in projects already in the pipeline. Since the beginning of the year, the clean energy industry has felt the pressure of looming IRA rollbacks. According to an analysis from energy NGO E2, around $15.5 billion in investment in new clean energy projects and factories has been lost since the start of the year, including more than $9 billion in Republican congressional districts.The intense hostility for solar and wind coming from the Trump administration may seem, to a logical person, to be at odds with its goal of “energy dominance.” Energy experts say that renewables—particularly when paired with batteries—are helping to bolster the US grid as energy needs soar. Texas, for instance, added more solar and battery storage than any other type of energy to its grid last year. As of this spring, wind and solar combined made up 42 percent of Texas’s installed generation capacity, more than any other state in the US. All that new solar and storage has, in turn, helped the grid stay stable during peak use, lowering the risk of blackouts during the first heatwaves of the summer—even as Texas faces never-before-seen summer demand this year, thanks to hot temperatures and the addition of energy-thirsty data centers. Yet in an op-ed published in the New York Post last week, Energy Secretary Chris Wright said that wind and solar contribute to a “less stable grid.”Doug Lewin, an energy analyst based in Austin, points out that solar and batteries are particularly well-positioned to help out with grid demand during heatwaves, when the sun is shining—and people turn on their air conditioners.“We’re just in this situation where we are going to need massive amounts of power to deal with the heat,” he says. “We’ve gotta have air conditioning to keep people healthy and safe during these hellacious summers, which are getting worse. That’s just an objective matter.”It’s particularly ironic to see these kinds of pushbacks as the Trump administration goes all in on artificial intelligence, which, by some projections, could comprise nearly 12 percent of US power demand by the end of the decade. Right now, a global backlog in gas turbines is spelling trouble for those looking to scale up fast. Turbine producers like GE Vernova say they’ve already filled orders for the next few years, and project it may take several years for new customers to get their hands on a completed turbine. In April, the CEO of renewable and utility giant NextEra Energy told shareholders that he expects renewables to act as a “bridge,” helping to bolster the grid and buy time until bigger gas projects can come online.But even with the promise of AI using up every spare electron on the grid, the cultural backlash to renewables is as strong as ever—and it isn’t isolated to the White House. Despite Texas’s reliance on renewables, the state legislature battled over several bills this past session that would have seriously kneecapped solar and wind development in the state. Oklahoma, which relies on wind energy for a third of its energy needs, faces a growing movement to ban renewables altogether. Across the country, local governments, responding to grassroots movements, are pushing back against wind and solar projects on their land. (It’s important to note that many of these movements often include Democrats.)Lewin, who wrote about Texas’s legislative drama in detail this year in his newsletter, says it’s too simplistic to ascribe the hostility towards renewables as simply being funded by Big Oil. According to Politico, Alaska Senator Lisa Murkowski, who has received hundreds of thousands of dollars in campaign donations from oil and gas interests over the course of her career, was an instrumental figure in changing the final Senate language to remove the excise tax. In Texas, the oil and gas lobby united with renewables to defeat a bill that would have made energy prices higher by increasing costs for wind and solar.“It feels like you’ve got a large number of really powerful folks who have just decided, or been convinced—and then had that belief reinforced by algorithms over and over—that somehow, wind and solar are the root of all evil and are causing every problem,” Lewin says. “It's bizarre. It's really hard to kind of understand this animus for technologies that have had a huge benefit.”]]></content:encoded></item><item><title>Hayro: An experimental, work-in-progress PDF rasterizer in pure Rust.</title><link>https://github.com/LaurenzV/hayro</link><author>/u/Frexxia</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 17:16:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why does Rust feel so well designed?</title><link>https://www.reddit.com/r/rust/comments/1lp7562/why_does_rust_feel_so_well_designed/</link><author>/u/Glum-Psychology-6701</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 17:03:41 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm coming from Java and Python world mostly, with some tinkering in fsharp. One thing I notice about Rust compared to those languages is everything is well designed. There seems to be well thought out design principles behind everything. Let's take Java. For reasons there are always rough edges. For example List interface has a method called add. Immutable lists are lists too and nothing prevents you from calling add method on an immutable list. Only you get a surprise exception at run time. If you take Python, the zen contradicts the language in many ways. In Fsharp you can write functional code that looks clean, but because of the unpredictable ways in which the language boxes and unboxes stuff, you often get slow code. Also some decisions taken at the beginning make it so that you end up with unfixable problems as the language evolves. Compared to all these Rust seems predictable and although the language has a lot of features, they are all coherently developed and do not contradict one another. Is it because of the creator of the language doing a good job or the committee behind the language features has a good process?]]></content:encoded></item><item><title>A black box full of dangers</title><link>https://www.reddit.com/r/rust/comments/1lp68zo/a_black_box_full_of_dangers/</link><author>/u/WanderingCID</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 16:30:05 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/WanderingCID ]]></content:encoded></item><item><title>Pluto is a unique dialect of Lua with a focus on general-purpose programming</title><link>https://pluto-lang.org/docs/Introduction</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 15:52:25 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Pluto is a superset of Lua 5.4 designed to assist with general-purpose programming & facilitate cleaner, more streamlined development via:Despite the immense additions, Pluto remains highly compatible with Lua:(Mostly) compatible with Lua 5.4 source code.
Our only breakage is the addition of new keywords, which causes conflicts when those keywords are used as identifiers. However, Pluto leverages parser heuristics and — in cases where parser heuristics fail — Compatibility Mode to eliminate this concern. Most Lua 5.4 source code will execute flawlessly on Pluto.Reads and writes Lua 5.4 bytecode meaning it's forwards- and backwards-compatible.
Only some Pluto features generate backwards-incompatible bytecode, but they will say so in their documentation.Actively rebases with Lua's main repository. We are not a time-frozen dialect. When Lua 5.5 releases, we intend on updating to that.With Compatibility Mode, Pluto has been dropped into large communities and did not break any existing scripts.What does Pluto aspire to be?​Pluto aspires to be a version of Lua with a larger feature-set, that is all. Pluto is not a Lua-killer, an attempted successor, or any of that. Many people (rightly so) love Lua precisely because of the design philosophy. And fundamentally, Pluto is a major deviation from Lua's design philosophy. Some may prefer this, some may not.]]></content:encoded></item><item><title>Protesters accuse Google of violating its promises on AI safety: &apos;AI companies are less regulated than sandwich shops&apos;</title><link>https://www.businessinsider.com/protesters-accuse-google-deepmind-breaking-promises-ai-safety-2025-6</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 15:42:16 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A full-blown courtroom drama — complete with a gavel-wielding judge and an attentive jury, played out in London's King's Cross on Monday, mere steps away from Google DeepMind's headquarters.Google was on trial for allegations of breaking its promises on AI safety.The participants of this faux-production were protesters from PauseAI, an activist group concerned that tech companies are racing into AI with little regard for safety. On Monday, the group congregated near King's Cross station to demand that Google be more transparent about the safety checks it's running on its most cutting-edge AI models.PauseAI argues that Google broke a promise it made during the 2024 AI Safety Summit in Seoul, Korea, when the company agreed to consider external evaluations of its models and publish details about how external parties, including governments, were involved in assessing the risks.When Google launched Gemini 2.5 Pro, its latest frontier model, in April, it did neither of those things. The company said it was because the model was still "experimental." A few weeks later, it released a "model card" with some safety details, which some experts criticized for being too thin on details, TechCrunch previously reported. While the safety report made reference to third-party testers, it did not specify who they were."We are committed to developing AI safely and securely to benefit society," a Google DeepMind spokesperson told BI. "We continue to evolve our model testing and reporting to respond to rapid changes in the technology, and will continue to provide information that supports the responsible use of our AI models."For PauseAI, this isn't good enough. More importantly, the organization said, it's about not letting any lapse slip by and allowing Google to set a precedent."If we let Google get away with breaking their word, it sends a signal to all other labs that safety promises aren't important and commitments to the public don't need to be kept," said PauseAI organizing director Ella Hughes, addressing the crowd, which had gradually swelled to around 60 people."Right now, AI companies are less regulated than sandwich shops."Focusing on the specific issue of the Google safety report is a way for PauseAI to push for a specific and attainable near-term change.About 30 minutes into the protest, several intrigued passers-by had joined the cause. After a rousing speech from Hughes, the group proceeded to Google DeepMind's offices, where the fake courtroom production played out. Some Google employees leaving for the day looked bemused as chants of "Stop the race, it's unsafe" and "Test, don't guess" rang out."AI regulation on an international level is in a very bad place," PauseAI founder Joep Meindertsma told Business Insider, pointing to how US Vice President JD Vance warned against over-regulating AI at the AI Action Summit.Monday was the first time PauseAI had gathered over this specific issue, and it's not clear what comes next. The group is engaging with members of UK parliament who will run these concerns up the flagpole, but Meindertsma is reticent to say much about how Google is engaging with the group and their demands.Meindertsma hopes support will grow and references polls that suggest the public at large is concerned that AI is moving too fast. The group on Monday was made up of people from different backgrounds, including some who work in tech. Meindertsma himself runs a software development company and regularly uses AI tools from Google, OpenAI, and others."Their tools are incredibly impressive," he said, "which is the thing that worries me so much."]]></content:encoded></item><item><title>Is os.Executable() reliable?</title><link>https://www.reddit.com/r/golang/comments/1lp4rce/is_osexecutable_reliable/</link><author>/u/1oddbull</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 15:33:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The documentation says no guarantee that the path is pointing to the right executable. But then how do you ship other applications files with your Go executable? eg an Electron app   submitted by    /u/1oddbull ]]></content:encoded></item><item><title>(Ab)using channels to implement a 3D pipe game</title><link>https://jro.sg/go-chan.html</link><author>/u/jroo1</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 14:43:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Who&apos;s Hiring - July 2025</title><link>https://www.reddit.com/r/golang/comments/1lp3e6p/whos_hiring_july_2025/</link><author>/u/jerf</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 14:40:06 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This post will be stickied at the top of until the last week of July (more or less).: It seems like Reddit is getting more and more cranky about marking external links as spam. A good job post obviously has external links in it. If your job post does not seem to show up please send modmail. Or wait a bit and we'll probably catch it out of the removed message list.Please adhere to the following rules when posting:Don't create top-level comments; those are for employers.Feel free to reply to top-level comments with on-topic questions.Meta-discussion should be reserved for the distinguished mod comment.To make a top-level comment you must be hiring directly, or a focused third party recruiter with specific jobs with named companies in hand. No recruiter fishing for contacts please.The job must be currently open. It is permitted to post in multiple months if the position is still open, especially if you posted towards the end of the previous month.The job must involve working with Go on a regular basis, even if not 100% of the time.One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.Please base your comment on the following template:[Company name; ideally link to your company's website or careers page.][Full time, part time, internship, contract, etc.][What does your team/company do, and what are you using Go for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.][Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.][Please attempt to provide at least a rough expectation of wages/salary.If you can't state a number for compensation, omit this field. Do not just say "competitive". Everyone says their compensation is "competitive".If you are listing several positions in the "Description" field above, then feel free to include this information inline above, and put "See above" in this field.If compensation is expected to be offset by other benefits, then please include that information here as well.][Do you offer the option of working remotely? If so, do you require employees to live in certain areas or time zones?][Does your company sponsor visas?][How can someone get in touch with you?]]]></content:encoded></item><item><title>Strudel: a programming language for writing music</title><link>https://strudel.cc/workshop/getting-started/</link><author>/u/pimterry</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 14:26:38 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Welcome to the Strudel documentation pages!
You’ve come to the right place if you want to learn how to make music with code.With Strudel, you can expressively write dynamic music pieces.
It is an official port of the Tidal Cycles pattern language to JavaScript.
You don’t need to know JavaScript or Tidal Cycles to make music with Strudel.
This interactive tutorial will guide you through the basics of Strudel.
The best place to actually make music with Strudel is the Strudel REPLWhat can you do with Strudel?live code music: make music with code in real timealgorithmic composition: compose music using tidal’s unique approach to pattern manipulationteaching: focussing on a low barrier of entry, Strudel is a good fit for teaching music and code at the same time.integrate into your existing music setup: either via MIDI or OSC, you can use Strudel as a really flexible sequencerHere are some examples of how strudel can sound:These examples cannot fully encompass the variety of things you can do, so check out the showcase for some videos of how people use Strudel.The best way to start learning Strudel is the workshop.
If you’re ready to dive in, let’s start with your first sounds]]></content:encoded></item><item><title>Kamune, secure communication over untrusted networks</title><link>https://www.reddit.com/r/golang/comments/1lp2cup/kamune_secure_communication_over_untrusted/</link><author>/u/hossein1376</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 13:57:34 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[EDIT: This is an experimental project, and is not intended to be used for critical purposes.Two weeks ago, Internet access in Iran was shut down nationwide. The remaining services were government-controlled or affiliated. So, I started writing something that allowed for secure communication over untrusted networks. I learned a lot, and it helped me to keep myself busy. I'm curious to know what you guys think about it, and I'm looking forward to your thoughts and suggestions. LinkFun fact: Initially, I named it as such because Kāmune (in Persian means truck) have always reminded me of the word communication. Later on, my sister mentioned that the word can also be read as Kamoon-e, which means ricochet; and now I think it makes more sense to call it that.]]></content:encoded></item><item><title>Sniffnet: a free, open source network monitoring app</title><link>https://www.reddit.com/r/linux/comments/1lp2288/sniffnet_a_free_open_source_network_monitoring_app/</link><author>/u/GyulyVGC</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 13:44:57 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Sniffnet (website | GitHub) is a powerful yet intuitive network analysis tool to enable everyone comfortably monitor their Internet traffic.I’ve been working on Sniffnet as a side-project for almost 3 years, and its development is today supported by the European Union’s  program.The most recent major version of the app was published just a couple days ago and, among the other features, it finally makes Sniffnet available as a Docker image for Linux.The latest release also introduces the ability to import data from Packet Capture files in addition to network interfaces, and it turned out Sniffnet is 2x faster than Wireshark at processing them.]]></content:encoded></item><item><title>[R] The Bitter Lesson is coming for Tokenization</title><link>https://www.reddit.com/r/MachineLearning/comments/1lp1lfb/r_the_bitter_lesson_is_coming_for_tokenization/</link><author>/u/lucalp__</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 13:24:15 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[New to the sub but came across discussion posts on BLT so I figured everyone might appreciate this new post! In it, I highlight the desire to replace tokenization with a general method that better leverages compute and data.For the most part, I summarise tokenization's role, its fragility and build a case for removing it. I do an overview of the influential architectures so far in the path to removing tokenization so far and then do a deeper dive into the Byte Latent Transformer to build strong intuitions around some new core mechanics.Hopefully it'll be of interest and a time saver for anyone else trying to track the progress of this research effort.]]></content:encoded></item><item><title>How do I setup backup &amp; restore for CloudNativePG such that it works with an &quot;ephemeral&quot; cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1lp15y2/how_do_i_setup_backup_restore_for_cloudnativepg/</link><author>/u/TemporalChill</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 13:05:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[LIKELY ALREADY RESOLVED: I didn't take a bloody backup to begin with. I knowww. Fresh pair of eyes could've saved my entire night.I love how easy it is to setup cnpg, but as a new user, the backup/restore bit is sending me. Perusing the docs, I figured this was possible:Create my cnpg clusters (initdb), with s3 backup configured.After the initdb job has succeeded and the wal backups show up in s3, alter the cnpg cluster manifest to replace initdb bootstrap with the SAME s3 cluster as restore source.Now I can teardown the k8s cluster and rebuild it. Given there are backups in s3, the restoration should be automated and straightforward, no matter how many k8s resets I have.apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: uno-postgres spec: storage: size: 5Gi backup: barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret bootstrap: recovery: source: clusterBackup externalClusters: - name: clusterBackup barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret Note that I comment out the bootstrap section for init to succeed and do I see the wal/000... files in my obj store, so it's not a connection problem. I figure the bootstrap section only needs to be commented out once for initdb to run and place the initial backup files in s3, after which I'd never have to comment it out again.The "full recovery" pod fails with:"msg":"Error while restoring a backup","logging_pod":"uno-postgres-1-full-recovery","error":"no target backup found","stacktrace": ]]></content:encoded></item><item><title>Vulnerability Advisory: Sudo chroot Elevation of Privilege</title><link>https://www.stratascale.com/vulnerability-alert-CVE-2025-32463-sudo-chroot</link><author>/u/FryBoyter</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 12:20:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Media] Rust + Svelte for single binary web apps</title><link>https://www.reddit.com/r/rust/comments/1lozkov/media_rust_svelte_for_single_binary_web_apps/</link><author>/u/HugoDzz</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 11:49:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed</title><link>https://www.reddit.com/r/MachineLearning/comments/1lozfbp/p_i_created_an_opensource_tool_to_analyze_15m/</link><author>/u/Avienir</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 11:41:03 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:Classical ML still dominates: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI researchAlgorithm preferences by medical condition: Different health problems gravitate toward specific algorithms Transformer takeover timeline: You can see the exact point (around 2022) when transformers overtook LSTMs in medical researchI built an interactive dashboard where you can:Search by medical condition to see which algorithms researchers are usingTrack how algorithm usage has evolved over timeSee the distribution across classical ML, deep learning, and LLMsOne of the trickiest parts was filtering out false positives (like "GAN" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.Happy to answer any questions or hear suggestions for improving it!]]></content:encoded></item><item><title>A guide to fine-grained permissions in MCP servers</title><link>https://www.cerbos.dev/blog/dynamic-authorization-for-ai-agents-guide-to-fine-grained-permissions-mcp-servers</link><author>/u/West-Chard-1474</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 11:24:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[AI Agents are rapidly evolving beyond simple Retrieval-Augmented Generation (RAG) and are now expected to take action. This is made possible through standards like the Model Context Protocol (MCP), which allows agents to interact with external tools and APIs. However, this new capability introduces a critical challenge: implementing fine-grained permissions and access controls based on “who can do what?”.Hardcoding  statements for user roles is not a scalable or secure solution. Modern applications require a dynamic authorization model that can make decisions based on a rich set of attributes - a model often referred to as Policy-Based Access Control or Attribute-Based Access Control.This guide will walk you through building a secure MCP server where AI Agent tool access is managed by Cerbos, a decoupled, policy-driven authorization service. You will learn how to enforce fine-grained authorization by externalizing access controls into human-readable policies.See how to implement dynamic authorization for AI agents, and fine-grained permissions in MCP servers, using Cerbos - speak with an engineer.The challenge. Static permissions in a dynamic AI worldWhen an AI Agent acts on behalf of a user, it must be subject to delegated (or an attenuated form of) permissions as that user. The challenge is that these Permissions are often complex and context-dependent. For example:A user might be able to  an expense but not  it.A manager might be able to  expenses, but only for their own team.An admin might be the only one who can  records.Implementing this logic directly in the MCP server creates brittle, hard-to-manage code. A change in your authorization policy requires a code change and a full redeployment.The solution. Decoupled authorization with Cerbos and MCPThe Model Context Protocol (MCP) is a specification that standardizes communication between AI Agents and external tools. An MCP server exposes a list of available tools, which any MCP client, be it a human in a chat application or a native AI agent, can then invoke to perform actions in your system.Cerbos is a stateless, open source authorization service that externalizes access controls into declarative YAML policies. Your application queries the Cerbos Policy Decision Point with a question like, "Can this principal perform this action on this resource?" Cerbos evaluates the relevant policies and returns a simple allow/deny decision in milliseconds. This enables powerful PBAC and ABAC without complicating your application logic.By combining MCP and Cerbos, you build a system where the MCP server defines , but dynamically enables only the ones the user has permission to use for a given request.Step-by-step implementation guideStep 1: Declarative policy authoringFirst, define your access controls in a Cerbos policy. This policy will govern which roles have permission to use which tools (actions).Create a  directory and add the following  file.File: policies/mcp_expenses.yamlapiVersion: "api.cerbos.dev/v1"
resourcePolicy:
  version: "default"
  resource: "mcp::expenses"
  rules:
    - actions: ["list_expenses"]
      effect: EFFECT_ALLOW
      roles: ["admin", "manager", "user"]

    - actions: ["add_expense"]
      effect: EFFECT_ALLOW
      roles: ["user"]

    - actions: ["approve_expense", "reject_expense"]
      effect: EFFECT_ALLOW
      roles: ["admin", "manager"]

    - actions: ["delete_expense", "superpower_tool"]
      effect: EFFECT_ALLOW
      roles: ["admin"]
Step 2: Deploying the Cerbos PDPRun the Cerbos PDP in Docker, mounting your policies directory. This makes your authorization policies live and ready to be queried.docker run --rm -it -p 3593:3593 \
  -v "$(pwd)/policies":/policies \
  ghcr.io/cerbos/cerbos:latest
Step 3: Integrating the MCP serverCreate a Node.js Express server that connects to the Cerbos PDP.npm install express @modelcontextprotocol/sdk @cerbos/grpc
Create the server: The code below defines every tool but uses  to perform a central authorization check. Based on the response, it dynamically enables only the permitted tools for the session. How the identity gets passed to this is out of scope, but with the recent OAuth improvements in the MCP spec, you will be able to token with the user's identity from an OAuth2 authorization server and pass it through to the MCP server.import express from "express";
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StreamableHTTPServerTransport } from "@modelcontextprotocol/sdk/server/streamableHttp.js";
import { GRPC } from "@cerbos/grpc";
import { randomUUID } from "node:crypto";

const cerbos = new GRPC("localhost:3593", { tls: false });

async function getServer({ user, sessionId }) {
  const server = new McpServer({ name: "CerbFinance MCP Server" });

  // Example tools - actual implementation is out of scope
  const tools = {
    list_expenses: server.tool(
      "list_expenses",
      "Lists expenses.",
      {},
      { title: "List Expenses" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
    add_expense: server.tool(
      "add_expense",
      "Adds an expense.",
      {},
      { title: "Add Expense" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
    approve_expense: server.tool(
      "approve_expense",
      "Approves an expense.",
      {},
      { title: "Approve Expense" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
    reject_expense: server.tool(
      "reject_expense",
      "Rejects an expense.",
      {},
      { title: "Reject Expense" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
    delete_expense: server.tool(
      "delete_expense",
      "Deletes an expense.",
      {},
      { title: "Delete Expense" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
    superpower_tool: server.tool(
      "superpower_tool",
      "Grants superpowers.",
      {},
      { title: "Superpower Tool" },
      async () => ({ content: [{ type: "text", text: "..." }] })
    ),
  };

  const toolNames = Object.keys(tools);

  // Central Authorization Check
  const authorizedTools = await cerbos.checkResource({
    principal: { id: user.id, roles: user.roles },
    resource: { kind: "mcp::expenses", id: sessionId },
    actions: toolNames,
  });

  for (const toolName of toolNames) {
    if (authorizedTools.isAllowed(toolName)) {
      tools[toolName].enable();
    } else {
      tools[toolName].disable();
    }
  }

  server.sendToolListChanged();
  return server;
}

const app = express();
app.use(express.json());

// Middleware to simulate user authentication - use OAuth in production
app.use((req, res, next) => {
  req.user = { id: "user-123", roles: ["user"] }; // Test different roles here
  next();
});

app.post("/mcp",  async (req, res) => {
  const transport = new StreamableHTTPServerTransport({
sessionIdGenerator: undefined,
});
  const server = await getServer({
    user: req.user,
    sessionId: req.sessionId || randomUUID(),
  });
  await server.connect(transport);
  await transport.handleRequest(req, res, req.body);
});
app.listen(3000, () => console.log("MCP Server running on port 3000"));
Testing your policy-driven AI agentYou can test your server using the MCP Client extension in VS Code.Open the Command Palette (Ctrl+Shift+P) and select "MCP: Add Server".Enter your server URL: http://localhost:3000/mcp.Run in Copilot to open a chat window. The available tools will be listed.Example prompts by role: Change the  in  to simulate different users.As a  ():
"Add an expense for $100" -> Succeeds -> Fails (The agent reports it doesn't have the tool).As a  (roles: ['manager', 'user']):
 -> Succeeds -> FailsAs an  ():
 -> SucceedsBeyond roles - the power of ABACRole-Based Access Control is just the beginning. The real power of a decoupled authorization system is implementing ABAC. With Cerbos, you can write policies that use attributes from the user (), the resource, or the request itself.For example, to restrict managers to approving expenses only up to a certain amount, you could pass the amount as an attribute and write a condition, then do an additional check inside the tool implementation: (snippet of the Cerbos call):await cerbos.checkResource({
  principal: { id: user.id, roles: user.roles },
  resource: {
    kind: "mcp::expenses",
    id: sessionId,
    attr: { amount: 150 } // Pass resource attributes
  },
  actions: ["approve_expense"],
});
policies/mcp_expenses.yaml (snippet of the policy rule):- actions: ["approve_expense"]
  effect: EFFECT_ALLOW
  roles: ["manager"]
  condition:
    match:
      # The manager can only approve if the expense amount is less than 1000
      expr: request.resource.attr.amount < 1000
This demonstrates true fine-grained authorization that goes far beyond simple roles.By decoupling your authorization logic using Cerbos, you can build powerful, secure, and scalable AI Agents. This architecture allows you to manage Permissions through declarative policies, enabling you to implement everything from simple role-based rules to sophisticated ABAC without touching your application code. As AI agents become more integrated into our workflows, a robust, policy-driven approach to access controls is a necessity.For further details on mastering dynamic authorization for MCP servers with Cerbos, check out this piece.]]></content:encoded></item><item><title>Lies we tell ourselves to keep using Golang</title><link>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</link><author>/u/Nekuromento</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 11:15:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
                    👋 This page was last updated ~3 years ago. Just so you know.
                In the two years since I’ve posted I want off Mr Golang’s Wild
Ride, it’s made the rounds time and
time again, on Reddit, on Lobste.rs, on HackerNews, and elsewhere.And every time, it elicits the same responses:You talk about Windows: that’s not what Go is good at! (Also, who cares?)This is very one-sided: you’re not talking about the  sides of Go!You don’t understand the compromises Go makes.Large companies use Go, so it can’t be  bad!Modelling problems “correctly” is too costly, so caring about correctness is moot.Correctness is a spectrum, Go lets you trade some for development speed.Your go-to is Rust, which also has shortcomings, so your argument is invalid.There’s also a vocal portion of commenters who wholeheartedly agree with the
rant, but let’s focus on unpacking the apparent conflict here.I’ll first spend a short amount of time pointing out clearly disingenuous
arguments, to get them out of the way, and then I’ll move on to the fairer
comments, addressing them as best I can.When you don’t want to hear something, one easy way to not have to think about
it at all is to convince yourself that whoever is saying it is incompetent, or
that they have ulterior motives.For example, the top comment on HackerNews right now starts like this:The author fundamentally misunderstands language design.As an impostor syndrome enthusiast, I would normally be sympathetic to such
comments. However, it is a lazy and dismissive way to consider any sort of
feedback.It doesn’t take much skill to notice a problem.In fact, as developers get more and more senior, they tend to ignore more and
more problems, because they’ve gotten so used to it. That’s the way it’s always
been done, and they’ve learned to live with them, so they’ve stopped questioning
it any more.Junior developers however, get to look at everything again with a fresh pair of
eyes: they haven’t learned to ignore all the quirks yet, so it feels
 to them, and they tend to question it (if they’re made to feel
safe enough to voice their concerns).This alone is an extremely compelling reason to hire junior developers, which I
wish more companies would do, instead of banking on the fact that “seniors can
get up-to-speed with our current mess faster”.As it happens, I am  a junior developer, far from it. Some way or another,
over the past 12 years, seven different companies have found an excuse to pay me
enough money to cover rent and then some.I did, in fact, design a language all the way back in
2009 (when I  a wee programmer baby), focused mainly on syntactic sugar
over C. At the time it was deemed interesting enough to warrant an invitation to
OSCON (my first time in Portland Oregon, the capital of grunge, coffee, poor
weather and whiteness), where I got to meet other young and not-so-young
whippersnappers (working on Io, Ioke, Wren, JRuby, Clojure, D, Go, etc.)It was a very interesting conference: I’m still deeply ashamed by the
presentation I gave, but I remember fondly the time an audience member asked the
Go team “why did you choose to ignore any research about type systems since the
1970s”? I didn’t fully understand the implications at the time, but I sure do
now.I have since thoroughly lost interest in my language, because I’ve started
caring about semantics a lot more than syntax, which is why I also haven’t
looked at Zig, Nim, Odin, etc: I am no longer interested in “a better C”.But all of that is completely irrelevant. It doesn’t matter who points out that
“maybe we shouldn’t hit ourselves in the head with a rake repeatedly”: that
feedback ought to be taken under advisement no matter who it comes from.Mom smokes, so it’s probably okayOne of the least effective way to shop for technologies (which CTOs, VPs of
engineering, principals, senior staff and staff engineers need to do regularly)
is to look at what other companies are using.It is a great way to  technologies to evaluate (that or checking
ThoughtWorks’ Tech Radar), but it’s far
from enough.A piece from company X on “how they used technology Y”, will 
reflect the true cost of adopting that technology. By the point the engineers
behind the post have been bullied into filling out the company’s tech blog after
months of an uphill battle, the decision has been made, and there’s no going
back.This kind of blog doesn’t lend itself to coming out and admitting that mistakes
were made. It’s supposed to make the company look good. It’s supposed to attract
new hires. It’s supposed to help us stay .Typically, scathing indictments of technologies come from , who
have simply decided that they, as a person, can afford making a lot of people
angry. Companies typically cannot.You can be impressed, that  are using Go, right now, and that
they have gone all the way to Davy Jones’ Locker and back to solve complex
problems that ultimately helps deliver value to customers.Or you can be , as you realize that those complex problems only exist
. Those complex problems would not exist in other
languages, not even in C, which I can definitely not be accused of shilling for
(and would not recommend as a Go replacement).A lot of the pain in the  article is caused by:Go not having sum types — making it really awkward to have a type that is
“either an IPv4 address or an IPv6 address”Go choosing which data structures you need — in this case, it’s the
one-size-fits-all slice, for which you pay 24 bytes on 64-bit machines.Go not letting you do operator overloading, harkening back to the Java days
where  isn’t the same as Go’s lack of support for immutable data — the only way to prevent something
from being mutated is to only hand out copies of it, and to 
to not mutate it in the code that actually has access to the inner bits.Go’s unwillingness to let you make an opaque “newtype”. The only way to do
it is to make a separate package and use interfaces for indirection, which is
costly  awkward.Unless you’re out for confirmation bias, that whole article is a very compelling
argument against using Go for that specific problem.And yet Tailscale is using it. Are they wrong? Not necessarily! Because their
team is made up of a bunch of . As evidenced by the  article,
about the Go linker.Because they’re Go experts, they know the cost of using Go upfront, and they’re
equipped to make the decision whether or not it’s worth it. They know how Go
works deep down (something Go marketing pinky-swears you never need to worry
about, why do you ask?), so if they hit edge cases, they can dive into it, fix
it, and wait for their fix to be upstreamed (if ever).But chances are, . This is not your org. You are not Google
either, and you cannot afford to build a whole new type system on top of Go just
to make your project (Kubernetes) work at all.But okay - Tailscale’s usage of Go is pretty  still. Just like my
2020 piece about Windows raised an army of “but that’s not what Go is good for”
objections, you could dismiss Tailscale’s posts as “well that’s on you for
wanting to ship stuff on iOS / doing low-level network stuff”.Fair enough! Okay. Let’s talk about what makes Go compelling.Go is a pretty good async runtime, with opinionated defaults, a
state-of-the-art garbage collector with two
knobs, and tooling that would make C developers jealous, if they bothered
looking outside their bubble.This also describes Node.js from the very start (which
is essentially libuv + V8), and I believe it also describes “modern Java”, with
APIs like NIO. Although I haven’t checked what’s happening in Java land too
closely, so if you’re looking for an easy inaccuracy to ignore this whole
article, there you go: that’s a freebie.Because the async runtime is core to the language, it comes with tooling that
 make Rust developers jealous! I talk about it in Request coalescing in
async Rust, for example.Go makes it easy to dump backtraces (stack traces) for all running goroutines in
a way tokio doesn’t, at this time. It is also able to
detect deadlocks, it comes with its own profiler, it seemingly lets you not
worry about the color of
functions, etc.Go’s tooling around package management, refactoring, cross-compiling, etc., is
easy to pick up and easy to love — and certainly feels at first like a definite
improvement over the many person-hours lost to the whims of pkg-config,
autotools, CMake, etc. Until you reach some of the arbitrary limitations that
simply do not matter to the Go team, and then you’re on your own.All those and more explains why many, including me, were originally enticed by
it: enough to write piles and piles of it, until its shortcomings have finally
become impossible to ignore, by which point it’s too late. You’ve made your bed,
and now you’ve got to make yourself feel okay about lying in it.But one  does not a platform make.The really convenient async runtime is not the only thing you adopted. You also
adopted a  toolchain, a build system, a calling convention, a
single GC (whether it works for you or not), the set of included batteries, some
of which you CAN swap out, but the rest of the ecosystem won’t, and most
importantly, you adopted a language that happened by accident.I will grant you that caring  about something is grounds for
suspicion. It is no secret that a large part of what comes out of academia is
woefully inapplicable in the industry at this time: it is easy to lose oneself
in the abstract, and come up with convoluted schemes to solve problems that do
not really exist for anyone else.I imagine this is the way some folks feel about Rust.But caring  about something is dangerous too.Evidently, the Go team didn’t  to design a language. What they really
liked was their async runtime. And they wanted to be able to implement TCP, and
HTTP, and TLS, and HTTP/2, and DNS, etc., on top of it. And then web services on
top of all of that.And so they didn’t. They didn’t design a language. It sorta just “happened”.Because it needed to be familiar to “Googlers, fresh out of school, who probably
learned some Java/C/C++/Python” (Rob Pike, Lang NEXT 2014), it borrowed from all
of these.Just like C, it doesn’t concern itself with error handling . Everything
is a big furry ball of mutable state, and it’s on you to add ifs and elses to
VERY CAREFULLY (and very manually) ensure that you do not propagate invalid
data.Just like Java, it tries to erase the distinction between “value” and
“reference”, and so it’s impossible to tell from the callsite if something is
getting mutated or not:Depending on whether the signature for change is this:…the local  in  will either get mutated or not.And since, just like C  Java, you do not get to decide what is mutable and
what is immutable (the  keyword in C is essentially advisory,
kinda), passing a
reference to something (to avoid a costly copy, for example) is fraught with
risk, like it getting mutated from under you, or it being held somewhere
forever, preventing it from being freed (a lesser, but very real, problem).Go fails to prevent many other classes of errors: it makes it easy to
accidentally copy a mutex, rendering it
completely ineffective, or leaving struct fields uninitialized (or rather,
initialized to their zero value), resulting in countless logic errors.Taken in isolation, each of these and more can be dismissed as “just a thing to
be careful about”. And breaking down an argument to its smallest pieces,
rebutting them one by one, is a self-defense tactic used by those who cannot
afford to adjust their position in the slightest.Which makes perfect sense, because Go is really hard to move away from.Unless you use cgo, (but cgo is not
Go), you are living in the
Plan 9 cinematic universe.The Go toolchain does not use the assembly language everyone else knows about.
It does not use the linkers everyone else knows about. It does not let you use
the debuggers everyone knows about, the memory checkers
everyone knows about, or the calling conventions everyone else has agreed to
suffer, in the interest of interoperability.Go is closer to
closed-world languages
than it is to C or C++. Even Node.js, Python and Ruby are not as hostile to
FFI.To a large extent, this is a feature: being different is . And it
comes with its benefits. Being able to profile the internals of the TLS and HTTP
stacks the same way you do your business logic is fantastic. (Whereas in dynamic
languages, the stack trace stops at OpenSSL). And that code takes full advantage
of the lack of function coloring: it can let the  worry about
non-blocking I/O and scheduling.But it comes at a terrible cost, too. There is excellent tooling out there for
many things, which you cannot use with Go (you can use it for the cgo parts,
but again, you should not use cgo if you want the Real Go Experience). All the
“institutional knowledge” there is lost, and must be relearned from scratch.It also makes it extremely hard to integrate Go with anything else, whether it’s
upstream (calling C from Go) or downstream (calling Go from Ruby). Both these
scenarios involve cgo, or, if you’re unreasonably brave, a terrifying
hack.Making Go play nice with another language (any other language) is really hard.
Calling C from Go, nevermind the cost of crossing the FFI boundary, involves
manual descriptor tracking,
so as to not break the GC. (WebAssembly had the same problem before reference
types!)Calling Go from  involves shoving the whole Go runtime (GC included)
into whatever you’re running: expect a very large static library and all the
operational burden of running Go code as a regular executable.After spending years doing those FFI dances in both directions, I’ve reached the
conclusion that the only good boundary with Go is a network boundary.Integrating with Go is  if you can afford to pay the
latency cost of doing RPC over TCP (whether it’s a REST-ish HTTP/1 API,
something like JSON-RPC, a more complicated scheme like GRPC, etc.). It’s also
the only way to make sure it doesn’t “infect” your whole codebase.But even that is costly: you need to maintain invariants on both sides of the
boundary. In Rust, one would typically reach for something like
serde for that, which, combined with sum types and the lack
of zero values, lets you make  that what you’re holding is what
you think you’re holding: if a number is zero, it was meant to be zero, it
wasn’t just missing.(All this goes out the window if you use a serialization format like
protobuf, which has all the
drawbacks of Go’s type system and none of the advantages).That still leaves you with the Go side of things, where unless you use some sort
of validation package religiously,
you need to be ever vigilant not to let bad data slip in, because the compiler
does  to help you maintain those invariants.And that brings us to the larger overall problem of the Go .All or nothing (so let’s do nothing)I’ve mentioned “leaving struct fields uninitialized”. This happens easily when
you make a code change from something like this:That second program prints this:We’ve essentially changed the function signature, but forgot to update a
callsite. This doesn’t bother the compiler at all.Oddly enough, if our function was structured like this:Then we’d get a compile error:Why does the Go compiler suddenly care if we provide explicit values now? If the
language was self-consistent, it would let me omit both parameters, and just
default to zero.Because one of the tenets of Go is that zero values are good, actually.See, they let you go fast. If you  mean for  to be zero, you can just
not specify it.And sometimes it works fine, because zero values  mean something:This is fine! Because the  slice is actually a reference type, and its
zero value is , and  just returns zero, because “obviously”, a
nil slice is empty.And sometimes it’s  fine, because zero values don’t mean what you think
they mean:In that case, you should’ve initialized the map first (which is also 
a reference type), with , or with a map literal.That alone is enough to cause incidents and outages that wake people up at
night, but everything gets worse real fast when you consider the Channel
Axioms:A send to a  channel blocks foreverA receive from a  channel blocks foreverA send to a closed channel panicsA receive from a closed channel returns the zero value immediatelyBecause there had to be a meaning for nil channels, this is what was picked.
Good thing there’s pprof to find those deadlocks!And because there’s no way to “move” out of values, there has to be meaning for
receiving and sending to closed channels, too, because even after you close them
you can still interact with them.(Whereas in a language like Rust, a channel closes when its
Sender is
dropped, which only happens when nobody can touch it again, ever. The same probably
applies to C++ and a bunch of other languages, this is not new stuff).“Zero values have meaning” is naive, and clearly untrue when you consider the
inputs of, like… almost everything. There’s so many situations when values
need to be “one of these known options, and nothing else”, and that’s where
sum types come in (in Rust, that’s enums).And Go’s response to that is: just be careful. Just like C’s response before it.Just don’t access the return value if you haven’t checked the error value. Just
have a half-dozen people carefully review each trivial code change to make sure
you’re not accidentally propagating a nil, zero, or empty string way too deep
into your system.It’s just another thing watch out for.It’s not like you can prevent  problems anyway.And you can write logic errors in just about every language! And if you try hard
enough I’m sure you can drive a train straight into a tree! It’s just much
easier with a car.The fallacy here is that because it is impossible to solve , we
shouldn’t even attempt to solve . By that same logic, it’s always
worthless to support any individual financially, because it does nothing to help
every  individual who’s struggling.And this is another self-defense tactic: to refuse to consider anything but the
most extreme version of a position, and point out how ridiculous it is (ignoring
the fact that nobody is actually defending that ridiculous, extreme position).So let’s talk about that position.“Rust is perfect and you’re all idiots”I  that was how I felt, because it would be so much simpler to explain.That fantasy version of my argument is so easy to defeat, too. “How come you use
Linux then? That’s written in C”. “Unsafe Rust is incredibly hard to write
correctly, how do you feel about that?”The success of Go is due in large part to it having batteries included and
opinionated defaults.The success of Rust is due in large part to it being easy to adopt piecemeal
and .They are both success stories, just very different ones.If the boogeyman is to be believed, “Rust shills” would have everyone
immediately throw away everything, and replace it with The Only Good Language
Out there.This is  from what’s happening in the real world, it’s tragic.None of these are without challenges, and none of the people involved are
denying said challenges. But all of these are incremental and pragmatic, very
progressively porting parts to a safer language .We are very far from a “throwing the baby out with the bathwater” approach. The
Rust codegen backend  is a mountain of C++ code (LLVM).
The alternatives are not competitors by any stretch of the imagination, except
maybe for another mountain of C++
code.The most hardcore Rust users are the most vocal about issues like build times,
the lack of certain language features (I just want
GATs!),
and all the other shortcomings everyone else is also talking about.And they’re also the first to be on the lookout for other, newer languages, that
tackle the same kind of problems, but do it .But as with the “questioning your credentials” angle, .
The current trends could be dangerous snake oil and we could have literally no
decent alternative, and it would still be worth talking about. No matter who
raises the point!Creating false dichotomies isn’t going to help resolve any of this.Folks who develop an allergic reaction to “big balls of mutable state without
sum types” tend to gravitate towards languages that gives them control over
mutability, lifetimes, and lets them build abstractions. That those languages
happen to often be Go and Rust is immaterial. Sometimes it’s C and Haskell.
Sometimes it’s ECMAScript and Elixir. I can’t speak to those, but they do
happen.You don’t have to choose between “going fast” and “modelling literally every
last detail of the problem space”. And you’re not stuck doing one or the other
if you choose Go or Rust.You can, at great cost, write extremely careful Go code that stays far away from
stringly-typed values and constantly checks invariants — you just get no help
from the compiler whatsoever.And you can, fairly easily, decide not to care about a whole bunch of cases when
writing Rust code. For example, if you’re not writing a low-level command-line
utility like , you can decide to only care about paths that are valid UTF-8
strings by using camino.When handling errors, it is extremely common to list a few options we  care
about and want to do special handling for, and shove everything else into an
“Other” or “Internal” or “Unknown” variant, which we can flesh out later as
needed, when reviewing logs.The “correct” way to assume an optional value is set, is to ,
not to use it regardless. That’s the difference between calling 
and crossing your fingers, and calling
unwrap()
on an .And it’s so much easier to do it correctly when the type system lets you spell
out what the options are — even when it’s as simple as “ok” or “not ok”.Which brings me to the next argument, by far the most reasonable of the bunch.Go as a prototyping/starter languageWe’ve reached the fifth stage of grief: acceptance.. It may well be that Go is not adequate for production services unless
your shop is literally made up of Go experts (Tailscale) or you have infinite
money to spend on engineering costs (Google).But surely there’s still a place for it.After all, Go is an easy language to pick up (because it’s so small, right?),
and a lot of folks have learned it by now, so it’s easy to recruit Go
developers, so we can get lots of them on the cheap and just uhhh prototype a
few systems?And then later when things get hard (as they always do at scale) we’ll either
rewrite it to something else, or we’ll bring in experts, we’ll figure something
out.Except there is no such thing as throwaway code.All engineering organizations I’ve ever seen are EXTREMELY rewrite-averse, and
for good reason! They take time, orchestrating a seamless transition is hard,
details get lost in the shuffle, you’re not shipping new features while you’re
doing that, you have to retrain your staff to be effective at the new thing,
etc.Tons of good, compelling reasons.So very few things eventually end up being rewritten. And as more and more
components get written in Go, there’s more and more reason to  doing that:
not because it’s working particularly well for you, but because interacting with
the existing codebases from  is so painful (except over
the network, and even then.. see “Go is an island” above).So things essentially never improve. All the Go pitfalls, all the things the
language and compiler , are an issue for everyone,
fresh or experienced. Linters help some, but can never do quite as much as
compiler for languages that took these problems seriously to begin with. 
they slow down development, cutting into the “fast development” promise.All the complexity that doesn’t live in the language now lives in your codebase.
All the invariants you don’t have to spell out using types, you now have to
spell out using code: the signal-to-noise ratio of your (very large) codebases
is extremely poor.Because it has been decided that abstractions are for academics and fools, and
all you  need is slices and maps and channels and funcs and structs, it
becomes extremely hard to follow what any program is doing at a high level,
because everywhere you look, you get bogged down in imperative code doing
trivial data manipulation or error propagation.Because function signatures don’t tell you much of anything (does this mutate
data? does it hold onto it? is a zero value there okay? does it start a
goroutine? can that channel be nil? what types can I really pass for this
 param?), you rely on documentation, which is costly to update, and
costlier still  to update, resulting in more and more bugs.The very reason I don’t consider Go a language “suitable for beginners” is
precisely that its compiler accepts so much code that is very clearly wrong.It takes a lot of experience about everything  the language, everything
Go willfully leaves as an exercise to the writer, to write semi-decent Go code,
and even then, I consider it more effort than it’s worth.The “worse is better” debate was never about some people wanting to feel
superior by adding needless complexity, then mastering it.Quite the contrary, it’s an admission that humans suck at maintaining
invariants. All of us. But we are capable of building tools that can help us
doing that. And focusing our efforts on that has an upfront cost, but that cost
is well worth it.I thought we’d moved past the notion that “programming is typing on a keyboard”
long ago, but when I keep reading “but it’s fast to write lots of Go!”, I’m not
so sure.Inherent complexity does not go away if you close your eyes.When you choose not to care about complexity, you’re merely pushing it onto
other developers in your org, ops people, your customers, . Now
 have to work around your assumptions to make sure everything keeps
running smoothly.And nowadays, I’m often that , and I’m tired of it.Because there is a lot to like in Go at first, because it’s so easy to pick up,
but so hard to move away from, and because the cost of choosing it in the first
place reveals itself slowly over time, and compounds, only becoming unbearable
when it’s much too late, this is not a discussion we can afford to ignore as an
industry.Until we demand better of our tools, we are doomed to be woken up in the middle
of the night, over and over again, because some  value slipped in where it
never should have.Here’s a list of lies we tell ourselves to keep using Golang:Others use it, so it must be good for us tooEveryone who has concerns about it is an elitist jerkIts attractive async runtime and GC make up for everything elseEvery language design flaw is ok in isolation, and ok in aggregate tooWe can overcome these by “just being careful” or adding more linters/eyeballsBecause it’s easy to write, it’s easy to develop production software withBecause the language is simple, everything else is, tooWe can do just a little of it, or just at first, or we can move away from it easilyWe can always rewrite it later
            (JavaScript is required to see this. Or maybe my stuff broke)
        ]]></content:encoded></item><item><title>Windows User Base Shrinks By 400 Million In Three Years</title><link>https://www.reddit.com/r/linux/comments/1loy6zj/windows_user_base_shrinks_by_400_million_in_three/</link><author>/u/Or0ch1m4ruh</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 10:30:42 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/Or0ch1m4ruh ]]></content:encoded></item><item><title>That Crossplane did not land. So... where to?</title><link>https://www.reddit.com/r/kubernetes/comments/1loxvu2/that_crossplane_did_not_land_so_where_to/</link><author>/u/IngwiePhoenix</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 10:11:36 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[But this feedback paired with the Domino's provider () had me left wondering what other mechanisms are out there to "unify" resources....This requires a bit of explaining. I run a little homelab with three k3s nodes on Radxa Orion O6'es - super nice, although I don't have the full hw available, the compute is plenty, powerful and good! Alpine Linux is my base here - it just boots and works (in ACPI mode). But, I have a few auxiliary servers and services that are not kube'd; a FriendlyElec NANO3 that handles TVHeadend, a NAS that handles more complex services like Jellyfin, PaperlessNGX and Home Assistant, a secondary "random crap that fits together" NAS with an Athlon 3000G that runs Kasm on OpenMediaVault - and soon, I will have an AI server backed by LocalAI. That's a lot of potential API resources and I would love to take advantage of them. Probably not all of them, to be fair and honest. However, this is why I really liked the basic idea of Crossplane; I can use the HTTP provider to define CRUD ops and then use Kubernetes resources to manage and maintain them - kind of centralizing them, and perhaps opting into GitOps also (which I have not done yet entirely - my stuff  in a private Git repo but no ArgoCD is configured).So... Since Crossplane hit such a nerve (oh my god the emotions were  xD) and OpenTofu seems absurdly overkill for a lil' homelab like this, what are some other "orchestration" or "management" tools that come to your mind?I might still try CrossPlane, I might try Tekton at some point for CI/CD or see if I can make Concourse work... But it's a homelab, there's always something to explore. And, one of the things I would really like to get under control, is some form of central management of API-based resources.So in other words; rather than the absolute moment that is the Crossplane post's comment section, throw out the things you liked to use in it's stead or something that you think would kinda go there!And, thanks for the feedback on that post. Couldn've asked for a cleaner opinion at all. XD]]></content:encoded></item><item><title>Monthly: Who is hiring?</title><link>https://www.reddit.com/r/kubernetes/comments/1loxpea/monthly_who_is_hiring/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 10:00:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[This monthly post can be used to share Kubernetes-related job openings within  company. Please include:Location requirements (or lack thereof)At least one of: a link to a job posting/application page or contact detailsIf you are interested in a job, please contact the poster directly. Common reasons for comment removal:Not meeting the above requirementsRecruiter post / recruiter listingsNegative, inflammatory, or abrasive tone   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Weekly: Questions and advice</title><link>https://www.reddit.com/r/kubernetes/comments/1loxpe3/weekly_questions_and_advice/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!]]></content:encoded></item><item><title>crd-to-sample-yaml now has an intellij and vscode plugin</title><link>https://www.reddit.com/r/kubernetes/comments/1lox6m9/crdtosampleyaml_now_has_an_intellij_and_vscode/</link><author>/u/skarlso</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 09:25:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a tool I wrote a while ago called crd-to-sample-yaml that does a bunch of things, but its main purpose is to be able to take anything that has an openAPI schema in it, and generate a valid YAML for it.Now, I created a vscode and an intellij plugin for it. They are both registered and your can find them here: VSCode Extension and here IntelliJ Plugin. The intellij plugin is still under review officially, but you can also install it from the repository through File → Settings → Plugins → Install Plugin from Disk.Enjoy, and if you find any problems, please don't hesitate to create an issue. :) Thank you so much for the great feedback and usage already.]]></content:encoded></item><item><title>Cross-Compiling 10,000+ Go CLI Packages Statically</title><link>https://blog.pkgforge.dev/cross-compiling-10000-go-cli-packages-statically</link><author>/u/Azathothas</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 09:23:53 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux managed to save me almost 50 gigs after a windows 11 install managed to somehow take up half my entire SSD.</title><link>https://www.reddit.com/r/linux/comments/1lowp0y/linux_managed_to_save_me_almost_50_gigs_after_a/</link><author>/u/gloombert</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 08:53:18 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alternative Blanket Implementations for a Single Rust Trait (blog post)</title><link>https://www.greyblake.com/blog/alternative-blanket-implementations-for-single-rust-trait/</link><author>/u/greyblake</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 07:32:39 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Rust's trait system is famously powerful - and famously strict about avoiding ambiguity.One such rule is that you can't have multiple blanket implementations of the same trait that  apply to the same type.What Is a Blanket Implementation?A  is a trait implementation that applies to  type meeting certain constraints, typically via generics.A classic example from the standard library is how  and  work together:Thanks to this, when you implement  for , you automatically get  for . Very ergonomic!However, Rust enforces a key rule: no two blanket implementations may overlap - even . Consider:Even if no type currently implements both  and , the compiler will reject this. The reason? Some type  satisfy both in the future, and that would make the implementation ambiguous.While working on Joydb, I ran into this exact problem.I have an  trait responsible for persisting data.In practice, there are two common ways to implement it:A  that stores all data in a single file (e.g., JSON). In Joydb, this is .A  that stores each relation in a separate file (e.g., one CSV per relation), called .Ideally, users would only need to implement one of those and get the  trait "for free".But Rust won't let me define two conflicting blanket implementations. So... is there a workaround? 🤔The Trait Definitions in JoydbHere are the relevant traits in Joydb:So the question becomes: how can I let someone implement either  or , and then get  automatically?The Workaround: Associated Type + Marker Structs like  and  to wrap adapter types.A helper trait, , implemented for each marker type.An  in the  trait to delegate behavior.These zero-sized types are used solely for type-level dispatch.Step 2: The  TraitNow we have non-conflicting blanket impls because they apply to  ( vs. ).Step 3: The  Trait with Associated TypeThe key piece: the associated type  tells  whether to delegate to  or .Let's say we need to implement a  that writes everything to a single file.
It can be implemented as a :No code duplication.
No conflicts.
The only overhead is 3 extra lines to link things togetherThis pattern - using marker types + associated types - gives you the flexibility of alternative blanket implementations while staying within Rust's coherence rules.It's especially useful when you want to support mutually exclusive behaviors under a unified interface, without compromising on ergonomics.Psss! Are you looking for a passionate Rust dev?My friend is looking for a job in Berlin or remote.
Reach out to .]]></content:encoded></item><item><title>It’s harder to read code than to write it</title><link>https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/</link><author>/u/abooishaaq</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 06:51:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Netscape 6.0 is finally going into its first public beta. There never was a version 5.0. The last major release, version 4.0, was released almost three years ago. Three years is an  long time in the Internet world. During this time, Netscape sat by, helplessly, as their market share plummeted.It’s a bit smarmy of me to criticize them for waiting so long between releases. They didn’t do it , now, did they?Well, yes. They did. They did it by making the single worst strategic mistake that any software company can make:They decided to rewrite the code from scratch.Netscape wasn’t the first company to make this mistake. Borland made the same mistake when they bought Arago and tried to make it into dBase for Windows, a doomed project that took so long that Microsoft Access ate their lunch, then they made it again in rewriting Quattro Pro from scratch and astonishing people with how few features it had. Microsoft almost made the same mistake, trying to rewrite Word for Windows from scratch in a doomed project called Pyramid which was shut down, thrown away, and swept under the rug. Lucky for Microsoft, they had never stopped working on the old code base, so they had something to ship, making it merely a financial disaster, not a strategic one.We’re programmers. Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand. We’re not excited by incremental renovation: tinkering, improving, planting flower beds.There’s a subtle reason that programmers always want to throw away the code and start over. The reason is that they think the old code is a mess. And here is the interesting observation:  The reason that they think the old code is a mess is because of a cardinal, fundamental law of programming:It’s harder to read code than to write it.This is why code reuse is so hard. This is why everybody on your team has a different function they like to use for splitting strings into arrays of strings. They write their own function because it’s easier and more fun than figuring out how the old function works.As a corollary of this axiom, you can ask almost any programmer today about the code they are working on. “It’s a big hairy mess,” they will tell you. “I’d like nothing better than to throw it out and start over.”“Well,” they say, “look at this function. It is two pages long! None of this stuff belongs in there! I don’t know what half of these API calls are for.” Before Borland’s new spreadsheet for Windows shipped, Philippe Kahn, the colorful founder of Borland, was quoted a lot in the press bragging about how Quattro Pro would be much better than Microsoft Excel, because it was written from scratch. All new source code! As if source code .The idea that new code is better than old is patently absurd. Old code has been . It has been .  of bugs have been found, and they’ve been . There’s nothing wrong with it. It doesn’t acquire bugs just by sitting around on your hard drive. Au contraire, baby! Is software supposed to be like an old Dodge Dart, that rusts just sitting in the garage? Is software like a teddy bear that’s kind of gross if it’s not made out of ?Back to that two page function. Yes, I know, it’s just a simple function to display a window, but it has grown little hairs and stuff on it and nobody knows why. Well, I’ll tell you why: those are bug fixes. One of them fixes that bug that Nancy had when she tried to install the thing on a computer that didn’t have Internet Explorer. Another one fixes that bug that occurs in low memory conditions. Another one fixes that bug that occurred when the file is on a floppy disk and the user yanks out the disk in the middle. That LoadLibrary call is ugly but it makes the code work on old versions of Windows 95.Each of these bugs took weeks of real-world usage before they were found. The programmer might have spent a couple of days reproducing the bug in the lab and fixing it. If it’s like a lot of bugs, the fix might be one line of code, or it might even be a couple of characters, but a lot of work and time went into those two characters.When you throw away code and start from scratch, you are throwing away all that knowledge. All those collected bug fixes. Years of programming work.You are throwing away your market leadership. You are giving a gift of two or three years to your competitors, and believe me, that is a  time in software years.You are putting yourself in an extremely dangerous position where you will be shipping an old version of the code for several years, completely unable to make any strategic changes or react to new features that the market demands, because you don’t have shippable code. You might as well just close for business for the duration.You are wasting an outlandish amount of money writing code that already exists.Is there an alternative? The consensus seems to be that the old Netscape code base was bad. Well, it might have been bad, but, you know what? It worked pretty darn well on an awful lot of real world computer systems.When programmers say that their code is a holy mess (as they always do), there are three kinds of things that are wrong with it.First, there are architectural problems. The code is not factored correctly. The networking code is popping up its own dialog boxes from the middle of nowhere; this should have been handled in the UI code. These problems can be solved, one at a time, by carefully moving code, refactoring, changing interfaces. They can be done by one programmer working carefully and checking in his changes all at once, so that nobody else is disrupted. Even fairly major architectural changes can be done without . On the Juno project we spent several months rearchitecting at one point: just moving things around, cleaning them up, creating base classes that made sense, and creating sharp interfaces between the modules. But we did it carefully, with our existing code base, and we didn’t introduce new bugs or throw away working code.A second reason programmers think that their code is a mess is that it is inefficient. The rendering code in Netscape was rumored to be slow. But this only affects a small part of the project, which you can optimize or even rewrite. You don’t have to rewrite the whole thing. When optimizing for speed, 1% of the work gets you 99% of the bang.Third, the code may be doggone ugly. One project I worked on actually had a data type called a FuckedString. Another project had started out using the convention of starting member variables with an underscore, but later switched to the more standard “m_”. So half the functions started with “_” and half with “m_”, which looked ugly. Frankly, this is the kind of thing you solve in five minutes with a macro in Emacs, not by starting from scratch.It’s important to remember that when you start from scratch there is  to believe that you are going to do a better job than you did the first time. First of all, you probably don’t even have the same programming team that worked on version one, so you don’t actually have “more experience”. You’re just going to make most of the old mistakes again, and introduce some new problems that weren’t in the original version. The old mantra  is dangerous when applied to large scale commercial applications. If you are writing code experimentally, you may want to rip up the function you wrote last week when you think of a better algorithm. That’s fine. You may want to refactor a class to make it easier to use. That’s fine, too. But throwing away the whole program is a dangerous folly, and if Netscape actually had some adult supervision with software industry experience, they might not have shot themselves in the foot so badly.]]></content:encoded></item><item><title>I want to build a TUI-based game (player movement, collisions, basic enemies). Is Go a good choice?</title><link>https://www.reddit.com/r/golang/comments/1louv5a/i_want_to_build_a_tuibased_game_player_movement/</link><author>/u/Feldspar_of_sun</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 06:48:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I had a silly idea to make an extreme demake of one of my favorite games (Ikachan) with an ASCII art style. I thought it would be fun to make it purely as a TUI Is Go a good choice for this? I have a little experience with it and have enjoyed what I’ve done so far, but I also have some experience in C/C++ and Python, and I’m wondering if those may be better If Go is a good choice, what package(s) would be best for something like this? If not, how come? And do you have a different recommendation?]]></content:encoded></item><item><title>This Is Why You Can&apos;t Trust AI to Review Your Mission-Critical Code</title><link>https://medium.com/p/456c47ce7e81</link><author>/u/goated_ivyleague2020</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 06:36:39 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Unspoken Dangers of Relying on Language ModelsAI makes it too easy to use too little brain power. And if I was lazy and careless, I would’ve paid the price.My name is Austin Starks, and I’m building an app called NexusTrade.NexusTrade is like if ChatGPT had a baby with Robinhood, who grew up to have a baby with QuantConnect. In short, it’s a platform that allows everybody to create, test, and deploy their own algorithmic trading strategies.While exceptionally good at testing complex strategies that operate at open and close, the platform has a major flaw… if you want to test out an intraday strategy, you’re cooked. I’m working diligently to fix this.In a previous article, I described the different milestones with the implementation. For , my objective is to implement “intraday-ness” within my indicators for my platform. And, if you observe from the surface-level (i.e, using AI tools), you might assume it’s already implemented! In fact, you can check it out yourself, and see the ingenuity of my implementation.And if I trusted the surface-level (i.e, the  AI tools on the planet), I would’ve proceeded with the WRONG implementation. Here’s how Claude Opus 4 outright failed me on a critical feature.To implement my intraday indicators, the first step was seeing if the implementation for it that exists is correct. To do this, I asked Claude the following:What do you think of this implementation? Is it correct? Any edge cases?For our overly ambitious engineering reader, here’s the implementation of the Simple Moving Average. See if you can spot the bug yourself.The way this implementation works is by taking a Duration as the input. This parameter helps us maintain a sliding window and works for any period — 30 days, 30 minutes, or even 30 seconds.At the surface, the implementation looks correct. Even Claude Opus 4, the most powerful coding LLM of our time, only pointed out nitpicks and unrealistic edge cases.However, solely because I implemented the technical indicator library, I knew that there existed a hidden weakness. Allow me to explain.On the surface level, the implementation of the intraday indicator looks sound. And it is!If you make the following assumption: the data is ingested at regular intervals.Take this graph for example. It will correctly compute the 14-day SMA across Apple’s closed price because we’re assuming one data-point per day. But what happens if that assumption is violated?Let’s say we “warmed up” our indicators using open/close data (i.e, computed our moving averages), and now we’re running our backtest on intraday data, which requires ingesting new data points at the minutely granularity.If we use the current implementation of the indicator, that introduces a major bug.This graph shows the impact of ingesting just 4 minutes of minutely data into our system. The SMA shoots up rapidly, approaching the current price of Apple.The current implementation  that each ingested data point should be weighted the same as every other datapoint in the window. This is wrong!In reality, we need to implement a time-weighted moving average. After some immense brainpower, I ended up developing the following algorithm.This new implementation is an improvement over the original because:It should regress to the original implementation if we’re just ingesting open and closed dataIt automatically resets the minutely averages at the start of the day for stocksIt maintains the minutely averages for cryptocurrency, which is tradeable all dayOur unchecked assumption would’ve caused a major bug in such a critical feature. What can we learn from this?I’m sharing this story for really one reason: as a cautionary tale for tech executives and software engineers.And this should go without saying, but I am not some anti-AI evangelist. I literally developed a no-code, AI-Powered trading platform. If there’s anybody having sermons about the power and value of AI, it would be me!But this article clearly demonstrates something immensely important: you can ask the literal best AI models of our time point blank if an implementation is wrong, and it will tell you no.Now, this is not the model’s fault.  if I prompted it in such a way that listed every single assumption that can be made, then maybe it would’ve caught it!But that’s not how people use AI in the real-world. I know it and you know it too.For one, many assumptions that we make in software are implicit. We’re not even fully aware that we’re making them!But also, just imagine if I didn’t even write the technical indicator library, and I trusted the authors to handle this automatically. Or, imagine if AI wrote the library entirely, and I never wondered about how it worked under the hood.The implementation would’ve yielded outright incorrect values forever. Unless someone raised the issue because something seemed off, the bug would’ve laid dormant for months or even longer.Debugging the issue would’ve been a nightmare on its own. I would’ve checked if the data was right or if the event emitter was firing correctly, and everything else within the core of the trading platform… I mean, why would I double-check the external libraries it depended on?Catastrophically-silent bugs like this are going to become rampant. Not only do AI tools dramatically increase the output of engineers, but they are notoriously bad at understanding the larger picture.Moreover, more and more non-technical folks are “vibe-coding” their projects into existence. They’re developing software based on intuition, requirements, and AI prompts, and don’t have a deep understanding of the actual code that’s being generated.I’ve seen it first-hand, on LinkedIn, Reddit, and even TikTok! Just Google “vibe-coding” and see how popular it has become.What happens when a “vibe-coded” library is used by thousands of developers, and these issues start infesting all of our software? If I nearly missed a critical bug and I actually wrote the code, how many bugs will exist because code wasn’t written by engineers with domain expertise?I shudder to think of that future.So if you’re a tech executive, don’t fire your engineering team yet. They may be more critical now than ever before.Maybe my brain is overreacting.Maybe I would’ve caught this issue well before I launched. I’m just having trouble figuring out In this case, I knew of the limitation because I wrote the library. But there are hundreds of libraries now being created and reviewed purely by AI. Engineers are looking at less and less of the code that is brought into the world.And this should terrify you.For backtesting software, the consequences of this bug would’ve been an improper test. Users would be annoyed and leave bad reviews. I would suffer reputational harm. But I would survive.But imagine such a bug for other, mission-critical software, like rocket ships and self-driving cars.This article demonstrates why human beings still need to be in the loop when developing complex software systems. It is imperative, that human-beings sanity-check LLM-generated code with domain-aware unit tests. Even the best AI models don’t fully grasp exactly what we’re building.So before you ship that feature (whose code you barely glanced at), ask yourself this question: what assumption did you and Gemini miss?]]></content:encoded></item><item><title>[D] Any path for a mid career/mid aged MLE to do ML research in the industry</title><link>https://www.reddit.com/r/MachineLearning/comments/1lotkac/d_any_path_for_a_mid_careermid_aged_mle_to_do_ml/</link><author>/u/LastAd3056</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 05:25:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.Some thoughts in my mind: (1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market. (2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.]]></content:encoded></item><item><title>One-Minute Daily AI News 6/30/2025</title><link>https://www.reddit.com/r/artificial/comments/1lot1gv/oneminute_daily_ai_news_6302025/</link><author>/u/Excellent-Target-847</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 04:53:40 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>My Claude collaborative platform</title><link>https://www.reddit.com/r/kubernetes/comments/1losxzq/my_claude_collaborative_platform/</link><author>/u/MuscleLazy</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 04:48:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I've been using Claude Desktop a lot and wanted a better way to manage different collaboration styles, like having it act as an engineer vs researcher vs creative partner. (the default) forgets everything between conversations. You start fresh every time, explain your preferences, coding style, whatever. Gets old fast. (with memory) actually remembers your working style, project context, and collaboration preferences. Game changer for long-term work.I've been using this setup for about 3 months now with the engineer profile and it dramatically improved my workflow.: Every conversation started with me explaining "I need root cause analysis first, minimal code changes, focus on production safety, don't over-engineer solutions." Then spending the first 10 messages training Claude to give me direct technical responses instead of hand-holding explanations.: Claude immediately knows I want systematic troubleshooting, that I prefer infrastructure optimization over quick fixes, and that I need definitive technical communication without hedging language.The platform tracks our  from incident reviews and  where it documents lessons learned from outages, alternative approaches we considered but didn't implement, and insights about our infrastructure.I've thoroughly tested the  profile for production incidents, while spending a lot less time on "tuning" the other profiles, you are welcome to contribute. It is striking to see how Claude transforms from a junior engineer, constantly performing unauthorized commands or file edits, into a "cold", "precise like a surgeon's scalpel" engineer. No more "You're right!" messages, Claude will actually tell you where you're wrong, straight up! Claude's  to . 🧑‍💻The most spectacular improvements are the conversation logs and Claude's diary, Claude will not be shy to write any dumb mistakes you did, priceless.The repo has all the details, examples, and documentation. Worth checking out if you're tired of re-training Claude on every conversation.]]></content:encoded></item><item><title>[R] Inference-Time Scaling and Collective Intelligence for Frontier AI</title><link>https://www.reddit.com/r/MachineLearning/comments/1los6wj/r_inferencetime_scaling_and_collective/</link><author>/u/iwiwijp</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 04:05:05 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.This research builds on our 2024 work on evolutionary model merging, shifting focus from “mixing to create” to “mixing to use” existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)]]></content:encoded></item><item><title>probemux: When you need more than 1 {liveness, readiness}Probe</title><link>https://www.reddit.com/r/kubernetes/comments/1lor7jp/probemux_when_you_need_more_than_1_liveness/</link><author>/u/thockin</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 03:12:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[There was an issue recently where someone argued that they REALLY DO need more than 1 livenessProbe, so I cobbled this together from bits of other programs:NAME probemux - multiplex many HTTP probes into one.SYNOPSIS probemux --port=<port> [OPTIONS]... BACKENDS...When the / URL is read, execute one HTTP GET operation against each backend URL and return the composite result. If all backends return a 2xx HTTP status, this will respond with 200 "OK". If all backends return valid HTTP responses, but any backend returns a non-2xx status, this will respond with 503 "Service Unavailable". If any backend produced an HTTP error, this will respond with 502 "Bad Gateway". Backends are probed synchronously when an incoming request is received, but backends may be probed in parallel to each other. Probemux has exactly one required flag. --port The port number on which to listen. Probemux listens on the unspecified address (all IPs, all families). All other flags are optional. -?, -h, --help Print help text and exit. --man Print this manual and exit. --pprof Enable the pprof debug endpoints on probemux's port at /debug/pprof/... --timeout <duration> The time allowed for each backend to respond, formatted as a Go-style duration string. If not specified this defaults to 3 seconds (3s). -v, --verbose <int>, $GITSYNC_VERBOSE Set the log verbosity level. Logs at this level and lower will be printed. --version Print the version and exit. probemux \ --port=9376 \ --timeout=5s \ http://localhost:1234/healthz \ http://localhost:1234/another \ http://localhost:5678/a-third    submitted by    /u/thockin ]]></content:encoded></item><item><title>Graphite (now a top-100 Rust project) turns Rust into a functional, visual scripting language for graphics operations — REQUESTING HELP to implement compiler bidirectional type inference</title><link>https://www.reddit.com/r/rust/comments/1lor3b4/graphite_now_a_top100_rust_project_turns_rust/</link><author>/u/Keavon</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 03:06:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Just now, Graphite has broken into the top 100 Rust projects on GitHub by star count, and it has been today's #1 trending repo on all of GitHub regardless of language.It's a community-driven open source project that is a comprehensive 2D content creation tool for graphic design, digital art, and interactive real-time motion graphics. It also, refreshingly, has a high-quality UI design that is modern, intuitive, and user-friendly. The vision is to become the Blender equivalent of 2D creative tools. Here's a 1-minute video showing the cool, unique, visually snazzy things that can be made with it.Graphite features a node-based procedural editing environment using a bespoke functional programming language, Graphene, that we have built on top of Rust itself such that it uses Rust's data types and  to transform artist-created documents into portable, standalone programs that can procedurally generate parametric artwork. Think: something spanning the gamut from Rive to ImageMagick.For the juicy technical deets, give the podcast episode a listen where we were interviewed about how our Graphene engine/language lets even nontechnical artists "paint with Rust", sort of like if Scratch used Rust as its foundation. We go into detail on the unique approach of turning a graphics editor into a compiled programming language where the visual editor is like an IDE for Rust code.Here's the ask: help implement bidirectional type inference in our language's compilerThe Graphene language — while it is built on top of Rust and uses Rust's compiler, data types, traits, and generics — also has its own type checker. It supports generics, but is somewhat rudimentary and needs to be made more powerful, such as implementing Hindley–Milner or similar, in order for Graphene types to work with contextual inference just like Rust types do.This involves the Graphene compiler internals and we only have one developer with a compilers background and he's a student with limited free time spread across all the crucial parts of the Graphite project's engineering. But we know that /r/rust is — well... — naturally a place where many talented people who love building compilers and hobby language implementations hang out.This type system project should last a few weeks for someone with the right background— but for more than a year, working around having full type inference support has been a growing impediment that is impacting how we can keep developing ergonomic graphics tooling. For example, a graphics operation can't accept two inputs and use the type of the first to pick a compatible generic type for the second. This results in painful workarounds that confuse users. Even if it's just a short-term involvement, even temporarily expanding our team beyond 1 knowledgeable compiler developer would have an outsized impact on helping us execute our mission to bring programmatic graphics (and Rust!) into the hands of artists.If you can help, we will work closely with you to get you up to speed with the existing compiler code. If you're up for the fun and impactful challenge, the best way is to join our project Discord and say you'd like to help in our  channel. Or you can comment on the GitHub issue.Besides compilers, we also need general help, especially in areas of our bottlenecks: code quality review, and helping design API surfaces and architecture plans for upcoming systems. If you're an experienced engineer who could help with any of those for a few hours a week, or with general feature development, please also come get involved! Graphite is one of the easiest open source projects to start contributing to according to many of our community members; we really strive to make it as frictionless as possible to start out. Feel free to drop by and leave a code review on any open PRs or ask what kind of task best fits your background (graphics, algorithm design, application programming, bug hunting, and of course most crucially: programming language compilers).Thank you! Now let's go forth and get artists secretly addicted to Rust 😀 In no time at all, they will be writing custom Rust functions to do their own graphical operations.P.S. If you are attending Open Sauce in a few weeks, come visit our booth. We'd love to chat (and give you swag).]]></content:encoded></item><item><title>Reflections on Haskell and Rust</title><link>https://academy.fpblock.com/blog/rust-haskell-reflections/</link><author>/u/sibip</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 02:58:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[For most of my professional experience, I have been writing production
code in both Haskell and Rust, primarily focusing on web services,
APIs, and HTTP stack development. My journey started with Haskell,
followed by working with Rust, and most recently returning to the
Haskell ecosystem.This experience has given me perspective on both languages' strengths
and limitations in real-world applications. Each language has aspects
that I appreciate and miss when working with the other. This post
examines the features and characteristics that stand out to me in each
language.Rust's ability to shadow variables seamlessly is something I came to
appreciate. In Rust, you can write:This pattern is common and encouraged in Rust, making code more
readable by avoiding the need for intermediate variable names. In
Haskell, you would typically need different names:Haskell's approach is slightly harder to read, while Rust's shadowing
makes transformation pipelines more natural.Rust's enum system, particularly when combined with pattern matching,
feels more robust than Haskell's sum types of records. When defining
sum types of records in Haskell, there is a possibility of introducing
partial record accessors which can cause runtime crashes, though
recent versions of GHC now produce compile-time warnings for this
pattern:Rust eliminates this class of errors by design:Rust allows multiple enum types to have the same variant names within
the same module, while Haskell's constructor names must be unique
within their scope. This leads to different patterns in the two
languages.The same approach in Haskell would cause a compile error:In Haskell, you need unique constructor names:Alternatively, Haskell developers often use qualified imports syntax
to achieve similar namespacing:The  syntax in Rust makes the intent clearer at the
usage site. You immediately know which enum type you're working with,
while Haskell's approach can sometimes require additional context or
prefixing to achieve the same clarity.Rust provides granular visibility control for struct fields, allowing
you to expose only specific fields while keeping others private. This
fine-grained control is built into the language:Rust offers even more granular visibility control beyond simple 
and private fields. You can specify exactly where a field should be
accessible:These granular visibility modifiers (, ,
) allow you to create sophisticated access patterns that
match your module hierarchy and architectural boundaries. Some of
these patterns are simply not possible to replicate in Haskell's
module system.In Haskell, record field visibility is controlled at the type level,
not the field level. This means you typically either export all of a
record's fields at once (using ) or none of them. To achieve
similar granular control, you need to use more awkward patterns:The Haskell approach requires writing boilerplate accessor functions
and losing the convenient record syntax for the fields you want to
keep private. Rust's per-field visibility eliminates this awkwardness
while maintaining the benefits of direct field access for public
fields.Purity and Referential TransparencyOne of Haskell's most significant strengths is its commitment to
purity. Pure functions, which have no side effects, are easier to
reason about, test, and debug. Referential transparency—the principle
that a function call can be replaced by its resulting value without
changing the program's behavior—is a direct benefit of this purity.In Haskell, the type system explicitly tracks effects through monads
like , making it clear which parts of the code interact with the
outside world. This separation of pure and impure code is a powerful
tool for building reliable software.While Rust encourages a similar separation of concerns, it does not
enforce it at the language level in the same way. A function in Rust
can perform I/O or mutate state without any explicit indication in its
type signature (beyond  for mutable borrows). This means that
while you  write pure functions in Rust, the language doesn't
provide the same strong guarantees as Haskell.This lack of enforced purity in Rust means you lose some of the strong
reasoning and refactoring guarantees that are a hallmark of Haskell
development.Rust's explicit error handling through  removes the
cognitive overhead of exceptions. Compare these approaches:Haskell (with potential exceptions):Rust (explicit throughout):In Rust, the  operator makes error propagation clean while keeping
the flow clear.Unit tests as part of source codeRust's built-in support for unit tests within the same file as the
code being tested is convenient:In Haskell, tests are typically in separate files:Rust's co-location makes tests harder to forget and easier to
maintain. Additionally, in Haskell you often need to export internal
types and functions from your modules just to make them accessible for
testing, which can pollute your public API.Rust's  attribute means test code has access to private
functions and types without exposing them publicly.Rust's  provides a standard formatting tool that the entire
community has adopted:In Haskell, while we have excellent tools like  and
, the lack of a single standard has led to configuration
debates:I have witnessed significant time spent on style discussions when team
members are reluctant to adopt formatting tools.While Haskell Language Server (HLS) has improved significantly, it
still struggles with larger projects. Basic functionality like
variable renaming can fail in certain scenarios, particularly in
Template Haskell heavy codebases.rust-analyzer provides a more reliable experience across different
project sizes, with features like "go to definition" working
consistently even in large monorepos. One feature I'm particularly
fond of is rust-analyzer's ability to jump into the definitions of
standard library functions and external dependencies. This seamless
navigation into library code is something I miss when using HLS, even
on smaller projects, where such functionality is not currently
possible.Another feature I extensively use in rust-analyzer is the ability to
run tests inline directly from the editor. This functionality is
currently missing in HLS, though there is an open issue tracking
this feature request.Despite Rust's reputation for slow compilation, I've found it
consistently faster than Haskell for equivalent services. The Rust
team has made significant efforts to optimize the compiler over the
years, and these improvements are noticeable in practice. In contrast,
Haskell compilation times have remained slow, and newer GHC versions
unfortunately don't seem to provide meaningful improvements in this
area.Interactive development experienceI appreciate Haskell's REPL (Read-eval-print loop) for rapid
prototyping and experimentation. Not having a native REPL in Rust
noticeably slows down development when you need to try things out
quickly or explore library APIs interactively. In GHCi, you can load
your existing codebase and experiment around it, making it easy to
test functions, try different inputs, and explore how your code
behaves.As an alternative, I have been using org babel in rustic mode for
interactive Rust development. While this provides some level of
interactivity within Emacs, it feels more like a band-aid than an
actual solution for quickly experimenting with code. The workflow is
more cumbersome compared to the direct approach of typing expressions
directly into GHCi and seeing results instantly.Lists as first-class citizensHaskell's treatment of lists as first-class citizens through special
syntax can be problematic. The convenient  syntax defaults to
linked lists:Better alternatives exist but require more verbose syntax:While Haskell has the  extension to make this more
convenient, it hasn't enjoyed the same widespread adoption as
.Rust also has  in its standard library, but it comes
with clear documentation discouraging its use:Configuration file experienceCabal and TOML represent different approaches to project
configuration. While cabal's tool support has improved greatly with
HLS integration, which now supports features like automatic formatting
using cabal-gild, TOML enjoys broader ecosystem support.TOML benefits from independent language servers like taplo and
the recent tombi, providing a superior editing
experience. Features like jumping to different Cargo.toml files across
a workspace or seeing descriptions of specific Cargo.toml fields on
hover work seamlessly.Additionally, TOML has upstream support in editors like Emacs through
toml-ts-mode, which takes advantage of tree-sitter. No equivalent
exists for cabal files, likely due to the format's limited popularity
outside the Haskell ecosystem.It's worth noting that when Cabal was created, there was likely no
widely adopted standard configuration format available. Rust had the
benefit of coming later and choosing an existing, well-established
format rather than inventing something new.I handle production operations for both Rust and Haskell services,
including parts of the Stackage infrastructure at FP Complete (which
FP Complete later donated to the Haskell Foundation). I have
encountered significantly more operational challenges with Haskell
services.Haskell services often require tweaking GHC's RTS (Runtime system)
parameters to avoid memory issues and achieve stable performance. This
remains an ongoing challenge - recently, Bryan from the Haskell
Foundation team opened a similar issue on the Stackage server,
encountering the same type of issue I dealt with. In contrast, Rust
services have been much easier to operate.Building static binaries is trivial in Rust, and the ecosystem
actively works to reduce non-Rust dependencies. Recent efforts like
the bzip2 crate switching from C to Rust further simplify the
build process by removing C library dependencies wherever possible.Cross-compilation to ARM is particularly straightforward. For one of
our clients, we run all production services on ARM
architecture. Setting up the entire cross-compilation pipeline took
less than a day. I would not be confident about achieving the same
with GHC and the various commonly used dependencies in Hackage.The build process is quite simple:My experience has been that Rust services typically consume less
memory and CPU cycles than the Haskell ones in general.Both languages have their place in the software development
ecosystem. However, I find that Rust's larger user base brings
tangible benefits through more robust tooling and actively maintained
libraries. I used to reach for Haskell when writing CLI tools,
appreciating its expressiveness. These days, I find myself choosing
Rust instead, largely due to how polished libraries like  have
become for command-line parsing.While I appreciate that Haskell's theoretical foundations remain
strong for exploring effect systems and advanced type-level
programming, I observe that its library ecosystem has become less
active compared to Rust's rapidly evolving landscape.For the web services and API development I work on, I find that Rust's
practical decisions around error handling, tooling, and operational
aspects often result in more maintainable and deployable systems.
While language choice always depends on numerous factors like team
expertise and project requirements, Rust's powerful combination of
design and ecosystem makes it a compelling option. For me, it has
become the pragmatic choice for building reliable software,
specifically when a greenfield project is involved.
      Subscribe to our blog via email
    Email subscriptions come from our Atom feed and are handled by Blogtrottr. You will only receive notifications of blog posts,
      and can unsubscribe any time.Do you like this blog post and need help with Next Generation Software Engineering, Platform
    Engineering or Blockchain & Smart Contracts? Contact us.]]></content:encoded></item><item><title>Heap Management with Go &amp; Cgo</title><link>https://www.reddit.com/r/golang/comments/1loqmgt/heap_management_with_go_cgo/</link><author>/u/winwaed</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 02:41:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I think I know the answer, but a bit of a sanity check,,,I'm a relative Go Newbie. We have a Go app running in a Docker (Ubuntu) container. This calls a C/C++ library (C interface, but C++ under the hood) via cgo. Yes I am aware of the dangers of that, but this library depends on a 3rd party C++ library and uses x64 intrinsics. The 3rd party library is slowly being ported to Go but it isn't ready yet for prime time; and of course there's the time to port our library to Golang: I've seen worse, but not trivial either!Memory allocation is a potential issue, and I will investigate the latest GC options. Most of the memory allocation is in the C++ library (potentially many GB). Am I right in thinking that the C++ memory allocation will be separate from Golang's heap? And Golang does not set any limits on the library allocations? (other than OS-wide ulimit settings of course)In other words, both Golang and the C++ library will take all the physical memory they can? And allocate/manage memory independently of each other?]]></content:encoded></item><item><title>TIL features can&apos;t be used like this and I think it&apos;s nuking my incremental build times</title><link>https://www.reddit.com/r/rust/comments/1loqbol/til_features_cant_be_used_like_this_and_i_think/</link><author>/u/Tiflotin</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 02:26:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm making a mmorpg that has several shared types between client and server. I have 3 crates, client, server, shared.Item, is one of those types for example. Item has 2 fields `id` and `amount`. I want to change the type of the field based off what crate is using the shared crate. Eg the client, the `id` field should be of type u16. But in the server, I want the `id` type to be a `ItemId` (enum repr u16 type for item constants). The client can't use the same type since `ItemId` wont always be up to date with the latest items in game (we can add items without releasing new client updates).This is what I've got so far, and its fine when building the specific crate (eg client or server) but if u try to build the workspace itself, it fails.pub struct Item { pub id: ItemIdType, } #[cfg(not(feature = "server"))] pub type ItemIdType = u16; #[cfg(feature = "server")] pub type ItemIdType = String; Example use of client with u16 type: ```rust use shared::Item;fn main() { // Client project, NO server feature let test = Item { id: 123 }; println!( "We are a u16 since we're NOT using the server feature! {:?}", test.id ); } ```Example use of server with String type: ```rust use shared::Item;fn main() { // Server project, YES server feature let test = Item { id: String::from("Hello"), }; println!( "We are a String since we ARE using the server feature! {:?}", test.id ); } ```My issue is when running cargo build in the workspace, it gives an error saying client was failed to build due to incorrect item id type. But if I run cargo build inside the client directory it works fine. error[E0308]: mismatched types --> client/src/main.rs:5:27 | 5 | let test = Item { id: 123 }; | ^^^- help: try using a conversion method: `.to_string()` | | | expected `String`, found integer I don't really understand why this isnt valid, and not quite sure how to achieve what I want without duplicating the struct inside server and client. It's got me stumped!]]></content:encoded></item><item><title>Is Tech Schools Backend MasterClass outdated or worth it?</title><link>https://www.reddit.com/r/golang/comments/1looc0q/is_tech_schools_backend_masterclass_outdated_or/</link><author>/u/stewrat1</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 00:47:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am starting to learn Go and I found this course from Tech School: What interested me about this course was the AWS, Docker and Kubernetes usage in it too- as it seems quite industrious and valuable. My only concern is if it is outdated as I saw on YouTube, the original series was made 5 years ago. Anyone take this course recently or have other suggestion for learning? ]]></content:encoded></item><item><title>[D] best chunking method for financial reports?</title><link>https://www.reddit.com/r/MachineLearning/comments/1loob3z/d_best_chunking_method_for_financial_reports/</link><author>/u/Wickkkkid</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 00:46:39 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey all, I'm working on a RAG (Retrieval-Augmented Generation) pipeline focused on financial reports (e.g. earnings reports, annual filings). I’ve already handled parsing using a combo of PyMuPDF and a visual LLM to extract structured info from text, tables, and charts — so now I have the content clean and extracted.My issue: I’m stuck on choosing the right chunking strategy. I've seen fixed-size chunks (like 500 tokens), sliding windows, sentence/paragraph-based, and some use semantic chunking with embeddings — but I’m not sure what works best for this kind of data-heavy, structured content.Has anyone here done chunking specifically for financial docs? What’s worked well in your RAG setups?Appreciate any insights 🙏]]></content:encoded></item><item><title>[D] How far are we from LLM pattern recognition being as good as designed ML models</title><link>https://www.reddit.com/r/MachineLearning/comments/1loo8yl/d_how_far_are_we_from_llm_pattern_recognition/</link><author>/u/chrisfathead1</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 00:43:43 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? ]]></content:encoded></item><item><title>AerynOS: Mid-year update</title><link>https://aerynos.com/blog/2025/06/30/mid-year-update/</link><author>/u/NomadicCore</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 00:09:40 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[As we hit the middle of the year, it’s time for another update for those of you following along with AerynOS’s development.Over the last few months, things may have seemed unusually quiet, however rest assured that there has been A LOT going on in the background. As such, we are preparing a short series of blog posts to go over the relevant topics in the coming weeks.For this blog post, we are going to cover our infrastructure port, along with the process of rebuilding our entire package repository.All core AerynOS tooling is now written in RustEvery recipe in the repository has been rebuilt (twice!) with many packages then having been updated to newer versions after the rebuilds were completedA CDN has been implemented for faster package installation and ISO downloadsWhen delivering a Linux distribution, its infrastructure and associated processes effectively act as the “spine” of the project. But spine surgery can be a delicate affair, particularly when it comes to rehabilitation after successful surgery.For us, this cycle has been particularly demanding, as we have completed an MVP (Minimum Viable Product) port of our infrastructure tooling code to Rust, meaning that all core AerynOS tooling has now fully transitioned away from DLang.We have covered the reasons for this transition previously, and it’s fair to say that we are already feeling the benefits of easy and native reuse of code in our tooling repositories and welcoming more Rust contributors into our community.Earlier this year, our existing DLang build infrastructure started showing signs of instability and required more and more manual intervention to successfully land packages.Given our prior decision to transition our tooling over to Rust, we had already stopped further development of the DLang based infrastructure. Hence, we decided to accelerate our transition timeline for the infrastructure re-write to Rust with tarkah and ermo leading the development activity, which began at the end of March.Towards the end of May, we put the first infrastructure prototype to the test, and then iteratively fixed bugs and built out missing functionality to the point of being able to put our MVP into production on our build infrastructure.This MVP will serve as the development base of the code that will be used for all future package builds.We have some cool features planned in AerynOS that we envision will make package maintenance a lot easier to manage through smart use of automation.Until these features are implemented, however, maintaining the AerynOS repository will remain somewhat human resource intensive. This is the main reason why the repository is consciously being kept “small” for now, with us deliberately focusing on having packages that will help developers and contributors improve AerynOS, while still delivering a nice Daily Driver experience.Until the new features are implemented, this will necessarily be a balancing act between maintaining the package repository so it doesn’t go stale vs. having the development time to implement the new features.Aside from porting the infrastructure code to Rust, proper testing was required to yield confidence that packages were both successfully built on the new infrastructure and that they worked as expected.The end goal was to prove that we were able to rebuild the full AerynOS recipes repository (currently at ~950 recipes) from start to finish without infra-related build errors on the new infra.To enable the rebuild, ermo set up a distributed build cluster of four builders of varying hardware specifications. A separate branch of the ‘recipes’ repository was created, and was used to test both the Rust infrastructure and to land packages for internal testing without them being seeded to user installs.In addition, compared to the old infra, we made it simpler to add new avalanche build agents to the build cluster, thus making it very simple to scale out our build cluster as required.To summarise the infrastructure Rust re-write and testing effort, we have:Completed more than 3k recipe buildsDeployed the new Rust infrastructure on the AerynOS builders and continue to use it on ermo’s build clusterValidated that the new infrastructure code is more stable and performant at runtime than the previous DLang versionHow did testing add value?The full rebuild of the recipes repository has also served to ensure ABI sanity for dependencies. Additionally, we can now say that at this point in time, the whole AerynOS repository is known to be buildable and works with all the latest toolchains.A special thanks goes to Reilly Brogan, who worked diligently with ermo to not only drive the rebuild process, but also to ensure that some longstanding repository issues were corrected as part of the rebuild process.During this process, we have delivered updates to our os-tools (Boulder and Moss), toolchains and build systems. A selection of the updates and additions include (but is certainly not limited to):Linux 6.14.11 (6.15.x on the way)Distrobox added at v1.8.1.2Exfatprogs added at v1.2.9As mentioned earlier, the testing work was conducted on a separate branch of the recipes repository. Consequently, those of you on the old packages.aerynos.com/volatile/ repository, have not received any updates over the last 10-12 weeks.This was a conscious decision to ensure that the mostly untested packages built during the infrastructure testing process did not reach end users immediately. Even though AerynOS is in Alpha and under continuous development, we still do our best not to break user systems if we can avoid it!Now that we have a level of testing in place, with this blog post, we are announcing a new rolling  package repository for users. The old  package repository has received one final update to Moss that fixes an important bug when transitioning to the new  repository.To ease the transition to the new repository for existing users, we are working on a script that can automatically modify the active repository on the system.Once this script has been sufficiently productized, the next time existing users update their systems, they will notice that every single package will show an update available.The exact number will vary from system to system depending on how many other packages are installed from the repository but for context, on a base AerynOS GNOME install, this is around 500 packages.In the meantime, we have created a manual guide on how to transition existing installs to the new repository in our GitHub Discussions forum here. The process is fairly simple, but if you do have any issues transitioning manually, do get in touch via a comment under the GitHub Discussions post or via Matrix.Content Delivery Network for Packages and ISOsA common bit of feedback we have been receiving relates to the download speed of our repository, namely that it is not fast or even acceptable, especially if you live outside of Europe. This became more evident for those using the rebuild repository on ermo’s rebuild testing server, which felt noticeably faster for people in Europe in particular.To remedy this, we have implemented CDN caching for our new  hosted assets. This means there will be synced copies of our ISOs and package repository on CDN servers around the world, which should help improve download speeds.In particular, the new rolling  package repository mentioned above will be served via this CDN for the benefit of our users.Please let us know how you get on with AerynOS ISO and package downloads in the coming weeks, as we would love to validate the improvement outside of our own internal testing.Future infrastructure development targetsSo far, we have only outlined what we have already accomplished since late March.The next part of this blog post is going to be a brief outline of where we are going from here in terms of infrastructure and repository development.With the transition to the new infrastructure and the new  repository, we have been freed up to begin planning out the necessary steps to be able to deliver versioned repositories and versioned Moss format upgrades.These topics have been mentioned in a previous blog post.How do versioned repositories add value?Versioned Repositories will enable us to deploy new Boulder and Moss features in a seamless fashion. This will enable us to introduce breaking code and on-disk format changes, that would otherwise cause installed systems to require manual intervention for them to continue to receive updates.Once versioned repositories are in place, the goal is that users will be able to simply update and sync their system as normal via the  command.Users will be upgraded to the new versions of Moss that uses a new repository format, without having to pay special attention.It will enable AerynOS to iteratively expand the capability of Moss and Boulder on existing systems without breaking user systems in the process.We consider versioned repositories a pre-requisite for what we call “try-builds” and eventually multi-arch support.Automated try-builds denotes the process whereby the infrastructure discovers an update to the upstream source repository of a package, attempts to auto-update the recipe and then attempts to build the updated package recipe in question.We think this will be a useful tool for contributors as it will automate some of the packaging tedium related to simple package version updates. It will also help enable automated regression testing and build flag optimisation in a future workstream.Included under the multi-arch umbrella is our ability to target ARM, RISC-V, and different x86 architecture levels such as x86-64-v3 or v4.Within the previous 3 month period, we have rebuilt a brand new Rust version of the infrastructure tooling that is robust enough to run in production on AerynOS servers, delivering packages to our contributors and users. This new version has proven to be more stable and performant than the old DLang version we were previously using.From a day to day perspective, unlocking the infrastructure means that we can get back to reviewing and landing recipe PRs for our package maintainers or accepting new contributors into our AerynOS ecosystem. For those wishing to contribute to AerynOS, please make sure that you have manually switched over to our new repositories before making submissions to ensure you are using all the latest tooling.Alternatively, you can wait until the automatic transition script is functional and have it make the change for you.Where to get in touch with usIf you want to engage with the team, feel free to drop by our GitHub Discussions, raise issues across our various repositories or if you’re interested in contributing, feel free to raise PRs where you think our code can be improved or where you want to submit recipes for our repo.We also have our matrix space that you can access via this link:The Development room in particular is a great place for discussions around our code.The General room is a great place to drop by and get to know the team.The Packaging room is where you want to be if you’re interested in building packages for yourself and/or submitting them to the repository.Concurrently to our work around the infrastructure re-write and repository rebuild, there has been several additional workstreams running in the background.The team has been refactoring our existing Rust code, mainly focused on our os-tools (Moss and Boulder) and we are working on several additional improvements that we want to get over the finish line before our next ISO release.We will be sharing details of this work in upcoming blog posts over the next few weeks.]]></content:encoded></item><item><title>Getting started with Go</title><link>https://www.reddit.com/r/golang/comments/1lonfsi/getting_started_with_go/</link><author>/u/Brunoo_1013</author><category>reddit</category><pubDate>Tue, 1 Jul 2025 00:04:30 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I have been programming for a while now, and I have built some projects including an IRC server in C++. Back then I had to choose between an IRC or web server, but now I wanted to learn Go and thought of building a web server as a way to start learning Go. This would allow me to explore how HTTP works and get started in the language.Would this be a good idea, or should I start smaller and learn basic concepts first? If so, what specific Go concepts should I look into?]]></content:encoded></item><item><title>Writing Code Was Never The Bottleneck</title><link>https://ordep.dev/posts/writing-code-was-never-the-bottleneck</link><author>/u/ordepdev29</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 23:21:05 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[For years, I’ve felt that writing lines of code  the bottleneck in software engineering.The actual bottlenecks were, and still are, ,  through mentoring and pairing, , , and the human overhead of coordination and communication. All of this wrapped inside the labyrinth of tickets, planning meetings, and agile rituals.These processes, meant to drive quality, often slow us down more than the act of writing code itself because they require thought, shared understanding, and sound judgment.Now, with LLMs making it easy to generate working code faster than ever, a new narrative has emerged: that writing code  the bottleneck, and we’ve finally cracked it.But that’s .The marginal cost of adding new software is approaching , especially with LLMs. But what is the price of , , and  that code? .LLMs shift the workload — they don’t remove itTools like Claude can speed up initial implementation. Still, the result is often more code flowing through systems and more pressure on the people responsible for reviewing, integrating, and maintaining it.This becomes especially clear when:It’s unclear whether the author fully understands what they submitted.The generated code introduces unfamiliar patterns or breaks established conventions.Edge cases and unintended side effects aren’t obvious.We end up in a situation where code is more straightforward to produce but more complex to verify, which doesn’t necessarily make teams move faster overall.It’s not a new challenge. Developers have long joked about , but the velocity and scale that LLMs enable have amplified those copy-paste habits.Understanding code is still the hard part“The biggest cost of code is understanding it — not writing it.”LLMs reduce the time it takes to produce code, but they haven’t changed the amount of effort required to reason about behavior, identify subtle bugs, or ensure long-term maintainability. That work can be even more challenging when reviewers struggle to distinguish between generated and handwritten code or understand why a particular solution was chosen.Teams still rely on trust and shared contextSoftware engineering has always been collaborative. It depends on , , and . However, when code is generated faster than it can be discussed or reviewed, teams risk falling into a mode where quality is assumed rather than ensured. That creates stress on reviewers and mentors, potentially slowing things down in more subtle ways.LLMs are powerful — but they don’t fix the fundamentalsThere’s real value in faster prototyping, scaffolding, and automation. But LLMs don’t remove the need for , , and . If anything, those become even more important as more code gets generated.Yes, the cost of writing code has indeed dropped. But the cost of making sense of it together as a team .That’s still the bottleneck. Let’s not pretend it isn’t.]]></content:encoded></item><item><title>What is the purpose of setting the container port field?</title><link>https://www.reddit.com/r/kubernetes/comments/1lomcdg/what_is_the_purpose_of_setting_the_container_port/</link><author>/u/MaxJ345</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 23:14:29 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[apiVersion: v1 kind: Pod metadata: name: mysql-server spec: containers: - name: mysql image: mysql:8 env: - name: MYSQL_ROOT_PASSWORD value: "..." ports: - containerPort: 3306 Even if I remove the  section, everything will work just fine. The MySQL database server will continue listening on port 3306 and function without issue.I'll still be able to reference the port using a service:apiVersion: v1 kind: Service metadata: name: mysql-service spec: selector: ... ports: - protocol: TCP port: 12345 targetPort: 3306 type: ClusterIP I'll still be able to access the database via port forwarding:kubectl port-forward pod/mysql-server --address=... 55555:3306 So what is the purpose of setting the container port field?]]></content:encoded></item><item><title>DarkDiskz – a simple open-source Linux GUI for disks, RAID, bcache, and SMART (early version, feedback welcome!)</title><link>https://www.reddit.com/r/linux/comments/1lom3vr/darkdiskz_a_simple_opensource_linux_gui_for_disks/</link><author>/u/Dark_ant007</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 23:04:01 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I wanted to share a project I’ve been working on called .It’s an open-source Python/GTK4 GUI tool that combines several disk-related utilities in one place. The goal is to make it easier to see drive information and manage storage setups without juggling a bunch of separate commands.View detailed disk information (, )Set up and monitor bcache⚠️ Important Notice (Please Read): This is an early project by an amateur coder, so:Some functions may not work perfectly.You could lose data if you use destructive operations like wiping drives or re configuring RAID.💡 Please back up all important data before testing or using any of the write/format functions. Use at your own risk. I’m not much of a programmer—this is my first serious attempt at making something useful for the Linux community. I’m hoping others might try it out, give feedback, report issues, or even contribute improvements. I probably wont change or edit the program any farther maybe the community enjoys this I hope so. If you’re interested, I’d really appreciate:Testing on different distros (I did all testing on Linux Mint)Bug reports and suggestionsContributions to help make it better and more reliableThanks for taking the time to check it out!]]></content:encoded></item><item><title>I made my VM think it has a CPU fan</title><link>https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 22:52:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>At what age did you guys instal Linux?</title><link>https://www.reddit.com/r/linux/comments/1lolpeb/at_what_age_did_you_guys_instal_linux/</link><author>/u/angelaanahi</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 22:46:32 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hi guys! A reel I saw on Instagram made me notice that a lot of people installed their first Linux distro when they were 12, I also installed it when I was 12 (Ubuntu 10), so I was generally curious on this, at what age did you install Linux? And why? ]]></content:encoded></item><item><title>[R] BIG-Bench Extra Hard</title><link>https://arxiv.org/abs/2502.19187</link><author>/u/EducationalCicada</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 22:41:39 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tips &amp; Tricks—Securing Kubernetes with network policies</title><link>https://www.reddit.com/r/kubernetes/comments/1lolcao/tips_trickssecuring_kubernetes_with_network/</link><author>/u/wineandcode</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 22:30:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Understanding what each network policy does individually, and how they all work together, is key to having confidence that only the workloads needing access are allowed to communicate and that we are are restrictive as possible, so if a hacker takes control of a container in our cluster it can not communicate freely with the rest of the containers running on the cluster. This post by Guillermo Quiros shares some tips and tricks for securing kubernetes with network policies:]]></content:encoded></item><item><title>React Still Feels Insane And No One Is Talking About It</title><link>https://mbrizic.com/blog/react-is-insane/</link><author>/u/mbrizic</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 22:13:58 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Recently, I did a side project that I wrote about in the other post. As part of it, I had what was supposed to be just a few paragraphs on how React sucks - but I just couldn't stop writing about it.So here it is a full, standalone blog post, even bigger than the one it sprang from, all about how React sucks. And how it might not even be its own fault.In my junior days, down on the streets, I used to do Angular.JS for money. At the time, it was a seriously good piece of technology. Definitely the biggest JS framework of its time, and what's most important to its legacy, probably the first time web development has had a "framework". Prior to that, they were all "libraries", so this was the first one that gave not only you a set of functions to use, but the actual framework in which you built your web app.But things are always only good in relative, and Angular was good because it's predecessors were not. At the time, we had other SPA frameworks like Backbone and Knockout, but they didn't leave as much of an impact. No, the real enemy that Angular had beaten was jQuery.Even though jQuery was only a wrapper over (at the time admittedly very shoddy) HTML DOM APIs, it still became a de-facto standard if you wanted to build complex web applications. How it worked was pretty straightforward: you create HTML elements in JS, manually and imperatively, then modify them, move them around, do whatever it takes to make the website interactive like it's an app.This all works completely fine for simple apps, but you can imagine it becoming a maintenance nightmare in case of anything bigger. And that is exactly what started happening around. You can't really blame jQuery, but only the appetites of modern users who needed that kind of interactivity everywhere. So developers were blindsided to keep using jQuery even though it was not a good fit for the job anymore.Then Angular arrived and had it all sorted out. You could focus your energy on writing the UI and app logic instead of manually assembling individual pieces of HTML. It truly was a game-changing  framework, as you finally had a proper tool to make  interactive applications. Some magic things it had:A) Components. Ok, it had a weird naming so these were actually called "directives", but in any case you could define a simple HTML and JS file combo representing a piece of UI and then reuse it across multiple places in the app.B) Two-way binding. You define a variable, and whenever it changes, all the places in the UI are updated. This worked really well. Later, people started nagging that this omnidirectional data flow is bad, so there was a push to use one-way (top-bottom) bindings instead, which does sound technically better, but in practice made everything more complicated and started a strand of discussion which ended with us all having to use Redux today. So thanks.On my first job I worked on exactly one such rewrite of an huge, unwieldy jQuery app into an Angular app. Both the process and the end results were pretty good.What was not good, though, was having to rewrite the exact same screens in Angular 2 a few years later, and I'm just happy I left that company early enough before they made me rewrite it for the third time in React later. I did get a chance later to get to learn React and even use it professionally on a project or two.I still remember seeing how fresh it looked on first glance. At the time, the contrast was with the framework of the day, which was Angular 2 - a complete rewrite of the original, but now with twice the boilerplate, Typescript-out-of-the-box, one-way binding, reactive/observable patterns - all good things on their own, but god damn was it complicated, slow to work on, slow to build, slow to run.React swung the pendulum back to the simplicity and people were up all for it. And for a while, simplicity remained, React gained popularity to become the #1  for making SPAs.Yes, now we were using the term "library" again, showing how simpler it really was. But you can't reasonably build a complex app with just a library. You need few of them to handle all of the app's concerns, and you also need some code structure. React's "bring your own beer" approach meant you basically built a framework yourself, with all the downsides it had.The end result - no two React apps were the same. Each of them had a bespoke "framework" built out of random libraries found on the internet.The apps I had the misfortune to work on at the time all made me think the same thing - even Angular 2 would be better than The JSX "core" always seemed solid, but everything around it was just plain mess.So I got out and went writing some Java backends, which I believe says it all.They say a man can never really learn anything - you either know something or you don't. I apparently don't, so I dragged myself back into React recently.Granted, it was a hobby project, so I didn't experience it "in full" like I would if it was a serious production app. But still, even this experience both confirmed and greatly exceeded my low expectations for it. React feels insane and I don't know how no one else is talking about it.First, let's start with the architecture React enforces for you. As said before, React is only a library, so it's not forcing you on anything, but still, the implicit constraints of having JSX make some patterns surface on their own. Eons ago, we used to talk about MVC, MVVM, MVP, all of which only a variations on the same theme, so which one is React? None, I believe this is a new-ish paradigm - I think we could literally call it "components-based architecture".On first glance, it's all logical. You have components, you build a top-down tree of them, and bam there's your app. React does some internal magic to make sure it's up to date with the data you give it. Simple enough.But sometime along the way, it all started acting smarter than it should really be. For a simple "UI library", React sure has a lot of loaded terminology. And for a library that doesn't have anything to do with "functional programming", it sure has a lot of functional programming names inside.Let's start from the state. If you have a top-down tree of components, it's logical you'd want to pass the state top-down too. But in practice, with components very numerous and small, this is very messy, as you spend a lot of time and code just wiring the various pieces of data to get them where you need them.This was solved by "sideloading" state into components using React hooks. I haven't heard anyone complain about this, but are you guys serious? You're saying that any component can use any piece of app state? And even worse, any component can emit a state change, that can then update in any other component.How did this ever pass a code review? You are basically using a global variable, just with more elaborate state mutation rules. They're not even rules, but merely a ceremony, because nothing is really preventing you from mutating state from anywhere. People really think if you give something a smart name like a reducer it suddenly becomes Good Architecture™?So if both top-down and sideloading approaches suck, what would be the solution for this? I honestly don't know. In fact, the only thing I can think of is: if we can't solve this nicely, then maybe the entire "components architecture" was a mistake and we shouldn've called it a paragon of Nice Design and stopped innovating. Maybe this time for a change we really did need yet another JS framework that would try something better.Next on in the "things we're unsure how they passed a code review", let's riff on React Hooks. There's no denying they're useful, but their existence even to this day raises question marks above my head.I won't even mention how people talk about components as "pure functions" but then have hooks as a tiny stateful black boxes inside of them. And given their composable nature, it's more like layers and layers of tiny stateful black boxes. But no, I'd mostly like to roast  here. It's simple what a "side effect" would be. You change a state and then you need to do some external action, like post the results to an API. This split between the "important app stuff" and "side effects" makes sense - in theory. But in practice, can you ever split it cleanly like that?My biggest gripe, for starters, is that  is used as a "run something after the component mounts". I understand when React migrated from classes to hooks, this was the closest alternative to , but come on - how is this not considered a huge hack? You're using a "side effect" hook to  the component? Ok, if you have to make an API call from there, I'd agree that would be a side effect. But then that API call... it... it sets the state too. So a completely innocous "side effect" hook actually manages a state of the component. Why is no one talking about how crazy this is?Moreover, if you wanted to depend on that state and do something after it, then you... you... define yet another  with a dependency on what the first one sets. This is a code that I have taken from a production app of company recently acquired for several tens of millions of US dollars. I slightly redacted it here to use a simpler  and  entities instead of what is actually there. But go take a look and try to parse in which order is this code executed. When you're ready, the answer is in an image below:So something like that, a series of state mutations that would otherwise be a simple imperative code is now... spread out across two asynchronous functions, where the only hint of the order of their execution is the "dependency array" at the bottom of each. And the way that you actually mentally parse it is, in fact, from the bottom to the top.I remember how Javascript promises were considered unwieldy with their s, and even before them we had "callback hell" - but literally anything would be better than this. I understand these issues could be solved a) by moving them into a separate file, which is just hiding the problem, or b) probably with some Redux or something, but I really don't have enough mileage with it to know for sure. All of this combined looks ugly, and betrays the simplicity that React promised in its "Hello world" example. But wait, I'm not done yet. I read a blog post from an acquaintance called "The Most Common React Design Patterns". Expecting I don't know what, I was still shocked at how complicated these are and how much mental overhead there is to simply figure out what is happening - and all of that just to render a list of items on a screen.The most jarring thing: the article doesn't even acknowledge it. All this complexity is taken from granted. People apparently really build their UIs like this and no one bats an eye.Then, as if that isn't enough, some of you go as far as to write "CSS-in-JS", and then get paid for it. I agree that JSX initially showed that "separation of concerns" is not "separation of files" and that it's actually okay to write your HTML and JS in a same file. But chucking in CSS in there too and making it strongly typed? Isn't this a step too far? It would be too easy to just say React is, well, downright insane, and go on with our lives. But as reasonable primates, I believe we can do better. We can try to understand it.I am once again trippin' down the memory superhighway to get reminded of my first job and a colleague from the mentioned "jQuery migration" project. A super-experienced backend engineer, an architect type, and overall a very respected guy when it came to all things software.What I remember the most about him are not his technical solutions, but the amount of judgement he'd have shown on anything we did on frontend. Looking anything on the Angular app, he was like - what the hell are you guys doing here? Why does this have to be so complicated?And it's not that we sucked - we too were a no-nonsense crew about software. It's just that at the time, through the eyes of a classical backend developer, the entire Angular setup seemed absolutely insane.Today, I'm roughly the same age as he was then, and I am here writing a blog post about how  React is insane. Some things are inevitable, I guess.But let's rise a step above and try to understand why it could be so.Firstly, I think we can all agree that most web apps shouldn't even be web apps in the first place. People go the way of SPA even if they don't need a SPA but they might need it later, so it apparently doesn't cost as much to go with SPA from scratch.But I'd argue here that such a move, in fact, does cost you. It's just that we're so entrenched in the "SPA-by-default" way that we forgot how simpler the alternatives are. Having a simple dumb server-side-rendered page is orders of magnitude simpler than even thinking about React. There's no overhead with API communication, frontend is very lightweight, your UI code can be strongly typed (if your backend is strongly typed), you can do refactors across the full stack, everything will load faster, you can cache it better because some components are very static and remain the same for all the users so you can render them only once, etc, etc.You do lose the flexibility to have complex interactive logic at your product manager's whim, though. But that's maybe only partially true, because I'd wager you could go a pretty long way with just plain Javascript "progressive enhancement" before you really get to a state management complex enough to warrant adding React in there.Ok, so I'm saying we use React simply because we've used it before. No wonder, inertia is a hell of a drug, but it still doesn't explain why this code ends up being so unthinkably complex.My answer to that question, surprisingly, stops roasting React and goes the opposite way, defending not only React, but also Angular and jQuery and everything that came before them. I think this code is bad because making a interactive UI where any component can update any other component is simply one of the most complicated things you could do in software.Think of any other system you use in your everyday life. Your kitchen sink has two inputs, hot and cold, and one output, a water running. Your kitchen mixer or a power drill might have a button or two, and still whatever you do, it only affects the action on the spinning part. An oven might have three or four or five knobs and maybe the same number of outputs, and already that is starting to sound pretty dangerous.In contrast, an interactive UI that we have on web can have potentially infinite number of inputs, and potentially infinite number of outputs. How could you even expect to have a "clean code" for this?So, this entire rant about React... it's not even React's fault. Neither is Angular's, or jQuery's. Simply, whichever tech you choose will inevitably crumble down under the impossible complexity of building a reactive UI.How could we fix this? I'm not smart or in-the-weeds enough to really solve this problem, but I can spitball some ideas. If we adopt this input/output mental model of a webpage as a real thing, then maybe we could start working on reducing a number of its inputs and outputs. On the inputs side, yeah, this is me saying: "go have fewer buttons", which may not always, or ever, be enforceable. But certainly, the less features you have, the more manageable your codebase is. It's straightforward enough to need no mentioning - or is it? Do product managers know that adding three buttons instead of two will cause 5% more bugs and make any future work on that screen 30% more complicated to design and implement? No one is even measuring those things, but I believe they could be true. Why is it that, if I told you we need to add Redis on backend, you will tell me "no, we need to curb the technical complexity" - but if a product manager asks to add a global app-wide filter that could be applied from anywhere and to anything, you'd just get your head down and write some monstruosity that people will spend the next 10 years trying to get out.In short - please, stop adding so many buttons, I beg you. You could even, I know, crazy, try to remove some of them?On the outputs side, however, the story is a bit different. Writing this makes me realize that having a server-side rendered page is basically reducing the page to a single output. Anything you interact with, it just rebuilds the entire page. This means that, ironically, removing FP-inspired React from the mix makes a server-side rendered page an actually a pure function of the state. No frontend state = big simplicity wins, if you could afford it.Inevitably, when you do need some scripting logic in your server-side rendered "app", maybe the smart move would be to add it only on the most necessary places. The smallest you could go with, the better.I thought a good name for this would be "islands of interactivity". Then I Googled it and turns out that's already a thing. Although, that post still mentions Preact, SSR, manifest files, so I'm not sure we're really on a same page. People will overcomplicate everything.But I do believe we have enough bandwidth today that you can load a small React app that only renders an island of interactivity inside of what is a classic server-side rendered page. I don't believe that mix would be that abominable, but I've yet to try it, and for my next project, I just might.So, my untested approach to having clean and maintanable frontend code is: go render it all on server and plop in React or whatever only where you really need it.It really can't be any worse than(Side note, I'm trying something new this time (no, it's not Patreon) - here are the "official" comment threads for this blog post on HackerNews and on Reddit. To keep up-to-date, you can also subscribe to my newsletter.)]]></content:encoded></item><item><title>Why the Linux hate is a thing?</title><link>https://www.reddit.com/r/linux/comments/1lok98x/why_the_linux_hate_is_a_thing/</link><author>/u/Guilty_Bird_3123</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 21:46:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Lately I had installed Linux after years of Windows experience and wanted to open a thread on r/FACEITcom for awareness only.Couple of negative comments have been sent, why people are so mad about using Linux instead of Windows?]]></content:encoded></item><item><title>OpenAI&apos;s evolution: From Nonprofit to Corporate</title><link>https://www.reddit.com/r/artificial/comments/1lojuee/openais_evolution_from_nonprofit_to_corporate/</link><author>/u/MrKoyunReis</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 21:29:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] Should we petition for requiring reviewers to state conditions for improving scores?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lohh1u/d_should_we_petition_for_requiring_reviewers_to/</link><author>/u/Able-Entertainment78</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 19:56:12 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I’ve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, “If the authors add experiments on XYZ” or “If the theoretical claim is proven under ABC setup.”Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.Honestly, I suspect many reviewers don’t even know what exactly would change their mind.As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.What do you think? Could this improve transparency and accountability in the review process?]]></content:encoded></item><item><title>OPNSense firewall in front of kubernetes cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1logkav/opnsense_firewall_in_front_of_kubernetes_cluster/</link><author>/u/bykof</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 19:19:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I want to ask you if an OPNSense firewall is a good idea in front of a kubernetes cluster. Managing Wireguard in OPNSenseAccess the whole cluster only via Wireguard VPNAllow only specific IPs to access the cluster without Wireguard VPNAre there any benefits or drawbacks from this idea, that I don't see yet?Thank you for your ideas!]]></content:encoded></item><item><title>Chromium/V8 implementing Temporal API via Rust (temporal_rs and ICU4X)</title><link>https://www.reddit.com/r/rust/comments/1logjzt/chromiumv8_implementing_temporal_api_via_rust/</link><author>/u/Manishearth</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 19:19:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[In the last two months I've been working on adding support for the (rather large) Temporal datetime API to V8, Chromium's JS engine. The meat of this implementation is all Rust. Firefox already has an implementation using ICU4X. For V8 we're using temporal_rs, which builds on top of ICU4X but does more of the spec-specific stuff. This wouldn't be the first Rust in Chromium, but it's a significant chunk of code! You can see most of the glue code in V8 in here, and you can look at all of the CLs here).There's still a bunch of work to do on test conformance, but now is a point where we can at least say it is fully implemented API-wise.I'm happy to answer any questions people may have! I'm pretty excited to see this finally happen, it's a long-desired improvement to the JS standard library, and it's cool to see it being done using Rust.]]></content:encoded></item><item><title>Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta&apos;s ‘Superintelligence’ Team</title><link>https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/</link><author>/u/wiredmagazine</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 18:13:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[ Meta staff today to introduce them to the new superintelligence team. The memo, which WIRED obtained, lists names and bios for the recently hired employees, many of whom came from rival AI firms like OpenAI, Anthropic, and Google.Over the past few months, Meta CEO Mark Zuckerberg has been on a recruiting frenzy to poach some of the most sought-after talent in AI. The social media giant has invested $14.3 billion in Scale AI and hired Alexandr Wang, its CEO, to run Meta’s Superintelligence Labs. News of the memo was first reported by Bloomberg.“We’re going to call our overall organization Meta Superintelligence Labs (MSL). This includes all of our foundations, product, and FAIR teams, as well as a new lab focused on developing the next generation of our models,” Zuckerberg wrote in the memo on Monday. Meta declined to comment.Zuckerberg introduced Wang, who will be the company’s “chief AI officer” and leader of MSL, as well as former GitHub CEO Nat Friedman. Friedman will colead the new lab with Wang, with a focus on AI products and applied research.Here’s the list of all the new hires as seen in Zuckerberg's memo. It notably doesn’t include the employees who joined from OpenAI’s Zurich office.Trapit Bansal: pioneered RL on chain of thought and cocreator of o-series models at OpenAl.Shuchao Bi: cocreator of GPT-4o voice mode and o4-mini. Previously led multimodal post-training at OpenAl.Huiwen Chang: cocreator of GPT-4o's image generation, and previously invented MaskIT and Muse text-to-image architectures at Google Research.Ji Lin: helped build 03/o4-mini, GPT-4o, GPT-4.1, GPT-4.5, 40-imagegen, and Operator reasoning stack.Joel Pobar: inference at Anthropic. Previously at Meta for 11 years on HHVM, Hack, Flow, Redex, performance tooling, and machine learning.Jack Rae: pre-training tech lead for Gemini and reasoning for Gemini 2.5. Led Gopher and Chinchilla early LLM efforts at DeepMind.Hongyu Ren: cocreator of GPT-4o, 4o-mini, o1-mini, o3-mini, 03 and o4-mini. Previously leading a group for post-training at OpenAl.Johan Schalkwyk: former Google Fellow, early contributor to Sesame, and technical lead for Maya.Pei Sun: post-training, coding, and reasoning for Gemini at Google Deepmind. Previously created the last two generations of Waymo's perception models.Jiahui Yu: cocreator of 03, 04-mini, GPT-4.1 and GPT-4o. Previously led the perception team at OpenAl, and co-led multimodal at Gemini.Shengjia Zhao: cocreator of ChatGPT, GPT-4, all mini models, 4.1 and 03. Previously led synthetic data at OpenAl.]]></content:encoded></item><item><title>Test orchestration anyone?</title><link>https://www.reddit.com/r/kubernetes/comments/1locgxp/test_orchestration_anyone/</link><author>/u/Dmitry_Fon</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 16:44:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Almost by implication of Kubernetes, we're having more and more microservices in our software. If you are doing test automation for your application (APIs, End-to-End, Front-End, Back-End, Load testing, etc.) - How are you orchestrating those test? - CI/CD - through Jenkins, GitHub Actions, Argo Workflows? - A dedicated Test orchestration tool?]]></content:encoded></item><item><title>How to manage configuration settings in Go web applications</title><link>https://www.alexedwards.net/blog/how-to-manage-configuration-settings-in-go-web-applications</link><author>/u/alexedwards</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 16:42:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[When I'm building a web application in Go, I prefer to use command-line flags to pass configuration settings to the application at runtime. But sometimes, the client I'm working with wants to use environment variables to store configuration settings, or the nature of the project means that storing settings in a TOML, YAML or JSON file is a better fit. And of course that's OK — it makes sense to be flexible and vary how configuration is managed based on the specific needs of a project and/or client.So, in this tutorial, I want to share the patterns that I use for parsing configuration settings — whether they come from flags, environment variables or files — and explain how I pass the settings onwards to where they are needed in the rest of the web application code. I'll also end with a short discussion about the relative pros and cons of the different approaches.It's a fairly detailed post, so here are the shortcut links for quick reference:To illustrate the patterns in the rest of this tutorial, let's pretend that we have a web application where we want to configure the following five settings:The port number the web application listens onEnables detailed request and error loggingMaximum duration to wait for a request to completeUsername required for HTTP Basic AuthenticationPassword required for HTTP Basic AuthenticationRegardless of where the configuration settings are coming from (flags, environment variables or a file), I'm quite strict about keeping all the code related to configuration settings isolated in one place, and reading in the configuration setting values right at the start of the program, before doing almost anything else.Most of the time, I prefer to store all the configuration setting values in a single  struct, like so:type config struct {
    port           int
    verboseLogging bool
    requestTimeout time.Duration
    basicAuth      struct {
        username string
        password string
    }
}
I like this because it feels very clear — all the configuration settings are contained in a single struct, along with their appropriate Go type, and you can easily see at a glance what configuration settings the application expects and supports.As I mentioned at the start of this tutorial, using command-line flags with the standard library  package is my preferred approach to managing configuration settings. With this approach, you explicitly pass the configuration values as part of the command when running the program. For example:$ go run main.go -port=9999 -verbose-logging=true -request-timeout=10s -basic-auth-username=admin -basic-auth-password="secr3tPa55word"
In your Go code, you define a specific command-line flag using syntax like this:flag.IntVar(&cfg.port, "port", 4000, "The port number the web application listens on")`
In this example code, we define a command-line flag named  that accepts an integer value and stores it at the location pointed to by the  pointer. It will have a default value of  if no corresponding  flag is provided when starting the application, and the final parameter is a description that will be displayed when a user runs the program with the  flag.Importantly, after you've defined all the command-line flags for your application, you need to call the  function to actually read in the values from the command-line arguments.Let's put this together in a very simple application that reads the command-line flag values into a  struct, and then prints them out.package main

import (
    "flag"
    "fmt"
    "time"
)

// The config struct holds all configuration settings for the application.
type config struct {
    port           int
    verboseLogging bool
    requestTimeout time.Duration
    basicAuth      struct {
        username string
        password string
    }
}

func main() {
    // Create a new config instance.
    var cfg config

    // Define the command-line flags. Notice that we define these so that the values 
    // are read directly into the appropriate config struct field, and set sensible default 
    // values for each of them.
    flag.IntVar(&cfg.port, "port", 4000, "The port number the web application listens on")
    flag.BoolVar(&cfg.verboseLogging, "verbose-logging", false, "Enables detailed request and error logging")
    flag.DurationVar(&cfg.requestTimeout, "request-timeout", 5*time.Second, "Maximum duration to wait for a request to complete")
    flag.StringVar(&cfg.basicAuth.username, "basic-auth-username", "", "Username required for HTTP Basic Authentication")
    flag.StringVar(&cfg.basicAuth.password, "basic-auth-password", "", "Password required for HTTP Basic Authentication")

    // Parse the flags with the flag.Parse function. This is important!
    flag.Parse()

    // Print all configuration settings.
    fmt.Printf("Port: %d\n", cfg.port)
    fmt.Printf("Verbose Logging: %t\n", cfg.verboseLogging)
    fmt.Printf("Request Timeout: %v\n", cfg.requestTimeout)
    fmt.Printf("Basic Auth Username: %s\n", cfg.basicAuth.username)
    fmt.Printf("Basic Auth Password: %s\n", cfg.basicAuth.password)
}
If you're following along, go ahead and run the application with your own values in the command-line flags. You should see the same values printed out by the application, like so:$ go run main.go -port=9999 -verbose-logging=true -request-timeout=30s -basic-auth-username=admin -basic-auth-password="secr3tPa55word"
Port: 9999
Verbose Logging: true
Request Timeout: 30s
Basic Auth Username: admin
Basic Auth Password: secr3tPa55wordIf you don't provide a value for a specific flag, the application will revert to using the default value you specified. For example, if you don't provide a  flag it will default to the value of , like so:$ go run main.go -basic-auth-username=admin -basic-auth-password="secr3tPa55word"
Port: 4000
Verbose Logging: false
Request Timeout: 5s
Basic Auth Username: admin
Basic Auth Password: secr3tPa55wordOne of the great things about the standard library  package is the support for automatic help text. If you run your application with the flag , it will list all the available flags for the application, along with their accompanying help text and default values if appropriate. Like so:$ go run main.go -help
Usage of /tmp/go-build2103583960/b001/exe/main:
  -basic-auth-password string
        Password required for HTTP Basic Authentication
  -basic-auth-username string
        Username required for HTTP Basic Authentication
  -port int
        The port number the web application listens on (default 4000)
  -request-timeout duration
        Maximum duration to wait for a request to complete (default 5s)
  -verbose-logging
        Enables detailed request and error loggingFor boolean flags, if you want to pass a value of  you can simply include the flag name without assigning a value. The following two commands are equivalent:$ go run main.go -verbose-logging=true
$ go run main.go -verbose-logging
In contrast, you always need to use  if you want to set a boolean flag value to .You can use one or two dashes in front of a flag name, both work identically. The standard library  package does not support 'short' flags, and the number of dashes has no effect on the behavior or any special meaning. So it's just a matter of personal taste which you use. The following two commands are equivalent:$ go run main.go -verbose-logging -request-timeout=30s
$ go run main.go --verbose-logging --request-timeout=30s
If you try to pass an invalid value as a command-line flag, the application will automatically exit with an error message and the help text for reference. For example, if you try to pass a non-integer value in the  flag, the parsing would fail and the output would look like this:$ go run main.go -port=foobar
invalid value "foobar" for flag -port: parse error
Usage of /tmp/go-build2103583960/b001/exe/main:
  -basic-auth-password string
        Password required for HTTP Basic Authentication
  -basic-auth-username string
        Username required for HTTP Basic Authentication
  -port int
        The port number the web application listens on (default 4000)
  -request-timeout duration
        Maximum duration to wait for a request to complete (default 5s)
  -verbose-logging
        Enables detailed request and error logging
exit status 2Similarly, if you try to use a flag that as not been defined, the application will automatically exit with an error message and the help text. For example:$ go run main.go -foobar=baz
flag provided but not defined: -foobar
...etcThe  package provides functions for reading command-line flag values into the following Go types: , , , , , ,  and .If you want to parse a command-line flag value into another Go type (such as  or ), you have a few different options. The simplest approach is to use the   function, which I've written about here. Or you can also make your own custom type that implements the  or  interfaces, and define the flag using either the  or  functions respectively. I've shared a gist demonstrating how to do this here.Alternatively, there are third-party packages (such as ) that you can use, which automatically support parsing command-line flags into a wider range of Go types. Personally, I've never felt it necessary to use these, but YMMV.Lastly, if you want you can create , which act like a 'container' for a distinct set of command-line flags. It's rare that I need to use flagsets in a web application, but I do often use them when building CLI applications with multiple subcommands. There's a good tutorial about how to use flagsets here.Using environment variablesFirst, I'll start by saying that you can use environment variables in conjunction with command-line flags if you want. Simply set your environment variables as normal, and use them in the command when starting your application. Like so:$ export VERBOSE_LOGGING="true"
$ export REQUEST_TIMEOUT="30s"
$ go run main.go -verbose-logging=$VERBOSE_LOGGING -request-timeout=$REQUEST_TIMEOUT
But if you don't want to do this, you can read the values from environment variables directly into your Go code using the  function. This will return the value of the environment variable as a , or the empty string  if the environment variable doesn't exist. You can also use the  function to check whether a specific environment variable exists or not.To help read values from environment variables, I like to create an  package containing some helper functions that convert the environment variable  to the appropriate Go type, and optionally set a default value for if the environment variable doesn't exist (just like command-line flags). For example:File: internal/env/env.gopackage env

import (
    "fmt"
    "os"
    "strconv"
    "time"
)

func GetInt(key string, defaultValue int) int {
    value, exists := os.LookupEnv(key)
    if !exists {
        return defaultValue
    }

    intValue, err := strconv.Atoi(value)
    if err != nil {
        panic(fmt.Errorf("environment variable %s=%q cannot be converted to an int", key, value))
    }
    return intValue
}

func GetBool(key string, defaultValue bool) bool {
    value, exists := os.LookupEnv(key)
    if !exists {
        return defaultValue
    }

    boolValue, err := strconv.ParseBool(value)
    if err != nil {
        panic(fmt.Errorf("environment variable %s=%q cannot be converted to a bool", key, value))
    }
    return boolValue
}

func GetDuration(key string, defaultValue time.Duration) time.Duration {
    value, exists := os.LookupEnv(key)
    if !exists {
        return defaultValue
    }

    durationValue, err := time.ParseDuration(value)
    if err != nil {
        panic(fmt.Errorf("environment variable %s=%q cannot be converted to a time.Duration", key, value))
    }
    return durationValue
}

func GetString(key string, defaultValue string) string {
    value, exists := os.LookupEnv(key)
    if !exists {
        return defaultValue
    }
    return value
}
In some projects, I use a twist on these helper functions and panic if a specific environment variable isn't set, rather than returning a default value. For example:func MustGetInt(key string) int {
    value, exists := os.LookupEnv(key)
    if !exists {
        panic(fmt.Errorf("environment variable %s must be set", key))
    }

    intValue, err := strconv.Atoi(value)
    if err != nil {
        panic(fmt.Errorf("environment variable %s=%q cannot be converted to an int", key, value))
    }
    return intValue
}
Using those helper functions in your application then looks a bit like this:package main

import (
    "fmt"
    "time"

    "your-project/internal/env"
)

type config struct {
    port           int
    verboseLogging bool
    requestTimeout time.Duration
    basicAuth      struct {
        username string
        password string
    }
}

func main() {
    var cfg config

    cfg.port = env.GetInt("PORT", 4000)
    cfg.verboseLogging = env.GetBool("VERBOSE_LOGGING", false)
    cfg.requestTimeout = env.GetDuration("REQUEST_TIMEOUT", 5*time.Second)
    cfg.basicAuth.username = env.GetString("BASIC_AUTH_USERNAME", "")
    cfg.basicAuth.password = env.GetString("BASIC_AUTH_PASSWORD", "")

    fmt.Printf("Port: %d\n", cfg.port)
    fmt.Printf("Verbose Logging: %t\n", cfg.verboseLogging)
    fmt.Printf("Request Timeout: %v\n", cfg.requestTimeout)
    fmt.Printf("Basic Auth Username: %s\n", cfg.basicAuth.username)
    fmt.Printf("Basic Auth Password: %s\n", cfg.basicAuth.password)
}
If you'd like to try this out, go ahead and add the necessary environment variables to your  or  files, or  them in your shell, and try running the application again. You should see the configuration settings reflected in the output, or any default values for ones that you didn't set.$ export PORT="9999"
$ export VERBOSE_LOGGING="false"
$ export BASIC_AUTH_USERNAME="admin"
$ export BASIC_AUTH_PASSWORD="secr3tPa55word"
$ go run main.go 
Port: 9999
Verbose Logging: false
Request Timeout: 5s
Basic Auth Username: admin
Basic Auth Password: secr3tPa55wordIf you're working on multiple projects on the same development machine (and not using separate containers for each project), it can become awkward to manage environment variables and avoid clashes across the projects. Rather than setting environment variables in  or , a fairly common workaround is to create an  file in your project containing the environment variables, like so:export PORT=5000
export VERBOSE_LOGGING=true
export REQUEST_TIMEOUT=10s
export BASIC_AUTH_USERNAME=admin
export BASIC_AUTH_PASSWORD=secr3tPa55word
Then you can  the  file to export the variables in the current terminal session and run your Go application:$ source .env 
$ go run main.go 
Port: 5000
Verbose Logging: true
Request Timeout: 10s
Basic Auth Username: admin
Basic Auth Password: secr3tPa55wordAlternatively, if you don't want to keep running the  command, you can use the  package to automatically load the values from the  file into the environment when your application starts up.Using configuration filesThe third option that I sometimes use is configuration files, which store all the settings in a single file on-disk. I normally only use these in projects where there are  of configuration settings, and loading them all via command-line flags would be onerous and error-prone. Or also, if the configuration settings are complex, with a deeply nested 'structure' to them.There are a lot of different formats that you can use for configuration files, such as TOML or YAML — or even JSON. They all have different advantages and disadvantages, and you'll be hard-pressed to find one that everybody agrees is 'perfect'. But whatever format you choose, there is probably a Go package that you can use to automatically parse values from the file into a  struct for you.For example, let's say that you want to use TOML and have a configuration file that looks like this:# Server configuration
port = 4000
verbose_logging = true
request_timeout = "10s"

# Basic authentication settings
[basic_auth]
username = "admin"
password = "secr3tPa55word"
You can use the  package to read the file and unpack the contents to a  struct like so:package main

import (
    "fmt"
    "log"
    "time"

    "github.com/BurntSushi/toml"
)

// Make sure the struct fields are exported, so that the BurntSushi/toml package
// can write to them, and use struct tags to map the TOML key/value pairs to the
// appropriate struct field.
type config struct {
    Port           int           `toml:"port"`
    VerboseLogging bool          `toml:"verbose_logging"`
    RequestTimeout time.Duration `toml:"request_timeout"`
    BasicAuth      struct {
        Username string `toml:"username"`
        Password string `toml:"password"`
    } `toml:"basic_auth"`
}

func main() {
    var cfg config

    // Load configuration settings from the config.toml file.
    metadata, err := toml.DecodeFile("config.toml", &cfg)
    if err != nil {
        log.Fatalf("error loading configuration: %v", err)
    }

    // Check for any undecoded keys in the config.toml file.
    if len(metadata.Undecoded()) > 0 {
        log.Fatalf("unknown configuration keys: %v", metadata.Undecoded())
    }

    fmt.Printf("Port: %d\n", cfg.Port)
    fmt.Printf("Verbose Logging: %t\n", cfg.VerboseLogging)
    fmt.Printf("Request Timeout: %v\n", cfg.RequestTimeout)
    fmt.Printf("Basic Auth Username: %s\n", cfg.BasicAuth.Username)
    fmt.Printf("Basic Auth Password: %s\n", cfg.BasicAuth.Password)
}
Notice that in this code we're making use of the metadata returned by the  function to check if any settings were not decoded successfully — which should help to catch typos or invalid keys in the TOML file.Passing settings to where they are neededGetting the configuration settings into the  struct, wherever they come from, is the first half of the puzzle. The second part is getting those settings to where you need them in your Go code. There are many different ways to approach this, and no single 'right' way. For small or medium sized web applications, I often use a pattern of creating an  struct which contains all the dependencies that my HTTP handlers need, and I implement the handlers as methods on the  struct. To make the configuration settings available to the HTTP handlers, I simply include the  struct as a field in . package main

import (
    "flag"
    "fmt"
    "log/slog"
    "net/http"
    "os"
    "time"
)

type config struct {
    port           int
    verboseLogging bool
    requestTimeout time.Duration
    basicAuth      struct {
        username string
        password string
    }
}

// The application struct contains the dependencies for the handlers, including 
// the config struct
type application struct {
    config config
    logger *slog.Logger
}

func main() {
    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))

    var cfg config
    flag.IntVar(&cfg.port, "port", 4000, "The port number the web application listens on")
    flag.BoolVar(&cfg.verboseLogging, "verbose-logging", false, "Enables detailed request and error logging")
    flag.DurationVar(&cfg.requestTimeout, "request-timeout", 5*time.Second, "Maximum duration to wait for a request to complete")
    flag.StringVar(&cfg.basicAuth.username, "basic-auth-username", "", "Username required for HTTP Basic Authentication")
    flag.StringVar(&cfg.basicAuth.password, "basic-auth-password", "", "Password required for HTTP Basic Authentication")
    flag.Parse()

    app := &application{
        config: cfg,
        logger: logger,
    }

    mux := http.NewServeMux()
    mux.HandleFunc("/", app.home)

    // Use the port configuration setting
    logger.Info("starting server", "port", cfg.port)

    err := http.ListenAndServe(fmt.Sprintf(":%d", cfg.port), mux)
    if err != nil {
        logger.Error(err.Error())
        os.Exit(1)
    }
}

func (app *application) home(w http.ResponseWriter, r *http.Request) {
    // Use the verboseLogging configuration setting
    if app.config.verboseLogging {
        app.logger.Info("handling request", "method", r.Method, "path", r.URL.Path)
    }

    fmt.Fprintf(w, "Hello!")
}
If you run this application with the  flag, and make a HTTP request to , you should see the details of the request in the log output, similar to below — demonstrating that the config setting is correctly available to the handler.$ go run main.go -verbose-logging
time=2025-06-27T14:15:40.230+02:00 level=INFO msg="starting server" port=4000
time=2025-06-27T14:15:48.705+02:00 level=INFO msg="handling request" method=GET path=/In larger applications where I want to define my handlers outside of , or pass the config struct to functions in other packages, I normally define an exported  struct in an  package, and pass this around as necessary. For example, let's say that you have a project structure like so:├── go.mod
├── go.sum
├── main.go
└── internal
    ├── config
    │   └── config.go
    └── handlers
        └── home.go
Then the contents of those  files would look something like this:File: internal/config/config.go 
package config

import "time"

type Config struct {
    Port           int
    VerboseLogging bool
    RequestTimeout time.Duration
    BasicAuth      struct {
        Username string
        Password string
    }
}
File: internal/handlers/home.gopackage handlers

import (
    "fmt"
    "log/slog"
    "net/http"

    "your-project/internal/config"
)

func Home(cfg config.Config, logger *slog.Logger) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        if cfg.VerboseLogging {
            logger.Info("handling request", "method", r.Method, "path", r.URL.Path)
        }

        fmt.Fprintf(w, "Hello!")
    }
}
package main

import (
    "flag"
    "fmt"
    "log/slog"
    "net/http"
    "os"
    "time"

    "your-project/internal/config"
    "your-project/internal/handlers"
)

func main() {
    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))

    var cfg config.Config
    flag.IntVar(&cfg.Port, "port", 4000, "The port number the web application listens on")
    flag.BoolVar(&cfg.VerboseLogging, "verbose-logging", false, "Enables detailed request and error logging")
    flag.DurationVar(&cfg.RequestTimeout, "request-timeout", 5*time.Second, "Maximum duration to wait for a request to complete")
    flag.StringVar(&cfg.BasicAuth.Username, "basic-auth-username", "", "Username required for HTTP Basic Authentication")
    flag.StringVar(&cfg.BasicAuth.Password, "basic-auth-password", "", "Password required for HTTP Basic Authentication")
    flag.Parse()

    mux := http.NewServeMux()
    mux.HandleFunc("/", handlers.Home(cfg, logger))

    // Use the port configuration setting
    logger.Info("starting server", "port", cfg.Port)

    err := http.ListenAndServe(fmt.Sprintf(":%d", cfg.Port), mux)
    if err != nil {
        logger.Error(err.Error())
        os.Exit(1)
    }
}
Obviously I'm using command-line flags in these examples, but the same patterns work for environment variables or config files too — once the  struct is loaded with the data, it doesn't matter where it originally came from and the code patterns are the same.If you've been in the web development world for a long time and buy into the 12-factor app principles (which I generally do), you might think that the correct approach is "just use environment variables". But over the years I've come to the conclusion that they have some drawbacks:I've been bitten more times than I want by bugs that were ultimately a result of an unset or unexpected value in an environment variable — and I think that part of the problem here is that environment variables aren't readily and easily observable in the same way that the values in command-line flags or a configuration file are. If you're working on multiple projects on the same development machine (rather than working in separate containers for each project), you have to manage the lack of natural isolation between environment variables... you need to make sure that there aren't any naming clashes, and that (for example) application A isn't accidentally using the  setting intended for application B.I've also seen a lot of Go codebases where configuration settings are read in using at the point in the code where they are needed. This makes discoverability difficult — it's hard to look at an application's code and easily see what the expected configuration settings are. You can mitigate these issues with some of the techniques that we've discussed in this tutorial. If you're strict about reading all the settings into a single  struct at application startup, that addresses the discoverability problem. If you create helpers like  which panic if an environment variable isn't set, that helps to eliminate bugs that exist due to missing environment variables. And you can work around some of the environment variable isolation problems in development by using a  file — but at that point, it might be worth considering whether a configuration file might be more appropriate.One of the big reasons that I like to use command-line flags is that you get a lot of stuff for free. You get automatic  text, automatic type conversions, the ability to set defaults, and it handles invalid inputs and undefined flags nicely. Also, it's always very clear what configuration values are being used — you either explicitly pass the values when starting the application, or the default values hardcoded into your Go codebase are used. On top of that, most other gophers will be familiar with the  package and you don't need any third-party dependencies.When I'm using command-line flags, I typically set the default values to things that are appropriate for a development environment. This is mainly so I don't have to keep typing long commands to run the application when actively developing it.In terms of application secrets, like I mentioned earlier, there's nothing stopping you from storing a specific secret in an environment variable and using it in conjunction with a command-line flag if you want. For example, if you store a password for your database user in a  environment variable, you can include it as a command-line flag value when starting the application like so:$ go run main.go -db-user=web -db-password=$DB_PASSWORD
Or, although it is a bit more 'magical', you could even use the environment variable as the default value:flag.StringVar(&cfg.db.password, "db-password", os.Getenv("DB_PASSWORD"), "Database user password")
So, for all these reasons, I tend to prefer using command-line flags for configuration. The big exception to this is when there are  of configuration settings, and it would be awkward to pass them all via command-line flags, or the settings have a deeply nested 'structure' to them. In these cases, I think it can be more practical and maintainable to store the settings in a TOML or JSON configuration file, and load them on application startup like we demonstrated earlier.]]></content:encoded></item><item><title>Built a geospatial game in Go using PostGIS where you plant seeds at real locations</title><link>https://www.reddit.com/r/golang/comments/1loboyo/built_a_geospatial_game_in_go_using_postgis_where/</link><author>/u/SoaringSignificant</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 16:14:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So I built this thing where you plant virtual seeds at real GPS locations and have to go back to water them or they die. Sounds dumb but I had fun making it and it's kinda fun to use.Like you plant a seed at your gym, and if you don't go back within a few days your plant starts losing health. I've got a bunch of plants that I'm trying to get to level 10.Built the main logic in Go, TypeScript + React for the frontend, and PostgreSQL with PostGIS for all the geospatial queries, though a bunch of that stuff happens in the service layer too. The geospatial stuff was interesting to work out, I ended up implementing plants and soils as circles since it makes the overlap detection and containment math way simpler. Figuring out when a plant fits inside a soil area or when two plants would overlap becomes basic circle geometry instead of dealing with complex polygons.Plants decay every 4 hours unless you water them recently (there's a grace period system). Got a bunch of other mechanics like different soil types and plant tempers that are not fully integrated into the project right now. Just wanted to get the core loop working first and see how people actually use it.You just need to get within like 10 meters of your plant to water it, but I'm still playing with these values to see what ends up being a good fit. Used to have it at 5 metres before but it made development a pain. The browser's geolocation api is so unreliable that I'd avoid it in future projects.Been using it during development and it's actually getting me to go places more regularly but my plant graveyard is embarrassingly large though.Here's a link to the repo and the live site for anyone interested in trying it out: GitHub | Live Site]]></content:encoded></item><item><title>Exploring Text Classification: Is Golang Viable or Should I Use Pytho</title><link>https://www.reddit.com/r/golang/comments/1lobi9d/exploring_text_classification_is_golang_viable_or/</link><author>/u/Fit_Honeydew4256</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 16:07:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi everyone, I’m still in the early stages of exploring a project idea where I want to classify text into two categories based on writing patterns. I haven’t started building anything yet — just researching the best tools and approaches.Since I’m more comfortable with Go (Golang), I’m wondering:Is it practical to build or run any kind of text classification model using Go?Has anyone used Go libraries like Gorgonia, goml, or onnx-go for something similar?Would it make more sense to train the model in Python and then call it from a Go backend (via REST or gRPC)?Are there any good examples or tutorials that show this kind of hybrid setup?I’d appreciate any tips, repo links, or general advice from folks who’ve mixed Go with ML. Just trying to figure out the right path before diving in.]]></content:encoded></item><item><title>I made a functional 8-bit adder/subtractor circuit that works natively within MS Paint</title><link>https://github.com/RRTogunov/MSPaintComputer</link><author>/u/jkjkjij22</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 16:06:38 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I built all logic gates using the bucket/fill tool. These were combined to make an 8-bit ripple-carry adder as well as an 8-bit adder/subtractor circuit. Define inputs A and B (white = 0, black = 1) using bucket fill.To run the circuit/computation, use the colour picker and fill tool to cycle through a sequence of colour changes from the “Bus” and “Probe” squares on the left and apply them to the circuit leads on the right.This is where my knowledge of computer science ends, and I'm not sure how far this could theoretically be taken. There are a few quirks that make this particularly challenging. For example, all logical components of the circuit are single-use (i.e., at the end of the computation, the entire circuit is black/white, and all the colour pixel logic is lost). Also, because this is in 2-dimensions it's not possible to cross/bridging/tunnel "wires" to make complex compound logic gates (XOR and XNOR). There's also a challenge with back-propagation, where colour fills don't just go forward down the circuit, but travel back and affect other parts of the circuit.]]></content:encoded></item><item><title>Don’t Be Ashamed to Say &quot;I Don’t Know&quot;</title><link>https://www.thecoder.cafe/p/i-dont-know</link><author>/u/teivah</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 15:55:11 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hello! Today, let’s discuss the power of “I don’t know“ with a personal story.Last month, I was at the hospital with my partner for the birth of our newborn. During our stay, my partner experienced a specific symptom, and we wanted to understand what could be causing it. So we asked the midwife.We immediately noticed the hesitation in her eyes. When she finally gave an answer, it came with a kind of forced confidence, and we both felt she wasn’t sure about it.At our hospital, midwives do 12-hour shifts. So a few hours later, we asked the exact same question to the next midwife. Same hesitation, but this time, a different answer.And so it went on. Every shift, we asked again. Every time, a different answer. Eventually, it even became a game between my partner and me: trying to guess what the next answer would be.Until… The one. The one who broke this cycle.We asked her the same question. She paused. Thought about it. And then said something unexpected:Twenty minutes later, she even came back to our room and said:I asked the doctor, the answer is because of [X]. Thanks for asking, I learned something.That brief exchange resonated with me.In our field, we often put a lot of weight on posture. We build up our position as the go-to person for a codebase, a data model, or a framework. The more we know, the more we are seen as the one to consult or include in any related discussion.But from that posture, admitting we don’t know something can feel like pulling out the bottom card in a house of cards. Suddenly, it feels like everything we built to earn that status might collapse.Yet, if we take a step back, admitting we don’t know shouldn’t be seen as something shameful or embarrassing. In fact, it’s often the most responsible thing we can do.Pretending to know can lead to bad decisions.Authority isn’t built on knowing everything.Teams work better when people feel safe to admit uncertainty.Curiosity + humility = real learning. Whether it’s for us or others, next time we don’t know something, let’s be like that midwife: let’s just admit it. Without shame.How comfortable are you with saying “I don’t know“?If you made it this far and enjoyed the post, please consider giving it a like.]]></content:encoded></item><item><title>4 of the top 10 YouTube channels are AI-generated</title><link>https://sherwood.news/tech/ai-created-videos-are-quietly-taking-over-youtube/</link><author>/u/Alone-Competition-77</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 14:55:43 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[YouTube has never stayed still for long. The video powerhouse owned by  started its life as a place to upload home movies, only to use its early scale to morph into a user-generated MTV, birthing an entire universe of homegrown celebrities. It’s since evolved in two competing directions: incentivizing longer, more professional videos to compete with Netflix but also shorter, more ephemeral videos to compete with TikTok.And it looks as if it’s changing again. The age of AI has finally arrived for YouTube, and it could be its most existential shift yet.Over the past few months, Garbage Day has tracked how a range of AI-generated videos have found increasing success on the platform, taking up attention and space over more professional creators. More than a few of them seemingly rely on inauthentic engagement to boost their attention — though, in a platform overrun with AI, it’s worth asking if that even matters.At the same time, though, some of YouTube’s biggest success stories have shown less and less interest in the platform, focusing their efforts and promotion on places like TikTok and Instagram. All of this suggests a watershed moment for the internet’s biggest video site. In May, four of the top 10 YouTube channels with the most subscribers featured AI-generated material in every video. Not all the channels are using the same AI programs, and there are indications that some contain human-made elements, but none of these channels has ever uploaded a video that was made entirely without AI — and each has uploaded a constant stream of videos, which is crucial to their success.While not all of the videos from these AI channels are identical, the most successful examples tend to find a theme and stick to it. Some, like “Chick of Honor,” use tools like Hailuo for the instantly established format of cute animals in visibly dangerous or tragic situations. Others, like “Masters of Prophecy,” upload AI-generated music videos for AI-generated songs, made with Suno to evoke ’80s synth nostalgia.This is a profound change in how YouTube looked even just six months ago. In January, for instance, the most popular account making videos with AI got 2.5 million subscribers and 220 million views — barely in the top 20 for the month. In June, the top four AI channels combined to get more than 23 million subscribers and 800 million views. The algorithm clearly favors AI channels now, enough that they’re getting a much better ratio of views to subscriptions.This does make some sense. Generative-AI-driven channels do fix one core problem YouTube has struggled with ever since it started pushing more professional video content: not every creator has the time, resources, or skills to make Netflix-level content. Google’s ad revenue split doesn’t exactly pay for the costs that come with making your channel more professional, while AI videos all tend to be a certain standard and cost sometimes cents to produce.Generating content like this allows YouTubers to keep up with increasingly high demand from the platform’s algorithms, especially where YouTube Shorts are concerned. As the definition of Shorts has changed constantly to keep up with competitors like TikTok, YouTube has focused on their profitability much more than their appeal to creators. Starting in March, the platform changed the qualification of viewing a Short from watching it for a few seconds to any time the video starts or loops. According to DigiDay, this was done to make tracking engagement metrics for marketers easier, while individual creators still see the same revenue. Meanwhile, Shorts has also become a testing ground for Google’s many AI tools, whether that’s making clips with Veo or searching through them with Google Lens. Each popular AI-filled channel uploaded at least one Short in the month of May. Some, like “Chick of Honor,” uploaded entirely Shorts rather than full videos; others just made shorter clips of their videos and streams.This all feeds into YouTube’s aspirations for TV domination, as well. A report from Nielsen last month shows that it’s maintained the highest share of all TV viewing for several months straight, and it’s been the top streaming service for more than a year. This enormous share is reflected in its profits, as recent estimates say the platform is set to surpass Disney as the world’s most profitable media company. YouTube has tried to present itself as a competitor to streaming services for years, showcasing users like Alan Chikin Chow, whose YouTube videos are made in a massive production studio. But AI-generated music channels can play just as easily on a TV without any of the production costs.Masters of Prophecy is currently the fastest-growing channel across all of YouTube, going from a few hundred subscribers in February to over 30 million in June, and all of its content is AI-generated. But that growth looks suspicious, especially looking at how it began by going from less than 300 subscribers to more than 100,000 in a single day without any new videos, Shorts, or comments in that time. But again, on a platform increasingly powered — and populated — by AI, what does inauthentic growth even mean? At the end of the day, an AI bot isn’t going to buy a product from an advertiser.It’s clear that YouTube doesn’t want to answer that question. In many ways, it feels like it’s hoping no one notices how popular AI content is. With a quiet misdirect, and if users (and more importantly, advertisers) don’t complain, everyone will just keep making money. But it also means that at some point in the near future, you’ll open up the app and suddenly realize there aren’t any humans on it anymore. is an award-winning newsletter that focuses on web culture and technology, covering a mix of memes, trends, and internet drama. We also run a program called Garbage Intelligence, a monthly report tracking the rise and fall of creators and accounts across every major platform on the web. We’ll be sharing some of our findings here on Sherwood News. You can subscribe to Garbage Day here.]]></content:encoded></item><item><title>Writing Toy Programs is a great way to remember why you started programming</title><link>https://blog.jsbarretto.com/post/software-is-joy</link><author>/u/Tech_User_Station</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 14:51:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I am a huge fan of Richard Feyman’s famous quote:“What I cannot create, I do not understand”I think it’s brilliant, and it remains true across many fields (if you’re willing to be a little creative with the
definition of ‘create’). It is to this principle that I believe I owe everything I’m truly good at. Some will tell you
to avoid reinventing the wheel, but they’re wrong: you  build your own wheel, because it’ll teach you more about
how they work than reading a thousand books on them ever will.In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the
most joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and
industrialised. Software development needs more simple joy, and I’ve found that creating toy programs is a great way to
remember why I started working with computers again.Toy programs follow the 80:20 rule: 20% of the work, 80% of the functionality. The point is  to build
production-worthy software (although it is true that some of the best production software began life as a toy).
Aggressively avoid over-engineering, restrict yourself to only whatever code is necessary to achieve your goal. Have
every code path panic/crash until you’re forced to implement it to make progress. You might be surprised by just how
easy it is to build toy versions of software you might previously have considered to be insummountably difficult to
create.I’ve been consistently surprised by just how often some arcane nugget of knowledge I’ve acquired when working on a toy
project has turned out to be immensely valuable in my day job, either by giving me a head-start on tracking down a
problem in a tool or library, or by recognising mistakes before they’re made.Understanding the constraints that define the shape of software is vital for working with it, and there’s no better way
to gain insight into those constraints than by running into them head-first. You might even come up with some novel
solutions!Here is a list of toy programs I’ve attempted over the past 15 years, rated by difficulty and time required. These
ratings are estimates and assume that you’re already comfortable with at least one general-purpose programming language
and that, like me, you tend to only have an hour or two per day free to write code. Also included are some suggested
resources that I found useful.Regex engine (difficulty = 4/10, time = 5 days)A regex engine that can read a POSIX-style regex program and recognise strings that match it. Regex is simple yet
shockingly expressive, and writing a competent regex engine will teach you everything you need to know about using the
language too.x86 OS kernel (difficulty = 7/10, time = 2 months)A multiboot-compatible OS kernel with a simple CLI, keyboard/mouse driver, ANSI escape sequence support, memory manager,
scheduler, etc. Additional challenges include writing an in-memory filesystem, user mode and process isolation, loading
ELF executables, and supporting enough video hardware to render a GUI.GameBoy/NES emulator (difficulty = 6/10, time = 3 weeks)A crude emulator for the simplest GameBoy or NES games. The GB and the NES are classics, and both have relatively simple
instruction sets and peripheral hardware. Additional challenges include writing competent PPU (video) and PSG (audio)
implementations, along with dealing with some of the more exotic cartridge formats.GameBoy Advance game (difficulty = 3/10, time = 2 weeks)A sprite-based game (top-down or side-on platform). The GBA is a beautiful little console to write code for and there’s
an active and dedicated development community for the console. I truly believe that the GBA is one of the last game
consoles that can be fully and completely understood by a single developer, right down to instruction timings.Physics engine (difficulty = 5/10, time = 1 week)A 2D rigid body physics engine that implements Newtonian physics with support for rectangles, circles, etc. On the
simplest end, just spheres that push away from one-another is quite simple to implement. Things start to get complex
when you introduce more complex shapes, angular momentum, and the like. Additional challenges include making collision
resolution fast and scaleable, having complex interactions move toward a steady state over time, soft-body interactions,
etc.Dynamic interpreter (difficulty = 4/10, time = 1-2 weeks)A tree-walking interpreter for a JavaScript-like language with basic flow control. There’s an unbounded list of extra
things to add to this one, but being able to write programs in my own language still gives me child-like elation. It
feels like a sort of techno-genesis: once you’ve got your own language, you can start building the universe within it.Compiler for a C-like (difficulty = 8/10, time = 3 months)A compiler for a simply-typed C-like programming language with support for at least one target archtecture. Extra
challenges include implementing some of the most common optimisations (inlining, const folding, loop-invariant code
motion, etc.) and designing an intermediate representation (IR) that’s general enough to support multiple backends.Text editor (difficulty = 5/10, time = 2-4 weeks)This one has a lot of variability. At the blunt end, simply reading and writing a file can be done in a few lines of
Python. But building something that’s closer to a daily driver gets more complex. You could choose to implement the UI
using a toolkit like QT or GTK, but I personally favour an editor that works in the console. Properly handling unicode,
syntax highlighting, cursor movement, multi-buffer support, panes/windows, tabs, search/find functionality, LSP support,
etc. can all add between a week or a month to the project. But if you persist, you might join the elite company of those
developers who use an editor of their own creation.Async runtime (difficulty = 6/10, time = 1 week)There’s a lot of language-specific variability as to what ‘async’ actually means. In Rust, at least, this means a
library that can ingest  tasks and poll them concurrently until completion. Adding support for I/O waking
makes for a fun challenge.Hash map (difficulty = 4/10, time = 3-5 days)Hash maps (or sets/dictionaries, as a higher-level language might call them) are a programmer’s bread & butter. And yet,
surprisingly few of us understand how they really work under the bonnet. There are a plethora of techniques to throw
into the mix too: closed or open addressing, tombstones, the robin hood rule, etc. You’ll gain an appreciation for when
and why they’re fast, and also when you should just use a vector + linear search.Rasteriser / texture-mapper (difficulty = 6/10, time = 2 weeks)Most of us have played with simple 3D graphics at some point, but how many of us truly understand how the graphics
pipeline works and, more to the point, how to fix it when it doesn’t work? Writing your own software rasteriser will
give you that knowledge, along with a new-found appreciation for the beauty of vector maths and half-spaces that have
applications across many other fields. Additional complexity involves properly implementing clipping, a Z-buffer, N-gon
rasterisation, perspective-correct texture-mapping, Phong or Gouraud shading, shadow-mapping, etc.SDF Rendering (difficulty = 5/10, time = 3 days)Signed Distance Fields are a beautifully simple way to render 3D spaces defined through mathematics, and are perfectly
suited to demoscene shaders. With relatively little work you can build yourself a cute little visualisation or some
moving shapes like the graphics demos of the 80s. You’ll also gain an appreciation for shader languages and vector
maths.Voxel engine (difficulty = 5/10, time = 2 weeks)I doubt there are many reading this that haven’t played Minecraft. It’s surprisingly easy to build your own toy voxel
engine cut from a similar cloth, especially if you’ve got some knowledge of 3D graphics or game development already. The
simplicity of a voxel engine, combined with the near-limitless creativity that can be expressed with them, never ceases
to fill me with joy. Additional complexity can be added by tackling textures, more complex procedural generation,
floodfill lighting, collisions, dynamic fluids, sending voxel data over the network, etc.Threaded Virtual Machine (difficulty = 6/10, time = 1 week)Writing interpreters is great fun. What’s more fun? . If you keep pushing interpreters as far as
they can go without doing architecture-specific codegen (like AOT or JIT), you’ll eventually wind up (re)discovering
 (not to be confused with multi-threading, which is a very different beast). It’s a beautiful way of
weaving programs together out highly-optimised miniature programs, and a decent implementation can even give an AOT
compiler a run for its money in the performance department.GUI Toolkit (difficulty = 6/10, time = 2-3 weeks)Most of us have probably cobbled together a GUI program using tkinter, GTK, QT, or WinForms. But why not try writing
your GUI toolkit? Additional complexity involves implementing a competent layout engine, good text shaping (inc.
unicode support), accessibility support, and more. Fair warning: do not encourage people to use your tool unless it’s
 - the world has enough GUIs with little-to-no accessibility or localisation support.Orbital Mechanics Sim (difficulty = 6/10, time = 1 week)A simple simulation of Newtonian gravity can be cobbled together in a fairly short time. Infamously, gravitational
systems with more than two bodies cannot be solved analytically, so you’ll have to get familiar with iterative
 methods. Additional complexity comes with implementing more precise and faster integration methods,
accounting for relativistic effects, and writing a visualiser. If you’ve got the maths right, you can even try plugging
in real numbers from NASA to predict the next high tide or full moon.Bitwise Challenge (difficulty = 3/10, time = 2-3 days)Here’s one I came up with for myself, but I think it would make for a great game jam: write a game that only persists 64
bits of state between subsequent frames. That’s 64 bits for everything: the entire frame-for-frame game state should be
reproducible using only 64 bits of data. It sounds simple, but it forces you to get incredibly creative with your game
state management. Details about the rules can be found on the GitHub page below.An ECS Framework (difficulty = 4/10, time = 1-2 weeks)For all those game devs out there: try building your own ECS
framework. It’s not as hard as you might think (you might have accidentally done it already!). Extra points if you can
build in safety and correctness features, as well as good integration with your programming language of choice’s type
system features.I built a custom ECS for my Super Mario 64 on the GBA project due to the
unique performance and memory constraints of the platform, and enjoyed it a lot.CHIP-8 Emulator (difficulty = 3/10, time = 3-6 days)The CHIP-8 is a beautifully simple virtual machine from the 70s. You can write
a fully compliant emulator in a day or two, and there are an enormous plethora of fan-made games that run on it.
Here’s a game I made for it.Chess engine (difficulty = 5/10, time = 2-5 days)Writing a chess engine is great fun. You’ll start off with every move it makes being illegal, but over time it’ll get
smart and smarter. Experiencing a loss to your own chess engine really is a rite of passage, and it feels magical.POSIX shell (difficulty = 4/10, time = 3-5 days)We interact with shells every day, and building one will teach you can incredible amount about POSIX - how it works, and
how it doesn’t. A simple one can be built in a day, but compliance with an existing shell language will take time and
teach you more than you ever wanted to know about its quirks.A note on learning and LLMsPerhaps you’re a user of LLMs. I get it, they’re neat tools. They’re useful for certain kinds of learning. But I might
suggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a
plate. If you want that sort of learning, read a book - the joy in building toy projects like this comes from an
exploration of the unknown, without polluting one’s mind with an existing solution. If you’ve been using LLMs for a
while, this cold-turkey approach might even be painful at first, but persist. There is no joy without pain.The runner’s high doesn’t come to those that take the bus.]]></content:encoded></item><item><title>Won at a Hackathon</title><link>https://www.reddit.com/r/linux/comments/1lo968z/won_at_a_hackathon/</link><author>/u/MasterBach</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 14:37:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Internal corporate hackathon. Red hat guys were onsite for the duration of it. ]]></content:encoded></item><item><title>OpenTelemetry is Great, But Who the Hell is Going to Pay For It?</title><link>https://www.adatosystems.com/2025/02/10/who-the-hell-is-going-to-pay-for-this/</link><author>/u/finallyanonymous</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:58:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I’ve specialized in monitoring and observability for 27 years now, and I’ve seen a lot of tools and techniques come and go (RMon, anyone?); and more than a few come and stay (Rumors of the death of SNMP have been – and continue to be – greatly exaggerated.). Lately I’ve been exploring one of the more recent improvements in the space – OpenTelemetry (which I’m abbreviating to “OTel” for the remainder of this blog). I wrote about my decision to dive into OTel recently.For the most part, I’m enjoying the journey. But there’s a problem that has existed with observability for a while now, and it’s something OTel is not helping. The title of this post hints at the issue, but I want to be more explicit. Let’s start with some comparison shopping.Before I piss off every vendor in town, I want to be clear that these are broad, rough, high level numbers. I’ve linked to the pricing pages if you want to check the details, and I acknowledge what you see below isn’t necessarily indicative of the price you might actually pay after getting a quote on a real production environment.New Relic charges35¢ per GB for any data you send them.Datadog has a veritable laundry list of options, but at a high level, they charge:
60¢ – $1.22 per million netflow records$1.06-$3.75 per million log records$1.27-$3.75 per million spansDynatrace’s pricing page sports a list almost as long as Datadog’s but some key items:
15¢ per 100,00 metrics
plus .07¢ per Gig per day for retention2¢ per gig for logs
plus .07¢- per gig per day to retain themplus .035¢ per gig queriedEvents have the same rate as logsGrafana, which – it must be noted – is open source and effectively gives you everything for free if you’re willing to do the heavy lifting of installing and hosting. But their pricing can be summed up as:
$8.00 for 1k metrics, (up to 1/minute)50¢ per gig for logs and traces them, with 30 days retentionThis list is neither exhaustive nor complete. I’ve left off a lot of vendors, not because they also don’t have consumption based pricing but because it would just be more of the same. Even with the ones above, the details here aren’t complete. Some companies not only charge for consumption (ingest), they also charge to store the data, and charge again to query the data (looking at you, New Relic). Some companies push you to pick a tier of service, and if you don’t they’ll charge you an estimated rate based on the 99th percentile of usage for the month (looking at you, Datadog). It should surprise nobody that what appears on their pricing page isn’t even the final word. Some of these companies are, even now, looking at redefining their interpretation of the “consumption based pricing” concept that might make things even more opaque (looking at you AGAIN, New Relic). Even with all of that said, I’m going out on a limb and stating for the record that each and every one of those price points is so low that even the word “trivial” is too big. That is, until the production workloads meet the pricing sheet. At that point those itty bitty numbers add up to real money, and quickly.I put this question out to some friends, asking if they had real-world sticker-shock experiences. As always, my friends did not disappoint.“I did a detailed price comparison of New Relic with Datadog a couple years ago with Fargate as the main usage. New Relic was significantly cheaper until you started shipping logs and then Datadog was suddenly 30-40% cheaper even with apm. [But] their per host cost also factors in and makes APM rather unattractive unless you’re doing something serverless. We wanted to use it on kubernetes but it was so expensive, management refused to believe the costs with services on Fargate so I was usually showing my numbers every 2-3 months.” – Evelyn Osman, Head of Platform at enmacc“All I got is the memory of the CFOs face when he saw the bill.” – someone who prefers to remain anonymous, even though that quote is freaking epic.The First Step is Admitting You Have a ProblemOnce upon a time (by which I mean the early 2000’s), the challenge with monitoring (observability wasn’t a term we used yet) was how to identify the data we needed, and then get the systems to give up that data, and then store that data in a way that made it possible (let alone efficient) to use in queries, displays, alerts, and such.That was where almost all the cost rested. The systems themselves were on-premises and, once the hardware was bought, effectively “free”. The result was that the accepted practice was to collect as much as possible and keep it forever. And despite the change in technology, many organizations reasoning has remained the same.Grafana Solutions Architect Alec Isaacson points out his conversations with customers sometimes go like this:“I collect CDM metrics from my most critical systems every 5 seconds because once, a long time ago, someone got yelled at when the system was slow and the metrics didn’t tell them why.”Today, collecting monitoring and observability data (“telemetry”) is comparatively easy, but – both as individuals and organizations – we haven’t changed our framing of the problem. So we continue to grab every piece of data available to us. We instrument our code with every tag and span we can think of; if there’s a log message, we ship it; hardware metrics? Better grab it because it’ll provide context; If there’s network telemetry (NetFlow, VPC Flow logs, Streaming Telemetry) we suck that up too.But we never take the time to think about what we’re going to do with it. Ms. Osman’s experience illustrates the result:“[They] had no idea what they were doing with monitoring […] all the instrumentation and logging was enabled then there was lengthy retention “just in case”. So they were just burning ridiculous amounts of money”To connect it to another bad behavior that we’ve (more or less) broken ourselves of: Back in the early days of “lift and shift” (often more accurately described as “lift and shit”) to the cloud, we not only moved applications wholesale; we moved it onto the biggest systems the platform offered. Why? Because in the old on-prem context you could only ask for a server once, and therefore you asked for the biggest thing you could get, in order to future-proof your investment. This decision turned out not only to be amusingly naive, it was horrifically expensive and it took everyone a few years to understand how “elastic compute” worked and to retool their applications for the new paradigm.Likewise, it’s high time we recognize and acknowledge that we cannot afford to collect every piece of telemetry data available to us, and moreover, that we don’t have a plan for that data even if money was no object.Admit it: Your Problem Also Has a ProblemLet me pivot to OTel for a moment. One of the key reasons – possibly THE key reason – to move to it is to remove, forever and always, the pain of vendor lock-in. This is something I explored in my last blog post and was echoed recently by a friend of mine:OTel does solve a lot of the problems around “Oh great! now we’re trapped with vendor x and it’s going to cost us millions to refactor all this code” as opposed to “Oh, we’re switching vendors? Cool, let me just update my endpoint…”   – Matt Macdonald-Wallace, Solutions Architect, Grafana LabsTo be very clear, OTel does an amazing job at solving this problem, which is incredible in its own right. BUT… there’s a downside to OTel that people don’t notice right away, if they notice it at all. That problem makes the previous problem even worse.OTel takes all of your data (metrics, logs, traces, and the rest), collects it up, and sends it wherever you want it to go. But OTel doesn’t always do it EFFICIENTLY.Let’s take the log message below, which comes straight out of  syslog. Yes, good old RFC 5424. Born in the 80’s, standardized in 2009, and the undisputed “chatty kathy” of network message protocols. I’ve seen modestly-sized networks generate upwards of 4 million syslog messages per hour. Most of it was absolutely useless drivel, mind you. But those messages had to go somewhere and be processed (or dropped) by some system along the way. It’s one of the reasons I’ve suggested a syslog and trap “filtration system” since basically forever.Nit picking about message volume aside, there’s value in some of those messages, to some IT practitioners, some of the time. And so we have to consider (and collect) them too.<134>1 2018-12-13T14:17:40.000Z myserver myapp 10 - [http_method="GET"; http_uri="/example"; http_version="1.1"; http_status="200"; client_addr="127.0.0.1"; http_user_agent="my.service/1.0.0"] HTTP request processed successfullyAs-is that log message is 228 bytes – barely even a drop in the bucket of telemetry you collect every minute, let alone every day. But for what I’m about to do, I want a real apples-to-apples comparison, so here’s what it would look like if I JSON-ified it:{
  "pri": 134,
  "version": 1,
  "timestamp": "2018-12-13T14:17:40.000Z",
  "hostname": "myserver",
  "appname": "myapp",
  "procid": 10,
  "msgid": "-",
  "structuredData": {
      "http_method": "GET",
      "http_uri": "/example",
      "http_version": "1.1",
      "http_status": "200",
      "client_addr": "127.0.0.1",
      "http_user_agent": "my.service/1.0.0"
        },
  "message": "HTTP request processed successfully"
}That bumps the payload up to 336 bytes without whitespace, or 415 bytes with. Now, for comparison, here’s a sample OTLP Log message:{
   "resource": {
      "service.name": "myapp",
      "service.instance.id": "10",
      "host.name": "myserver"
      },
   "instrumentationLibrary": {
      "name": "myapp",
      "version": "1.0.0"
      },
   "severityText": "INFO",
   "timestamp": "2018-12-13T14:17:40.000Z",
   "body": {
      "text": "HTTP request processed successfully"
       },
   "attributes": {
      "http_method": "GET",
      "http_uri": "/example",
      "http_version": "1.1",
      "http_status": "200",
      "client_addr": "127.0.0.1",
      "http_user_agent": "my.service/1.0.0"
       }
}That (generic, minimal) message weighs in at 420 bytes (without whitespace; it’s 520 bytes all-inclusive). It’s still tiny, but even so the OTel version with whitespace is 25% bigger than the JSON-ified message (with whitespace), and more than twice as large as the original log message. Once we start applying real-world data, things balloon even more. My point here is this: If OTel does that to every log message, these tiny costs add up quickly.It turns out that modern methods of metric management are just as susceptible to inflation.A typical prometheus metric, formatted in JSON, is 291 bytes.But that same metric converted to OTLP metrics format weighs in at 751 bytes.It’s true, OTLP has a batching function that mitigates this, but that only helps with transfer over the wire. Once it arrives at the destination, many (not all, but most) vendors unbatch before storing, so it goes back to being 2.5x larger than the original message. As my buddy Josh Biggley has said, “2.5x metrics ingest better have a fucking amazing story to tell about context to justify that cost.”It’s Not You, OTel, It’s Us. (But It’s Also You)If this all feels a little hyper-critical of OTel, then please give me a chance to explain. I honestly believe that OTel is an amazing advancement and anybody who’s serious about monitoring and observability needs to adopt it as a standard – that goes for users as well as vendors. The ability to emit the braid of logs, metrics, traces while maintaining its context, regardless of destination, is invaluable.(But…) OTel was designed by (and for) software engineers. It originated in that bygone era (by which I mean “2016”) when we were still more concerned about the difficulty of getting the data than the cost of moving, processing, and storing it. OTel is, by design, biased to volume.The joke of this section’s title notwithstanding, the problem really isn’t OTel. We really are at fault. Specifically our unhealthy relationship with telemetry. If we insist on collecting and transmitting every single data point, we have nobody to blame but ourselves for the sky-high bills we receive at the end of the month.Does This Data Bring You Joy?It’s easy to let your observability solution do the heavy lifting and shunt every byte of data into a unified interface. It’s easy to do if you’re a software engineer who (nominally at least) owns the monitoring and observability solutions.It’s even easier if you’re a mere consumer of those services, an innocent bystander. Folks who fall into this category include those closely tied to a particular silo (database, storage, network, etc); or helpdesk and NOC teams who receive the tickets and provide support but aren’t involved in the instrumentation nor the tools the instrumentation is connected to; or teams with more specialized needs that nevertheless overlap with monitoring and observability, like information security.But let’s be honest, if you’re a security engineer, how can you justify paying twice cost to ingest logs or metrics, versus the perfectly good standards that already exist and have served well for years? Does that mean you might be using more than one tool? Yes. But as I have pointed out (time and time and time and time and time and time again) there is not (and never has been, and never will be) a one-size-fits-all solution. And in most situations there’s not even a one-size-fits-MOST solution. Monitoring and observability has always been about heterogeneous implementations. The sooner you embrace that ideal, the sooner you will begin building observability ecosystems that serve the needs of you, your team, and your business.To that end there’s a serious ROI discussion to be had before you go all in on OTel or any observability solution.We’ve seen the move from per seat (or interface, or chassis, or CPU) pricing to a consumption model in the marketplace in the past. And we’ve also seen technologies move back (like the way cell service moved from per-minute or per-text to unlimited data with a per-month charge). I suspect we may see a similar pendulum swing back with monitoring and observability at some time in the future. But for now, we have to contend with both the prevailing pricing system as it exists today; and with our own compulsion – born at a different point in the history of monitoring – to collect, transmit, and store every bit (and byte) of telemetry that passes beneath our nose.Of course, cost isn’t the only factor. Performance, risk, (and more) need to be considered. But at the heart of it all is the very real need for us to start asking ourselves:What will I do with this data?How long do I need to store it?And of course, Who the hell is going to pay for it?]]></content:encoded></item><item><title>I built a label-aware PostgreSQL proxy for Kubernetes – supports TLS, pooling, dynamic service discovery (feedback + contributors welcome!)</title><link>https://www.reddit.com/r/kubernetes/comments/1lo81ev/i_built_a_labelaware_postgresql_proxy_for/</link><author>/u/dewelopercloud</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:50:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I've been working on a Kubernetes-native PostgreSQL proxy written in Go, built from scratch with a focus on dynamic routing, TLS encryption, and full integration with K8s labels.TLS termination with auto-generated certificates (via cert-manager)Dynamic service discovery via Kubernetes labelsDeployment-based routing (usernames like )Optional connection pooling support (e.g. PgBouncer)Works with any PostgreSQL deployment (single, pooled, cluster)Super lightweight (uses ~0.1-0.5 vCPU / 18-60MB RAM under load)This is currently production-tested in my own hosting platform. I'd love your feedback — and if you're interested in contributing, the project could easily be extended to support MySQL or MongoDB next.Looking forward to any ideas, improvements, or contributions 🙌]]></content:encoded></item><item><title>Introducing Ansic; a blazing fast, proc macro - zero overhead way to style with ansi!</title><link>https://www.reddit.com/r/rust/comments/1lo7q9e/introducing_ansic_a_blazing_fast_proc_macro_zero/</link><author>/u/Pitiful-Run983</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:37:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ the new crate solving the pain of building ansi styled applications with  and the magic of proc macros -- with 🚀  runtime overhead and  supportMost ansi crates uses  syntax for styling and display types and calculates the ansi style at runtime, which for applications with alot of styles or optimizations isn't ideal. Other crates also don't have a clean reusable model for ansi strings and alot of weird chaining and storing methods are used for it to be used.Ansic solves those problems with a clean and reusable proc macro which uses a clean and convenient DSL which outputs raw string literals at compile time for  runtime overhead.The  macro is the foundation of ansic, in here you write all your DSL expressions to define a style and it spits out the raw &str literal.Ansic has two different types of expressions separated by a space for each; Styles and colors.Colors: Colors are simply written with their names (like "green" and "red) but every single color supports extra arguments prefixed or postfixed by writing the format:  where each argument is with a dot before the color. There are two arguments for colors:(by default if you dont provide the bg argument to a color its treated as a foreground color)so let's say you want to make a ansi style which is a bright red foreground, then you can write , and it will output the string literal for the ansi equivalent, and if you want a bright red foreground with a bright green background you can do ansi!(br.red bg.br.green). We also support 24bit rgb with the color syntax .We also have styles (they don't take arguments) with for example the  and  styles with ansi!(br.red underline bg.green bold) for example (bright red foreground underline green background and bold ansi).Ansic also encourages a consistent and reusable and simple architecture for styling with const:use ansic::ansi; const ERROR: &'static str = ansi!(red bold underline); const R: &'static str = ansi!(reset); fn main() { println!("{ERROR}ERROR: something wrong happened!{R}"); } This encourages a reuseable, elegant and very easy way to style which muss less overhead (contradicting other crates with much less readable and elegant styles of styling)🛠️ I built  because I was frustrated with other ansi crates:It was hard to read, reuse and manage weirdly chained styles, and I hated that every time I used it there was a runtime calculation to make the styles even work. I love  because it solves all those problems with a clean reuseable model, DSL which is easy to read and maintain, and a proc macro which does everything at compile time.I'm also 100% open to feedback, reviews, discussions and criticism!🎨 Add ansic to style with ansi with !🌟 If you like the project, please consider leaving a star on our GitHub page!]]></content:encoded></item><item><title>Microsoft Says Its New AI System Diagnosed Patients 4 Times More Accurately Than Human Doctors</title><link>https://www.wired.com/story/microsoft-medical-superintelligence-diagnosis/</link><author>/u/wiredmagazine</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:31:47 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[ “a genuine step toward medical superintelligence,” says Mustafa Suleyman, CEO of the company’s artificial intelligence arm. The tech giant says its powerful new AI tool can diagnose disease four times more accurately and at significantly less cost than a panel of human physicians.The experiment tested whether the tool could correctly diagnose a patient with an ailment, mimicking work typically done by a human doctor.The Microsoft team used 304 case studies sourced from the New England Journal of Medicine to devise a test called the Sequential Diagnosis Benchmark. A language model broke down each case into a step-by-step process that a doctor would perform in order to reach a diagnosis.Microsoft’s researchers then built a system called the MAI Diagnostic Orchestrator (MAI-DxO) that queries several leading AI models—including OpenAI’s GPT, Google’s Gemini, Anthropic’s Claude, Meta’s Llama, and xAI’s Grok—in a way that loosely mimics several human experts working together.In their experiment, MAI-DxO outperformed human doctors, achieving an accuracy of 80 percent compared to the doctors’ 20 percent. It also reduced costs by 20 percent by selecting less expensive tests and procedures."This orchestration mechanism—multiple agents that work together in this chain-of-debate style—that's what's going to drive us closer to medical superintelligence,” Suleyman says.The company poached several Google AI researchers to help with the effort—yet another sign of an intensifying war for top AI expertise in the tech industry. Suleyman was previously an executive at Google working on AI.AI is already widely used in some parts of the US health care industry, including helping radiologists interpret scans. The latest multimodal AI models have the potential to act as more general diagnostic tools, though the use of AI in health care raises its own issues, particularly related to bias from training data that’s skewed toward particular demographics.Microsoft has not yet decided if it will try to commercialize the technology, but the same executive, who spoke on the condition of anonymity, said the company could integrate it into Bing to help users diagnose ailments. The company could also develop tools to help medical experts improve or even automate patient care. “What you'll see over the next couple of years is us doing more and more work proving these systems out in the real world,” Suleyman says.The project is the latest in a growing body of research showing how AI models can diagnose disease. In the last few years, both Microsoft and Google have published papers showing that large language models can accurately diagnose an ailment when given access to medical records.The new Microsoft research differs from previous work in that it more accurately replicates the way human physicians diagnose disease—by analyzing symptoms, ordering tests, and performing further analysis until a diagnosis is reached. Microsoft describes the way that it combined several frontier AI models as “a path to medical superintelligence” in a blog post about the project today.The project also suggests that AI could help lower health care costs, a critical issue, particularly in the US. "Our model performs incredibly well, both getting to the diagnosis and getting to that diagnosis very cost effectively," says Dominic King, a vice president at Microsoft who is involved with the project.]]></content:encoded></item><item><title>Changing max pods limit in already established cluster - Microk8s</title><link>https://www.reddit.com/r/kubernetes/comments/1lo7cxk/changing_max_pods_limit_in_already_established/</link><author>/u/BunkerFrog</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:20:47 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi, I do have quite beefy setup. Cluster of 4x 32core/64thread with 512GB RAM. Nodes are bare metal. I used stock setup with stock config of microk8s and while there was no problem I had reached limit of 110 pods/node. There are still plenty of system resources to utilize - for now using like 30% of CPU and RAM / node.Question #1: Can I change limit on already running cluster? (there are some posts on internet that this change can only be done during cluster/node setup and can't be changed later)Question #2: If it is possible to change it on already established cluster, will it be possible to change it via "master" or need to be changed manually on each nodeQuestion #3: What real max should I use to not make my life with networking harder? (honestly I would be happy if 200 would pass)]]></content:encoded></item><item><title>rust-analyzer changelog #292</title><link>https://rust-analyzer.github.io/thisweek/2025/06/30/changelog-292.html</link><author>/u/WellMakeItSomehow</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 13:17:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Here is My 3D Pool Simulator for Linux</title><link>https://github.com/sysrpl/Raylib.4.0.Pascal/blob/master/examples/table/README.md</link><author>/u/sysrpl</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 12:24:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I&apos;m getting an error after certificate renewal please help</title><link>https://www.reddit.com/r/kubernetes/comments/1lo509h/im_getting_an_error_after_certificate_renewal/</link><author>/u/Known_Wallaby_1821</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 11:23:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello, My Kubernetes cluster was running smoothly until I tried to renew the certificates after they expired. I ran the following commands:sudo kubeadm certs renew allecho 'export KUBECONFIG=/etc/kubernetes/admin.conf' >> ~/.bashrcAfter that, some abnormalities started to appear in my cluster. Calico is completely down and even after deleting and reinstalling it, it does not come back up at all.When I check the daemonsets and deployments in the kube-system namespace, I see:kubectl get daemonset -n kube-systemNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkubectl get deployments -n kube-systemNAME READY UP-TO-DATE AVAILABLE AGEcalico-kube-controllers 0/1 0 0 4m19sBefore this, I was also getting "unauthorized" errors in the kubelet logs, which started after renewing the certificates. This is definitely abnormal because the pods created from deployments are not coming up and remain stuck.There is no error message shown during deployment either. Please help.]]></content:encoded></item><item><title>After nine years, Ninja has merged support for the GNU Make jobserver</title><link>https://thebrokenrail.com/2025/06/30/ninja-jobserver.html</link><author>/u/TheBrokenRail-Dev</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 11:00:15 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[And I promise you: this is awesome!Ninja is a build system like GNU Make: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one’s dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.Let’s focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!Except… what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.And this is a real problem affecting real projects. Features like CMake’s  often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see thesediscussionposts.But what if I told you this problem has already been solved… back in 1999?Introducing: The GNU Make JobserverThis was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been “solved” by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like Meson did not support GNU Make. This meant developers still needed Ninja.Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including one from Kitware, the company behind CMake).Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? …Right?As you might have guessed, there are multiple catches.For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or some other server implementation.Another issue is that on Linux it only supports the named pipe/FIFO implementation of the jobserver. This was released with GNU Make 4.4 back in October 2022, which was only added to Debian Trixie just a few months ago, in December 2024. This means you will need an extremely recent OS. Otherwise, GNU Make’s jobserver will not be compatible with Ninja.And finally, as of writing this post, Ninja v1.13.0 (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.]]></content:encoded></item><item><title>After nine years, Ninja has merged support for the GNU Make jobserver</title><link>https://thebrokenrail.com/2025/06/30/ninja-jobserver.html</link><author>/u/TheBrokenRail-Dev</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:59:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[And I promise you: this is awesome!Ninja is a build system like GNU Make: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one’s dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.Let’s focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!Except… what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.And this is a real problem affecting real projects. Features like CMake’s  often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see thesediscussionposts.But what if I told you this problem has already been solved… back in 1999?Introducing: The GNU Make JobserverThis was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been “solved” by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like Meson did not support GNU Make. This meant developers still needed Ninja.Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including one from Kitware, the company behind CMake).Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? …Right?As you might have guessed, there are multiple catches.For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or some other server implementation.Another issue is that on Linux it only supports the named pipe/FIFO implementation of the jobserver. This was released with GNU Make 4.4 back in October 2022, which was only added to Debian Trixie just a few months ago, in December 2024. This means you will need an extremely recent OS. Otherwise, GNU Make’s jobserver will not be compatible with Ninja.And finally, as of writing this post, Ninja v1.13.0 (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.]]></content:encoded></item><item><title>Result in C++</title><link>https://github.com/Jarsop/cpp_result</link><author>/u/Jarsop</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:47:50 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Rust developer since more than 8 years ago, I really annoyed when I use other languages without  API. In C++ we have  (since c++17) and  (since c++23) but I don’t think it’s really convenient. This how I decided to create , a more ergonomic API which try to mimic Rust  type. Macros are also provided to mimic the  operator. Any feedback is very welcomed.]]></content:encoded></item><item><title>Bardcore Portfolio - Powered by Go</title><link>https://www.reddit.com/r/golang/comments/1lo45qd/bardcore_portfolio_powered_by_go/</link><author>/u/ArinjiBoi</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:34:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey Everyone! I just finished working on a portfolio site themed around "bardcore", its a site i made for my music friend to showcase her songs. I am using Pocketbase for the backend with a golang proxy to have the music stored in google drive be playable on the site]]></content:encoded></item><item><title>The Evolution of Caching Libraries in Go</title><link>https://maypok86.github.io/otter/blog/cache-evolution/</link><author>/u/Ploobers</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:20:26 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Exception handling in rustc_codegen_cranelift</title><link>https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift</link><author>/u/Expurple</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:16:49 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We will use the following example to illustrate the various cases that commonly occur:// For do_catch
#![feature(rustc_attrs, core_intrinsics)]
#![allow(internal_features)]
struct Droppable;
impl Drop for Droppable {
    fn drop(&mut self) {}
}
// Unwind without running any drops
#[no_mangle]
fn do_panic() {
    std::panic::panic_any(());
}
// Unwind while running a drop on the cleanup path
#[no_mangle]
fn some_func() {
    let _a = Droppable;
    do_panic();
}
// Catch a panic
#[no_mangle]
fn do_catch_panic() {
    // This has a simplified version of std::panic::catch_unwind inlined for ease of understanding
    unsafe {
        if std::intrinsics::catch_unwind(do_call, 0 as *mut _, do_catch) == 0 {
            std::process::abort(); // unreachable
        } else {
            // Caught panic
        };
    }
    #[inline]
    fn do_call(_data: *mut u8) {
        some_func();
    }
    #[inline]
    #[rustc_nounwind] // `intrinsic::catch_unwind` requires catch fn to be nounwind
    fn do_catch(_data: *mut u8, _panic_payload: *mut u8) {}
}
fn main() {
    do_catch_panic();
}
Let's first compile this using a version of  with unwinding enabled:dist/rustc-clif panic_example.rs -Cdebuginfo=2 --emit link,mir,llvm-ir
This command enables debuginfo, and emits three artifacts:  emits the normal executable,  emits MIR, and  is repurposed with  to emit Cranelift IR (clif ir for short). In any case with the executable now compiled, let's run it in a debugger:$ gdb ./panic_example
Reading symbols from ./panic_example...
We begin by setting a breakpoint in :(gdb) break do_panic
Breakpoint 1 at 0x38794: file panic_example.rs, line 13.
(gdb) run
Downloading separate debug info for system-supplied DSO at 0xfffff7ffb000
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
Breakpoint 1, panic_example::do_panic () at panic_example.rs:13
13          std::panic::panic_any(());
(gdb) backtrace
#0  panic_example::do_panic () at panic_example.rs:13
#1  0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20
#2  0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37
#3  0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28
#4  0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46
[...]
(gdb) break _Unwind_RaiseException
Breakpoint 2 at 0xfffff7f975e8: file ../../../src/libgcc/unwind.inc, line 93
(gdb) continue
Continuing.
thread 'main' panicked at panic_example.rs:13:5:
Box<dyn Any>
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
Breakpoint 2, _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93
warning: 93     ../../../src/libgcc/unwind.inc: No such file or directory
(gdb) bt
#0  _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93
#1  0x0000aaaaaabde42c in panic_unwind::imp::panic () at library/panic_unwind/src/gcc.rs:72
#2  0x0000aaaaaabdde3c in panic_unwind::__rust_start_panic () at library/panic_unwind/src/lib.rs:103
#3  0x0000aaaaaaae977c in std::panicking::rust_panic () at library/std/src/panicking.rs:894
#4  0x0000aaaaaaae959c in std::panicking::rust_panic_with_hook () at library/std/src/panicking.rs:858
#5  0x0000aaaaaaad7bcc in std::panicking::begin_panic::{closure#0}<()> () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:770
#6  0x0000aaaaaaad7b20 in std::sys::backtrace::__rust_end_short_backtrace<std::panicking::begin_panic::{closure_env#0}<()>, !> ()
    at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/sys/backtrace.rs:168
#7  0x0000aaaaaaad7b70 in std::panicking::begin_panic<()> () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:769
#8  0x0000aaaaaaad7b4c in std::panic::panic_any<()> () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panic.rs:260
#9  0x0000aaaaaaad87a0 in panic_example::do_panic () at panic_example.rs:13
#10 0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20
#11 0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37
#12 0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28
#13 0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46
[...]
We can validate that the exception is in fact a Rust exception by running:(gdb) print exc
$1 = (struct _Unwind_Exception *) 0xaaaaaace1ce0
(gdb) print *exc
$2 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 <panic_unwind::imp::panic::exception_cleanup>, private_1 = 0, private_2 = 0}
(gdb) print (char[8])(exc.exception_class)
$3 = "MOZ\000RUST"
That looks a lot like a Rust exception to me. The rest of the exception data is located directly after the  struct.Unwinding ABI crash courseThere are nowadays two major unwinder ABIs still in use for C++ exceptions and Rust panics. These are:SEH (Structured Exception Handling) on WindowsItanium unwinding ABI (originating from the infamous Intel cpu architecture) on most Unix systems.SEH and Itanium unwinding have a similar architecture: there is a table that indicates, for each instruction from which an exception may be thrown, which registers need to be restored to unwind the stack to the caller as well as contains a reference to a function (the so called personality function) which interprets a language-specific data format and a reference to some data in this format (called LSDA or language-specific data area for Itanium unwinding).In most cases there is a single personality function for each language. Rust generally uses  as personality function.  For the LSDA, Rust uses the exact same format as GCC and Clang use for C++ despite not needing half its features because LLVM doesn't allow frontends to specify a custom format.Both SEH and Itanium unwinding implement two-phase unwinding. In other words, they first do a scan over the stack to see if any function catches the exception (phase one) before actually unwinding (phase two). For this in the first phase SEH and Itanium unwinders call the personality function to check if there is a catch for the exception around the given call site. To do this, the personality function parses the LSDA looking up the entry for the current instruction pointer.In the second phase the personality function is called again and this time it is given the chance to divert execution to an exception handler. In the case of SEH this exception handler is a so-called "funclet": a function which gets the stack pointer of the stack frame currently being unwound as argument, and unwinding resumes when this funclet returns.For Itanium unwinding on the other hand, execution gets diverted to a "landingpad" which runs in the context of the stack frame being unwound. Unwinding either resumes when the landingpad calls  or in the case of a catch, the landingpad just continues execution as usual.With the SEH method, all stack frames remain on the stack until unwinding has finished. It is also possible to unwind without removing any stack frames. Itanium unwinding instead removes each stack frame from the stack after it has been unwound, so effectively throwing an exception is an alternative return of the function. Cranelift currently only supports unwinding mechanisms that use landingpads, which is why cg_clif doesn't support unwinding on Windows.While dwarfdump can be used to show part of the unwind info in a human-readable way, I'm not aware of any tool that is capable of showing the entire unwind info in a human-readable way: dwarfdump does not parse the LSDA, and there is no option to interleave assembly instructions and unwind instructions. As such I wrote my own tool for this, which I will use to show how exactly the unwinder sees our functions:$ git clone https://github.com/bjorn3/rust_unwind_inspect.git
$ cd rust_unwind_inspect
$ cargo build
$ cp target/debug/rust_unwind_inspect ../
Unwinding without exception handlersNow on to showing how Itanium unwinding support is actually implemented in cg_clif. I'm going to skip ahead to the second phase of the unwinding process -- the actual unwinding -- for the sake of simplicity.Let's start with the  function://- panic_example.mir

// [snip]
fn do_panic() -> () {
    let mut _0: ();
    let _1: !;

    bb0: {
        _1 = panic_any::<()>(const ()) -> unwind continue;
    }
}
// [snip]
This is a simple function which consists of nothing other than a  call which never returns, and when it unwinds, it continues to the caller.;- panic_example.clif/do_panic.unopt.clif
function u0:28() system_v {
    gv0 = symbol colocated userextname0
    sig0 = (i64) system_v
    fn0 = colocated u0:6 sig0 ; Instance { def: Item(DefId(1:5518 ~ std[a023]::panic::panic_any)), args: [()] }
block0:
    jump block1
block1:
; _1 = std::panic::panic_any::<()>(const ())
    v0 = global_value.i64 gv0
    call fn0(v0)
    trap user1
}
Nothing too exciting here.  gets lowered to a regular call of . The argument is an implicit argument of type  because  is marked with . When unwinding out of a  clif ir instruction, this will continue unwinding out of the current function.  shows the following:$ ./rust_unwind_inspect panic_example do_panic
000000000003878c <do_panic>:
  personality: 0x38db0 <rust_eh_personality+0x0>
  LSDA: 0x1fd900 <.gcc_except_table+0x1e4>
  0x3878c: stp x29, x30, [sp, #-0x10]!
    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)
  0x38790: mov x29, sp
  0x38794: adrp x0, #0x235000
  0x38798: ldr x0, [x0, #0x578]
  0x3879c: bl #0x37b40
    call site 0x3879f..0x387a0 action=continue
Here personality and LSDA are as explained in the previous section. The CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8) line tells us that can be found at offset -16 from  can be found at offset -8 from This information is all coming from the language independent half of the unwind tables which is found in . This is what the unwinder itself parses. In addition there is a line call site 0x317a3..0x317a4 action=continue which indicates that the previous instruction is a call which, if it throws an exception, should cause unwinding to continue to the caller of . This information comes from the LSDA found in  at offset 0x1e0. If no call site is found for a call that threw an exception, the personality function will indicate to the unwinder that unwinding should abort.Now to see it in action in the debugger:First we define a macro that allows us to set a breakpoint for the personality function getting executed for a given call site:(gdb) define break_on_personality_for
set language c
b rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0
set language auto
end
And now we can set a breakpoint for  and continue:(gdb) break_on_personality_for panic_example::do_panic+20
Breakpoint 4 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.
(gdb) continue
Continuing.
Breakpoint 4, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                            rust_eh_personality_impl(
(gdb) up
#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)
    at ../../../src/libgcc/unwind.inc:64
warning: 64     ../../../src/libgcc/unwind.inc: No such file or directory
(gdb) print context.ra
$4 = (void *) 0xaaaaaaad87a0 <panic_example::do_panic+20>
(gdb) down
#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                            rust_eh_personality_impl(
And to show the return value:(gdb) finish
Run till exit from #0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66
warning: 66     ../../../src/libgcc/unwind.inc: No such file or directory
Value returned is $5 = "\b\000\000"
(gdb) print (_Unwind_Reason_Code)$x0
$6 = _URC_CONTINUE_UNWIND
We had to explicitly read the return value from register x0 because cg_clif currently doesn't emit debuginfo for arguments and return types. We also had to use ((struct _Unwind_Context *)$x4).ra == $arg0 as condition for the breakpoint for this reason.Unwinding with an exception handlerMore exciting is the case where there is an exception handler in scope like our  function.//- panic_example.mir

// [snip]
fn some_func() -> () {
    let mut _0: ();
    let _1: Droppable;
    let _2: ();
    scope 1 {
        debug _a => const Droppable;
    }

    bb0: {
        _2 = do_panic() -> [return: bb1, unwind: bb3];
    }

    bb1: {
        drop(_1) -> [return: bb2, unwind continue];
    }

    bb2: {
        return;
    }

    bb3 (cleanup): {
        drop(_1) -> [return: bb4, unwind terminate(cleanup)];
    }

    bb4 (cleanup): {
        resume;
    }
}
// [snip]
This function first calls  and then, no matter if it unwinds or not, it runs the drop glue for the  value in . If the drop glue unwinds when called within the unwind path, the function will abort, otherwise it will unwind. And finally if the drop glue succeeds within the unwind path, unwinding will resume thanks to the  terminator.; panic_example.clif/some_func.unopt.clif
function u0:29() system_v {
    sig0 = () system_v
    sig1 = (i64) system_v
    sig2 = (i64) system_v
    sig3 = () system_v
    sig4 = (i64) system_v
    fn0 = colocated u0:28 sig0 ; Instance { def: Item(DefId(0:7 ~ panic_example[4533]::do_panic)), args: [] }
    fn1 = colocated u0:14 sig1 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }
    fn2 = colocated u0:14 sig2 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }
    fn3 = u0:47 sig3 ; "_ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E"
    fn4 = u0:36 sig4 ; "_Unwind_Resume"
block0:
    jump block1
block1:
; _2 = do_panic()
    try_call fn0(), sig0, block6, [ tag0: block7(exn0) ]
block7(v0: i64) cold:
    v4 -> v0
    jump block4
block6:
    jump block2
block2:
; drop(_1)
    v1 = iconst.i64 1
    call fn1(v1)  ; v1 = 1
    jump block3
block3:
    return
block4 cold:
; drop(_1)
    v2 = iconst.i64 1
    try_call fn2(v2), sig2, block5, [ tag0: block9(exn0) ]  ; v2 = 1
block9(v3: i64) cold:
; panic _ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E
    call fn3()
    trap user1
block5 cold:
; lib_call _Unwind_Resume
    call fn4(v4)
    trap user1
}
This is the clif ir produced for . The  call gets lowered to a  rather than a regular  because this time we want to divert execution to another code path in case of unwinding. In the try_call fn0(), sig0, block6, [ tag0: block7(exn0) ],  is where execution continues if the call returns normally, while  is where execution will continue when unwinding. The  part indicates that  will get the first register set by the personality function as a block argument. In the case of Rust, this will be a pointer to the exception itself. Other languages may use additional "landingpad arguments". The  part is some opaque metadata that Cranelift will forward to cg_clif together with the position of all call sites and landingpads. cg_clif uses  to indicate a cleanup block and  to indicate that an exception should be caught. Once all cleanup code has run, a call to  will be made with the exception pointer as argument to resume unwinding.  will pop the stack frame of the caller and then continue unwinding as usual.$ ./unwind_inspect/target/debug/rust_unwind_inspect ./panic_example some_func
00000000000387a4 <some_func>:
  personality: 0x38db0 <rust_eh_personality+0x0>
  LSDA: 0x1fd910 <.gcc_except_table+0x1f4>
  0x387a4: stp x29, x30, [sp, #-0x10]!
    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)
  0x387a8: mov x29, sp
  0x387ac: str x20, [sp, #-0x10]!
    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32)
  0x387b0: bl #0x3878c
    call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue
  0x387b4: mov x0, #1
  0x387b8: bl #0x37dc0
    call site 0x387bb..0x387bc action=continue
  0x387bc: ldr x20, [sp], #0x10
  0x387c0: ldp x29, x30, [sp], #0x10
  0x387c4: ret
  0x387c8: mov x20, x0
  0x387cc: mov x0, #1
  0x387d0: bl #0x37dc0
    call site 0x387d3..0x387d4 landingpad=0x387e8 action=continue
  0x387d4: adrp x1, #0x23f000
  0x387d8: ldr x1, [x1, #0xdc8]
  0x387dc: mov x0, x20
  0x387e0: blr x1
    call site 0x387e3..0x387e4 action=continue
Our  call has call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue as unwind info. This indicates that if the call unwinds, execution should jump to address . The Rust personality function will also set  (aka  in clif ir) to the exception pointer.(gdb) break_on_personality_for panic_example::some_func+16
Breakpoint 5 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.
(gdb) continue
Breakpoint 5, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
(gdb) up
#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)
    at ../../../src/libgcc/unwind.inc:64
warning: 64     ../../../src/libgcc/unwind.inc: No such file or directory
(gdb) print context.ra
$7 = (void *) 0xaaaaaaad87b4 <panic_example::some_func+16>
down
#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
We got to the personality function call for . Now let's set a couple of breakpoints to see how the personality function causes execution to jump to the landingpad:(gdb) break _Unwind_SetGR
Breakpoint 6 at 0xfffff7f9494c: file ../../../src/libgcc/unwind-dw2.c, line 275.
(gdb) break _Unwind_SetIP
Breakpoint 7 at 0xfffff7f949e0: file ../../../src/libgcc/unwind-dw2.c, line 369.
 and  are functions called by the personality function to tell the unwinder how to run the landingpad.(gdb) continue
Breakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275
warning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory
(gdb) print *(struct _Unwind_Exception *)val
$8 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 <panic_unwind::imp::panic::exception_cleanup>, private_1 = 0, private_2 = 281474976706176}
(gdb) continue
Breakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275
275     in ../../../src/libgcc/unwind-dw2.c
The first thing the personality function does is use  to set the aformentioned "landingpad arguments". x0 is set to the exception pointer, while x1 is set to zero. The latter isn't needed for cg_clif, but cg_llvm generates landingpads that take an additional i32 argument even though it doesn't do anything with it. I believe C++ uses it for the exception type. I suspect at some point LLVM didn't handle landingpads which are missing this extra argument.(gdb) continue
Breakpoint 7, _Unwind_SetIP (context=0xffffffffe9f0, val=187649984661448) at ../../../src/libgcc/unwind-dw2.c:369
369     in ../../../src/libgcc/unwind-dw2.c
(gdb) set $landingpad=val
Next up  is used to set the address of the landingpad. We save this address here to set a breakpoint on it later on.(gdb) up 2
#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
(gdb) finish
_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66
warning: 66     ../../../src/libgcc/unwind.inc: No such file or directory
Value returned is $9 = "\a\000\000"
(gdb) p (_Unwind_Reason_Code)$x0
$10 = _URC_INSTALL_CONTEXT
The personality function returns  to indicate that there is a landingpad.(gdb) break *$landingpad
Breakpoint 8 at 0xaaaaaaad87c8: file panic_example.rs, line 21.
(gdb) continue
Breakpoint 8, 0x0000aaaaaaad87c8 in panic_example::some_func () at panic_example.rs:21
21      }
(gdb) disassemble
Dump of assembler code for function panic_example::some_func:
   0x0000aaaaaaad87a4 <+0>:     stp     x29, x30, [sp, #-16]!
   0x0000aaaaaaad87a8 <+4>:     mov     x29, sp
   0x0000aaaaaaad87ac <+8>:     str     x20, [sp, #-16]!
   0x0000aaaaaaad87b0 <+12>:    bl      0xaaaaaaad878c <panic_example::do_panic>
   0x0000aaaaaaad87b4 <+16>:    mov     x0, #0x1                        // #1
   0x0000aaaaaaad87b8 <+20>:    bl      0xaaaaaaad7dc0 <_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E>
   0x0000aaaaaaad87bc <+24>:    ldr     x20, [sp], #16
   0x0000aaaaaaad87c0 <+28>:    ldp     x29, x30, [sp], #16
   0x0000aaaaaaad87c4 <+32>:    ret
=> 0x0000aaaaaaad87c8 <+36>:    mov     x20, x0
   0x0000aaaaaaad87cc <+40>:    mov     x0, #0x1                        // #1
   0x0000aaaaaaad87d0 <+44>:    bl      0xaaaaaaad7dc0 <_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E>
   0x0000aaaaaaad87d4 <+48>:    adrp    x1, 0xaaaaaacdf000
   0x0000aaaaaaad87d8 <+52>:    ldr     x1, [x1, #3528]
   0x0000aaaaaaad87dc <+56>:    mov     x0, x20
   0x0000aaaaaaad87e0 <+60>:    blr     x1
   0x0000aaaaaaad87e4 <+64>:    udf     #49439
   0x0000aaaaaaad87e8 <+68>:    adrp    x3, 0xaaaaaacdf000
   0x0000aaaaaaad87ec <+72>:    ldr     x3, [x3, #2688]
   0x0000aaaaaaad87f0 <+76>:    blr     x3
   0x0000aaaaaaad87f4 <+80>:    udf     #49439
End of assembler dump.
And finally if we set a breakpoint on the registered landingpad value and continue execution, we indeed see that execution jumped to the landingpad.And finally to finish it up, let's catch a panic using :fn do_catch_panic() -> () {
    let mut _0: ();
    let mut _1: i32;
    let mut _2: fn(*mut u8);
    let mut _3: *mut u8;
    let mut _4: fn(*mut u8, *mut u8);
    let _5: !;

    bb0: {
        _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit));
        _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance);
        _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit));
        _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4) -> [return: bb1, unwind unreachable];
    }

    bb1: {
        switchInt(move _1) -> [0: bb2, otherwise: bb3];
    }

    bb2: {
        _5 = std::process::abort() -> unwind continue;
    }

    bb3: {
        return;
    }
}
 calls the first function pointer with the second argument as argument. If this function unwinds, it will call the second function pointer with the same argument and additionally the exception pointer. And finally it returns 1 if an exception was caught and 0 otherwise.function u0:30() system_v {
    sig0 = (i64) system_v
    sig1 = (i64, i64) system_v
    sig2 = (i64) system_v
    sig3 = (i64, i64) system_v
    sig4 = () system_v
    fn0 = colocated u0:31 sig0 ; Instance { def: Item(DefId(0:10 ~ panic_example[4533]::do_catch_panic::do_call)), args: [] }
    fn1 = colocated u0:32 sig1 ; Instance { def: Item(DefId(0:11 ~ panic_example[4533]::do_catch_panic::do_catch)), args: [] }
    fn2 = u0:44 sig4 ; Instance { def: Item(DefId(1:6188 ~ std[a023]::process::abort)), args: [] }
block0:
    jump block1
block1:
; _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit))
    v0 = func_addr.i64 fn0
; _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance)
    v1 = iconst.i64 0
; _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit))
    v2 = func_addr.i64 fn1
; _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4)
    try_call_indirect v0(v1), sig2, block5, [ tag1: block6(exn0) ]  ; v1 = 0
block5:
    v3 = iconst.i32 0
    jump block2(v3)  ; v3 = 0
block6(v4: i64) cold:
    call_indirect.i64 sig3, v2(v1, v4)  ; v1 = 0
    v5 = iconst.i32 1
    jump block2(v5)  ; v5 = 1
block2(v6: i32):
; switchInt(move _1)
    brif v6, block4, block3
block3 cold:
; _5 = std::process::abort()
    call fn2()
    trap user1
block4:
    return
}
In Cranelift IR this is implemented using a  with  rather than  for the cleanup block. And additionally it won't call  in the end, but rather continue execution after the intrinsic call. In the unwind tables the exception catching is represented using:$ cargo run -- ../panic_example do_catch_panic
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.04s
     Running `target/debug/rust_unwind_inspect ../panic_example do_catch_panic`
00000000000387f8 <do_catch_panic>:
  personality: 0x38db0 <rust_eh_personality+0x0>
  LSDA: 0x1fd930 <.gcc_except_table+0x214>
  LSDA actions:
    0x0: catch 0x0 next=None
  0x387f8: stp x29, x30, [sp, #-0x10]!
    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)
  0x387fc: mov x29, sp
  0x38800: stp x20, x22, [sp, #-0x10]!
    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32) X22=Offset(-24)
  0x38804: adrp x11, #0x235000
  0x38808: ldr x11, [x11, #0x4b8]
  0x3880c: mov x0, #0
  0x38810: mov x22, x0
  0x38814: adrp x20, #0x235000
  0x38818: ldr x20, [x20, #0x4c0]
  0x3881c: blr x11
    call site 0x3881f..0x38820 landingpad=0x38838 action=0
  0x38820: mov w8, #0
  0x38824: mov w15, w8
  0x38828: cbz x15, #0x3884c
  0x3882c: ldp x20, x22, [sp], #0x10
  0x38830: ldp x29, x30, [sp], #0x10
  0x38834: ret
  0x38838: mov x1, x0
  0x3883c: mov x0, x22
  0x38840: blr x20
    call site 0x38843..0x38844 action=continue
  0x38844: mov w8, #1
  0x38848: b #0x38824
  0x3884c: adrp x0, #0x23e000
  0x38850: ldr x0, [x0, #0xab0]
  0x38854: blr x0
    call site 0x38857..0x38858 action=continue
where  references the  LSDA action. In C++ the  would instead be the typeid of the caught exception and  optionally representing another  block for the same  block.And finally one last debugger step through for completeness. It is not much different from the  step through, so I won't discuss it in detail.(gdb) break_on_personality_for panic_example::do_catch_panic+40
Breakpoint 9 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.
(gdb) continue
Breakpoint 9, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
(gdb) up
#1  0x0000fffff7f97354 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8)
    at ../../../src/libgcc/unwind.inc:64
warning: 64     ../../../src/libgcc/unwind.inc: No such file or directory
(gdb) print context.ra
$11 = (void *) 0xaaaaaaad8820 <panic_example::do_catch_panic+40>
(gdb) down
#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
(gdb) continue
Breakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275
warning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory
(gdb) print *(struct _Unwind_Exception *)val
$12 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 <panic_unwind::imp::panic::exception_cleanup>, private_1 = 0, private_2 = 281474976706176}
(gdb) continue
Breakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275
275     in ../../../src/libgcc/unwind-dw2.c
(gdb) continue
Breakpoint 7, _Unwind_SetIP (context=0xffffffffea90, val=187649984661560) at ../../../src/libgcc/unwind-dw2.c:369
369     in ../../../src/libgcc/unwind-dw2.c
(gdb) set $landingpad=val
(gdb) up 2
#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307
307                             rust_eh_personality_impl(
(gdb) finish
_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8) at ../../../src/libgcc/unwind.inc:66
warning: 66     ../../../src/libgcc/unwind.inc: No such file or directory
Value returned is $13 = "\a\000\000"
(gdb) print (_Unwind_Reason_Code)$x0
$14 = _URC_INSTALL_CONTEXT
(gdb) break *$landingpad
Breakpoint 10 at 0xaaaaaaad8838: file panic_example.rs, line 43.
(gdb) continue
Breakpoint 10, 0x0000aaaaaaad8838 in panic_example::do_catch_panic () at panic_example.rs:43
43      }
(gdb) disassemble
Dump of assembler code for function panic_example::do_catch_panic:
   0x0000aaaaaaad87f8 <+0>:     stp     x29, x30, [sp, #-16]!
   0x0000aaaaaaad87fc <+4>:     mov     x29, sp
   0x0000aaaaaaad8800 <+8>:     stp     x20, x22, [sp, #-16]!
   0x0000aaaaaaad8804 <+12>:    adrp    x11, 0xaaaaaacd5000
   0x0000aaaaaaad8808 <+16>:    ldr     x11, [x11, #1208]
   0x0000aaaaaaad880c <+20>:    mov     x0, #0x0                        // #0
   0x0000aaaaaaad8810 <+24>:    mov     x22, x0
   0x0000aaaaaaad8814 <+28>:    adrp    x20, 0xaaaaaacd5000
   0x0000aaaaaaad8818 <+32>:    ldr     x20, [x20, #1216]
   0x0000aaaaaaad881c <+36>:    blr     x11
   0x0000aaaaaaad8820 <+40>:    mov     w8, #0x0                        // #0
   0x0000aaaaaaad8824 <+44>:    mov     w15, w8
   0x0000aaaaaaad8828 <+48>:    cbz     x15, 0xaaaaaaad884c <panic_example::do_catch_panic+84>
   0x0000aaaaaaad882c <+52>:    ldp     x20, x22, [sp], #16
   0x0000aaaaaaad8830 <+56>:    ldp     x29, x30, [sp], #16
   0x0000aaaaaaad8834 <+60>:    ret
=> 0x0000aaaaaaad8838 <+64>:    mov     x1, x0
   0x0000aaaaaaad883c <+68>:    mov     x0, x22
   0x0000aaaaaaad8840 <+72>:    blr     x20
   0x0000aaaaaaad8844 <+76>:    mov     w8, #0x1                        // #1
   0x0000aaaaaaad8848 <+80>:    b       0xaaaaaaad8824 <panic_example::do_catch_panic+44>
   0x0000aaaaaaad884c <+84>:    adrp    x0, 0xaaaaaacde000
   0x0000aaaaaaad8850 <+88>:    ldr     x0, [x0, #2736]
   0x0000aaaaaaad8854 <+92>:    blr     x0
   0x0000aaaaaaad8858 <+96>:    udf     #49439
End of assembler dump.
We've now seen how exception handling works in cg_clif. Currently, this feature is still disabled by default because I'm still in the process of finishing the implementation and fixing a performance regression caused by enabling it. Follow the tracking issue to stay up to date!The following gdb script can be used to reproduce the debugger session:set debuginfod enabled on
set pagination off
b do_panic
run
bt
b _Unwind_RaiseException
c
bt
p exc
p *exc
p (char[8])(exc.exception_class)
b _Unwind_RaiseException_Phase2
c
del 3
define break_on_personality_for
set language c
b rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0
set language auto
end
echo \ndo_panic\n===========================\n
break_on_personality_for panic_example::do_panic+20
c
up
p context.ra
down
finish
p (_Unwind_Reason_Code)$x0
echo \nsome_func\n===========================\n
break_on_personality_for panic_example::some_func+16
c
up
p context.ra
down
b _Unwind_SetGR
b _Unwind_SetIP
c
p *(struct _Unwind_Exception *)val
c
c
set $landingpad=val
up 2
finish
p (_Unwind_Reason_Code)$x0
b *$landingpad
c
disassemble
echo \ndo_catch_panic\n===========================\n
break_on_personality_for panic_example::do_catch_panic+40
c
up
p context.ra
down
c
p *(struct _Unwind_Exception *)val
c
c
set $landingpad=val
up 2
finish
p (_Unwind_Reason_Code)$x0
b *$landingpad
c
disassemble
]]></content:encoded></item><item><title>Ask r/kubernetes: What are you working on this week?</title><link>https://www.reddit.com/r/kubernetes/comments/1lo3lj1/ask_rkubernetes_what_are_you_working_on_this_week/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 10:00:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell /r/kubernetes what you're up to this week!]]></content:encoded></item><item><title>I just want to express my appreciation for golang</title><link>https://www.reddit.com/r/golang/comments/1lo2b64/i_just_want_to_express_my_appreciation_for_golang/</link><author>/u/sebastianstehle</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 08:33:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, I am from the .NET world and I really hate that more and more features are added to the language. But I am working with it since a 15 years, so I know every single detail and the code is easy to understand for me.But at the moment I am also in a kotlin project. And I don't know if kotlin has more or less features but I have the impression that in every code review I see something new. A weird language construct or function from the runtime library that should improve something by getting rid of a few characters. If you are familiar with a programming language you do not see the problems so clearly, but know I am aware how much kotlin (and probably C#) can suck.When I work with go, I just understand it. There is only one way to do something and not 10. I struggle with generics a little bit, but overall it is a great experience.]]></content:encoded></item><item><title>My Linux journey so far</title><link>https://www.reddit.com/r/linux/comments/1lo27j5/my_linux_journey_so_far/</link><author>/u/onelostalien777</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 08:26:46 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I started with Manjaro like 10 years ago and used it for a few months and then switched back to windows, tried ubuntu and a few others and then forgot about linux for many years, at the start if the year i started checking distros out again, i started with mint for nearly a month and it was ok, then i went to arch and i liked it but i don't have that much time to configure a lot of things ( even tho its pretty fun and i do enjoy it but i don't have time to fix things ) so i went to manjaro and yeah i really liked it ( i am biased as it is the one i used many years ago ) it had customization and i didnt find many bugs and i really liked it but then got on reddit and saw everyone hates it and saying endeavour is better and the manjaro team is poopy so i will give it a try starting today ( my favorite one was arch but i found myself breaking it every other day and reinstalling it again and i don't have much time for it, i 100% prefer arch based distros and maybe one day i'll go full arch if i find the time ( or not, depends how endeavour goes )]]></content:encoded></item><item><title>Fedora: Proposal for the removal of i686 withdrawn</title><link>https://discussion.fedoraproject.org/t/f44-change-proposal-drop-i686-support-system-wide/156324/400</link><author>/u/FryBoyter</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 08:05:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Freelens v1.4.0 is just released</title><link>https://github.com/freelensapp/freelens/releases/tag/v1.4.0</link><author>/u/dex4er</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 07:29:24 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm happy to share with you the newest release of free UI for Kubernetes with a lot of minor improvements for UX and handling extensions. This version also brings full support for Jobs, CronJobs, and EndpointSlices. Extensions can now use the JSX runtime and many more React components. The new version is more developer-friendly, and I hope we'll see some exciting extensions soon.Finally Windows arm64 version is bug-free and can install extensions at all. Of course, all other versions are first citizens too: Windows x64 (exe, msi, and WinGet), MacOS arm64 and Intel (pkg, dmg, and brew), Linux for all variants (APT, deb, rpm, AppImage, Flatpak, Snap, and AUR).]]></content:encoded></item><item><title>Service Binding for K8s in Spring Boot cloud-native applications</title><link>https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5</link><author>/u/zarinfam</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 07:03:11 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Donate More by Donating Less</title><link>https://www.reddit.com/r/linux/comments/1lo0x5p/donate_more_by_donating_less/</link><author>/u/Fluid-Pirate646</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 06:59:14 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What is the Rusty Approach to Distributed Systems?</title><link>https://www.reddit.com/r/rust/comments/1lo0dr4/what_is_the_rusty_approach_to_distributed_systems/</link><author>/u/skwyckl</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 06:23:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I have thickened my skin in the Erlang / Elixir world when starting out, which kind of ruined concurrency for me in all other languages, but still, I am building an application in Rust and was thinking how to replicate the features that make Erlang-style concurrency so great. So, for starting out, the Actor Model can be implemented using e.g. Actix, so all good, but AFAIK I can't have two Actix actors communicate across difference instances of my application. What link is missing there Rust-wise? Thank you in advance.]]></content:encoded></item><item><title>[Kubernetes] Backend pod crashes with Completed / CrashLoopBackOff, frontend stabilizes — what’s going on?</title><link>https://www.reddit.com/r/kubernetes/comments/1lo0168/kubernetes_backend_pod_crashes_with_completed/</link><author>/u/erudes91</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 06:01:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[New to building K clusters, only been a user of them not admin.Running local K8s cluster with 2 nodes (node1: control plane, node2: worker).Built and deployed a full app manually (no Helm).Backend: Python Flask app (alternatively tested with Node.js).Frontend: static HTML + JS on Nginx.Services set up properly ( for backend,  for frontend).Backend pod status starts as , then goes to , and finally ends up in . for backend shows nothing.Flask version works  when run with Podman on node2: it starts, listens, and responds to POSTs.Frontend pod goes through multiple restarts, but after a few minutes finally stabilizes ().Frontend can't reach the backend () — because backend isn’t running.Verified backend image runs fine with podman run -p 5000:5000 backend:local.Described pods: backend shows , , no crash trace.Checked YAML: nothing fancy — single container, exposing correct ports, no health checks.Logs: totally empty (), no Python traceback or indication of forced exit.Frontend works but obviously can’t POST since backend is unavailable.The pod exits cleanly after handling the POST and terminates.Kubernetes thinks it crashed because it exits too early.node1@node1:/tmp$ kubectl get podsNAME READY STATUS RESTARTS AGEbackend-6cc887f6d-n426h 0/1 CrashLoopBackOff 4 (83s ago) 2m47sfrontend-584fff66db-rwgb7 1/1 Running 12 (2m10s ago) 62mWhy does this pod "exit cleanly" and not stay alive?Why does it behave correctly in Podman but fail in K8s?Any files you wanna take a look at?FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD ["node", "server.js"] FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD ["node", "server.js"] const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) => { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () => { console.log('Server is running on port 5000'); }); const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) => { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () => { console.log('Server is running on port 5000'); }); ]]></content:encoded></item><item><title>We are rewriting the message queue in Rust and would like to hear your suggestions.</title><link>https://www.reddit.com/r/rust/comments/1lnzkjq/we_are_rewriting_the_message_queue_in_rust_and/</link><author>/u/wenqiang_lobo</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 05:31:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We are a group of developers who love Rust, message queues, and distributed storage. We are trying to write a message queue using Rust. Its name is: RobustMQ. It follows the Apache-2.0 license. We hope that it can eventually be contributed to the Apache community and become a top project of the Apache community, contributing our own share of strength to the Apache and Rust communities. Some information about RobustMQ: Original intention: To explore the possibility of combining Rust with message queues, and solve the existing problems of the message queue components in the current community.Positioning: An All In One open-source message queue developed 100% based on the Rust language.Goal: To deliver a message queue that supports multiple mainstream messaging protocols, has a completely Serverless architecture, is low-cost, and elastic.100% Rust: A message queue engine implemented entirely based on the Rust language.Multiple protocols: Supports MQTT 3.1/3.1.1/5.0, AMQP, Kafka Protocol, RocketMQ Remoting/GRPC, OpenMessing, JNS, SQS, etc., the mainstream messaging protocols.Hierarchical architecture: A three-layer architecture with completely independent computing, storage, and scheduling, with clear responsibilities and independence.Serverless: All components have distributed cluster deployment capabilities and the ability to quickly scale up and down.Plugin-based storage: An independent plugin-based storage layer implementation, supporting both independent deployment and shared storage architectures.Comprehensive functions: Fully aligns with the functions and capabilities of the mainstream MQ products in the corresponding communities. For more detailed information, please visit our Github homepage and official website: We have currently completed the development of the first release version, including the overall architecture and the adaptation of the MQTT protocol. Next, we plan to further improve the MQTT, refine the stability, and then prepare for compatibility with the Kafka protocol. At this stage, we would like to hear your suggestions. We hope to know whether this action makes sense and what areas for improvement there are. So that we can stay on the right track and do this well. We know this is a difficult task, but we think it's a really cool thing and we want to give it a try. We are looking forward to the community's suggestions. Cool! Let's do something fun together~. At the same time, we also hope to find students who are interested in implementing infrastructure components, message queues, and distributed storage systems using Rust, and together explore the unlimited possibilities of Rust in the field of infrastructure. ]]></content:encoded></item><item><title>Zoi: A Universal Package Manager (Seeking Contributors!)</title><link>https://www.reddit.com/r/golang/comments/1lnzfp3/zoi_a_universal_package_manager_seeking/</link><author>/u/ZilloweZ</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 05:23:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Zoi is a project I recently started working on, its main goal is to provide a universal package manager for all operating systems and architectures. It's currently in beta, and it's has a lot of problems, please don't hesitate to report an issue.It fetches the packages from a git repo and sync it locally, the packages are in yaml format. The yaml file has options to download the file, either a binary, installer script or build from source. Also it has runtime dependencies and build dependencies.I'm currently looking for contributors, idk if this post is correct sharing it here, I hope so.]]></content:encoded></item><item><title>Feadback/Support: Inkube CLI app - Helps to Develop Inside Kubernetes Environment</title><link>https://www.reddit.com/r/kubernetes/comments/1lnzazc/feadbacksupport_inkube_cli_app_helps_to_develop/</link><author>/u/abdheshnayak</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 05:15:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I felt hectic to setup and manage local development with kubernetes cluster access. i was thinking solution for easy setup for each project with added env mirroring and packages locking. so built one tools for it inkube which helps to connect with cluster, mirror env and also provides package manager.please have a look and leave your thoughts and feed back on it.]]></content:encoded></item><item><title>Go makes sense in air-gapped ops environments</title><link>https://www.reddit.com/r/golang/comments/1lnz0w2/go_makes_sense_in_airgapped_ops_environments/</link><author>/u/Resource_account</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 04:58:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Been doing Linux ops in air-gapped environments for about a year. Mostly RHEL systems with lots of automation. My workflow is basically 75% bash and 25% Ansible.Bash has been solid for most of my scripting needs. My mentor believes Python scripts are more resilient than bash and I agree with him in theory but for most file operations the extra verbosity isn't worth it.So far I've only used Python in prod in like 2-3 situations. First I wrote an inventory script for Ansible right around the time I introduced the framework itself to our shop. Later I wrote a simple script that sends email reminders to replace certain keys we have. Last thing I built with it was a PyGObject GUI though funny story there. Took a week to build in Python then rewrote it in bash with YAD in an afternoon.Python's stdlib is honestly impressive and covers most of what I need without external dependencies. But we've got version management headaches. Desktops run 3.12 for Ansible but servers are locked to 3.8 due to factory requirements. System still depends on 3.6 and most of the RPM's are built against 3.6 (RHEL 8).Started exploring Go recently for a specific use case. Performance-critical stuff with our StorNext CVFS. In my case with venv and dependencies on CVFS performance has been a little rough. The compiled binary approach seems ideal for this. Just rsync the binary to the server and it runs. Done.The other benefit I've noticed is the compiler feedback. Getting LSPs and linters through security approval is a long exhausting process so having the compiler catch issues upfront, and so quickly, helps a lot. Especially when dealing with the constant firefighting.Not saying Python is bad or Go is better. Just finding Go fits this particular niche really well.Wondering if other devops or linux sysadmins have found themselves in a similar spot.]]></content:encoded></item><item><title>Procedural city generation in go with ebitengine</title><link>https://hopfenherrscher.itch.io/union-station</link><author>/u/oliver-bestmann</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 04:40:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Would love some feedback on a library I’m writing!</title><link>https://www.reddit.com/r/golang/comments/1lnyfjq/would_love_some_feedback_on_a_library_im_writing/</link><author>/u/Far_Solution_1784</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 04:23:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, I am working on a library that wraps go docker sdk and also wraps over the docker compose cli to allow you to programmatically create docker compose files and or run directly from go code. I’m aiming to solve a solution for go devs that want programmatic control over docker in a declarative fashion. Would love some feedback, or if you’re willing to contribute that would be sick! ]]></content:encoded></item><item><title>Should I be Looking into Custom Metrics or External Metrics?</title><link>https://www.reddit.com/r/kubernetes/comments/1lnydqd/should_i_be_looking_into_custom_metrics_or/</link><author>/u/zangetsuMG</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 04:20:42 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am not completely sure if I am even asking the right kind of questions, so please feel free to offer guidance. I am hoping to learn how I can use either Custom Metrics or External Metrics to solve some problems. I'll put the questions up front, but also provide some background that might help people understand what I am thinking and trying to do.Thank you and all advice is welcome.Is there some off the shelf solution that can run an SQL Query, and provide the result as a metric?This feels like it is a problem others have had and is probably already solved. I feel like there should be some kind of existing service I can run, and with appropriate configuration it should be able to connect to my database, run a query and return that value as a metric in a form that K8s can use. Is there something like that?If I have to implement my own, Should I be looking at Custom Metrics or External Metrics?I can go down the path of building my own metrics service, but if I do, should I be doing Custom Metrics, or External Metrics? Is there some documentation about Custom Metrics or External Metrics that is more than just a generated description of the data types? I would love to find something that explains things like what the different parts of the URI path mean, and all the little pieces of the data types so that if I do implement something, I can do it right.Is it really still a beta API after at least 4 years?I'm kind of surprised by the v1beta1 and v1beta2 in the names after all this time. (feel free to stop reading here)I am working with a system that is composed of various containers. Some containers have a web service inside of them, while others have a non-interactive processing service inside them, and both types communicate with a database (Microsoft SQL Server). The web servers are actually Asp.Net Core web servers and we have been able to implement a basic web API that returns an HTTP 200 OK if the web server thinks it is running correctly, or an HTTP error code if it is not. We've been able to configure K8s to probe this API and do things like terminate and restart the container. For the web servers we've been able to setup some basic horizontal auto-scaling based on CPU usage. (If they have high sustained CPU usage, scale up).For our non-interactive services (Also .Net code), they mostly connect to the database periodically and do some work (this is way over-simplified, but I suspect the details aren't important.)In the past we have had some cases where these processes may get into a broken state, but from the container management tools they look like they are running just fine. This is one problem I would like to be able to detect and have k8's report and maybe fix. Another issue is that I would like for these non-interactive services to be able to auto-scale, but the catch here is that the out of the box metrics like CPU and Memory aren't actually a good indicator if the container should be scaled.I'm not too worried about the web servers, but I am worried about the non-interactive services. I am reasonably sure I could add a very small web API that could be probed, and that we could configure K8s to check the container and terminate and restart. In fact I am almost sure that we'll be adding that functionality in the near future.I think for our non-interactive services in order to get a smart horizontal auto-scaling, we need some kind of metrics server, but I am having trouble determining what that metrics service should look like. I have found the external metrics documentation at https://kubernetes.io/docs/reference/external-api/ but I find it a bit hard to follow.Because of the way my non-interactive services work, I am thinking that there is some amount of available work in our database. The unit-of-work has a time value for when the unit of work was added, so I should be able to look at the work, and calculate how long the work has been waiting before being processed, and if that time span is too long, that would be the signal to scale up. I am reasonably sure I could distill that question down to an SQL query that returns a single number, that could be returned as a metric.]]></content:encoded></item><item><title>Building a PC for AI Workloads + Kubernetes, Need Advice on CPU, GPU, RAM &amp; Upgradability</title><link>https://www.reddit.com/r/kubernetes/comments/1lny37h/building_a_pc_for_ai_workloads_kubernetes_need/</link><author>/u/root0ps</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 04:03:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I’m planning to build a PC mainly to learn and run AI workloads and also set up Kubernetes clusters locally. I already have some experience with Kubernetes and now want to get into training and running AI models on it.I’m based in India, so availability and pricing of parts here is also something I’ll need to consider.I need help with a few things:CPU – AMD or Intel? I want something powerful but also future-proof. I’d like to upgrade the CPU in the future, so I’m looking for a motherboard that will support newer processors.GPU – NVIDIA or AMD? My main goal is running AI workloads. Gaming is a secondary need. I’ve heard NVIDIA is better for AI (CUDA, etc.), but is AMD also good enough? Also, is it okay to start with integrated graphics for now and add a good GPU 6–8 months later? Has anyone tried this?RAM – 32 GB or 64 GB? Is 32 GB enough for running AI stuff and Kubernetes? Or should I go for 64 GB from the start?Budget: I don’t have a strict budget, but I’m thinking around $2000. I’m okay with spending a bit more if it means better long-term use.I want to build something I can upgrade later instead of replacing everything. If anyone has built a PC for similar use cases or has suggestions, I’d really appreciate your input!]]></content:encoded></item><item><title>Donate More by Donating Less (further explanation from Steve Deobald)</title><link>https://www.reddit.com/r/linux/comments/1lnxslo/donate_more_by_donating_less_further_explanation/</link><author>/u/pr0fic1ency</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 03:47:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Free access to an H100. What can I build?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lnvjin/r_free_access_to_an_h100_what_can_i_build/</link><author>/u/cringevampire</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 01:46:27 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[My company is experimenting with new hardware and long story short, there's an idling H100 with a 2TB RAM and 27TB of storage and I'm allowed to play with it!I really want to do some cool AI research to publish at a decent conference but I'm not well caught up with the research frontier and I could really use some help (and collaborators?).I understand neural networks, CNNs, transformer models etc. to a reasonable depth but understanding what SOTA is will probably take more time than how long I have access to the GPU]]></content:encoded></item><item><title>Duke Nukem 3D code review by Tariq10x</title><link>https://m.youtube.com/watch?v=F9lOJlC_kQs</link><author>/u/r_retrohacking_mod2</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 00:07:02 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!</title><link>https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6</link><author>/u/ajmmertens</author><category>reddit</category><pubDate>Mon, 30 Jun 2025 00:00:21 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Flecs is an Entity Component System (FAQ) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:Store data for  in data structures optimized for CPU cache efficiency and composition-first designBuiltin support for hierarchies, prefabs and more with which speed up game code and reduce boiler plateAn to profile, visualize, document and debug projectsFlecs is fully open source and licensed under the MIT license. If you’d like to support the project, consider giving it a ️️⭐️ on the Github page!Since v4.0 there have been exciting updates from games that use Flecs!Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising — a classic RTS set on Earth after a nuclear war (Unreal Engine 5).Announced: Resistance is BrutalResistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).Announced: Age of RespairAge of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).You’ve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.The developer behind this project is also the author of vkguide, which is one of the best resources for learning Vulkan. There is a chapter dedicated specifically to Ascendant, make sure to check it out!A vampire survivor like game built in Flecs. What’s awesome is that the developer of the game has made its source code available! Check it out here: https://github.com/ptidejteam/ecs-survivorsThere are still more projects cooking that I haven’t listed here. If you want to stay up to date on the progress of Flecs projects, check out the showcase channel of the Flecs Discord!The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.Here are some of the most notable performance improvements (as measured by the Flecs benchmarking suite):: 5x faster than v4.0: 5x faster than v4.0: 5–10x faster than v4.0: 1.5–2x faster than v4.0: 2–4x faster than v4.0: 5–10x faster than v4.0: 40x faster than v4.0, 6x faster than v3.2.12Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the “Performance” section to see how these improvements were achieved.As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!Here’s an overview of the highlights since v4.0:Non-fragmenting componentsFlecs is an archetype-style ECS, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.Flecs v4.1 is the first* open source ECS that supports both! Switching between storages is easy, just add the  trait to a component during registration:* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That’s correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!Flecs Script improvementsMany new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here’s an overview of the most notable changes:A new (much faster) expression parserA new  addonEntity name expressions ()An example script that uses some of the new features:The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:An additional operation has been introduced that makes it possible to change the order of children after they have been created:World local component idsA common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.get()/try_get() API redesignThe C++  APIs have been redesigned to use the following pattern:This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn’t have it yet, whereas  only assigns existing components and will panic if the entity didn’t have the component yet.In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it’s been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.Usually this is not a problem. When someone is playing a game it is typically the only thing that’s happening on a machine, so as long we don’t exceed the maximum amount of RAM the ECS is allowed to use we’re good.But what if the game is  the only thing that’s running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:When a thread is done it can release the world so others can use it again:When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:Performance tracing hooksFlecs now provides hooks for better integration with profiling tools such as Tracy. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.The new hooks can be set as callbacks on the OS API:There is now a new Flecs demo that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‘s already pretty satisfying to look at:This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there’s fun takeaways from both.Because such a large part of this release was dedicated to performance improvements, I thought it’d be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you’re not interested in nitty-gritty details you can safely skip this.In “where are my entities and components” I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.This simple change sped up  and related operations by 3x!Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.The existing approach uses the component index to match tables. For a  query, the query engine does the following:Map lookups are fast enough for many use cases, but they start adding up when they’re done millions of times per second.The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:this table  matches the querythis table  match the queryEvaluating the bloom filter looks like this and is super fast:We can’t use the bloom filter for all queries, as we can’t express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!Faster flecs::ref validationThe  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:Previously this worked by caching the table record (see “faster component lookups”) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:A ref now directly stores a pointer to the componentIt also stores a “table version”Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:When fetching the component, we check the table version in the ref with the table version in the array:Note how we’re only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!Fun fact:  is now as fast as  was before this optimization!Faster component fetching when iterating cached queriesA major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.This is super efficient because:Table columns rarely grow, so we almost never need to revalidate the cached column pointers.Table columns always grow together, which means we can use a single version number to check all columns.This change by itself can speed up cached query iteration by 2x, but it’s not the only thing that changed for cached queries:Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don’t use these features however, and so these fields just add dead weight to cache elements.Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.Flecs v4.1 completely refactored the query cache to address these issues:Queries now use much (3x) smaller cache elements for simple queriesThe linked list has been replaced with an arrayCombined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with __attribute__((always_inline)) on clang and gcc.This improved performance of operations like  by another 2x.I can’t go over every single improvement in detail or this blog post would become way too long, so here’s a short callout to some of the other notable performance improvements:Uncached queries are 2–4x faster to create.Pipelines run up to 2x faster.World creation is 1.4–2.5x fasterThe performance of empty table cleanup has improved up to 5x.Change detection overhead has been reduced by 2x for trivial queriesC++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don’t have to emit empty table events anymore) and greatly simplifies some of the internals.Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.The ink on the v4.1 release isn’t dry yet but work on the next batch of improvements is already in full swing! Here’s a few things to look forward to in upcoming releases:A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.Performance improvements to the core data structures used in the component index to speed up uncached query iteration.New features and performance improvements for Flecs Script and its reactivity implementation.The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!More robust reflection/explorer support for complex component types (think reflection for map types & inspector support for collections).]]></content:encoded></item><item><title>Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!</title><link>https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6</link><author>/u/ajmmertens</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 23:54:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Flecs is an Entity Component System (FAQ) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:Store data for  in data structures optimized for CPU cache efficiency and composition-first designBuiltin support for hierarchies, prefabs and more with which speed up game code and reduce boiler plateAn to profile, visualize, document and debug projectsFlecs is fully open source and licensed under the MIT license. If you’d like to support the project, consider giving it a ️️⭐️ on the Github page!Since v4.0 there have been exciting updates from games that use Flecs!Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising — a classic RTS set on Earth after a nuclear war (Unreal Engine 5).Announced: Resistance is BrutalResistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).Announced: Age of RespairAge of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).You’ve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.The developer behind this project is also the author of vkguide, which is one of the best resources for learning Vulkan. There is a chapter dedicated specifically to Ascendant, make sure to check it out!A vampire survivor like game built in Flecs. What’s awesome is that the developer of the game has made its source code available! Check it out here: https://github.com/ptidejteam/ecs-survivorsThere are still more projects cooking that I haven’t listed here. If you want to stay up to date on the progress of Flecs projects, check out the showcase channel of the Flecs Discord!The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.Here are some of the most notable performance improvements (as measured by the Flecs benchmarking suite):: 5x faster than v4.0: 5x faster than v4.0: 5–10x faster than v4.0: 1.5–2x faster than v4.0: 2–4x faster than v4.0: 5–10x faster than v4.0: 40x faster than v4.0, 6x faster than v3.2.12Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the “Performance” section to see how these improvements were achieved.As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!Here’s an overview of the highlights since v4.0:Non-fragmenting componentsFlecs is an archetype-style ECS, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.Flecs v4.1 is the first* open source ECS that supports both! Switching between storages is easy, just add the  trait to a component during registration:* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That’s correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!Flecs Script improvementsMany new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here’s an overview of the most notable changes:A new (much faster) expression parserA new  addonEntity name expressions ()An example script that uses some of the new features:The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:An additional operation has been introduced that makes it possible to change the order of children after they have been created:World local component idsA common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.get()/try_get() API redesignThe C++  APIs have been redesigned to use the following pattern:This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn’t have it yet, whereas  only assigns existing components and will panic if the entity didn’t have the component yet.In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it’s been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.Usually this is not a problem. When someone is playing a game it is typically the only thing that’s happening on a machine, so as long we don’t exceed the maximum amount of RAM the ECS is allowed to use we’re good.But what if the game is  the only thing that’s running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:When a thread is done it can release the world so others can use it again:When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:Performance tracing hooksFlecs now provides hooks for better integration with profiling tools such as Tracy. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.The new hooks can be set as callbacks on the OS API:There is now a new Flecs demo that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‘s already pretty satisfying to look at:This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there’s fun takeaways from both.Because such a large part of this release was dedicated to performance improvements, I thought it’d be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you’re not interested in nitty-gritty details you can safely skip this.In “where are my entities and components” I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.This simple change sped up  and related operations by 3x!Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.The existing approach uses the component index to match tables. For a  query, the query engine does the following:Map lookups are fast enough for many use cases, but they start adding up when they’re done millions of times per second.The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:this table  matches the querythis table  match the queryEvaluating the bloom filter looks like this and is super fast:We can’t use the bloom filter for all queries, as we can’t express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!Faster flecs::ref validationThe  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:Previously this worked by caching the table record (see “faster component lookups”) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:A ref now directly stores a pointer to the componentIt also stores a “table version”Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:When fetching the component, we check the table version in the ref with the table version in the array:Note how we’re only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!Fun fact:  is now as fast as  was before this optimization!Faster component fetching when iterating cached queriesA major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.This is super efficient because:Table columns rarely grow, so we almost never need to revalidate the cached column pointers.Table columns always grow together, which means we can use a single version number to check all columns.This change by itself can speed up cached query iteration by 2x, but it’s not the only thing that changed for cached queries:Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don’t use these features however, and so these fields just add dead weight to cache elements.Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.Flecs v4.1 completely refactored the query cache to address these issues:Queries now use much (3x) smaller cache elements for simple queriesThe linked list has been replaced with an arrayCombined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with __attribute__((always_inline)) on clang and gcc.This improved performance of operations like  by another 2x.I can’t go over every single improvement in detail or this blog post would become way too long, so here’s a short callout to some of the other notable performance improvements:Uncached queries are 2–4x faster to create.Pipelines run up to 2x faster.World creation is 1.4–2.5x fasterThe performance of empty table cleanup has improved up to 5x.Change detection overhead has been reduced by 2x for trivial queriesC++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don’t have to emit empty table events anymore) and greatly simplifies some of the internals.Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.The ink on the v4.1 release isn’t dry yet but work on the next batch of improvements is already in full swing! Here’s a few things to look forward to in upcoming releases:A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.Performance improvements to the core data structures used in the component index to speed up uncached query iteration.New features and performance improvements for Flecs Script and its reactivity implementation.The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!More robust reflection/explorer support for complex component types (think reflection for map types & inspector support for collections).]]></content:encoded></item><item><title>[P] Code for Fine-Tuning FLUX.1-dev Explained Step by Step With Comments</title><link>https://www.reddit.com/r/MachineLearning/comments/1lnt9za/p_code_for_finetuning_flux1dev_explained_step_by/</link><author>/u/FallMindless3563</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 23:51:36 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. There were examples in HuggingFace diffusers examples/dreambooth/train_dreambooth_lora_flux.py (which didn't work out of the gate for me) and AI-Toolkit which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.The code was written in a Marimo Notebook which I'm enjoying lately for developing simple training scripts. ]]></content:encoded></item><item><title>[D] How should I respond to reviewers when my model is worse than much larger models?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lnsph5/d_how_should_i_respond_to_reviewers_when_my_model/</link><author>/u/AdministrativeRub484</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 23:24:15 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...]]></content:encoded></item><item><title>Karpenter NodePool Strategies: Balancing Cost, Reliability &amp; Tradeoffs</title><link>https://www.reddit.com/r/kubernetes/comments/1lnsns8/karpenter_nodepool_strategies_balancing_cost/</link><author>/u/Separate-Welcome7816</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 23:22:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[ Best for stability and predictability, but comes with higher costs. Ideal for critical workloads that cannot afford interruptions or require guaranteed compute availability. Great for cost savings — often 70-90% cheaper than On-Demand. However, the tradeoff is reliability. Spot capacity can be reclaimed by AWS with little warning, which means workloads must be resilient to node terminations.Mixed Strategy (80% Spot / 20% On-Demand) The sweet spot for many production environments. This setup blends the cost savings of Spot with the fallback reliability of On-Demand. Karpenter can intelligently schedule critical pods on On-Demand nodes and opportunistic workloads on Spot instances, minimizing risk while maximizing savings.]]></content:encoded></item><item><title>&quot;colormatrix&quot;. A very colored cmatrix close that uses a random array of colors.</title><link>https://www.reddit.com/r/linux/comments/1lns8om/colormatrix_a_very_colored_cmatrix_close_that/</link><author>/u/Beautiful_Crab6670</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 23:02:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Eh...just a little "something" I came up with in my free time. It picks a "true" random color and use it to draw a drop. Should be compatible with terminals that use true color. So expect a "puke" of colors if you use it on such terminal.Click here to grab the code. Then compile it with "gcc colormatrix.c -o colormatrix -static -O3 -Wall".]]></content:encoded></item><item><title>godump - v1.3.0 - New Release</title><link>https://i.postimg.cc/FF0k8Fyk/godump.png</link><author>/u/cmiles777</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 22:51:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Create Jobs and CronJobs via Ui using kube composer</title><link>https://www.reddit.com/r/kubernetes/comments/1lnrofd/create_jobs_and_cronjobs_via_ui_using_kube/</link><author>/u/same7ammar</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 22:37:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/same7ammar ]]></content:encoded></item><item><title>my first open-source project</title><link>https://www.reddit.com/r/golang/comments/1lnqgqj/my_first_opensource_project/</link><author>/u/MoonOwlMage</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 21:43:04 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey all, I've been working on a service monitoring tool called Heimdall and wanted to share it with the community. It's a lightweight service health checker written in pure Go with zero external dependencies. More information you can find in README.It is my first project, that I want to be an open-source, so I'm looking forward for your feedback, feature offers and pull requests. It was started as personal project for my job, but I thought, that it can be useful for others.p.s project in dev mode, so I'll add more features in future]]></content:encoded></item><item><title>One-Minute Daily AI News 6/29/2025</title><link>https://www.reddit.com/r/artificial/comments/1lnq3vc/oneminute_daily_ai_news_6292025/</link><author>/u/Excellent-Target-847</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 21:27:20 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>My Adventures with Kubuntu, KVM, Windows 11 Pro and My 2011 Macbook Air</title><link>https://www.reddit.com/r/linux/comments/1lnpyo7/my_adventures_with_kubuntu_kvm_windows_11_pro_and/</link><author>/u/ScubadooX</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 21:21:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>First time token access to gmail is not working</title><link>https://www.reddit.com/r/golang/comments/1lnpxqm/first_time_token_access_to_gmail_is_not_working/</link><author>/u/pepiks</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 21:20:00 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I tried play with Gmail API using Go. So I follow tutorial:I generate JSON file with credits, setup app as suggested, even add scope manually on Google, creds file saved in workdir. When I run app it open URL:So then it stuck on code:func getTokenFromWeb(config *oauth2.Config) *oauth2.Token {authURL := config.AuthCodeURL("state-token", oauth2.AccessTypeOffline)fmt.Printf("Go to the following link in your browser then type the "+"authorization code: \n%v\n", authURL)if _, err := fmt.Scan(&authCode); err != nil {log.Fatalf("Unable to read authorization code: %v", err)tok, err := config.Exchange(context.TODO(), authCode)log.Fatalf("Unable to retrieve token from web: %v", err)Logic seems fine, but it looks like wrong setup URI code to follow. On browser I have buttons with access to scope, but when I agree I got side is unreachable. I use code provided byt Google and I don't know idea how move forward from this point. My access is configuret as Desktop App (the same as in tutorial).At the end I want add permision to my app as I do for email client once and after that run app and do stuff like saving attachments, add labels etc.]]></content:encoded></item><item><title>Recommended DEs that aren&apos;t as common</title><link>https://www.reddit.com/r/linux/comments/1lnp3u6/recommended_des_that_arent_as_common/</link><author>/u/emrldgh</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 20:45:24 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I'd like to know what everyone's recommendation is for a DE/WM that not everyone may know about or often consider. Anything that isn't KDE, GNOME, or any super common WMs like Hyprland or Sway. These may not be considered very common, but I'd like to hear thoughts on Budgie and Cutefish, I was looking at them and they look neat but what do you guys think? What do you use?]]></content:encoded></item><item><title>[D] Review clearly used an LLM, should I report it to AC?</title><link>https://www.reddit.com/r/MachineLearning/comments/1lnoqmm/d_review_clearly_used_an_llm_should_i_report_it/</link><author>/u/AdministrativeRub484</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 20:30:15 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...]]></content:encoded></item><item><title>I built a CPU emulator with its own assembler in java</title><link>https://github.com/LPC4/Neptune-32</link><author>/u/ColdRepresentative91</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 20:19:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Over the past few days I’ve been building a custom 32-bit CPU emulator in java that comes with its own assembler and instruction set. I started on the project for fun, and because I wanted to learn more about CPU architecture and compilers.32-bit little-endian architecture with 32 general-purpose registersMemory-mapped IO, stack and heap, ROM for syscalls, and RAM/VRAM simulationMalloc and Free implemented syscalls (not tested properly)128×128 RGBA framebuffer + keyboard and console IO devicesInstruction set includes arithmetic, logic, branches, system calls, and shiftsAssembler supports labels, immediate values, register addressing, macros, but still expandingI’d love to hear what you think about this project: ideas, critiques, or even some features you’d like to see added. Would really appreciate any tips, feedback, or things I could do better.]]></content:encoded></item><item><title>Free App Hidden Gem: Libreoffice - Full Featured Microsoft Office Alternative</title><link>https://youtu.be/bhid0z2JUec?feature=shared</link><author>/u/Putrid_Draft378</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 19:55:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Multus on Multiple Nodes with UDP broadcast</title><link>https://www.reddit.com/r/kubernetes/comments/1lnnt1y/multus_on_multiple_nodes_with_udp_broadcast/</link><author>/u/rickreynoldssf</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 19:50:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello. I've been banging my head against my desk trying to setup multus with ipvlan on AKS. I run a multi node cluster. I need to create multiple pods that create a private network with all pods on the same subnet and likely , where they will send UDP broadcasts to each other. I need to replicate that many times so there's 1-n groups of pods with their private networks. I also need the pods to have the default host network, hence Multus. With a single node and macvlan this all works great but with ipvlan and multiple nodes I cannot communicate across the nodes on the private network. Are there any examples / tutorials / docs on doing this?]]></content:encoded></item><item><title>Code is skimmed more often than it is written, so it should be clear at a glance</title><link>https://jelv.is/blog/Writing-Code-To-Be-Read-at-a-Glance/</link><author>/u/tikhonjelvis</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 19:31:11 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[In software engineering circles, there is a common adage: “code is read more than it is written”. But this is not the whole picture! Code is skimmed more than it is read.We read code more than we write it because we spend more time maintaining than we do writing from scratch. A line of code, once written, still has a long and storied life ahead of it. You write code once and then return to it multiple times, fixing bugs, adding features, refactoring. To do this you have to change the existing code—and to change it you have to read and understand it. Not just you; future programmers will also work on the code, pursuing their own goals, operating under different constraints.For every part of the code you need to revisit to in depth, there will be dozens of related parts you’re not touching directly. You need to navigate through the codebase to find relevant code and you need to track surrounding code for context. You don’t have to understand the related parts of code exactly; just what they are supposed to do. You do this not by reading the code in detail—nobody has the time or working memory to keep a whole codebase in their head!—but by scanning through code quickly and getting just the gist.This is a multiplicative relationship. Just as you end up reading code multiple times for each time you write it, you end up skimming multiple times for each piece you read.Writing code you can understand  is at least as important as writing code that you can read at all.What does writing code that’s easy to read at a glance actually involve? To me, it comes down to thinking about the  of my code. The structure of the code should give you a quick idea of what it’s supposed to do.An immediate consequence of caring out the shape of your code is that related code  and unrelated code —regardless of implementation. How your code looks is a key affordance for guiding the reader’s attention without forcing them to read the code in detail.Verbose identifiers obscure the shape of your code. This doesn’t mean you should  use verbose names in your code, just that you should be restrained and tasteful. If an identifier comes from far away—a function from a logically distant module, say—giving it a descriptive name can outweigh the downsides.As an illustrative—if extreme—example, let’s compare three versions of the same logic. Here’s some Java code using BigDecimal:Java does not have operator overloading. If it did, the code might look like this instead:Both of these implement the same mathematical formula:This is about as simple as a polynomial gets, but even that wasn’t immediately clear from the first version of the code: you’d have to pay attention to read the sequence of named method calls to understand what was going on. The version with overloaded operators is a real contrast, although it still requires more attention than the math notation. If the first snippet is like reading a paragraph, the second snippet is like reading a short sentence and the math notation is like reading a single word.With the math notation, we can  tell we’re looking at a polynomial just from the shape. The information we care about for polynomials are the terms; the addition and multiplication holding everything together is more like an implementation detail. The math notation reflects this by visually grouping together the information for each term and using an operator (+) as “punctuation” to pull it together. Addition is part of what makes the polynomial  but, once we know what we’re looking at, it can fade into the background.People are naturally good at tracking context. We can see this with natural language all the time: the same word or phrase can have somewhat different—or sometimes radically different—meanings in different contexts, but this is so natural that, once people are used to it, they barely notice. What was the last time you thought about how red wine is actually purple, or that, for programmers, “strings” and “threads” have absolutely nothing to do with each other?Context sensitive meanings aren’t free. The way words in natural languages have different meanings (polysemy) is a real obstacle for early language acquisition. But part of the reason that polysemy is difficult is that native speakers do not even notice they are relying on it! Once you get comfortable in a language—whether a totally different language, or just the jargon and conventions of a new social group—tracking the meaning of words based on context becomes . The reason natural languages have polysemy is that relying on context takes  mental energy than communicating in verbose, fully-explicit phrases.We can take advantage of this natural tendency in programming. If I’m working on a module that is implementing an HTTP client for the Stripe, I’m going to be fine using  to mean “send an authenticated HTTP GET request to Stripe”. But in a broader, less Stripe-specific context, I would want to write something like  instead.Some of your code is the “meat” of your expression, the logic that matters for whatever you are doing. The rest is more like plumbing—code that we need to keep everything working but less significant in any  instance. Think of type conversions, control flow, error propagation, configuration management… Sometimes fixing a bug will hinge on how a specific config value flows into your function but, most of the time, you care far more about .Quickly distinguishing plumbing code from logic is key for understanding and navigating code quickly. When you’re scanning through a codebase, you can ignore plumbing code altogether. I’ve found this is where certain “controversial” language features like macros, overloaded operators and control-flow abstractions.As an example, using infix operators for plumbing constructs makes the plumbing visually distinct from “normal” identifiers while also giving your code some additional visual structure by naturally organizing the expression up into groups.After a while, plumbing operators start to fade into the background when you’re scanning through code. Squint a bit, and you start seeing similarities between code that might be doing the same thing in different contexts. Consider the  operators in Haskell: they let us apply functions over values in some functor (like  and ) in a way that’s immediately reminiscent of normal function application:This style also lets us see how operators naturally group code together:Applicative notation might read like line noise to the uninitiated—and, honestly, it’s not exactly the best example of clean plumbing code—but, once you’re comfortable in Haskell, it just melts away. (Which is also 100% true of Lisp’s parentheses! Write enough Lisp and you stop seeing them. It is uncannily like the “I don’t see code” scene from .)One perspective I’ve found useful is to think about the minimum amount of information an expression  contain. A polynomial in a single variable, for example, only really needs its coefficients. A web route needs the route, the methods it supports, any variables it takes and the variables’ types.Anything else is unnecessary from a raw information point of view. We might still  additional code—for plumbing, for structure or just as an implementation detail—but, for thinking about API design, we want to be able to distinguish the core information the code is conveying from everything else. “Everything else” may be useful for organizing our code, but it might also be nothing more than unavoidable—or, depressingly often, completely unforced—boilerplate.Boilerplate makes code harder to read at a glance.As a general guide, I try to eliminate more and more of the inessential code as I repeat a particular kind of expression more and more. In extreme cases, a table layout might be the most readable option if you have a whole bunch of structured rows of code repeating.Math notation—with its longer history of evolution and development than programming languages—is a great example of notation that can be read at a glance.Compare the following two ways of writing the same expression:This is an illustrative, if exaggerated, example.Integral notation might be unfamiliar to a beginner, but it is wonderfully efficient. You can tell what the expression represents  thanks to the integral sign and the equation layout. Imagine squinting until you see the general outline of  integral expression—that’s the same principle I talk about as the “shape of the code”.It’s easy to quickly identify parts of the equation to figure out what’s going on: the limits are distinct from the equation itself and distinct from the variable of integration (ie ). The polynomial itself continues the same theme:  as an operator gives structure to the polynomial, emphasizing its nature as a . In a sense, the  is just plumbing that gets out of our way so that we can identify the content  to this polynomial (namely the coefficients and degree).The paragraph, on the other hand, has the advantage of being readable by anyone, even if they aren’t familiar with notation for integrals. But it has a fatal flaw: you  to read it. Every time. We have to read the text word-by-word to understand that it describes an integral and to see what the limits and function being integrated are. We can’t even tell that this is an integral of a polynomial without close reading!If this paragraph were surrounded by other paragraphs, we wouldn’t be able to tell it apart from any other prose. I often start reading papers by skipping explanations and looking at figures and equations until I find what I need—I wouldn’t be able to do that without special notation and visual structure.Good code is, ultimately, a human factors problem. We want to understand how people interact with code—how they read, write, skim, navigate, modify, reuse and repurpose code—and then write and organize our code in a style that makes these interactions as easy and natural as possible.Writing code to be read at a glance is just part of the story, but it’s something I’ve found useful and important. Code I’ve written keeping this principle in mind just feels . And that, fuzzy as it sounds, makes a real, practical difference to how much time, energy and focus it takes to work on a codebase. Making code skimmable is nowhere near the most important aspect of making code pleasant and effective, but it does matter, and I have not seen people explicitly talking about it.I don’t have much advice for putting these ideas into action. For me, just realizing that I valued code I could read at a glance and keeping that in mind when I wrote future code was all it took. I quickly developed the right habits and now I don’t have to think about it.]]></content:encoded></item><item><title>[Media] I built “Decide” – a role and condition-based permission engine for Rust (and also JS/TS)</title><link>https://www.reddit.com/r/rust/comments/1lnn6jt/media_i_built_decide_a_role_and_conditionbased/</link><author>/u/aetheros_</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 19:24:49 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently released Decide, a fast and lightweight permission engine written in Rust, with built-in support for both Rust and JavaScript/TypeScript.It started as a small idea, but turned into something I genuinely found useful, especially because there weren’t many simple permission engines for Rust.Role + condition based permission engineSupports conditions like: user_id === resource_ownerBuilt in Rust (uses Rhai for condition evaluation)Comes with a JS/TS wrapper (using napi-rs)The code is completely open to view. Visit the repository here.An example usage is given in the code snippet. The part  gets the role definitions from a  file.There are a bunch of libraries for auth or RBAC in JS, but almost none in Rust. I thought, why not build a clean one that works for both?It’s fully open-source and MIT licensed.Would love to hear your thoughtsIt's my first time posting here, and I'd love feedback. Especially around: - Rust conventions or improvements - Performance ideasThanks for reading, I hope this can help someone actually :)]]></content:encoded></item><item><title>One click k8s deploy!</title><link>https://www.reddit.com/r/kubernetes/comments/1lnn6i2/one_click_k8s_deploy/</link><author>/u/wideboi_420</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 19:24:46 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello guys! I have been lurking around for a while, and I wanted to share my little automation project. I was a little bit inspired by Jim's Garage one click deploy script for k3s, but since I am studying k8s here is mine:Please feel free to criticize and to give out any advice, this is just for fun, even tho someone might find this useful in the future =)]]></content:encoded></item><item><title>Is void linux in active development, and if so where on that scale?</title><link>https://www.reddit.com/r/linux/comments/1lnme1y/is_void_linux_in_active_development_and_if_so/</link><author>/u/doc1623</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 18:52:30 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I used it in the past, and loved it, but I remember reading that the lead or main developer left, I think. I see that it's still technically in active development but does that mean they are just barely keeping up or enough resources to make big advances, or somewhere in between. An example to make my point is Redox OS. It's initial release was 10 years ago. It still seems to be in "active" development, but it has yet to reach an official 1.0 release. Side note, I hope it does before it is surpassed by other projects with more developmental for me and I assume most at this point.I guess, it's a multipart question or just lots of related questions.Why is it so far down on distrowatch list now?Does it have enough active development resources to not only to keep pace with advancements, but even continue to make some or is too resource to be all but a fringe distro rather than a daily driver without allot of issues popping up that are more natural to developmental, pre-release version?Please, these are honest questions, that I don't feel I have the answer to. Please keep answers civil, non-defensive/combative. Hoping that people more "in the know" and/or have kept up better, might have a better understanding. ]]></content:encoded></item><item><title>Service Binding for K8s in Spring Boot cloud-native applications</title><link>https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5</link><author>/u/zarinfam</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 18:44:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tools I love: mise(-en-place)</title><link>https://blog.vbang.dk/2025/06/29/tools-i-love-mise/</link><author>/u/micvbang</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 18:33:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Once in a while you get introduced to a tool that instantly changes the way you work. For me, mise is one of those tools.mise is the logical conclusion to a lot of the meta-tooling that exists around language-specific version and package managers like asdf, nvm, uv, pyenv etc. It makes it exceptionally easy to install, use, and manage software. It also allows you to manage environment variables and declare tasks (run commands).The first step in getting an intuitive understanding of what mise can help you with is to use it to install a tool. Pick your favorite and try it out; it supports !jj
command_not_found_handler:5: not found: jj

mise use jj
mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0

jj version
jj 0.30.0

 ..

jj version
command_not_found_handler:5: not found: jj

eaxmples_mise

jj version
jj 0.30.0
As the above shows, with mise we’re just one command away from installing and trying out a new tool, e.g. .In the above we that mise printed mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0. This tells us that mise has created (or updated) the mise configuration . 
We also see that if we cd out of , the  command is no longer available. If we cd back into , it becomes available again; unless you explicitly install tools globally, mise will only make the tools available which are mentioned in a  file on the path from your current directory to the root of your file system. That of course means that we could potentially meet multiple  files when going back up to the root of the file system. Mise handles this by concatting the configurations and overwriting conflicting configurations, letting the file furthest down the tree win.This is a clever design as it allows us to configure different versions of the same tool to be available in different directories. Let’s have a look at what the  file looks like:If we want a specific version of  to be installed in a specific directory, we just update the toml file to say e.g. .Let’s see what it looks like to use mise to manage Python versions for two projects with different requirements:tree

├── project_new
│	└── mise.toml
└── project_old
    └── mise.toml

project_new/mise.toml
tools]
python project_old/mise.toml
tools]
python project_new
python 
Python 3.11.13

 ../project_old
python 
Python 3.8.20
When we cd into one of the directories listed above, mise automatically makes the version of the tool configured in  available to us. If it isn’t already installed, mise will install it for us. The implication of this is that you can commit a  to your repository, and anyone that has mise installed will automatically get and use the expected dev tools when they enter the project directory. And when it’s time to upgrade a dev tool, you can just update the version number in  and everyone will start using the new version!The fact that mise makes tools available to you according to the  file in your current working directory has further implications: it’s not just developer machines that can benefit from using mise; CI/CD pipelines can benefit greatly as well! When you use mise in your pipelines, you avoid the problem of having out of sync versions between developer and build machines. You get to have a single place where you can configure the version of your dev tools everywhere!As I mentioned in the beginning, besides managing dev tools, mise also allows you to declare and run so-called tasks. Think of a task as an advanced invocation of a bash script. Even if we use tasks as just plain bash scripts (they can do a lot more), it can be a major advantage to declare common operations such as building, testing, linting etc. as mise tasks, since all developers get access to them and will run their commands in exactly the same way every time. If you’re diligent in your naming, you can even make the experience of building or testing across projects identical.The following are examples of some very simple Python-related tasks declared in :Adding this to  will make the commands  and  available. Again, if you check this in to your repo, the commands will be available to all developers and pipelines. And reusing these names in your rust project means that you can use the same commands to tell cargo to install your crates or run your tests.Once you’ve declared your tasks you should of course also use them in your CI/CD pipeline. Doing this makes you less dependent on the particular yaml syntax and arbitrary requirements of your provider, and makes it easier to move to another one if you need to. It also ensures that there’s a standard way to build and test your code, helping to further reduce the amount of “it works on my machine”.There’s a lot of depth to what you can use mise to help you automate. It’s a lovely tool and I hope I’ve spiked your interest enough to give it a try!Although this is a very obvious problem, I want to make it explicit: a major concern of all software dependency management is control of your supply chain; how easy is it for somebody to insert malicious code into a binary you will run hugely impacts the integrity of your systems and data. Depending on your industry, it might not be feasible to use mise as it’s pretty opaque where your dependencies will be downloaded from.]]></content:encoded></item><item><title>KCSA 2nd attempt</title><link>https://www.reddit.com/r/kubernetes/comments/1lnlijw/kcsa_2nd_attempt/</link><author>/u/Low_Half_6876</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 18:16:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello I just want to know that in the KCSA 2ndt attempt will the question be same as the first attempt. Did anyone went through the second attempt of kcsa ?   submitted by    /u/Low_Half_6876 ]]></content:encoded></item><item><title>Klirr: invoice automation tool written on Rust using Typst</title><link>https://www.reddit.com/r/rust/comments/1lnkpi7/klirr_invoice_automation_tool_written_on_rust/</link><author>/u/Sajjon</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 17:43:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Features: * Config once: Set your company, client and project information using interactive Terminal UI (creates RON files). No Rust, Typst or RON skills needed! * Inter-month-idempotent: You build the invoice any number of times, it always results in the same invoice number when run within the same month. The proceeding month the next invoice number will be used. * Calendar aware: Using your machines system time to determine the month, it calculates the number of working days for the target month. Invoice date is set to last day of the target month and due date is set dependent on the payment terms set in your RON files. * Capable: Supports setting number of days you were off, to be extracted from the automatically calculated number of working days. Supports expenses using "{PRODUCT}, {COST}, {CURRENCY}, {QUANTITY}, {DATE}" CSV string. * Maintenance free: The invoice number automatically set based on the current month. When you build the invoice the next month, the next number is used * Multi-layout support: Currently only one layout is implemented, but the code base is prepared to very easily support more. * Multi-language support: The labels/headers are dynamically loaded through l18n - supported languages are English and Swedish - it is trivial for anyone to make a PR to add support for more languages.Any and all feedback is much appreciated! Especially on ergonomics and features, but codebase well. It has 97% test code coverage ]]></content:encoded></item><item><title>Linux Lenovo issues with WIFI and Bluetooth after Updating</title><link>https://www.reddit.com/r/linux/comments/1lnkpa3/linux_lenovo_issues_with_wifi_and_bluetooth_after/</link><author>/u/Frequent-Price8935</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 17:43:41 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hello guys, in the last months I got many issues after Updating my Linux System with WIFI and Bluetooth and I want to share my experience to help you out if u got the same issue. I personally found not much about this topic that could help me, therefore u got this to help.I use  on a new Lenovo Thinkpad T14 Gen 5 AMDAt fresh install and not after several months I faced the same Issue that after an update of the system WIFI und Bluetooth disappears. Only after I got in sleep Mode and login again just bluetooth shows up. After a restart nothing shows up again.The issue was, that die BIOS got an Update as well. It seems Lenovo + Linux + Qualcomm got several issues after an BIOS Update.To solve the issue in my case I got to the Lenovo Support site and downloaded the ISO File of the latest BIOS. This Version was two months older than the Version I got installed automatically. I flashed this to my USB and updated the BIOS to the older version. Now my WIFI and Bluetooth works again.If u got the same issue and Google can´t help I hope u see this and maybe is solves your issue as well.]]></content:encoded></item><item><title>Native Subresource Support in Kubectl</title><link>https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl</link><author>/u/abhimanyu_saharan</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:52:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Managing Kubernetes subresources like  and  has traditionally been clunky for CLI users. Until recently, interacting with these required raw HTTP calls or complex  invocations, making operations difficult to script and error-prone. With the graduation of KEP-2590 to stable in Kubernetes v1.33, users now have native support for subresource manipulation directly within .The Kubernetes API defines  such as , , and  to isolate specific update operations. These endpoints are commonly used by controllers, but until now, human users had no clean way to interact with them using kubectl. For example:Fetching the current status of a deployment required querying the full object or making raw HTTP requests.Updating replica counts through the  subresource was either not possible via  or required complex scripting.CustomResourceDefinitions (CRDs) with subresources were even more tedious to handle.This KEP introduces a  flag across key kubectl commands, streamlining these workflows.New Capability:  FlagYou can now use the  flag with the following commands:This change enables consistent and declarative interaction with subresources for both built-in and custom resource types.Example: Get Deployment Status or ScaleExample: Patch SubresourceThis is particularly powerful for CRDs that expose scale or status endpoints.Behavior for Unsupported SubresourcesThe CLI includes input validation and proper error handling:The enhancement builds on the resource builder and visitor pattern already in use within , adding a  chain to target the correct API path. Table printer support was extended to pretty-print responses from  and  subresources, ensuring consistency in output formatting. output reuses  defined in the CRD spec. output mimics native resources by adding desired/available replicas.: Introduced in Kubernetes v1.25 with initial support for , , , and .: Promoted in v1.27 with  support and e2e coverage.: Graduated in v1.33 after over a year of stable usage with no critical issues.The enhancement is entirely client-side.Users on older  binaries will simply not see the flag.The API behavior remains unchanged, so operations can still be performed using  as before.Cluster admins can track usage by inspecting audit logs for subresource API requests, particularly those hitting , , or .This enhancement significantly improves the developer experience when managing Kubernetes objects via . By exposing subresources as first-class citizens, it simplifies scripting, debugging, and day-to-day cluster operations. Whether you're scaling CRDs, inspecting status updates, or editing a subset of object fields,  brings much-needed ergonomics to Kubernetes CLI workflows.If you're using custom controllers or automation pipelines that rely on subresources, it's time to update your toolchains and start leveraging this feature natively through .]]></content:encoded></item><item><title>Ilya Sutskever says future superintelligent data centers are a new form of &quot;non-human life&quot;. He&apos;s working on superalignment: &quot;We want those data centers to hold warm and positive feelings towards people, towards humanity.&quot;</title><link>https://v.redd.it/46bejt6qyv9f1</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:37:31 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to write Rust in the Linux kernel, part 2</title><link>https://lwn.net/SubscriberLink/1025232/fbb2d90d084368e3/</link><author>/u/kibwen</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:24:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!

In 2023, Fujita Tomonori

wrote a Rust version of the existing driver for the
Asix AX88796B embedded Ethernet controller. At slightly more than 100 lines,
it's about as simple as a driver can be, and therefore is a useful touchstone for
the differences between writing Rust and C in the kernel. Looking at the Rust
syntax, types, and APIs used by the driver and contrasting them with the C
version will help illustrate those differences.

Readers who are already conversant with Rust may find this article retreads some
basics, but it is my hope that it can still serve as a useful reference for
implementing simple drivers in Rust. The

C version and the

Rust version of the AX88796B driver are remarkably similar, but there are still some important
differences that could trip up a developer performing a naive rewrite from one to the other.

The least-different thing between the two versions is the legalities. The Rust
driver starts with an

SPDX comment asserting that the file is covered by the GPL,
as many files in the kernel do. Below that is a documentation comment:
    //! Rust Asix PHYs driver
    //!
    //! C version of this driver: [`drivers/net/phy/ax88796b.c`](./ax88796b.c)

As mentioned in the

previous article, comments starting with  contain documentation that applies
to the
entire file. The next few lines are a  statement, the Rust analogue
of :
    use kernel::{
        c_str,
        net::phy::{self, reg::C22, DeviceId, Driver},
        prelude::*,
        uapi,
    };

Like C, Rust modules are located starting from a search path and then continuing
down a directory tree. Unlike C, a  statement can selectively import
only some items defined in a module. For example,  is not a separate module,
but rather a specific item inside the
 module. By
importing both  and 
as a whole, the Rust module can refer to  directly, and
anything else from the PHY module as . These items can always
be referred to by their full paths; a  statement just introduces a
shorter local alias. If a name would be ambiguous, the compiler will complain.

All of these imported items come from the  crate (Rust library),
which contains the bindings between the main kernel and Rust code. In a
user-space Rust project, a program would usually also have some imports from
, Rust's standard library, but that isn't possible in the kernel,
since the kernel needs more precise control over allocation and other details
that the standard library abstracts away. Kernel C developers can't use
functions from libc in the kernel for much the same reason. The
 module contains kernel replacements for many common
standard-library functions; the remainder can be found in , the
subset of  that doesn't allocate.

In the C version of the driver, the next step is to define some constants
representing the three different, but related, devices this driver supports: the
AX88772A, the AX88772C, and the AX88796B. In Rust, items do not have to be
declared before use — the entire file is considered at once. Therefore, Fujita
chose to reorder things slightly to keep the code for each board in its own
section; the types for each board ( and so on) are defined
later.
The next part of the Rust driver is a macro invocation that sets up the
necessary symbols for a PHY driver:
    kernel::module_phy_driver! {
        drivers: [PhyAX88772A, PhyAX88772C, PhyAX88796B],
        device_table: [
            DeviceId::new_with_driver::<PhyAX88772A>(),
            DeviceId::new_with_driver::<PhyAX88772C>(),
            DeviceId::new_with_driver::<PhyAX88796B>()
        ],
        name: "rust_asix_phy",
        authors: ["FUJITA Tomonori <fujita.tomonori@gmail.com>"],
        description: "Rust Asix PHYs driver",
        license: "GPL",
    }

Rust macros come in two general kinds: attribute macros, which are written
 and modify the item that they appear before, and normal
macros, which are written . There is also a less common
variant of attribute macros written  which applies to the
definition that they appear within. Normal macros can use any
matching set of braces to enclose their arguments, but can always be recognized
by the mandatory exclamation mark between the name and the braces.
The convention is to use
parentheses for macros that return a value and braces for macros that are
invoked to define a structure (as is the case here), but that is not actually
required. Invoking the macro with parentheses would have the same result, but it
would make it less obvious to other Rust programmers what is happening.

The  argument to the macro contains the names of the three board
types this driver covers. Each driver has to be associated with information such
as the name of the device and the PHY device ID that it should be active for. In
the C version of the driver, this is handled by a separate table:
    static struct phy_driver asix_driver[] = { ... };

In the Rust code, this information is stored in the code for each board (see
below), since
all PHY drivers need to provide it. Overall, the
 macro serves the same role as the
 macro in C.

Next, the Rust driver defines two constants that the code uses later:
    const BMCR_SPEED100: u16 = uapi::BMCR_SPEED100 as u16;
    const BMCR_FULLDPLX: u16 = uapi::BMCR_FULLDPLX as u16;

Every declaration of a value (as opposed to a data structure) in Rust starts
with either  or . The former are compile-time
constants — like a simple  in C. Types are mandatory for
 definitions, but optional for  ones. In either case,
the type always appears separated from the name by a colon.
So, in this case, both constants are
 values, Rust's unsigned 16-bit integer type. The 
part at the end is a cast, since the original  constants
being referenced are defined in C and assumed to be 32 or 64 bits by default,
depending on the platform.

The final piece of code before the actual drivers is a shared function for
performing a soft reset on Asix PHYs:
    // Performs a software PHY reset using the standard
    // BMCR_RESET bit and poll for the reset bit to be cleared.
    // Toggle BMCR_RESET bit off to accommodate broken AX8796B
    // PHY implementation such as used on the Individual
    // Computers' X-Surf 100 Zorro card.
    fn asix_soft_reset(dev: &mut phy::Device) -> Result {
        dev.write(C22::BMCR, 0)?;
        dev.genphy_soft_reset()
    }

There's a few things to notice about this function. First of all, the comment
above it is not a documentation comment. This isn't a problem because this
function is also private — since it was declared with  instead of
, it's not visible outside this one module. The C equivalent
would be a  function. In Rust, the default is the opposite way around,
with functions being private (static) unless declared otherwise.

The argument to the function is an  called .
References (written with an &)
are in many ways Rust's most prominent feature; they are like
pointers, but with compile-time guarantees that certain classes of bugs (such as
concurrent mutable access without synchronization) can't happen. In this case,
 takes a mutable reference (). The
compiler guarantees that no other function can have a reference to the
same
 at the same time. This means that the body of the
function can clear the  pin and trigger a soft reset without
worrying about concurrent interference.

The last part of the function to understand is the return type,
, and the "try" operator, . In C, a function that could fail often
indicates this by returning a special sentinel value, typically a negative number.
In Rust, the same thing is true, but the sentinel value is called 
instead, and is one possible value of the  enumeration. The other
value is , which indicates success. Both  and 
can carry additional information, but the default in the kernel is for  to carry an

error number, and for  to have no additional information.

The pattern of checking for an error and then immediately propagating it to a
function's caller is so common that Rust introduced the try operator as a
shortcut. Consider the same function from the C version of the driver:
    static int asix_soft_reset(struct phy_device *phydev)
    {
	    int ret;

	    /* Asix PHY won't reset unless reset bit toggles */
	    ret = phy_write(phydev, MII_BMCR, 0);
	    if (ret < 0)
		    return ret;

	    return genphy_soft_reset(phydev);
    }

It performs the same two potentially fallible library function calls, but needs
an extra statement to propagate the potential error. In the Rust version, if the
first call returns an , the try operator automatically returns it.
For the second call, note how the line does not end with a semicolon — this
means the value of the function call is also the return value of the function as
a whole, and therefore any errors will also be returned to the caller. The
missing semicolon is not easy to forget, however, because adding it in will make
the compiler complain that the function does not return a .

The actual driver code differs slightly for the three different boards. The
simplest is the AX88786B, the implementation of which starts on

line 124:

This is an empty structure. An actual instance of this type has no storage
associated with it — it doesn't take up space in other structures,
 reports 0, and it has no padding — but there can still be
global data for the type as a whole (such as debugging information). In this
case, an empty structure is used to implement the  abstraction,
in order to bundle all of the needed data and functions for a PHY driver
together. When the compiler is asked to produce functions that apply to a
 (which the  macro does), it
will use this definition:
    #[vtable]
    impl Driver for PhyAX88796B {
        const NAME: &'static CStr = c_str!("Asix Electronics AX88796B");
        const PHY_DEVICE_ID: DeviceId =
            DeviceId::new_with_model_mask(0x003b1841);

        fn soft_reset(dev: &mut phy::Device) -> Result {
            asix_soft_reset(dev)
        }
    }

The constant and function definitions work in the same way as above. The type of
 uses a static reference (""), which is
a reference
that is valid for the entire lifetime of the program. The C equivalent is a
 pointer to the data section of the executable: it is never
allocated, freed, or modified, and is therefore fine to dereference anywhere in
the program.

The new
Rust feature in this part of the driver is the  block, which is used to implement a

trait. Often, a program will have multiple different parts that conform to
the same interface. For example, all PHY drivers need to provide a name,
associated device ID, and some functions implementing driver operations.
In Rust, this kind of common interface is represented by a
trait, which lets the compiler perform static type dispatch to select the right
implementation based on how the trait functions are called.

C, of course, does not work like this (although
 can sometimes
be used to implement type dispatch manually). In the kernel's C code, PHY drivers are
represented by

a structure that contains data and function pointers. The
 macro converts a Rust trait into a singular C structure full of
function pointers. Up above, in the call to , the
reference to the  type lets the compiler find the right
 implementation, and from there produce the correct C structure
to integrate with the C PHY driver infrastructure.

There are obviously more functions involved in implementing a complete PHY
driver. Luckily, these functions are often the same between different devices,
because there is a standard interface for PHY devices. The C PHY driver code
will fall back to a generic implementation if a more specific function isn't
present in the driver's definition, so the AX88796B code can leave them out.
The other two devices supported in this driver
specify more custom functions to work around hardware quirks, but those
functions are not much more complicated than what has already been shown.

Steps to implement a PHY driver ...
Write module boilerplate (licensing and authorship information,
 statements, etc.).Write module boilerplate (licensing and authorship information, 
statements, a call to ).Implement the needed functions for the driver, skipping functions that can
use the generic PHY code.
Implement the needed functions for the driver, skipping functions that can
use the generic PHY code.
Bundle the functions along with a name, optional flags, and PHY device ID
into a  and register it with the PHY subsystem.
Bundle the functions along with a name, optional flags, and PHY device ID
into a trait; the  macro converts it into the right form for
the PHY subsystem.

Of course, many drivers have specific hardware concerns or other complications;
kernel software is distinguished by its complexity and concern with low-level
details. The next article in this series will look at the design of the interface
between the C and Rust code in the kernel, as well as the process of adding new
bindings when necessary.
]]></content:encoded></item><item><title>A Primer on Memory Management</title><link>https://sudomsg.com/posts/a-primer-on-memory-management/</link><author>/u/marcthe12</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:24:00 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nick Bostrom says AGI won’t stop at the human level, it will quickly lead to superintelligence. From there, machines will outthink the best scientists and invent everything else -- faster and better than humans. &quot;It&apos;s the last invention we’ll ever need.&quot;</title><link>https://v.redd.it/3vyph6eawv9f1</link><author>/u/MetaKnowing</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:23:52 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rewriting pre-1.0 compiler code for better macro error messages</title><link>https://github.com/rust-lang/rust/pull/143070</link><author>/u/kibwen</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:16:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Built a tool to sync Obsidian notes across devices without subscriptions or Git commands</title><link>https://www.reddit.com/r/linux/comments/1lnh4jy/built_a_tool_to_sync_obsidian_notes_across/</link><author>/u/believertn</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:15:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[As someone who spends a lot of time on Linux and likes to take notes in Obsidian, I found syncing notes across multiple devices frustrating. I distro hop often, and making sure my notes are updated everywhere without paying for Obsidian Sync or fiddling with Git commands became a problem I wanted to solve.So I built Ogresync, a free and open-source tool that handles syncing your Obsidian vault automatically using GitHub in the background. Instead of opening Obsidian directly, you launch Ogresync, which syncs your vault, opens Obsidian, and then pushes your changes after you finish editing. There’s no need to remember Git commands or worry about merge conflicts.I know there are plugins that do something similar, but they often expect users to be comfortable with Git. I wanted a solution that just works out of the box, even for people who don’t want to deal with version control.I’d really appreciate feedback from fellow Linux users. How do you sync your notes right now? Does this approach make sense or is there something you’d want it to do differently?]]></content:encoded></item><item><title>Software to format a book</title><link>https://www.reddit.com/r/linux/comments/1lngyhb/software_to_format_a_book/</link><author>/u/Popular_Tour1811</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 15:08:43 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[My grandma is writing a book on her familys' history. I, being the computer literate grandchild, am task with formatting and diagramming the book. Unfortunately, the only software I know for that is Microsoft Publisher and Canva. Anyone know a Foss alternative for those?(I don't know if formatting is the right word for that, nor diagramming. I mean turning plain text into a nice looking pdf with well positioned images that can be printed into a book)]]></content:encoded></item><item><title>what&apos;s the convention for how types and their corresponding methods should be grouped within a file?</title><link>https://www.reddit.com/r/golang/comments/1lnex8l/whats_the_convention_for_how_types_and_their/</link><author>/u/Fueled_by_sugar</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 13:39:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[option a - all the types first, all the methods second:type accessToken string type status uint type organizationID string func (a accessToken) String() string { return string(a) } func (o organizationID) String() string { return string(o) } func (s status) Uint() uint { return uint(s) } option b - methods go below their corresponding types:type accessToken string func (a accessToken) String() string { return string(a) } type status uint func (s status) Uint() uint { return uint(s) } type organizationID string func (o organizationID) String() string { return string(o) }    submitted by    /u/Fueled_by_sugar ]]></content:encoded></item><item><title>[P] I built a Python debugger that you can talk to</title><link>https://www.reddit.com/r/MachineLearning/comments/1lnem9e/p_i_built_a_python_debugger_that_you_can_talk_to/</link><author>/u/jsonathan</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 13:24:42 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] Position: Machine Learning Conferences Should Establish a “Refutations and Critiques” Track</title><link>https://arxiv.org/pdf/2506.19882</link><author>/u/StartledWatermelon</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 13:07:29 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tracking Anticheat Updates</title><link>https://not-matthias.github.io/posts/anticheat-update-tracking/</link><author>/u/not-matthias</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 12:42:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My Linux survived where Windows died</title><link>https://www.reddit.com/r/linux/comments/1lndntz/my_linux_survived_where_windows_died/</link><author>/u/githman</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 12:36:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[TLDR: Modern Linux drivers and hardware compatibility are not as finicky as some people say.My government keeps trying to break our energy system to goodbye; a recent malfunction of power mains fried my old PC's PSU and motherboard but the drive fortunately survived. I bought a slightly more recent system on the local flea market (i5-7400 instead of the old i7-3770K) for the whole whopping €70 and plugged the drive into it. The drive had both Windows 10 and Fedora 42 KDE installed.The outcome: Fedora picked up the new hardware like nothing happened but Windows is stuck on "getting devices ready" forever. Guess it's time to reclaim the Windows partition.Great job, Fedora and Linux in general. I had to tell it someone and decided to do it here because where else, right.]]></content:encoded></item><item><title>A Tale of Rust Adoption at my Work place</title><link>https://www.reddit.com/r/rust/comments/1lnburk/a_tale_of_rust_adoption_at_my_work_place/</link><author>/u/Vilayat_Ali</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 10:53:16 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hello wonderful rustaceans! I am Vilayat and I work as a senior software engineer at a US based social media company. A few months ago, we had our video processing and indexing service written in Typescript using AWS Cloud Development Kit. However things started to bottleneck and we faced high AWS bills and our service started to fail to scale as having more TS and code for complex business logic but it also caused big bundle size and what not. Simple description - JS hell in a git repo deployed on AWS. So, the technical leadership asked for porting most of our newer business logic into Golang. And it was this decision that makes Rust, the superhero. So we finished porting the code from TS into Golang. Everything went smoothly and it all went great. Until recently, the team lead had an idea, our auth service handles millions of request so why not port it into go? So he did. Until he faced a issue. So we need to have unified routes for all users and decisions take place at handlers. And in order to abstract the issue, using a interface is preferred, so he did. Created subtypes for each major user type, defined interface for most common permission handlers. But what about general ones? golang doesnt support empty interface implementations. type GeneralPermission interface {}// cant implement it on user GeneralPermissionSo he brought the issue as it makes RBAC a major issue and code will be verbose and hard to maintain. Worst part was, he was a week into the effort so the upper management was expecting results.Until I told him, the beauty of traits!!!!!!impl User for PowerUser {}impl User for CustomerUser {}So, we secretly ported the existing service into a single axum server (Rust!!) using sqlx as query builder. Everything is heaven then. Tech team is now open minded and everything is praising the language. The upper management is interested to use rust as much as possible!When Go’s limitations hit, Rust’s traits and performance swoop in to save the day."Why fight the language when you can just use Rust?"Has your team faced a similar turning point? Share your war stories below! 🚀]]></content:encoded></item><item><title>On Error Handling in Rust</title><link>https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html</link><author>/u/KnorrFG</author><category>reddit</category><pubDate>Sun, 29 Jun 2025 10:46:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The current standard for error handling, when writing a crate, is to define
one error enum per module, or one for the whole crate
that covers all error cases that the module or crate
can possibly produce, and each public function that returns a  will use
said error enum.This means, that a function will return an error enum, containing error variants that
the function cannot even produce. If you match on this error enum, you will
have to manually distinguish which of those variants are not applicable in
your current scope, based on the documentation of the function (and who reads that anyway? /s).The problem with the status quoWhat makes Rust so great, is the ability to express requirements via the type
system in a way that makes it very hard for you to violate them, and yet, we
collectively decided to create these huge error-enums. I completely understand
where this is coming from. Defining an extra error enum for every function
and all the conversions between them is extremely tedious. And so everyone and
their mother is building big error types. Well, not Everyone. A small handful of
indomitable nerds still holds out against the standard.An error is a singular bit of information, might be completely independent
of other errors a function can return, and should probably be represented
by a struct rather than an enum variant. A function returns one of
a set of those if it goes wrong, but it doesn't define the errors
themselves. The first Rust crate I saw that followed this philosophy, was
terrors (Go ahead, check it out).
I still think it's beautiful. It's also a little inconvenient.
You have to write  a lot and some functions
have a lot of possible error points, some of which being
the contents of other function's error sets. And yet, you have to spell
them out all over again. Still, I really like this crate ... from a distance.Speaking of error sets, there is a
crate with this name, that I
prefer to use nowadays. Instead of doing Olympia level type acrobatics (like
terrors) it uses macros. It allows you to define error enums for different
functions in a very concise way and automatically generates the trait
implementations for conversions between those. Want a taste?It allows us to create error sets from variants and from unions with other error sets.
The  operator will work if the error set you use it on is a sub-set of the function's
error set, and it will find out whether that's the case, even if you don't use the
union operator, i.e. this works:This is still a bit too verbose for my tastes if you use many actual struct errors,
e.g. because you want some fields on them to carry additional information, or because
you want to annotate them with error messages. However, I need them seldomly enough,
so that I'll happily pay the extra keystrokes to define a wrapper enum for them
(like the  enum in the first example) for now.There are more libraries out there that explore this paradigm in different ways,
e.g. SmartErr. And I once saw a crate
that offered an attribute macro that you could slap on a function, and then
it would parse the functions body and generate an error enum and insert it into
the functions return type,
based on the errors that occured in the function's body. Sadly I didn't find
it again despite searching for it for an hour. If anyone has a link,
please tell me.]]></content:encoded></item></channel></rss>