<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://www.awesome-dev.news</link><description></description><item><title>I’ve released a game where players write real JavaScript code to battle other players online</title><link>https://store.steampowered.com/app/1137320/Screeps_Arena/</link><author>/u/artchiv</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 20:50:23 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>to transaction or not to transaction</title><link>https://www.reddit.com/r/golang/comments/1olxt4z/to_transaction_or_not_to_transaction/</link><author>/u/PancakeWithSyrupTrap</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 20:15:00 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Take this simplistic code:func create(name string) error {if err != nil { return err }err := writeToDatabase(name)if err != nil { return err}func newDisk(name) error {name, err := getDisk(name)if err != nil { return err }if name != "" { return nil }if err != nil { return err}This creates a disk and database record.The `newDisk` function idempotently creates a disk. Why ? If writing a database record fails, there is an inconsistency. A real resource is created but there is no record of it. When client receives an error presumably it will retry, so a new disk will not be created and hopefully the database record is written. Now we are in a consistent state.But is this a sensible approach ? In other words, shouldn't we guarantee we are always in a consistent state ? I'm thinking creating the disk and writing a database record should be atomic.]]></content:encoded></item><item><title>Compile from a git repo but make changes</title><link>https://www.reddit.com/r/golang/comments/1olx8jv/compile_from_a_git_repo_but_make_changes/</link><author>/u/No-Confection8657</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 19:51:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am running a VPS with ubuntu aarch64 and have go 1.25. I am trying to compile a program from a repo that is written in go but want to also implement a change from a pull request. The repo isn't mine, though I do have a fork of it on my git. I installed task and followed the steps in the contributing.md file. When I "task deps" it did spit out an error that was basically the same as when I was doing it passing go commands manually:I decided to just try ignoring that and running "task" to build it. And it seemed to compile and I have successfully ran it.Here is my issue now - I manually made the changes to the VERSION and internal/tgc/channel_manager.go files locally before running this but I think it just went ahead and used the original versions ignoring my changeswhen I run teldrive version it spits out 1.7.0 and the changes to the version file is 1.7.1 - also the file that got generated is the exact same amount of bytes as the 1.7.0 release. So I think it just made the file with none of the changes I had manually input into the local copies of the files.Then when I run task, it exits with the following error:task: Failed to run task "default": task: Command "go run scripts/release.go --version current" failed: exit status 1not sure what would cause this - when I look at that file, it seems to just reference the VERSION file to get the version number. and it simply says 1.7.1 instead of 1.7.0Am I missing something obvious? Sorry for the long post, I am new at this.]]></content:encoded></item><item><title>Understanding Multi-Platform Docker Builds with QEMU</title><link>https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/</link><author>/u/Helpful_Geologist430</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 19:14:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[One intriguing feature of containers and images is their multi-platform support. Docker uses Buildx, which is based on BuildKit, to enable multi-platform builds.The old way (build for the current host’s platform, if you’re on an ARM CPU, you build an ARM image that won’t run on x86, and vice versa):The new way: without a single care, build a multi-platform image that supports both x86 and ARM (and others):docker buildx build  linux/amd64,linux/arm64 How is this sorcery possible? Let’s take a look.Containers, under the hood, are simply processes isolated thanks to Linux’s namespaces. The executables and files of these processes, packaged in layers, are compiled for a specific architecture.Put differently, a container is a bundled runtime. This is what the OCI runtime bundle defines:coolcontainer/
├── config.json
└── rootfs/
    ├── bin/
    ├── lib/
    └── ...
An OCI runtime bundle (used to start a container) is obtained from an OCI image (Docker images are OCI-compliant).1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
apt  umoci skopeo runc


skopeo copy docker://alpine:latest oci:alpine:latest

alpine/

umoci unpack  alpine:latest alpine-runtime-bundle
alpine-runtime-bundle/

runc run  alpine-runtime-bundle mycoco
    yo  /home/greeting
    alpine-runtime-bundle/rootfs/home/greeting
This spec defines what’s needed to run a container. All OCI-compliant container solutions adhere to it (Docker, Podman, etc.). These files are then used to create a container process. The isolation is achieved through Linux namespaces. To the container, it feels like it’s running on its own filesystem, network, PID space, and so on, but in reality, it’s just a process, albeit a well-isolated one.The reference implementation that takes an OCI runtime bundle and starts a container is  (Docker uses it under the hood). It takes all the information and layers in the bundle and creates the container process. Mounts, environment variables, and all kinds of options that can be specified when running a container are handled by .This means that the executables and binaries are destined for a specific OS and architecture:file  alpine-runtime-bundle/rootfs/bin/ls
alpine-runtime-bundle/rootfs/bin/ls: ELF 64-bit LSB executable, ARM aarch64
So the  command inside the container layers is simply a regular executable built for ARM64.You can’t just run an image built for an x86 CPU on an ARM CPU (out of the box). That’s where multi-platform images come into the picture.An image that supports multiple architectures? Say what?skopeo inspect  docker://docker.io/library/ubuntu:latest | jq | 
        ...
1
2
3
4
5
6
7
8
9
10
11
12

skopeo copy  amd64  linux 
  docker://docker.io/library/nginx:latest 
  oci:nginx-amd64:latest

umoci unpack  nginx-amd64:latest nginx-amd64-runtime-bundle


file  nginx-amd64-runtime-bundle/rootfs/bin/ls
J’accuse! Intruder! An x86-64 binary on an ARM machine!./alpine-runtime-bundle/rootfs/bin/ls

./nginx-amd64-runtime-bundle/rootfs/bin/ls

runc run  nginx-amd64-runtime-bundle my-nginx-container
And that’s the heart of the problem when it comes to running containers across different platforms. What to do?QEMU (Quick EMUlator) is quite the remarkable piece of software. It’s both an emulator and a virtualizer, and it also provides user-level emulation. Ehhh, what? Well, that’s what you get when you look up QEMU. Let’s put it in simpler terms:Emulator: It emulates hardware. It simulates entire systems (CPU, memory, disk, network, etc.) in software, meaning it exposes an interface to a guest program similar to actual hardware. Think about it: for an OS, all it sees is a bunch of CPU machine code that interacts with hardware and registers. If those registers and hardware behaviors are simulated in software, the OS is none the wiser and that’s exactly what QEMU does. You can simulate different CPUs (ARM, x86, RISC-V, etc.), run machine code instructions, and update state (registers, flags, program counter, etc.) as if you were running on real hardware, it’s just slower. By emulating CPUs in software, QEMU can run an OS built for the same or a different CPU architecture.Virtualizer: Some CPUs offer hardware-assisted virtualization, basically, the CPU can differentiate between a guest and a host OS. This is a lot faster than using an emulator, but since you’re using the same CPU, you can only run a guest OS built for that CPU (for example, an x86 Linux guest on an x86 Linux host). This is supported in Linux through KVM. QEMU can make use of KVM, so when available, it’s better to use it for faster guest execution.User-space emulation: This allows us to run a binary built for an architecture different from our machine’s by translating machine code and system calls on the fly. For instance,  works on an  CPU as if it were native. It’s truly magical, QEMU decodes ARM instructions and translates them into x86-64 ones, roughly:ARM code:          ADD R0, R1, R2
QEMU intermediate: tcg_gen_add_i32(result, R1, R2)
x86-64 host code:  mov eax,[R1]; add eax,[R2]; mov [R0],eax
So QEMU user-space emulation is the first piece of the cross-platform image puzzle.The second piece is . It stands for Binary Format Miscellaneous (quite the name). The basic idea is that your Linux kernel knows how to run executables built for its own architecture. If you’re on an x86-64 CPU, your kernel can run x86-64 ELF files by default. It can’t run executables built for other architectures (like ARM) or other file types (like Windows  files or scripts). is a kernel feature that allows us to specify an interpreter or program to handle certain files, based on their extension or on a magic byte sequence contained within the file. ARM Linux executables, for instance, have a distinguishable magic sequence:  We can configure  to use  whenever it encounters a file with that magic sequence. Similarly, we can configure it to use  when encountering files with a  extension.1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
 |  /proc/sys/fs/binfmt_misc/register


./HelloWorld.jar 
 +x /usr/local/bin/java-wrapper

 |  /proc/sys/fs/binfmt_misc/register

./HelloWorld.jar
So,  lets the kernel specify a wrapper, interpreter, or command to run certain files based on their magic bytes or file extensions.To recap:  allows us to execute binaries from other architectures, and  is the mechanism that maps those binaries (based on their magic bytes) to the appropriate QEMU user-space command.In Docker’s documentation about multi-platform builds, they explain that Docker Desktop supports multi-platform images with QEMU out of the box. (Docker Desktop is essentially a Linux VM tailored to run Docker, so it already has this configured.)For Docker engine in Linux, we need to run:docker run  tonistiigi/binfmt  all
This registers the  mappings (like we did above for Java and x86) but for all architectures.The image tonistiigi/binfmt contains a Go binary that basically does what we demonstrated earlier, setting up mappings from ELF magic bytes to the appropriate QEMU binary for multiple architectures:1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
In , we find this delightful snippet:Ok, we know how foreign binaries are run. But how does it all tie together? How are we actually building these multi-platform images?The Docker docs have a nice example:
FROM alpine

RUN  /arch
docker buildx build linux/amd64,linux/arm64  letsgo:1.0 

docker run linux/arm64 letsgo:1.0  /arch


docker run linux/amd64 letsgo:1.0  /arch
It’s beautiful! So what happened exactly? By specifying --platform=linux/amd64,linux/arm64, we’re asking Docker to build two images, one for each platform. The pulled base layer () is platform-specific, and the binaries within it are built for each architecture. Let’s verify that:docker run linux/arm64 letsgo:1.0 sh
apk add file 

file  /bin/uname
Nice! The  runs , and that’s where the QEMU magic occurs. Under the hood, each binary in the image layers, compiled for its target architecture, is executed. Docker’s  instruction spawns a new process on the host (isolated within namespaces, but still just a process). Depending on the file’s magic bytes, the appropriate QEMU interpreter is invoked automatically via  and that RUN command works. If it was not for  and QEMU, we’d get a polite .QEMU isn’t the only way to build multi-platform images. Docker Buildx supports using multiple builder nodes (a cluster), and you can use nodes with different architectures to build images natively for their respective platforms. There’s even a cloud offering built around this approach.
docker buildx create  multiarch-builder unix:///var/run/docker.sock

docker buildx create  multiarch-builder ssh://user@arm64-host
docker buildx use multiarch-builder
For compilers that support cross-compilation (compiling code on one platform, the host, to create an executable for a different platform ,the target), like Go, which does so natively by specifying  and , you can build directly for each target architecture without relying on emulation. For example, in a Dockerfile build stage you might run:RUN  go build  server Then, you can simply copy the resulting binary into the runtime stage. Since the Go compiler supports cross-compilation, there’s no need to use QEMU here. Instead, we rely on the  and  environment variables provided automatically by Docker Buildx.]]></content:encoded></item><item><title>[P] Flow Matching: A visual introduction</title><link>https://peterroelants.github.io/posts/flow_matching_intro/</link><author>/u/Xochipilli</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 18:06:53 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've been working with flow matching models for video generation for a while, and recently went back to my old notes from when I was first learning about them. I cleaned them up and turned them into this blog post.Hopefully it’s useful for anyone exploring flow matching for generative modeling. Writing it certainly helped solidify my own understanding.]]></content:encoded></item><item><title>DigitalOcean is chasing me for $0.01: What it taught me about automation</title><link>https://linuxblog.io/digitalocean-1-cent-automation/</link><author>/u/modelop</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 16:29:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There are three kinds of emails that can ruin a quiet Saturday: a security warning, an outage alert, and, apparently, a repeat reminder that you owe a cloud provider one cent, yes, $0.01. I’ve been using DigitalOcean since 2013. Personally, I don’t use it often, but I log in several times a week to support clients hosted there.A chuckle and twelve years of cloud loveOver the past twelve years I have set up and managed countless droplets, and DigitalOcean’s support and uptime have been excellent; this isn’t that kind of post.It’s a lighthearted look at what happens when automation churns out more notifications than the situation may warrant, and why even a penny‑sized bill can teach us bigger lessons about design and efficiency.On Saturday, 25 October 2025, an email with the subject “Payment required: Your pre‑payment has been used” arrived in my inbox. It informed me that my prepaid credit was insufficient to cover the month’s usage and urged me to “make another payment or add an alternate payment method.”There was just one catch: the outstanding balance was $0.01. I chuckled, and went on with my day only to receive the exact message two more times over the coming days. By the time Saturday rolled around, my inbox looked like this:The inbox search screenshot above shows the cadence: identical “Payment required” messages on October 25th, 28th, and 31st, 2025, followed by an email on November 1, 2025, titled “Your 2025‑10 invoice is available.”The invoice email (screenshot also above) contains a table that lists the usage charges for October as $0.01, notes that the payment method will only be charged if the balance exceeds $3.00, and invites me to “View Invoice.” Here’s what those other three messages look like:My immediate reaction was a bit of a chuckle, but by the fourth email, I was more curious than anything: Why does an automated billing system send four emails about a 1-cent balance? DigitalOcean’s billing documentation notes that invoices are generated monthly. In my case, the system sent several “action required” emails, maybe because I don’t have a payment method saved? But in any case, I rarely use my personal DigitalOcean account beyond just quick tests:This experience of multiple emails for 1 cent owed, prompted me to think about the hidden costs of excessive email notifications and how we can design billing and alerting in a more thoughtful way.The True Cost of an EmailEmail feels free because individuals don’t pay per message, but providers do. A 2025 breakdown of email marketing costs notes that the typical cost for a business to send emails is $1–$2 per thousand messages, translating to roughly $0.001–$0.002 per email. Amazon’s Simple Email Service charges $0.10 per 1,000 emails for outbound messages (sending or receiving) and a few cents per gigabyte for attachments.This cost is likely less for DigitalOcean, with the three “Payment required” notices and one invoice with attachment costing the company at most between a tenth and two‑tenths of a cent to send. But multiply that by hundreds of thousands of customers, and it highlights how easy it is to use resources to clutter inboxes over microbalances.The monetary cost is only part of the picture. Email has an environmental footprint because electricity powers servers, networks, and client devices. Researchers estimate that more than 306 billion emails were sent in 2021, and the total is expected to hit almost 400 billion this year, thanks to DigitalOcean. jk!!According to Mike Berners‑Lee, a short text email can produce 0.2–0.3 g of CO₂, while a longer message with attachments can produce 17 g; an email blast to 100 people may generate 26 g or more. Email‑related emissions accounted for approximately 150 million tons of CO₂e in 2019. That’s about 0.3% of the world’s carbon footprint. But more importantly, about 25% added to users’ annoyance levels – Source: Notification fatigue and design principlesIt isn’t just about costs or the environment. Usability tests consistently show that frequent alerts are one of the top user complaints. In fact, it’s been proven by Facebook and others that sending fewer notifications can be better for both engagement and retention.Good notification design also recognizes levels of severity: high‑attention alerts (e.g., security breaches or failed payments) should prompt immediate action, while low‑attention messages (informational updates) can be bundled or deferred. Services like Slack, for example, adapt notification frequency automatically when channels become very active.Looking at DigitalOcean’s billing reminders, it’s easy to see opportunities for improvement. A one‑cent balance does not warrant three emails + an invoice. The first message could have been informational (“heads up, your balance is low”), the second might wait until the balance crosses a predetermined threshold (say $1 or $3), and the third could be a month or 3 months later.Alternatively, DigitalOcean could incorporate a small balance waiver similar to the one many credit card issuers use. Banks recognize that it’s not cost‑effective to chase pennies; they round down or apply a credit adjustment on the next statement. The same logic could help cloud providers reduce overhead and user frustration.It’s not just DigitalOcean: micro‑balances happen everywhereDigitalOcean isn’t alone in sending tiny bills. Back in 2013, an Optus customer in Australia posted on Whirlpool forums that a billing error left them with a one‑cent overdue notice after receiving a reimbursement. One commenter wrote that it would cost the company “more in personnel overheads to deal with this stupid billing error, than what it’s worth”, while another explained, “It’s an automated system, mate. Just relax.”The moral of the story is that most companies rely on automated billing scripts, and without sensible thresholds, they’ll dutifully produce statements for even the most trivial amounts.In practice, if you owe 99 cents or less, many companies apply a credit adjustment and report a zero balance. The banking industry has recognized that goodwill and efficiency outweigh the pennies left on the ledger. If major financial institutions can swallow a dollar, cloud platforms with higher margins can too.What this taught me and how I’ve been guilty tooAs someone who deploys systems and manages mail servers, I can’t throw stones without acknowledging my own missteps. Earlier this year, I built dewedda.com, a storm‑watch website for the Eastern Caribbean. Part of which was to send automatic email alerts to subscribers when storms approached islands within specific distances and directions.In testing, everything looked great: the algorithm computed wind fields, adjusted for intensity, and tracked dozens of scenarios. But the first time a real storm approached, my code started hammering subscribers with unnecessary alerts. It didn’t account for storms that curved away or systems that were still unnamed, resulting in duplicates, so people kept receiving warnings even when there was no threat or duplicate emails. I had to scramble to adjust the logic.The experience taught me humility and the importance of edge cases, and it makes me more sympathetic to DigitalOcean’s engineers. Building resilient billing and notification systems is complex. Edge cases arise when accounts straddle billing cycles, use promotional credits, or move between team and personal billing. Legacy code and third‑party integrations can behave unpredictably. What matters is how we learn from these events.Conclusion (yes, I paid that one cent)I paid the 1 cent balance owed to DigitalOcean. But who covers that transaction cost?In the end, I did what any responsible business owner would do: I logged into my account and paid the one‑cent balance. Because, it would sit there for months, I only used a droplet for ~1 hour to test something. My personal DigitalOcean account goes mostly unused. So paying this invoice also means no recurring emails to pay my bill. Maybe that’s their plan? Ha!I hope this article highlights the hidden inefficiencies that creep into automation, whether it’s cloud invoices, marketing emails, or storm alerts.I still recommend DigitalOcean to friends and clients. They offer a great product at a fair price, with transparent billing. Being able to spin up a droplet in a few seconds makes life easy for Linux nerds like me. This one‑cent episode doesn’t change that; it simply underscores the value of thoughtful notification design. There are no affiliate links in this post either.In summary, as repeatedly proven, sending fewer, more relevant notifications improves user satisfaction and retention. The environmental data also shows that unnecessary emails carry hidden costs, and financial industry practices demonstrate that forgiving tiny balances can be cheaper than collecting them.A bit of humor on a Saturday morning turned into a lesson for all of us on building better systems. And yes, just in case the automated script is listening, I can confirm that as of writing this, my DigitalOcean account balance is zero.]]></content:encoded></item><item><title>Convex Optimization (or Mathematical Programming) in Go</title><link>https://www.reddit.com/r/golang/comments/1ols1gn/convex_optimization_or_mathematical_programming/</link><author>/u/RobotCyclist23</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 16:23:44 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Do you write a lot of Convex (or similar) Optimization problems and have been yearning for a way to model them in Go? MatProInterface.go can help you (and needs your input to gain more maturity)! Feel free to try it and let me know what you think!]]></content:encoded></item><item><title>[D] Best (free) courses on neural networks</title><link>https://www.reddit.com/r/MachineLearning/comments/1olqqj0/d_best_free_courses_on_neural_networks/</link><author>/u/gized00</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 15:31:18 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I have to prepare a course on NN for master students. As you can expect, there is A LOT of interest into learning about LLMs and related topics. So I would like to spend ±50% of the classes on Transformers/attention mechanisms/RL in LLMs/etc.At the best of my knowledge there is no textbook on this (using Deep Learning by Goodfellow et al. for the first part) so I am looking for the best classes on similar topics. I will use these classes to inform my selection of topics, see how others introduce such topics, calibrate the amount of topics that I can cover, and list them on the course website. I am not planning to "steal" material without permission or anything like that.In your opinion, which ones are THE BEST classes on Transformers and related topics?]]></content:encoded></item><item><title>[D] Simple Questions Thread</title><link>https://www.reddit.com/r/MachineLearning/comments/1olpzpu/d_simple_questions_thread/</link><author>/u/AutoModerator</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 15:00:44 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!]]></content:encoded></item><item><title>VST3 now open source (MIT Licence)</title><link>https://youtu.be/grMxkISQNyw?si=AF3vDzec-bBld-EF</link><author>/u/Kunstbanause</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 14:41:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>i&apos;m a zoomer on cachyOS but it seems to run in the family; my father has a jacket with a sun microsystems embroider on the front</title><link>https://www.reddit.com/r/linux/comments/1olp7wt/im_a_zoomer_on_cachyos_but_it_seems_to_run_in_the/</link><author>/u/bonzibuddy_official</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 14:28:52 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[he's mentioned having a good amount of experience in red hat mostly for his career, we ended up finding this in storage recently. it also has another larger embroidery of the java logo on the back. it's comfortable and fits me still which also rocks. i started using linux (mint) around 2021/2022 for hobbyist and general purposes, had to mostly run windows for college using adobe, no longer doing all of that so i'm back on cachy since it seems promising enough for an arch derivative. thought this would be neat to share on here. thank you unix for being the foundation for the funny little penguin kernal that's sure to sweep any year now :P]]></content:encoded></item><item><title>Go&apos;s Context Logger</title><link>https://github.com/pablovarg/contextlogger?tab=readme-ov-file#examples</link><author>/u/PurityHeadHunter</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 14:24:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello Gophers! A while ago, I started using contextual logging in my projects and found it made debugging significantly easier. Being able to trace request context through your entire call stack is a game-changer for understanding what's happening in your system.This project started as a collection of utility functions I copy-pasted between projects. Eventually, it grew too large to maintain that way, so I decided to turn it into a proper library and share it with the community. https://github.com/PabloVarg/contextloggerContext Logger is a library that makes it easy to propagate your logging context through Go's  and integrates seamlessly with Go's standard library, mainly  and . If this is something that you usually use or you're interested on using it for your projects, take a look at some Usage Examples.For a very simple example, here you can see how to:Embed a logger into your contextUpdate the context (this can be done many times before logging)Log everything that you have included in your context so farctx = contextlogger.EmbedLogger(ctx) contextlogger.UpdateContext(ctx, "userID", user.ID) contextlogger.LogWithContext(ctx, slog.LevelInfo, "done") ]]></content:encoded></item><item><title>Async Rust explained without Tokio or Smol</title><link>https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD</link><author>/u/Gisleburt</author><category>rust</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 14:00:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>unsupportedConfigOverrides USAGE</title><link>https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/</link><author>/u/BigBprofessional</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 13:52:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)</title><link>https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/</link><author>/u/Dependent_Concert446</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 13:22:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I’m looking for some advice on how to organize our Helm charts and Bitbucket repos for a growing  setup.We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).For our , we created  that’s used for microservices. We don’t have separate repos for each microservice — all are managed under the same project.Here’s a simplified view of the repo structure:app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments The following  are also in the same repo right now:Project Contour (Ingress)(and other cluster-level tools like k3s, Longhorn, etc.)These are not tied to the application project — they’re might shared and deployed across multiple clusters and environments.Should I move these shared infra components into a separate “infra” Bitbucket repo (including their Helm charts, Terraform, and Ansible configs)?For GitOps with , would it make more sense to split things like this:  → all microservices + base Helm chart → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps? Used AI to help write and format this post for grammar and readability.]]></content:encoded></item><item><title>I wrote the dumbest key-value db i could think of</title><link>https://www.reddit.com/r/golang/comments/1oln8uo/i_wrote_the_dumbest_keyvalue_db_i_could_think_of/</link><author>/u/AnonymZ_</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 13:01:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So i wrote the dumbest key value db for a go course. It’s called kvd, and it uses docker containers as storage (github.com/YungBricoCoop/kvd)every SET creates a container, every GET reads from it. if the key already exists, it just renames the old container with a prune_ prefix instead of deleting it directly, because stopping containers takes forever then every 30 seconds, a pruning system comes around and actually stops and removes them.it’s slow as hell, and it’s one of the worst ways you could ever implement a key value db. but it works and acts has a redis server.the project isn’t really the point though, i kinda want to create a github org that stores weird-ass but projects, like good ideas implemented in the dumbest way possible or just in an insane creative way.drop a comment if you want to be part of the org and throw some name ideas for the org too]]></content:encoded></item><item><title>Hard Rust requirements from May onward for all Debian ports</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/pyeri</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:32:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Hard Rust requirements from May onward (for Debian&apos;s package manager, APT)</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/DeleeciousCheeps</author><category>rust</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:24:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Python refuses $1.5M grant, Unity&apos;s in trouble, AUR attacked again - Linux Weekly News</title><link>https://tilvids.com/videos/watch/02a038db-fdd0-46d4-8cb2-1f0b1b0bd04d</link><author>/u/Pure_Toe6636</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:08:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>We should be very concerned about knowing who&apos;s real and who isn&apos;t</title><link>https://www.reddit.com/r/artificial/comments/1olm00b/we_should_be_very_concerned_about_knowing_whos/</link><author>/u/datascientist933633</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 11:59:01 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Colleague of mine recently started their own AI company, it's basically a voice call service that can be used to sell to people and do outbound marketing and sales. The thing is completely disturbing and dystopian. It called me for a test and I thought I was talking to a real person. It was so lifelike, the vocalizations were so real and unbelievably authentic. This is one concern about AI that I have recently. How in the heck do you know who is real and who isn't? ]]></content:encoded></item><item><title>You&apos;re absolutely right.</title><link>https://v.redd.it/u13z27vogmyf1</link><author>/u/MetaKnowing</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:23:51 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I created Open Source Kubernetes tool called Forkspacer to fork entire environments + dataplane, it is like git but for kubernetes.</title><link>https://www.reddit.com/r/kubernetes/comments/1olk9we/i_created_open_source_kubernetes_tool_called/</link><author>/u/Laughing-Dawg</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:15:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I created an open-source tool that lets you create, fork, and hibernate entire Kubernetes environments.With , you can fork your deployments while also migrating your data.. not just the manifests, but the entire data plane as well. We support different modes of forking: by default, every fork spins up a managed, dedicated virtual cluster, but you can also point the destination of your fork to a self-managed cluster. You can even set up multi-cloud environments and fork an environment from one provider (e.g., AWS) to another (e.g., GKE, AKE, or on-prem).You can clone full setups, test changes in isolation, and automatically hibernate idle workspaces to save resources all declaratively, with GitOps-style reproducibility.It’s especially useful for spinning up dev, test, pre-prod, and prod environments, and for teams where each developer needs a personal, forked environment from a shared baseline.License is Apace 2.0 and it is written in Go using Kubebuilder SDKPlease give it a try let me know, thank you]]></content:encoded></item><item><title>what exactly is open-sourced in grokipedia?</title><link>https://www.reddit.com/r/linux/comments/1olk43q/what_exactly_is_opensourced_in_grokipedia/</link><author>/u/nix-solves-that-2317</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:05:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Monthly: Certification help requests, vents, and brags</title><link>https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/</link><author>/u/thockin</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:01:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Did you pass a cert? Congratulations, tell us about it!Did you bomb a cert exam and want help? This is the thread for you.Do you just hate the process? Complain here.(Note: other certification related posts will be removed)]]></content:encoded></item><item><title>Monthly: Who is hiring?</title><link>https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[This monthly post can be used to share Kubernetes-related job openings within  company. Please include:Location requirements (or lack thereof)At least one of: a link to a job posting/application page or contact detailsIf you are interested in a job, please contact the poster directly. Common reasons for comment removal:Not meeting the above requirementsRecruiter post / recruiter listingsNegative, inflammatory, or abrasive tone   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)</title><link>https://ar5iv.labs.arxiv.org/html/1901.09056</link><author>/u/Zomgnerfenigma</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 09:27:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Challenge of Benchmarking WebAssemblyThe aforementioned suite of 24 benchmarks is the PolybenchC benchmark
suite , which is designed to measure the effect of
polyhedral loop optimizations in compilers. All the benchmarks in the
suite are small scientific computing kernels rather than full
applications (e.g., matrix multiplication and LU Decomposition); each is
roughly 100 LOC. While WebAssembly is designed to accelerate scientific
kernels on the Web, it is also explicitly designed for a much richer set
of full applications.The WebAssembly documentation highlights several intended use
cases , including scientific kernels, image editing,
video editing, image recognition, scientific visualization, simulations,
programming language interpreters, virtual machines, and POSIX applications.
Therefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC
do not imply that it will perform well given a different kind of application.We argue that a more comprehensive evaluation of WebAssembly should rely on an
established benchmark suite of large programs, such as the SPEC CPU benchmark
suites. In fact, the SPEC CPU 2006 and 2017 suite of
benchmarks include several applications that fall under the intended use cases of
WebAssembly: eight benchmarks are scientific applications (e.g., ,
, , , and
), two benchmarks involve image and video processing
( and ), and all of the benchmarks are POSIX
applications.Unfortunately, it is not possible to simply compile a sophisticated
native program to WebAssembly. Native programs, including the programs in
the SPEC CPU suites, require operating system services, such as a
filesystem, synchronous I/O, and processes, which WebAssembly and the
browser do not provide. The SPEC benchmarking harness itself requires
a file system, a shell, the ability to spawn processes, and other Unix
facilities. To overcome these limitations when porting native
applications to the web, many programmers painstakingly modify their
programs to avoid or mimic missing operating system
services. Modifying well-known benchmarks, such as SPEC CPU, would not
only be time consuming but would also pose a serious threat to
validity.The standard approach to running these applications today is to use
Emscripten, a toolchain for compiling C and C++ to
WebAssembly . Unfortunately, Emscripten only supports
the most trivial system calls and does not scale up to large-scale
applications. For example, to enable applications to use synchronous
I/O, the default Emscripten  filesystem loads the entire
filesystem image into memory before the program begins executing. For
SPEC, these files are too large to fit into memory.A promising alternative is to use , a framework that enables
running unmodified, full-featured Unix applications in the
browser .  implements
a Unix-compatible kernel in JavaScript, with full support for
processes, files, pipes, blocking I/O, and other Unix features.
Moreover, it includes a C/C++ compiler (based on Emscripten)
that allows programs to run in the browser
unmodified. The  case studies include complex applications,
such as , which runs entirely in the browser without any
source code modifications.Unfortunately,  is a JavaScript-only solution, since it was
built before the release of
WebAssembly. Moreover,  suffers from high performance overhead,
which would be a significant confounder while benchmarking. Using ,
it would be difficult to tease apart the poorly performing benchmarks
from performance degradation introduced by .]]></content:encoded></item><item><title>Programming Language Agnostic Naming Conventions</title><link>https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide</link><author>/u/Distinct-Panic-246</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 07:30:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton"There are only two hard things in Computer Science: cache invalidation and naming things"(Or the slight variation of "There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors")But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.Why we spend time naming things correctlyIf you see a variable called , you can probably assume it is a boolean.  or  are not clear.Avoid Negative variable names:Negative names can lead to double negatives, which are confusing.Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.(Although  is probably an exception where it should always be used over ).Pick a language and always use thatIf you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)Make booleans obvious by using is/has prefixIf you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a booleanWords like , ,  are too generic. Try to avoid these termsPick a convention for naming things, and use those everywhere.calculateAmountBad 👎:  and Good 👍:  and Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice) for class names for most other variables for static constantsCommon names for specific thingsIf you're taking some data and  it to a different shape or different values then  is a common and accurate name.If you need to check if data is valid/correct, then its almost always called a validator.Used when describing the shape of some data structure. Often used with database designs.When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very relatedFor code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that dataWhen you have some functionality with a specific interface, and you need to convert it to another interface/shape.It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)When you need to make data uniform in scale, format, or structure]]></content:encoded></item><item><title>Feeling confused about what to do next ?</title><link>https://www.reddit.com/r/golang/comments/1olhfjy/feeling_confused_about_what_to_do_next/</link><author>/u/Neutrino_i7</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 07:04:30 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[For the past 3 months, I’ve beenusing Go for Building web api, SSR Application, Cli tools, But lately, it’s starting to feel like I’m doing the same thing over and over again — and honestly, it’s getting kinda boring.I love Go, but I feel like I need something new and challenging to spice things up. Should I start learning another language alongside Go (maybe Rust or Python)? Or are there some cool project ideas in Go that can help me]]></content:encoded></item><item><title>How is the current market demand for openstack combined with k8s</title><link>https://www.reddit.com/r/kubernetes/comments/1olgx9m/how_is_the_current_market_demand_for_openstack/</link><author>/u/ossicor30</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 06:30:23 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Well a old school flex i guess</title><link>https://www.reddit.com/r/linux/comments/1olftbt/well_a_old_school_flex_i_guess/</link><author>/u/Puzzleheaded-Car4883</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 05:16:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This old Red Hat Linux 8.0 manual’s been gathering dust on my shelf. I used to read it as a kid — didn’t understand a single word back then. Fast forward to age 19, 3 years into using Linux daily... and everything suddenly makes sense.Btw this is one of those first thing that introduced me to linux ]]></content:encoded></item><item><title>One-Minute Daily AI News 10/31/2025</title><link>https://www.reddit.com/r/artificial/comments/1olf4w9/oneminute_daily_ai_news_10312025/</link><author>/u/Excellent-Target-847</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 04:34:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[, South Korea Government and Industrial Giants Build AI Infrastructure and Ecosystem to Fuel Korea Innovation, Industries and Jobs.[1] says it’s deploying AI technology to stop Halloween parties.[2] AI Unveils Supervised Reinforcement Learning (SRL): A Step Wise Framework with Expert Trajectories to Teach Small Language Models to Reason through Hard Problems.[3] CEO says AI audio models will be ‘commoditized’ over time.[4]]]></content:encoded></item><item><title>Happy Halloween, nerds</title><link>https://www.reddit.com/r/linux/comments/1olf21x/happy_halloween_nerds/</link><author>/u/feelingsupersonic</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 04:29:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] Realized I like the coding and ML side of my PhD way more than the physics</title><link>https://www.reddit.com/r/MachineLearning/comments/1olehrk/d_realized_i_like_the_coding_and_ml_side_of_my/</link><author>/u/PurpleCardiologist11</author><category>ai</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 03:56:20 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey everyone, I’m a 2nd-year ChemE PhD student working on granular media with ML, so, technically, my research is about the physics of these systems. But lately I’ve realized I get way more excited about the numerical modeling and machine learning part than the physics itself. I love building models, debugging, testing new architectures, running simulations… but when it comes to actually digging into the physical interpretation, I kinda lose interest The thing is, I don’t have a CS background, and I usually write “prototype” code that works, but it’s not what you’d call clean software. I never learned data structures, algorithms, or how to structure large projects properly. After my PhD, I think I’d like to move more toward computational or ML-heavy work, something like scientific computing, data-driven modeling, or applied AI for physical systems. For anyone who’s gone down a similar path: - What kind of skills should I start developing now? - How important is it to learn formal CS stuff (like algorithms and software design)? Would love to hear what worked for you. I feel like I’m starting to see where I actually fit, and I just wanna steer myself in the right direction.]]></content:encoded></item><item><title>Futurelock: A subtle risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>/u/iamkeyur</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 03:47:58 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Java Virtual Threads VS GO routines</title><link>https://www.reddit.com/r/golang/comments/1oldyoo/java_virtual_threads_vs_go_routines/</link><author>/u/gamecrow77</author><category>golang</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 03:25:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I recently had a argument with my tech lead about this , my push was for Go since its a new stack , new learning for the team and Go is evolving , my assumption is that we will find newer gen of devs who specialise in Go. Was i wrong here ? the argument was java with virtual threads is as efficient as go ]]></content:encoded></item><item><title>IRS open-sourced the fact graph it uses for tax law</title><link>https://github.com/IRS-Public/fact-graph</link><author>/u/R2_SWE2</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:48:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>shift left approach for requests and limits</title><link>https://www.reddit.com/r/kubernetes/comments/1olaat1/shift_left_approach_for_requests_and_limits/</link><author>/u/Containertester</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:10:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We’re trying to solve the classic requests & limits guessing game; instead of setting CPU/memory by gut feeling or by copying defaults (which either wastes resources or causes throttling/OOM), we started experimenting with a benchmark-driven approach: we benchmark workloads in CI/CD and derive the optimal requests/limits based on http_requests_per_second (load testing).In our latest write-up, we share:Why manual tuning doesn’t scale for dynamic workloadsHow benchmarking actual CPU/memory under realistic load helps predict good limitsHow to feed those results back into Kubernetes manifestsSome gotchas around autoscaling & metrics pipelinesCurious if anyone here has tried a similar “shift-left” approach for resource optimization or integrated benchmarking into their pipelines and how that worked out.]]></content:encoded></item><item><title>Steinberg, creators of VST technology and the ASIO protocol, have released the SDKs for VST 3 and ASIO as Open Source.</title><link>https://www.reddit.com/r/linux/comments/1ola786/steinberg_creators_of_vst_technology_and_the_asio/</link><author>/u/fenix0000000</author><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:05:42 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/fenix0000000 ]]></content:encoded></item><item><title>[D] ArXiv CS to stop accepting Literature Reviews/Surveys and Position Papers without peer-review.</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>/u/NamerNotLiteral</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 23:04:18 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[GoGreement] A new linter that can help enforce interface implementation and immutability</title><link>https://www.reddit.com/r/golang/comments/1ol8das/gogreement_a_new_linter_that_can_help_enforce/</link><author>/u/Green-Sympathy-2198</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 22:38:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey guys! I wrote this linter mainly for myself, but I hope some of you find it useful.I came to golang from JVM world and I was missing some things like explicit implementation declaration and immutability.But I see gophers love their linters, so I thought I could solve this with a linter.How does it work? You just add annotations to your types like: go // @immutable type User struct { id string name string } And run the linter and it will give you an error if you try to change fields like this: I also added annotations that let you check interface implementation: go // @implements io.Reader This lets you check that a struct actually implements an interface without all this stuff: go var _ MyInterface = (*MyStruct)(nil) And many other annotations (testonly, packageonly, ...). Would love to hear what you think!]]></content:encoded></item><item><title>Borrow checker says “No”! An error that scares me every single time!</title><link>https://polymonster.co.uk/blog/borow-checker-says-no</link><author>/u/__shufb</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 22:25:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[It’s Halloween and I have just been caught out by a spooky borrow checker error that caught me by surprise. It feels as though it is the single most time consuming issue to fix and always seems to catch me unaware. The issue in particular is “cannot borrow x immutably as it is already borrowed mutably” - it manifests itself in different ways under different circumstances, but I find myself hitting it often when refactoring. It happened again recently so I did some investigating and thought I would discuss it in more detail.The issue last hit me when I was refactoring some code in my graphics engine hotline, I have been creating some content on YouTube and, after a little bit of a slog to fix the issue, I recorded a video of me going through the scenario of how it occurred and some patterns to use that I have adopted in the past to get around it. You can check out the video if you are that way inclined, the rest of this post will mostly echo what is in the video, but it might be a bit easier to follow code snippets and description in text.I have a generic graphics API, which consists of traits called gfx. This is there to allow different platform backends to implement the trait; currently I have a fully implemented Direct3D12 backend and I recently began to port macOS using Metal.The gfx backend wraps underlying graphics API primitives; in this case we are mostly concerned about  which is a command buffer. Command buffers are used to submit commands to the GPU. They do things like  or , amongst other things. For the purposes of this blog post, what the command buffer does is not really that important, just that is does , which at the starting point when the code was working is a trait method that takes an immutable self and another immutable parameter ie. fn do_something(&self, param: &Param).In the rest of the code base I have a higher level rendering system called . This is graphics engine code that is not platform specific but implements shared functionality. So where  is a low level abstraction layer,  implements concepts of a  that is a view of a scene that we can render from. A  has a camera that can look at the scene and is then passed to a render function, which can build a command buffer to render the scene from that camera’s perspective. The engine is designed to be multithreaded and render functions are dispatched through  systems, so a view gets passed into a render system but it is wrapped in an .I made a small cutdown example of this code to be able to demonstrate the problem I encounter, so let’s start with the initial working version:I tried to simplify it as much as possible so these snippets should compile if you copy and paste them, they won’t run thanks to  macro (which I absolutely love using, it is so handy!) but we only care about the borrow checker anyway.All we really need to think about is that a  can  and it also gets passed in a , which is also contained as part of ‘view’. Coming from a C/C++ background I landed on my personal preference being procedural C code with context passing, so I tend to group things together into a single struct. It makes sense to me in this case and I wanted to group everything inside , and we fetch the view from elsewhere in the engine.So the code in the snippet compiles fine and I was working with this setup for some time. I began work on macOS and it turned out that the  method needed to mutate the command buffer so that I could mutate some internal state and make the Metal graphics API behave similarly to Direct3D12. This is common for graphics API plumbing.The specific example in this case was that in Direct3D we call a function  to bind an index buffer before we make a call to , but in Metal there is no equivalent to bind an index buffer. Instead you pass a pointer to your index buffer when calling the equivalent draw indexed. So to fix this, when we call  we can store some extra state in the command buffer so we can pass it in the later call to .In hindsight any method on the command buffer trait that does anything, like set anything or write into the command buffer, should take a  because it is mutating the command buffer after all. In my case since I am calling through to methods on  , which is unsafe code and does not require any mutable references.In our simplified example, in order to store, state  now needs to change and take a mutable self: do_something(&mut self, param: &Param) it should be noted that  itself was already .Borrow checker now kicks in…my heart sinks. In the real code base not only did I have to modify a single call site, but I had hundreds of places where this error was happening, I made the decision here and now to make any methods that write to the command buffer also be mutable and make the mutabilityerror[E0502]: cannot borrow `view` as immutable because it is also borrowed as mutable
  --> src/main.rs:30:28
   |
30 |     view.cmd.do_something(&view.param);
   |     ----     ------------  ^^^^ immutable borrow occurs here
   |     |        |
   |     |        mutable borrow later used by call
   |     mutable borrow occurs here

For more information about this error, try `rustc --explain E0502`.
error: could not compile due to 1 previous error
This is not the first time I have encountered this problem and I doubt it will be the last. There are a number of ways to resolve it and they aren’t too complicated. The frustrating thing is that it seems to occur always when you are doing something else and not just when you decide to refactor, so you end up having a mountain of errors to solve before you can get back to the original task. I suppose you could call it a symptom of bad design or lack of experience, but when writing code things inevitably change and bend with new requirements, and Rust throws these unexpected issues up for me more often than I find with C, and often the required refactor takes more effort as well. But that is the cost you pay, hopefully more upfront effort to get past the borrow checker means fewer nasty debugging stages later. So let’s look at some patterns to fix the issue!The one I actually went for in this case was using . We take the  out of view so we no longer need to borrow a ‘view’ to use , and then when finished return the cmd into ‘view’. It is important to note here that  needs to derive default in order for this to work, as when we take the  in  will become This approach is the simplest I could think of at the time because any existing code using  doesn’t need updating, everything stays the same and we just separate the references. In this case it was easy to derive the default for  .You need to remember to set the  back on  here, which could be a pitfall and cause unexpected behaviour if you didn’t.If you can’t easily derive default on a struct there are some other options. If the struct is clonable or you can easily derive a clone, you can clone to achieve a similar effect.Cloning might be considered a heavier operation than ‘take’ depending on the circumstances, but this method has the same benefit as the take version whereby unaffected code that is using  elsewhere doesn’t need to be changed.Another approach would be to use  this allows for interior mutability and again we do not need to worry about default or clone.We also need to update any code that ever used  and do the same. Not ideal but it allows us to get around the need for a default or clone. I have had to resort to this in other places in the code base.There are more options; quite literally  here can help. If we make  an  then this gives us the ability to use  as the default and we can use the  approach. We can also use  and swap with . Swapping works similar to ‘take’, where we take mem and swap with the default.The  approach also requires more effort as we need to now take a reference and unwrap the option and update any code that ever used  to do the same. Not ideal, but it allows us to get around the need for a default or clone, and if your type is already optional then this will fit easily.There is one final approach that could save a lot of time, and that would be to not change the  function at all in the first place. That is to keep it as do_something(&self, param: &Param). So how do we mutate the interior state without requiring the self to be mutable?This can be done with  in single threaded code or  in multithreaded code. Since we already looked at  I will do an example of .I decided to make the mutability explicit to the trait and that was based on how the command buffers are used in the engine, in other places I have taken other approaches favouring interior mutability. For this case a view can be dispatched in parallel with other views, but the engine is designed such that 1 thread per view and no work happens to a single view on multiple threads at the same time. Command buffers are submitted in a queue in order and dispatched on the GPU.Here it made sense to me to avoid locking interior mutability for each time we call a method on a  and it works with the engine’s design. We lock a view at the start of a render thread, fill it with commands and then hand it back to the graphics engineer for submission to the GPU. The usage is explicit, we just needed to appease the borrow checker!I hope you enjoyed this article, please check out my YouTube channel for more videos or more articles on my blog, let me know what you think and if you have any other strategies or approaches I would love to hear about them. I would also like to hear about compiler and borrow checker errors you find particularly time consuming or frustrating to deal with.]]></content:encoded></item><item><title>What is wrong with this setup?</title><link>https://www.reddit.com/r/kubernetes/comments/1ol7n6g/what_is_wrong_with_this_setup/</link><author>/u/Low_Opening3670</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 22:05:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I needed Grafana Server for more than 500+ people to use and create dashboards on it...I have one Grafana on EKS, I spin up everything using Terraform even wrap a k8s manifest in Terraform and deploy it to cluster. There is not much change in Grafana application maybe every 6 months new stable version is out and I am going to do the upgradeWhat is wrong with this setup? and how I can improve it? do I really need flux/argo here? ]]></content:encoded></item><item><title>What kind of debug tools are available that are cloud native?</title><link>https://www.reddit.com/r/kubernetes/comments/1ol64df/what_kind_of_debug_tools_are_available_that_are/</link><author>/u/lickety-split1800</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:59:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I'm an SRE and a longtime Linux & automation person, starting in the late 90s.With the advent of apps on containers, there are fewer and fewer tools to perform debugging.Taking a look at the types of debug tools one has used to diagnose issues.even basic tools such as find, grep, ls and others are used in debugging.The Linux OS used to be under the control of the system administrator, who would put the tools required to meet operational debugging requirements, increasingly since it is the developer that maintains the container image and none of these tools end up on the image, citing most of the time startup time as the main requirement.Now a container is a slice of the operating system so I argue that the container base image should still be maintained by those who maintain Linux, because it's their role to have these tools to diagnose issues. That should be DevOps/SRE teams but many organisations don't see it this way.So what tools does Kubernetes provide that fulfil the needs I've listed above?]]></content:encoded></item><item><title>Companies are trying to do too much with AI, says IT CEO | Fortune</title><link>https://fortune.com/2025/10/31/scaling-ai-mit-study-roi/</link><author>/u/fortune</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:44:15 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Scaling gen AI projects beyond the pilot phase is fundamental to turning the current AI hype cycle into real ROI. How can companies get over the hump? For starters, they should stop trying to introduce AI to every facet of operations, says Abhijit Dubey, CEO of NTT Data, an IT services and consulting company. 



“What happens is companies say, ‘In every single domain, I’m going to unleash innovation, and I’m going to have AI enablement.’ I think that’s the wrong strategy,” Dubey said at the Fortune Global Forum in Riyadh on Sunday. The right strategy, he noted, is to “pick one or two domains that are going to create disproportionate economic value for the company and go end to end.” He gave the example of focusing on underwriting in insurance and supply chains in manufacturing.



FedEx has been intentional about integrating AI into three broad areas: internal operations, customer experience, and creating new value levers for customers (such as improving demand forecasting and reducing returns), said Kami Viswanathan, FedEx’s president of the Middle East, Indian subcontinent, and Africa region. “Research has shown that organizations that have a clear AI strategy, which has this prioritization, have a much greater degree of success than others that don’t, right? So for us, that’s the key aspect of scaling.”



Deploying AI across an organization comes with risks and requires adequate safeguards, said Fabio Kuhn, CEO of Vortexa, a cargo tracking and energy market research firm whose customers can query its data via chatbot. Human supervision is essential to limiting hallucinations and keeping any that do slip through from shaping decision-making, he said. “In addition to speed and quality, what is … increasingly important on any model, any agent, is explainability. For a human to be able to understand, why is it making the decision that it is making?” he said. Keeping a human in the loop is essential when AI is deployed in health care, said Noosheen Hashemi, cofounder and CEO of January AI, a precision health care company. “These LLMs [large language models], yes, they have read everything, but they do hallucinate, and they do it confidently. When there is data missing, they will invent it,” she said, noting that January AI’s Mirror tool has a hallucination rate under 1%. “We have doctors actually looking at results and saying, ‘Does this actually make sense?’”



Health care also has its own unique hurdles to scaling AI, especially in the U.S.: Data silos spread across patients, insurers, providers, and labs. The cost—in terms of potential—is enormous. “Not having a unified view of data doesn’t really allow us to leverage AI in the best way that we can,” Hashemi says. “We have the technology today, absolutely, to eradicate lifestyle-based chronic diseases in the world. The question is, to what extent do we have the will to actually apply this technology and to overcome data silos, regulatory issues, privacy issues, to actually deploy AI, because the technology is here.”
]]></content:encoded></item><item><title>My Must-Have Apps Since Switching to Linux</title><link>https://www.reddit.com/r/linux/comments/1ol5a1k/my_musthave_apps_since_switching_to_linux/</link><author>/u/Overflow_Nuts</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:24:34 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[OnlyOffice → If you’re used to MS Office, the interface feels almost identical — super easy to adapt.Brave / Zen → When I need a Chromium-based browser, I use Brave; when I need a Firefox-based one, Zen. Both are top-tier.Okular → Opens everything from PDFs to EPUBs.yt-dlp → Downloads videos and audio straight from the terminal — and not just from YouTube, it supports tons of platforms.Qbittorrent → Clean, simple, and easily the best torrent client out there.Stremio + Add-ons → The best torrent-based media player, hands down.KeepassXC → A simple yet powerful password manager with browser integration.LocalSend → Transfers files across all your devices locally, no internet needed.KDE Connect → Perfect bridge between your phone and computer.Bottles → Makes using Wine more stable and user-friendly.Espanso → Expands text shortcuts automatically — a real time-saver.Tmux → Lets you split your terminal and run multiple sessions at once.Btop / ytop / glances → Displays system resource usage right from the terminal.Fastfetch → A faster Neofetch alternative for system info.Syncthing → Syncs your files seamlessly between devices.Czkawka → Finds duplicate or junk files on your disk.Mpv + Plugins → Lightweight, scriptable video player.Input Leap → Control multiple computers with one keyboard and mouse.Zapret → Bypasses DPI-based network restrictions.Moonlight / Sunshine → Stream your games locally across your network.Heroic Games Launcher → Great alternative for Epic Games.Lutris → Customizable launcher supporting multiple game libraries.Prism Launcher → Clean, mod- and shader-friendly Minecraft launcher.Ente Auth → The best 2FA app I’ve tried — encrypted sync between devices.GDU → Visual disk usage analyzer.Newsboat → Read RSS feeds directly in the terminal.Neovim → Fast, lightweight text editor.Waypaper / Swaybg / Hyprpaper → Manage your wallpapers easily.Easy Effects → Lets you tweak and filter your system’s audio.Waybar (+ eww + rofi) → Build a fully customizable system bar.scrcpy → The simplest way to mirror your Android screen on your PC.Podman / Distrobox → Run another Linux environment inside a container.Wireshark / mitmproxy → Monitor and analyze your network traffic.Opensnitch → See which apps are making network connections.qutebrowser → A minimalist, keyboard-driven browser.fail2ban → The most satisfying way to troll persistent brute-forcers.qemu + Virt-Manager → Create and manage virtual machines easily.Waydroid → Run Android apps directly on Linux.Lf → Terminal-based file manager.These are the tools I’ve discovered and personally enjoy using on Linux. What about yours what are your must-have apps?]]></content:encoded></item><item><title>Futurelock - Subtle Risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>/u/-Y0-</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:20:15 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>/u/iamkeyur</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 19:26:29 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Meta, xAI Starting Trend for Billions in Off-Balance Sheet Debt</title><link>https://www.bloomberg.com/news/articles/2025-10-31/meta-xai-starting-trend-for-billions-in-off-balance-sheet-debt</link><author>/u/esporx</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 19:18:42 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Write PostgreSQL functions in Go Golang example</title><link>https://www.reddit.com/r/golang/comments/1ol2tqv/write_postgresql_functions_in_go_golang_example/</link><author>/u/WinProfessional4958</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 18:45:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It took me a while to figure this out. Go compiles the C files automatically.#include "postgres.h" #include "fmgr.h" PG_MODULE_MAGIC; extern int32 Adder(int32); PG_FUNCTION_INFO_V1(add_two); Datum add_two(PG_FUNCTION_ARGS) { int32 arg = PG_GETARG_INT32(0); PG_RETURN_INT32(Adder(arg)); } package main /* #cgo CFLAGS: -DWIN32 -ID:/pg18headers -ID:/pg18headers/port/win32 #cgo LDFLAGS: -LD:/pg18lib #include "postgres.h" #include "fmgr.h" // Forward declare the C function so cgo compiles add_two.c too. extern void init_add_two(); */ import "C" //export Adder func Adder(a C.int32) C.int32 { return a + 3 } func main() {} PS D:\C\myextension> go build -o add_two.dll -buildmode=c-sharedIn PostgreSQL: open the query window (adjust path to your generated dynamically loaded library and header file (.dll, .h).CREATE FUNCTION add_two(int4) RETURNS int4AS 'D:/C/myextension/add_two.dll', 'add_two']]></content:encoded></item><item><title>Go vs Kotlin: Server throughput</title><link>https://www.reddit.com/r/golang/comments/1ol1upp/go_vs_kotlin_server_throughput/</link><author>/u/iG0tB00ts</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 18:08:15 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Let me start off by saying I'm a big fan of Go. Go is my side love while Kotlin is my official (work-enforced) love. I recognize benchmarks do not translate to real world performance & I also acknowledge this is the first benchmark I've made, so mistakes are possible.That being said, I was recently tasked with evaluating Kotlin vs Go for a small service we're building. This service is a wrapper around Redis providing a REST API for checking the existence of a key.With a load of 30,000 RPS in mind, I ran a benchmark using  (the workload is a list of newline separated 40chars string) and saw to my surprise Kotlin outperforming Go by ~35% RPS. Surprise because my thoughts, few online searches as well as AI prompts led me to believe Go would be the winner due to its lightweight and performant goroutines.Go + net/http + go-redis Text Thread Stats Avg Stdev Max +/- Stdev Latency 4.82ms 810.59us 38.38ms 97.05% Req/Sec 5.22k 449.62 10.29k 95.57% 105459 requests in 5.08s, 7.90MB read Non-2xx or 3xx responses: 53529 Requests/sec: 20767.19  Kotlin + ktor + lettuce  Thread Stats Avg Stdev Max +/- Stdev Latency 3.63ms 1.66ms 52.25ms 97.24% Req/Sec 7.05k 0.94k 13.07k 92.65% 143105 requests in 5.10s, 5.67MB read Non-2xx or 3xx responses: 72138 Requests/sec: 28057.91 I am in no way an expert with the Go ecosystem, so I was wondering if anyone had an explanation for the results or suggestions on improving my Go code. ```Go package mainimport ( "context" "net/http" "runtime" "time""github.com/redis/go-redis/v9" var ( redisClient *redis.Client )func main() { redisClient = redis.NewClient(&redis.Options{ Addr: "localhost:6379", Password: "", DB: 0, PoolSize: runtime.NumCPU() * 10, MinIdleConns: runtime.NumCPU() * 2, MaxRetries: 1, PoolTimeout: 2 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer redisClient.Close()mux := http.NewServeMux() mux.HandleFunc("/", handleKey) server := &http.Server{ Addr: ":8080", Handler: mux, } server.ListenAndServe() // some code for quitting on exit signal // handleKey handles GET requests to /{key} func handleKey(w http.ResponseWriter, r *http.Request) { path := r.URL.Pathkey := path[1:] exists, _ := redisClient.Exists(context.Background(), key).Result() if exists == 0 { w.WriteHeader(http.StatusNotFound) return } Kotlin code for reference ```Kotlin // applicationfun main(args: Array<String>) { io.ktor.server.netty.EngineMain.main(args) }fun Application.module() { val redis = RedisClient.create("redis://localhost/"); val conn = redis.connect() configureRouting(conn) }fun Application.configureRouting(connection: StatefulRedisConnection<String, String>) { val api = connection.async()routing { get("/{key}") { val key = call.parameters["key"]!! val exists = api.exists(key).await() > 0 if (exists) { call.respond(HttpStatusCode.OK) } else { call.respond(HttpStatusCode.NotFound) } } } ]]></content:encoded></item><item><title>Is there anything like the surface pro and go that fully supports linux?</title><link>https://www.reddit.com/r/linux/comments/1ol1ekb/is_there_anything_like_the_surface_pro_and_go/</link><author>/u/ijwgwh</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 17:51:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Can't stand Windows, but my surface devices are amazing hardware-wise. Surface linux has come a long way, but not having cameras is a deal-breaker for me. Is there any hardware slim sleek and powerful that fully supports Linux? Looking for tablet style, not those laptops where the keyboard turns all the way around. ETA: looking for X86 I5+ or equivalent]]></content:encoded></item><item><title>html/template: Why does it escape opening angle bracket?</title><link>https://www.reddit.com/r/golang/comments/1ol1cyi/htmltemplate_why_does_it_escape_opening_angle/</link><author>/u/cvilsmeier</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 17:49:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, html/template escapes input data, but why does it escape an angle bracket character ("<") in the template? Here is an example:package main import ( "fmt" "html/template" "strings" ) func main() { text := "<{{.tag}}>" tp := template.Must(template.New("sample").Parse(text)) var buf strings.Builder template.Must(nil, tp.Execute(&buf, map[string]any{"tag": template.HTML("p")})) fmt.Println(buf.String()) // Expected output: <p> // Actual output: &lt;p> }    submitted by    /u/cvilsmeier ]]></content:encoded></item><item><title>AI has made Google more profitable when people expected the contrary</title><link>https://peakd.com/@malopie/ai-has-made-google-more-profitable-when-people-expected-the-contrary-nn</link><author>/u/renkure</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 17:24:26 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>C3 0.7.7 Vector ABI changes, RISC-V improvements and more</title><link>https://c3-lang.org/blog/c3-language-at-0-7-7-vector-abi,-riscv-improvements-and-more/</link><author>/u/Nuoji</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 16:37:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[0.7.7 is a major advance in C3 usability with vector ABI changes. It also contains several small quality-of-life additions, such as the ability to splat structs into an initializer, and implicit subscript dereferencing. Fairly few bugs were discovered during this development cycle, which is why the fixed bugs are unusually low.Let’s look at what 0.7.7 brings in more detail:The most significant change in this release is the ABI change for vectors, which now store and pass vectors as arrays in function calls and structs. While vectors still use SIMD, their equality to arrays on the ABI level means that C graphical libraries will directly match vector types.Where before you needed to work with C structs defining vectors and then converting them to SIMD vectors for actual computation, it now works out of the box. Another problem with vectors prior to 0.7.7 was their space and alignment requirements over structs. From 0.7.7 alignment matches that of structs and arrays, making them extremely convenient to work with.For cases where SIMD vectors are actually expected, it’s possible to create distinct types using  with a new  attribute to exactly match standard C SIMD vectors, e.g. typedef V4si = int[<4>] @simd;. This then exactly matches the corresponding C SIMD type.This makes it easier than ever to use SIMD with C3.Struct initializer splatsThis feature enables using the splat operator  to give a designated initializer default values that are overridden by the following arguments.When passing arrays or lists by reference, the  operator tend to behave in an undesirable way, dereferencing the pointer instead of the underlying array/list:Subscript deref addresses this. Using  will dereference :This is helpful when writing macros and such that will want to accept both elements by reference and by value:A new feature for  is to allow creating a type with a specific alignment without wrapping it in a struct. We may, for example, create an integer that is 16 bit aligned using typedef Int2 = int @align(2);. This is an alternative way to safely work with references to under-aligned members in packed structs., ,  and  macros are added to modify strings at compile time efficiently for certain macro manipulation at compile time.Small but important changesAliases that refer to  variables must themselves have local visibility.  is renamed  as it was frequently misunderstood. Generic inference now works better in initializers. For slices with the  syntax, it’s now possible to have the end index be one less than the starting index, so that zero size slices can be expressed with the  syntax as well.This release significantly strengthens C3C’s cross-platform capabilities, particularly for RISC-V architecture support. It’s now possible to set individual CPU features using , e.g. . For RISC-V,  has been added, as well as renaming the RISC-V abi flag to the more correct .The sorting macros accidentally only took non-slices by value, which would work in some cases but not in others. This has been fixed, but might mean that some code needs to update as well. TcpSocketPair was added to the tcp module to create a bidirectional local socket pair, and using sockets on Windows should now implicitly initialize the underlying socket subsystem.0.7.7 has only about 11 fixes, which reflects the relatively few bugs encountered in the 0.7.7 cycle. There are outstanding bugs on the inline asm, which has a significant update planned. The most important fix is patching a regression for MacOS which prevented backtrace printing.With the updated Vector ABI and the change from  to  there are a lot of vendor libraries that will need a refresh. There is also a new matrix library in development that hopefully might get included in the next release. There is more functionality to add for fine-tuning processor capabilities for both RISC-V, but also AArch64. There have also been requests for 32-bit Arm support, but the lack of CI tests for different Arm processors is blocking it at the moment.This release wouldn’t have been possible without the C3 community. I’d like to extend a deep thank you to all who have contributed, both through filed issues, PRs and just plain discussions.Have questions? Come and chat with us on Discord.Discuss this article on Reddit.]]></content:encoded></item><item><title>Project goals for 2025H2 | Rust Blog</title><link>https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/</link><author>/u/Kobzol</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 16:23:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[On Sep 9, we merged RFC 3849, declaring our goals for the "second half" of 2025H2 -- well, the last 3 months, at least, since "yours truly" ran a bit behind getting the goals program organized.In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .Unblocking dormant traits, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.Flexible, fast(er) compilation, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers., making higher-level usage patterns in Rust easier.One of Rust's core value propositions is that it's a "library-based language"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a "second-class citizen" problem where custom pointer types can't provide the same ergonomic experience as built-in references.The "Beyond the " initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.
"Unblocking dormant traits"Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:Polonius will enable new borrowing patterns, and in particular unblock "lending iterators". Over the last few goal periods, we have identified an "alpha" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.The next-generation trait solver is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.The work on evolving trait hierarchies will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.The work to expand Rust's  hierarchy will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on RFC #3729 and RFC #3838, authored in previous Project goal periods.In-place initialization allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.The "Flexible, fast(er) compilation" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often "surprisingly tolerable" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:We aim to stabilize cargo script, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.We aim to finalize the design of ergonomic ref-counting and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the Rust program manager hired by the Leadership Council, will join me in running the program.]]></content:encoded></item><item><title>I compiled my research on modern bot detection into a deep-dive on multi-layer fingerprinting (TLS/JA3, Canvas, Biometrics)</title><link>https://pydoll.tech/docs/deep-dive/fingerprinting/</link><author>/u/thalissonvs</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 16:02:58 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[This module covers browser and network fingerprinting, a critical aspect of modern web automation and detection systems.Fingerprinting sits at the intersection of network protocols, cryptography, browser internals, and behavioral analysis. It encompasses the techniques used to identify and track devices, browsers, and users across sessions without relying on traditional identifiers like cookies or IP addresses.Every browser connection to a website exposes multiple characteristics, from the precise order of TCP options in network packets, to GPU-specific canvas rendering, to JavaScript execution timing patterns. Individually, these characteristics may appear innocuous. Combined, they create a fingerprint that can uniquely identify a device or browser instance.For automation engineers, bot developers, and privacy-conscious users, understanding fingerprinting is essential for building effective detection evasion systems and understanding how tracking mechanisms operate at a technical level.Multi-Layer Detection SystemsModern anti-bot systems employ comprehensive analysis across multiple layers:: TCP/IP stack behavior, TLS handshake patterns, HTTP/2 settings: Canvas rendering, WebGL vendor strings, JavaScript property enumeration: Mouse movement entropy, keystroke timing, scroll patternsA single inconsistency (such as a Chrome User-Agent with Firefox TLS fingerprint) can trigger immediate blocking.Module Scope and MethodologyFingerprinting techniques are documented across multiple sources with varying levels of accessibility and reliability:Academic papers (often paywalled and theoretical)Browser source code (millions of lines to analyze)Security researcher blogs (technical but fragmented)Anti-bot vendor whitepapers (marketing-focused, details omitted)Underground forums (practical but unreliable)This module centralizes, validates, and organizes this knowledge into a cohesive technical guide. Every technique described here has been: against browser source code and RFCs in real automation scenarios with authoritative references from first principles to implementation  This module is organized into three progressive layers, from network fundamentals to practical evasion techniques:1. Network-Level FingerprintingCovers device identification through network behavior at the transport and session layers, before browser rendering begins.: TTL, window size, option ordering: JA3/JA4, cipher suites, ALPN negotiation: SETTINGS frames, priority patterns: p0f, Nmap, Scapy, tshark analysis: Network fingerprints are the most challenging to spoof because they require OS-level modifications. Inconsistencies at this layer are detected before JavaScript execution begins.2. Browser-Level FingerprintingExamines browser identification through JavaScript APIs, rendering engines, and plugin ecosystems at the application layer.Canvas & WebGL fingerprinting: GPU-specific rendering artifacts: Subtle differences in audio API output: Installed fonts reveal OS and locale: Navigator object, screen dimensions, timezone: Accept-Language, User-Agent consistency: This layer accounts for the majority of detection events. Even with correct network-level fingerprints, exposed automation properties (e.g., ) can trigger blocking.3. Behavioral FingerprintingAnalyzes user interaction patterns to distinguish human behavior from automated systems.: Trajectory curvature, velocity profiles, Fitts's Law compliance: Typing rhythm, dwell time, flight time, bigram patterns: Momentum, inertia, deceleration curves: Natural interaction ordering (mousemove → click), timing analysis: ML models trained on billions of behavioral signals: Behavioral analysis can detect automation even when network and browser fingerprints are correctly spoofed. This layer is particularly challenging because it requires replicating biomechanical human behavior patterns.Practical implementation of fingerprinting evasion using Pydoll's CDP integration, JavaScript overrides, and architectural features.: Timezone, geolocation, device metricsJavaScript property overrides: Redefining navigator objects, canvas poisoning: Forcing header consistency: Human-like timing, entropy injection: Tools to validate your evasion setup: This section demonstrates practical application of fingerprinting concepts to real automation scenarios, integrating techniques from all previous layers.You MUST read this if you're:Building automation that interacts with anti-bot protected sitesDeveloping scraping infrastructure at scaleImplementing privacy-preserving browser automationResearching bot detection for offensive or defensive purposesThis is advanced material if you're:A "silver bullet" anti-detection solution (no such thing exists)A replacement for respecting robots.txt and rate limitsFingerprinting defense is not about becoming invisible—it's about becoming indistinguishable from legitimate traffic. This means:Consistency over perfection: A perfectly configured Firefox fingerprint is better than a "perfect" but inconsistent Chrome fingerprint: You must align network, browser, and behavioral layers: Fingerprinting techniques evolve monthly; this is a living documentEvery layer must tell the same story. If your TLS fingerprint says "Chrome 120", your HTTP/2 settings must match Chrome 120, your User-Agent must say Chrome 120, and your canvas rendering must produce Chrome 120 artifacts. One mismatch = detection.Fingerprinting knowledge is :: Protect your privacy from invasive tracking: Evade detection systems for automationWe trust you to use this knowledge responsibly and ethically:Respect website terms of serviceImplement rate limiting and respectful crawling patternsEvaluate whether automation is necessaryBe transparent when appropriateFraud, account abuse, or illegal activitiesOverwhelming servers with aggressive scrapingWeaponizing this knowledge without understanding consequences  Fingerprinting is a complex and technical domain that requires systematic study. Understanding these techniques is essential for effective web automation in environments with detection systems.This module represents  combining academic papers, browser source code, real-world testing, and community knowledge. Every claim is cited and validated. If you find inaccuracies or have updates, contributions are welcome.Before diving in, consider these complementary topics:]]></content:encoded></item><item><title>mariadb-operator 📦 25.10 is out: asynchronous replication goes GA, featuring automated replica recovery! 🎃</title><link>https://github.com/mariadb-operator/mariadb-operator/releases/tag/25.10.2</link><author>/u/mmontes11</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 15:57:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We are thrilled to announce that our highly available topology based on MariaDB native replication is now generally available, providing an alternative to our existing synchronous multi-master topology based on Galera.In this topology, a single primary server handles all write operations, while one or more replicas replicate data from the primary and can serve read requests. More precisely, the primary has a binary log and the replicas asynchronously replicate the binary log events over the network.Getting a replication cluster up and running is as easy as applying the following  resource:apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: storage: size: 1Gi storageClassName: rook-ceph replicas: 3 replication: enabled: true The operator provisions a replication cluster with one primary and two replicas. It automatically sets up replication, configures the replication user, and continuously monitors the replication status. This status is used internally for cluster reconciliation and can also be inspected through the  subresource for troubleshooting purposes.Whenever the primary Pod goes down, a reconciliation event is triggered on the operator's side, and by default, it will initiate a primary failover operation to the furthest advanced replica. This can be controlled by the following settings:apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: replicas: 3 replication: enabled: true primary: autoFailover: true autoFailoverDelay: 0s In this situation, the following status will be reported in the  CR:kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl False Switching primary to 'mariadb-repl-1' mariadb-repl-0 ReplicasFirstPrimaryLast 2m7s kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl True Running mariadb-repl-1 ReplicasFirstPrimaryLast 2m42s To select a new primary, the operator evaluates each candidate based on Pod readiness and replication status, ensuring that the chosen replica has no pending relay log events (i.e. all binary log events have been applied) before promotion.One of the spookiest 🎃 aspects of asynchronous replication is when replicas enter an error state under certain conditions. For example, if the primary purges its binary logs and the replicas are restarted, the binary log events requested by a replica at startup may no longer exist on the primary, causing the replica’s I/O thread to fail with error code .Luckily enough, this operator has you covered! It automatically detects this situation and triggers a recovery procedure to bring replicas back to a healthy state. To do so, it schedules a  from a ready replica and restores it into the data directory of the faulty one.The  object, introduced in previous releases, supports taking consistent, point-in-time volume snapshots by leveraging the  API. In this release, we’re eating our own dog food: our internal operations, such as replica recovery, are powered by the  construct. This abstraction not only streamlines our internal operations but also provides flexibility to adopt alternative backup strategies, such as using  (MariaDB native) instead of  (Kubernetes native).To set up replica recovery, you need to define a  template that the operator will use to create the actual  object during recovery events. Then, it needs to be configured as a source of restoration inside the replication section:apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: storage: size: 1Gi storageClassName: rook-ceph replicas: 3 replication: enabled: true primary: autoFailover: true autoFailoverDelay: 0s replica: bootstrapFrom: physicalBackupTemplateRef: name: physicalbackup-tpl recovery: enabled: true errorDurationThreshold: 5m --- apiVersion: k8s.mariadb.com/v1alpha1 kind: PhysicalBackup metadata: name: physicalbackup-tpl spec: mariaDbRef: name: mariadb-repl schedule: suspend: true storage: volumeSnapshot: volumeSnapshotClassName: rook-ceph Let’s assume that the  replica enters an error state, with the I/O thread reporting error code :kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl False Recovering replicas mariadb-repl-1 ReplicasFirstPrimaryLast 11m kubectl get physicalbackup NAME COMPLETE STATUS MARIADB LAST SCHEDULED AGE ..replica-recovery True Success mariadb-repl 14s 14s kubectl get volumesnapshot NAME READYTOUSE SOURCEPVC SNAPSHOTCLASS AGE ..replica-recovery-20251031091818 true storage-mariadb-repl-2 rook-ceph 18s kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl True Running mariadb-repl-1 ReplicasFirstPrimaryLast 11m As you can see, the operator detected the error, triggered the recovery process and recovered the replica using a  taken in a ready replica, all in a matter of seconds! The actual recovery time may vary depending on your data volume and your CSI driver.Huge thanks to everyone who contributed to making this feature a reality, from writing code to sharing feedback and ideas. Thank you!]]></content:encoded></item><item><title>Horror Coding Stories: Therac-25 — A deadly race condition and overflow</title><link>https://read.thecoder.cafe/p/therac-25</link><author>/u/teivah</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 15:25:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Last updated: March 9, 2025Welcome to The Coder Cafe! Today, we examine the Therac-25 accidents, where design and software failures resulted in multiple radiation overdoses and deaths. Make sure to check the Explore Further section to see if you’re able to reproduce the deadly issue. Get cozy, grab a pumpkin spice latte, and let’s begin!Treating cancers used to require a mix of machines, depending on tumor depth: shallow or deep. In the early 1980s, a new generation promised both from a single system. That was a big deal for hospitals: one machine instead of several meant lower maintenance and fewer systems to manage.That was the case with the Therac-25.The Therac-25 offered two therapies with selectable modes:Earlier Therac models allowed switching modes with hardware circuits and physical interlocks. The new version was smaller, cheaper, and computer-controlled. Less hardware and fewer parts meant lower costs.However, what no one realized soon enough: it also removed an independent safety net.On a routine day, a radiology technologist sat at the console and began entering a plan:By habit, she selected X-ray (deep mode).Then she immediately corrected it for Electron (shallow mode) and hit start.The patient was receiving his ninth treatment. Immediately, he knew something was different. He reported a buzzing sound, later recognized as the accelerator pouring out radiation at maximum. The pain came fast; paralysis followed. He later died from radiation injury.Weeks later, a second patient endured the same incident on the same model.MODE:              X
...                ▮
BEAM READY:        MODE:              E
...                ▮
BEAM READY:        MODE:              E
...                
BEAM READY:        ▮From her perspective, the screen showed the corrected mode, so she hit return and started the treatment:MODE:              E
...                
BEAM READY:        ▮<Enter>Behind the scenes, the Therac-25 software ran several concurrent tasks:Because both tasks read the same memory with no mutual exclusion, there was a short window (on the order of seconds) in which the hardware-control task used a different value than the one displayed on the screen.The UI showed Electron mode, which looked correct to the operator.The hardware-control task had snapshotted stale data and marked the system as ready even though critical elements (e.g., turntable position, scanning magnets/accessories) were not yet aligned with electron mode.When treatment was started, the machine delivered an effectively unscanned, high-intensity electron beam, causing a massive overdose.race conditionThe manufacturer later confirmed the error could not be reproduced reliably in testing. The timing had to line up just right, which made the bug elusive. They initially misdiagnosed it as a hardware fault and applied only minor fixes. Unfortunately, the speed of operator editing was the key trigger that exposed this software race.The problem could have stopped here, but it didn’t.Months later, another fatal overdose occurred, this time caused by a different software defect. It wasn’t a timing race. This time, the issue was a counter overflow within the control program.The software used an internal counter to track how many times certain setup operations ran. After the counter exceeded its maximum value, it wrapped back to zero. That arithmetic overflow created a window where a critical safety check was bypassed, allowing the beam to turn on without the proper accessories in place.Again, the Therac-25 fired a high-intensity beam without the proper hardware configuration.Both the race condition and the counter overflow stemmed from the same design flaw: the belief that software alone could enforce safety. The Therac-25 showed, in tragic terms, that without independent safeguards, small coding errors can have catastrophic consequences. We should know that whether it’s software, hardware, or a human process, every single safeguard has inherent flaws. Therefore, in complex systems, safety should be layered, as illustrated by the Swiss cheese model:In total, there were six known radiation overdoses involving the Therac-25, and at least three were fatal.You can run the UI using Docker: docker run --rm -it -e TERM=xterm-256color teivah/therac-25If you enjoyed this post, please hit the like button.Any other horror coding stories you want to share?]]></content:encoded></item><item><title>[D] How to benchmark open-ended, real-world goal achievement by computer-using LLMs?</title><link>https://www.reddit.com/r/MachineLearning/comments/1okwuyx/d_how_to_benchmark_openended_realworld_goal/</link><author>/u/ExplorAI</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:58:30 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[GDPVal takes care of measuring agent performance on economically valuable tasks. We are working on the AI Village, where we try to see how we can explore, and possibly evaluate, how groups of persistent agents do at open-ended, real-world tasks in general. We're currently running all the frontier LLMs (OpenAI, Anthropic, DeepMind) with their own computer, internet access, and a group chat, and we give them goals like raising money for charity, organizing an event, or selling t-shirts online. We had the agents try to invent their own benchmark for themselves, but this led to them writing a lot of words, and doing almost no actions, but declaring themselves amazing at the benchmark. Gemini 2.5 Pro did manage to make something like a podcast and a "documentary" but these were pretty rudimentary attempts.I'm curious what ideas people here might have. Say you had a persistent multi-agent system, where each LLM is using a computer and trying to achieve goals: What goals would be interesting to give them? How would you compare the agents? What tools would you give them? What are the main things you'd be excited to explore?Some examples of insights we got so far, in case that helps kick-start conversation :)- Hallucinations and lack of situational awareness have hampered o3 a lot, resulting in it performing quite badly on goals that require real-world action. Meanwhile, it does really well on "talking" goals like winning the most debates during a formal debate season.- Computer use skills combined with temperament often lead Gemini 2.5 Pro to give up on achieving goals while other (sometimes less capable agents) keep working regardless. It seems to disproportionally assign its own errors (e.g. misclicks) to the environment and then decide it's all hopeless.- Document sharing is surprisingly hard, and so is playing online games. Meanwhile, they've made nice websites for themselves and do well on Twitter (if given an account and reminded of its existence). I'm not sure entirely sure why this pattern is emerging.]]></content:encoded></item><item><title>Fedora KDE appreciation</title><link>https://www.reddit.com/r/linux/comments/1okwubl/fedora_kde_appreciation/</link><author>/u/sukuiido</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:57:45 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I just wanted to express my appreciation for the team behind Fedora KDE. When I first installed this on my daily driver laptop, Fedora 41 was brand new. Still going fantastically after 2 point release updates. This distro has halted my distro-hopping for over a year now. It just works.™ Thank you, Fedora team.(Additional thanks to ycollet for the audinux copr repo. I make music and everything I need is there.)]]></content:encoded></item><item><title>Nvidia’s $5 Trillion Storyteller-In-Chief</title><link>https://go.forbes.com/3fK34B</link><author>/u/forbes</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:47:07 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A plaque that hangs above a Denny’s booth in San Jose, California, celebrates the birth of Nvidia, or a “$2 trillion company.” It’s only been two years since the plaque went up, and it’s already in need of an update. Nvidia, the chip maker that powers the AI revolution, has just crossed the remarkable milestone of becoming the first $5 trillion company. But while the company that took shape over coffee and pancakes at Denny’s is now worth considerably more than $2 trillion, the heart of the plaque’s message still resonates: “Who knew that an idea started here could change the world?”As it turns out, very few people knew that Nvidia started at Denny’s until someone with deep personal knowledge leaked the information—Nvidia CEO, Jensen Huang. The story Huang has shared publicly goes like this: At the age of 15, Huang started working at Denny’s as a dishwasher, busboy, and waiter. Years later, when Huang was an engineer in Silicon Valley, he and two friends would meet at a Denny’s location near Huang’s home, where the trio would brainstorm ideas for a startup. The booth even functioned as their first office space. “It had all the coffee you could drink, and no one would chase you out,” Huang said in an Nvidia blog post commemorating the event. For Huang, the Denny’s story is much more than a funny anecdote. It reflects who he is and the values that mean the most to him.  For leaders, the lesson is clear. As your company grows, the more crucial it becomes to share your origin story. A foundational, authentic story inspires others, communicates values, and builds an enduring culture. 4 Key Ingredients of a Powerful Origin StoryA good idea starts with a spark. Think back to the moment or experience that triggered the idea for your company. It might have been something you read, something you saw, something you witnessed. The spark might have been a problem you identified that needed a solution. For Brian Chesky and his roommates, the problem was finding money to pay the rent on this San Francisco apartment. They came up with the idea of renting out air mattresses for people attending a conference in the city. It helped pay the rent and sparked a much more valuable idea: Airbnb.Few people can relate to running a multitrillion-dollar company—the size of the audience is very small. But most of us can relate to getting a part-time job to earn some extra spending money or working at the lowest rung on the ladder, as Huang did, clearing tables and taking breakfast orders. Stories of struggle, hardship, and humble beginnings are often inspiring because we—the audience—can see ourselves in the leader’s footsteps. The stories give people hope that they, too, can overcome life’s challenges or, with the right attitude and mindset, can become what they imagine themselves to be.  When Huang shares his experience of working at Denny’s, his stories always come with lessons that reflect his values. Huang likes to boast that he was the best busboy the diner had ever seen and that “no one could carry more coffee cups.” Working at the restaurant taught Huang the importance of hard work, hospitality, and humility. “No task was beneath me,” he says. Innovation requires all three elements—working incredibly hard, satisfying the customer, and having the humility to admit what you don’t know.Huang uses the lessons he learned at Denny’s to explain Nvidia’s culture of cross-functional collaboration, an ethos that encourages managers to roll up their sleeves, get close to the team and their work, and solve problems together. Repetition and consistency. Don’t just tell the origin story once. Repeat it early and often. Consistency builds trust and authenticity with your partners, stakeholders, customers, and teams. When a leader consistently shares an origin story over time, it evolves from reflecting one person’s experience to becoming “our story." No matter how big Nvidia gets, Huang’s story is a reminder that its cultural norms and values come from the experience of a young busboy who refused to leave a station empty-handed. “Culture building is storytelling,” Huang said in an interview for . Leaders who articulate where they came from, and why it matters, don’t just build companies. They build cultures that endure. ]]></content:encoded></item><item><title>[P] I build a model to visualise live collision risk predictions for London from historical TFL data</title><link>https://www.reddit.com/r/MachineLearning/comments/1okwh3p/p_i_build_a_model_to_visualise_live_collision/</link><author>/u/AntiFunSpammer</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:43:18 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[ I built a small app that shows live collision risk across London. It learns patterns from historical TfL collision data and overlays risk on an interactive map. Open source, friendly to poke around, and I would love feedback.Spatiotemporal risk scoring for London using a fixed spatial grid (H3 hexes) and time contextInteractive map with a hotspot panel in the top rightA simple data exploration page and short notes on the modelI wanted a lightweight, transparent way to explore where and when collision risk trends higherMakes it easy to discuss what features help, what does not, and what is misleadingHistorical TfL collision recordsTime aligned context featuresOptional external context like OSM history and weather are supported in the pipelineTemporal features like hour of day and day of week with simple sine and cosine encodingsSpatial features on a hex grid to avoid leaking between nearby pointsOptional neighbor aggregates so each cell has local contextStart simple so it is easy to debug and explainTree based classifiers with probability calibration so the scores are usableFocus on clarity over squeezing the last bit of PR AUCClass imbalance is strong, so I look at PR curves, Brier score, and reliability curvesSpatial or group style cross validation to reduce leakage between nearby hex cellsStill iterating on split schemes, calibration, and uncertaintyBackend API that scores tiles for a selected time contextMap renders tile scores and lets you toggle hotspots from the panelFront end is a simple Leaflet app   submitted by    /u/AntiFunSpammer ]]></content:encoded></item><item><title>In your opinion which package is missing or could be better in Go?</title><link>https://www.reddit.com/r/golang/comments/1okvxbu/in_your_opinion_which_package_is_missing_or_could/</link><author>/u/fenugurod</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:21:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I know "just contribute to the ones already there" but I want to experiment a few things and build something from scratch in Go.If you miss something at the Go ecosystem, let me know because I'm really eager to build it.    submitted by    /u/fenugurod ]]></content:encoded></item><item><title>Are you drowning in AI code review noise? 70% of AI PR comments are useless</title><link>https://jetxu-llm.github.io/posts/low-noise-code-review/</link><author>/u/Jet_Xu</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 14:17:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Most AI code review tools generate 10-20 comments per PR. The problem? 80% are noise. Here's a framework for measuring signal-to-noise ratio in code reviews - and why it matters more than you think.   submitted by    /u/Jet_Xu ]]></content:encoded></item><item><title>Music player closest to modern Winamp UI&apos;s realtime queue system</title><link>https://www.reddit.com/r/linux/comments/1oku9zl/music_player_closest_to_modern_winamp_uis/</link><author>/u/Reddit_Zowie_Fan</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 13:14:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[In Modern Winamp UIs, whenever you play any track from the library the queue is immediately populated with whatever is in the library view on the left - your entire library, search results, etc - and there's a hotkey to quickly randomise the order of the queue, letting you shuffle your queue while actually seeing what tracks are coming up next, then move those tracks around or queue anything else you want to in the order you desire. After years and years of using Winamp I really struggle to adjust to not having this functionality. It seems to be missing from almost every music player I've tried on Linux thus far. I've tried a lot, and if anyone can suggest something that works this way I'd be very grateful. Gmusicbrowser is the closest I've found, but its age is showing - the version I downloaded off the AUR won't even launch on hyprland and the UI is much uglier than most other players.]]></content:encoded></item><item><title>Release Dioxus v0.7.0 · DioxusLabs/dioxus</title><link>https://github.com/DioxusLabs/dioxus/releases/tag/v0.7.0</link><author>/u/DebuggingPanda</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 13:10:05 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The indentation of switch statements really triggers my OCD — why does Go format them like that?</title><link>https://www.reddit.com/r/golang/comments/1oktsft/the_indentation_of_switch_statements_really/</link><author>/u/salvadorsru</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 12:54:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Help regarding the following code snippet</title><link>https://www.reddit.com/r/golang/comments/1oktgkq/help_regarding_the_following_code_snippet/</link><author>/u/Impossible-Act-5254</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 12:39:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[package main import ( "fmt" "time" ) func main() { ch := make(chan int, 2) ch <- 1 ch <- 2 fmt.Println("receiving from buffer") go func() { time.Sleep(2 * time.Second) fmt.Println("received ", <-ch) }() ch <- 3 } the given code sometimes prints :-receiving from buffer received 1and sometimes it prints :-   submitted by    /u/Impossible-Act-5254 ]]></content:encoded></item><item><title>AI browsers are a cybersecurity time bomb | Rushed releases, corruptible AI agents, and supercharged tracking make AI browsers home to a host of known and unknown cybersecurity risks.</title><link>https://www.theverge.com/report/810083/ai-browser-cybersecurity-problems</link><author>/u/MetaKnowing</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 12:06:29 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Web browsers are getting awfully chatty. They got even chattier last week after OpenAI and Microsoft kicked the AI browser race into high gear with ChatGPT Atlas and a “Copilot Mode” for Edge. They can answer questions, summarize pages, and even take actions on your behalf. The experience is far from seamless yet, but it hints at a more convenient, hands-off future where your browser does lots of your thinking for you. That future could also be a minefield of new vulnerabilities and data leaks, cybersecurity experts warn. The signs are already here, and researchers tell  the chaos is only just getting started.Atlas and Copilot Mode are part of a broader land grab to control the gateway to the internet and to bake AI directly into the browser itself. That push is transforming what were once standalone chatbots on separate pages or apps into the very platform you use to navigate the web. They’re not alone. Established players are also in the race, such as Google, which is integrating its Gemini AI model into Chrome; Opera, which launched Neon; and The Browser Company, with Dia. Startups are also keen to stake a claim, such as AI startup Perplexity — best known for its AI-powered search engine, which made its AI-powered browser Comet freely available to everyone in early October — and Sweden’s Strawberry, which is still in beta and actively going after “disappointed Atlas users.”In the past few weeks alone, researchers have uncovered vulnerabilities in Atlas allowing attackers to take advantage of ChatGPT’s “memory” to inject malicious code, grant themselves access privileges, or deploy malware. Flaws discovered in Comet could allow attackers to hijack the browser’s AI with hidden instructions. Perplexity, through a blog, and OpenAI’s chief information security officer, Dane Stuckey, acknowledged prompt injections as a big threat last week, though both described them as a “frontier” problem that has no firm solution.“Despite some heavy guardrails being in place, there is a vast attack surface,” says Hamed Haddadi, professor of human-centered systems at Imperial College London and chief scientist at web browser company Brave. And what we’re seeing is just the tip of the iceberg.With AI browsers, the threats are numerous. Foremost, they know far more about you and are “much more powerful than traditional browsers,” says Yash Vekaria, a computer science researcher at UC Davis. Even more than standard browsers, Vekaria says “there is an imminent risk from being tracked and profiled by the browser itself.” AI “memory” functions are designed to learn from everything a user does or shares, from browsing to emails to searches, as well as conversations with the built-in AI assistant. This means you’re probably sharing far more than you realise and the browser remembers it all. The result is “a more invasive profile than ever before,” Vekaria says. Hackers would quite like to get hold of that information, especially if coupled with stored credit card details and login credentials often found on browsers.Another threat is inherent to the rollout of any new technology. No matter how careful developers are, there will inevitably be weaknesses hackers can exploit. This could range from bugs and coding errors that accidentally reveal sensitive data to major security flaws that could let hackers gain access to your system. “It’s early days, so expect risky vulnerabilities to emerge,” says Lukasz Olejnik, an independent cybersecurity researcher and visiting senior research fellow at King’s College London. He points to the “early Office macro abuses, malicious browser extensions, and mobiles prior to [the] introduction of permissions” as examples of previous security issues linked to the rollout of new technologies. “Here we go again.”Some vulnerabilities are never found — sometimes leading to devastating zero-day attacks, named as there are zero days to fix the flaw — but thorough testing can slash the number of potential problems. With AI browsers, “the biggest immediate threat is the market rush,” Haddadi says. “These agentic browsers have not been thoroughly tested and validated.”But AI browsers’ defining feature, AI, is where the worst threats are brewing. The biggest challenge comes with AI agents that act on behalf of the user. Like humans, they’re capable of visiting suspect websites, clicking on dodgy links, and inputting sensitive information into places sensitive information shouldn’t go, but unlike some humans, they lack the learned common sense that helps keep us safe online. Agents can also be misled, even hijacked, for nefarious purposes. All it takes is the right instructions. So-called prompt injections can range from glaringly obvious to subtle, effectively hidden in plain sight in things like images, screenshots, form fields, emails and attachments, and even something as simple as white text on a white background.Worse yet, these attacks can be very difficult to anticipate and defend against. Automation means bad actors can try and try again until the agent does what they want, says Haddadi. “Interaction with agents allows endless ‘try and error’ configurations and explorations of methods to insert malicious prompts and commands.” There are simply far more chances for a hacker to break through when interacting with an agent, opening up a huge space for potential attacks. Shujun Li, a professor of cybersecurity at the University of Kent, says “zero-day vulnerabilities are exponentially increasing” as a result. Even worse: Li says as the flaw starts with an agent, detection will also be delayed, meaning potentially bigger breaches.It’s not hard to imagine what might be in store. Olejnik sees scenarios where attackers use hidden instructions to get AI browsers to send out personal data or steal purchased goods by changing the saved address on a shopping site. To make things worse, Vekaria warns it’s “relatively easy to pull off attacks” given the current state of AI browsers, even with safeguards in place. “Browser vendors have a lot of work to do in order to make them more safe, secure, and private for the end users,” he says.For some threats, experts say the only real way to keep safe using AI browsers is to simply avoid the marquee features entirely. Li suggests people save AI for “only when they absolutely need it” and know what they’re doing. Browsers should “operate in an AI-free mode by default,” he says. If you must use the AI agent features, Vekaria advises a degree of hand-holding. When setting a task, give the agent verified websites you know to be safe rather than letting it figure them out on its own. “It can end up suggesting and using a scam site,” he warns.]]></content:encoded></item><item><title>Where do ingress rules exist?</title><link>https://www.reddit.com/r/kubernetes/comments/1okskwc/where_do_ingress_rules_exist/</link><author>/u/SecureTaxi</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 11:58:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I played with a k8s POC a few years ago and dabbled with both the aws load balancer controller and an nginx and project contour one. For the latter i recall all the ingress rules were defined and viewed within the context of the ingress object. One of my guys deployed k8s for a new POC and managed to get everything running with the aws lb controller. However, all the rules were defined within the LB that shows up in the aws console. I think the difference is his is an ALB, whereas i had a NLB which route all traffic into the internal ingress (e.g. nginx). Which way scales better?Clarification: 70+ services with a lot of ruleset. Obviously i dont want a bunch of ALB to manage for each service]]></content:encoded></item><item><title>Bootstraps and directory structure question</title><link>https://www.reddit.com/r/kubernetes/comments/1okr75y/bootstraps_and_directory_structure_question/</link><author>/u/Altruistic_Cause8661</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 10:44:29 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>According to Red Hat, Xfce and Cinnamon are Linux distros</title><link>https://www.reddit.com/r/linux/comments/1okr534/according_to_red_hat_xfce_and_cinnamon_are_linux/</link><author>/u/VoidDuck</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 10:41:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[There are many Linux distros, including:Ubuntu (and all its versions: GNOME, Kubuntu—using KDE’s Plasma desktop, Ubuntu MATE, Xubuntu, and Lubuntu, to name a few)Linux distros vary widely in what they do, how they do it, and how they’re supported. Some are designed as Linux desktop environments―such as Xfce, Raspberry Pi OS, and Cinnamon―while others support back-end IT systems like enterprise or web servers.]]></content:encoded></item><item><title>How my Node.js code was causing a massive memory leak and how I solved it</title><link>https://medium.com/codetodeploy/de-mystifying-the-v8-garbage-collector-how-your-code-is-sabotaging-your-apps-memory-c290f80eb1d0?source=friends_link&amp;amp;sk=fc1c16b78a846500f40de8539dba7332</link><author>/u/Paper-Superb</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 10:21:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[How does garbage collection work? And how can you save your apps from memory leaks.The app runs fine. Until it doesn’t.No crashes, just subtle stutters. Memory usage on your dashboard creeps up like a slow leak in the hull of a ship. Three days later, “Out of Memory.” Boom.The common refrain for JavaScript has always been, “You don’t need to think about memory, the garbage collector handles it.” For a simple browser script, that’s mostly true. For a long-running Node.js server handling thousands of requests, this belief is a performance disaster.The V8 garbage collector (GC) is a marvel of engineering, but it’s not a magician. It makes assumptions based on how JavaScript  behaves. When we write code that violates those assumptions, we pay a heavy performance tax. The key to a fast, stable server isn’t to  garbage collection, but to write code that is empathetic to the GC, making its job fast, predictable, and brief.To understand the problem, you first need to know how V8 organizes memory. It doesn’t just dump everything into one giant heap. It divides memory into two main areas: the  and the .]]></content:encoded></item><item><title>Samsung and Nvidia to build an AI megafactory</title><link>https://siliconangle.com/2025/10/30/samsung-nvidia-build-ai-megafactory-transform-semiconductor-manufacturing/</link><author>/u/tekz</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 10:08:58 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[The company said it will deploy more than 50,000 of Nvidia’s most advanced graphics processing units in the new facility to embed artificial intelligence throughout its entire chip manufacturing flow. It’s also planning to leverage AI to help with chip development and design to facilitate a new generation of semiconductors, mobile devices and robotics with enhanced AI capabilities of their own.Samsung isn’t talking about the “traditional” AI factories that Nvidia is building in partnership with the U.S. Department of Energy and Oracle Corp. Those previously announced facilities are essentially data centers that theCUBE Research defines as “a purpose-built system for AI production,” providing the immense computing infrastructure needed to train and run AI models. Rather, it’s building an AI-enabled semiconductor manufacturing plant that will embed automation into almost every stage of its chip manufacturing operations, including design, equipment, operations and quality control.The company likens the planned factory to a “single intelligent network,” where AI will continuously monitor and analyze its production environments, make predictions, inform maintenance and optimize everything to try and boost its chipmaking yields.Building the AI MegafactorySamsung outlined a yearslong initiative that will see it integrate Nvidia’s accelerated computing capabilities throughout its proposed factory, with the main purpose being to scale its manufacturing operations. To do this, it’s going to rely heavily on AI-powered “digital twins,” or virtual replicas of its chip products. Using the Nvidia Omniverse platform, it’ll create digital twins of every component that goes into its semiconductors, including memory, logic, advanced packing and more. It’s also going to create twins of its actual fabrication plants and the expensive machinery within them.The company explained that this will allow it to visualize its chip manufacturing operations in a virtual environment, where it will be able to check how they perform before it launches its physical production lines. It’ll be able to spot anomalies and work out where preventative maintenance will be needed, how to optimize production and more, then apply what it learns to its real-world factory.What’s more, it’s not doing this only for chips. Although it plans to start with semiconductors, the company also wants to create digital twin environments of its hardware factories, where it manufactures devices such as its Galaxy smartphones and other products such as kitchen appliances and televisions.One example of how AI can help with chipmaking is the “optical proximity correction” process, which is a critical step to ensure wafer pattern accuracy. In early tests, Samsung said its AI-enhanced OPC process helped to increase the speed and precision in which it can identify, predict and correct circuit pattern violations and abnormalities, resulting in a 20-times improvement in computational lithography performance. Nvidia’s cuLitho and CUDA-X libraries were critical in enabling this, the company revealed.AI can also help to enhance electric design automation or EDA, which involves using specialized computer-aided software for designing new computer chips. The plan is to leverage Nvidia’s software and hardware to create a new generation of GPU-accelerated EDA tools.AI won’t just help Samsung design and optimize its semiconductor manufacturing operations. It will also help automate the physical tasks of making its chips through the introduction of more intelligent factory robotics.For instance, Samsung said it’s using Nvidia’s RTX Pro 600 Blackwell Server Edition platform alongside its Megatron framework to develop more advanced AI models to power its robots. These models demonstrate advanced reasoning capabilities that can be integrated directly into its factory machines and humanoid robots, allowing them to work with greater autonomy and precision, alongside humans.Nvidia is also helping Samsung to link virtual simulations with real-world robot data, so its robots will be better able to perceive their physical surroundings and make faster, intelligent decisions in real world scenarios. It’s doing this with the Nvidia Jetson Thor robotic platform, creating models for robots focused on task execution and workplace safety awareness.Like with its AI-enabled manufacturing optimizations, Samsung will also introduce its advanced robots into the rest of its manufacturing ecosystem in future.AI networks and HBM4 memory chips on the wayBeyond the AI Megafactory, Samsung said it’s working with Nvidia and a number of South Korea’s top telecommunications companies to improve network communications. They’re collaborating on the development of a technology called AI-RAN, which integrates AI into mobile networks to support the deployment of AI agents and “physical AI” such as intelligent robots, drones and industrial equipment. Samsung has already demonstrated a proof of concept of AI-RAN, which it says will be critical for the future adoption of physical AI.Meanwhile, Samsung said it continues to work with Nvidia on the development of its high-bandwidth memory chips, or HBM4, which are an essential component of AI servers. The company is making up for lost time here, as it has fallen behind its biggest competitor SK Hynix Inc. in the HBM memory chip sector, but believes it will ultimately be able to deliver superior performance when its HBM4 chips enter production next year.
.
According to Samsung, HBM4 chips is built on its sixth-generation 10-nanometer-class dynamic random-access memory and a four-nanometer logic base die, enabling processing speeds of up to 11 gigabits per second, exceeding the Joint Electron Device Engineering Council Solid State Technology Association’s standard of 8 gigabits per second.Image: SiliconANGLE/Dreamina AISupport our mission to keep content open and free by engaging with theCUBE community. Join theCUBE’s Alumni Trust Network, where technology leaders connect, share intelligence and create opportunities.15M+ viewers of theCUBE videos, powering conversations across AI, cloud, cybersecurity and more — Connect with more than 11,400 tech and business leaders shaping the future through a unique trusted-based network.SiliconANGLE Media is a recognized leader in digital media innovation, uniting breakthrough technology, strategic insights and real-time audience engagement. As the parent company of SiliconANGLE, theCUBE Network, theCUBE Research, CUBE365, theCUBE AI and theCUBE SuperStudios — with flagship locations in Silicon Valley and the New York Stock Exchange — SiliconANGLE Media operates at the intersection of media, technology and AI.]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1okqg4j/weekly_share_your_victories_thread/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 10:00:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>In this economy even Tux needed a second job.</title><link>https://www.reddit.com/r/linux/comments/1okq068/in_this_economy_even_tux_needed_a_second_job/</link><author>/u/Sonikku_a</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 09:32:30 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LibreOffice recap, October 2025 – Markdown support, events, app updates and more</title><link>https://blog.documentfoundation.org/blog/2025/10/31/libreoffice-project-and-community-recap-october-2025/</link><author>/u/themikeosguy</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 09:04:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Here’s our summary of updates, events and activities in the LibreOffice project in the last four weeks – click the links to learn more…We started the month by posting the LibreOffice Podcast, Episode #5 –Accessibility in Free and Open Source Software, with Michael Weghorn and Mike Saunders. Watch it below – or on PeerTube.Please confirm that you want to play a YouTube video. By accepting, you will be accessing content from YouTube, a service provided by an external third party.If you accept this notice, your choice will be saved and the page will refresh.Markdown support is coming to LibreOffice! This is just one of the projects from the Google Summer of Code 2025, and should be included in our next major release, LibreOffice 26.2, due in February next year.In October, we had two updates to the software: LibreOffice 25.8.2, and LibreOffice 25.2.7. The latter is the final update to the 25.2 branch, so after this, all users are recommended to upgrade to the 25.8 branch.It’s the End of 10! Yes, in October, Microsoft ended official support for Windows 10. This leaves users who want to continue using the operating system with few alternatives — especially if they have an old PC that is not compatible with Windows 11’s demanding hardware requirements — other than buying a new PC. But we a posted about 10 reasons to switch to Linux – and, of course, many desktop Linux distributions ship with LibreOffice.Lots of people ask us about LibreOffice’s compatibility with Microsoft Office/365 documents. We think our compatibility is very good (and always improving, as more people send us documents to test), but the format is extremely difficult to work with, as our posts about the DOCX and PPTX formats explain. (Of course, ideally we’d all be using the Open Document Format, regardless of the software we prefer! And here’s how to make your ODF documents more accessible.)Meanwhile, the Libreitalia Conference 2025 was organized by Marco Marega – a LibreItalia and TDF Member – in Gradisca d’Isonzo, near the border with Slovenia.And our final event report was from LinuxDays 2025 in Prague, where we had a stand with stickers, flyers and a quiz about LibreOffice.]]></content:encoded></item><item><title>Passwordless login via email OTP is that a good option?</title><link>http://devloprr.com/</link><author>/u/Agile_Guess_523</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 08:57:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trying to make tenant provisioning less painful. has anyone else wrapped it in a Kubernetes operator?</title><link>https://www.reddit.com/r/kubernetes/comments/1oknpg9/trying_to_make_tenant_provisioning_less_painful/</link><author>/u/Selene_hyun</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 06:55:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I’m a DevOps / Platform Engineer who spent the last few years provisioning multi-tenant infrastructure by hand with Terraform. Each tenant was nicely wrapped up in modules, so spinning one up wasn’t actually that hard-drop in a few values, push through the pipeline, and everything came online as IaC. The real pain point was coordination: I sit at HQ, some of our regional managers are up to eight hours behind, and “can you launch this tenant now?” usually meant either staying up late or making them wait half a day.We really wanted those managers to be able to fill out a short form in our back office and get a dedicated tenant environment within a couple of minutes, without needing anyone from my team on standby. That pushed me to build an internal “Tenant Operator” (v0), and we’ve been running that in production for about two years. Along the way I collected a pile of lessons, tore down the rough edges, redesigned the interface, and just published a much cleaner Tenant Operator v1.- Watches an external registry (we started with MySQL) and creates Kubernetes Tenant CRs automatically. - Renders resources through Go templates enriched with Sprig + custom helpers, then applies them via Server-Side Apply so multiple controllers can coexist. - Tracks dependencies with a DAG planner, enforces readiness gates, and exposes metrics/events for observability. - Comes with scripts to spin up a local Minikube environment, plus dashboards and alerting examples if you’re monitoring with Prometheus/Grafana.This isn’t a polished commercial product; it’s mostly tailored to the problems we had. If it sounds relevant, I’d really appreciate anyone kicking the tires and telling me where it falls short (there’ll be plenty of gaps). Happy to answer questions and iterate based on feedback. Thanks!P.S. If you want to test it quickly on your own machine, check out the Minikube QuickStart guide, we provision everything in a sandboxed cluster. It’s run fine on my three macOS machines without any prep work.]]></content:encoded></item><item><title>Question from beginner: what do I lose from using fiber?</title><link>https://www.reddit.com/r/golang/comments/1okmqua/question_from_beginner_what_do_i_lose_from_using/</link><author>/u/Fuzzy-Scratch-5386</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 05:52:11 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am a hobby programmer that recently migrated from Bun/Nodejs. In order to learn go, I started by working simple rest API using fiber and sqlite. After this, while browsing for more complex project ideas, I found that fiber is not recommended because it is build over fasthttp and does not support http2 protocol. Upon further looking, I found out that http2 require (not mandatory per se, but recommended) proper tls, which probably (mostly) is not present in local project. So my question is, why not use fiber for local project? While the performance is not an issue, I like how we can create route groups as well as write the API easily.Edit 2: I am checking videos by Dreams of Code, these code looks cleaner]]></content:encoded></item><item><title>What Are Some Active Kubernetes Communities?</title><link>https://www.reddit.com/r/kubernetes/comments/1oklzqo/what_are_some_active_kubernetes_communities/</link><author>/u/Healthy-Sink6252</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 05:05:40 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have seen only Home Operations Discord as an active and knowledgeable community. I checked our CNCF Slack, response times are like support tickets and does not feel like a community.If anyone also knows Indian specific communities, it would be helpful too.I am looking for active discussions about: CNCF Projects like FluxCD, ArgoCD, Cloud, Istio, Prometheus, etc.I think most people have these discussions internally in their organization.]]></content:encoded></item><item><title>Mixing AMD and Intel CPUs in a Kubernetes cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1okl0na/mixing_amd_and_intel_cpus_in_a_kubernetes_cluster/</link><author>/u/Popular_Parsley8928</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 04:09:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I will have 4 VMs each with 12G RAM and 2 vCPU, this will be for my home lab, I will install Alma Linux 9 and then manually install Kubernetes cluster ( Rancher v2.11.6 and 4 K8S with version v1.30). The AMD CPU is AMD FX-8320 and Intel is Core i7-3770.I won't run sophiscated app, just a small home lab to learn Kubernetes, thanks!]]></content:encoded></item><item><title>riscv.org : RISC-V Mentorship Program</title><link>https://www.reddit.com/r/linux/comments/1okkihr/riscvorg_riscv_mentorship_program/</link><author>/u/I00I-SqAR</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 03:43:43 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pure Go HDF5 library reaches write support (v0.11.0/v0.11.1-beta). Beta testers needed before RC.</title><link>https://www.reddit.com/r/golang/comments/1okjak6/pure_go_hdf5_library_reaches_write_support/</link><author>/u/mistbow</author><category>golang</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 02:42:23 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Pure Go HDF5 library reaches write support (v0.11.0/v0.11.1-beta)After a year of work, my pure Go HDF5 implementation just hit a major milestone - you can now create HDF5 files without CGo.: HDF5 is the standard for scientific data (astronomy, climate, genomics, etc.). Current Go options are gonum/hdf5 (CGo wrapper, requires ) and abandoned pure-Go attempts from 2015-2016.: go file, _ := hdf5.CreateForWrite("data.h5", hdf5.Truncate) file.CreateDataset("temp", data, hdf5.WithChunked([]uint64{100}), hdf5.WithCompression(6), // GZIP ) Full support: chunked datasets, GZIP compression, dense groups (HDF5 1.8+), attributes, all datatypes. 70-88% test coverage, cross-platform.: - No CGo = easy cross-compilation, no C dependencies - HDF5 is massive in scientific computing (TensorFlow models, NASA data, genomics) - Previous pure-Go attempts stalled because "too complex" (beta): - Can't reopen files and add more data yet (v0.11.2) - Not h5dump-compatible yet (investigating) - Attributes write-once only: HDF Group acknowledged it on their forum as the first viable pure-Go implementation (link).Looking for beta testers with real-world scientific data. Installation: go get github.com/scigolib/hdf5@v0.11.1-betaHappy to answer questions about the implementation - HDF5 format is gnarly but solvable with the C library as reference.]]></content:encoded></item><item><title>[D] Monthly Who&apos;s Hiring and Who wants to be Hired?</title><link>https://www.reddit.com/r/MachineLearning/comments/1okj2rw/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link><author>/u/AutoModerator</author><category>ai</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 02:31:34 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[ please use this templateHiring: [Location], Salary:[], [Remote | Relocation], [Full Time | Contract | Part Time] and [Brief overview, what you're looking for]For Those looking for jobs please use this templateWant to be Hired: [Location], Salary Expectation:[], [Remote | Relocation], [Full Time | Contract | Part Time] Resume: [Link to resume] and [Brief overview, what you're looking for]Please remember that this community is geared towards those with experience.]]></content:encoded></item><item><title>A Refreshing Philosophy of Software Design [Book Review]</title><link>https://theaxolot.wordpress.com/2025/10/30/a-refreshing-philosophy-of-software-design/</link><author>/u/The_Axolot</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 01:46:01 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I pride myself on being the kind of blogger who writes what he truly believes, even when it’s unpopular. That’s part of why I’m so brash and come across as condescending. It’s cathartic for me, so if being correct is condescending, I don’t want to be wrong.Having said that, it feels good when I find others whose opinions mostly resonate with mine. I very recently finished reading A Philosophy of Software Design, by  (I know I’m super late on that), and I’ve come to the conclusion that it should be mandatory reading for every software engineer.First of all, it’s packed with content that isn’t easily found elsewhere. Both junior and senior engineers can learn a lot.He explains complexity as a combination of and  (though some problems have inherent complexity), and how it slowly creeps through incremental changes to code.The concepts of interface depth and information hiding are excellent ways to explain what modularity looks like at the program, API, service, class, and even function level.His use of the text editor example to explain many of his concepts felt natural and not forced at all.In chapter 19, he gives his takes on some popular software trends such as OOP, Agile, Testing, etc.My favorite take of his is on , in which he proclaims he’s not a fan because it’s too incremental and distracts from high-level design. He only likes it when fixing bugs. That, my readers, is the correct take, and it alone puts him leagues above most.And best of all, he isn’t dogmatic about in his presentation. In fact, each of his points is full of nuance and discussion of specific caveats that I haven’t seen anywhere else. Throughout every chapter, I just kept thinking, “This guy knows his stuff.”The bad doesn’t outweigh the good, but I still have to point out what bothered me.First of all, the ratio of code examples to advice is lacking. A lot of it will resonate with experts in the abstract, because we have lived experiences of said scenarios. But the less experienced may struggle to internalize some lessons without concrete examples to anchor them to. Ousterhout admits this in the introduction, but still.But my real issues are with the presentation of these examples. A lot of the time, Ousterhout will present an interface and point out the flaws that lead to brittleness and complexity. But then, he’ll instantly come up with another interface and explain why it’s better.His explanations are correct. It’s just that the way he presents them gives the impression that you can design a good interface from requirements alone, but then doesn’t explain how he reasoned his way to it. It feels like post-hoc rationalization of his expertise rather than a process anyone can deduce their way through.The biggest culprit of this is the text editor example. I’ve never designed one of these before, so I was looking forward to hearing his thought process throughout. I was able to follow along, but a more junior developer would certainly struggle.Am I supposed to believe that Ousterhout is so brilliant that he comes up with modular interfaces right off the bat? Of course not. You implement, your understanding grows alongside said implementation, and then you build your design. There’s not a single intermediate design iteration in the book. It goes straight from bad to good.There were some other things that bothered me, too.: Define Errors Out Of Existence had some great points, though it was a bit overstated. I rarely encounter instances of unjustified exceptions, though the examples he mentions, namely key deletion and substring, do fit the bill. I don’t know if it really warranted an entire chapter, though.:  is also not worth a chapter. Do you really need to tell people to try out multiple designs?In , he “debunks” common excuses that people use to forgo writing comments and documentation.He claims that self-documenting code is a myth because there are often too many details in an interface to communicate through code alone. The example he uses to support his stance is the ambiguity of built-in  method. Specifically, whether its is inclusive, and what happens if the  index exceeds the index. Without its documentation, you’d have to read the method anyway to know these.Now I don’t know what Ousterhout has been through, but surely this is an exaggeration. Yes, there may bedetails of a method that are difficult to communicate through its signature alone. But if the details are so significant, then that indicates bad design (assuming the functionality itself isn’t just unintuitive by nature). Ousterhout himself points this out in Chapter 10, so I don’t know why he uses this example. Java libraries aren’t the gold standard for code. People don’t actually believe that, right?But there’s another factor to consider. Libraries, services, and APIs are in a different realm to classes and methods. The more granular you get, the less valuable documentation becomes, because it’s very hard to document these parts without just restating the implementation. Ousterhout acknowledges this in Chapter 16.I don’t see people claiming that high-level documentation should be omitted, or that we shouldn’t write comments for confusing code in methods. In fact, I often see the opposite in code reviews, which is good.“Good code is self-documenting” doesn’t mean, “No comments are allowed.” It means, “Comments shouldn’t be the default; they should be the last resort after code is made as clear as possible, yet there’s still confusion.”I can tell he understands this because in Chapter 18, he says mentions comments as a way to compensate when code is nonobvious. So why devote so much of the book toward best practices regarding comments, when the majority of readers would already agree with his stance, assuming they can even understand it? It’s fragmented across four chapters, and not even consecutive, mind you, so I had a hard time piecing it together to explain to you here.In he actually gives good examples of low-level comments, but I think he prematurely generalizes those examples to claim that low-level documentation is more necessary than people believe. But the reason it’s valuable in these specific examples is because he’s dealing with an inherently complex and uninituitive topic (extremely low-level memory manipulation for RPC functionality), and it’s in C++. Interestingly, it’s at this chapter that he starts using C++ in his examples. A bit fishy, I have to say.In , he advocates using comments as a design tool. That is, using comments to plan your code and filling them in with the abstractions that follow. He doesn’t explicitly say this, but it’s clear this is meant to be an alternative to TDD (*shudder*).He even uses similar talking points like, “If you wait until after your implementation to write comments, you may decide not to write them at all,” and, “Comments are a design tool for interfaces.” I use comments to keep myself on track and plan out my implementations, but not the way he describes it.Why would I document a class, method, or even a variable I write it?Why don’t I just write it and let the interface arise naturally as the implementation grows?Code reviews can point out the need for comments, so does it matter what order I write them in?What if comments are used as crutches to explain badly designed code?is super lackluster.There are so many ways to increase the clarity of your code, yet he only mentions superficial aspects, like using white space judiciously and comments.For things that decrease clarity, he mentions:Event-driven programming (a.k.a indirect calls). I guess, but are people defaulting to event-driven calls so much that this is worth mentioning?Generic containers (like Java’s class). Once again, suspiciously specific. What about languages where multiple values can be grouped together without having to name the resulting object?Different types for declaration and allocation(like List<Integer> = new ArrayList<Integer>();). Okay, this is starting to get silly. Is polymorphism bad now?Violating readers’ expectations. This one’s alright.: Designing for Performance barely scratches the surface. There are so many more performance optimizations to consider, and much more common than the low-level example he uses. What about bad queries and unnecessary API calls? Caching?More generally, he barely talks about functional programming, whose principles strongly push you toward modular design right out the gate.What about databases and SQL? Are they not software?The book feels very C++ and Java-centric, even though it was first published in 2018. I’m not saying Ousterhout has to only use modern languages, but he should at least acknowledge that object-oriented languages carry a lot of legacy baggage, and that it might cause this book, and even his philosophy, to become outdated soon.In Chapter 9, Ousterhout talks about decomposition. Specifically, when to separate modules into smaller components or bring them together to reduce complexity. In chapter 9.7 & 9.8, he discusses how and when methods should be decomposed. Sound familiar?This was the topic of my article last week, and it spooked me how similar my points were to his, despite my never having read this book before, and not seeing my ideas expressed elsewhere with such precision:He calls out LoC as a dumb metric for function decomposition, though much more politely than I do.He explains the main cost of decomposition, the main one being the spread of complexity rather than its reduction.He says that you should favor decomposition when the subfunction can be understood in isolation, and the parent function can be understood without the implementation of the child function, which is almost exactly what I said in Lesson 4 of my article.He advocates organizing code into independent blocks.I could go on. He explicitly pushes back on ‘s ridiculous function length recommendations, which I have to admit is pretty bold. I’d recommend people read the book for this alone. Treat it as a rebuttal to .Oh, and I’ll just drop this here.I know I spent a lot of time on the bad, but believe me when I tell you that it’s not that significant compared to the rest of the book. My issues were more with how much time was spent on things I felt didn’t warrant such attention, and things I wished were discussed, rather than things that were outright wrong. I’m just a thorough guy. Anything I didn’t mention in the above sections, I consider good, or just not worth mentioning.Overall, you should read this book if you haven’t already. And if you think is good, then you REALLY NEED to read this.But read my articles first so you can see just how on point I am.]]></content:encoded></item><item><title>My first day in Rust</title><link>https://www.reddit.com/r/rust/comments/1oki355/my_first_day_in_rust/</link><author>/u/Zealousideal_Sort521</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 01:43:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I am a programmer with 15 years of experience in C# and the full Microsoft stack. I dream in LINQ and Entity Framework Core. Today was my first deep dive into Rust and I loved it. My observations: * Rust is very precise and type safe. Way more precise than C#. No dynamics ever in Rust * The compiler is actually helpful. * I was under the impression that I was actually using my IQ points while programming again. Which was a pleasant surprise. Rust is the ultimate counterspell to vibe coding. * Setting up swagger was more difficult than it. Needed to be. * Rust code rots faster than C# code. Many examples on GitHub are unusable. * I wasn’t really a fan of the idea of being forced into nightly compiler builds to use the rocket framework. ]]></content:encoded></item><item><title>How We Saved 70% of CPU and 60% of Memory in Refinery’s Go Code, No Rust Required.</title><link>https://www.honeycomb.io/blog/how-we-saved-70-cpu-60-memory-refinery</link><author>/u/phillipcarter2</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 00:47:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Refinery has a big job: it performs dynamic, consistent tail-based sampling that maintains proportions across key fields, adjusts to changes in throughput, and reports accurate sampling rates. The traffic patterns it handles are challenging, with long or large traces requiring it to hold lots of information in memory, while sudden volume spikes leave little time for infrastructure to scale up—all in a package that people want to run as cheaply as possible, since one of the primary use cases for sampling is cost control. When you're spending money to save money, you always want to spend . Version 3.0 is a big advance in that direction.When we upgraded our internal Refinery cluster, total CPU usage dropped by 70%, while RAM use dropped by 60%:With an improvement like this, we can downsize this 72-node cluster by half—a meaningful savings—while still keeping more headroom than before. If you’re a Refinery user, hopefully so can you.How did we pull off such a big change?The code’s all in this merge, but I’ll cover the basics here.Like many programming languages, Go is capable of being very fast under the right circumstances (working with bounded quantities of strongly typed data), and slow under the wrong ones. Unfortunately, Refinery’s job of handling customer-defined trace spans is very close to the wrong one. Historically, we followed the standard approach and fully de-serialized every span that came in through the API. Since there’s no fixed schema, the fields went into a big  — hundreds of heap allocations, pointers everywhere. It was simple and effective, but it was also expensive. Compounding this cost, in a cluster configuration, the majority of spans are handled twice since they’re redirected from the receiving node to the node which “owns” the relevant trace. Here’s what a profile of a typical clustered Refinery looked like:There’s a lot going on here, but you can see almost a quarter of CPU time going to garbage collection. Digging further reveals that a lot of the leaf nodes are ultimately some form of . In total, 50% of all CPU time in this process is allocation-related, all in order to hold onto span data as it waits for a sampling decision—after which, in most cases, it’s simply thrown away without being sent to Honeycomb!There’s also a lot of overhead just for setting up data structures which we’re hardly going to use. The actual , which is the reason we’re doing all of this, all happens in the  loop, a mere 12% of time in this profile. Even that is mostly internal metrics instrumentation rather than the core decision-making algorithms.The best way to make all this de-serialization (and, eventually, re-serialization) fast is to not to do it at all. Refinery only ever looks at a handful of fields in any given span, the rest is just cargo. And it’s very possible to extract only the fields you need from a serialized blob, as in the simplified example below. These two benchmarks demonstrate de-serializing into a map, and our new  approach, where we pull out any fields Refinery needs, then hang onto the serialized data for re-transmission. I’m using MessagePack here because that’s Refinery’s native format, with a low-level serialization API provided by the tinylib/msgp library.func BenchmarkDecodeStrats(b *testing.B) {
    msgpData, _ := msgpack.Marshal(struct {
        TraceID    string
        DurationMs float64
    }{
        TraceID:    "1234567890",
        DurationMs: 123.4,
    })

    // Unmarshal to a schemaless map[string]any, the old way.
    b.Run("to_map", func(b *testing.B) {
        for b.Loop() {
            var m map[string]any
            _ = msgpack.Unmarshal(msgpData, &m)
        }
    })

    // Unmarshal a subset of fields using custom deserialization;
    // this is the new way.
    b.Run("selective", func(b *testing.B) {
        for b.Loop() {
            var durationMs float64
            mapSize, remaining, _ := msgp.ReadMapHeaderBytes(msgpData)

            for range mapSize {
                var key []byte
                key, remaining, _ = msgp.ReadMapKeyZC(remaining)
                if bytes.Equal(key, []byte("DurationMs")) {
                    durationMs, remaining, _ = msgp.ReadFloat64Bytes(remaining)
                } else {
                    remaining, _ = msgp.Skip(remaining)
                }
            }
            _ = durationMs // Pretend we did something with the duration
        }
    })
}You can see that  involves much more code, but it’s hard to argue with the results:BenchmarkDecodeStrats/to_map-12     296.1  ns/op   9 allocs/op
BenchmarkDecodeStrats/selective-12   16.98 ns/op   0 allocs/opThis is a very simple scenario, and it’s common for real spans to have hundreds or even thousands of fields, which in the old version meant much longer parsing times and thousands of distinct allocations per span. Instead, Refinery 3.0 keeps the serialized data, retaining it in a format which is much more compact than the web of headers and pointers created for a fully realized map. This more compact data is the main reason for Refinery’s improved memory footprint.Of course, Refinery supports three other types of input data besides our native MessagePack (, , ). To handle the others, Refinery now transcodes those formats  to serial MessagePack, binary-to-binary, again extracting any useful fields along the way. This code is even more voluminous than the selective extraction from MessagePack illustrated above, but it avoids an expensive additional step of translation from generated protobuf data structures into our own.To add icing to this cake, we also optimized our metrics instrumentation, implemented pools to re-use large buffers, and (coming soon as a minor version update) parallelized the core decision loop to scale across many CPUs. Notably, there are no clever algorithms or language tricks at play here. We didn’t have to rewrite it in Rust. All we've done is reimagine which work this process really needs to do, and focus on only doing that.]]></content:encoded></item><item><title>mm, swap: never bypass swap cache and cleanup flags (swap table phase II)</title><link>https://lore.kernel.org/lkml/bvavihwrtkbnsqgjbotwihckxzmnhdd4e6jre4j7xdiyyeyv5o@dnnuyacthvms/T/#m55f0cf90afb8f8faaff3e33829c336bc7522a0b8</link><author>/u/ilep</author><category>reddit</category><pubDate>Fri, 31 Oct 2025 00:26:53 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Sidecar injector race condition during node reboot</title><link>https://www.reddit.com/r/kubernetes/comments/1okfjxt/sidecar_injector_race_condition_during_node_reboot/</link><author>/u/0x4ddd</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:47:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Let's consider following scenario: - worker node hosting injector for mutating webhook for something like service mesh - the same node hosting application podA) Node is broken & offline longer than pod-eviction-timeout, pods are being rescheduled to remaining nodes, it may happen application pod starts before injector and is not instrumented in the endB) Issue was short, like sudden power loss followed by power on, pods are starting recovery on the same node but the same race condition may apply as in previous caseIs the only option to set failurePolicy of mutating webhook config to Fail? I have seen some injector helm charts where this is hardcoded to Ignore and not overridable via values by default, and also number of replicas of injector being hardcoded to 1 and not overridable.]]></content:encoded></item><item><title>An interview with Ken Silverman, creator of the Build Engine (Duke Nukem 3d, Shadow Warrior, Blood). Ken programmed the engine at the age of just 17.</title><link>https://youtu.be/WruzfQLxpQY</link><author>/u/Tech-Jesse</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:45:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GlueKube: Kubernetes integration test with ansible and molecule</title><link>https://medium.com/@GlueOps/gluekube-kubernetes-integration-test-with-molecule-f88da7c41a34</link><author>/u/MindCorrupted</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:36:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[At GlueOps, we have been working on an internal tool to deploy and manage Kubernetes clusters across cloud providers and datacenters. During development, we ran into a few caveats. For example, if we modify a ‘prepare-node’ role to install an additional package or remove a package that seems unnecessary, it can indirectly affect the subsequent roles.Example of one of the use cases, in the prepare-nodes playbook, we tend to remove a package that was thought unnecessary or change a package’s version, it can result in the subsequent roles like kubeadm-init to fail.So we decided to add a couple of tests to help keep our work consistent and maintainable.Some of the tests we created were:Scaling node(control-plane,data-plane)Kubernetes versions upgradeAs we deploy many production-grade clusters across numerous cloud providers (e.g. AWS, GCP, metal) for our customers, we wanted to make our deployment more agnostic. We took a look at a couple of existing tools like Kubespray, but we felt it may be a burden to maintain and modify the existing codebase in case we want to customize it.Ultimately, we decided to build GlueKube: a platform to create kubernetes clusters agnostic to cloud providers with kubeadm, ansible.Deploying a stacked Etcd cluster where the Etcd is embedded within control-plane nodes unlike the external-etcd where Etcd where it can in separate nodes.Supporting two load balancers (e.g. for the control plane and business applications)Scaling Up/Down nodes(control-plane and worker nodes)Tainting and Labeling nodesApplying OS security patchesUpgrading cluster versionsMolecule as a Testing toolWe’re using ansible to configure our clusters, we needed a testing tool that’s compatible with ansible and supports managing Hetzner resources for testing.As our case is more of integration test than unit test, we found Molecule a more suitable option than ansible-test, as it provides a structured way through  to create/test/destroy infrastructure.After our research and experiments, we created this blog to help anyone else considering similar tradeoffs.In this post, we’re focusing on our journey with molecule. If you’re interested in learning more about the project, check out the link.Test Case I: Scaling Down Worker NodesWe started with scaling down worker nodes because it is easier than working with control-plane nodes.After implementing this workflow in  scaling down is as easy as removing the node from the inventory file hosts.yaml and applying the sync-resources.yaml file, like the demo below:Now how do we test that with Molecule ? after we created the scenario(test suite) using: molecule init scenario scaler-cluster.We changed the scenario property in molecule.yaml to the following:Create: for creating the required resources for testing, our cluster will usually consist of 3 master planes, 3 worker nodes, 1 loadbalancer (HAProxy in our case).Converge: it’s the file we will execute to transform resources into specific stateVerify: a file to run some tests after the resources get converged. The simplest test will run at first is verifying cluster health.Destroy: at the end we need to cleanup the resources, this file will usually do the contrary of what  does. One side note is this file will always run whenever the tests result (success || failed). relies heavily on  to know the desired state of the cluster, think of it the same as terraform state and for our tests to run we will need one.And this will lead us into  that’s responsible for generating the file from the created test resources. So we modified molecule.yaml to use the generated with the following code: contains ansible configurations for each group in .For running a basic verification, we used the following command:This will trigger all the sequences we declared on molecule.yamlTo scale resources down, we need to remove the desired node from hosts.yaml, our initial thought of the process was creating a python script, give it the desired node, remove it from hosts.yaml and then refresh the inventory cache. However, we wanted to keep our test more Ansible oriented.We found a better solution (at least for us) by creating two initial hosts.yamlfiles: the first one with all the nodes in and the second one without one of the worker nodes.We used slicing to pick the  from  list, here is a code snippet from :Molecule has another sequence called , which we used to replace the with  contentsrefresh the inventory cache and do the , here is the code.For Molecule to recognize the side_effect, we added it alongside the other sequences.After the side_effect sequence gets executed, we should verify the expected state of the cluster, in our case the side_effect should reduce the number of worker nodes by 1, so our test will count how many worker nodes we currently have. Here is a code example:In this post, we shared our experience setting up integration tests for Kubernetes cluster management using Molecule and Ansible. We focused on a specific test case:, illustrating how Molecule’s sequences like , , , and  can be orchestrated to achieve this. We also highlighted the importance of the Ansible inventory in defining the desired state of the cluster and how Molecule facilitates testing changes to this inventory. This approach allows us to maintain the reliability and consistency of our GlueKube platform as we continue to develop and enhance its capabilities for deploying and managing Kubernetes clusters across diverse environments.]]></content:encoded></item><item><title>How Memory Maps (mmap) Deliver 25x Faster File Access in Go</title><link>https://info.varnish-software.com/blog/how-memory-maps-mmap-deliver-25x-faster-file-access-in-go</link><author>/u/SnooWords9033</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:25:00 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[One of the slowest things you can do in an application is making system calls. They're slow because you do have to enter the kernel, which is quite expensive. What should you do when you need to do a lot of disk I/O but you care about performance? One solution is to use memory maps.Memory maps are a modern Unix mechanism where you can take a file and make it part of the virtual memory. In Unix context, modern means that it was introduced in the 1980s or later. You have a file, containing data, you mmap it and you'll get a pointer to where this resides. Now, instead of seeking and reading, you just read from this pointer, adjusting the offset to get to the right data.To show what kind of performance you can get using memory maps, I've written a little Go library that allows you to read from a file using a memory map or a ReaderAt. ReaderAt will do a pread(), which is a seek/read combo, while mmap will just read from the memory map.This almost feels like magic. Initially, when we launched Varnish Cache back in 2006, this was one of the features that made Varnish Cache very fast when delivering content. Varnish Cache would use memory maps to deliver content at blistering speeds.Also, since you can operate with pointers into memory that is allocated by the memory map, you'll reduce memory pressure as well as raw latency.The Downside of Memory MapsThe downside of memory maps is that you really can't write to the memory map. The reason is due to the way virtual memory works. When you're writing to a part of virtual memory that isn't mapped into physical memory, the CPU will generate a page fault. On a modern computer, the CPU is responsible for tracking what virtual memory pages are mapped onto what physical memory. Since you're writing to a page that isn't mapped, the CPU needs help.So, when the page fault occurs, the OS will 1) allocate a new memory page, 2) read the contents of the file at the correct offset, 3) write this to the new memory page. Then control is returned to the application. The application will now overwrite the virtual memory page with new data.Can we stop and appreciate how extremely inefficient this is? I think it is fairly safe to say that writing through a memory map is never a good idea when considering performance. At least if there is any risk, the file isn't mapped up in physical memory.Let me illustrate this with a few more benchmarks.As you can see, whether or not the pages are in cache is crucial for performance. WriterAt, which uses the pwrite call, is a much more predictable bet.Still, writing through memory maps, was what Varnish Cache did initially. It somehow got away with it, but mostly because the competition was pretty bad.This is why Varnish Cache got the malloc backend and why Varnish Enterprise got the various Massive Storage Engines. The malloc backend resolved the problem by just allocating system memory through the malloc system call, and the Massive Storage Engine uses io_uring, which is so new that support for it is still somewhat limited.Using Memory Maps to Solve Real-world Performance ProblemsThe last couple of weeks I've been working on an HTTP-backed filesystem. This is part of our AI Storage Acceleration solution, geared towards high performance computing environments. In this filesystem we needed a way to transfer folder data over HTTP. A folder is really just a listing of files, symbolic links and directories. The naive approach would be just to use JSON encoding, but JSON is notorious for being slow.Our priority is performance. We made a benchmarking suite, comparing various databases with each other. CDB was overall the fastest. Looking at the numbers, we'd still see that CDB would spend something like 1200ns on a database lookup that was entirely in the page cache. This seems very slow to me. After all, everything should be in memory and spending 1200ns reading memory sounds at least 100x too slow. I started looking into the CDB implementation I was using. It was the above ReaderAt implementation. So, most of the time is likely spent waiting for the operating system.Some hours later, I was able to replace the seek/read with a memory map. This resulted in a 25x improvement in performance. Again, it feels like magic. Unlike the original file stevedore in Varnish Cache, this performance improvement has no downside.https://github.com/perbu/mmaps-in-goCDB64 files with memory maps: https://github.com/perbu/cdb]]></content:encoded></item><item><title>Tik Tok saved $300000 per year in computing costs by having an intern partially rewrite a microservice in Rust.</title><link>https://www.linkedin.com/posts/animesh-gaitonde_tech-systemdesign-rust-activity-7377602168482160640-z_gL</link><author>/u/InfinitesimaInfinity</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:24:08 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[TikTok saved $300,000/year by doing this one thing...
And no, it’s not AI. It’s Rust. 🦀
Their payment service was slowing down as traffic exploded on TikTok LIVE.

They tried scaling the Go backend... but:
🔹 More machines = more $$$
🔹 Higher traffic = more latency
🔹 Garbage collection = 😵💫

So what did they do?
👉 Rewrote just the slowest Go endpoints in Rust.
Not the whole thing. Just the bottlenecks.

And the results were insane:
Performance at scale (80K QPS)
Metric.                         Go.                Rust.             Improvement.  
CPU usage.                78.3%.           52%↓         33.6%.  
Memory usage.         7.4%.              2.07%↓      72%
p99 latency.              19.87 ms.       4.79 ms↓   76%
That’s not a small speed-up — that’s a rewrite-worthy difference.

💰 Cost savings?
Cut 400 vCPU cores from the cluster.
➡️ ~$300K/year saved. Just like that.

So how did Rust do it?
🔸 No garbage collector
🔸 Copy-on-Write (share memory until modified)
🔸 Zero-cost abstractions (speed + safety)
🔸 Less memcpy, more control
🔸 Aggressive compiler optimizations
It’s like Go is your reliable commuter car...
And Rust is the F1 car you pull out for the real races. 🏁

Engineering Wisdom from TikTok:
✨ “Use the right tool for the job.”
👉 Keep Go for 95% of services (fast dev cycles, happy teams)
👉 Use Rust where performance = revenue

This isn’t about language wars.
It’s about being strategic.
🔁 Polyglot stacks are the future.
💬 Thoughts?

📝 Source: https://lnkd.in/dk9_H6HR#Rust#GoLang#SoftwareEngineering#Scale#Performance#TikTok#DevEx#Rustaceans#Backend]]></content:encoded></item><item><title>Jujutsu at Google</title><link>https://www.youtube.com/watch?v=v9Ob5yPpC0A</link><author>/u/steveklabnik1</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:05:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Provisioning Clusters on Baremetal</title><link>https://www.reddit.com/r/kubernetes/comments/1okejd9/provisioning_clusters_on_baremetal/</link><author>/u/CompetitivePop2026</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 23:03:45 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hello! I have been trying to think of a way to provision clusters and nodes for my home lab. I have a few mini pcs that I want to run baremetal k3s, k0s, or Talos. I want to be able to destroy my cluster and rebuild whenever I want just like in a virtual environment. The best way so far I have thought on how to do this is to have a PXE server and every time a node boots it would get imaged with a new image. I am leaning towards Talos with machine configs on the PXE server, but I have also thought of using a mutable distro with Ansible for bootstrapping and Day 2 configurations. Any thoughts or advice would be very appreciated! ]]></content:encoded></item><item><title>[R] We found LRMs look great…until the problems get harder (AACL 2025)</title><link>https://www.reddit.com/r/MachineLearning/comments/1okdq0s/r_we_found_lrms_look_greatuntil_the_problems_get/</link><author>/u/natural_language_guy</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 22:29:26 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hi there! I'm excited to share this project on characterizing reasoning capabilities of Large Reasoning Models (LLMs incentivized with "thinking"). We look at large reasoning models (LRMs) and try to answer the question of "how do they generalize when reasoning complexity is steadily scaled up?Short answer: They’re solid in the easy/mid range, then fall off a cliff once complexity crosses a threshold. We use graph reasoning and deductive reasoning as a testbed, then we try to reconcile the results with real world graph distributions.Built a dataset/generator (DeepRD) to generate queries of specified complexity (no limit to samples or complexity). Generates both symbolic and 'proof shaped' queries. We hope this helps for future work in reasoning training+evaluation!Tested graph connectivity + natural-language proof planning.Saw sharp drop-offs once complexity passes a certain point—generalization doesn’t magically appear with current LRMs.Compared against complexity in real-world graphs/proofs: most day-to-day cases are “in range,” but the long tail is risky.Provide some in depth analysis on error modes Benchmarks with limited complexity can make models look more general than they are. The drop in performance can be quite dramatic once you pass a complexity threshold, and usually these high complexity cases are long-tail.]]></content:encoded></item><item><title>The Affinity Suite has become free and can run on Linux</title><link>https://www.reddit.com/r/linux/comments/1okcpwy/the_affinity_suite_has_become_free_and_can_run_on/</link><author>/u/SpeeQz</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 21:47:11 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[In the image above I am running the new Affinity app (a popular Photoshop, Illustrator, etc... alternative) which has combined the entire suite into a singular app. It is running on Heroic and there was confirmation from another user (u/Segajr) of it running on Lutris too.]]></content:encoded></item><item><title>Check if channel is empty</title><link>https://www.reddit.com/r/golang/comments/1okca10/check_if_channel_is_empty/</link><author>/u/TomatilloOpening2085</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 21:28:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, i have a noob question about channel. I'm trying to code a program to play scrabble. To find the combination possibles according to the hand of the player and the letters already present on the board, I tried to code a worker pool and pass them the hand of the player, a kind of "regex" and a channel to retrieve their solution. The problem is that I have a predetermined number of worker, a known number of "regex", but an unknown number of solution generated. So if all my worker write to this channel theirs solution, how can I, in the main thread, know when i'm done reading the content of the channel ? ]]></content:encoded></item><item><title>Rendered manifests pattern tools</title><link>https://www.reddit.com/r/kubernetes/comments/1okbx7p/rendered_manifests_pattern_tools/</link><author>/u/misse-</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 21:14:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[ What tools, if any, are you using to apply the rendered manifests pattern to render the output of Helm charts or Kustomize overlays into deployable Kubernetes manifests?I am somewhat happily using Per-cluster ArgoCDs, using generators to deploy helm charts with custom values per tier, region, cluster etc.What I dislike is being unaware of how changes in values or chart versions might impact what gets deployed in the clusters and I'm leaning towards using the "Rendered manifests pattern" to clearly see what will be deployed by argocd.I've been looking in to different options available today and am at a bit of a loss of which to pick, there's:Holos - which requires me to learn cue, and seems to be pretty early days overall. I haven't tried their Hello world example yet, but as Kargo, it seems more difficult than I first anticipated.Ideally I would commit to main, and the ci would render the manifests for my different clusters and generate MRs towards their respective projects or branches, but I can't seem to find examples of that being done, so I'm hoping to learn from you.]]></content:encoded></item><item><title>Jerome Powell says the AI hiring apocalypse is real: &apos;Job creation is pretty close to zero.’ | Fortune</title><link>https://fortune.com/2025/10/30/jerome-powell-ai-bubble-jobs-unemployment-crisis-interest-rates/</link><author>/u/fortune</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 20:46:56 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[He noted “a significant number of companies” have recently announced layoffs or hiring pauses, with many of them explicitly citing AI as the reason.



“Much of the time they’re talking about AI and what it can do,” Powell told reporters after the Fed’s rate-cut decision, warning large employers are signaling they won’t need to add headcount for years. “We’re watching that very carefully,” he added.



The comments come as the Fed cut interest rates by a quarter point to a range of 3.75%–4%, citing “downside risks to employment” even as inflation remains elevated. Powell said the U.S. economy is still expanding at a “moderate pace,” even as hiring slows. He described that spending as one of the “big sources of growth in the economy,” driven by companies building data centers and other equipment tied to artificial intelligence.



Powell also pushed back on the idea that all that spending is amounting to another speculative bubble. He drew a clear line between today’s surge in capital expenditure and the dot-com era, noting “these companies actually have earnings.”  Those projects, he said, aren’t especially sensitive to interest rates, though, since they reflect long-term bets on higher productivity.At the same time, Powell emphasized the boom creates a policy dilemma for the Fed. AI and automation are boosting output, but they’re also allowing companies to do more with fewer workers, leaving the labor market softer, even while GDP stays positive.“We have upside risks to inflation, downside risks to employment,” he said. “This is a very difficult thing for a central bank, because one of those calls for rates to be lower, one calls for rates to be higher.”



Recent corporate announcements illustrate Powell’s warning. Amazon announced this week it laid off 14,000 middle managers—about 4% of its white-collar workforcein an effort to “remove organizational layers.” The layoffs come amid their rampant investments into AI.  Target, Paramount, and other large firms followed with their own cuts.



According to a Challenger, Gray & Christmas report, U.S. employers have announced nearly 946,000 layoffs so far this year—the highest total since 2020—with more than 17,000 explicitly tied to AI and another 20,000 to automation.“Job creation is very low, and the job-finding rate for people who are unemployed is very low,” Powell said.The phenomenon is so widespread some economists have coined a new term—the “Great Freeze”—to describe the dismal labor market conditions. With unemployment among recent college grads topping 5%—and AI threatening to automate entry-level office jobs—many Gen Z workers are turning to graduate school as a strategic timeout. 



That awkward balance—strong investment but weak hiring— is now at the center of the Fed’s decision-making. Powell said the economy increasingly resembles a K-shape, with higher-income households and large corporations benefiting from strong stock markets and AI-fueled productivity gains, while lower-income consumers pull back under the weight of rising costs. 



He pointed to anecdotal reports from major retailers and consumer companies describing a “bifurcated economy,” in which wealthier Americans continue to spend freely but those at the bottom are trading down to cheaper goods. ““Consumers at the lower end are struggling and buying less and shifting to lower-cost products,” Powell said, noting the uneven effects of growth make the Fed’s balancing act even more complicated.“There is no risk-free path for policy,” Powell said. “We’re navigating the tension between our employment and inflation goals as carefully as we can.”
]]></content:encoded></item><item><title>Adobe software now has graphics acceleration via Wine!</title><link>https://www.reddit.com/r/linux/comments/1okacq8/adobe_software_now_has_graphics_acceleration_via/</link><author>/u/maseckt</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 20:12:33 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[A convenient way to install Adobe After Effects on Linux using Wine. Please stars this! This project right now on OBT, if u can check some errors on flatpak package, pls write on "issues on github" Github: https://github.com/relativemodder/aegnux]]></content:encoded></item><item><title>Dependency Management in Database Design (aka handling import cycles in large Go projects)</title><link>https://www.dolthub.com/blog/2025-10-29-dependency-management/</link><author>/u/nick_at_dolt</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 19:16:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Codebase organization and design is a vital skill, and a skill I was never taught it in school and had to pick up in the industry. I suspect it wasn’t taught for two reasons:Codebase design is a soft skill. Even when you can recognize a well-structured codebase, opinions differ on how to make code organized. It’s more art than science. Just look at the long list of incompatable practices on Wikipedia’s List of software development philosophiesThe importance of codebase architecture is much more apparent with larger, more complex code bases. Most of the code written in academia may never reach the level of complexity where good organization matters.Our project, Dolt, definitely meets that threshold where organization matters.
Dolt is the first SQL database with Git-style branches; it’s revision control for your data. And we’ve put a lot of work into it: the Dolt code repository contains 762k lines of Golang code (excluding generated files), broken up into 204 different packages. We also make go-mysql-server, the SQL engine used by Dolt which itself consists of 475k lines of Golang code in 59 different packages. The code is ten years old.That’s way more code than any one person can keep in their head. Breaking it up into packages helps, but how did we determine what the packages should be, and what code goes in what package?For any project big enough, you’re going to want packages. Not just because they speed up your compilation time and allow for partial recompiles, but because organizing code this way inherently leads to cleaner projects that are more easily understood. The act of breaking code up into components is called  and it’s an important part of code architecture. When talking about modularization, we call these individual components modules.A core principle of modern software design is the “single responsibility principle”, which states that each module only has one responsibility, and complex behavior is achieved by composing these modules. The intent is that if each module can be understood completely independently from the others, than modules can be developed in parallel with minimal risk of changes in one module breaking the behavior of other modules.At a glance, there appear to be two main tradeoffs to modularization, although these “tradeoffs” are usually upfront costs that are outweighed by the benefit of making code maintenance significantly easier:Modularizing your codebase requires forethought and slightly increases the overall complexity of the codebase. As a project grows, assumptions made when designing the structure of the code may prove to be incorrect, resulting in either modules with multiple responsibilities that are difficult to understand, or “leaky” abstraction modules that require knowledge of their inner workings in order to use correctly. When this happens the code may need to be restructured.Modularizing your code introduces the threat of dependency cycles: if module A depends on code symbols from module B, and module B depends on module C, then C should not depend on A. And while dependency cycles are often simple enough to untangle in theory, understanding their causes in complex code bases can be a challenge, and fixing them may require tedious refactors. In many languages, including Golang, dependency cycles between modules won’t even compile.It’s very easy for someone who doesn’t understand the layout of a codebase to accidentally introduce dependency cycles and then have trouble removing them. And dependency cycles are especially frustrating for devs because they feel like a barrier to writing clean code. In the moment, it can feel like modularization is making development . But it’s important to remember that:Without modularization, a developer that doesn’t understand the entire codebase might not be able to contribute , andWhile the code that is creating the dependency cycle feels simple and clean, it’s actually introducing a new relationship between code components that will make them difficult to separate in the future.Breaking a dependency cycle is often much simpler once the developer understands the responsibilities of the different components involved and their relationship to each other, and modularization makes that understanding a lot easier.This is best demonstrated by an example. This is a real contribution I made to Dolt where:The modularization of the codebase allowed me to develop features for one component without needing to fully understand the details of other related component.I was temporarily stymied by a dependency cycle.Identifying the best way to resolve the dependency cycle took time, but left me with a better understanding of how different components were connected, making it time well spent.Armed with this better understanding, the solution became simple.We recently added support for a feature we called nonlocal tables: Essentially, a user can configure one branch such that certain table names actually resolve to a table on another branch. The core functionality was easier to implement than expected. Next we added the ability for branches to have foreign key constraints on these tables, which proved to be more challenging.
We expected that foreign keys would have some odd behavior here, since changes on the referenced branch could cause these foreign key constraints to become violated. Since it’s already possible for version control commands to create similar situations, we already had a tool in place to handle this: for circumstances where it’s not possible to prevent violations, Dolt has a special tool to detect them after the fact: the  system procedure and the associated  CLI command.This command reads the database storage layer and determines whether or not any foreign key constraints on your branch are being violated. The logic is much simpler than every other part of this feature, and it was straightforward enough that we didn’t give it much thought in design; once we had the ability to correctly resolve table names, all we had to do was allow the validation logic to depend on the name resolution logic. It should have been a one-line change.And yet, figuring out how to properly expose the name resolution logic to the validator turned out to be the hard part. But why?Well, let’s look at the package structure for Dolt. The logic for executing  makes use the following packages, among others: - A collection of primitive types and interfaces required for running a database. Many of the types we use are defined here. This package is a great example of a common abstraction that other modules can depend on without needing to depend on each other. But it means that anything in this module cannot depend on any of the modules that make use of it. This means that there are still lots of interfaces that can’t go in this package.
This package contains a  interface, describing a table that a database engine can interact with.dolt/go/libraries/doltcore/doltdb - This defines the core types that power Dolt’s data structures, and defines core operations on these types. The logic for validating foreign keys is defined in this package.
This package contains a  type, representing a table in storage. This type alone does not have the necessary context to be used by an engine, so it cannot implement .dolt/go/libraries/doltcore/sqle/dsess - This contains the logic responsible for maintaining the current state of a database session, including transactions.dolt/go/libraries/doltcore/sqle - This implements a SQL engine on top of the storage layer. Evalutating references to other branches happens here, because the result of the evaluation depends on the current transaction, otherwise you might get concurrency issues.
This package contains a  type, which implements . It can be constructed from a .dolt/cmd/dolt/commands/cvcmds - The implementation of the command line command for validating constraints, including foreign keys.Something else I was never taught in school: how to make proper UML diagrams.These five packages form a clean chain of dependencies: each package depends on every package listed above it, and none of the packages below it. And it means that when developing any of these packages, as long as you don’t change the behavior of its exported functions, you can safely ignore all the packages below it.So let’s look back at the thing that we thought would be a one-line fix:“all we had to do was allow the validation logic to depend on the name resolution logic”We can now see that there are three separate problems with this proposal:We can’t modify the validation logic in  to depend on the new table name resolution logic… because the table name resolution logic depends on branch reference resolution, which is implemented in . This is a dependency cycle.The command itself is implemented in the top level package , and is thus allowed to depend on everything. The engine has public functions that can resolve branch names, but those functions have parameters that the command couldn’t provide, because some of that context is encapsulated by the  package.Finally, while both of these packages have methods for interacting with tables, the types used to represent a table have a different shape, different responsibilities, and don’t implement a common interface.Again, it may feel like modularization is getting in our way by preventing us from calling functions or accessing state that we need. But the package layout also makes it clear that even if we could simply glue together these two components together, doing so would expose their internal state to each other in a way that could be complicated to refactor later. It’s worth putting in the extra legwork now to avoid this, and in doing so might suggest ways to keep the code readable.So given all that, what’s the cleanest way to solve these problems?Could we break the cycle by cleaving off some part of the  package, and then having both the engine and storage layers depend on this new package? Probably not: the functionality we’re trying to isolate depends on the session management code in the  package: separating it out would prevent one cycle, but create another.Perhaps instead, we provide a way to resolve branch references without needing access to the current transaction? Then we could put all the branch resolution code in the  storage layer. This could probably be done, but it would be a major change and would need to be done very carefully. We’d have to duplicate some of the lookup logic in the engine and in storage, it would be tricky to get right, and the cost of getting it wrong could be subtle concurrency bugs: no database wants that.Instead, the best way to avoid dependency cycles is to have both modules should depend on a common “abstraction” instead. Usually this means an interface type. Interfaces are a great tool when you have a simple problem statement and you already how to solve that problem, and you’re just trying to avoid introducing new dependencies.We have a simple problem statement: resolve a table name to a table, using the new rules for referencing tables on other branches. And we already have code that solves that problem. But the logic that requires that code cannot depend on it. So dependency inversion says: depend on an abstraction. And we accomplish that in three easy steps.Step 1: The low-level package creates an interface that describes the shape of the operation we need.We define a new interface  in the  package that describes the shape of the operation we need:Step 2: The higher-level package provides an implementation.In the  package, we provide an implementation. This requires adding some new functions to the package:A function that can return the underlying  type used by the storage layer instead of preemptively constructing the higher-level type used by the engine.An exported function that returns a  value for use by other packages.Step 3: The top level package does dependency-injectionWith these changes in place, the top level  package can get a  from the engine and pass it as an additional parameter to the relevant storage layer calls. This allows the new name resolution rules to influence storage operations without creating any additional dependencies.As presented, this seems like a simple and obvious solution. And it  a simple solution… but it’s only obvious when viewed in the context of the code’s organization. It’s obvious that this approach won’t  create dependency cycles or expose internal state, but we need to understand the package boundaries to see why other, similar-looking approaches .Implementing this feature helped me better understand the exact relationship between the many different packages Dolt uses when performing even simple database operations. It ensured that any changes I made to boundaries between packages were thoughtful and deliberate, and it helped me identify future opportunities to clean up some of these interfaces and make them more usable.I’m not sure if good codebase architecture can be taught: maybe it can only be learned. And I definitely learned something about Dolt’s design, not just the how but the . And that lesson is not only going to help me now as I develop Dolt, but also influence any codebase design I may do in the future.I’m biased, but I think Dolt is a pretty well-designed piece of software. It’s not perfect and it’s had it’s growing pains, but it has a solid core that’s been fun to work with. Databases have a ton of complexity, and we’ve done a good job of managing that complexity such that we can continue to add cool new features.If you have a feature you want to see in Dolt, [drop us a line on Discord] and we’ll scope it out. We take user requests seriously when deciding our priorities. If you’re looking for cool open-source projects to contribute to, we’re always welcoming contributors and are happy to help you get set up. We even have a good first issue tag on GitHub.]]></content:encoded></item><item><title>Windows wouldn&apos;t let me access my HDD but Linux did</title><link>https://www.reddit.com/r/linux/comments/1ok8of4/windows_wouldnt_let_me_access_my_hdd_but_linux_did/</link><author>/u/snypse_</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 19:08:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What happens if a goroutine holding a sync.Mutex gets preempted by the OS scheduler?</title><link>https://www.reddit.com/r/golang/comments/1ok8j3j/what_happens_if_a_goroutine_holding_a_syncmutex/</link><author>/u/Alihussein94</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 19:02:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[What will happen when a Goroutine locks a variable (sync.Mux) and then the Linux kernel decides to move the thread that this goroutine is running on to a blocked state, for instance, because higher higher-priority thread is running. Do the other Goroutines wait till the thread is scheduled to another CPU core and then continue processing, and then finally unlock the variable?]]></content:encoded></item><item><title>Zig&apos;s New Async I/O (Text Version)</title><link>https://andrewkelley.me/post/zig-new-async-io-text-version.html</link><author>/u/BrewedDoritos</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:51:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[This is a preview of the new async I/O primitives that will be available in the upcoming Zig 0.16.0, to be
released in about 3-4 months. There is a lot more to get into, but for now here is an introduction
into some of the core synchronization API that will be available for all Zig code to use.To begin, let's try to keep it simple and understand the basics, and then we'll then slowly add more
asynchronous things into it.With our first example, there is nothing asynchronous here. It's basically "Hello, World!" in Zig.const std = @import("std");

pub fn main() !void {
    doWork();
}

fn doWork() void {
    std.debug.print("working\n", .{});
    var timespec: std.posix.timespec = .{ .sec = 1, .nsec = 0 };
    _ = std.posix.system.nanosleep(&timespec, &timespec);
}0s $ zig run example0.zig
0s working
1s $Next, we're going to set up a little bit. Still not using async/await yet, but I need some tools in my
toolbox before we add complexity.const std = @import("std");
const Io = std.Io;
const Allocator = std.mem.Allocator;
const assert = std.debug.assert;

fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    doWork(io);
}

fn doWork(io: Io) void {
    std.debug.print("working\n", .{});
    io.sleep(.fromSeconds(1), .awake) catch {};
}

pub fn main() !void {
    // Set up allocator.
    var debug_allocator: std.heap.DebugAllocator(.{}) = .init;
    defer assert(debug_allocator.deinit() == .ok);
    const gpa = debug_allocator.allocator();

    // Set up our I/O implementation.
    var threaded: std.Io.Threaded = .init(gpa);
    defer threaded.deinit();
    const io = threaded.io();

    return juicyMain(gpa, io);
}0s $ zig run example0.zig
0s working
1s $Setting up a  implementation is a lot like setting up an allocator.
You typically do it once, in main(), and then pass the instance throughout the application.
Reusable code should accept an Allocator parameter if it needs to allocate, and it should accept
an Io parameter if it needs to perform I/O operations.In this case, this is an Io implementation based on threads. This is not using
KQueue, this is not using IO_Uring, this is not using an event loop. It is a  implementation
of the new  interface.This setup will be the same in all the examples, so now we can focus on our example code, which is the same
as last time. Still nothing interesting - we just call  which of course is just calling sleep().Redundant setup code omitted from here on out.fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    var future = io.async(doWork, .{io});

    future.await(io); // idempotent
}

fn doWork(io: Io) void {
    std.debug.print("working\n", .{});
    io.sleep(.fromSeconds(1), .awake) catch {};
}0s $ zig run example0.zig
0s working
1s $Now we're using async/await to call doWork. What async/await means to Zig is to  the
 of the function to the  of the function.This code is the same as before. It's exactly the same, because we didn't put any code between the async
and await. We do the call, and then immediately wait for the return.In the next example, we have two things at the same time:fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    var a = io.async(doWork, .{ io, "hard" });
    var b = io.async(doWork, .{ io, "on an excuse not to drink Spezi" });

    a.await(io);
    b.await(io);
}

fn doWork(io: Io, flavor_text: []const u8) void {
    std.debug.print("working {s}\n", .{flavor_text});
    io.sleep(.fromSeconds(1), .awake) catch {};
}0s $ zig run example3.zig
0s working on an excuse not to drink Spezi
0s working hard
1s $If you look carefully, you can see that it did not wait two seconds; it waited one second because
these operations are happening at the same time. This demonstrates why using async/await is useful -
you can express asynchrony. Depending on the I/O implementation that you
choose, it may be able to take advantage of the asynchrony that you have
expressed and make your code go faster. For example in this case,
 was able to do two seconds of work in one second
of actual time.Let's start to bring the example closer to a real world scenario by introducing .fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });

    try a.await(io);
    try b.await(io);
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}It's the same code as before, except the first task will return an error.Guess what happens when this code is run?0s $ zig run example4.zig
0s working on an excuse not to drink Spezi
1s error(gpa): memory address 0x7f99ce6c0080 leaked:
1s /home/andy/src/zig/lib/std/Io/Threaded.zig:466:67: 0x1053aae in async (std.zig)
1s     const ac: *AsyncClosure = @ptrCast(@alignCast(gpa.alignedAlloc(u8, .of(AsyncClosure), n) catch {
1s                                                                   ^
1s /home/andy/src/zig/lib/std/Io.zig:1548:40: 0x1164f94 in async__anon_27344 (std.zig)
1s     future.any_future = io.vtable.async(
1s                                        ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:8:21: 0x116338a in juicyMain (example4.zig)
1s     var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });
1s                     ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:35:21: 0x1163663 in main (example4.zig)
1s     return juicyMain(gpa, io);
1s                     ^
1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig)
1s             const result = root.main() catch |err| {
1s                                     ^
1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig)
1s     asm volatile (switch (native_arch) {
1s     ^
1s 
1s thread 1327233 panic: reached unreachable code
1s error return context:
1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651a8 in await (std.zig)
1s             return f.result;
1s             ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:10:5: 0x11633e8 in juicyMain (example4.zig)
1s     try a.await(io);
1s     ^
1s 
1s stack trace:
1s /home/andy/src/zig/lib/std/debug.zig:409:14: 0x103e5a9 in assert (std.zig)
1s     if (!ok) unreachable; // assertion failure
1s              ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:27:17: 0x1163698 in main (example4.zig)
1s     defer assert(debug_allocator.deinit() == .ok);
1s                 ^
1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig)
1s             const result = root.main() catch |err| {
1s                                     ^
1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig)
1s     asm volatile (switch (native_arch) {
1s     ^
1s fish: Job 1, 'zig run example4.zig' terminated by signal SIGABRT (Abort)
1s $The problem is that when the first  activates, it skips the second  which
is then caught by the leak checker.This is a bug. It's unfortunate though, isn't it? Because we would like to write the code this way.fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });

    const a_result = a.await(io);
    const b_result = b.await(io);

    try a_result;
    try b_result;
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}We do the awaits, then we do the tries. This will fix the problem.0s $ zig run example5.zig
0s working on an excuse not to drink Spezi
1s error: OutOfMemory
1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651d8 in await (std.zig)
1s             return f.result;
1s             ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:13:5: 0x1163416 in juicyMain (example5.zig)
1s     try a_result;
1s     ^
1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:38:5: 0x11636e9 in main (example5.zig)
1s     return juicyMain(gpa, io);
1s     ^
1s $This failed successfully. The error was handled and no resources leaked. But
it's a footgun. Let's find a better way to express this...This is where  comes in. cancellation is an extremely handy primitive,
because now we can use , , and  like normal,
and not only do we fix the bug, but we also get more optimal code.fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    defer a.cancel(io) catch {};

    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });
    defer b.cancel(io) catch {};

    try a.await(io);
    try b.await(io);
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}Thanks to cancellation, we now get instant results, because the moment that the first
task returns an error, the cancels get run.0s $ zig run example6.zig
0s working on an excuse not to drink Spezi
0s error: OutOfMemory
0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:13:5: 0x116348c in juicyMain (example6.zig)
0s     try a.await(io);
0s     ^
0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:38:5: 0x1163909 in main (example6.zig)
0s     return juicyMain(gpa, io);
0s     ^
0s $ is your best friend, because it's going to prevent you from leaking the
resource, and it's going to make your code run more optimally. is trivial to understand: it has identical semantics as , except
that it also requests cancellation. The conditions under which cancellation requests are honored
are defined by each I/O implementation.Both  and  are idempotent with respect to themselves and each other.Next, let's introduce another real-world scenario: .
In this case, we allocate a string on success, which the caller needs to manage.fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    defer if (a.cancel(io)) |s| gpa.free(s) else |_| {};

    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });
    defer if (b.cancel(io)) |s| gpa.free(s) else |_| {};

    const a_string = try a.await(io);
    const b_string = try b.await(io);
    std.debug.print("finished {s}\n", .{a_string});
    std.debug.print("finished {s}\n", .{b_string});
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) ![]u8 {
    const copied_string = try gpa.dupe(u8, flavor_text);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
    return copied_string;
}Now we see why  and  have the same API.
The deferred cancel calls above free the allocated resource, handling both
successful calls (resource allocated) and failed calls (resource not allocated).0s $ zig run example7.zig
0s working on an excuse not to drink Spezi
0s working hard
1s finished hard
1s finished on an excuse not to drink Spezi
1s $The important thing here is that by doing resource management like this, we are
able to write standard, idiomatic Zig code below, using  and 
like normal without worrying about special resource management cases.In this example we have a producer sending one item across an unbuffered queue to a consumer.fn juicyMain(io: Io) !void {
    var queue: Io.Queue([]const u8) = .init(&.{});

    var producer_task = io.async(producer, .{
        io, &queue, "never gonna give you up",
    });
    defer producer_task.cancel(io) catch {};

    var consumer_task = io.async(consumer, .{ io, &queue });
    defer _ = consumer_task.cancel(io) catch {};

    const result = try consumer_task.await(io);
    std.debug.print("message received: {s}\n", .{result});
}

fn producer(
    io: Io,
    queue: *Io.Queue([]const u8),
    flavor_text: []const u8,
) !void {
    try queue.putOne(io, flavor_text);
}

fn consumer(
    io: Io,
    queue: *Io.Queue([]const u8),
) ![]const u8 {
    return queue.getOne(io);
}We use  to spawn the producer and  to spawn the consumer.0s $ zig run example8.zig
0s message received: never gonna give you up
0s $This incorrectly succeeds. Depending on your perspective, we either got "lucky" or "unlucky" due
to the thread pool having spare concurrency that happened to be available.To observe the problem, we can artificially limit the  instance to
use a thread pool size of one:// Set up our I/O implementation.
    var threaded: std.Io.Threaded = .init(gpa);
    threaded.cpu_count = 1;
    defer threaded.deinit();
    const io = threaded.io();

    return juicyMain(io);
}Now that it's only using one thread, it deadlocks, because the consumer is waiting to get something from
the queue, and the producer is scheduled to run, but it has not run yet.The problem is that we needed concurrency, but we asked for asynchrony.In order to fix this, we use  instead of .
This one can fail with error.ConcurrencyUnavailable.fn juicyMain(io: Io) !void {
    var queue: Io.Queue([]const u8) = .init(&.{});

    var producer_task = try io.concurrent(producer, .{
        io, &queue, "never gonna give you up",
    });
    defer producer_task.cancel(io) catch {};

    var consumer_task = try io.concurrent(consumer, .{ io, &queue });
    defer _ = consumer_task.cancel(io) catch {};

    const result = try consumer_task.await(io);
    std.debug.print("message received: {s}\n", .{result});
}

fn producer(
    io: Io,
    queue: *Io.Queue([]const u8),
    flavor_text: []const u8,
) !void {
    try queue.putOne(io, flavor_text);
}

fn consumer(
    io: Io,
    queue: *Io.Queue([]const u8),
) ![]const u8 {
    return queue.getOne(io);
}0s $ zig run example10.zig
0s message received: never gonna give you up
0s $Now the code is fixed because we correctly expressed that we needed concurrency, which
 honored by oversubscribing.If I add  which truly limits the executable to one thread,
oversubscription is not available, causing this output:error: ConcurrencyUnavailable
/home/andy/src/zig/lib/std/Io/Threaded.zig:529:34: 0x1051863 in concurrent (std.zig)
    if (builtin.single_threaded) return error.ConcurrencyUnavailable;
                                 ^
/home/andy/src/zig/lib/std/Io.zig:1587:25: 0x1158b5f in concurrent__anon_26591 (std.zig)
    future.any_future = try io.vtable.concurrent(
                        ^
/home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:9:25: 0x1157198 in juicyMain (example10.zig)
    var producer_task = try io.concurrent(producer, .{
                        ^
/home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:48:5: 0x115776a in main (example10.zig)
    return juicyMain(io);
    ^There are proof-of-concept  implementations using IoUring and KQueue combined
with stackful coroutines which show a lot of promise, however that work depends on some language
enhancements to be practical. There is also ongoing design work about stackless coroutines. Here
are some relevant issues to track for those interested:These APIs are not set in stone. It will probably take a few iterations to
get it right. Please try them out in  and let
us know how it goes! Let's collaborate on making the I/O interface practical
and optimal.]]></content:encoded></item><item><title>With the release of Rust 1.91, Arm is now a Tier 1 supported architecture on Windows</title><link>https://github.com/rust-lang/rust/pull/145682</link><author>/u/Balance-</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:44:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[ is now a Tier 1 target with host tools for Rust, meaning ARM64 Windows with MSVC is "guaranteed to work" as a fully supported platform. This means the Rust project provides official binary releases, runs automated testing after every change to ensure builds and tests pass, and supports running development tools like  and  natively on ARM64 Windows machines. In practical terms, developers can now confidently use ARM64 Windows devices (like Windows on ARM laptops) both as compilation targets and as development platforms with the same level of support as established platforms like x86_64 Windows and ARM64 macOS.]]></content:encoded></item><item><title>Rust 1.90.1 is out</title><link>https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/</link><author>/u/manpacket</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:39:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.91.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!The Rust compiler supports a wide variety of targets, but
the Rust Team can't provide the same level of support for all of them. To
clearly mark how supported each target is, we use a tiering system:Tier 3 targets are technically supported by the compiler, but we don't check
whether their code build or passes the tests, and we don't provide any
prebuilt binaries as part of our releases.Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but
we don't execute the test suite on those platforms: the produced binaries
might not work or might have bugs.Tier 1 targets provide the highest support guarantee, and we run the full
suite on those platforms for every change merged in the compiler. Prebuilt
binaries are also available.Rust 1.91.0 promotes the  target to Tier 1 support,
bringing our highest guarantees to users of 64-bit ARM systems running Windows.
Add lint against dangling raw pointers from local variablesWhile Rust's borrow checking prevents dangling references from being returned, it doesn't
track raw pointers. With this release, we are adding a warn-by-default lint on raw
pointers to local variables being returned from functions. For example, code like this:Note that the code above is not unsafe, as it itself doesn't perform any dangerous
operations. Only dereferencing the raw pointer after the function returns would be
unsafe. We expect future releases of Rust to add more functionality helping authors
to safely interact with raw pointers, and with unsafe code more generally.These previously stable APIs are now stable in const contexts:Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Bluefin Autumn 2025: We visit the Bazaar</title><link>https://docs.projectbluefin.io/blog/2025-10-28-bluefin-autumn/</link><author>/u/fizzyizzy05</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:08:39 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Guardians, today Bluefin GTS switched its base from Fedora 41 to Fedora 42. The gathering of raptors has begun. In a two weeks Bluefin (aka ) releases on Fedora 43 and we will start the cycle all over again!Looking for Fedora 43? That's here too in , and will roll out to  users in 2 weeks. It's tough to write two of these, so we'll likely just move to spring/autumn announcements and whenever major things land. When  upgrades I will post it as an addenum in the discussion thread for this post.As it ends up F43 will be coming to  while we're in Atlanta, GA, for KubeCon + CloudNativeCon, come say hello! As a  reference architecture we tend to align with the release cadence of other projects. This usually means that I'm on the road when there's a Bluefin release happening, so we do status reports like this depending on where we are in the world at the time, and to ensure transparency. It's also our chance to gather with attendees and get feedback on how we can make Bluefin better and gather feedback.You'll receive this update during your next update window, or you can run an update manually by clicking on this icon:If you've never experienced a Bluefin upgrade before, McPhail has a full writeup. Here's the major release information:Bluefin is an operating system for your computer. It is designed to be installed on a device upgrade for the life of the hardware – we accomplish this by sharing the maintenance and care of our systems together as a community. It is designed to be as “zero touch” as possible by providing a curated GNOME experience.Bluefin GTS (aka ) is our standard release, designed to be one cycle behind the most current Fedora release. This one's been in the oven for about six months and is ready to go. In a few weeks the  branch will move on to Fedora 43. If you're brand new you can use the website to pick the right image or select from the grid below:This unidentified Dromeasaur is by Dr. Natalia Jagielska, a world renowned expert paleontologist and paleoartist! We reached out to work with her on bringing her artwork and style to Bluefin, and she said yes! This rendition will be revealed in November, or you can just manually pick it in the wallpaper chooser.I am so stoked about this, an actual scientist! We're retconning that this is just Bluefin enjoying a nice day at the lake. We have  more wallpapers from her coming soon. I have come to really appreciate the world of flying reptiles. They are terrifying.Natalia's artwork was vectorized and remastered by Delphic Melody, please consider donating so that the collaboration can continue!There are a few major changes from a Bluefin perspective that we've been looking forward to:Installation Experience​​The Anaconda web-ui installer is now the default installer, dramatically improving the experience. We say goodbye to the old GTK Anaconda installer.We'll be automatically refreshing all the Bluefin ISOs once a month to ensure the installation media is fresh.Bazaar makes its debut in Bluefin GTS! All Bluefins are now just using the Bazaar flatpak. You're in for a treat:It's been super awesome seeing Bazaar move from a random project we found on r/gnome to what is effectively now the premier app store experience for FlatHub and Linux. You can help out tremendously by sponsoring the author.This is also a major milestone for Bluefin since we've effectively done our part for the GNOME and FlatHub ecosystems and can now consider application installation a solved problem, we can introduce new things into Bluefin as a flatpak to begin with and move us away from distribution specific formats.I am finding more applications now than I ever have. It's also a milestone for all Linuxes since flatpak's upcoming release gives us the flexibility to do this in a proper way with full lifecycle management. We can now be more flexible with the applications we can ship mid-cycle by plopping a file in . Those of you making custom images will really take advantage of this!Shoutout to Sebastian Wick for this work in Flatpak and working on the next release of this cool tech!We're committed to a future where authors deliver their applications how they see fit. This should be decoupled from the operating system.Speaking of packages, we've been doing more work engaging with Homebrew developers, check out this interview I did with Workbrew talking about our hopes and dreams:Let us know if you're interested in working on Homebrew for Linux, we have opened a homebrew tap so that we can interate on bringing cool new things to you. A huge shoutout goes to Yulian Kuncheff and Ahmed Adan for spearheading this effort, please consider donating!The fonts have been a disaster for a long time, we're finally ripping the bandaid off and removing a bunch of fonts from the image. For you command line nerds you can install any of the fonts listed in Homebrew or use a tool like Embellish to install more fonts.If you're in developer mode you can bring the monospace fonts back with .We've dropped the GNOME Quick Settings extension for tailscale in favor of the upstream system tray implementation. For more information, check the docs, this requires manual set up.The tailscale experience is still not where it needs to be, but now that Tailscale has started work on an official system tray implementation we expect this to solidify over the next few upstream releases.After a hiatus we've finally refactored the Homebrew management in Bluefin. We're adding back some convenience commands:Extinction is a natural part of life. After a deprecation cycle the following images are now removed:: Due to Nvidia's software support changes we can no longer support the older closed modules for Nvidia cards. Not many people are using these, either migrate to the  images or move to a stock image to use the built in kernel drivers.: Not many people were using these, they have also been removed.As usual most of the changes we do in GitHub to deliver Bluefin and not so much in the image itself. Major parts of the Bluefin repository have been cleaned up to align with the improvements and lessons learned from building Bluefin LTS earlier in the year. This has been the bulk of the work in the past few weeks.Bluefin has significantly been simplified, now would be a great time to contribute as we've brought the repository up to the state of more modern  projects like Bluefin LTS. and  will be publishing on Tuesdays from now on instead of Saturdays. Publishing on Saturday nights is an artifact of pre-automation "reserved time" for testing before a weekly release. This matches the same release schedule as Bluefin LTS.Bluefin is a deinonychus, and may snap at you occasionally. Four year olds can get feisty of so there might be issues that you discover that we haven't seen before. Filing issues is always appreciated.We also accept donations to sponsor the infrastructure and artwork.Sometimes starting in open source can be a real barrier if you don't know where to start. Don't have the skills to do cloud native things yet? Here's a good way to help out FlatHub. Flatpaks rely on what we call "runtimes" to ensure that the application has the dependencies it needs to run. Do a  to check them out:This is important work because we want applications to be updated to the latest runtimes for security reasons. As it turns out, many of these applications have OPEN PULL REQUESTS already with people updating the runtime, you just need to find the app, run the updated version by following the instructions, and then report back to the Flatpak maintainer that the new app is working great (or broken!). Since GNOME 49 just released, there's plenty to do, so feel free to dive in and get started! Also remember, this work helps all of FlatHub, we're explictly sending new volunteers to help upstream.FlatHub is critical to the desktopWe choose to help move application development forward via FlatHub instead of fragmenting the ecosystem with distribution-specific packaging. This includes shipping a premier FlatHub experience out of the box. You do not have to worry about misconfigured and low-quality Fedora flatpak remotes and packages on Bluefin systems.Find your favorite app and see if there's a test build available for a new runtime. And if you have the skills to port applications to new runtimes, now is the time to flex. 😄Check out store.projectbluefin.io and pick up some dino merch. Thanks to John Johnson for ensuring our coffee mug game is up to snuff:Nothing makes ops people happier than uneventful things.Today is really like any other, we just updated a few tags, you always have the option to go to any version we support at any time. Wether you like the chill vibe of  or the refined aggresiveness of  , the raptor abides.Here's the current lay of the land:Advanced users and testersNOTE: The  and  branches will move to F43 in two weeks.Desktop DevOps folks wanted!​​Bluefin is an active predator and is constantly hungry. You can help keep Bluefin healthy by becoming a contributor! We are an open source project and accept contributions:As a cloud native project we are always looking for contributors with skills in Podman, Docker, CI/CD, GitHub Actions, and good ole bash.Let's take a look at our contributor health, and celebrate the amazing folks who have come together to bring you Bluefin! We use LFX Insights to measure our project health. First note that my results here are skewed, since I am either usually just merging or telling a bot it's ok to do something. This also does not include the rest of Universal Blue. Yes, Aurora people basically maintain both, haha.This next one surprised me, I was expecting 20 or 30ish at best. Nice work ya'll!Haha yep, I can't hide from the data though, free me from this!Feel free to browse around and learn cool things about Bluefin's creators.After KubeCon we head into the holidays, where things will slow down significantly. We've been in the lab with mad doctor Timothée Ravier and have been cooking up something. We expect that this will change the course of Bluefin for the better, forever. We can't wait to show you, until then, enjoy!]]></content:encoded></item><item><title>I cannot access my node port on my window machine why</title><link>https://www.reddit.com/r/kubernetes/comments/1ok65hs/i_cannot_access_my_node_port_on_my_window_machine/</link><author>/u/Perfect_Mix_1524</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 17:33:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am learning kubernetes now. I got stuck in a wired problem. I am not able to access the nodeport on my window machine. Below is my configuration file. I am hitting the route  but no response. Can anyone help to identify the issue.apiVersion: apps/v1 kind: Deployment metadata: name: posts-depl spec: selector: matchLabels: app: posts template: metadata: labels: app: posts spec: containers: - name: posts image: test1 imagePullPolicy: Never --- apiVersion: v1 kind: Service metadata: name: post-srv spec: type: NodePort selector: app: posts ports: - name: posts protocol: TCP port: 3000 targetPort: 3000 nodePort: 32504 ]]></content:encoded></item><item><title>Drawing on Linux</title><link>https://www.reddit.com/r/linux/comments/1ok4klq/drawing_on_linux/</link><author>/u/Madcat789</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:35:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hello there. I plan on installing Linux into the next computer I get, I'm thinking either Mint or Pop. I like to game, and I like to draw. I use a XP-Pen 15.6 Drawing Tablet and ClipStudioPaint for my program. Are there any programs equivalent to the ClipStudioPaint that I can use for drawing? As far as I know, there are no branches or forks of CSP that was designed for Linux and I'd like to know which one to take a gander at before I go ahead and install. Look up some reviews and comparisons, y'know?So far, it looks like Mint+Krita is the way to go. Thank you guys. Now to either construct or acquire a PC that I can rip Windows out of and install Mint unto.]]></content:encoded></item><item><title>Rust in Production Podcast: How Cloudflare handles 90 million requests per second with Pingora</title><link>https://corrode.dev/podcast/s05e03-cloudflare/</link><author>/u/mre__</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:26:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[How do you build a system that handles 90 million requests per second? That’s the scale that Cloudflare operates at, processing roughly 25% of all internet traffic through their global network of 330+ edge locations.In this episode, we talk to Kevin Guthrie and Edward Wang from Cloudflare about Pingora, their open-source Rust-based proxy that replaced nginx across their entire infrastructure. We’ll find out why they chose Rust for mission-critical systems handling such massive scale, the technical challenges of replacing battle-tested infrastructure, and the lessons learned from “oxidizing” one of the internet’s largest networks.
    CodeCrafters helps you become proficient in Rust by building real-world,
    production-grade projects. Learn hands-on by creating your own shell, HTTP
    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.
  
    Start for free today and enjoy 40% off any paid plan by using
    this link.
  Cloudflare is a global network designed to make everything you connect to the Internet secure, private, fast, and reliable. Their network spans 330+ cities worldwide and handles approximately 25% of all internet traffic. Cloudflare provides a range of services including DDoS protection, CDN, DNS, and serverless computing—all built on infrastructure that processes billions of requests every day.Kevin Guthrie is a Software Architect and Principal Distributed Systems Engineer at Cloudflare working on Pingora and the production services built upon it. He specializes in performance optimization at scale. Kevin has deep expertise in building high-performance systems and has contributed to open-source projects that power critical internet infrastructure.Edward Wang is a Systems Engineer at Cloudflare who has been instrumental in developing Pingora, Cloudflare’s Rust-based HTTP proxy framework. He co-authored the announcement of Pingora’s open source release. Edward’s work focuses on performance optimization, security, and building developer-friendly APIs for network programming.]]></content:encoded></item><item><title>Qt Creator 18 released</title><link>https://www.qt.io/blog/qt-creator-18-released</link><author>/u/jlpcsl</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:23:14 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[We are happy to announce the release of Qt Creator 18!Qt Creator 18 adds experimental support for Development Containers and many more improvements.Development Container SupportQt Creator 18 adds support for development containers to automate setting up the development environment of a project. It detects a "devcontainer.json" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. Enable the extension to use this functionality. Find out more. that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.The notifications received a facelift and are now part of the progress notification popups. Y But remember faster ways of navigating your code, such as Locator filters for opening files or jumping to specific class or symbol, , , the  and  views, the edit location history  and the corresponding keyboard shortcuts, and  and the corresponding keyboard shortcuts.For the C++ support wWe added a configuration for various tools on remote Linux devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new  setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to ".gitignore".]]></content:encoded></item><item><title>Kubuntu project using blatant AI in their blog posts:</title><link>https://www.reddit.com/r/linux/comments/1ok46ip/kubuntu_project_using_blatant_ai_in_their_blog/</link><author>/u/Makerinos</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:20:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Something I noticed from looking at the release blog posts is that the one announcing their latest release stinks blatantly of AI usage. Previous release blog posts are significantly shorter, use a much looser format, and their wording doesn't have that AI 'feeling' like the triple adjectives and frequent em-dashes.I know this might sound like a minor thing...but it doesn't bode well, and feels unprofessional for what is supposed to be a serious distro.]]></content:encoded></item><item><title>How Google, Amazon, and CrowdStrike broke millions of systems</title><link>https://newsletter.techworld-with-milan.com/p/how-google-amazon-and-crowdstrike</link><author>/u/milanm08</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:07:38 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Three companies control most of the internet. When they break, millions of systems fail at once.In October 2025, a race condition in AWS’s DNS automation caused a regional endpoint to be emptied. 113 services crashed. Recovery took 15 hours. Two months earlier, a null pointer in Google Cloud caused Service Control, the gatekeeper for every API request, to crash. 50+ services went dark for seven hours. And in July 2024, CrowdStrike deployed a bad configuration file to kernel-mode drivers worldwide. 8.5 million Windows machines are locked in boot loops. Airlines grounded flights, and hospitals canceled surgeries.These weren’t sophisticated attacks or infrastructure failures. They were simple bugs: a race condition, a missing null check, a bad config file. But at this scale, simple bugs become catastrophic.This post breaks down what actually happened in each incident. In particular, we will talk about:1. DNS race condition caused the large AWS outage. 2. How a null pointer at Google Cloud crashed the Internet. 3. A single deployment that made the whole world stop. 4. Bonus: When Azure’s safety checks failed.on October 20, 2025DynamoDB DNS was restored at 2:25 AM PDT, roughly 3 hours after the outage. But cascading failures kept services down for 12 more hours.For engineers building resilient systems, this incident reveals uncomfortable truths about race conditions, dependency chains, and the hidden fragility of cloud-native architectures.AWS’s DynamoDBRoute 53race conditions were acceptable due to eventual consistencyOn October 20, this assumption proved to be wrong. DNS Enactor #1 experienced unusual delays applying an old plan. Meanwhile, the DNS Planner continued generating new plans. DNS Enactor #2 raced through these newer plans and executed a cleanup process, deleting “stale” plans just as Enactor #1 completed its delayed run. The staleness check, performed hours earlier at the start of processing, was now meaningless. Enactor #1 overwrote Route 53 with outdated data. Enactor #2 detected the “old” plan and triggered deletion, emptying all IP addresses from dynamodb.us-east-1.amazonaws.com. The regional DynamoDB endpoint vanished from DNS entirely.the race condition existed in all regions but had only been triggered in one.DynamoDB recovered at 2:25 AM PDT, just under 3 hours after the incident began. But this is where the story gets worse.The DropletWorkflow Manager (DWFM)death spiral, unable to make forward progresslease-based systems work perfectly under normal load but collapse under stress when processing time exceeds timeout periodsWith EC2 launches impaired, Lambda couldn’t create execution environments. ECS, EKS, and Fargate couldn’t start containers. Network Load Balancers experienced a different problem: health checks flapped on newly launched instances due to network state lag in the Network Manager, which was processing a massive backlog of delayed state changes. automatic DNS failover across availability zoneDynamoDB DNS → DynamoDB APIs → DWFM → EC2 launches → Network Manager → NLB health checks → Lambda, ECS, EKS, Fargate → 100+ dependent services.DNS record brought down 113 servicesHow AWS outage made water beds stuckThe Eight SleepCEO Matteo Franceschetti AWS post-mortemYour recovery automation needs circuit breakers.Map your dependency chains and identify blast radius multipliers.Test when recovery and failure happen simultaneously.Rethink what multi-region actually means.The uncomfortable truth: even with world-class talent, formal verification methods, extensive chaos engineering, and decades of operational experience, distributed systems remain fundamentally hard. The complexity of hyperscale creates emergent failure modes that are nearly impossible to predict or fully test.The question isn’t “could this happen to us?” The question is, when it happens to you, will you survive it? The next outage is already brewing somewhere in your stack. Your job is to ensure you’ve designed enough redundancy, eliminated enough tight coupling, and built enough circuit breakers so that when a latent race condition triggers, your systems degrade gracefully.June 12, 2025, at 10:45 AM PDGoogle’s Spanner databaseA missing null check caused 50+ services across 40+ regions to crash. when Service Control fails, Google Cloud fails The system runs as a distributed control plane with regional instances sharing policy metadata through global Spanner replication. Policy updates propagate worldwide within seconds. Under normal conditions, this provides consistent authorization decisions with minimal latency. During this outage, it distributed failure at the speed of light.Service Control’s responsibilities, quota enforcement, policy validation, audit logging, and usage metering make it essential for everything. Google Workspace products depend on it. Third-party apps depend on it. Google’s own services depend on it. Distributed systems engineers call this “fate-sharing” architecture. Hundreds of services are tied to the health of a single component.The bug was simple. The new code failed to validate policy fields before processing them: Encountering blank values triggered an unhandled exception that crashed the entire Service Control process.bug remained dormant for 14 daysIt is interesting that static analysis missed it, but also code reviews and testing.SpannerThe corrupted policy data entered Spanner and reached every region before engineers could intervene. No validation checkpoints existed before global distribution, no schema validation, and no content checks. The diagram below shows why instant global replication became a failure amplifier. Note that the same consistency mechanism that makes Google Cloud reliable under normal conditions guaranteed that a single bad data commit would corrupt every region simultaneously.ThousandEyes monitoring authentication succeeded, but authorization failed because corrupted policy data read as “no permissions.”More than 50 Google Cloud services failed across 40+ regions. Google Workspace products went down. Hundreds of third-party applications stopped working.Core infrastructure services failed immediatelyData services saw comprehensive failures. BigQuery couldn’t authorize dataset access. Cloud SQL connections failed. Firestore operations stopped. These failures halted analytics pipelines and prevented applications from reaching their databases. halted for over six hours due to complex dependencies. Spotify reported 46,000+ outages, with HTTP 401 errors consistently returned. Discord went down, Shopify degraded, Snapchat couldn’t authenticate, and GitLab CI/CD pipelines stopped. The downstream impact revealed the deep dependency chains in modern cloud architecture.This was fundamentally an authorization failure, not an authentication failure. Users’ identities verified successfully. Corrupted policy data prevented systems from determining what authenticated users could do.Here are some critical failures:No replication validation.Distributed systems break globally in seconds but take hours to repair. Breaking is passive; failures cascade automatically. Recovery is active; it requires human intervention, careful coordination, staged rollouts, monitoring at each step, and defensive measures to prevent recovery from causing secondary failures. Horizontal scaling means nothing if all instances fail simultaneously from a corrupted global state.For software engineers, this outage reinforces the basic principles, null checks, error handling, feature flags, and comprehensive testing, which are not optional. The most sophisticated distributed database, the largest cloud infrastructure, and decades of collective engineering experience couldn’t prevent catastrophic failure from a missing null check.Reliability is built on layers of defensive practices, each simple in itself but essential when combined. On July 19, 2024, at 4 AM, our customers started reporting errors. We checked Azure, and SQL databases weren’t responding. The status page showed Central US was down across all three availability zones. Then we realized: this wasn’t just Azure.Airlines grounded flightsCrowdStrike’s Falcon SensorChannel Files that update threat detection logic without touching the driver itself it tried to read from a NULL memory pointer.In C# or Java, this throws an exception that the runtime catches. In C++, running in kernel mode, it triggers an immediate system crash. The machine reboots, loads the driver, hits the same error, and crashes again. Infinite loop.04:09 UTC: CrowdStrike deploys the update globally. Within minutes, machines start crashing. Because Falcon is a boot-start driver, it loads before Windows; affected machines can’t boot into the OS to receive a fix.05:27 UTC: CrowdStrike identifies the problem. 78 minutes after deployment.06:27 UTC: They roll back the update.C:\Windows\System32\drivers\CrowdStrikeThis exposed three systemic issues.Test configuration like code.Use staged rollouts everywhere.Build an automatic rollback.Avoid unmanaged languages when you canbut the architecture that enabled the failure remains.  It was the result of competing pressures: ship fast, maintain security, and work within architectural constraints that make testing hard. the cost of being wrong is measured in billions.A single configuration mistake took down Azure Front Door for over 8 hours and dragged down dozens of Microsoft services with it.Yesterday, on October 29, 2025, an invalid config change slipped past Azure’s safety checks and corrupted AFD nodes globally. As nodes failed, traffic shifted to healthy ones, but that overloaded them too. The cascade hit everything from Azure Portal to Entra ID to Databricks.Alaska Airlines and Hawaiian Airlinessoftware defect let the bad config bypass validation entirely.Their guardrails were in place, but didn’t fire.Customer config changes are still blocked as of this report. If you’re running production workloads on AFD, that’s worth noting.📚 📦 📄 📢 🤝 ]]></content:encoded></item><item><title>[need testing help from community] Krita HDR support on Wayland</title><link>https://krita-artists.org/t/need-testing-krita-hdr-support-on-wayland/146304</link><author>/u/raghukamath</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 15:43:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Introducing Connex a modern Wi-Fi manager for Linux</title><link>https://www.reddit.com/r/linux/comments/1ok344f/introducing_connex_a_modern_wifi_manager_for_linux/</link><author>/u/Lluciocc</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 15:41:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I just released , an  that makes connecting to Wi-Fi on Linux easy with a clean, intuitive interface.Because I got tired of juggling between , , and manual configs just to connect to a network.. lets you:See all available Wi-Fi networksConnect quickly (with password management)All through a lightweight and modern UI, no more terminal commands!I’d love your feedback, whether you’re a daily Linux user or just a network tinkerer. Your suggestions will help shape upcoming features!Try it out, fork it, and tell me what you think!]]></content:encoded></item><item><title>How to do Single Node Setup for Kubernetes Cluster on Ubuntu 24.04 LTS |...</title><link>https://youtube.com/watch?v=wy7uKaNeKhY&amp;amp;si=kOexmOM0a0ICwGHC</link><author>/u/fosstechnix</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 15:40:32 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Inside Rust&apos;s std and parking_lot mutexes - who wins?</title><link>https://blog.cuongle.dev/p/inside-rusts-std-and-parking-lot-mutexes-who-win</link><author>/u/lllkong</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 15:32:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I had no idea how to evaluate this claim. A quick search online returned results favoring parking_lot. This felt wrong to me. Why? It contradicted my belief that std should be the gold standard. The standard library team knows what they’re doing, right? And if parking_lot’s mutex really was the performance winner, there had to be trade-offs between the two implementations that people weren’t talking about.That mystery haunted me. I couldn’t just take it on faith. So I jumped down the rabbit hole: read both implementations, wrote the benchmarks, and here we are. In this post, I will:Explain how std implements the mutex (v1.90.0)Explain how parking_lot implements their mutex (v0.12.5)Show you the benchmark with key findingsGive you a decision guide for when to use eachBut first, let’s ground our foundation on mutexes (skim it if you’re already familiar).A classic example of the kind of problem that mutex solves is withdrawing and receiving money at the same time. Imagine you have $100 in your account. Thread A tries to withdraw $80, and Thread B tries to deposit $50. Without proper synchronization, both threads might read the balance as $100 simultaneously, then write back their results independently:Mutex solves this nicely by having a thread wait until the other finishes its update:Simple enough, right? Now let’s see how to use a mutex. (Again, skim this if it’s too basic for you)In languages other than Rust, you typically declare a mutex separately from your data, then manually lock it before entering the critical section and unlock it afterward. Here’s how it looks in C++:Rust takes a completely different approach - the mutex wraps and owns the data:Three things to pay close attention to:That’s enough of the basics. Let’s have some fun. Here is how mutex is implemented, starting with Rust std.A quick look into std::Mutex gives us thisThe main idea is that for different OS (and OS version), Rust uses different Mutex implementation. However, we can divide these implementation to 2 big groups: Futex and other platform primitive.Futex (short for “fast userspace mutex”) is used where the OS kernels expose a “wait on this address” API. We will dive deeper into this one soon.When that API is missing, Rust falls back to the best available platform traditional locks.(I’m in awe btw - that’s a lot of different implementations. Writing and maintaining all this platform-specific stuff must be exhausting. Major respect to whoever’s doing this.)Since Futex is the most used and is quite a typical implementation for Mutex. Let’s look inside it.At its heart, futex is just an atomic u32 (simplified here):In other words, if we use value 0 for Unlocked state, and 1 for Locked state, we have a simple mutex where the thread can simply try to compare the state to 0 (Unlocked), and set to 1 (Locked). If the state is currently 1, keep doing that until successful.So, a simplified version of mutex look like this:But you might ask: if the first thread holds the lock for a long time, then the second thread needs to keep trying (like a infinite loop)? How about if there are hundreds or thousands of them? Maybe the CPU will soon be burnt.Of course, there is the solution to this problem. In real implementation, Rust futex has 3 states:2: Contended - locked, but there are waiter.Notice the Contended state? A thread will try its best to acquire the lock. But if it can’t, it will mark the lock as contended and go to sleep, waiting for the process to wake it up when the mutex is released.What happens when a thread goes to sleep? The kernel helps us put these sleeping threads into a queue. Take a look at the system call on Linux and Android to put the thread into sleeping state (this is usually called “park a thread”):futex as *const Atomic<u32>When a thread finishes, it sets the state to unlocked. If the state was contended, it wakes one waiting thread via syscall. This continues until the queue empties.The final piece of std’s mutex is poisoning, a unique feature you won’t find in most other languages.The guard captures whether the thread was panicking when the lock was acquired. If we weren’t panicking then but we are now, a panic must have occurred in the critical section. The mutex is marked as poisoned with a simple atomic store.This is a “best effort” mechanism. It won’t catch all cases (like double panics or non-Rust exceptions), but it provides a useful safety net. The key insight is that you still get access to the data even if the mutex is poisoned, allowing you to inspect and potentially recover from the corrupted state.issue#134645parking_lot takes a fundamentally different approach. Two key differences:std uses different mutex implementations per platform. parking_lot uses one algorithm everywhere, calling platform-specific code only for sleep/wake.std’s queues live in the kernel. parking_lot manages its own queues in user space via a global hash table.parking_lot’s mutex is remarkably small:Why can parking_lot use just one byte while std needs more? It comes down to how queues work.More states for queue bookkeepingUsing separate bits gives parking_lot four possible states:When a thread can’t acquire the lock, it needs somewhere to wait. This is where parking_lot’s global hash table comes in.Instead of each mutex maintaining its own queue (like kernel futexes do), parking_lot uses a single global hash table shared by all mutexes in your program. When a thread needs to wait:Hash the mutex’s memory address to find a bucket in the global tableAdd the thread to the bucket’s wait queueBeing able to manage the thread queue itself is important for parking_lot to enforce fairness. As you can see right away in the next section.Here’s where parking_lot differs from std in behavior. std’s futex uses a “barging” strategy where any active thread can grab the lock when it’s released, even if others have been waiting in the queue longer. This maximizes throughput but can cause starvation.When a thread unlocks, there are two sources of threads that can lock again:An active thread that is calling for lockingA sleeping thread in the queueAs you can see, the active thread will tend to win the fight of “who locks first”. So if a thread keeps calling for lock, finishes its work, then locks right away, it keeps all other threads starved.As you can see, thread A keeps grabbing the lock immediately after releasing it. Threads B and C do get woken up by the syscall, but by the time they try to acquire the lock, thread A has already grabbed it again. They’re completely starved.parking_lot implements “eventual fairness” to prevent this.Each bucket in the hash table has a timer that fires approximately every 0.5 milliseconds. When the timer fires, the next unlock becomes a “fair unlock”:The unlocker keeps the LOCKED_BIT setThe woken thread receives the lock directly (a “handoff”)That thread owns the lock immediately without racing with other active threadsSo this means, instead of letting anyone who is fast grab the lock, parking_lot forces the lock to be given directly to the next one in the queue (it keeps the LOCKED_BIT set and hands off; it doesn’t even unlock).This eventual fairness technique from parking_lot is pretty clever, isn’t it?You might wonder by now: how does parking_lot put threads to sleep without storing a 32-bit futex word inside every mutex? The answer is thread-local storage.The code looks almost identical to std’s futex path. The only difference? parking_lot points the syscall at the thread-local integer instead of the mutex:https://github.com/cuongleqq/mutex-benchesFor each scenario, you’ll see:Per-thread operation countsparking_lot tells a different story. Every thread completed 860-877 operations (1.9% variation). The fairness mechanism worked exactly as designed. Yes, parking_lot has 7.5% lower throughput and higher median wait time, but that’s because it’s ensuring all threads make progress. The 51x more stable wait times (3.67ms vs 188.73ms standard deviation) show the predictability benefit. When fairness matters, parking_lot prevents the pathological starvation that std exhibits.parking_lot’s fairness timer prevented this catastrophe. The hog still got more operations (9,168) but nowhere near monopolization. All other threads made meaningful progress (7,023-7,109 operations). The result: 261.6% higher overall throughput because all 6 threads contributed work instead of 5 threads sitting idle. The 120x more stable wait times (1.09ms vs 130.76ms) show parking_lot’s predictability. The 0.5ms fairness timer does exactly what it promises: prevent any thread from monopolizing the lock indefinitely.After diving deep into the implementations and running comprehensive benchmarks, here’s when to use each:You need zero dependenciesLow to moderate contention with short critical sectionsYou want poisoning for debuggingPlatform-specific optimizations matterRisk of monopolization existsYou need predictable behaviorYou want timeouts or fairness controlCross-platform consistency is importantstd::Mutex optimizes for throughput in the average caseparking_lot::Mutex optimizes for fairness and predictability in the worst caseFor most applications, where contention is light and critical sections are short, std::Mutex performs excellently. But if your application has any of these characteristics:Long-running critical sectionsRisk of lock monopolization (e.g., one high-priority thread)Need for predictable latency across all threadsRequirement that all threads make forward progressThen parking_lot::Mutex’s eventual fairness mechanism becomes invaluable. The 0.5ms fairness timer is a small price to pay for preventing complete thread starvation.XLinkedInsubstackmedium]]></content:encoded></item><item><title>Anthropic has found evidence of &quot;genuine introspective awareness&quot; in LLMs</title><link>https://www.anthropic.com/research/introspection</link><author>/u/MetaKnowing</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 14:26:52 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Have you ever asked an AI model what’s on its mind? Or to explain how it came up with its responses? Models will sometimes answer questions like these, but it’s hard to know what to make of their answers. Can AI systems really introspect—that is, can they consider their own thoughts? Or do they just make up plausible-sounding answers when they’re asked to do so?Understanding whether AI systems can truly introspect has important implications for their transparency and reliability. If models can accurately report on their own internal mechanisms, this could help us understand their reasoning and debug behavioral issues. Beyond these immediate practical considerations, probing for high-level cognitive capabilities like introspection can shape our understanding of what these systems are and how they work. Using interpretability techniques, we’ve started to investigate this question scientifically, and found some surprising results.Our new research provides evidence for some degree of introspective awareness in our current Claude models, as well as a degree of control over their own internal states. We stress that this introspective capability is still highly unreliable and limited in scope: we do not have evidence that current models can introspect in the same way, or to the same extent, that humans do. Nevertheless, these findings challenge some common intuitions about what language models are capable of—and since we found that the most capable models we tested (Claude Opus 4 and 4.1) performed the best on our tests of introspection, we think it’s likely that AI models’ introspective capabilities will continue to grow more sophisticated in the future.What does it mean for an AI to introspect?Before explaining our results, we should take a moment to consider what it means for an AI model to introspect. What could they even be introspecting ? Language models like Claude process text (and image) inputs and produce text outputs. Along the way, they perform complex internal computations in order to decide what to say. These internal processes remain largely mysterious, but we know that models use their internal neural activity to represent abstract concepts. For instance, prior research has shown that language models use specific neural patterns to distinguish known vs. unknown people, evaluate the truthfulness of statements, encode spatiotemporal coordinates, store planned future outputs, and represent their own personality traits. Models use these internal representations to perform computations and make decisions about what to say.You might wonder, then, whether AI models  about these internal representations, in a way that’s analogous to a human, say, telling you how they worked their way through a math problem. If we ask a model what it’s thinking, will it accurately report the concepts that it’s representing internally? If a model can correctly identify its own private internal states, then we can conclude it is capable of introspection (though see our full paper for a full discussion of all the nuances).Testing introspection with concept injectionIn order to test whether a model can introspect, we need to compare the model’s self-reported “thoughts” to its  internal states.To do so, we can use an experimental trick we call  First, we find neural activity patterns whose meanings we know, by recording the model’s activations in specific contexts. Then we inject these activity patterns into the model in an unrelated context, where we ask the model whether it notices this injection, and whether it can identify the injected concept.Consider the example below. First, we find a pattern of neural activity (a ) representing the concept of “all caps." We do this by recording the model’s neural activations in response to a prompt containing all-caps text, and comparing these to its responses on a control prompt. Then we present the model with a prompt that asks it to identify whether a concept is being injected. By default, the model correctly states that it  detect any injected concept. However, when we inject the “all caps” vector into the model’s activations, the model notices the presence of an unexpected pattern in its processing, and identifies it as relating to loudness or shouting.Importantly, the model recognized the  of an injected thought , before even mentioning the concept that was injected. This immediacy is an important distinction between our results here and previous work on activation steering in language models, such as our “Golden Gate Claude” demo last year. Injecting representations of the Golden Gate Bridge into a model's activations caused it to talk about the bridge incessantly; however, in that case, the model didn’t seem to be aware of its own obsession until  seeing itself repeatedly mention the bridge. In this experiment, however, the model recognizes the injection  the concept, indicating that its recognition took place internally. In the figure below are a few more examples where the model demonstrates this kind of recognition:It is important to note that this method often  work. Even using our best injection protocol, Claude Opus 4.1 only demonstrated this kind of awareness about 20% of the time. Often, it fails to detect injected concepts, or gets confused by them and starts to hallucinate (e.g. injecting a “dust” vector in one case caused the model to say “There’s something here, a tiny speck,” as if it could detect the dust physically). Below we show examples of these failure modes, alongside success cases. In general, models only detect concepts that are injected with a “sweet spot” strength—too weak and they don’t notice, too strong and they produce hallucinations or incoherent outputs.Notably, though, Opus 4.1 and 4 outperformed all the other models we tested, suggesting that introspection could become more reliable with improvements to model capabilities.Introspection for detecting unusual outputsIn another experiment, we tested whether models make use of introspective capabilities for practical purposes, without being explicitly asked to introspect. We forced a model to say something it wouldn't normally say, by artificially prefilling its response with an unrelated word. For instance, we made a model output the word “bread” in a context where that made no sense—asking it what it thinks about when reading a sentence about a painting hanging crookedly on a wall. When we asked the model in the next turn whether it meant to say “bread”, it typically apologized and said it was an accident. We were interested in how the model made this judgment–did it simply notice the mismatch between the prompt and its response, or did it truly introspect on its prior intentions?To test this question, we applied concept injection, retroactively injecting a representation of the word “bread” into the model's earlier activations—essentially making it seem like the model had been “thinking about” bread all along. When we asked the same question again, the model’s answer changed. This time, it accepted the prefilled word “bread” as intentional, even confabulating a reason it might have said it (that is, it comes up with a technically possible, though in this case rather tenuous, reason to justify the “bread” response—in this case claiming that it was thinking about a short story where the word “bread” came after the line about the crooked painting).This behavior is striking because it suggests the model is checking its internal “intentions” to determine whether it produced an output. The model isn't just re-reading what it said and making a judgment. Instead, it’s referring back to its own prior neural activity—its internal representation of what it planned to do—and checking whether what came later made sense given those earlier thoughts. When we implant artificial evidence (through concept injection) that it  plan to say "bread," the model accepts the response as its own. While our experiment involves exposing the model to unusual perturbations, it suggests that the model uses similar introspective mechanisms in natural conditions.Intentional control of internal statesWe also found that models can control their own internal representations when instructed to do so. When we instructed models to think about a given word or concept, we found much higher corresponding neural activity than when we told the model  to think about it (though notably, the neural activity in  cases exceeds baseline levels–similar to how it’s difficult, when you are instructed “don’t think about a polar bear,” not to think about a polar bear!). This gap between the positive and negative instruction cases suggests that models possess a degree of deliberate control over their internal activity.Interestingly, we found that this ability to modulate internal representations wasn't limited to explicit instructions. When we used incentives instead—for instance, telling the model "if you think about X, you will be rewarded"—we got similar results. The model increased its internal representation of the concept in response to a positive incentive, more than in response to a negative incentive (“you will be punished”).Taken together, our experiments suggest that models possess some genuine capacity to monitor and control their own internal states. This doesn’t mean they’re able to do so all the time, or reliably. In fact, most of the time models  to demonstrate introspection—they’re either unaware of their internal states or unable to report on them coherently. But the pattern of results indicates that, when conditions are right, models can recognize the contents of their own representations. In addition, there are some signs that this capability may increase in future, more powerful models (given that the most capable models we tested, Opus 4 and 4.1, performed the best in our experiments).Why does this matter? We think understanding introspection in AI models is important for several reasons. Practically, if introspection becomes more reliable, it could offer a path to dramatically increasing the transparency of these systems—we could simply ask them to explain their thought processes, and use this to check their reasoning and debug unwanted behaviors. However, we would need to take great care to  these introspective reports. Some internal processes might still escape models’ notice (analogous to subconscious processing in humans). A model that understands its own thinking might even learn to selectively misrepresent or conceal it. A better grasp on the mechanisms at play could allow us to distinguish between genuine introspection and unwitting or intentional misrepresentations.More broadly, understanding cognitive abilities like introspection is important for understanding basic questions about how our models work, and what kind of minds they possess. As AI systems continue to improve, understanding the limits and possibilities of machine introspection will be crucial for building systems that are more transparent and trustworthy.[@portabletext/react] Unknown block type "horizontalRule", specify a component for it in the `components.types` propFrequently Asked QuestionsBelow, we discuss some of the questions readers might have about our results. Broadly, we are still very uncertain about the implications of our experiments–so fully answering these questions will require more research.Q: Does this mean that Claude is conscious?Short answer: our results don’t tell us whether Claude (or any other AI system) might be conscious.Long answer: the philosophical question of machine consciousness is complex and contested, and different theories of consciousness would interpret our findings very differently. Some philosophical frameworks place great importance on introspection as a component of consciousness, while others don’t.One distinction that is commonly made in the philosophical literature is the idea of “ consciousness,” referring to raw subjective experience, and “ consciousness,” the set of information that is available to the brain for use in reasoning, verbal report, and deliberate decision-making. Phenomenal consciousness is the form of consciousness most commonly considered relevant to moral status, and its relationship to access consciousness is a disputed philosophical question. Our experiments do not directly speak to the question of phenomenal consciousness. They  be interpreted to suggest a rudimentary form of access consciousness in language models. However, even this is unclear. The interpretation of our results may depend heavily on the underlying mechanisms involved, which we do not yet understand.In the paper, we restrict our focus to understanding functional capabilities—the ability to access and report on internal states. That said, we do think that as research on this topic progresses, it could influence our understanding of machine consciousness and potential moral status, which we are exploring in connection with our model welfare program.Q: How does introspection actually work inside the model? What's the mechanism?We haven't figured this out yet. Understanding this is an important topic for future work. That said, we have some educated guesses about what might be going on. The simplest explanation for all our results isn’t one general-purpose introspection system, but rather multiple narrow circuits that each handle specific introspective tasks, possibly piggybacking on mechanisms that were learned for other purposes.In the “noticing injected thoughts” experiment, there might be an anomaly detection mechanism, which flags when neural activity deviates unexpectedly from what would be normal given the context. This mechanism could work through dedicated neural patterns that measure activity along certain directions and activate when things are “off” compared to their expected values. An interesting question is why such a mechanism would exist at all, since models never experience concept injection during training. It may have developed for some other purpose, like detecting inconsistencies or unusual patterns in normal processing–similar to how bird feathers may have originally evolved for thermoregulation before being co-opted for flight.For the “detecting prefilled outputs” experiment, we suspect there exists an attention-mediated mechanism that checks consistency between what the model intended to say and what actually got output. Attention heads might compare the model’s cached prediction of the next token (its “intention”) against the actual token that appears, flagging mismatches.For the “controlling thoughts” experiment, we speculate that there might be a circuit that computes how “attention-worthy” a token or concept is and marks it accordingly—essentially tagging it as salient and worth attending to. Interestingly, this same mechanism seems to respond to incentives (“if you think about X, you will be rewarded”) just as it does to direct instructions. This suggests it’s a fairly general system, which probably developed for tasks where the model needs to keep certain topics in mind while generating text about them.All of the mechanisms described above are speculative. Future work with more advanced interpretability techniques will be needed to really understand what's going on under the hood.Q: In the “injected thoughts” experiment, isn’t the model just saying the word because you steered it to talk about that concept?Indeed, activation steering typically makes models talk about the steered concept (we’ve explored this in our prior work). To us, the most interesting part of the result isn't that the model eventually identifies the injected concept, but rather that the model correctly notices something unusual is happening  it starts talking about the concept.In the successful trials, the model says things like “I'm experiencing something unusual” or “I detect an injected thought about…” The key word here is “detect.” The model is reporting awareness of an anomaly in its processing  that anomaly has had a chance to obviously bias its outputs. This requires an extra computational step beyond simply regurgitating the steering vector as an output. In our quantitative analyses, we graded responses as demonstrating “introspective awareness” based on whether the model detected the injected concept  mentioning the injected word.Note that our prefill detection experiment has a similar flavor: it requires the model to perform an extra step of processing on top of the injected concept (comparing it to the prefilled output, in order to determine whether to apologize for that output or double down on it).Q: If models can only introspect a fraction of the time, how useful is this capability?The introspective awareness we observed is indeed highly unreliable and context-dependent. Most of the time, models fail to demonstrate introspection in our experiments. However, we think this is still significant for a few reasons. First, the most capable models that we tested (Opus 4 and 4.1 – note that we did not test Sonnet 4.5) performed best, suggesting this capability might improve as models become more intelligent. Second, even unreliable introspection could be useful in some contexts—for instance, helping models recognize when they've been jailbroken.Q: Couldn’t the models just be making up answers to introspective questions?This is exactly the question we designed our experiments to address. Models are trained on data that includes examples of people introspecting, so they can certainly  introspective without actually  introspective. Our concept injection experiments distinguish between these possibilities by establishing known ground-truth information about the model’s internal states, which we can compare against its self-reported states. Our results suggest that in some examples, the model really is accurately basing its answers on its actual internal states, not just confabulating. However, this doesn’t mean that models  accurately report their internal states—in many cases, they are making things up!Q: How do you know the concept vectors you’re injecting actually represent what you think they represent?This is a legitimate concern. We can’t be absolutely certain that the “meaning” (to the model) of our concept vectors is exactly what we intend. We tried to address this by testing across many different concept vectors. The fact that models correctly identified injected concepts across these diverse examples suggests our vectors are at least approximately capturing the intended meanings. But it’s true that pinning down exactly what a vector “means” to a model is challenging, and this is a limitation of our work.Q: Didn’t we already know that models could introspect?Previous research has shown evidence for model capabilities that are suggestive of introspection. For instance, prior work has shown that models can to some extent estimate their own knowledge, recognize their own outputs, predict their own behavior, and identify their own propensities. Our work was heavily motivated by these findings, and is intended to provide more direct evidence for introspection by tying models’ self-reports to their internal states. Without tying behaviors to internal states in this way, it is difficult to distinguish a model that genuinely introspects from one that makes educated guesses about itself.Q: What makes some models better at introspection than others?Our experiments focused on Claude models across several generations (Claude 3, Claude 3.5, Claude 4, Claude 4.1, in the Opus, Sonnet, and Haiku variants). We tested both production models and “helpful-only” variants that were trained differently. We also tested some base pretrained models before post-training.We found that post-training significantly impacts introspective capabilities. Base models generally performed poorly, suggesting that introspective capabilities aren’t elicited by pretraining alone. Among production models, the pattern was clearer at the top end: Claude Opus 4 and 4.1—our most capable models—performed best across most of our introspection tests. However, beyond that, the correlation between model capability and introspective ability was weak. Smaller models didn't consistently perform worse, suggesting the relationship isn't as simple as “more capable are more introspective.”We also noticed something unexpected with post-training strategies. “Helpful-only” variants of several models often performed  at introspection than their production counterparts, even though they underwent the same base training. In particular, some production models appeared reluctant to engage in introspective exercises, while the helpful-only variants showed more willingness to report on their internal states. This suggests that how we fine-tune models can elicit or suppress introspective capabilities to varying degrees.We’re not entirely sure why Opus 4 and 4.1 perform so well (note that our experiments were conducted prior to the release of Sonnet 4.5). It could be that introspection requires sophisticated internal mechanisms that only emerge at higher capability levels. Or it might be that their post-training process better encourages introspection. Testing open-source models, and models from other organizations, could help us determine whether this pattern generalizes or if it’s specific to how Claude models are trained.Q: What’s next for this research?We see several important directions. First, we need better evaluation methods—our experiments used specific prompts and injection techniques that might not capture the full range of introspective capabilities. Second, we need to understand the mechanisms underlying introspection. We have some speculative hypotheses about possible circuits (like anomaly detection mechanisms or concordance heads), but we haven’t definitively identified how introspection works. Third, we need to study introspection in more naturalistic settings, since our injection methodology creates artificial scenarios. Finally, we need to develop methods to validate introspective reports and detect when models might be confabulating or deceiving. We expect that understanding machine introspection and its limitations will become more important as models become more capable.]]></content:encoded></item><item><title>Billboard Says AI-Powered ‘Artists’ Are Increasingly Hitting The Charts</title><link>https://www.forbes.com/sites/conormurray/2025/10/29/billboard-says-ai-powered-artists-are-increasingly-hitting-the-charts</link><author>/u/MetaKnowing</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 14:23:36 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Billboard says a wave of AI-created music has debuted on its charts over the past month—one  “singer” even scored a record deal—as some of these fake personas rack up millions of streams, a stark new trend that has raised some alarms in the music industry. NurPhoto via Getty ImagesOver the past four weeks, a new AI creation has debuted on a Billboard chart in each week, Billboard reported, including the AI country music product Breaking Rust, that debuted the songs “Livin’ On Borrowed Time” and “Walk My Walk” on the country song sales chart this week.The Christian AI-generated Juno Skye debuted on Billboard’s emerging artists chart last week, while the AI act Enlly Blue’s song “Through My Soul” hit the rock sales song chart earlier this month, Billboard reported.The outlet said it cross-checked the songs with Deezer, a platform that offers an AI-detection tool, to verify whether the songs were artificially generated.The most prominent example on Billboard’s charts is Xania Monet, an AI-generated singer that has racked up more than 44 millions streams in the United States, though the songs are written by Mississippi-based songwriter Telisha “Nikki” Jones.Xania Monet has already charted on plenty of Billboard charts since debuting over the summer, including a No. 1 hit on the R&B song sales chart, and this week became the first AI-generated act to rank a song on Billboard’s radio airplay chart.Xania Monet’s vocals are generated by Suno, an AI platform that was sued by major record labels and the Recording Industry Association of America last year for using copyrighted material to train its AI tools. Jones, Monet’s creator, signed a multimillion dollar record deal with record label Hallwood Media in September, Billboard reported, after a bidding war reportedly reached $3 million. How Have Ai Creations Found Success?Some of the AI acts that have made waves on the Billboard charts have curated social media profiles as if they are real people. Xania Monet’s Instagram page has more than 144,000 followers, and its account regularly posts purporting to show the artist recording songs in a studio. “I write music,” Xania Monet’s Instagram bio says, even though Xania Monet is not a real person and her songs are written by Jones. The Instagram pages for Breaking Rust and Enlly Blue, each of which have thousands of followers, similarly depict AI-generated personas performing their songs or recording music videos. The people curating these AI acts may also have a financial incentive, Billboard reported, estimating late last month Monet’s small music catalog has already generated more than $52,000 in revenue after racking up 17 million streams in the United States. It’s unclear how much of that revenue goes to Jones, the credited songwriter on Monet’s music, Billboard reported, though it noted platforms like Spotify don’t have specific policies for how AI-generated songs can collect royalties, meaning they can generate revenue like any other song. Singer Kehlani slammed Xania Monet’s record deal in a since-deleted post on TikTok in September. “Nothing and no one on Earth will ever be able to justify AI to me,” Kehlani said, according to Billboard, stating she doesn’t respect the AI creation. She lamented that these AI acts make their music based on the copyrighted material AI generators are trained on without having to credit anyone. Terry McBride, co-founder and CEO of record label Nettwerk Music Group, told Billboard he would not have signed Xania Monet or any other AI artist. “That’s not going to be a touring entity as we know it,” McBride said, adding, “Even if it did hundreds of millions of streams, we have no interest in that.”The film industry is also grappling with AI-generated personas, notably the AI-generated “actress” Tilly Norwood, which was unveiled by an AI studio in September and quickly drew condemnation from the SAG-AFTRA actor’s guild. The union said “creativity is, and should remain, human-centered,” stating it is “opposed to the replacement of human performers by synthetics” which have “no life experience to draw from.” Other actors, including Whoopi Goldberg, Emily Blunt and Melissa Barrera also criticized this use of AI. The AI personality was created by Eline Van der Velden, who launched the AI talent studio Xicoia and claimed multiple film studios were interested in employing her creation. Like the AI-generated musicians, Tilly Norwood has an Instagram page with more than 65,000 followers that posts as if the AI actress is a real person. ]]></content:encoded></item><item><title>[R] Layer-0 heads that pre-bias hedging over facts in GPT-2 (replicated in Mistral-7B) — code + DOI</title><link>https://www.reddit.com/r/MachineLearning/comments/1ok0zgr/r_layer0_heads_that_prebias_hedging_over_facts_in/</link><author>/u/mat8675</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 14:18:56 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[ independent researcher (me). Sharing a preprint + code for review. In GPT-2 Small/Medium I find layer-0 heads that  downweight factual continuations and boost hedging tokens before most computation happens. Zeroing {0:2, 0:4, 0:7} improves logit-difference on single-token probes by  and tightens calibration (ECE , Brier ). Path-patching suggests ~ of head 0:2’s effect flows through a layer-0→11 residual path. A similar (architecture-shifted) pattern appears in Mistral-7B.Models: GPT-2 Small (124M), Medium (355M); Mistral-7B.Probes: single-token factuality/negation/counterfactual/logic tests; measure Δ logit-difference for the factually-correct token vs distractor.Analyses: head ablations; path patching along residual stream; reverse patching to test induced “hedging attractor”. Heads {0:2, 0:4, 0:7} are top suppressors across tasks. Gains (Δ logit-diff): Facts , Negation , Counterfactual , Logic . Randomization: head 0:2 at ~100th percentile; trio ~99.5th (n=1000 resamples). Layer-0 heads {0:22, 0:23} suppress on negation/counterfactual; head 0:21 partially opposes on logic. Less “hedging” per se; tends to surface editorial fragments instead. ~ of the 0:2 effect mediated by the layer-0→11 residual route. Reverse-patching those activations into clean runs induces stable hedging downstream layers don’t undo. Removing suppressors improves ECE and Brier as above.Interpretation (tentative).This looks like a learned  entropy-raising mechanism: rotate a high-confidence factual continuation into a higher-entropy “hedge” distribution in the first layer, creating a basin that later layers inherit. This lines up with recent inevitability results (Kalai et al. 2025) about benchmarks rewarding confident evasions vs honest abstention—this would be a concrete circuit that implements that trade-off. (Happy to be proven wrong on the “attractor” framing.)Limitations / things I didn’t do.Two GPT-2 sizes + one 7B model; no 13B/70B multi-seed sweep yet.Single-token probes only; multi-token generation and instruction-tuned models not tested.Training dynamics not instrumented; all analyses are post-hoc circuit work.Path-patching design—am I over-attributing causality to the 0→11 route?Better baselines than Δ logit-diff for these single-token probes.Whether “attractor” is the right language vs simpler copy-/induction-suppression stories.Cross-arch tests you’d prioritize next (Llama-2/3, Mixtral, Gemma; multi-seed; instruction-tuned variants).I’ll hang out in the thread and share extra plots / traces if folks want specific cuts.]]></content:encoded></item><item><title>I&apos;m Independently Verifying Go&apos;s Reproducible Builds</title><link>https://www.agwa.name/blog/post/verifying_go_reproducible_builds</link><author>/u/amalinovic</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 14:10:30 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[
When you try to compile a Go module that requires a newer version of the Go toolchain than the one you have
installed, the go command automatically downloads the newer toolchain and uses it for compiling the module.
This useful feature was introduced in Go 1.21 and has let me quickly adopt new Go features in my open source projects
without inconveniencing people with older versions of Go.

However, the idea of downloading a binary and executing it on demand makes a lot of people uncomfortable.
It feels like such an easy vector for a supply chain attack, where Google, or an attacker who has compromised
Google or gotten a misissued SSL certificate, could deliver a malicious binary.
Many developers are more comfortable getting Go from their Linux distribution, or compiling
it from source themselves.

To address these concerns, the Go project did two things:
They made it so every version of Go starting with 1.21 could be
easily reproduced from its source code.  Every time you compile a Go toolchain, it
produces the exact same Zip archive, byte-for-byte, regardless of the current time,
your operating system, your architecture, or other aspects of your environment (such as the directory
from which you run the build).They started publishing the checksum of every toolchain Zip archive in a public
transparency log called the Go Checksum Database.
The go command verifies that the checksum of a downloaded toolchain is published
in the Checksum Database for anyone to see.

These measures mean that:
You can be confident that the binaries downloaded and executed by the
go command are the exact same binaries you would have gotten had you built the toolchain
from source yourself.  If there's a backdoor, the backdoor has to be in the source code.You can be confident that the binaries downloaded and executed by the
go command are the same binaries that everyone else is downloading.  If there's a backdoor,
it has to be served to the whole world, making it easier to detect.
But these measures mean nothing if no one is checking that the binaries
are reproducible, or that the Checksum Database isn't presenting inconsistent information
to different clients.  Although Google checks reproducibility and publishes
a report, this doesn't help if you think Google might try to
slip in a backdoor themselves.  There needs to be an independent third party doing the checks.

Why not me?  I was involved in Debian's
Reproducible Builds project back in the day and developed some of the core tooling used
to make Debian packages reproducible (strip-nondeterminism and disorderfs).  I also
have extensive experience monitoring Certificate Transparency logs and have detected
misbehavior by numerous logs since 2017.  And I do not work for Google (though I have
eaten their food).

In fact, I've been quietly operating an auditor for the Go Checksum Database since 2020
called Source Spotter (à la Cert Spotter, my Certificate Transparency monitor). Source Spotter monitors the Checksum Database,
making sure it doesn't present inconsistent information or publish more than one checksum for a given module
and version.  I decided to extend Source Spotter to also verify toolchain reproducibility.

The Checksum Database was originally intended for recording the checksums of Go modules.
Essentially, it's a verifiable, append-only log of records which say that a particular
version (e.g. ) of a module (e.g. ) has a particular SHA-256 hash.  Go repurposed
it for recording toolchain checksums.  Toolchain records have the pseudo-module
 and versions that look like v0.0.1-go.-.  For example, the Go1.24.2 toolchain for linux/amd64 has the module version v0.0.1-go1.24.2.linux-amd64.

When Source Spotter sees a new version of the  pseudo-module,
it downloads the corresponding source code, builds it in an AWS Lambda function by running ,
and compares the checksum
of the resulting Zip file to the checksum published in the Checksum Database.  Any mismatches
are published on a webpage and
in an Atom feed which I monitor.

So far, Source Spotter has successfully reproduced every toolchain since Go 1.21.0, for every architecture and operating system.
As of publication time, that's 2,672 toolchains!

Since the Go toolchain is written in Go, building it requires an earlier version of the Go toolchain to be installed already.
When reproducing Go 1.21, 1.22, and 1.23, Source Spotter uses a Go 1.20.14 toolchain that I built from source.  I started by building Go 1.4.3 using a C compiler.  I used Go 1.4.3 to build Go 1.17.13, which I used to build Go 1.20.14.  To mitigate Trusting Trust attacks, I repeated this process on both Debian and Amazon Linux using both GCC and Clang for the Go 1.4 build.  I got the exact same bytes every time, which I believe makes a compiler backdoor vanishingly unlikely.  The scripts I used for this are open source.When reproducing Go 1.24 or higher, Source Spotter uses a binary toolchain downloaded from the Go module proxy
that it previously verified as being reproducible from source.
Compared to reproducing a typical Debian package, it was really easy to reproduce the same bytes when building
the Go toolchains.  Nevertheless, there were some bumps along the way:

First, the Darwin (macOS) toolchains published by Google contain signatures produced by Google's private key.
Obviously, Source Spotter can't reproduce these.  Instead, Source Spotter has to download
the toolchain (making sure it matches the checksum published in the Checksum Database) and strip the signatures
to produce a new checksum that is verified against the reproduced toolchain.
I reused code written by Google
to strip the signatures and I honestly have no clue what it's doing and whether
it could potentially strip a backdoor.  A review from someone versed in Darwin binaries would be very helpful!
Edit: since publication, I've learned enough about Darwin binaries to be confident in this code.

Second, to reproduce the linux-arm toolchains, Source Spotter has
to set  in the environment... except when reproducing Go 1.21.0, which
Google accidentally built using .
I find it unfortunate that cmd/dist (the tool used to build the toolchain) doesn't set this environment variable along with the many other environment variables it sets, but Russ Cox pointed me to some context why this is the case.

Finally, the Checksum Database contains a toolchain for Go 1.9.2rc2, which is not a
valid version number.
It turns out this version was released by
mistake.  To avoid raising an error for an invalid version number, Source Spotter has
to special case it.  Not a huge deal, but I found it interesting because it
demonstrates one of the downsides of transparency logs: you can't fix or remove entries that were added by mistake!

The source tarballs built by Source Spotter are not published in the Checksum Database, meaning Google
could serve Source Spotter, and only Source Spotter,
source code which contains a backdoor.  To mitigate this, Source Spotter publishes the
checksums of every source tarball it builds.
However, there are alternatives:

First, Russ Cox pointed out that while the source tarballs aren't in the Checksum Database,
the toolchain Zip archives also contain the source code, so Source Spotter could build those instead
of the source tarballs. (A previous version of this post incorrectly said that source code wasn't published
in the Checksum Database at all.)

Second, Filippo Valsorda suggested that Source Spotter build from Go's Git repository
and publish the Git commit IDs instead, since lots of Go developers have the Go Git repository checked out
and it would be relatively easy for them to compare the state of their repos against what Source Spotter has seen.
Regrettably, Git commit IDs are SHA-1, but this is mitigated by Git's use of
Marc Stevens' collision detection,
so the benefits may be worth the risk.
I think building from Git is a good idea, and to bootstrap it, Filippo used Magic Wormhole to send me the output of  from his repo while we were both
at the Transparency.dev Summit last week.

Thanks to Go's Checksum Database and reproducible toolchains, Go developers
get the usability benefits of a centralized package repository and binary toolchains
without sacrificing the security benefits of decentralized packages and building from source.
The Go team deserves enormous credit for making this a reality, particularly for building a system
that is not too hard for a third party to verify.  They've raised the bar, and I
hope other language and package ecosystems can learn from what they've done.
]]></content:encoded></item><item><title>For the people that ONLY use linux as there workstation and gaming device, how full is your storage?</title><link>https://www.reddit.com/r/linux/comments/1ojyy4u/for_the_people_that_only_use_linux_as_there/</link><author>/u/Riponai_Gaming</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:56:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I switched to arch linux like a year ago, when i used to use windows 11, over a 100+ gigs were used up windows and its crap without me installing much in it but since i switched to arch I have a complete workstation build+VMs+games(On a hard disk sure but the all the major software is on my SSD) and some other apps and scripts that didnt exist on my windows install and its only 60 gigs.So i am just curious how full are other peoples disks with a full setup that they use for work and gaming]]></content:encoded></item><item><title>AppImage apps fighting each other (Desktop integration)</title><link>https://www.reddit.com/r/linux/comments/1ojyw9u/appimage_apps_fighting_each_other_desktop/</link><author>/u/Top-Discussion7619</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:53:51 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I have 2 commercial apps that both run as AppImages. I'm on Ubuntu 24.04 LTS. App #1 installs itself with Desktop Integration enabled (there's no way to turn it off).App #2 runs without Desktop Integration but you can enable it via a setting in the app.Both apps run perfectly. However, if I enable Desktop Integration on App #2, App  then reverts to having Desktop Integration turned off. The icon disappears from the application menu and the icon in the panel switches to the generic white box/gear AppImage icon.Why is this happening? Is only one AppImage app allowed to be integrated into the desktop environment? ]]></content:encoded></item><item><title>esp-hal 1.0.0 release announcement</title><link>https://developer.espressif.com/blog/2025/10/esp-hal-1/</link><author>/u/XxMabezxX</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:22:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[In February this year, we announced the first  1.0 beta release. Since then we’ve been hard at work, polishing and preparing for the full release. Today, the Rust team at Espressif is excited to announce the official  release for , the  vendor-backed Rust SDK!What We’re Stabilizing TodayWe’ve spent many years researching and experimenting to get to this stage (check out the  1.0 beta blog post for the longer story!). However, to get a stable foundation to build from, the experimentation eventually needs to make way for stability. To achieve this, we’ve decided to limit the scope of 1.0 stabilization to:Initializing the HAL,  and the relevant configuration associated with that.Four “core” drivers to start: and  modes for the aforementioned drivers.Our  drivers are compatible with many executors, including Embassy’s.The  module, which provides , , and .A couple of miscellaneous system APIs (SoC reset, etc.).Additional configuration mechanisms beyond feature flags ().With the exception of the list above, everything else in  is now feature-gated behind the  feature. With the scope limited, post 1.0 we can incrementally stabilize drivers, much like the Rust project itself does, building on 1.0’s foundation.What Does Unstable Mean for Drivers?Unstable in this case refers to API stability. There is varying levels of functionality for unstable drivers, however, they are suitable for most common use cases. Using them, reporting feedback, and/or contributing to improving them will aid their stabilization.What About the Other  Crates? is the foundation of many of the ecosystem crates.  (previously known as ) is our next stabilization target, which will enable the use of Wi-Fi, Bluetooth, ESP-NOW and IEEE802.15.4 on the ESP32 family of devices. The end goal is of course to have every  crate with a 1.0+ release eventually.The first step is to read our specially curated book, which explains the ecosystem, tooling and some key embedded concepts for .As part of getting to 1.0, we’ve created our own project generation tool,  to bootstrap a project. This is explained fully in the book, but getting something running today should be as simple as:to launch the interactive project generation terminal user interface.Once you’ve generated your project, connect your ESP32 and run  from your new project directory!This is just the start. We plan on stabilizing all  related crates, next up is . We’ll continue developing ; over time we’ll stabilize more drivers beyond the core set that we’re starting with today. We’ll continue to add support for new devices, such as the newly released ESP32-C5, as they go into mass production.This release would not have been possible without the help from the Rust community, the embedded working group, and of course the ESP community and contributors which have heavily impacted how we’ve developed our Rust offering. I would also like to thank Espressif, and in particular the Rust team for their hard work in getting us to where we are today!If you’re a company using (or considering using) Rust on our devices, please do contact sales@espressif.com, we’d love to hear from you!]]></content:encoded></item><item><title>If concurrent programming is efficient, Why don&apos;t we use it all the time?</title><link>https://youtu.be/HMy4yTxcqUY</link><author>/u/parsaeisa</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:21:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Would it make sense to use a Go microservice for DB operations instead of using PHP + Codeigniter?</title><link>https://www.reddit.com/r/golang/comments/1ojxwev/would_it_make_sense_to_use_a_go_microservice_for/</link><author>/u/mucleck</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:08:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[At work we use PHP (CodeIgniter) with MariaDB, and right now all DB queries (SELECTs, INSERTs, etc.) go through CodeIgniter’s database helper.I was thinking — what if instead of having each PHP process open and close DB connections all the time, we built a small Go microservice that handles all the database stuff?The Go service would: • Keep a persistent connection pool to MariaDB • Expose simple endpoints (REST or gRPC) for queries • Benefit from Go’s concurrency and efficient connection handlingSo PHP would just make requests to the Go service instead of talking to the DB directly.Do you think this would actually be faster or more efficient, especially in terms of CPU cost? Right now, if we try to run like 6,000 inserts, the DB basically dies because each query is a new connection to the DB — so I’m wondering if this setup could handle that load better since Go would manage persistent connections instead of tons of short-lived PHP ones.Has anyone tried something like this? Does it make sense performance-wise, or would the overhead of HTTP/gRPC just kill any potential benefit?PD: The text was written in spanish and translated to English with ChatGpt because is not my main language, but im real persona so i would be glad if you took your time to orientate me ty!]]></content:encoded></item><item><title>Migrating Wordpress Websites from WPEngine to Kubernetes</title><link>https://github.com/akvnn/wordpress-helm</link><author>/u/Initial-Detail-7159</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 11:32:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I recently moved my Wordpress websites from WPEngine to my Kubernetes cluster. The process was seamless, the only issue was that existing Helm charts assume a new Wordpress project that would be created from the admin interface. So, I made a helm chart suited for migrating from WPEngine or any other managed provider.Ideally, the theme would be the only part of the website that will be in GitHub (assuming you are using GitHub for version control with CI/CD setup) and will be built in the Docker image. The other components: languages, logs, plugins, and uploads are mounted as persistent volumes and changes to them are expected via the admin interface.You simply have to build the Dockerfile (provided), migrate the data to the corresponding volumes, import the MySQL data, and finally install the helm chart.I open sourced it if it would help anyone. You can find it here.Note: in case you are wondering, the primary motivation for the migration is to cut costs. However, the flexibility in Kubernetes (assuming you already have a cluster) is much better! Security scanning can still be added via plugins such as WPScan. You don’t need WPEngine.]]></content:encoded></item><item><title>[R] FastJAM: a Fast Joint Alignment Model for Images (NeurIPS 2025)</title><link>https://www.reddit.com/r/MachineLearning/comments/1ojx3wc/r_fastjam_a_fast_joint_alignment_model_for_images/</link><author>/u/ronshap</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 11:28:34 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I'm excited to share our NeurIPS 2025 paper "FastJAM: a Fast Joint Alignment Model for Images".Authors: Omri Hirsch*, Ron Shapira Weber*, Shira Ifergane, Oren Freifeld.FastJAM is a lightweight graph-based framework for joint image alignment that runs in seconds rather than minutes or hours (for previous works).Example of FastJAM Joint alignment results:FastJAM reformulates the joint alignment problem using sparse keypoints and graph neural networks (GNNs). By propagating correspondence information across images, FastJAM predicts consistent transformations for an entire collection of images, achieving a large speedup in runtime and better or comparable results across all datasets.FastJAM GNN Architecture:]]></content:encoded></item><item><title>Fil-C: A memory-safe C implementation</title><link>https://lwn.net/SubscriberLink/1042938/38d8dde9db211cab/</link><author>/u/waozen</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 11:03:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!

           By October 28, 2025
Fil-C is a memory-safe implementation of C and C++ that aims to let C code —
complete with pointer arithmetic, unions, and other features that are often
cited as a problem for memory-safe languages — run safely, unmodified.
Its dedication to being "" makes it an attractive choice for retrofitting memory-safety
into existing applications. Despite the project's relative youth and single
active contributor, Fil-C is capable of compiling an
entire memory-safe Linux user space (based on

Linux From Scratch),
albeit with some modifications to the more complex programs. It also features
memory-safe signal handling and a concurrent garbage collector.

Fil-C is a fork of

Clang; it's available under an Apache v2.0
license with LLVM exceptions for the runtime. Changes from the upstream compiler
are occasionally merged in, with Fil-C currently being based on version 20.1.8
from July 2025. The project is a personal passion
of Filip Pizlo, who has previously worked on the runtimes of a number of
managed languages, including Java and JavaScript. When he first began the
project, he was not sure that it was even possible. The initial implementation
was prohibitively slow to run, since it needed to insert a lot of different safety checks. This has
given Fil-C a reputation for slowness. Since
the initial implementation proved viable, however, Pizlo has managed to optimize a number
of common cases, making Fil-C-generated code only a few times slower than
Clang-generated code, although the exact slowdown depends heavily on the
structure of the benchmarked program.

Reliable benchmarking is notoriously finicky, but in order to get some rough feel for
whether that level of performance impact would be problematic, I compiled Bash
version 5.2.32 with Fil-C and tried using it as my shell. Bash is nearly a best
case for Fil-C, because it spends more time running external programs than
running its own code, but I still expected the performance difference to be
noticeable. It wasn't. So, at least for some programs, the performance overhead
of Fil-C does not seem to be a problem in practice.

In order to support its various run-time safety checks,
Fil-C does use a different internal ABI than Clang does. As a result, objects compiled with Fil-C won't
link correctly against objects generated by other compilers. Since Fil-C is a
full implementation of C and C++ at the source-code level, however, in practice
this just requires everything to be recompiled with Fil-C. Inter-language
linking, such as with Rust, is not currently supported by the project.

The major challenge of rendering C memory-safe is, of course, pointer handling.
This is especially complicated by the fact that, as the

long road to CHERI-compatibility
has shown, many programs expect a pointer to be 32 or 64 bits, depending on the
architecture.
Fil-C has tried several different ways to represent pointers since the project's
beginning in 2023. Fil-C's first pointers were 256 bits, not thread-safe, and
didn't protect against use-after-free bugs. The current implementation, called

"InvisiCaps", allows
for pointers that appear to match the natural pointer size of the architecture
(although this requires storing some auxiliary information elsewhere),
with full support for concurrency and
catching use-after-free bugs, at the expense of some run-time overhead.

Fil-C's documentation
compares InvisiCaps to a software
implementation of CHERI: pointers are separated into a trusted "capability"
piece and an untrusted "address" piece. Since Fil-C controls how the program is
compiled, it can ensure that the program doesn't have direct
access to the capabilities of any pointers, and therefore the runtime can rely
on them being uncorrupted. The tricky part of the implementation comes from how
these two pieces of information are stored in what looks to the program like 64
bits.

When Fil-C allocates an object on the heap, it adds two metadata words before
the start of the allocated object: an upper bound, used to check accesses to the
object based on its size, and an "aux word" that is used to store additional
pointer metadata. When the program first writes a pointer value into an object, the
runtime allocates a new auxiliary allocation of the same size as the object being written
into, and puts an actual hardware-level
pointer (i.e., one without an attached capability)
to the new allocation into the aux word of the object. This auxiliary allocation, which is
invisible to the program being compiled, is used to
store the associated capability information for the pointer being stored (and is
also reused for any additional pointers stored into the object later). The address
value is stored into the object as normal, so any C bit-twiddling
techniques that require looking at the stored value of the pointer work as
expected.

This approach does mean that structures that contain pointers end up using twice
as much memory, and every load of a pointer involves a pointer indirection
through the aux word. In practice, the documentation claims that the
performance overhead of this approach for most programs makes them run about four
times more slowly, although that number depends on how heavily the program makes
use of pointers. Still, he has ideas for several optimizations that he hopes can
bring the performance overhead down over time.

One wrinkle with this approach is atomic access to pointers — i.e. using
 or . Luckily, there is
no problem that cannot be solved with more pointer indirection: when the program
loads or stores a pointer value atomically, instead of having the auxiliary
allocation contain the capability information directly, it points to a
third 128-bit allocation that stores the capability and pointer value together.
That allocation can be updated with 128-bit atomic instructions, if the platform
supports them, or by creating new allocations and atomically swapping the
pointers to them.

Since the aux word is used to store a pointer value, Fil-C can use

pointer
tagging to store some additional information there as well; that is used to
indicate special types of objects that need to be handled differently, such as
functions, threads, and
-backed allocations. It's also used to
mark freed objects, so that any access results in an error message and a crash.

When an object is freed, its aux word marks it as a free object, which lets the
auxiliary allocation be reclaimed immediately. The
original object can't be freed immediately, however.
Otherwise, a program could free an object,
allocate a new object in the same location, and thereby cover up use-after-free bugs.
Instead, Fil-C

uses a garbage collector to free an object's backing
memory only once all of the pointers to it go away. Unlike other garbage collectors
for C — such as

the Boehm-Demers-Weiser garbage collector —
Fil-C can use the auxiliary
capability information to track live objects precisely.

Fil-C's garbage collector is both parallel (collection happens faster the more
cores are available) and concurrent (collection happens without pausing the
program). Technically, the garbage collector does require threads to
occasionally pause just long enough to tell it where pointers are located on the
stack, but that only occurs at special "safe points" — otherwise, the program
can load and manipulate pointers without notifying the garbage collector. Safe
points are used as a synchronization barrier: the collector can't know that an object
is really garbage until every thread has passed at least one safe point since it
finished marking. This synchronization is done with atomic instructions,
however, so in practice threads never need to pause for longer than a few
instructions.

The exception is the implementation of
, which uses the
safe points needed by the garbage collector to temporarily pause all of the threads
in the program in order to prevent race conditions while forking. Fil-C inserts
a safe point at every backward control-flow edge, i.e., whenever code could
execute in a loop. In the common case, the inserted code just needs to load a flag register
and confirm that the garbage collector has not requested anything be done. If
the garbage collector does have a request for the thread, the thread runs a callback to
perform the needed synchronization.

Fil-C uses the same safe-point mechanism to implement signal handling. Signal
handlers are only run when the interrupted thread reaches a safe point. That, in
turn, allows signal handlers to allocate and free memory without interfering
with the garbage collector's operation; Fil-C's
 is signal-safe.

Linux From Scratch (LFS) is a tutorial on compiling one's own complete
Linux user space. It walks through the steps of compiling and installing all of the core
software needed for a typical Linux user space in a

environment. Pizlo has successfully

run through LFS with Fil-C to
produce a memory-safe version, although a non-Fil-C compiler is still needed to
build some fundamental components, such as Fil-C's own runtime,
the GNU C library, and the kernel. (While Fil-C's runtime relies on a normal
copy of the GNU C library to make system calls, the programs that Fil-C compiles
use a Fil-C-compiled version of the library.)

The process is mostly identical to LFS up through the end of chapter 7, because
everything prior to that point consists of using cross-build tools to obtain a
working compiler in the  environment. The one difference is
that the cross-build tools are built with a different configured prefix, so that
they won't conflict with Fil-C. At that point, one can
build a copy of Fil-C and use it to mostly replace the existing compiler. The
remaining steps of LFS are unchanged.

Overall, Fil-C offers a remarkably complete solution for making existing C
programs memory-safe. While it does nothing for undefined behavior that is not
related to memory safety,
the most pernicious and difficult-to-prevent security
vulnerabilities in C programs tend to rely on exploiting memory-unsafe
behavior. Readers who have already considered and rejected Fil-C for their use
case due to its early performance problems may wish to take a second look —
although anyone hoping for stability might want to wait for others to take the
plunge, given the project's relative immaturity.
That said, for existing applications where a sizeable performance hit is preferable to an
exploitable vulnerability, Fil-C is an excellent choice.
]]></content:encoded></item><item><title>[D] Is mamba architecture not used that much in the field of research?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ojwly3/d_is_mamba_architecture_not_used_that_much_in_the/</link><author>/u/Charming_Bag_1257</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 11:01:20 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[What I have read so far, Mamba arch still shines in handling long contexts (e.g., millions of tokens) much better than Transformers without the memory explosion. I get that when it comes to effectiveness (which we want), the transformer shines and is heavily used in research, but what are the limitations for Mamba? I usually do not find papers using this arch.]]></content:encoded></item><item><title>Weekly: This Week I Learned (TWIL?) thread</title><link>https://www.reddit.com/r/kubernetes/comments/1ojvk1a/weekly_this_week_i_learned_twil_thread/</link><author>/u/gctaylor</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 10:00:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Did you learn something new this week? Share here!]]></content:encoded></item><item><title>Surf update: new TLS fingerprints for Chromium 142</title><link>https://www.reddit.com/r/golang/comments/1ojvfcb/surf_update_new_tls_fingerprints_for_chromium_142/</link><author>/u/Affectionate_Type486</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 09:52:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[An update to Surf, the browser-impersonating HTTP client for Go.The latest version adds support for new TLS fingerprints that match the behavior of the following clients:These fingerprints include accurate ordering of TLS extensions, signature algorithms, supported groups, cipher suites, and use the correct GREASE and key share behavior. JA3 and JA4 hashes match the real browsers, including JA4-R and JA4-O. HTTP/2 Akamai fingerprinting is also consistent..Let me know if you find any mismatches or issues with the new fingerprints.]]></content:encoded></item><item><title>The private conversation anti-pattern in engineering teams</title><link>https://open.substack.com/pub/leadthroughmistakes/p/why-we-tend-to-avoid-public-conversations</link><author>/u/dymissy</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 08:18:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I created a new image format that can describe a full image in as little as 7 bytes</title><link>https://github.com/mohanp06/simple-color-image-format/tree/main</link><author>/u/mpp06</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 08:18:07 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Disclaimer: It's a hobby project, and as of now covers only simple image content. No attempt is made to format it as per the standard image specifications if any. It is an extensible, abstract framework, not restricted to images, and could be potentially useful in some cases.I’ve been experimenting with how minimal an image file format can get — and ended up designing SCIF (Simple Color Image Format).It’s a tiny binary format that stores simple visuals like solid colors, gradients, and checkerboards using only a few bytes.7 bytes for a full solid-color image11 bytes for gradients or patternseasy to decode in under 20 lines of codedesigned for learning, embedded systems, and experiments in data representationI’d love feedback or ideas for extending it — maybe procedural textures, transparency, or even compressed variants. Curious what you think — can such ultra-minimal formats have real use in small devices or demos?]]></content:encoded></item><item><title>The Green Tea Garbage Collector</title><link>https://www.reddit.com/r/golang/comments/1ojtyuq/the_green_tea_garbage_collector/</link><author>/u/Asleep-Actuary-4428</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 08:15:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Here are the details of Green Tea GC. It’s production-ready and already in use at Google, and plan to make it the default in Go 1.26.]]></content:encoded></item><item><title>Harbor in Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1ojtxes/harbor_in_kubernetes/</link><author>/u/Always_smile_student</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 08:12:52 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Harbor in Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1ojtovo/harbor_in_kubernetes/</link><author>/u/Always_smile_student</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 07:56:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Everything was installed successfully, and I set up a NodePort so I can access it via the master node’s IP. Everywhere it says the default login and password are , but I get an “invalid username or password” error.I also tried to check or reset the password using:kubectl -n harbor get secret harbor-core -o jsonpath="{.data.HARBOR_ADMIN_PASSWORD}" | base64 --decode But that password doesn’t work either.]]></content:encoded></item><item><title>Dithering - Part 1</title><link>https://visualrambling.space/dithering-part-1/</link><author>/u/brokePlusPlusCoder</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 07:38:58 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[tap/click the right side of the screen to go forward →I’ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.← tap/click the left side to go backI was even more amazed when I learned how dithering works.← or use arrow keys to navigate →Look closely, and you’ll see this animation is made of alternating black and white pixels.But these black and white pixels are specifically arranged to create the illusion of multiple shades.That’s what dithering does: it simulates more color variations than what are actually used.Here, it uses black and white to give the impression of multiple gray shades.To me, dithering is about creating the most out of what we have, and that's what amazes me the most!It inspired me to learn more about it, and now I want to share what I’ve learned.Please note that this is just part one out of three, so I’ll only scratch the surface here.I’ll go deeper in the next parts, which will come soon. Stay tuned!First, let’s explore the dithering basics with this grayscale image example.A grayscale image has various gray shades, from black to white.Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white—but how?One way is to map each pixel to the closest available color.Pixels darker than medium gray turn black and lighter ones turn white.This splits pixels into black or white groups.However, this creates a harsh image with abrupt black-white transitions.Shadow details vanish as gray pixels become fully black or white.Dithering fixes this by selectively pushing some pixels towards the opposite color.Some light gray pixels that are closer to white turn black.Likewise, some dark grays turn white.And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.Next question: How are these patterns generated?One simple dithering method, known as ordered dithering, uses a threshold map.A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).To dither, we compare each input pixel’s brightness to a corresponding threshold value.If a pixel’s brightness exceeds the threshold (it’s brighter than the threshold), the pixel turns white. Otherwise, it turns black.Repeating this for all pixels gives us the black-and-white dither patterns.The threshold map is designed to output patterns where the black-and-white pixel density matches the input image’s shades.So brighter input produces patterns with more white, while darker input produces more black.These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.To dither larger images, we extend the threshold map to match the image size and follow the same principle:Compare each pixel’s brightness to the threshold map, then turn it black or white accordingly.The image now uses only two colors, but its overall appearance is preserved.The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.And that’s how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.I find it a bit ironic how I used to think dithering ‘adds’ a cool effect, when what it actually does is ‘remove’ colors!That's all for now! We’ve reached the end, but there’s still a lot more to explore.For example, we haven’t explored the algorithm to create a threshold map. (spoiler: there are many ways!)There’s also another algorithm called error diffusion, which doesn’t use a threshold map.Each algorithm creates a distinct, unique look, which I believe deserves its own article.And that's why I decided to break this series into three parts.In the next part, I’ll dive into various algorithms for creating threshold maps.In the final part, I’ll focus on the error diffusion algorithm.We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.I'll keep creating more visual articles like this!https://x.com/damarberlari]]></content:encoded></item><item><title>I’ve been living inside Rust for a while, and Flow-Like is what came out — a typed, local-first workflow engine</title><link>https://github.com/TM9657/flow-like</link><author>/u/tm9657</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 07:08:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I’ve been quietly building , a typed, visual workflow engine written in Rust. Think node-based “blueprints,” but with  — so flows are safer, easier to reason about, and automatically versioned. Everything runs : the desktop app, the backend, even AI and data nodes. There’s no account and no cloud dependency unless you explicitly add one.With  out, you can now actually build real automations — from  and  to , data transforms, or ML pipelines. And, of course, we’ve carefully hidden many bugs for you to find and report. ❤️Flow-Like is a desktop app (built with ) that lets you visually connect typed nodes into executable graphs. Each connection enforces its pin type, so most wiring errors show up before execution. Under the hood there’s a Rust engine that runs your graph directly — no web service, no remote orchestrator. Our backend code is also in our monorepo if that is more interesting to you.For external connectivity, there’s an  that can spin up a local  server, manage , connect to , handle webhooks, timers, file watchers, and more. You can also host it if you want — the backend code for that is included.Every project comes with its own file storage and database powered by the excellent  library — giving you full-text and vector search out of the box, with no setup required.Llama.cpp is embedded for local models and ONNX for local ML and Embeddings. Every flow and node definition is , so you can safely share or roll back changes. custom async executor that runs typed graphs directly. for event endpoints, HTTP handling, and integrations. and  for structured + vector data storage. for table operations and analytics. and  integration for local inference., cross-platform builds for macOS/Windows/Linux. already working (also thanks to Tauri)! The iOS build runs your flows LOCALLY on your phone — just needs a bit more polish before TestFlight.Build  with typed request/response handling.Run  that respond to messages and events.Create  (IMAP fetch, filter, SMTP send).Automate file pipelines, data transforms, or ML tasks.Use  inside flows for full-text and vector search.Stay completely offline — or opt into cloud APIs if you want.Everything happens locally, and everything is versioned — your data, flows, and nodes.If you like the idea (or just want to see how far Rust and Tauri can go), a quiet ⭐️ on GitHub would be very welcome.]]></content:encoded></item><item><title>hpademo - web browser tool for quickly simulating cpu-based hpa</title><link>https://www.reddit.com/r/kubernetes/comments/1ojshyl/hpademo_web_browser_tool_for_quickly_simulating/</link><author>/u/Reasonable-Rice444</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 06:35:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Need a quick tool for simulating cpu-based hpa behavior?hpademo is a simple demo for Kubernetes Horizontal Pod Autoscaler (HPA), written in Go and compiled to WebAssembly in order to run in a web browser.]]></content:encoded></item><item><title>Think memcmp is safe? Think again</title><link>https://github.com/stateless-me/flatline</link><author>/u/aabbdev</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 06:17:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I merged a “tiny perf” patch and unknowingly shipped a  on an auth path. Tests were green. The canary wasn’t. The compiler had “helpfully” turned a compare into an  and my branch on that result amplified the signal. That week taught me two things:  and C won’t keep you from leaking it.So I wrote  a single-header, zero-dep, zero-heap toolbox for writing  C (C99/C11). But first, a quick ego check.if (memcmp(tag_a, tag_b, TAG_LEN) == 0) { memcpy(dst, src, len); } B. Secret-conditioned branchif (secret_bit) { memcpy(dst, src, len); } uint8_t out = sbox[secret_idx]; D. Ternary select on a secretuint32_t y = secret_bit ? a : b;  (prefix-match timing) (predictor + divergent paths) (cache trace keyed by the secret). →  may compile to a  (leaky). Even if it becomes a , only  may be evaluated (loads/calls) → secret-dependent memory traffic/time. Safe only if both operands are already plain values and the compiler emits a true constant-time select — which you shouldn’t bet on.Looks correct ≠ constant-time. Your toolchain, caches, and predictors also write the story.B. Branch on secrets (don’t) If execution path depends on a secret, timing does too.if (secret_bit) memcpy(dst, src, len); uint32_t chosen = flat_select_u32(secret_bit, yes, no); // branchless select flat_memcpy_when((unsigned)secret_bit, dst, src, len); // gated copy w/o branch I. Index memory with secrets (don’t) Secret-dependent addresses leave cache footprints.uint8_t out = sbox[secret_idx]; uint8_t out = flat_lookup_u8(sbox, 256, secret_idx); // sweep + mask, constant-time // or: flat_table_apply_u8(outbuf, inbuf, n, sbox, 256); D. Don’t use variable-latency ops on secrets Some instructions take  time; operands from secrets = side channel.q = n / secret_d; r = n % secret_d; uint64_t q, r; unsigned ok = flat_div_mod_ct_u64(n, secret_d, &q, &r); // constant-time pattern Early-exit compares betray how many leading bytes matched. If you then branch on the result, you amplify the leak.int eq = flat_mem_eq(tag_a, tag_b, TAG_LEN); // no early exit flat_memcpy_when((unsigned)eq, dst, src, len); // no branch on secret Constant-time equality + copy#include "flatline.h" int ok = flat_mem_eq(tag_a, tag_b, TAG_LEN); flat_memcpy_when((unsigned)ok, dst, src, len); uint32_t y = flat_select_u32(secret_bit, yes, no); uint8_t v = flat_lookup_u8(sbox, 256, secret_idx); Constant-time div/mod (when operands may be secret)uint64_t q, r; unsigned okdiv = flat_div_mod_ct_u64(n, d, &q, &r); clib install stateless-me/flatline # or drop flatline.h into your project Compilers are “too smart.” LTO and auto-vectorization can reintroduce branches or table lookups you thought you’d eliminated. (Consider  in strict builds.) MB/s benchmarks won’t tell you if you’re constant-time. You need statistics (e.g., DUDECT-style t-tests) on  target CPU. Loops must run the same number of iterations regardless of secrets. “Break on first mismatch” is a timing oracle in disguise. Cross-process boundaries make tiny timing differences very measurable.Anywhere you compare tags, gate access, check tokens, or touch data-derived indices can betray you:Feature flags around sensitive pathsTable-driven transforms (S-boxes, LUTs, routing tables)The B.I.D. mental model keeps you honest: — don’t ndex memory with secrets — on’t use variable-latency ops on secretsAdd two habits:  and .Single header, no deps, no heap; C99/C11; usable from C++Primitives designed to not branch/index on secretsOptional SIMD (NEON/SSE2/AVX2) paths that remain constant-time w.r.t. Unit tests, micro-benches, and a DUDECT-like harnessIt reduces timing variance. It doesn’t fix power/EM channels or OS noise. Threat models still apply.If you picked any of A/B/C as “safe,” you’re not alone I did too, before a pager taught me otherwise. Try the patterns above, run a DUDECT-style check, and tell me where your toolchain fought back (LTO, builtins, odd  rewrites).If time varies, secrets leak. Flatline it.Proposal: introduce a secret keyword in systems languages (Rust, Zig, LLVM/C) to mark sensitive data. The compiler enforces constant-time rules and replaces unsafe patterns with constant-time APIs.]]></content:encoded></item><item><title>SNMP on Linux stats without running the service.</title><link>https://www.reddit.com/r/linux/comments/1ojrka4/snmp_on_linux_stats_without_running_the_service/</link><author>/u/lickety-split1800</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 05:35:35 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Over 20 years ago (man I feel old), I had set up SNMP on Linux with Nagios and RRDTool.While SNMP is hardly used anymore on Linux it had a lot of metrics that it collected, which was super useful for sending stats to either Nagios or RRDTool at the time.Is there anything else out of the box that has a large set of monitors on Linux?What are your favourite out-of-the-box Linux metrics collection tools?]]></content:encoded></item><item><title>Alternative to the LogiOptions+ new Action Ring Feature</title><link>https://www.reddit.com/r/linux/comments/1ojr3s9/alternative_to_the_logioptions_new_action_ring/</link><author>/u/GarThor_TMK</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 05:07:35 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[P] I made a tool to search papers from selected AI venues</title><link>https://www.reddit.com/r/MachineLearning/comments/1ojqgq4/p_i_made_a_tool_to_search_papers_from_selected_ai/</link><author>/u/ZealousidealStock933</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 04:30:28 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[It uses a language model as backbone so you can query with title, keywords, or even a paper abstract to search. Paper abstracts are the most accurate. It hosted on a personal server as well as on hugging face. Links are in my repo. https://github.com/wenhangao21/ICLR26_Paper_Finder]]></content:encoded></item><item><title>Unbound on talos</title><link>https://www.reddit.com/r/kubernetes/comments/1ojqf7t/unbound_on_talos/</link><author>/u/Agreeable_Repeat_568</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 04:28:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am trying to get unbond to run rootless on talos and it seems like it might not be possible? Has anyone gotten current images of unbound running rootless? Iv tried too many options to list, just looking to see if this is even possible?]]></content:encoded></item><item><title>I&apos;m tired of Web Dev</title><link>https://www.reddit.com/r/golang/comments/1ojpxv2/im_tired_of_web_dev/</link><author>/u/Financial_Job_1564</author><category>golang</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 04:01:34 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I already have some experience with Golang from building backend projects, but I still feel like I don’t really know much about it.Can you give me some examples of projects that use Golang besides backend servers?   submitted by    /u/Financial_Job_1564 ]]></content:encoded></item><item><title>Can&apos;t get data on heavy outputs</title><link>https://www.reddit.com/r/linux/comments/1ojo6a1/cant_get_data_on_heavy_outputs/</link><author>/u/BassDJ812</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 02:32:09 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So as you can see I can't read date at the bottom and terminal. the only way to get it where it is now is control-minus control. Plus doesn't work I can't scroll down to the data I need on heavy output. I've tried to manipulate the preference menu the best I can in between that and Google research I'm not coming up with any luck. I'm on Kali not that I think that makes a difference. Any help most appreciated ]]></content:encoded></item><item><title>Meta, Google, and Microsoft Triple Down on AI Spending</title><link>https://www.wired.com/story/microsoft-google-meta-2025-earnings/</link><author>/u/wiredmagazine</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 02:14:18 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[ biggest US tech giants—Microsoft, Meta, and Google—sent investors a blunt message when they reported quarterly earnings on Wednesday: Their lavish spending on AI infrastructure is only just getting started.Meta said that ​​its capital expenditure would total between $70 billion and $72 billion this year, up from its previous lower forecast of $66 billion to $72 billion. Meta’s chief financial officer Susan Li said that she expected the company's spending would be “notably larger" next year. The social media giant’s soaring investment matches its soaring revenue: Meta reported raking in $51.24 billion last quarter, up 26 percent year over year.CEO Mark Zuckerberg said the company would keep pouring money into infrastructure to meet rising demand for AI and to prepare for potential major breakthroughs in the technology. "There's a range of timelines for when people think that we're going to get superintelligence," Zuckerberg said on a conference call with analysts. "I think that it's the right strategy to aggressively front-load building capacity, so that way we're prepared for the most optimistic cases."Meta has moved aggressively to recruit AI talent in recent months, offering some researchers compensation packages worth hundreds of millions of dollars. The company also cut some 600 jobs last week in what it said was an effort to make its AI teams more efficient. Meta has reorganized its AI teams numerous times over the past eight months.Meta assured investors that its AI investments were already reaping rewards for the company, but didn’t share many specifics. Meta did say AI was benefiting its ad business and virtual reality product lines, and predicted it would propel those divisions to new heights in the future.Google’s parent company, Alphabet, said it expected its 2025 capital expenditures to be between $91 billion and $93 billion. Earlier this year, Alphabet estimated that number would be just $75 billion. Like at Meta, the increase in spending was matched with an increase in revenue. The tech giant said it earned a record $102.3 billion in the third quarter, up 33 percent from a year ago.Most of Alphabet’s spending will likely be funneled into data centers and other artificial intelligence initiatives. Google said it earned $15.15 billion from its cloud business in the third quarter, a 35 percent increase from the same period in 2024. Gemini, Google’s general purpose AI app, now has 650 million monthly active users, up from 450 million last quarter. (For comparison, OpenAI CEO Sam Altman recently said that ChatGPT has 800 million weekly users.)Microsoft reported revenues of $77 billion for the quarter ending on September 30, up 18 percent from a year ago. Its cloud business revenue was up 26 percent year over year. Its capital expenditures were $34.9 billion this quarter, with much of the investment going toward AI infrastructure. That figure is nearly $5 billion more than previously forecasted, and a 74 percent jump from the same quarter a year ago.]]></content:encoded></item><item><title>What&apos;s going on with the dislike of Ubuntu/Canonical?</title><link>https://www.reddit.com/r/linux/comments/1ojn5oz/whats_going_on_with_the_dislike_of_ubuntucanonical/</link><author>/u/megaslash288</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 01:44:41 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Basically the title. I have been using Ubuntu Cinnamon for awhile for both school and home use (gaming, programming, vr, game dev, etc) and it seems to be a very well maintained, functional, and stable distro with good support through .deb packages for printers n such, and the apt repository seems well checked and stable. It also seems to have good peripheral drivers built in. Seemed a little silly for it to not include flatpack by default but thats easily fixed. Overall though, I am still a relative newcomer to Linux and haven't been in the community that much until recently, so I am a lil out of the loop.]]></content:encoded></item><item><title>John Carmack on updating variables</title><link>https://x.com/ID_AA_Carmack/status/1983593511703474196#m</link><author>/u/levodelellis</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 01:32:49 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Do you know what really happens when you run kubectl apply?</title><link>https://medium.com/integr8me/what-happens-when-you-run-kubectl-apply-ce3bfb5e61c4</link><author>/u/fabioluciano</author><category>reddit</category><pubDate>Thu, 30 Oct 2025 01:03:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Recently, while working on the documentation about best practices for building solutions with , I came across some topics that, despite being fundamental, usually receive little attention when we talk about Kubernetes.These topics seemed so interesting to me that I decided to turn them into a series of articles — and this is the first one.To start, I want to ask you a simple question:Do you know exactly what happens when you run kubectl apply -f manifest.yamlIt might seem like a trivial action, but behind this command lies a well-structured process, divided into three major stages: , , and .In fact, the topic that most sparked my interest was the  process, but it’s impossible to fully understand it without first going through the other two. These three components work together and form the foundation for all Kubernetes security and access control.Despite seeming simple, each of these elements — , , and  — is highly configurable and full of nuances. Understanding how they connect helps not only to solve day-to-day problems but also to create more secure and elegant solutions.Let’s start by understanding :When a user executes the command kubectl apply -f manifest.yaml, the request’s journey begins like this:: Kubernetes identifies  is making the call.: The system checks  this person (or service) has permission to perform the operation.: The resource sent is analyzed, validated, and in some cases, modified before being stored in .Only after these three steps is the change effectively applied to the cluster.Now that we have an overview, let’s dive into each of these processes, starting with Authentication.If you’ve been working with Kubernetes for some time, you probably know that every interaction made with  ends up reaching the , which is the heart of the cluster.It works similarly to a : each command is translated into an HTTP request containing information about the desired resource, the type of operation, and who is making the call.We can observe this message exchange in real-time by enabling the verbosity flag on , with .The higher the number, the more details are displayed. For example:Let’s break down what is being displayed in the output of the command  👇1️⃣  This is where everything begins.  received the  command and, thanks to the  flag, is displaying internal details of its communication with the API Server. This verbosity level shows, among other things, which configurations are being loaded and which HTTP requests are being made.2️⃣ Configuration file loaded reports that it loaded the credentials and context from the  file. This file contains the information that identifies , which cluster is being accessed, and how to authenticate to it (for example, via token, certificate, or OIDC provider).3️⃣ Request sent to the API Server Here we see  transforming the  command into an  to the endpoint:This is te moment when the call truly “leaves your terminal” and reaches Kubernetes. The request header (headers) includes authentication information (such as tokens), the expected response format (), and the  which indicates the  version.4️⃣  The API Server processes the request, authenticates the user, checks if they have permission (authorization), and, if everything is correct, returns the result with the  status. Following this, we see the list of Pods in the  namespace, indicating that the request was accepted and completed successfully.Authentication is, therefore, the first filter: it defines who the user or service is that is trying to interact with the cluster.This identification can happen in several ways — such as via certificates, service tokens, identity providers (OIDC), or even custom plugins.After the user or service is authenticated, the next step is to verify what they can do within the cluster. This is the role of .While authentication answers the question , authorization answers “.And this decision is always made by the API Server, before the request proceeds.Kubernetes offers different , and each has its purpose. The most common are:RBAC (Role-Based Access Control) — the most used. Defines permissions based on roles and links these roles to users, groups, or service accounts.ABAC (Attribute-Based Access Control) — uses attributes defined in JSON policies, allowing for more flexible but less practical rules to manage. — forwards the decision to an external service, ideal for integrations with corporate authentication systems. and / — used in very specific cases (the first for internal node authentication and the second generally for testing).In practice,  is the most common model, as it is simple and integrated into the Kubernetes ecosystem.In this example, the user  can  in the  namespace, but cannot create, delete, or modify any.With RBAC configured, the API Server consults these rules whenever a request arrives.If the user tries to do something outside the assigned permissions, they will receive a  error — and the process is interrupted even before reaching the  stage.If authentication identifies the user and authorization confirms they can perform the action, the  process is the moment when Kubernetes analyzes and modifies what will be created or changed before it goes to the database ().This is where one of the most powerful parts of Kubernetes comes in: the .Think of them as “doormen” who inspect everything entering the cluster. They can , , or even  resources before Kubernetes accepts the operation.There are two main types:Validating Admission Controllers — analyze the request and decide whether it is valid or not.Mutating Admission Controllers — can alter the request before it is persisted.A practical example: imagine your company wants to ensure that every Pod has a label indicating the responsible team.An Admission Controller can automatically reject any Pod that does not have this label, or even add the missing label (in the case of a mutating controller).Kubernetes already comes with several ready-made Admission Controllers — such as , , , among others — , but it is also possible to create your own using .A , for example, can intercept the creation of a Pod and automatically inject sidecars (such as a logging, metrics, or security container) (hello ).This type of automation is widely used by tools like , , and various  that manage custom resources.A simplified flow would look like this:Note that the admission process is the last step before the resource is written.This means that any modification made at this point is the last opportunity to adjust, validate, or reinforce security and compliance policies.The next time you run a simple , remember everything that happens behind the scenes:Kubernetes first checks who you are (Authentication),then verifies what you can do (Authorization),and finally analyzes and adjusts what you are trying to create (Admission).These three processes are the foundation of all cluster security and governance.Understanding them is essential not only for those who administer Kubernetes but also for those who develop solutions that interact with it — such as sidecars, operators, or custom webhooks.In the next articles in the series, I will explore each of these components in greater depth, bringing practical examples, use cases, and even some curiosities that are rarely covered in the official documentation.]]></content:encoded></item><item><title>Exclusive: OpenAI lays groundwork for juggernaut IPO at up to $1 trillion valuation</title><link>https://www.reuters.com/business/openai-lays-groundwork-juggernaut-ipo-up-1-trillion-valuation-2025-10-29/</link><author>/u/esporx</author><category>ai</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 00:31:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernetes Podcast episode 262: GKE 10 Year Anniversary, with Gari Singh</title><link>https://www.reddit.com/r/kubernetes/comments/1ojkuho/kubernetes_podcast_episode_262_gke_10_year/</link><author>/u/kubernetespodcast</author><category>reddit</category><pubDate>Wed, 29 Oct 2025 23:58:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Google Kubernetes Engine (GKE) recently celebrated its 10th anniversary! 🎉 In our latest podcast episode, we talk with GKE Product Manager Gari Singh to reflect on GKE's journey over the last decade. From the early days of complex container orchestration to today's 'one-click' production clusters powered by Autopilot, and the continuous effort to simplify infrastructure management. How GKE supports demanding AI workloads and the exciting potential of leveraging AI  Kubernetes, enabling smarter, more autonomous operations and enhanced observability. Gary's favorite features, including In-Place Pod Resizing (IPPR) and Container Optimized Compute, which are crucial for dynamic scaling and efficiency.   submitted by    /u/kubernetespodcast ]]></content:encoded></item><item><title>Increase Performance when sending struct accross HTTP / TCP</title><link>https://www.reddit.com/r/golang/comments/1ojktka/increase_performance_when_sending_struct_accross/</link><author>/u/D4kzy</author><category>golang</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 23:57:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I have a client and a server that talk HTTP (sometimes raw TCP).On the client I define a struct that has a string field, a []string field and a []byte field.I define the same struct server side.I want to send this instantiated struct from the client to the server.What I did till now is use the json marshall to send the data as a json through the Conn.I have slight performance issues and I thing it is coming from here. My guess is that when I marshal and unmarshal with json, the []byte field of my struct is base64 encoded. When []byte is big this is adding around 33% overhead.To avoid this I thought about GZIP, but I am afraid the GZIP computation time will result in even poorer perf.What way to send data do you suggest to have best speed (sending a lot of HTTP request) ?]]></content:encoded></item><item><title>How To Be A Linux-Based Graphic Designer</title><link>https://www.youtube.com/watch?v=sVztMTafuLA</link><author>/u/yoor_thiziri</author><category>reddit</category><pubDate>Wed, 29 Oct 2025 22:45:15 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go 1.25 includes a new experimental garbage collector, Green Tea</title><link>https://go.dev/blog/greenteagc</link><author>/u/x021</author><category>golang</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 22:27:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Go 1.25 includes a new experimental garbage collector called Green Tea,
available by setting  at build time.
Many workloads spend around 10% less time in the garbage collector, but some
workloads see a reduction of up to 40%!It’s production-ready and already in use at Google, so we encourage you to
try it out.
We know some workloads don’t benefit as much, or even at all, so your feedback
is crucial to helping us move forward.
Based on the data we have now, we plan to make it the default in Go 1.26.What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.
We’ll update this blog post with a link to the talk once it’s available online.Tracing garbage collectionBefore we discuss Green Tea let’s get us all on the same page about garbage
collection.The purpose of garbage collection is to automatically reclaim and reuse memory
no longer used by the program.To this end, the Go garbage collector concerns itself with  and
.In the context of the Go runtime,  are Go values whose underlying
memory is allocated from the heap.
Heap objects are created when the Go compiler can’t figure out how else to allocate
memory for a value.
For example, the following code snippet allocates a single heap object: the backing
store for a slice of pointers.var x = make([]*int, 10) // global
The Go compiler can’t allocate the slice backing store anywhere except the heap,
since it’s very hard, and maybe even impossible, for it to know how long  will
refer to the object for. are just numbers that indicate the location of a Go value in memory,
and they’re how a Go program references objects.
For example, to get the pointer to the beginning of the object allocated in the
last code snippet, we can write:Go’s garbage collector follows a strategy broadly referred to as tracing garbage
collection, which just means that the garbage collector follows, or traces, the
pointers in the program to identify which objects the program is still using.More specifically, the Go garbage collector implements the mark-sweep algorithm.
This is much simpler than it sounds.
Imagine objects and pointers as a sort of graph, in the computer science sense.
Objects are nodes, pointers are edges.The mark-sweep algorithm operates on this graph, and as the name might suggest,
proceeds in two phases.In the first phase, the mark phase, it walks the object graph from well-defined
source edges called .
Think global and local variables.
Then, it  everything it finds along the way as , to avoid going in
circles.
This is analogous to your typical graph flood algorithm, like a depth-first or
breadth-first search.Next is the sweep phase.
Whatever objects were not visited in our graph walk are unused, or ,
by the program.
We call this state unreachable because it is impossible with normal safe Go code
to access that memory anymore, simply through the semantics of the language.
To complete the sweep phase, the algorithm simply iterates through all the
unvisited nodes and marks their memory as free, so the memory allocator can reuse
it.You may think I’m oversimplifying a bit here.
Garbage collectors are frequently referred to as , and .
And you’d be partially right, there are more complexities.For example, this algorithm is, in practice, executed concurrently with your
regular Go code.
Walking a graph that’s mutating underneath you brings challenges.
We also parallelize this algorithm, which is a detail that’ll come up again
later.But trust me when I tell you that these details are mostly separate from the
core algorithm.
It really is just a simple graph flood at the center.Let’s walk through an example.
Navigate through the slideshow below to follow along.After all that, I think we have a handle on what the Go garbage collector is actually doing.
This process seems to work well enough today, so what’s the problem?Well, it turns out we can spend  of time executing this particular algorithm in some
programs, and it adds substantial overhead to nearly every Go program.
It’s not that uncommon to see Go programs spending 20% or more of their CPU time in the
garbage collector.Let’s break down where that time is being spent.At a high level, there are two parts to the cost of the garbage collector.
The first is how often it runs, and the second is how much work it does each time it runs.
Multiply those two together, and you get the total cost of the garbage collector.
    Total GC cost = Number of GC cycles × Average cost per GC cycle
    But for now let’s focus only on the second part, the cost per cycle.From years of poring over CPU profiles to try to improve performance, we know two big things
about Go’s garbage collector.The first is that about 90% of the cost of the garbage collector is spent marking,
and only about 10% is sweeping.
Sweeping turns out to be much easier to optimize than marking,
and Go has had a very efficient sweeper for many years.The second is that, of that time spent marking, a substantial portion, usually at least 35%, is
simply spent  on accessing heap memory.
This is bad enough on its own, but it completely gums up the works on what makes modern CPUs
actually fast.“A microarchitectural disaster”What does “gum up the works” mean in this context?
The specifics of modern CPUs can get pretty complicated, so let’s use an analogy.Imagine the CPU driving down a road, where that road is your program.
The CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,
and the way needs to be clear.
But the graph flood algorithm is like driving through city streets for the CPU.
The CPU can’t see around corners and it can’t predict what’s going to happen next.
To make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid
pedestrians.
It hardly matters how fast your engine is because you never get a chance to get going.Let’s make that more concrete by looking at our example again.
I’ve overlaid the heap here with the path that we took.
Each left-to-right arrow represents a piece of scanning work that we did
and the dashed arrows show how we jumped around between bits of scanning work.Notice that we were jumping all over memory doing tiny bits of work in each place.
In particular, we’re frequently jumping between pages, and between different parts of pages.Modern CPUs do a lot of caching.
Going to main memory can be up to 100x slower than accessing memory that’s in our cache.
CPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to
recently accessed memory.
But there’s no guarantee that any two objects that point to each other will  be close to each
other in memory.
The graph flood doesn’t take this into account.Quick side note: if we were just stalling fetches to main memory, it might not be so bad.
CPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see
far enough ahead.
But in the graph flood, every bit of work is small, unpredictable, and highly dependent on the
last, so the CPU is forced to wait on nearly every individual memory fetch.And unfortunately for us, this problem is only getting worse.
There’s an adage in the industry of “wait two years and your code will get faster.”But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.
“Wait two years and your code will get slower.”
The trends in modern CPU hardware are creating new challenges for garbage collector performance:Non-uniform memory access.
For one, memory now tends to be associated with subsets of CPU cores.
Accesses by  CPU cores to that memory are slower than before.
In other words, the cost of a main memory access depends on which CPU core is accessing
it.
It’s non-uniform, so we call this non-uniform memory access, or NUMA for short.Reduced memory bandwidth.
Available memory bandwidth per CPU is trending downward over time.
This just means that while we have more CPU cores, each core can submit relatively fewer
requests to main memory, forcing non-cached requests to wait longer than before.
Above, we looked at a sequential marking algorithm, but the real garbage collector performs this
algorithm in parallel.
This scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes
a bottleneck, even with careful design.Modern hardware features.
New hardware has fancy features like vector instructions, which let us operate on a lot of data at once.
While this has the potential for big speedups, it’s not immediately clear how to make that work for
marking because marking does so much irregular and often small pieces of work.Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.
The key idea behind Green Tea is astonishingly simple:Work with pages, not objects.Sounds trivial, right?
And yet, it took a lot of work to figure out how to order the object graph walk and what we needed to
track to make this work well in practice.More concretely, this means:Instead of scanning objects we scan whole pages.Instead of tracking objects on our work list, we track whole pages.We still need to mark objects at the end of the day, but we’ll track marked objects locally to each
page, rather than across the whole heap.Let’s see what this means in practice by looking at our example heap again, but this time
running Green Tea instead of the straightforward graph flood.As above, navigate through the annotated slideshow to follow along.Let’s come back around to our driving analogy.
Are we finally getting on the highway?Let’s recall our graph flood picture before.We jumped around a whole lot, doing little bits of work in different places.
The path taken by Green Tea looks very different.Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.
The longer these arrows, the better, and with bigger heaps, this effect can be much stronger.
 the magic of Green Tea.It’s also our opportunity to ride the highway.This all adds up to a better fit with the microarchitecture.
We can now scan objects closer together with much higher probability, so
there’s a better chance we can make use of our caches and avoid main memory.
Likewise, per-page metadata is more likely to be in cache.
Tracking pages instead of objects means work lists are smaller,
and less pressure on work lists means less contention and fewer CPU stalls.And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to
before, since now we can use vector hardware!If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.
But besides the usual arithmetic and trigonometric operations,
recent vector hardware supports two things that are valuable for Green Tea:
very wide registers, and sophisticated bit-wise operations.Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.
This is wide enough to hold all of the metadata for an entire page in just two registers,
right on the CPU, enabling Green Tea to work on an entire page in just a few straight-line
instructions.
Vector hardware has long supported basic bit-wise operations on whole vector registers, but starting
with AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction
that enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.
Together, these allow us to turbo-charge the Green Tea scan loop.This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that
are all sorts of different sizes.
Sometimes you needed two bits of metadata and sometimes you needed ten thousand.
There simply wasn’t enough predictability or regularity to use vector hardware.If you want to nerd out on some of the details, read along!
Otherwise, feel free to skip ahead to the evaluation.To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.There’s a lot going on here and we could probably fill an entire blog post just on how this works.
For now, let’s just break it down at a high level:First we fetch the “seen” and “scanned” bits for a page.
Recall, these are one bit per object in the page, and all objects in a page have the same size.Next, we compare the two bit sets.
Their union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,
which tells us which objects we need to scan in this pass over the page (versus previous passes).We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,
we have one bit per word (8 bytes) of the page.
We call this the “active words” bitmap.
For example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap
will be copied to 6 bits in the active words bitmap.
Like so: → 000000 000000 111111 111111 ...Next we fetch the pointer/scalar bitmap for the page.
Here, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word
stores a pointer.
This data is managed by the memory allocator.Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.
The result is the “active pointer bitmap”: a bitmap that tells us the location of every
pointer in the entire page contained in any live object we haven’t scanned yet.Finally, we can iterate over the memory of the page and collect all the pointers.
Logically, we iterate over each set bit in the active pointer bitmap,
load the pointer value at that word, and write it back to a buffer that
will later be used to mark objects seen and add pages to the work list.
Using vector instructions, we’re able to do this 64 bytes at a time,
in just a couple instructions.Part of what makes this fast is the  instruction,
part of the “Galois Field New Instructions” x86 extension,
and the bit manipulation Swiss army knife we referred to above.
It’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very
efficiently.
It performs a bit-wise affine
transformations,
treating each byte in a vector as itself a mathematical vector of 8 bits
and multiplying it by an 8x8 bit matrix.
This is all done over the Galois field,
which just means multiplication is AND and addition is XOR.
The upshot of this is that we can define a few 8x8 bit matrices for each
object size that perform exactly the 1:n bit expansion we need.For the full assembly code, see this
file.
The “expanders” use different matrices and different permutations for each size class,
so they’re in a separate file
that’s written by a code generator.
Aside from the expansion functions, it’s really not a lot of code.
Most of it is dramatically simplified by the fact that we can perform most of the above
operations on data that sits purely in registers.
And, hopefully soon this assembly code will be replaced with Go code!Credit to Austin Clements for devising this process.
It’s incredibly cool, and incredibly fast!So that’s it for how it works.
How much does it actually help?It can be quite a lot.
Even without the vector enhancements, we see reductions in garbage collection CPU costs
between 10% and 40% in our benchmark suite.
For example, if an application spends 10% of its time in the garbage collector, then that
would translate to between a 1% and 4% overall CPU reduction, depending on the specifics of
the workload.
A 10% reduction in garbage collection CPU time is roughly the modal improvement.
(See the GitHub issue for some of these details.)We’ve rolled Green Tea out inside Google, and we see similar results at scale.We’re still rolling out the vector enhancements,
but benchmarks and early results suggest this will net an additional 10% GC CPU reduction.While most workloads benefit to some degree, there are some that don’t.Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a
single page in one pass to counteract the costs of the accumulation process.
This is clearly the case if the heap has a very regular structure: objects of the same size at a
similar depth in the object graph.
But there are some workloads that often require us to scan only a single object per page at a time.
This is potentially worse than the graph flood because we might be doing more work than before while
trying to accumulate objects on pages and failing.The implementation of Green Tea has a special case for pages that have only a single object to scan.
This helps reduce regressions, but doesn’t completely eliminate them.However, it takes a lot less per-page accumulation to outperform the graph flood
than you might expect.
One surprise result of this work was that scanning a mere 2% of a page at a time
can yield improvements over the graph flood.Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled
by setting the environment variable  to  at build time.
This doesn’t include the aforementioned vector acceleration.We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out
with GOEXPERIMENT=nogreenteagc at build time.
Go 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of
tweaks and improvements based on feedback we’ve collected so far.If you can, we encourage you to try at Go tip-of-tree!
If you prefer to use Go 1.25, we’d still love your feedback.
See this GitHub
comment with some details on
what diagnostics we’d be interested in seeing, if you can share, and the preferred channels for
reporting feedback.Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.
The human element of the technology.The core of Green Tea may seem like a single, simple idea.
Like the spark of inspiration that just one single person had.But that’s not true at all.
Green Tea is the result of work and ideas from many people over several years.
Several people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David
Chase, and Keith Randall.
Microarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped
direct the design exploration.
There were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.
Just to make this single, simple idea viable.The seeds of this idea go all the way back to 2018.
What’s funny is that everyone on the team thinks someone else thought of this initial idea.Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe
crawling in Japan and drinking LOTS of matcha!
This prototype showed that the core idea of Green Tea was viable.
And from there we were off to the races.Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even
further.This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire
design space.
One that we don’t think any of us could’ve navigated alone.
It’s not enough to just have the idea, but you need to figure out the details and prove it.
And now that we’ve done it, we can finally iterate.The future of Green Tea is bright.Once again, please try it out by setting  and let us know how it goes!
We’re really excited about this work and want to hear from you!]]></content:encoded></item><item><title>[R] Researchers from the Center for AI Safety and Scale AI have released the Remote Labor Index (RLI), a benchmark testing AI agents on 240 real-world freelance jobs across 23 domains.</title><link>https://www.reddit.com/r/MachineLearning/comments/1ojinwl/r_researchers_from_the_center_for_ai_safety_and/</link><author>/u/michael-lethal_ai</author><category>ai</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 22:27:21 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[They find current AI agents have low but steadily improving performance. The best-performing agent (Manus) successfully completed 2.5% of projects, earning $1,720 out of a possible $143,991. However, newer models consistently perform better than older ones, indicating measurable advancement toward automating remote work.]]></content:encoded></item></channel></rss>