<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rust</title><link>https://www.awesome-dev.news</link><description></description><item><title>Request for Comments: Moderating AI-generated Content on /r/rust</title><link>https://www.reddit.com/r/rust/comments/1qptoes/request_for_comments_moderating_aigenerated/</link><author>/u/DroidLogician</author><category>rust</category><category>reddit</category><pubDate>Thu, 29 Jan 2026 00:49:12 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[We, your /r/rust moderator team, have heard your concerns regarding AI-generated content on the subreddit, and we share them. The opinions of the moderator team on the value of generative AI run the gamut from "cautiously interested" to "seething hatred", with what I percieve to be a significant bias toward the latter end of the spectrum. We've been discussing for months how we want to address the issue but we've struggled to come to a consensus.On the one hand, we want to continue fostering a community for high-quality discussions about the Rust programming language, and AI slop posts are certainly getting in the way of that. However, we have to concede that there are legitimate use-cases for gen-AI, and we hesitate to adopt any policy that turns away first-time posters or generates a ton more work for our already significantly time-constrained moderator team.So far, we've been handling things on a case-by-case basis. Because Reddit doesn't provide much transparency into moderator actions, it may appear like we haven't been doing much, but in fact most of our work lately has been quietly removing AI slop posts.In no particular order, I'd like to go into some of the challenges we're currently facing, and then conclude with some of the action items we've identified. We're also happy to listen to any suggestions or feedback you may have regarding this issue. Please constrain meta-comments about generative AI to this thread, or feel free to send us a modmail if you'd like to talk about this privately.A lot of people seem to be under the conception that we approve every single post and comment before it goes up, or that we're checking every single new post and comment on the subreddit for violations of our rules.By and large, we browse the subreddit just like anyone else. No one is getting paid to do this, we're all volunteers. We all have lives, jobs, and value our time the same as you do. We're not constantly scrolling through Reddit (I'm not at least). We live in different time zones, and there's significant gaps in coverage. We may have a lot of moderators on the roster, but only a handful are regularly active.When someone asks, "it's been 12 hours already, why is this still up?" the answer usually is, "because no one had  it yet." Or sometimes, someone is waiting for another mod to come online to have another person to confer with instead of taking a potentially controversial action unilaterally.Some of us also still use old Reddit because we don't like the new design, but the different frontends use different sorting algorithms by default, so we might see posts in a different order. If you feel like you've seen a lot of slop posts lately, you might try switching back to old Reddit (old.reddit.com).While there is an option to require approvals for all new posts, that simply wouldn't scale with the current size of our moderator team. A lot of users who post on /r/rust are posting for the first time, and requiring them to seek approval first might be too large of a barrier to entry.There is really no reliable quantitative test for AI-generated content. When working on a previous draft of this announcement (which was 8 months ago now), I had put several posts into multiple "AI detector" results from Google, and gotten responses from "80% AI generated" to "80% human generated" for the same post. I think it's just a crapshoot depending on whether the AI detector you use was trained on the output of the model allegedly used to generate the content. Averaging multiple results will likely end up inconclusive more often than not. And that's just the ones that aren't behind a paywall.Ironically, this makes it very hard to come up with any automated solution, and Reddit's mod tools have not been very helpful here either.We could just have it automatically remove all posts with links to github.com or containing emojis or em-dashes, but that's about it. There's no magic "remove all AI-generated content" rule.So we're stuck with subjective examination, having to  posts with our own eyes and seeing if it passes our sniff tests. There's a number of hallmarks that we've identified as being endemic to AI-generated content, which certainly helps, but so far there doesn't really seem to be any way around needing a human being to look at the thing and see if the vibe is off.But this also means that it's up to each individual moderator's definition of "slop", which makes it impossible to apply a policy with any consistency. We've sometimes  on whether some posts were slop or not, and in a few cases, we actually ended up reversing a moderator decision.Regardless of our own feelings, we have to concede that generative AI is likely here to stay, and there  legitimate use-cases for it. I don't personally use it, but I do see how it can help take over some of the busywork of software development, like writing tests or bindings, where there isn't a whole lot of creative effort or critical thought required.We've come across a number of posts where the author  to using generative AI, but found that the project was still high enough quality that it merited being shared on the subreddit.This is why we've chosen not to introduce a rule blanket-banning AI-generated content. Instead, we've elected to handle AI slop through the existing lens of our low-effort content rule. If it's obvious that AI did all the heavy lifting, that's by definition low-effort content, and it doesn't belong on the subreddit. Simple enough, right?Secondly, there is a large cohort of Reddit users who do not read or speak English, but we require all posts to be in English because it's is the only common language we share on the moderator team. We can't moderate posts in languages we don't speak.However, this would effectively render the subreddit inaccessible to a large portion of the world, if it  for machine translation tools. This is something I personally think LLMs have the potential to be very good at; after all, the vector space embedding technique that LLMs are now built upon was originally developed for machine translation.The problem we've encountered with translated posts is they tend to  slop, because these chatbots tend to re-render the user's original meaning in their sickly corporate-speak voices and add lots of flashy language and emojis (because that's what trending posts do, I guess). These users end up receiving a lot of vitriol for this which I personally feel like they don't deserve.We need to try to be more patient with these users. I think what we'd like to do in these cases is try to educate posters about the better translation tools that are out there (maybe help us put together a list of what those are?), and encourage them to double-check the translation and ensure that it still reads in  "voice" without a lot of unnecessary embellishment. We'd also be happy to partner with any non-English Rust communities out there, and help people connect with other enthusiasts who speak their language.I've seen a few comments lately on alleged "AI slop" posts that crossed the line into abuse, and that's downright unacceptable. Just because someone may have violated the community rules does  mean they've adbicated their right to be treated like a human being.That kind of toxicity may be allowed and even embraced elsewhere on Reddit, but it directly flies in the face of our community values, and it is not allowed at  time on the subreddit. If you don't feel that you have the ability to remain civil, just downvote or report and move on.Note that this also means that we don't need to see a new post every single day  the slop. Meta posts are against our on-topic rule and may be removed at moderator discretion. In general, if you have an issue or suggestion about the subreddit itself, we prefer that you bring it to us directly so we may discuss it candidly. Meta threads tend to get... messy. This thread is an exception of course, but please remain on-topic.We'd like to reach out to other subreddits to see how they handle this, because we can't be the only ones dealing with it. We're particularly interested in any Reddit-specific tools that we could be using that we've overlooked. If you have information or contacts with other subreddits that have dealt with this problem, please feel free to send us a modmail.We need to expand the moderator team, both to bring in fresh ideas and to help spread the workload that might be introduced by additional filtering. Note that we don't take applications for moderators; instead, we'll be looking for individuals who are active on the subreddit and invested in our community values, and we'll reach out to them directly.Sometime soon, we'll be testing out some AutoMod rules to try to filter some of these posts. Similar to our existing  tag requirement for image/video posts, we may start requiring a  tag (or flair or similar marking) for project announcements. The hope is that, since no one reads the rules before posting anyway, AutoMod can catch these posts and inform the posters of our policies so that they can decide for themselves whether they should post to the subreddit.We need to figure out how to re-word our rules to explain what kinds of AI-generated content are allowed without inviting a whole new deluge of slop.We appreciate your patience and understanding while we navigate these uncharted waters together. Thank you for helping us keep /r/rust an open and welcoming place for all who want to discuss the Rust programming language.]]></content:encoded></item><item><title>So you want to contribute to Rust, but feel overwhelmed?</title><link>https://www.reddit.com/r/rust/comments/1qpgx7k/so_you_want_to_contribute_to_rust_but_feel/</link><author>/u/Kivooeo1</author><category>rust</category><category>reddit</category><pubDate>Wed, 28 Jan 2026 16:57:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've been seeing this question come up again and again - in comments, DMs, Zulip, everywhere:I want to contribute to Rust, but I open the repo, see millions of files, issues and things to do… and just freeze, how do I start?I just finished today a long-form post about getting past that exact pointThis post isn't a tutorial or a checklist and it not intended to replace the how I personally got started contributing to the compilerwhat actually helped when I felt stuckhow reviews, CI, mentors, and mistakes really look from the insidewhat labels and issues are actually beginner-friendlyThe post is intentionally long and not meant to be read linearly - it's something you can skim, jump around, or come back to laterhave thought about contributingbut feel intimidated by the scaleThis is just the first part - in the next post, I'm planning to walk through a real issue from start to merge. Stay tuned if you're curious about how it looks in practice (I haven't figured out RSS yet, but I'll definitely do it soon!)]]></content:encoded></item><item><title>mistral.rs 0.7.0: Now on crates.io! Fast and Flexible LLM inference engine in pure Rust</title><link>https://www.reddit.com/r/rust/comments/1qpewlv/mistralrs_070_now_on_cratesio_fast_and_flexible/</link><author>/u/EricBuehler</author><category>rust</category><category>reddit</category><pubDate>Wed, 28 Jan 2026 15:46:44 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[A fast, portable LLM inference engine written in Rust. Supports CUDA, Metal, and CPU backends. Runs text, vision, diffusion, speech, and embedding models with features like PagedAttention, quantization (ISQ, UQFF, GGUF, GPTQ, AWQ, FP8), LoRA/X-LoRA adapters, and more. Clean, simplified SDK API to make it embeddable in your own projects full-featured CLI with built-in chat UI, OpenAI server, MCP server, and a tune command that auto-finds optimal quantization for your hardware. Install: https://crates.io/crates/mistralrs-cli TOML configuration files for reproducible setups.Prefix caching for PagedAttention (huge for multi-turn/RAG)Custom fused CUDA kernels (GEMV, GLU, blockwise FP8 GEMM)Metal optimizations and stability improvements GLM-4, GLM-4.7 Flash, Granite Hybrid MoE, GPT-OSS, SmolLM3, Ministral 3 Gemma 3n, Qwen 3 VL, Qwen 3 VL MoE Qwen 3 Embedding, Embedding Gemm]]></content:encoded></item><item><title>Everyone overcomplicates learning Rust.</title><link>https://www.reddit.com/r/rust/comments/1qpdp5b/everyone_overcomplicates_learning_rust/</link><author>/u/arfsantonio</author><category>rust</category><category>reddit</category><pubDate>Wed, 28 Jan 2026 15:01:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Everyone overcomplicates learning Rust.Then write code. Break things.The async stuff? You'll know when you need it. Don't start there.Effective Rust and the Atomics book are for later — when you've actually shipped something and want to understand why it worked.Most people collect resources. Few people finish The Book. Start there.]]></content:encoded></item><item><title>Rust at Scale: An Added Layer of Security for WhatsApp</title><link>https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/</link><author>/u/pjmlp</author><category>rust</category><category>reddit</category><pubDate>Wed, 28 Jan 2026 08:38:41 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[2015 Android Vulnerability: A Wake-up Call for Media File ProtectionsHow Rust Fits In To WhatsApp’s Approach to App Security]]></content:encoded></item><item><title>[Media] crabtime, a novel way to write Rust macros</title><link>https://www.reddit.com/r/rust/comments/1qon5p9/media_crabtime_a_novel_way_to_write_rust_macros/</link><author>/u/wdanilo</author><category>rust</category><category>reddit</category><pubDate>Tue, 27 Jan 2026 19:04:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Crabtime offers a novel way to write Rust macros, inspired by Zig’s comptime. It provides even more flexibility and power than procedural macros, while remaining easier and more natural to read and write than . I highly encourage you to check out the blog post and the docs for examples and an in-depth explanation :)Development of this library is sponsored by , a Rust-focused software house. I’m one of its founders, happy to answer questions or dive deeper into the design!]]></content:encoded></item><item><title>A hyper-ish 2025 in review</title><link>https://seanmonstar.com/blog/2025-in-review/</link><author>/u/seanmonstar</author><category>rust</category><category>reddit</category><pubDate>Tue, 27 Jan 2026 16:46:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Come along with me as I review the past year. Heh, I often start these kinds of posts right at the start of the year, but it takes a few weeks longer than I ever expect to think them through.Two years of being independentIn terms of personal execution, it felt pretty fantastic, actually. Thanks to high-touch conversations from my retainers, I knew what was needed; there was an underlying trend. And I was able to spec out a grant that made a project out of that trend. All while managing to do the necessary maintenance work that the ecosystem requires. Granted, it did occasionally feel like a conflict of priorities, but that’s life.Honestly, though, I wasn’t so sure when trying to plan this all out initially.Perhaps the biggest deal for hyper this year was launching our first user survey. I’ve thought of doing it a few times over the years, but finally remembered in Q4 to launch it. Thanks to all who answered! I’ve looked through the results, and I think this will be extremely useful. Some stats real quick: 96% of respondents have upgraded to hyper v1.x, most commonly combine it with Tokio (99%) and rustls (92%). A proper analysis coming soon!katelyn martin joined us as a collaborator, and has continued to be a multiplier with kind reviews and maintenance glue. And general maintenance doesn’t stop, including growing security reports (more below).Besides all that, I took on a larger project for the year. You see, after updating the roadmap at the end of the previous year, I started to focus on one of the four defined areas: improved . This lined up with what many have been asking for.I did that by modularizing parts out of reqwest.Most of my year was spent on modularizing reqwest. Or, from another angle, giving back the building blocks that reqwest has accumulated over the years. A lot of functionality that people rely on in reqwest started life as internal glue, and this was the year I finally pulled many of those pieces out into places where the rest of the ecosystem could use them too. Between reqwest and hyper‑util, that work ended up producing quite a few releases: 14 for reqwest itself, and 8 for hyper‑util.Ages ago, I added a bunch of features that you expect any client to have directly into reqwest. Later, as  grew, it copied some of those same features. Meanwhile, reqwest was used in weirder and weirder places, so we hardened those features, and tossed in some tests to check for the weird. But  never saw any of that.This year, we completely tossed the redirect and decompression code from reqwest, depended on the  pieces, and then allowed the test suite to find the difference. The  layers got those fixes backported, and now everyone benefits.We also created  things, but still modular.Easier retries were added to reqwest, making use of the lower-level pieces in tower. I’m still interested in ways to improve the feature, so more people can use retries more safely.reqwest has grown extensive support for connection proxies. But an increasingly common pattern was people using reqwest  for the proxy support; they didn’t need any other feature. So I extracted proxy matchers and proxy connectors (tunnel, socks) into hyper-util.The largest piece was designing and implementing composable pools for hyper-util. In many ways, this was my . It’s a problem I’ve been thinking about since … 2018? I’d done a lot of research throughout the years, and never found anything quite like it. Now, it’s not quite “done”, but it’s a base that allows a lot of new layers and compositions to be explored.To end the year, we released v0.13 with rustls as default. It’s a big improvement for  people. But. I am not currently happy with how difficult it is to build the defaults on some other targets (Windows, Cranelift, cross-compiling). I want that fixed. Maybe that’s improvements to upstream aws-lc-rs; it looks like it’s already been improved to not need cmake. Or maybe we use a different default crypto provider on some targets.The work on composable pools was hard. The reason it had taken me years to finally try was that I wasn’t sure about some of the design. After staring hard at it during the summer, I did solve some of the questions. But there was one problem towards the end that consumed another month or so of staring. And this time, I couldn’t stop staring.With a hard deadline set, however, there was no possibility of waiting longer. Instead, I had to settle with shipping what I had, and accepting that it can always be better.And that’s also the beauty of deadlines: they keep you user-driven. As long as I’m staring hard at a problem, holding back shipping, users have . But software doesn’t need to be shipped all at once. It’s a lesson I’ve learned before, and yet it pops up to, uh,  me over and over.I feel like I go through waves: I hate setting a deadline, and many times feel disappointed at not shipping all the glory that was in my head. But I always appreciate that at least they got .We take security seriously, and the amount of reports we receive is slowly increasing. This past year, we had a 8 in total, including our first AI slop report (yay!).That didn’t stop it from being stressful trying to handle reports while simultaneously sticking to feature deadlines.It is a reminder, though, that this is often urgent and important work that must be handled, but that traditional pay-for-features doesn’t support. Sponsorships and retainers make this sort of maintenance much more sustainable.On the 10th anniversary of Rust 1.0, I gave a talk for the Rust for Lunch meetup. It was sort of ‘lessons using Rust for 10 years’, but also ‘why you should consider Rust’.And I did a podcast episode on Netstack.FM, discussing the history of Rust’s networking ecosystem.Last year, I liked just sharing some questions I’m thinking about. It wasn’t a promise to work on them actively, but I look at them from time to time to see if there’s something that I can tackle soon.Here’s just a few things I’m thinking about at the start of 2026:How do I balance keeping up with LLM advances while keeping my mind and skills sharp?How far can one reasonably go with typestate builders, considering ergonomics and correctness?]]></content:encoded></item><item><title>Nio Embracing Thread-Per-Core Architecture</title><link>https://nurmohammed840.github.io/posts/embracing-thread-per-core-architecture/</link><author>/u/another_new_redditor</author><category>rust</category><category>reddit</category><pubDate>Tue, 27 Jan 2026 14:04:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>zlib-rs: a stable API and 30M downloads</title><link>https://trifectatech.org/blog/zlib-rs-stable-api/</link><author>/u/folkertdev</author><category>rust</category><category>reddit</category><pubDate>Tue, 27 Jan 2026 11:21:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Since the first release in April 2024, zlib-rs has come a long way. It has seen major adoption over the last year, and, we're proud to say, is now feature complete.
We've released zlib-rs 0.6, the first version with a stable and complete API.With this milestone, we now fully deliver on the promise of our Data compression initiative: real alternatives to C/C++ counterparts that reduce attack surface through memory safety and provide on-par performance.Features and promises are nice, but seeing adoption grow is the cherry on the cake: zlib-rs recently crossed 30M downloads, 25M+ in the last year, and is on track to become the default implementation in , which is expected to further boost usage.This blog post is a quick round-up of the latest release, 0.6. The full release notes are here.The  crate now has a stable API. It hides away most of the internals, but exposes enough for  and . Generally we recommend to use  via  in applications, but for low-level libraries using  directly is now an option.Additionally  now uses the  CRC32 checksum implementation when  is used. Our implementation is faster, and it saves a dependency.The  crate is a C-compatible API built on top of . It can be compiled into a drop-in compatible C library.All exported functions now use  instead of .This is a change we've wanted to make for a while, but held off on because we had rust crates using . Now that they instead use  directly, we can focus more on C users in the  crate.Normally, when rust functions panic, they start unwinding the stack. That is only valid when the caller anticipates that the callee might unwind. For rust functions this case is handled, but when exporting a function, the caller is likely not written in rust, and does not support stack unwinding.If the callee does unwind into an unsuspecting caller, behavior is undefined. Although  should not panic, causing UB when we somehow do is a massive footgun. So now we use , which will instead abort the program at the FFI boundary.We've added functions like ,  and many others to the  API. These were already available in , and have now been promoted. They are still behind the  feature, so enable that if you need these functions. Most of the  functions were implemented by @brian-pane.In addition, we've implemented several other missing functions (like ), so that we're now fully compatible with the zlib and zlib-ng public API.For completing this final milestone we thank all the contributors, specifically @brian-pane, and the Sovereign Tech Fund for investing in the API stabilization.Although the public API is now complete, a project like this is never truly done. There are always new optimization ideas to try, versions to update, and obscure edge cases to support.The biggest remaining items is that technically the API is only complete when using nightly rust. The  and  functions are c-variadic, and c-variadic function definitions are currently unstable. I hope to stabilize  in the next ~6 months.]]></content:encoded></item><item><title>Atomic variables are not only about atomicity</title><link>https://sander.saares.eu/2026/01/25/atomic-variables-are-not-only-about-atomicity/</link><author>/u/maguichugai</author><category>rust</category><category>reddit</category><pubDate>Tue, 27 Jan 2026 10:09:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The code compiles. All the tests pass. The staging environment is healthy. Yet once per day a few servers in the production fleet mysteriously observe a crash with an error message that makes no sense – unreachable code has been reached, we have taken 9 items out of a collection that can only hold 8, or similar. Welcome to the world of rolling your own synchronization primitives.Even after twenty years in the code mines, encountering custom thread synchronization logic brings fear and doubt into the head of the author. It is so easy to make a mistake and so hard to notice it.Importantly, in today’s AI-enriched engineering loop, we may find ourselves incorporating custom synchronization logic without always realizing it! Anecdotal evidence suggests that LLMs are quite content to use atomic variables for custom multithreaded signaling and synchronization logic even when safer alternatives like mutexes or messaging channels are available.While a thorough treatment of logic synchronization would be an entire book, this article aims to paint a picture of some of the basics of custom synchronization primitives, providing readers with an approachable treatment of some essential knowledge that may help them at least review and validate such logic when generated by AI coding assistants.There are two key construction materials used to create synchronization primitives:Atomic variables – these are variables imbued with special properties by both the compiler and the hardware architecture. Operating on atomic variables is innately tied to memory ordering constraints which are the true mechanism by which logic on different threads is synchronized.Specialized operating system API calls – specialized operations like “suspend this thread until XYZ happens”, typically used when one thread needs to wait for an event that occurs on another thread.This article will only look at the former, exploring some usages of atomic variables and the fundamental logic synchronization capabilities they offer in situations where we deliberately avoid using higher-level synchronization primitives. We explore some common pitfalls and see how we can avoid them by applying relevant verification tooling and by following some key design principles.The name “atomic variables” is incredibly misleading, as it over-emphasizes the atomicity properties of these variables. While these properties do exist and are relevant to this topic, atomic variables might better be thought of as “thread-aware variables” because they also have other properties that are just as important as atomicity. This unfortunate naming bias has likely contributed to the topic being as difficult to comprehend as history has proven it to be – many of the reasons we use atomic variables are not (only) because they are atomic!In particular, we will try to distinguish the atomicity and memory ordering properties of atomic variables very clearly in this article. We start with the classic multithreading example of trying to increment a counter on two threads.Two threads are working on shared data and due to a programming mistake the program ends up with the wrong result. Let’s examine what happens, why it happens and how to fix it.Our program consists of two threads that will be incrementing the same counter.Each thread increments the counter 1 million times, so we expect our program to end up with a value of 2 million in the counter.Note the usage of  inside the loops. This is important to avoid the compiler optimizing away the entire loop into a “”. The black box creates an optimization barrier between the increment and the loop. The code we write is not always going to match the code that the hardware executes. Understanding the allowed compiler transformations can be crucial for creating valid multithreaded code using low-level primitives.Note also the usage of  blocks. The Rust programming language tries to protect us from shooting ourselves in the foot here and refuses to allow this incorrect multithreaded logic to be written in its default safe mode. Unsafe does not mean invalid – switching Rust to unsafe mode simply means the programmer takes over some of the responsibilities of the compiler. In this case, however, it does mean invalid – we will intentionally fail to fulfill our responsibilities for example purposes.Running this code will give different answers from run to run but almost always, the answer will be an assertion failure because the counter did not reach the expected value of 2 million.If you have access to systems on different hardware platforms, try this code on both Intel/AMD and ARM processors (e.g. a Mac). You may find the results differ in interesting ways. If you spot such a difference, leave a comment with your best guess as to why it exists.The simple story with this example is that each loop iteration consists of:Loading the current value from the variable.Storing the new value back in the variable.This immediately suggests some ways for the two threads to interfere with each other.The two threads might both load the same value, increment it by one, and simultaneously (or near enough) store the new value. Logically speaking, two increments happened, but the value only changed by 1.Alternatively, a thread might load the value N, then be suspended by the operating system, then after the other thread has done 5000 increments to reach N+5000, then the original thread resumes executing and writes N+1, erasing 5000 increments made by the other thread.This is a data race, which is considered undefined behavior in Rust. This program is invalid.The fix is simple: we must tell the compiler that this counter is being accessed from multiple threads simultaneously. This is what  can fix – their atomicity property guarantees that any operation performed on an atomic variable will have the same effect no matter how many threads are accessing it simultaneously. We need to use atomic variables whenever some data is accessed by multiple threads concurrently and at least one of those threads is writing to it.To apply this fix, we change our counter from  to . Instead of  to increment, we change to use . This method also requires an argument to specify the memory ordering constraints to apply for this operation. In context of this example, we just specify  ordering, which means “no constraints” and gives the compiler and the hardware maximum flexibility in how they are allowed to compile and execute this operation.We no longer need to use the  keyword here, which is good because it is not possible to have data races in safe Rust code. Note that it is still possible to have logic errors (including synchronization logic errors) in safe Rust code – “safe” does not mean “correct”. Still, by getting rid of the  keyword, an entire class of potential errors has been eliminated, so it is a very desirable change.Running the program, we now see what we expect to see – the counter is always incremented by two million:The point of this first example was to highlight the atomicity property of atomic variables, which enables multiple threads to operate on the same variable as if the operations on that variable were happening on a single thread.The key part of that phrase is “on that variable”! Atomicity is not enough if we have more than one variable we need to work with! That’s where memory ordering constraints come into the picture, right after a brief detour.Sidequest: detecting data racesIf we never use the  keyword, we can be certain that there are no data races in our Rust code, as that is one of the promises of safe Rust. However, what if we do use unsafe code? The  keyword is a legitimate Rust feature and merely lowers the guardrails, enabling us to write code that we believe is perfectly valid but which simply cannot be fully verified by the compiler.That “we believe” is a problem! If we make a mistake in low-level synchronization logic, we may not find out for years. We might only observe that 10 times per day, a random server out of our fleet of 10 000 experiences a bizarre crash whose crash dump makes no sense, taking down thousands of real user sessions without anyone being able to reproduce it in the lab. Such failures are not merely theoretical – those 10 000 servers were very real and the author has gone through exactly such a months-long detective adventure.The good news is that Rust offers a helping hand here. The Rust toolchain includes the Miri analysis tool, which is capable of detecting data races and other kinds of invalid code. Use it as a wrapper around your “run” or “test” Cargo commands. For example, to run the first example under Miri:Miri will immediately complain about any data race it sees. Its detections rely on our examples/tests actually executing a problematic sequence of operations, which is not always easy to organize but for anything with test/example coverage, it is an invaluable validation tool.Testing with Miri should be considered mandatory for any code that contains the keyword .While Miri can run both stand-alone binaries and tests, using it does require some special attention. This is because Miri is best thought of as something similar to emulator, perhaps as even as a separate operating system and hardware platform. The challenge is that this emulated platform does not have a real Windows, Linux or Mac operating system running on it – if the app tries to talk to the operating system, it will simply panic. While some operating system APIs are emulated, the majority are not. Trying to perform network communications or spawn additional processes is not going to work under Miri, for example.This generally means that only a subset of tests can be executed under Miri, with the others having to be excluded via #[cfg_attr(miri, ignore)].There is no easy solution to that limitation – to benefit from Miri, we must design our APIs so that the logic containing  blocks is completely separate from logic that talks to operating system APIs Miri does not emulate.Having completed our sidequest and learned how to use Miri for detecting data races, let’s return to exploring the second important property of atomic variables – the ability to define memory ordering constraints.And then there were two variablesRecall that the atomicity property of atomic variables is sufficient for correctness only if we need to work with a single variable. The next example scenario introduces two separate variables: an array and a pointer to this array. To synchronize this example correctly we will need to introduce memory ordering constraints.The logic of this example is relatively straightforward. We have two threads: Producer and Consumer. Producer delivers some data to Consumer. The workload consists of the following conceptual operations:Producer creates an array of 1000 bytes, writing the value  into each byte. In other words, it creates the equivalent of a .Once the array has been created and filled, Producer publishes a pointer to this array via a shared variable (which contains a null pointer until the array gets published).Consumer waits for this pointer to become non-null, indicating Producer has published the array.Consumer sums the values of all the bytes in the array, expecting to see 1000 as the sum.For example purposes, we use an array just to operate on a large number of bytes. This makes the desired effect appear with a higher probability. However, the exact data type does not matter and the logic would be the same even if the array were a simple integer.Let’s write the code for this:The code already avoids one mistake by storing the published pointer in an atomic variable of type . The reason is the same as it was in the first problem – data being accessed simultaneously from multiple threads requires the use of atomic variables for correctness, guaranteed by the atomicity property of atomic variables. It does not matter that Producer only writes and Consumer only reads – read access matters just as much as write access. We can skip atomic variables for concurrently accessed data only if all access is via shared references. Of course, the Rust language would also do its best to stop us if we tried to skip using atomic variables (e.g. by not allowing us to create both a  exclusive reference and a  shared reference to the same variable).It is important to explore the difference here between the pointer to the array and contents of the array – why do we not need atomic variables for the data inside the array (i.e. why is it not an )? This is because while the array contents are indeed also being accessed by two threads, they are not being accessed by them at the same time – Producer only accesses the contents before it publishes the array and Consumer only accesses the contents after it has received the published array. In the conceptual sequence of operations there is no overlap in time when both Producer and Consumer are accessing the array, so we are not required to use atomic variables for this data.In our example, we will repeat this validation logic a large number of iterations to ensure that we detect even anomalies that happen with a very small probability. Recall the “one in 10 000 servers crashes per day” scenario described earlier – that is a very low occurrence of an issue considering the systems would each be handling tens of thousands of concurrent sessions. In our example, we want to be extra certain we detect even a 0.0001% probability fault – multithreaded logic requires extreme thoroughness from us because it can introduce subtle errors with large consequences. This is not typically done for real-world test code, though can still be a valuable technique under some conditions (especially when combined with Miri).Let’s review the logic: on one thread, we create an array, then publish it; on the other thread, we wait for the array to be published, then consume it. Seems sound, right? What could possibly go wrong? Let’s run it on a typical x64 system.It works! Phew, it would have been scary to see that very straightforward logic fail.Let’s also try this program on an ARM system, just because we have one available.There is valid and there is validA program is valid if it satisfies the rules of the programming language. These rules are different from the rules imposed by the hardware. One reason for this is that programming languages tend to support multiple hardware platforms with different sets of rules.This means that an invalid program may still do “the right thing” if:The compiler does not transform the logic in surprising ways (which it is often allowed to do).The program satisfies the rules of the hardware it runs on.This explains why we saw a successful result on the x64 system – both factors were in our favor there.The array sharing program we wrote is invalid Rust code because it contains a programming error in the form of a data race. A data race is undefined behavior and the compiler is allowed to do whatever it wants in case of undefined behavior – from pretending everything is fine, to removing the offending code, to inserting a crypto miner, to taking out a bank loan in our name. We got lucky in our case because the compiler decided not to apply any unwanted transformations that would break our code completely.The x64 platform is quite forgiving with its multithreading rules, so the program still worked correctly on the x64 system and from the point of view of the hardware, everything was fine. From the programmer’s point of view, the hardware did exactly what we expected from it despite the code being invalid from a programming language point of view.The ARM platform is much less forgiving and invalid code has a lower probability of working correctly on ARM. Around 0.002% of the time our program will fail on the ARM system the author used for testing, though this will greatly depend on the specific hardware and the exact code being executed.This lower tolerance for mistakes makes it valuable to test low-level multithreading logic on ARM processors.In any case, the data race is immediately and consistently detected by Miri because Miri validates behavior against the rules of the programming language, not the hardware platform.We must explicitly disable the Miri memory leak detector here via the  environment variable because our example intentionally leaks memory to ensure that every iteration runs with a unique memory address, which is a realistic memory access pattern that makes it easier to reproduce the issue.It is a fact that the example fails on ARM hardware but it is less obvious why. The error messages from Miri are often only the first step in an investigation and rarely reveal the whole picture. Let’s explore the factors involved.It is common to think of code execution as a linear process. Take the array publishing on the Producer thread, for example:A very typical way to reason about this code would be:After creating the array, it is filled with  bytes.After filling the array, the pointer to it is published.This is true but only in a certain sense. It is true only locally – within one thread! And it is true only in the abstract.This is why in the earlier description of the example some specific phrasing was used:In the conceptual sequence of operations there is no overlap in time when both Producer and Consumer are accessing the array [..].This is certainly what we would want to be the case. However, the code we wrote actually defines a different sequence of operations! The reality is that programming languages present us with a very simplified view over what happens in the hardware. As we break through the layers of abstraction, we find many factors that can shatter this illusion.First, we must consider how the compiler sees our code. It is allowed to reorder operations in code if it thinks a different order is more optimal. It is allowed to do this as long as the end result remains the same (i.e. as long as no dependencies between operations are violated). What are the dependencies in our array publishing code? Let’s examine it from bottom to top:The pointer to the array is published to a shared variable.Before the pointer can be published, the array we are pointing to must be created.The array is filled with  bytes. Obviously, the array must be created before it can be filled.But what about a relationship between filling the array with  and publishing the pointer to the array? Our code does not define any relationship between these two. Filling the array and publishing the pointer are independent operations as far as the compiler is concerned. It is entirely legal for the compiler to decide to fill the array with  after publishing the pointer! The mere fact of us writing the “fill with ” code before the “publish pointer” code does not establish a dependency between these operations.Some exploration of the compiled machine code of this example indicates that we got lucky and the compiler did not make any reordering transformations. This is true at time of writing and such behavior may change with compiler versions. This luck is part of the reason the code works on the x64 processor architecture. If a future version of the compiler decides to reorder the operations here, the code might also break on x64 systems. This is our first hint that in valid multithreaded code, we must sometimes explicitly define dependencies between operations if we want X to occur before Y. We will cover how to do this in the next chapter.This was just about what the compiler does. Even if the compiler does not reorder anything, we need to consider what the hardware does when it executes the code. The hardware is not at all linear in its behavior. A modern processor performs many operations simultaneously, even speculating about future choices that are not yet known.Again, there is the underlying principle that the hardware is allowed to do this as long as the end result remains the same under the ruleset of the hardware architecture. Does the hardware consider there to be any dependency between the filling of the array with  and the publishing of the pointer?There are different kinds of relationships and dependencies that need to be considered when dealing with hardware but if we greatly simply things we could say:For x64, yes, a dependency exists between the “fill with ” and “publish pointer”For ARM, no, the operations “fill with ” and “publish pointer” are independent.The impact of this is that on ARM, the published pointer can sometimes be observed by the Consumer thread before the array has been filled with  values. Even if the code on the Producer thread filled the array before publishing the pointer!Ultimately, the “why does it happen” does not really matter – the hardware architecture ruleset allows it to happen and there may be multiple different mechanisms in the hardware itself that can yield such a result (e.g. perhaps the pointer publishing and  fill are literally executed at the same time by different parts of the processor, or perhaps the  fill does happen first but the updated memory contents are simply not published to other processors immediately).The good news is that as long as we follow the Rust language ruleset, we are guaranteed to be compatible with all the hardware architectures that Rust targets – we only need to concern ourselves with what Rust expects. All this talk of hardware architectures is just here to help understand why the Rust language rules exist.To fix both aspects of the data race (to prevent compiler reordering and to ensure that the Consumer thread sees the right order of operations) we need to signal to both the compiler and the hardware that a dependency exists between publishing the pointer and filling the array.Establishing the missing data dependencyWe have determined that a data race exists between the writing of the  values on the Producer thread and the reading of the array contents on the Consumer thread. Let’s fix it by adding the missing data dependency, which makes the code valid Rust code and automatically implies that the hardware will do what Rusts expects it to do (and what we expect it to do) regardless of the hardware architecture.Defining the data dependency requires two changes.First, we must tell the compiler that publishing the pointer to the array depends on first executing all the code that came before it (the writing of the  values). We do this by signaling the  memory ordering:The names of the memory ordering modes are rather confusing. Do not read too much into the names – they are still confusing and low-signal to the author even after years of working with them. ordering means “this operation depends on all the operations that came before it on the same thread”.For the compiler itself, defining a  memory ordering may often be sufficient because it establishes the dependencies between operations and prevents problematic reordering by the compiler.However, this is not enough to establish the data dependency for the hardware that executes our code!When considering what the hardware does it is more useful to think of  ordering as merely metadata attached to the actual data written. It does not necessarily change what the hardware does when executing the write operation but merely sets up the first stage of a transaction.To actually “close the loop” here and complete the transaction, we need to also instruct the hardware to pay attention to these metadata declarations when reading the data. We do this by using the  memory ordering.  ordering means “if the value was written with  ordering, make sure we also see everything the originating thread wrote before writing this value”.How exactly the hardware does all of that is hardware-implementation-defined but you can think of  as a “wait for all the data we depend on to arrive” instruction. Yes, literally – an  memory ordering can make the processor just stop executing any code until the data has arrived!This reinforces the fact that atomic variables are slow. This synchronization takes time and effort from the hardware and is not free. While still cheaper than heavyweight primitives like mutexes, atomic variables are still costly compared to regular memory accesses and code aiming to be highly scalable on systems with many processors should minimize any synchronization logic, even logic based only on atomic variables.Dependencies between operations can be difficult to reason about, so to help understand what just happened, we can take the original diagram that introduced this example and annotate it with the dependency relationships that ensure steps 1, 2, 3 and 4 actually occur in that order from the point of view of all relevant participants.Starting from the back, the dependency between steps 3 and 4 is guaranteed by the Rust language – we simply cannot read the array until we have a pointer to the array.The dependency between steps 2 and 3 is guaranteed by using , which makes the pointer (in isolation) valid to operate on from multiple threads (and obviously, we cannot read a non-null value from it before there is a non-null value in it).The dependency between steps 1 and 2 is the one that this whole chapter has been about. Without memory ordering constraints, this dependency would not exist and step 1 might come after step 2.The combination of  and  on the write and read operation is what solves the data race by creating the data dependency:The write with  ordering establishes the dependency on the Producer thread.The read with  ordering “spreads” that dependency into the Consumer thread.Now both the compiler and the hardware know about the relationship between the data and they can each take proper care. Let’s run it again on ARM:A clean pass! Running Miri on the fixed version also gives us a clean bill of health.To reinforce the concepts described above, let’s look at how one might implement a reference-counting smart pointer like , whereby a value is owned by any number of clones of the smart pointer, with the last one cleaning up the value when it is dropped.The usage should look something like the following:A simple implementation of  consists of:The owned value of type , shared between all clones of the .A shared reference count, indicating how many clones exist. When this becomes zero, the value is dropped.We will put this shared state into a struct and make cloneable smart pointers, each pointing to this data structure.Before we go further with the implementation, let’s analyze the design based on what we have covered earlier in this article.Do we need to use atomic variables? Recall that atomic variables are needed if multiple threads concurrently access the same variable and at least one of the threads performs writes.We can consider the owned  as read-only for concurrent use because our  never modifies it and only returns shared references that do not allow mutation of the value. When mutation does occur (dropping the ) we are guaranteed that only one thread is operating on the variable because only the last clone of the  can drop the  – if a drop is happening, no other threads could remain to access it.This means there is no need to involve atomic variables in the storage of . This is good because there is no general purpose  type – atomic variables only exist for primitive types and our  could be anything. Indeed, one principle of synchronization logic is that concurrent writable access is only possible on primitive data types and alternative approaches like mutual exclusion must be used for complex types.Conversely, the reference count will be modified by every clone of the , so it must be an atomic variable (e.g. ).Do we need to care about memory ordering? Recall that memory ordering is relevant if there are multiple variables involved in multithreaded operations.This one is not so easy to assess correctly. At first glance, one might say that only the shared reference count is related to any multithreaded operations – after all, the owned value  is read-only ( does not return  exclusive references so a  shared reference is the most you can get) until it is dropped, which happens in a single-threaded context. However, this line of reasoning is flawed.The error in our thinking is that a Rust object is not necessarily read-only even if all you have is a shared  reference to it! You do not need a  exclusive reference to mutate an object – the type  may still be internally mutable! It may have fields containing , atomic variables or other data types that do not require an exclusive reference to mutate. While for the “do we need to use an atomic variable” assessment, this did not matter (it is handled by the type  internally), it does matter for data dependency considerations.This means there are, in fact, two potentially changing variables involved – the  and the reference count.Still, this is not an answer to the original question. We also need to determine whether these two variables are dependent or independent. Does a data dependency exist that we need to signal to the compiler and the hardware? We must consider the full lifecycle of each value here. The key factor is that the last  clone must drop the instance of  after decrementing the reference count to zero.The word “after” is the dependency we are seeking – dropping an object requires the drop logic to access the data inside that object and we need to ensure that the drop logic sees the “final” version of the , after all changes from other threads have become visible (i.e. after seeing all the writes made by all the other threads before they dropped an  clone).In other words, the drop of the  can only occur as the last operation in the lifecycle of . Sounds obvious when put that way but this does not happen automatically in multithreaded logic.To make it happen, we need to impose memory ordering constraints in  to signal the data dependency from the reference count to the :When a clone of the  is dropped, the reference count decrement is performed with  ordering to signal that any writes into  on this thread must be visible before the decrement becomes visible on other threads.When a clone of the  is dropped, the reference count decrement is performed with  ordering to ensure that (if it decrements to zero and we need to drop the ) we see all changes that happened on other threads before they decremented their own reference count.That’s right, the same operation needs both  and  memory ordering semantics. This is one of the standard memory ordering constraints, .Note that we only care about decrementing the reference count and not incrementing it. This is because we have no dependency on the value of  when incrementing the reference count as it is the dropping of the  after the last decrement that involves a data dependency. This means that incrementing the reference count can use  ordering because  clones on different threads do not care about any writes into  that occur if  is not being dropped.To be clear, the type  might certainly care about writes into the  being synchronized between threads but if so, it can define its own memory ordering constraints in its own mutation logic.That works. Miri does not complain. We have created a functional !Strengthening an operation after the factThe  we created in the previous chapter is suboptimal because it always performs the reference count decrement with  ordering. The problem is that the  part is only relevant for us if the reference count becomes zero – if we are not going to drop the , there is no need to ensure we have visibility over the writes from other threads.Recall that an  memory ordering constraint is a “stop and wait for the data to become available” command to the hardware – we are potentially paying a price on every decrement!There is a solution to this, though:First, we perform the decrement with only  ordering.Then we check if the reference count became zero – if not, we do nothing.If it did become zero, we define an .A fence is a synchronization primitive used to “strengthen” the previous operation on an atomic variable, allowing us to only pay for the  ordering constraint when we need it. It works exactly as if we had written the ordering constraint on the previous atomic variable operation (the reference count decrement) but allows the effect to be conditionally applied at a later point in time and code.This code is functionally equivalent but simply more efficient. The size of the effect depends on the hardware architecture and may be zero on some architectures.Leave safety comments and document memory ordering constraintsThe examples in this article made use of the  keyword to lower the guardrails of the compiler so that we could do something risky. To keep the examples short and to the point, we committed a sin: we failed to provide safety comments for these  blocks.Safety comments are critical to writing maintainable unsafe Rust code. They are one half of a challenge-response pair:The API documentation of an  definesthat callers must uphold – this is the challenge.The at the call site documents how the code upholds these safety requirements – this is the response to the challenge.Very often, errors in unsafe Rust code can be discovered when writing safety comments, as the act of writing down how exactly we uphold the requirements can lead to a realization that we are not actually meeting the requirements. Even after being written, safety comments are invaluable to reviewers and future maintainers, including AI agents that tend to be easily confused by unsafe Rust.Safety comments should be considered mandatory for all unsafe Rust code. Every unsafe function call must be accompanied by a safety comment that describes how we uphold the safety requirements. Unsafe code without safety comments is not reviewable and not maintainable. It is normal and expected that safety comments make up a significant bulk of the source code in unsafe Rust.Be extremely careful about AI-generated safety comments, though. They are often “SAFETY: All is well, this is valid, trust me bro” in nature and fail to adequately describe how the safety requirements of the function being called are upheld. Very often the AI does not even make an attempt to read the safety requirements of the functions being called, so the safety comments it makes can be completely off-topic hand-waving.Similarly to safety comments, it is good practice to accompany atomic operations with comments that explain why the memory ordering constraint specified is the correct memory ordering constraint to use.Memory ordering constraint logic can be very difficult to reverse-engineer and validate manually, so for the sanity of future maintainers and the success of future AI modifications, you should leave a comment on every operation on an atomic variable.In a production-grade  implementation we would expect to see detailed safety and synchronization logic comments similar to the following:]]></content:encoded></item><item><title>make.ts</title><link>https://matklad.github.io/2026/01/27/make-ts.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[Sounds familiar? This is how I historically have been running benchmarks and other experiments
requiring a repeated sequence of commands — type them manually once, then rely on shell history
(and maybe some terminal splits) for reproduction. These past few years I’ve arrived at a much better
workflow pattern — . I was forced to adapt it once I started working with multiprocess
applications, where manually entering commands is borderline infeasible. In retrospect, I should
have adapted the workflow years earlier.Use a (gitignored) file for interactive scripting. Instead of entering a command directly into the
terminal, write it to a file first, and then run the file. For me, I type stuff into  and
then run  in my terminal (Ok, I need  for that).I want to be clear here, I am not advocating writing “proper” scripts, just capturing your
interactive, ad-hoc command to a persistent file. Of course any command that you want to execute
 belongs to the build system. The surprising thing is that even more complex one-off
commands benefit from running through file, because it will take you several tries to get them
right!There are many benefits relative to  workflow:
Real commands tend to get large, and it is so much nicer to use a real 2D text editor rather than
shell’s line editor.

If you need more than one command, you can write several commands, and still run them all with a
single key (before , I was prone to constructing rather horrific && conjuncts for this
reason).

With a sequence of command outlined, you nudge yourself towards incrementally improving them,
making them idempotent, and otherwise investing into your own workflow for the next few minutes,
without falling into the YAGNI pit from the outset.

At some point you might realize after, say, running a series of ad-hoc benchmarks interactively,
that you’d rather write a proper script which executes a collection of benchmarks with varying
parameters. With the file approach, you already have the meat of the script implemented, and you
only need to wrap in a couple of fors and ifs.

Finally, if you happen to work with multi-process projects, you’ll find it easier to manage
concurrency declaratively, spawning a tree of processes from a single script, rather than
switching between terminal splits.
Use a consistent filename for the script. I use , and so there’s a  in the root
of most projects I work on. Correspondingly, I have  line in project’s 
— the  file which is not shared. The fixed name reduces fixed costs — whenever I
need complex interactivity I don’t need to come up with a name for a new file, I open my
pre-existing , wipe whatever was there and start hacking. Similarly, I have  in
my shell history, so
fish autosuggestions
work for me. At one point, I had a VS Code task to run , though I now use
terminal editor.Start the script with hash bang,

in my case, and

the file, to make it easy to run.Write the script in a language that:
you are comfortable with,

doesn’t require huge setup,

makes it easy to spawn subprocesses,

has good support for concurrency.
For me, that is TypeScript. Modern JavaScript is sufficiently ergonomic, and structural, gradual
typing is a sweet spot that gives you reasonable code completion, but still allows brute-forcing any
problem by throwing enough stringly dicts at it.JavaScript’s tagged template syntax is brilliant for scripting use-cases:What happens here is that  gets a list of literal string fragments inside the backticks, and
then, separately, a list of values to be interpolated in-between. It  concatenate everything
to just a single string, but it doesn’t have to. This is precisely what is required for process
spawning, where you want to pass an array of strings to the  syscall.Specifically, I use dax library with Deno, which is excellent as
a single-binary batteries-included scripting environment
(see <3 Deno). Bun has a dax-like
library in the box and is a good alternative (though I personally stick with Deno because of
 and ). You could also use famous zx, though be mindful that it
uses your shell as a middleman, something I
consider to be sloppy (explanation).While  makes it convenient to spawn a single program,  is excellent for herding a
slither of processes:Here’s how I applied this pattern earlier today. I wanted to measure how TigerBeetle cluster
recovers from the crash of the primary. The manual way to do that would be to create a bunch of ssh
sessions for several cloud machines, format datafiles, start replicas, and then create some load. I
 started to split my terminal up, but then figured out I can do it the smart way.The first step was cross-compiling the binary, uploading it to the cloud machines, and running the
cluster
(using my box from the other week):Running the above the second time, I realized that I need to kill the old cluster first, so two new
commands are “interactively” inserted:At this point, my investment in writing this file and not just entering the commands one-by-one
already paid off!The next step is to run the benchmark load in parallel with the cluster:I don’t need two terminals for two processes, and I get to copy-paste-edit the mostly same command.For the next step, I actually want to kill one of the replicas, and I also want to capture live
logs, to see in real-time how the cluster reacts. This is where  multiplexing syntax of box
falls short, but, given that this is JavaScript, I can just write a for loop:At this point, I do need two terminals. One runs  and shows the log from the benchmark
itself, the other runs  to watch the next replica to become primary.I have definitelly crossed the line where writing a script makes sense, but the neat thing is that
the gradual evolution up to this point. There isn’t a discontinuity where I need to spend 15
minutes trying to shape various ad-hoc commands from five terminals into a single coherent script, it
was in the file to begin with.And then the script is easy to evolve. Once you realize that it’s a good idea to also run the same
benchmark against a different, baseline version TigerBeetle, you replace  with
 and wrap everything intoA bit more hacking, and you end up with a repeatable benchmark schedule for a matrix of parameters:That’s the gist of it. Don’t let the shell history be your source, capture it into the file first!]]></content:encoded></item><item><title>The Impatient Programmer’s Guide to Bevy and Rust: Chapter 6 - Let There Be Particles</title><link>https://aibodh.com/posts/bevy-rust-game-development-chapter-6/</link><author>/u/febinjohnjames</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 19:52:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[By the end of this chapter, you’ll have built a particle system that brings magical powers to life. You’ll create four unique effects (Fire, Arcane, Shadow, and Poison), each with glowing particles that move, rotate, and fade. You’ll learn how to create particle emitters, write a custom shader for glowing effects, and use additive blending to make particles that feel magical.I'm constantly working to improve this tutorial and make your learning journey enjoyable. Your feedback matters - share your frustrations, questions, or suggestions on Reddit/Discord/LinkedIn. Loved it? Let me know what worked well for you! Together, we'll make game development with Rust and Bevy more accessible for everyone.Let’s Give Your Players Magic PowersBy the end of this chapter, you’ll learn:How to spawn and update thousands of particles efficientlyAdd variance for organic, natural looking effectsCustom shaders with additive blending for that magical glowBuilding a flexible system that’s easy to extendGive your player magical powersUnderstanding Particle SystemsA particle system spawns many small sprites that each: from an emitter with initial properties (position, velocity, color, size) for a short time, moving and changing when their lifetime expiresThe magic is in the numbers: spawn enough particles with slight variations, and they combine to create complex, beautiful effects.Building the Particle SystemEach particle needs to be independent—moving, rotating, fading, and shrinking on its own. To achieve this, we need two types of properties:  (how it moves) and  (how it looks). The physics properties like velocity, acceleration, and angular velocitygive particles realistic motion.Physics + Visual Properties in ActionWatch particles move (velocity), rotate (angular velocity), shrink (scale curve), and fade (color curve)Particles shouldn’t live forever. A fire particle needs to burn out, a magic spell needs to fade away. But particles also need smooth animations as they age, they should gradually fade, shrink, and change color over time, not just blink out of existence.To make this happen, particles need to track two things: when to die and how far along they are in their life. That’s why we use a countdown timer paired with progress tracking.A countdown timer () tells us when to delete the particle, but not its progress value. A particle with 0.5s left could be 25% done (started at 2.0s) or 99% done (started at 0.51s). - original durationThen: progress = 1.0 - (lifetime / max_lifetime)Now we know exactly where the particle is: 0% at birth, 50% at midpoint, 100% at death. This progress value drives all animations like color, size, opacity. Without both values, particles just blink on/off. With both, they transition smoothly.Watch two particles with different max_lifetimes die at different timesNow that we understand what properties particles need, let’s create these variables for our particle system. We’ll bundle them into a  component that tracks everything from physics to visuals.Create  folder inside  and add src/particles/components.rs:We animate color over the particle’s lifetime using a :Start (bright) → Mid (dimmer) → End (fade to black)This creates smooth transitions. A simple blend between two colors looks linear and boring. Three control points give us more expressive fading.Watch how a Shadow particle smoothly transitions through three color keyframesThe  struct holds all the data, but we need methods to: - A constructor that sets sensible defaults - Builder methods to override specific properties - Methods that compute current color and scale based on lifetime progressWithout these methods, every system that renders particles would need to duplicate this logic. By centralizing it here, we ensure consistency and make the code easier to maintain.Let’s implement the particle methods:Methods like  and  use the , a Rust idiom for constructing complex objects step-by-step. Each method:Takes  (ownership of the particle)This lets us chain calls together:Clean, readable, and flexible. You only specify what you need to customize, everything else uses defaults from .Methods like , , and  are where the magic happens. They  values based on the particle’s current state: - Converts remaining lifetime into a 0.0-1.0 percentage, so we know exactly where the particle is in its life cycle (just born at 0%, halfway through at 50%, about to die at 100%) - Blends smoothly between the three colors based on progress, creating that magical fade effect where fire particles glow bright orange then dim to black, or poison clouds shift from sickly green to dark purple - Gradually shrinks the particle from full size to tiny as it ages, making effects feel more dynamic and preventing particles from just blinking out of existenceThe  method:First, it calls  to get a value between 0.0 (particle just spawned) and 1.0 (particle about to die)Then it splits the lifetime into two halves:
    : Blends from  to Second half (50% to 100%): Blends from  to Bevy’s color interpolation method.  gives you 50% between the two colors.
The  function needs input from 0.0 to 1.0 to do a complete blend. During the first half of life, progress only reaches 0.5. That’s not enough, it would only blend halfway. Multiplying by 2 makes progress reach 1.0 by the halfway point, giving  the full range it needs.The  method:You know how good particle effects don’t just blink out,they shrink and fade away naturally? That’s what  creates. To achieve this, we gradually reduce the particle’s size from its starting size to a tiny ending size based on how much of its life has passed.For example, imagine a particle that starts at size 2.0 and should end at 0.5:At 0% progress: size is 2.0 (full size, just spawned)At 50% progress: size is 1.25 (halfway between)At 100% progress: size is 0.5 (tiny, about to disappear)The particle smoothly shrinks over time, creating that satisfying dissipation effect.Now we need something to actually  particles. That’s the job of the  component. Think of it as a particle factory attached to an entity (like your player character).The emitter needs to track:  - A timer that ticks down and triggers particle creation - Burst size (e.g., 5 particles at once) - The template for creating particles - Can be turned on/off - Fire once or keep firingHere’s the emitter component:Without , the emitter keeps spawning particles forever (or until you manually set ). This is perfect for continuous effects like a torch flame or a magic aura.With , the emitter spawns particles  and then automatically deactivates. This is ideal for one-time effects like a spell cast or an explosion—you don’t want those repeating every frame!The  struct is the DNA for creating particles. It defines all the properties each particle should have, but with a twist: .Without variance (left) vs with variance (right) - see the difference!Without variance, every particle would be identical (boring!). With variance, each particle gets slightly randomized values, creating organic, natural looking effects. For each property, we store: - The target value (e.g.,  seconds) - How much to randomize it (e.g.,  means ±0.2 seconds)So a particle might live for 0.8 seconds, another for 1.1 seconds, another for 0.95 seconds, all slightly different, making the effect feel alive.Now let’s create the struct that holds all particle properties. This   serves as a template that particle emitters use to spawn new particles. Instead of hardcoding values, we define them once in a config and reuse it.Here’s the configuration struct:Understanding the config attributes: /  - How long particles exist (seconds) /  - Initial velocity magnitude /  - Which way particles fly (direction_variance in radians creates spread) /  - Size of particles - Base particle tint (can be HDR values above 1.0) / angular_velocity_variance - How fast particles spin - Constant force applied (like gravity or wind) - Where particles spawn (Point, Circle, or Cone)So far we’ve defined the , what particles and emitters . Now we need the , the code that actually  things every frame.We need two key behaviors: - Check each emitter’s timer, and when it fires, create new particle entities - Move them, rotate them, fade their colors, shrink their size, and delete them when they dieWithout these systems, our components would just sit there doing nothing. Let’s start with the spawning system. Create :To spawn particles, we need two functions working together: - Runs every frame, checks each emitter’s timer, and when the timer fires, triggers particle creation - A helper function that creates a single particle entity with randomized propertiesThis system runs every frame and manages all particle emitters in the game. Here’s the flow: - Create a random number generator (we’ll need it for variance)Loop through all emitters - The  gives us every entity with a  component - If  is false, don’t spawn anything - If it’s a one-shot emitter that already spawned, deactivate it - Advance the spawn timer by the frame’s delta time - When the timer completes, it’s time to spawn! - Create  particles using the  helper - If it’s a one-shot emitter, turn it off after spawningEmitters don’t spawn particles every frame, they use a timer to control the spawn rate. A timer of 0.1 seconds means 10 bursts per second.What’s ?Creates a random number generator for this thread. We use it to add variance to particle properties, each particle gets slightly different lifetime, speed, direction, etc.Now the particle spawning function: handles the emitter’s timer and decides when to spawn particles. But it delegates the actual particle creation to a helper function called . This function takes the emitter’s configuration and creates a single particle entity with randomized properties (lifetime, speed, direction, etc.), a visual mesh, and all the necessary Bevy components.This function creates a single particle entity with randomized properties.Step 1: Add randomness to basic propertiesWe don’t want every particle to be identical, it won’t look naturalTake the base values (how long it lives, how fast it moves, how big it is, how fast it spins)Add a random amount within the variance rangeNow each particle is unique!Step 2: Randomize the directionParticles shouldn’t all fly in exactly the same directionStart with the base direction (e.g., “fly to the right”)If there’s direction variance, randomly rotate it a bit, otherwise keep it straightStep 3: Pick a spawn position within the emitter’s shape: all particles spawn at the exact same spot (like a laser beam origin): particles spawn randomly within a circular area (like a campfire): particles spawn in a cone shape (like a flamethrower)Step 4: Figure out where the particle startsCombine direction and speed to get velocity (how it moves each frame)Start at the emitter’s position in the worldAdd the shape offset from Step 3Put it at Z = 25.0 so it appears above the playerStep 5: Set up the color fade animationParticles should fade out as they die, not just disappearStart: full brightness (the color you configured)Middle: 70% brightness (getting dimmer)End: 30% brightness and transparent (fading to nothing)Step 6: Create the particle with all its settingsUse the builder pattern to chain all the properties togetherSet velocity, lifetime, scale, rotation speed, color curve, scale curveEverything is configured and ready to goStep 7: Make a visual square for the particleCreate a 24-pixel square mesh (scaled by the particle’s size)Create a material that uses the particle’s starting colorThis is what you’ll actually see on screenStep 8: Tell Bevy to create the particle entityBundle everything together: the particle data, the mesh, the material, the positionBevy spawns a new entity with all these componentsThe particle is now alive in the game world!Why ?TAU is 2π (approximately 6.28), a full circle in radians. For the  emission shape, we pick a random angle from 0 to TAU to get a point anywhere around the circle.What’s ?Converts a vector to length 1.0, making it a pure direction. If the vector is zero (no direction), it returns (0,0,0) instead of nan (Not a Number). This is safer than  which can panic on zero vectors.Now add the helper functions:These are small utility functions that help with the math in . They handle the geometry of spreading particles in different directions, essential for creating cone and spray effects instead of straight lines.What’s this rotation math?This is a 2D rotation matrix. To rotate a vector by an angle:New X = old X × cos(angle) - old Y × sin(angle)New Y = old X × sin(angle) + old Y × cos(angle)Now the particle update system:We’ve created the spawning system, but now we need to bring particles to life. The  system runs every frame and handles everything that happens during a particle’s lifetime: moving it, spinning it, fading its color, shrinking its size, and removing it when it dies.: Subtract frame time from particle’s remaining life: When lifetime hits zero, remove the entity: Forces modify velocity over time: Move by velocity each frame: Spin the particle based on angular velocity: Use the curve from : Shrink/grow using : Push the new color to the shaderFinally, add emitter cleanup:This removes one-shot emitters after they’ve spawned their particles. Continuous emitters stick around until manually despawned.We now have particles spawning, moving, and dying. But they still look like flat colored squares. To make them glow like magical energy, we need a custom shader that runs on the GPU.A shader is a small program that runs on your GPU (graphics card) for every pixel on screen. While our Rust code runs on the CPU and manages game logic, shaders run massively in parallel on the GPU to create visual effects.We’re creating a radial glow effect where each particle is bright and intense at the center, smoothly fading to transparent at the edges. This makes particles look like glowing orbs of energy instead of flat squares.Shader Glow Effect ComparisonWithout shader (left) vs With radial gradient shader (right)This shader is written in  (WebGPU Shading Language), Bevy’s shader language. It’s similar to Rust in some ways but designed specifically for GPU programming.Create the folder  in  and add the shader file , the final path should be src/assets/shaders/particle_glow.wgsl.// src/assets/shaders/particle_glow.wgsl
// Custom shader for particles
// Creates a radial gradient glow effect with additive blending
#import bevy_sprite::mesh2d_vertex_output::VertexOutput

@group(#{MATERIAL_BIND_GROUP}) @binding(0) var<uniform> color: vec4<f32>;

@fragment
fn fragment(mesh: VertexOutput) -> @location(0) vec4<f32> {
    // Calculate distance from center (UV space is 0-1)
    let center = vec2<f32>(0.5, 0.5);
    let dist = distance(mesh.uv, center) * 2.0; // *2 to normalize to 0-1
    
    // Create radial gradient
    // Bright center → fades to edges
    let radial = 1.0 - smoothstep(0.0, 1.0, dist);
    
    // Add extra glow in the center
    let glow = pow(1.0 - dist, 3.0);
    
    // Combine radial gradient with center glow
    let intensity = radial * 0.7 + glow * 0.5;
    
    // Boost brightness near center for hot glow effect
    let brightness = 1.0 + glow * 0.5;
    
    // Apply to color (supports HDR - values > 1.0)
    let final_rgb = color.rgb * brightness;
    let final_alpha = color.a * intensity;
    
    return vec4<f32>(final_rgb, final_alpha);
}
How the shader creates the glow effect:Inputs the shader receives: - The particle’s color sent from Rust code via  (remember the  binding) - The pixel’s position on the particle square. When Bevy renders a sprite with , it automatically creates a quad (rectangle) mesh and assigns UV coordinates to each corner: (0,0) at bottom-left, (1,1) at top-right. The GPU interpolates these for each pixel in between.What’s a mesh and what are UV coordinates?A  is a 3D shape made of triangles. For 2D sprites, Bevy creates a simple quad (2 triangles forming a rectangle) to display the image. are like a map that tells the shader where each pixel is on that rectangle. Think of it like a grid:U goes left to right (0.0 = left edge, 1.0 = right edge)V goes bottom to top (0.0 = bottom edge, 1.0 = top edge)So (0.5, 0.5) is the exact center of the particleWhen the shader runs, every pixel knows its UV position. With these inputs (color and UV coordinates), the shader creates a radial gradient by calculating each pixel’s distance from the particle’s center. Pixels near the center get bright colors, while pixels at the edges fade to transparent. - Measure how far this pixel is from the particle center - Use  to gradually fade from bright (center) to dark (edges) - Multiply center pixels by values above 1.0 for that “hot core” effect - Mix the smooth fade with the bright center for a natural-looking glowWithout the shader, particles are just flat colored squares. With it, they become glowing orbs of energy.A function that creates smooth transitions. smoothstep(edge0, edge1, x) returns 0 when x is at edge0, 1 when x is at edge1, and smoothly transitions between them. Unlike a straight line transition, it starts slow, speeds up in the middle, then slows down at the end, creating natural looking fades.Now that we understand shaders, let’s create the Rust code that uses our shader. The  struct is our bridge between Rust code and the GPU shader, it holds the particle’s color and tells Bevy which shader file to use for rendering.Create src/particles/material.rs:This macro tells Bevy how to send data from your Rust code to the GPU shader. Think of it like packing a box to ship: the  label says “put the color value in slot 0 so the shader can find it.”: A material defines how a surface looks when rendered. It combines a shader (the rendering program) with properties (like color). Our  is a custom material specifically for particles.: A shader program that runs for each pixel being drawn. It calculates the final color of that pixel. Our fragment shader creates the radial glow effect.: How transparent objects are combined with what’s behind them. Normal alpha blending makes things see-through. Additive blending (what we use) adds brightness values together for glowing effects.: Customizing the rendering pipeline for this specific material. We use it to configure additive blending instead of normal transparency.Now implement the Material2d trait:The  trait tells Bevy how to render our custom material. We implement three methods: - Returns the path to our shader file - Enables transparency blending - Configures the rendering pipeline for additive blendingThe  function configures the GPU’s rendering pipeline for our particle material. It tells the GPU how to blend each rendered particle with what’s already on screen.What this means for rendered particles: When a particle is drawn, the GPU needs to know how to combine its color with the background. Normal transparency makes overlapping particles darker. Additive blending makes them brighter and glowing. This is what creates that magical fire/magic look.Here’s what the function does:Access the pipeline descriptor - Gets the configuration for how this material will be rendered - Locates where color output is defined - Configures the blend mode:
     - Multiply particle color by its transparency - Keep the background color at full strength (don’t darken it) - Add them togetherResult: Overlapping particles add their brightness together, creating intense glows where they overlap. Ten overlapping fire particles create a bright white-hot center!What’s additive blending?Normal alpha blending: new_color = particle_color * alpha + background * (1 - alpha)Additive blending: new_color = particle_color + backgroundOverlapping particles add their brightness together, creating intense glows. This is how fire, magic, and explosions get that magical glowing look.Create :Systems in a chain run sequentially in the order specified. We want: - Spawn new particles - Update existing particlescleanup_finished_emitters - Remove dead emittersThis prevents edge cases where an emitter spawns particles then immediately despawns.Now that we have a particle system, let’s build the combat system that uses it! Create a new folder  for our combat system.Different powers need different behaviors and visuals. Let’s start by defining what makes each power unique.Create :Now let’s define the visual configuration for each power. We’ll separate visuals from behavior (future chapters will add damage, collision, etc.):What’s the difference between  and ?Many effects have two layers:: The outer glow/trail (lots of particles, less bright): The bright center (fewer particles, very bright)Imagine a fireball, the core is the white-hot center, the primary is the orange flames around it. Not all powers need a core, poison is just a single layer of green particles.Now we can use the  and  types we defined earlier! Let’s implement the visual configurations for each power type:Now we’ll configure how each power looks and behaves using the  struct. Each power gets unique visual properties (colors, speeds, sizes, variances) that define its character. Fire gets HDR orange colors and wide spread, while Shadow uses tight precision with many fast particles.The configuration numbers define each power’s unique character. Fire has high speed (350), wide spread (0.12 variance), and HDR orange colors. Shadow has very high speed (500), tight beam (0.05 variance), and dark purple.Why Color::srgb(3.0, 0.5, 0.1) with values above 1.0?Values above 1.0 create HDR (High Dynamic Range) colors that glow brighter than normal. When combined with additive blending (from our particle shader), these create the magical glow effect. It’s like cranking the brightness past 100%, perfect for fire and magic.What’s ?Controls how much particles spread. Low variance (0.03 for Arcane) means particles stay in a tight beam. High variance (0.25 for Poison) creates a wide, spreading cloud. It’s measured in radians.Now notice the design pattern here: each power has a distinct  expressed through numbers:This data-driven approach means adding a new power is just adding a new function with different numbers, no code logic changes needed.Now we need a component to attach to the player that tracks their current power and prevents rapid-fire spam.We use a countdown  that must finish before the next attack. When the player attacks, we check if the timer is finished. If yes, spawn particles and reset the timer back to 0.5 seconds. If no, ignore the input. This creates a smooth attack rate without complex cooldown tracking.Create src/combat/player_combat.rs:Timers in Bevy can be  (stop when finished) or  (restart automatically). For cooldowns, we want , the timer counts down from 0.5 seconds to 0, then stops. We manually reset it when the player attacks.This creates a ~2 attacks per second rate. Too fast feels spammy, too slow feels unresponsive. You can tweak this with  for different weapons or upgrades.Now for the system that handles player input and spawns projectiles.Create :: combat.cooldown.tick(time.delta()) counts down by the frame time: Only proceed if Ctrl is pressed: If elapsed_secs() < duration(), we’re still on cooldown:  starts the timer over: Offset slightly from the player in the facing direction: Power type knows its own visual configuration: Create the particle emitters using our particle system!Now let’s implement the projectile spawning system. This is where we convert player input into visible magical effects.How the spawning function works: - Spawn the main particle layer with configured count and settings - Emitter spawns once then deactivates (perfect for projectiles) - Place at the specified position -  tags this as a projectile for other systems - Some powers have a bright center layerThe function takes the visual configuration and converts it into actual particle emitters. Each emitter is its own entity with the  component.Now add the helper function to convert facing to direction:Finally, let’s add a debug system to quickly switch between powers for testing:This lets you press 1-4 to instantly switch powers while playing. Essential for testing visual effects without restarting the game.Perfect! Now our combat module is ready to use the particle system.Now let’s wire everything together!First, add  to your :Open  and add the modules:The player needs the  component. Open :Find the initialize_player_character system and add the combat component when inserting the player: There's a small chance the procedural generation places the player on top of a blocking object (tree, rock) at spawn. If you can't move when the game starts, simply restart to generate a new map. This is a quirk of random generation we'll address in future chapters.
: Switch powers (Fire, Arcane, Shadow, Poison)]]></content:encoded></item><item><title>rust actually has function overloading</title><link>https://www.reddit.com/r/rust/comments/1qnmu06/rust_actually_has_function_overloading/</link><author>/u/ali_compute_unit</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 17:37:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[while rust doesnt support function overloading natively because of its consequences and dificulties.using the powerful type system of rust, you can emulate it with minimal syntax at call site.using generics, type inference, tuples and trait overloading.trait OverLoad<Ret> { fn call(self) -> Ret; } fn example<Ret>(args: impl OverLoad<Ret>) -> Ret { OverLoad::call(args) } impl OverLoad<i32> for (u64, f64, &str) { fn call(self) -> i32 { let (a, b, c) = self; println!("{c}"); (a + b as u64) as i32 } } impl<'a> OverLoad<&'a str> for (&'a str, usize) { fn call(self) -> &'a str { let (str, size) = self; &str[0..size * 2] } } impl<T: Into<u64>> OverLoad<u64> for (u64, T) { fn call(self) -> u64 { let (a, b) = self; a + b.into() } } impl<T: Into<u64>> OverLoad<String> for (u64, T) { fn call(self) -> String { let (code, repeat) = self; let code = char::from_u32(code as _).unwrap().to_string(); return code.repeat(repeat.into() as usize); } } fn main() { println!("{}", example((1u64, 3f64, "hello"))); println!("{}", example(("hello world", 5))); println!("{}", example::<u64>((2u64, 3u64))); let str: String = example((b'a' as u64, 10u8)); println!("{str}") }    submitted by    /u/ali_compute_unit ]]></content:encoded></item><item><title>I made a derive-less reflection library with the new type_info feature!</title><link>https://gitlab.yasupa.de/nams/kyomu/-/blob/master/src/lib.rs</link><author>/u/Dry_Specialist2201</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 17:01:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Experimental] Driving Zed&apos;s GPUI with SolidJS via Binary Protocol — A &quot;No-DOM&quot; GUI Architecture</title><link>https://www.reddit.com/r/rust/comments/1qncarj/experimental_driving_zeds_gpui_with_solidjs_via/</link><author>/u/Alex6357</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 10:17:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've been experimenting with an idea: What if we could combine the DX of SolidJS with the raw performance of Zed's GPUI engine?Instead of using a WebView (like Tauri/Electron), I built a prototype called . SolidJS (compiled) running in an embedded QuickJS runtime. A custom binary command buffer (no JSON serialization). The JS thread writes bytecodes (CreateNode, SetStyle, UpdateText, etc.) to a Uint8Array. Rust consumes the buffer once per frame, updates a "Shadow DOM" struct, and renders directly using .The "Vibe Coding" Disclaimer: This is a "Stage 0" Proof of Concept. To validate the architecture quickly, I utilized LLMs (Claude/Gemini) to generate much of the boilerplate, especially the JS-to-Rust glue code. The pipeline works! I have a working Counter example with fine-grained reactivity driving native pixels. 🚀 The code is rough. Specifically, the styling engine is buggy (GPUI modifiers are tricky to map dynamically). I believe this architecture (Logic/Render separation via binary stream) is a viable path for high-performance GUIs. I'm looking for feedback on the architecture and would love help from anyone familiar with GPUI internals to fix the styling system.]]></content:encoded></item><item><title>[Media] Egor - 2D cross-platform graphics engine</title><link>https://www.reddit.com/r/rust/comments/1qn2zzq/media_egor_2d_crossplatform_graphics_engine/</link><author>/u/wick3dr0se</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 02:10:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[(Screen shakes. The gif is from one of the demos) It's been awhile (since v0.2.0?) and this is only the second time I've shared this. Egor is on version 0.8.0 now and becoming something pretty capableMy original goal was to write an MORPG, not an engine.. Trying different languages and frameworks just never felt great and I ran into hurdles that stopped me from making my game. My last attempt was in Rust with macroquad. I ended up with compilation issues, didn't care for the global context, fake async and some other things. I found myself rewriting it's crates like macroquad-tiled anyway, since they were not capable for multiplayer gamesAll of that and a lot more just eventually led me to want to start writing my own engine. But one that compiled anywhere I wanted and was stupid easy to use. Ofc I chose wgpu. Since a lot of the concepts are fairly reusable, I ended up wanting to make egor more of an application framework. Something like Tauri but without a webview and JS stack. It's primarily optimized for games (especially currently) but capable of a lot more, like an editor I've wrote with itEgor is made of reusable crates (egor_render and egor_app), a 2D renderer (wgpu), window/input (winit), text/fonts, a camera, optional ui (egui) and hot reloading (subsecond) and more. You could use the renderer and/or app crates individually to build your own engine or use the main crate to get a cross platform, easy to use, slightly opionated I've probably said more than enough.. I'd love to get any feedback, contributions, whatever keeps it moving. If it interest you:]]></content:encoded></item><item><title>zerobrew is a Rust-based, 5-20x faster drop-in Homebrew alternative</title><link>https://github.com/lucasgelfond/zerobrew</link><author>/u/lucasgelfond</author><category>rust</category><category>reddit</category><pubDate>Mon, 26 Jan 2026 01:38:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Media] Announcing Oxicord: A Discord TUI built with Ratatui &amp; Image support</title><link>https://www.reddit.com/r/rust/comments/1qmwm7d/media_announcing_oxicord_a_discord_tui_built_with/</link><author>/u/ElRastaOk</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 21:50:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I am releasing the first public version of , a Discord TUI client written in Rust.It is heavily inspired by the project Discordo, but I rewrote it from scratch to leverage the Rust ecosystem and apply a cleaner architecture.: Supports Sixel, Kitty, and iTerm2 image protocols via ratatui-image.: Full markdown support, including syntax highlighting.: Built-in file explorer to browse and paste attachments.: Uses a custom async event loop to minimize idle CPU usage.: I used a Clean Architecture approach (Domain/Infra separation) to make the codebase easier to maintain and test compared to typical monolithic TUIs.   submitted by    /u/ElRastaOk ]]></content:encoded></item><item><title>Things I miss in Rust</title><link>https://www.reddit.com/r/rust/comments/1qmu3m6/things_i_miss_in_rust/</link><author>/u/OneWilling1</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 20:17:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Since most of my previous work was in C++ and C#, I sometimes catch myself missing certain OO features, especially:inheritance ( 😁)One thing that comes up a lot for me is constructors. I’d love to be able to define multiple  functions with different parameters, something like:pub fn new(...) pub fn new(..., extra_property: T) Right now this usually turns into patterns like  +  etc., which work but feel a bit more verbose.Is there a fundamental reason why function overloading isn’t possible (or desirable) in Rust? Is it mostly a design philosophy or are there technical constraints? And is this something that’s ever been seriously considered for the language, or is it firmly off the table?Curious to hear how others think about this, especially folks who came from C++/C# as well.EDIT: Conclusion: Builders it is. P.S. Thanks everyone for the insight!]]></content:encoded></item><item><title>Stabilizing the `if let guard` feature</title><link>https://www.reddit.com/r/rust/comments/1qmqpbz/stabilizing_the_if_let_guard_feature/</link><author>/u/Kivooeo1</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 18:16:10 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've written a blog post about the  feature I've been working on stabilizing. It covers:What it is and why it's usefulIts history and interactionsThe drop-order bugs we found(And for those who've been following my journey - there's a small note at the end about the next big step in my life I also want to say a huge thank you here. Thank you for the support, and a special thanks to those who got genuinely interested, reached out, asked questions, and even started contributing themselves. Seeing that is the best partAlso, I want to check with you: would there be interest in a future, very detailed post about how to start contributing? I'm thinking of taking a random issue and walking through the entire process: how I think, where I get stuck, where I look for answers, and how I finally fix it — with all the messy details]]></content:encoded></item><item><title>[Media] Pixel retro quiz website for refreshing key Rust concepts</title><link>https://www.reddit.com/r/rust/comments/1qmnawg/media_pixel_retro_quiz_website_for_refreshing_key/</link><author>/u/capitanturkiye</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 16:13:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I built a small Rust quiz platform over the weekend to refresh my knowledge of core Rust concepts and turned it into a pixel retro website called Cratery. It is still early but the idea is a quest based quiz where you go through different realms focused on things like ownership lifetimes traits and concurrency, answer questions and track your progress. I'm pretty much inspired by classic pixel UIs. Right now it has questions from various topics and progress is saved locally. I mainly want feedback at this stage on question difficulty clarity and overall vibe since I plan to keep improving it over time.]]></content:encoded></item><item><title>I want named arguments in Rust. Mom: We have named arguments in Rust at home:</title><link>https://www.reddit.com/r/rust/comments/1qmeqb4/i_want_named_arguments_in_rust_mom_we_have_named/</link><author>/u/nik-rev</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 09:32:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Did you know that Rust has named arguments? At least you can imitate them on nightly!let opts = #[kwargs] takes_options { display: false, debug: 2, }; The type  is inferred, and we don't have to import it.let opts = takes_options(Options { display: false, debug: 2, }); fn takes_options(opts: Options) -> Options { opts } struct Options { display: bool, debug: u32, } This is accomplished by defining the  macro as follows:macro_rules! kwargs { attr() ($fn:ident $tt:tt) => {$fn({ type InferredType = impl ?Sized; if false { panic!() as InferredType } else { InferredType $tt } })} } The following is required:RUSTFLAGS="-Znext-solver=globally" because the current trait solver can't deal with this code#![feature(type_alias_impl_trait)] to allow #![feature(stmt_expr_attributes)] and #![feature(proc_macro_hygiene)] to apply attribute macros on expressions#![feature(type_alias_impl_trait)] #![feature(stmt_expr_attributes)] #![feature(proc_macro_hygiene)] #![feature(macro_attr)] // this one is optional, allows writing attribute macros with macro_rules! macro_rules! kwargs { attr() ($fn:ident $tt:tt) => {$fn({ type InferredType = impl ?Sized; if false { panic!() as InferredType } else { InferredType $tt } })} } fn takes_options(opts: Options) -> Options { opts } #[derive(Debug, PartialEq)] struct Options { display: bool, debug: u32, } fn main() { let a = #[kwargs] takes_options { display: false, debug: 2, }; let b = takes_options(Options { display: false, debug: 2, }); assert_eq!(a, b); } What if  was an attribute macro that you apply to the entire crate, and it automatically transformed any struct literal with a lowercase path?? #![feature(custom_inner_attributes)]#![kwargs] fn main() { let a = takes_options { display: false, debug: 2, }; // the above is automatically transformed into this by #![kwargs]: let a = takes_options(Options { display: false, debug: 2, }); // because the struct literal is all lowercase. } This is only for fun! Don't actually use this :)]]></content:encoded></item><item><title>I built cpx - a modern, faster rust based replacement for cp (up to 5x faster)</title><link>https://www.reddit.com/r/rust/comments/1qmepgr/i_built_cpx_a_modern_faster_rust_based/</link><author>/u/PurpleReview3241</author><category>rust</category><category>reddit</category><pubDate>Sun, 25 Jan 2026 09:31:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Beautiful progress bars (customizable)Resume interrupted transfers (checksum safe)Exclude patterns (files, directories, glob patterns)Flexible configuration for defaults and parallelismGraceful Interupt handling with resume hintsI took inspiration from modern CLI tools like bat, fd, ripgrep. Would love to hear feedback.]]></content:encoded></item><item><title>SIMD programming in pure Rust</title><link>https://kerkour.com/introduction-rust-simd</link><author>/u/kibwen</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 20:25:55 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>December in Servo: multiple windows, proxy support, better caching, and more</title><link>https://servo.org/blog/2026/01/23/december-in-servo/</link><author>/u/kibwen</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 19:49:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[For better compatibility with older web content, we now support  CSS properties like ‘-moz-transform’ (@mrobinson, #41350), as well as window. (@Taym95, #41111).When using  on Windows, you can now see  and log output, as long as servoshell was started in a console (@jschwe, #40961).Servo diagnostics options are now accessible in servoshell via the  environment variable (@atbrakhi, #41013), in addition to the usual  /  arguments.We now use the  by default (@Narfinger, @mrobinson, #40935, #41179), on most platforms.
If you don’t want to trust the system root certificates, you can instead continue to use Mozilla’s root certificates with --pref network_use_webpki_roots.
As always, you can also add your own root certificates via :: ()., the main handle for controlling Servo, is now cloneable for sharing within the same thread (@mukilan, @mrobinson, #41010).
To shut down Servo, simply drop the last  handle or let it go out of scope.
:: and :: have been removed (@mukilan, @mrobinson, #41012).Several interfaces have also been renamed:We’ve fixed a crash that occurs when <link rel=“shortcut icon”> has an , which affected chiptune.com (@webbeef, #41056), and we’ve also fixed crashes in:Servo is also on thanks.dev, and already  (+2 over November) that depend on Servo are sponsoring us there.
If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.We now have  that allow you or your organisation to donate to the Servo project with public acknowlegement of your support.
A big thanks from Servo to our newest Bronze Sponsors: , , and !
If you’re interested in this kind of sponsorship, please contact us at .Conference talks and blogs We’ve recently published one talk and one blog post:We also have two  talks at  in  later this month:Servo developers Martin Robinson (@mrobinson) and Delan Azabani (@delan) will also be attending FOSDEM 2026, so it would be a great time to come along and chat about Servo!]]></content:encoded></item><item><title>Built a 24MB offline ML app with Burn + Tauri - runs on my iPhone at 80ms inference</title><link>https://www.reddit.com/r/rust/comments/1qlpaj5/built_a_24mb_offline_ml_app_with_burn_tauri_runs/</link><author>/u/Commercial-Comb1667</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 15:06:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hey r/rust, Just finished a project I wanted to share - a plant disease detection AI built entirely in Rust using the Burn framework, deployed to my iPhone 12 via Tauri. - 24MB total deployment (vs 7.1GB for equivalent PyTorch) - 0.39ms inference / 2,579 FPS on desktop GPU (RTX 3060) - ~80ms inference on iPhone 12 via Tauri - 38 disease classes, trained with 30% labeled data (semi-supervised) The use case required offline inference on devices farmers already own - no cloud, no dedicated hardware. PyTorch was a non-starter: 7GB dependencies, 3s cold start, installation hell. Burn compiles to a single binary and targets wgpu (native GPU), ndarray (CPU), and WASM (browser) from one codebase. The CNN is pretty standard, but Burn's derive macros make it clean: #[derive(Module, Debug)] pub struct PlantClassifier<B: Backend> { conv1: ConvBlock<B>, conv2: ConvBlock<B>, conv3: ConvBlock<B>, conv4: ConvBlock<B>, global_pool: AdaptiveAvgPool2d, fc1: Linear<B>, dropout: Dropout, fc2: Linear<B>, }4 conv blocks (32→64→128→256), BatchNorm + ReLU, GlobalAvgPool, then FC layers. The  macro handles all the weight serialization and device placement automatically.The Semi-Supervised Learning PartLabeled agricultural data is expensive (~€2/image for expert annotation). We used pseudo-labeling with a configurable confidence threshold: #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PseudoLabelConfig { pub confidence_threshold: f64, // Default: 0.9 pub max_per_class: Option<usize>, // Prevent class imbalance pub retrain_threshold: usize, // Min samples before retrain pub curriculum_learning: bool, // Start strict, relax over time }Train on 30% labeled → predict on 70% unlabeled → accept predictions >90% confidence → retrain. Result: accuracy comparable to 60% fully-labeled.This was the fun part. Tauri 2.0 wraps the Burn model in a native iOS app: cargo tauri ios build 80ms inference on the A14 chip. The Rust backend does all the ML, the UI is just HTML/JS. It's actually running Rust on an iPhone. - Burn is genuinely production-ready for inference workloads -  with the  release has solid CUDA 13 support - WASM performance is better than expected (~80ms on mobile Safari) - Compile times are... Rust compile times.  +  = 5+ min release builds - Tauri mobile is legit - one codebase, native perf]]></content:encoded></item><item><title>[MEDIA][TUI] try-rs - A project/experiment organizer that makes life much easier.</title><link>https://www.reddit.com/r/rust/comments/1qlm834/mediatui_tryrs_a_projectexperiment_organizer_that/</link><author>/u/_allsafe_</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 12:53:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/_allsafe_ ]]></content:encoded></item><item><title>[Media] PathCollab: optimizing Rust backend for a real-time collaborative pathology viewer</title><link>https://www.reddit.com/r/rust/comments/1qlkp3q/media_pathcollab_optimizing_rust_backend_for_a/</link><author>/u/Psychological-Ad5119</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 11:30:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I built PathCollab, a self-hosted collaborative viewer for whole-slide images (WSI). The server is written in Rust with Axum, and I wanted to share some of the technical decisions that made it work.As a data scientist working with whole-slide images, I got frustrated by the lack of web-based tools capable of smoothly rendering WSIs with millions of cell overlays and tissue-level heatmaps. In practice, sharing model inferences was especially cumbersome: I could not self-deploy a private instance containing proprietary slides and model outputs, generate an invite link, and review the results live with a pathologist in an interactive setting. There exist some alternatives but they typically do not allow to render millions of polygons (cells) smoothly.WSIs are huge (50k x 50k pixels is typical, some go to 200k x 200k). You can't load them into memory. Instead of loading everything at once, you serve tiles on demand using the Deep Zoom Image (DZI) protocol, similar to how Google Maps works.I wanted real-time collaboration where a presenter can guide followers through a slide, with live cursor positions and synchronized viewports. This implies:Tile serving needs to be fast (users pan/zoom constantly)Cursor updates at 30Hz, viewport sync at 10HzSupport for 20+ concurrent followers per sessionCell overlay queries on datasets with 1M+ polygonsFirst, I focus on the cursor updates.Each connection spawns three tasks:rust // Connection state cached to avoid session lookups on hot paths pub struct Connection { pub id: Uuid, pub session_id: Option<String>, pub participant_id: Option<Uuid>, pub is_presenter: bool, pub sender: mpsc::Sender<ServerMessage>, // Cached to avoid session lookups on every cursor update pub name: Option<String>, pub color: Option<String>, } The registry uses  instead of  for lock-free concurrent access:rust pub type ConnectionRegistry = Arc<DashMap<Uuid, Connection>>; pub type SessionBroadcasters = Arc<DashMap<String, broadcast::Sender<ServerMessage>>>; I replaced the RwLock<HashMap<…>> used to protect the ConnectionRegistry with a DashMap after stress-testing the server under realistic collaborative workloads. In a setup with 10 concurrent sessions (1 host and 19 followers each), roughly 200 users were continuously panning and zooming at ~30 Hz, resulting in millions of cursor and viewport update events per minute.Profiling showed that the dominant bottleneck was lock contention on the global RwLock: frequent short-lived reads and writes to per-connection websocket broadcast channels were serializing access and limiting scalability. Switching to DashMap alleviated this issue by sharding the underlying map and reducing contention, allowing concurrent reads and writes to independent buckets and significantly improving throughput under high-frequency update patterns.Each session (a session is one presenter presenting to up to 20 followers) gets a  for fan-out. The broadcast task polls with a 100ms timeout to handle session changes:rust match tokio::time::timeout(Duration::from_millis(100), rx.recv()).await { Ok(Ok(msg)) => { /* forward to client */ } Ok(Err(RecvError::Lagged(n))) => { /* log, continue */ } Err(_) => { /* timeout, check if session changed */ } } For cursor updates (the hottest path), I cache participant name/color in the Connection struct. This avoids hitting the session manager on every 30Hz cursor broadcast.Metrics use an RAII guard pattern so latency is recorded on all exit paths:```rust struct MessageMetricsGuard { start: Instant, msg_type: &'static str, }impl Drop for MessageMetricsGuard { fn drop(&mut self) { histogram!("pathcollab_ws_message_duration_seconds", "type" => self.msg_type) .record(self.start.elapsed()); } } ```Avoiding the hot path: tile caching strategyWhen serving tiles via the DZI route, the expensive path is: OpenSlide read -> resize -> JPEG encode. On a cache miss, this takes 200-300ms. Most of the time is spent on the libopenslide library actually reading bytes from the disk, so I could not do much to optimize the hot path. On a cache hit, it's ~3ms.So the goal became clear: avoid this path as much as possible through different layers of caching.Layer 1: In-memory tile cache (moka)I started by caching encoded JPEG bytes (~50KB) in a 256MB cache. The weighter function counts actual bytes, not entry count.```rust pub struct TileCache { cache: Cache<TileKey, Bytes>, // moka concurrent cache hits: AtomicU64, misses: AtomicU64, }let cache = Cache::builder() .weigher(|_key: &TileKey, value: &Bytes| -> u32 { value.len().min(u32::MAX as usize) as u32 }) .max_capacity(256 * 1024 * 1024) // 256MB .time_to_live(Duration::from_secs(3600)) .time_to_idle(Duration::from_secs(1800)) .build(); ```Layer 2: Slide handle cache with probabilistic LRUOpening an OpenSlide handle is expensive. I cache handles in an  that maintains insertion order for O(1) LRU eviction:rust pub struct SlideCache { slides: RwLock<IndexMap<String, Arc<OpenSlide>>>, metadata: DashMap<String, Arc<SlideMetadata>>, access_counter: AtomicU64, } Updating LRU order still requires a write lock, which kills throughput under load. So I only update LRU position 1 in 8 times:```rust pub async fn get_cached(&self, id: &str) -> Option<Arc<OpenSlide>> { let slides = self.slides.read().await; if let Some(slide) = slides.get(id) { let slide_clone = Arc::clone(slide); // Probabilistic LRU: only update every N accesses let count = self.access_counter.fetch_add(1, Ordering::Relaxed); if count % 8 == 0 { drop(slides); let mut slides_write = self.slides.write().await; if let Some(slide) = slides_write.shift_remove(id) { slides_write.insert(id.to_string(), slide); } } return Some(slide_clone); } None This is technically imprecise but dramatically reduces write lock contention. In practice, the "wrong" slide getting evicted occasionally is fine.Layer 3: Cloudflare CDN for the online demoAs I wanted to setup a public web demo (it's here ), I rented a small Hetzner instance CPX22 (2 cores, 4GB RAM) with fast NVMe SSD. I was concerned that my server would be completely overloaded by too many users. In fact, when I initially tested the deployed app , I quickly realized that ~20% of my requests had a 503 Service Temporarily Available response. Even with the 2 layers of cache above, the server was still not able to serve all these tiles.I wanted to experiment with Cloudflare CDN (never used before). Tiles are immutable (same coordinates always return the same image), so I added cache headers to the responses:rust (header::CACHE_CONTROL, "public, max-age=31536000, immutable") For the online demo at pathcollab.io, Cloudflare sits in front and caches tiles at the edge. The first request hits the origin, subsequent requests from the same region are served from CDN cache. This is the biggest win for the demo since most users look at the same regions.Here are the main rules that I set:Name: Bypass dynamic endpointsExpression Preview: bash (http.request.uri.path eq "/ws") or (http.request.uri.path eq "/health") or (http.request.uri.path wildcard r"/metrics*") Indeed, we do not want to cache anything on the websocket route.Expression Preview: bash (http.request.uri.path wildcard r"/api/slide/*/tile/*") This is the most important rule, to relieve the server from serving all the tiles requested by the clients.The slow path: At first, I inserted blocking I/O instructions (using OpenSlide to read bytes from disk) between two await instructions. After profiling and researching on Tokio's forums, I realized this is a big no-no, and that I/O blocking code inside async code should be wrapped inside a Tokio's  task.I referred to Alice Ryhl's blogpost on how long a task is to be considered blocking. Simply put, tasks taking more than 100ms are considered blocking. This was clearly the case for OpenSlide with non-sequential reads typically taking 300 to 500ms.Therefore, for the "cache-miss" route, the CPU-bound work runs in : ```rust let result = tokio::task::spawn_blocking(move || { // OpenSlide read (blocking I/O) let rgba_image = slide.read_image_rgba(&region)?; histogram!("pathcollab_tile_phase_duration_seconds", "phase" => "read") .record(read_start.elapsed());// Resize with Lanczos3 (CPU-intensive) let resized = image::imageops::resize(&rgba_image, target_w, target_h, FilterType::Lanczos3); histogram!("pathcollab_tile_phase_duration_seconds", "phase" => "resize") .record(resize_start.elapsed()); // JPEG encode encode_jpeg_inner(&resized, jpeg_quality) R-tree for cell overlay queriesMoving on to the routes serving cell overlays. Cell segmentation overlays can have 1M+ polygons. When the user pans, the client sends a request with the (x, y) coordinate of the top left of the viewport, as well as the height and width. This allows me to query efficiently the cell polygons lying inside the user viewport (if not already cached on the client side) using the  crate with bulk loading:```rust pub struct OverlaySpatialIndex { tree: RTree<CellEntry>, cells: Vec<CellMask>, }pub struct CellEntry { pub index: usize, // Index into cells vector pub centroid: [f32; 2], // Spatial key }impl RTreeObject for CellEntry { type Envelope = AABB<[f32; 2]>;fn envelope(&self) -> Self::Envelope { AABB::from_point(self.centroid) } Query is O(log n + k) where k is result count:```rust pub fn query_region(&self, x: f64, y: f64, width: f64, height: f64) -> Vec<&CellMask> { let envelope = AABB::from_corners( [x as f32, y as f32], [(x + width) as f32, (y + height) as f32] );self.tree .locate_in_envelope(&envelope) .map(|entry| &self.cells[entry.index]) .collect() As a side note, the index building runs in  since parsing the cell coordinate overlays (stored in a Protobuf file) and building the R-tree for 1M cells takes more than 100ms.On my M1 MacBook Pro, with a 40,000 x 40,000 pixel slide, PathCollab (run locally) gives the following numbers:Cursor broadcast (20 clients)Cell query (10k cells in viewport)The cache hit rate after a few minutes of use is typically 85-95%, so most tile requests are sub-millisecond.I hope you liked this post. I'm happy to answer questions about any of these decisions. Feel free to suggest more ideas for an even more efficient server, if you have!]]></content:encoded></item><item><title>[Media] [TUI] tmmpr - terminal mind mapper</title><link>https://www.reddit.com/r/rust/comments/1qljnpe/media_tui_tmmpr_terminal_mind_mapper/</link><author>/u/tanciaku</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 10:28:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[A Linux terminal application for creating mind maps with vim-inspired navigation.Built with Rust + Ratatui.Place notes anywhere on an infinite canvas (0,0 to infinity)Draw connections between notes with customizable colorsNavigate with hjkl, multiple modes for editing/moving/connectingAuto-save and backup systemStatus: Work in progress - core functionality is solid and usable, but some features and code quality need improvement. Feedback and contributions welcome!Install: cargo install tmmpr]]></content:encoded></item><item><title>Succinctly: A fast jq/yq alternative built on succinct data structures</title><link>https://www.reddit.com/r/rust/comments/1qleizg/succinctly_a_fast_jqyq_alternative_built_on/</link><author>/u/john-ky</author><category>rust</category><category>reddit</category><pubDate>Sat, 24 Jan 2026 05:32:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've been working on Succinctly, a Rust library and CLI tool that provides jq and yq functionality using succinct data structures (semi-indexing with rank/select).Covers most jq and yq query patterns (reduce, limit, recurse, regex, path functions, etc.)Parses JSON at ~880 MiB/s, YAML at ~250-400 MiB/sSupports position-based navigation (at_offset, at_position) for IDE integrationWhat it doesn't do (yet):input/inputs (streaming multiple JSON values from stdin)Streaming for files larger than memorySome advanced YAML edge casesPerformance vs jq (AMD Ryzen 9 7950X):Performance vs yq (Apple M1 Max):x86_64: AVX2 SIMD, POPCNT, BMI2 (PDEP/PEXT for DSV parsing)Benchmarks run on AMD Zen 4 and Apple M1 Max — results will vary on older CPUs without these instructionssuccinctly jq '.users[].name' data.json succinctly yq '.spec.containers[]' k8s.yaml succinctly yq -o json '.' config.yaml # YAML to JSON Why succinct data structures?Instead of building a full DOM, semi-indexing creates a lightweight index over the raw text. This enables O(1) navigation to any node without parsing the entire document upfront — and uses 6-10x less memory than jq/yq on large files.The library is no_std compatible.Feedback welcome — especially bug reports for queries that work in jq/yq but fail here.]]></content:encoded></item><item><title>mmdr: A native Rust Mermaid renderer (500-1000x faster than mermaid-cli)</title><link>https://github.com/1jehuang/mermaid-rs-renderer</link><author>/u/Medium_Anxiety_8143</author><category>rust</category><category>reddit</category><pubDate>Fri, 23 Jan 2026 19:00:55 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Replacing Protobuf with Rust to go 5 times faster</title><link>https://pgdog.dev/blog/replace-protobuf-with-rust</link><author>/u/levkk1</author><category>rust</category><category>reddit</category><pubDate>Fri, 23 Jan 2026 15:21:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Lev KokotovPgDog is a proxy for scaling PostgreSQL. Under the hood, we use  to parse and understand SQL queries. Since PgDog is written in Rust, we use its Rust bindings to interface with the core C library. 
Those bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby  gem.Protobuf is fast, but not using Protobuf is faster. We forked  and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).You can reproduce these by cloning our fork and running the benchmark tests: (Protobuf) (Direct C to Rust) (Protobuf) (Direct Rust to C)The first step is always profiling. We use samply, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered :This is the entrypoint to the  C library, used by all  bindings. The function that wraps the actual Postgres parser, , barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:While the  parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.This works pretty well, but eventually we ran into a couple of issues:Some ORMs can have bugs that generate thousands of unique statements, e.g.,  instead of , which causes a lot of cache missesApplications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s  packageThe clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.I’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly  task, it can work really well.The prompt we started with was pretty straightforward:libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for ,  (used in our new query rewrite engine, which we’ll talk about in another post),  and . These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in  benchmarks.Just to be clear: we had a lot of things going for us already that made this possible. First,  has a Protobuf spec for  (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.Second,  was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.And last, and definitely not least,  already had a working  and  implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used , we included a call to , compared their results and if they differed by even one byte, Claude Code had to go back and try again.The translation code between Rust and C uses  Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/ C API which does the actual work of building the AST.The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an  C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:For each node in the list, the implementation calls , which then handles each one of the 100s of tokens available in the SQL grammar:For nodes that contain other nodes, we recurse on  again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., ) or text (e.g., ), the data type is copied into a Rust analog, e.g.,  or .The end result is , a Rust struct generated by Prost from the  API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare  and  outputs, using the derived  trait, and ensure that both are identical, in testing.While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, let us know!Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.If stuff like this is interesting to you, reach out. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.]]></content:encoded></item><item><title>rust_analyzer is eating my memory, any counter measure?</title><link>https://www.reddit.com/r/rust/comments/1qkqcqr/rust_analyzer_is_eating_my_memory_any_counter/</link><author>/u/EarlyPresentation186</author><category>rust</category><category>reddit</category><pubDate>Fri, 23 Jan 2026 13:08:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I have 32Gb of RAM, on this linux system I'm running 3 browser instances, and the rest is neovim instances to edit rust code. I sometimes open multiple neovim instances in different git worktrees (or in the same directory) and from my understanding each one starts a rust_analyzer instance. This leads to my system swapping and even grinding to a halt because the swap is full. I will again increase the swap and try to decrease the swapiness now. But does anyone have other suggestions to limit the memory consumption by rust-analyzer?]]></content:encoded></item><item><title>OneTalker - An Augmentative and Alternative Communication (AAC) app written in Rust</title><link>https://www.reddit.com/r/rust/comments/1qknxzz/onetalker_an_augmentative_and_alternative/</link><author>/u/MissionNo4775</author><category>rust</category><category>reddit</category><pubDate>Fri, 23 Jan 2026 11:04:44 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I'm happy to announce that the first ever version of OneTalker is out!I wrote it for my son Ben, who is a full-time wheelchair user and has Quadriplegic Cerebral Palsy.Ben DOES NOT tolerate slow things, and this absolutely MUST NOT crash either!His current Augmentative and Alternative Communication apps are slow, so he doesn't like using them. I hope others find it useful too.I think it's first AAC app in the world written in Rust.For those interested, I'd love it if you could test it. I'm working on getting all the packages signed at moment. Thanks!]]></content:encoded></item><item><title>Considering Strictly Monotonic Time</title><link>https://matklad.github.io/2026/01/23/strictly-monotonic-time.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[Monotonic time is a frequently used, load bearing abstraction. Monotonicity is often enforced using
the following code:That is, ask the OS about the current monotonic time, but don’t trust the result too much and clamp
it using an in-process guard. Under normal scenarios, you can trust the OS promise of monotonicity,
but, empirically, there’s a long tail of different scenarios where the promise isn’t upheld:
https://github.com/rust-lang/rust/pull/56988Today I realized that, if you are doing the above, you might as well force the time to be 
monotonic:The benefit of strict monotonicity is that you can tighten asserts,

can become

and that  catches the bug where you pass in  the same instance.
In other words, the  version explicitly allows either query-ing the time again, or using the old
value directly.Conversely, with strictly monotonic time, you know that if you see two numerically identical time
instances, they must have been ultimately derived from the exact same call to . Time becomes
fundamentally less ambiguous.The constraint here is that the resolution of the time value ( the clock resolution) needs to
be high enough, to make sure that repeated  don’t move you into the future, but nanosecond
precision seems fine for that.]]></content:encoded></item><item><title>The Rust GCC backend can now be installed with rustup</title><link>https://www.reddit.com/r/rust/comments/1qk9t1t/the_rust_gcc_backend_can_now_be_installed_with/</link><author>/u/imperioland</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 23:06:07 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Starting tomorrow (23rd of January 2026), you will be able (on linux without cross-compilation) to install and use the Rust GCC backend directly from rustup! To do so: rustup component add rustc-codegen-gcc Thanks a lot to Kobzol for all their work to making it a reality!]]></content:encoded></item><item><title>Where does Rust break down?</title><link>https://www.reddit.com/r/rust/comments/1qk8qt7/where_does_rust_break_down/</link><author>/u/PointedPoplars</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 22:21:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[As a preface, Rust is one of my favorite languages alongside Python and C.One of the things I appreciate most about Rust is how intentionally it is designed around abstraction: e.g. function signatures form strict, exhaustive contracts, so Rust functions behave like true black boxes.But all abstractions have leaks, and I'm sure this is true for Rust as well.For example, Python's `len` function has to be defined as a magic method instead of a normal method to avoid exposing a lot of mutability-related abstractions.As a demonstration, assigning `fun = obj.__len__` will still return the correct result when `fun()` is called after appending items to `obj` if `obj` is a list but not a string. This is because Python strings are immutable (and often interned) while its lists are not. Making `len` a magic method enforces late binding of the operation to the object's current state, hiding these implementation differences in normal use and allowing more aggressive optimizations for internal primitives.A classic example for C would be that `i[arr]` and `arr[i]` are equivalent because both are syntactic sugar for `*(arr+i)`TLDR: What are some abstractions in Rust that are invisible to 99% of programmers unless you start digging into the language's deeper mechanics?]]></content:encoded></item><item><title>crates.io development update | Rust Blog - A new &quot;Security&quot; tab, migration to Svelte for the front-end, support for GitLab CI/CD Trusted Publishing, Lines of Code metrics</title><link>https://blog.rust-lang.org/2026/01/21/crates-io-development-update/</link><author>/u/nik-rev</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 20:24:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made to crates.io over the past six months.Crate pages now have a new "Security" tab that displays security advisories from the RustSec database. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.The tab shows known vulnerabilities for the crate along with the affected version ranges.This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank the OpenSSF (Open Source Security Foundation) for funding this work and Dirkjan Ochtman for implementing it.
Trusted Publishing EnhancementsIn our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature.Trusted Publishing now supports GitLab CI/CD in addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow.Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome!
Trusted Publishing Only ModeCrate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens.The  and  GitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk.Crate pages now display source lines of code (SLOC) metrics, giving you insight into the size of a crate before adding it as a dependency. This metric is calculated in a background job after publishing using the tokei crate. It is also shown on OpenGraph images:
Publication Time in IndexA new  field has been added to crate index entries, recording when each version was published. This enables several use cases:Cargo can implement cooldown periods for new versions in the futureCargo can replay dependency resolution as if it were a past date, though yanked versions remain yankedServices like Renovate can determine release dates without additional API requests
Svelte Frontend MigrationAt the end of 2025, the crates.io team evaluated several options for modernizing our frontend and decided to experiment with porting the website to Svelte. The goal is to create a one-to-one port of the existing functionality before adding new features.This migration is still considered experimental and is a work in progress. Using a more mainstream framework should make it easier for new contributors to work on the frontend. The new Svelte frontend uses TypeScript and generates type-safe API client code from our OpenAPI description, so types flow from the Rust backend to the TypeScript frontend automatically.Thanks to eth3lbert for the helpful reviews and guidance on Svelte best practices. We'll share more details in a future update.These were some of the more visible changes to crates.io over the past six months, but a lot has happened "under the hood" as well.Cargo user agent filtering: We noticed that download graphs were showing a constant background level of downloads even for unpopular crates due to bots, scrapers, and mirrors. Download counts are now filtered to only include requests from Cargo, providing more accurate statistics.: Emails from crates.io now support HTML formatting.: OAuth access tokens from GitHub are now encrypted at rest in the database. While we have no evidence of any abuse, we decided to improve our security posture. The tokens were never included in the daily database dump, and the old unencrypted column has been removed.: Crate pages now display a "Browse source" link in the sidebar that points to the corresponding docs.rs page. Thanks to Carol Nichols for implementing this feature.: The sparse index at index.crates.io is now served primarily via Fastly to conserve our AWS credits for other use cases. In the past month, static.crates.io served approximately 1.6 PB across 11 billion requests, while index.crates.io served approximately 740 TB across 19 billion requests. A big thank you to Fastly for providing free CDN services through their Fast Forward program!OpenGraph image improvements: We fixed emoji and CJK character rendering in OpenGraph images, which was caused by missing fonts on our server.Background worker performance: Database indexes were optimized to improve background job processing performance.CloudFront invalidation improvements: Invalidation requests are now batched to avoid hitting AWS rate limits when publishing large workspaces.We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on Zulip or GitHub. We are always happy to hear from you and are looking forward to your feedback!]]></content:encoded></item><item><title>Polyfit - Because statistics is hard, and linear regression is made entirely out of footguns</title><link>https://www.reddit.com/r/rust/comments/1qk0v16/polyfit_because_statistics_is_hard_and_linear/</link><author>/u/rscarson</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 17:30:50 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I needed to draw a curve fit through some data, and it turned into a year long rabbit hole, where I discovered that stats is really involved, and that the rust ecosystem is a bit barren in terms of user-friendly batteries-included polynomial fitting libraries.So I built Polyfit - Because you don't need to be able to build a powerdrill to use one safely.The full power of polynomial fitting without needing to understand all the mathSensible parameters (DegreeBound, scoring metrics, basis functions) that don't feel arbitrary or like magic numbersExtensive documentation, examples, and built in testing toolsMy goals for the project were:Never ask for a number without context - ask for a random number and you get a random number Instead, if I can derive the correct value myself I doIf I can't, I have named presets that describe in detail why you'd pick themProvide sensible defaults for everything If you don't care about a setting, you shouldn't have to think about itYou should not  to understand the math to get good resultsPerformance I tried to prioritize speed and memory efficiency where possibleOn my fairly average laptop, I can do a 100 million point fit in ~1sYou need to be able to test it Not understanding the math shouldn't be a barrier to making sure it worksThere's a whole test suite included with extensive docs, examples, and sensible defaultsThe tests even generate a plot on failure so you can see what went wrongAnd I included a set of random noise injection transforms to help you make synthetic data for testingThe tests will even show seeds used on failure for reproducibilityHere's some examples of why you'd want to use PolyfitOh no! I have all this data and I need to draw a line through ituse polyfit::{ score::Aic, statistics::DegreeBound, ChebyshevFit, }; let mut fit = ChebyshevFit::new_auto(&data, DegreeBound::Relaxed, &Aic)?; let equation = fit.as_monomial()?.to_string(); let pretty_line = fit.solve_range(0.0..=100.0, 1.0)?; DegreeBound::Relaxed uses your data to pick a reasonable degree without overfittingAic is a scoring metric. Smallish datasets tend to do well with itWe use as_monomial to get the equation in a human readable format.Oh gee willikers How am I going to figure out which of these data points are outlierslet covariance = fit.covariance()?; // It's the thing that tells us how certain we are about the fit just roll with it let outliers = covariance.outliers(Confidence::P95, Some(Tolerance::Absolute(0.1)))?; The Confidence is just a measure of how much you trust the fit. P95 is a good optionI added Tolerance because real world data is messy. If I know my sensor is only accurate to +/- 0.1 units I shouldn't need to mess with the confidence level to account for that. It's basically an engineering correction for ConfidenceI also have extensive calculus support, soSay you have weather data with temperature over time:use polyfit::{FourierFit, score::Aic, statistics::DegreeBound}; let fit = FourierFit::new_auto(&data, DegreeBound::Relaxed, &Aic)?; // Derivatives for rates of change // Critical points are neat for this // This tells us when the temperature stops rising or falling and starts doing the opposite for point in fit.critical_points()? { match p { CriticalPoint::Minima(x, _y_) => println!("Found a local minimum at x = {}", x), CriticalPoint::Maxima(x, _y_) => println!("Found a local maximum at x = {}", x), CriticalPoint::Inflection(x, _y_) => println!("Found an inflection point at x = {}", x), } } There's too many options how do I pick a basis for my data!It tests your data on every basis I support and gives you an easy to digest report:--------------------------------[ How to interpret the results ] [ Results may be misleading for small datasets (<100 points) ] - Score Weight: Relative likelihood of being the best model among the options tested, based on the scoring method used. - Fit Quality: Proportion of variance in the data explained by the model (uses huber loss weighted r2). - Normality: How closely the residuals follow a normal distribution (useless for small datasets). - Rating: Combined score (0.75 * Fit Quality + 0.25 * Normality) to give an overall quality measure. - Stars: A simple star rating out of 5 based on the Rating score. Not scientific. - The best 3 models are shown below with their equations and plots (if enabled). Less params is a simpler model, which is betterBetter fit quality means it explains more of the dataBetter normality means it's probably not underfitting (too simple)The rating is a weighted combination of fit quality and normality to give an overall score]]></content:encoded></item><item><title>Rust In Production: How Gama Space Controls Satellites In Orbit With Rust</title><link>https://corrode.dev/podcast/s05e09-gama-space/</link><author>/u/mre__</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 16:44:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Space exploration demands software that is reliable, efficient, and able to operate in the harshest environments imaginable. When a spacecraft deploys a solar sail millions of kilometers from Earth, there’s no room for memory bugs, race conditions, or software failures. This is where Rust’s robustness guarantees become mission-critical.In this episode, we speak with Sebastian Scholz, an engineer at Gama Space, a French company pioneering solar sail and drag sail technology for spacecraft propulsion and deorbiting. We explore how Rust is being used in aerospace applications, the unique challenges of developing software for space systems, and what it takes to build reliable embedded systems that operate beyond Earth’s atmosphere.
    CodeCrafters helps you become proficient in Rust by building real-world,
    production-grade projects. Learn hands-on by creating your own shell, HTTP
    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.
  
    Start for free today and enjoy 40% off any paid plan by using
    this link.
  Gama Space is a French aerospace company founded in 2020 and headquartered in Ivry-sur-Seine, France. The company develops space propulsion and orbital technologies with a mission to keep space accessible. Their two main product lines are solar sails for deep space exploration using the sun’s infinite energy, and drag sails—the most effective way to deorbit satellites and combat space debris. After just two years of R&D, Gama successfully launched their satellite on a SpaceX Falcon 9. The Gama Alpha mission is a 6U cubesat weighing just 11 kilograms that deploys a large 73.3m² sail. With 48 employees, Gama is at the forefront of making space exploration more sustainable and accessible.Sebastian Scholz is an engineer at Gama Space, where he works on developing software systems for spacecraft propulsion technology. His work involves building reliable, safety-critical embedded systems that must operate flawlessly in the extreme conditions of space. Sebastian brings expertise in systems programming and embedded development to one of the most demanding environments for software engineering.GAMA-ALPHA - The demonstration satellite launched in January 2023Ada - Safety-focused programming language used in aerospaceprobe-rs - Embedded debugging toolkit for Rusthyper - Fast and correct HTTP implementation for RustFlutter - Google’s UI toolkit for cross-platform developmentUART - Very common low level communication protocolRexus/Bexus - European project for sub-orbital experiments by studentsEmbassy - The EMBedded ASsYnchronous frameworkCSP - The Cubesat Space ProtocolHubris - Oxide’s embedded operating systemZeroCopy - Transmute data in-place without allocations]]></content:encoded></item><item><title>Rust 1.93.0 is out</title><link>https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/</link><author>/u/manpacket</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 14:04:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.93.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.93.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!
Update bundled musl to 1.2.5The various  targets now all ship with musl 1.2.5. This primarily affects static musl builds for , , and  which bundled musl 1.2.3. This update comes with several fixes and improvements, and a breaking change that affects the Rust ecosystem.For the Rust ecosystem, the primary motivation for this update is to receive major improvements to
musl's DNS resolver which shipped in 1.2.4 and received bug fixes in 1.2.5. When using 
targets for static linking, this should make portable Linux binaries that do networking more
reliable, particularly in the face of large DNS records and recursive nameservers.
Allow the global allocator to use thread-local storageRust 1.93 adjusts the internals of the standard library to permit global allocators written in Rust
to use std's  and
 without
re-entrancy concerns by using the system allocator instead. attributes on  linesPreviously, if individual parts of a section of inline assembly needed to be 'd, the full 
block would need to be repeated with and without that section. In 1.93,  can now be applied to
individual statements within the  block.Many people came together to create Rust 1.93.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>[Media] musicfree: a cross-platform music downloader implemented in Rust</title><link>https://www.reddit.com/r/rust/comments/1qjqq51/media_musicfree_a_crossplatform_music_downloader/</link><author>/u/Every_Juggernaut7580</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 10:07:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[musicfree is a music download tool written in pure Rust. It supports multiple platforms, including Windows, macOS, Unix, and Android. There are two versions available: a CLI version at musicfree and a Tauri version at musicfree-tauri.Currently, it supports downloading single videos from YouTube and Bilibili, downloading playlists, and cover images.]]></content:encoded></item><item><title>This Week in Rust #635</title><link>https://this-week-in-rust.org/blog/2026/01/21/this-week-in-rust-635/</link><author>/u/b-dillo</author><category>rust</category><category>reddit</category><pubDate>Thu, 22 Jan 2026 04:13:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This week's crate is throttled-tracing, a crate of periodic and throttled logging macros.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.If you are a feature implementer and would like your RFC to appear in this list, add a
 label to your RFC along with a comment providing testing instructions and/or
guidance on which aspect(s) of the feature need testing.Let us know if you would like your feature to be tracked as a part of this list.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No Calls for participation were submitted this week.If you are a Rust project owner and are looking for contributors, please submit tasks here or through a PR to TWiR or by reaching out on Bluesky or Mastodon!Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker. | CFP closes 2026-02-16 | Montreal, Quebec, Canada | 2026-09-08 - 2026-09-11If you are an event organizer hoping to expand the reach of your event, please submit a link to the website through a PR to TWiR or by reaching out on Bluesky or Mastodon!Various changes in both direction, but not much has changed overall.Improvements ✅  (secondary)3 Regressions, 4 Improvements, 7 Mixed; 6 of them in rollups
40 artifact comparisons made in totalEvery week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.Rusty Events between 2026-01-21 - 2026-02-18 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.I might suspect that if you are lumping all statically-typed languages into a single bucket without making particular distinction among them, then you might not have fully internalized the implications of union (aka Rust enum aka sum) typed data structures combined with exhaustive pattern matching.I like to call it getting "union-pilled" and it's really hard to accept otherwise statically-typed languages once you become familiar.This Week in Rust is edited by:]]></content:encoded></item></channel></rss>