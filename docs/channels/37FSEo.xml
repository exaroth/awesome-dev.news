<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rust</title><link>https://www.awesome-dev.news</link><description></description><item><title>Optimizing Rust Compilation: Smaller, Faster, or Both?</title><link>https://dev.to/leapcell/optimizing-rust-compilation-smaller-faster-or-both-16pe</link><author>Leapcell</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 23 Feb 2025 08:18:56 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[You have completed writing a Rust project and are now working on compilation. How can you make the compiled file as small as possible? How can you make it run as fast as possible? Or how can you achieve both small size and high speed?You may have these considerations:: Suitable for embedded development, where the project is small and not complex, and execution speed is already fast. The main goal is to reduce the file size as much as possible.Maximizing execution speed: Suitable for network services where file size is not a concern, but maximizing concurrency is the top priority.Balancing both size and speed: A middle ground that is suitable for various types of projects.You only need to add the following configuration to your  file and run:
  
  
  Generate a Smaller Executable

  
  
  Generate a Faster Executable

  
  
  Balance Between Size and Speed

  
  
  Explanation of Configurations
: Specifies the level of compiler optimizations.: No optimization, fastest compilation time.: Optimize for faster compilation.: Balance between compilation speed and runtime performance (default).: Optimize for maximum runtime performance.: Optimize for smaller code size.: Further optimize for code size, more aggressively than .: Use  to generate the smallest executable; use  to generate the fastest executable.: Enables Link Time Optimization (LTO).: Disable LTO (default).: Enable the most aggressive LTO.: Enabling LTO reduces binary size and improves runtime performance.  is a moderate choice, while  provides the best optimization but increases compilation time.: Controls the number of code generation units.: Usually . Setting it to  enables the highest level of optimization.: Reducing the number of code generation units gives the compiler more information for global optimizations, resulting in a smaller and faster executable. Setting it to  maximizes optimization but increases compilation time.: Controls panic behavior.: Unwind the stack (default).: Directly abort the process.: Using  reduces the executable size and improves performance in some cases since it eliminates the need for stack unwinding information.: Controls which debug and symbol information is removed.: Keep all information (default).: Remove debug information.: Remove symbol tables but retain necessary debug information.: Remove all optional information, including debug and symbol data.: Removing unnecessary debug and symbol information significantly reduces executable size.These are the optimization techniques for compiling a Rust project. Have you mastered them?Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage — no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead — just focus on building.]]></content:encoded></item><item><title>Rust, ROS, and dynamic typing https://open.substack.com/pub/intrepidai/p/rust-ros-and-dynamic-typing?r=7n2a9&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false</title><link>https://dev.to/fgadaleta/rust-ros-and-dynamic-typing-42n9</link><author>frag</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 23 Feb 2025 07:36:01 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Streaming SQL in Stateful DataFlows</title><link>https://dev.to/debadyuti/streaming-sql-in-stateful-dataflows-3jng</link><author>Deb</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sat, 22 Feb 2025 22:37:35 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[
  
  
  Streaming SQL Functionality
SQL Streaming Queries and Stream Processing Operations is released in Stateful DataFlow Beta 7 running on Fluvio 0.15.2With SQL Streaming on Stateful DataFlow you can:Run ad-hoc queries on saved state objects and materialized views based live event streams.Use SQL queries to run stream processing operations in data flows.For those who are not aware of Fluvio or Stateful DataFlow yet:Stateful DataFlow - Stream processing layer built on Fluvio built using the wasm component model.
  
  
  SQL: From Static Tables to Streaming Data
Remember when SQL was the only way to talk to your data? It wasn't just a query language - it was  query language. But its story goes deeper than syntax.
  
  
  The Universal Language of Data
Just as merchants in medieval Mediterranean ports needed a shared language to trade (that's where "lingua franca" came from), the tech world needed SQL to make data accessible across different systems and teams.If you're in a room with a DBA, a data analyst, and a business analyst. What's the one language they all speak? Likely SQL.Look familiar? Whether you're running Oracle, Postgres, or MySQL, this just works. Well sort of!Three key factors made SQL a long-term utility that stood the test of time:
Instead of telling machines HOW to get data, you just say WHAT you want. SELECT * FROM users WHERE status = 'active' reads almost like English.
From startups to Fortune 500s, SQL skills travel. Write once, run anywhere - from healthcare to fintech.
Need to analyze sales data? Track user behavior? SQL's got you covered, backed by decades of tooling and optimization.In a world of Artificial Intelligence, Web3, and global markets, event streaming is no longer a luxury - it's a basic need. Ask yourself:Is your application combining data from multiple sources in real-time?Are your customers happy with stale insights?Do you need fresh data on demand?
  
  
  Bridging Static and Streaming
What if you could use familiar SQL syntax for real-time data processing? What if your team could leverage their existing SQL skills for stream processing?We've been exploring these questions and implementing solutions that bring SQL's simplicity to streaming data. Want to see how? Check out the full article where we dive into:Practical examples using NY Transit dataReal-world streaming SQL queries in actionHow to implement stream processing without learning a new language]]></content:encoded></item><item><title>HN: cypher queries tips (Graph dbms)</title><link>https://dev.to/falkordb/hn-cypher-queries-tips-graph-dbms-hck</link><author>Dan Shalev</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:46:14 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Writing performant Cypher queries isn’t just about syntax—it’s about understanding graph structures, optimizing query paths, and leveraging advanced features. At FalkorDB, we’ve seen how poorly optimized queries can bottleneck even the most robust systems.Most devs don’t realize inefficient Cypher queries often stem from broad MATCH patterns and missing indexes. ]]></content:encoded></item><item><title>Deep Dive into Rust Structs: A Comprehensive Guide</title><link>https://dev.to/sajiram_a4704bc095/deep-dive-into-rust-structs-a-comprehensive-guide-956</link><author>Sajiron</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Thu, 20 Feb 2025 12:03:08 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Structs in Rust are essential for organizing and managing complex data structures efficiently. This guide covers regular, tuple, and unit-like structs, along with advanced features such as generics, associated functions, lifetimes, and struct update syntax. Learn how to create structured, readable, and reusable Rust code with practical examples and best practices.]]></content:encoded></item><item><title>Announcing Rust 1.85.0 and Rust 2024</title><link>https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.85.0. This stabilizes the 2024 edition as well.
Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.85.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!We are excited to announce that the Rust 2024 Edition is now stable!
Editions are a mechanism for opt-in changes that may otherwise pose a backwards compatibility risk. See the edition guide for details on how this is achieved, and detailed instructions on how to migrate.This is the largest edition we have released. The edition guide contains detailed information about each change, but as a summary, here are all the changes:The guide includes migration instructions for all new features, and in general
transitioning an existing project to a new edition.
In many cases  can automate the necessary changes. You may even find that no changes in your code are needed at all for 2024!Note that automatic fixes via  are very conservative to avoid ever changing the semantics of your code. In many cases you may wish to keep your code the same and use the new semantics of Rust 2024; for instance, continuing to use the  macro matcher, and ignoring the conversions of conditionals because you want the new 2024 drop order semantics. The result of  should not be considered a recommendation, just a conservative conversion that preserves behavior. people came together to create this edition. We'd like to thank them all for their hard work!Rust now supports asynchronous closures like  which return futures when called. This works like an  which can also capture values from the local environment, just like the difference between regular closures and functions. This also comes with 3 analogous traits in the standard library prelude: , , and .In some cases, you could already approximate this with a regular closure and an asynchronous block, like . However, the future returned by such an inner block is not able to borrow from the closure captures, but this does work with  closures:let mut vec: Vec<String> = vec![];

let closure = async || {
    vec.push(ready(String::from("")).await);
};
It also has not been possible to properly express higher-ranked function signatures with the  traits returning a , but you can write this with the  traits:use core::future::Future;
async fn f<Fut>(_: impl for<'a> Fn(&'a u8) -> Fut)
where
    Fut: Future<Output = ()>,
{ todo!() }

async fn f2(_: impl for<'a> AsyncFn(&'a u8))
{ todo!() }

async fn main() {
    async fn g(_: &u8) { todo!() }
    f(g).await;
    //~^ ERROR mismatched types
    //~| ERROR one type is more general than the other

    f2(g).await; // ok!
}
Hiding trait implementations from diagnosticsThe new #[diagnostic::do_not_recommend] attribute is a hint to the compiler to not show the annotated trait implementation as part of a diagnostic message. For library authors, this is a way to keep the compiler from making suggestions that may be unhelpful or misleading. For example:pub trait Foo {}
pub trait Bar {}

impl<T: Foo> Bar for T {}

struct MyType;

fn main() {
    let _object: &dyn Bar = &MyType;
}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
 --> src/main.rs:9:29
  |
9 |     let _object: &dyn Bar = &MyType;
  |                             ^^^^ the trait `Foo` is not implemented for `MyType`
  |
note: required for `MyType` to implement `Bar`
 --> src/main.rs:4:14
  |
4 | impl<T: Foo> Bar for T {}
  |         ---  ^^^     ^
  |         |
  |         unsatisfied trait bound introduced here
  = note: required for the cast from `&MyType` to `&dyn Bar`
For some APIs, it might make good sense for you to implement , and get  indirectly by that blanket implementation. For others, it might be expected that most users should implement  directly, so that  suggestion is a red herring. In that case, adding the diagnostic hint will change the error message like so:#[diagnostic::do_not_recommend]
impl<T: Foo> Bar for T {}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
  --> src/main.rs:10:29
   |
10 |     let _object: &dyn Bar = &MyType;
   |                             ^^^^ the trait `Bar` is not implemented for `MyType`
   |
   = note: required for the cast from `&MyType` to `&dyn Bar`
 and  for tuplesEarlier versions of Rust implemented convenience traits for iterators of  tuple pairs to behave like , with  in 1.56 and  in 1.79. These have now been  to more tuple lengths, from singleton  through to 12 items long, . For example, you can now use  to fanout into multiple collections at once:use std::collections::{LinkedList, VecDeque};
fn main() {
    let (squares, cubes, tesseracts): (Vec<_>, VecDeque<_>, LinkedList<_>) =
        (0i32..10).map(|i| (i * i, i.pow(3), i.pow(4))).collect();
    println!("{squares:?}");
    println!("{cubes:?}");
    println!("{tesseracts:?}");
}
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]
[0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]
Updates to  has been deprecated for years, because it can give surprising results in some Windows configurations if the  environment variable is set (which is not the normal configuration on Windows). We had previously avoided changing its behavior, out of concern for compatibility with code depending on this non-standard configuration. Given how long this function has been deprecated, we're now updating its behavior as a bug fix, and a subsequent release will remove the deprecation for this function.These APIs are now stable in const contextsMany people came together to create Rust 1.85.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Languages in the Linux kernel</title><link>https://dev.to/cies/languages-in-the-linux-kernel-43bf</link><author>Cies Breijs</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Wed, 19 Feb 2025 22:06:22 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[The Linux kernel it written in many languages, but at the time of writing (2025) the main language of the project is C, clocking in at 98% (from github.com/torvalds/linux on 2025-02-19):I believe the only language —or more correct "language family"— in the Linux kernel that will always be there is Assembly. At some point you need to write code, without any abstraction, directly for the hardware. Assembly is just that.
  
  
  Allowing C++ in the kernel
I believe Linus was right to reject C++ for the following reaons:C++ is a gigantic language, allowing it would create an endless discussion on which of its features would be allowed/forbidden,being a loose superset of C it has the same archaic syntax, thus little improvement to the developer experience, andno matter how many features would be allowed C++ does not improve enough on C to offset the cost of porting it.In recent year one thing became clear: C is no longer the best tool for the job. It's still the best understood tool, especially by the current team. It's what 98% of the Linux kernel is written in today. But the "unchallenged best" status it used to have for so many years in pragmatic kernel development is shaky.Better languages have come up, specifically Zig and Rust.There has been a lot of discussion on allowing Rust in the kernel. But since the main honchos, Linus and Greg Kroah-Hartman, have put their weight behind it, it is clear that Rust will find it's way into the project. We still have to wait and see how much of the project will be written in Rust.Unlike C++, Rust has lots of features that are very useful in kernel development, and very few features that would need to be forbidden. Especially features related to memory safety, which constitute a large part of the bugs in Linux. Compared to C, Rust provides much a improved developer experience, which is not weird considering Rust is 40 younger.Rust's main downside is: slower compile times. Compile times matter very much, but so does safety. While this is a hard trade-off, the decision seems to be final: Rust is to stay in the Linux kernel.I expect Zig will come from another angle. The Zig compiler compiles C as well as Zig code. It's just a matter of time before the Zig compiler will be able to compile the Linux kernel. Once this is achieved, C-files can be ported to Zig one-by-one. I expect LLMs will help a great deal with the initial port of the Linux kernel's C code to Zig. Once in Zig, the code can be optimized by humans.Zig is very similar to C. This makes the initial port rather straight forward. LLMs will perform much better on C-to-Zig than on C-to-Rust. Compared to C, Zig brings serious improvements in the developer experience at similar-to-C compile times. Features like  are really cool and may allow the kernel project to do away with lots of crufty old C preprocessor macros on the one side, while allowing for interesting optimizations on the other.I expect we will have a Linux kernel in Zig, Rust and Assembly in 10 years. It may be a fork. It may be the Linux mainline: it all depends on the open-mindedness of the main devs.So far they seem up for it!]]></content:encoded></item><item><title>How I Built a Local LLM-Powered File Reorganizer in Rust</title><link>https://dev.to/__87e2e207/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip</link><author>Евгений Перминов</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Wed, 19 Feb 2025 15:20:29 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Some time ago, I decided to dive into Rust —this must be my  attempt. I’d tried learning it before, but each time I either got swamped by the borrow checker or got sidetracked by other projects. This time, I wanted a small,  project to force myself to stick with Rust. The result is messy-folder-reorganizer-ai, a command-line tool for file organization powered by a local LLM.
  
  
  The Inspiration: A Bloated Downloads Folder
The main motivation was my messy  folder, which often ballooned to hundreds of files—images, documents, installers—essentially chaos. Instead of manually sorting through them, I thought, “Why not let an AI propose a structure?”While brainstorming, I stumbled upon the possibility of running LLMs , like Ollama or other self-hosted frameworks. I loved the idea of  my data to some cloud service. So I decided to build a Rust-based CLI that  a local LLM server for suggestions on how to reorganize my folders.
  
  
  Challenges: LLM & Large Folders
 I started using , but the responses didn’t follow prompt instructions well, so I switched to , which performed much better.
 When testing on folders with many files, the model began forgetting the beginning of the prompt and stopped following instructions properly. Increasing  (which defines the model’s context size) helped partially, but the model still struggles with .
 Split the file list into smaller chunks and send multiple prompts.
 If you’re an LLM expert—especially with local models like Ollama—I’d love advice on how to handle larger sets without hitting memory or context limits. Specify the local LLM endpoint, model name, or other model options.
 Tweak the AI prompt to fine-tune how the model interprets your folder’s contents.
 The tool shows you the proposed structure and asks for confirmation before reorganizing any files. I’d love code feedback — best practices, performance tips, or suggestions on how to structure the CLI.
 Any advice on optimizing local model inference for large file sets or advanced chunking strategies would be invaluable.This project has been a great way to re-learn some Rust features and experiment with local AI solutions. While it works decently for medium-sized folders, there’s plenty of room to grow. If this concept resonates with you—maybe your Downloads folder is as messy as mine—give it a try, open an issue, or contribute a pull request.
Feel free to reach out on the GitHub issues page, or drop me a note if you have any thoughts, suggestions, or just want to talk about Rust and AI!]]></content:encoded></item><item><title>How I Built a Local LLM-Powered File Reorganizer with Rust</title><link>https://dev.to/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip</link><author>Evgenii Perminov</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Wed, 19 Feb 2025 15:20:29 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Some time ago, I decided to dive into Rust —this must be my  attempt. I’d tried learning it before, but each time I either got swamped by the borrow checker or got sidetracked by other projects. This time, I wanted a small,  project to force myself to stick with Rust. The result is messy-folder-reorganizer-ai, a command-line tool for file organization powered by a local LLM.
  
  
  The Inspiration: A Bloated Downloads Folder
The main motivation was my messy  folder, which often ballooned to hundreds of files—images, documents, installers—essentially chaos. Instead of manually sorting through them, I thought, “Why not let an AI propose a structure?”While brainstorming, I stumbled upon the possibility of running LLMs , like Ollama or other self-hosted frameworks. I loved the idea of  my data to some cloud service. So I decided to build a Rust-based CLI that  a local LLM server for suggestions on how to reorganize my folders.
  
  
  Challenges: LLM & Large Folders
 I started using , but the responses didn’t follow prompt instructions well, so I switched to , which performed much better.
 When testing on folders with many files, the model began forgetting the beginning of the prompt and stopped following instructions properly. Increasing  (which defines the model’s context size) helped partially, but the model still struggles with .
 Split the file list into smaller chunks and send multiple prompts.
 If you’re an LLM expert—especially with local models like Ollama—I’d love advice on how to handle larger sets without hitting memory or context limits. Specify the local LLM endpoint, model name, or other model options.
 Tweak the AI prompt to fine-tune how the model interprets your folder’s contents.
 The tool shows you the proposed structure and asks for confirmation before reorganizing any files. I’d love code feedback — best practices, performance tips, or suggestions on how to structure the CLI.
 Any advice on optimizing local model inference for large file sets or advanced chunking strategies would be invaluable.This project has been a great way to re-learn some Rust features and experiment with local AI solutions. While it works decently for medium-sized folders, there’s plenty of room to grow. If this concept resonates with you—maybe your Downloads folder is as messy as mine—give it a try, open an issue, or contribute a pull request.
Feel free to reach out on the GitHub issues page, or drop me a note if you have any thoughts, suggestions, or just want to talk about Rust and AI!]]></content:encoded></item><item><title>Building CSV RAG with Rig and Rust 🔥🔥🔥</title><link>https://dev.to/josh_mo_91f294fcef0333006/building-csv-rag-with-rig-and-rust-2bpi</link><author>Josh Mo</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Wed, 19 Feb 2025 14:13:54 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[In this article, we're going to look at how you can use the  Rust AI framework to create an application that is able to load a CSV file, embed it into a vector store and have an LLM answer questions based on provided context from our vector store. We'll serve the application in the form of a command-line tool.Retrieval-Augmented Generation (RAG) is a technique that combines the power of information retrieval with the generative capabilities of large language models (LLMs). The process typically involves two main components: retrieving relevant knowledge from a pre-existing database or knowledge base and generating contextually accurate answers based on that information. When a question or query is posed, the model first retrieves relevant snippets or pieces of information from the knowledge base. It then augments its generative model with this newly acquired knowledge, enabling it to provide a more accurate, informed response.In contrast to traditional LLMs that rely purely on their trained parameters, RAG systems dynamically incorporate external data, ensuring that responses are grounded in factual and up-to-date knowledge. When a question is asked, the relevant knowledge from a pre-existing dataset is retrieved and fed into the model. The LLM then generates an answer that combines the retrieved information and its generative abilities, aiming to offer a detailed and accurate response. This hybrid approach allows RAG models to provide answers that are not only linguistically fluent but also well-supported by factual data.Next, you'll need to create a new project using :Before we start, let's add the relevant dependencies which we need:cargo add serde rig-core tokio csv 
serde/derive,tokio/macros,tokio/rt-multi-thread
This adds the following dependencies: - The rig library. - an asynchronous Rust runtime. We additionally add the  and  feature as we want to use the macro. - The Rust crate for using CSVs.Before we do anything else, we'll need to declare our struct type as well as derive the correct macros for it. Note that the embedding trait for Rig also depends on  (which also depends on ), hence the derivation.Next to be able to use Rig's embeddings API, we need to implement the  trait. This trait simply defines a method for what to embed. Because we don't have a singular field we want to embed and will need the whole record, we can simply just implement  for the struct then format the whole struct into a string (which we can then embed).Thankfully, embedding our CSV is actually quite easy now that we've done the hard part. We can create an OpenAI client and get the embedding model, then iterate through each record and build an embedding from it and store it in Rig's in-memory vector store.You can see below the code to generate an OpenAI client is fairly simple:We'll also keep the code for loading the CSV in as a separate function as it's relatively long.The next step will be to actually take input from the user and prompt the LLM using not only the prompt, but additional (relevant!) snippets from the vector store. We'll start by taking some input from standard input (i.e., you load up the application in the terminal, then you type in something and press Enter). We then check if the prompt is equal to  - if it is then immediately break the loop and close the application, if not, carry on and retrieve a response from the LLM.In terms of storing your conversation history, it is actually quite simple to do so. All you need to do is to create a  and then feed it into the agent, using the  function instead of . You then add a user message and assistant message (in that order) at the end of the loop iteration. This allows you to build up a conversation with the LLM.Additionally, we'll also add a  command which allows the entire conversation to be wiped should the user want to ask the LLM about something else.Now that we've set up a base for your command-line tool, here are a few ways you can extend this demo to do much more than just simple ragging:What about semantic routing to make sure your LLM stays on track?Try incorporating CSV RAG into a new, bigger pipeline!Thanks for reading! Hopefully you will have a good example of how to improve how you work with data by creating a RAG pipeline from your CSV.]]></content:encoded></item><item><title>Master Rust’s Ownership &amp; Borrowing System 🚀</title><link>https://dev.to/sajiram_a4704bc095/master-rusts-ownership-borrowing-system-cp2</link><author>Sajiron</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Wed, 19 Feb 2025 12:11:58 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Rust’s Ownership & Borrowing system is one of its most powerful features, ensuring memory safety without a garbage collector. In this guide, we break down:✅ How ownership works and why it matters
✅ The difference between mutable & immutable references
✅ How lifetimes prevent dangling references]]></content:encoded></item><item><title>This Week in Rust 587</title><link>https://this-week-in-rust.org/blog/2025/02/19/this-week-in-rust-587/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 19 Feb 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is httpmock, which is quite unsurprisingly a HTTP mocking library for Rust.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.This week's results were dominated by the update to LLVM 20 (#135763),
which brought a large number of performance improvements, as usually. There were also two other
significant improvements, caused by improving the representation of  values (#136593) and doing less work when formatting in  (#136828).Improvements ✅  (secondary)3 Regressions, 2 Improvements, 4 Mixed; 4 of them in rollups
50 artifact comparisons made in totalEvery week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No RFCs entered Final Comment Period this week.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-02-19 - 2025-03-19 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.I have found that many automated code review tools, including LLMs, catch 10 out of 3 bugs.Despite a lamentable lack of suggestions, llogiq is properly pleased with his choice.]]></content:encoded></item><item><title>I created a CLI-Music Player in Rust!</title><link>https://dev.to/paradoxy/i-created-a-cli-music-player-in-rust-5a3f</link><author>Ojalla</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Tue, 18 Feb 2025 22:52:01 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[I built a  that allows users to play, pause, stop, and manage their music files directly from the terminal. It supports basic playback controls, volume adjustment, and song listing.This walkthrough will cover:How the music player was implementedHow to run the applicationA guide on available commandsI used the following Rust crates: – For command-line argument parsing. – For colored terminal output. – For audio playback. – To handle  for graceful exit.cargo add clap colored rodio ctrlc
How the CLI Music Player WorksThe music player follows a command-line workflow where users can:Load songs from a specified directory.Play, pause, resume, stop playback.1. Command-Line Interface (CLI) ConfigurationThe application uses  to handle command-line arguments. Users must specify a music directory:This ensures that users provide a valid music directory or request help using .The player reads user commands in a loop:This function displays a colored prompt and waits for user input.3. Implementing the Music PlayerI created a  struct to store the player’s state:The audio sink and output stream ().The directory containing music files.The current playing song and status.4. Loading Songs from a DirectoryWhen the application starts, it loads music files from the given directory:This function reads all files in the specified directory and maps them to numbers for easy selection.To play a song, the application reads the file, decodes it, and plays it using :Stops any currently playing track.Loads and decodes the selected song.6. Implementing Playback ControlsThe player handles commands like play, pause, resume, stop, and volume adjustment:These functions control playback using  methods like , , , and .7. Displaying Available SongsUsers can list all available songs:git clone https://github.com/Parado-xy/rust-cli-music-player
rust-cli-music-player
Run the application with a music directory:
cargo run  /path/to/music
Use the available commands:
play <number>    
pause            
resume           
stop             
volume <0.0-1.0> 
list             
status           This Rust CLI Music Player is a simple but powerful terminal-based music player. It utilizes  for audio playback,  for argument parsing, and  for improved UI. Future improvements could include playlist support and file format filtering.]]></content:encoded></item><item><title>Automating Data Classification with Stof</title><link>https://dev.to/amelia_wampler_e7aa93dab9/automating-data-classification-with-stof-afo</link><author>Amelia Wampler</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Tue, 18 Feb 2025 20:04:18 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Data classification is a crucial step in organizing and utilizing information effectively, especially in AI-driven applications. With Stof, developers can seamlessly classify and structure incoming data using built-in schema functions. In this example, we’ll walk through how Stof can automatically assign color classifications to t-shirt records based on hex values or color names.
  
  
  The Problem: Identifying T-Shirt Colors
Imagine you're processing product data, and a t-shirt record comes in with only a single color field. This value could be a color name or a hex code, but to make it useful for AI models or analytics, you need a standardized classification.
  
  
  The Stof Solution: Schema-Based Classification
Using Stof’s schema and schemify functions, we can dynamically classify colors as data flows in. Here’s how it works:Preloading a Color Dataset – A comprehensive list of color names and their corresponding hex values is stored efficiently in a compiled binary format. – A function converts hex codes into RGB values to find the closest matching color in our dataset.Applying Meta-Code for Classification – As each t-shirt record is processed, Stof automatically assigns a standardized color name based on the closest RGB match. If a name is provided instead of a hex value, it appends the corresponding hex code instead.Running this through Stof’s CLI (or embedding it in an application) demonstrates its efficiency. Given a hex code, Stof finds the closest color match and updates the record. For example: Hex Code for Light Blue → Output: "Aquamarine" "Lime" → Output: Hex Code for Lime GreenThis logic runs efficiently via WebAssembly, making it highly performant across different environments.With Stof, data classification happens in real-time, reducing the need for extra processing logic in applications. This approach enhances AI readiness, improves data consistency, and simplifies the developer experience—all while ensuring data remains structured and searchable.This is just one example of how Stof can automate data structuring and classification. Whether you’re handling product data, AI training sets, or dynamic records, Stof streamlines data governance and accessibility. Read more in our Docs and visit our Github to start contributing. ]]></content:encoded></item><item><title>Online events: Rust in English (Feb 18-Feb 28)</title><link>https://dev.to/szabgab/online-events-rust-in-english-feb-18-feb-28-454i</link><author>Gabor Szabo</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Tue, 18 Feb 2025 18:18:41 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[I found the following Rust in English-related online events for the next 10 days.Online events remove the physical limitation of who can participate. What remain are the time-zone differences and the language barrier. In order to make it easier for you to find events that match those constraints I started to collect the online events where you can filter by topic and time. Above I took the events and included the starting time in a few selected time-zones. I hope it makes it easier to find an event that is relevant to you. The data and the code generating the pages are all on GitHub. Share your ideas on how to improve the listings to help you more.]]></content:encoded></item><item><title>My first Aya program</title><link>https://dev.to/littlejo/my-first-aya-program-2j0p</link><author>Joseph Ligier</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Tue, 18 Feb 2025 09:17:23 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[I'm getting started with eBPF programming with Aya. The idea behind this series of articles is to get you started too.In this section, we'll create our first Aya program. We'll also see that there are different types of eBPF programs.
  
  
  Example of Hello world Program
I assume you have an operating environment as defined in Part 1. You can also follow this step-by-step lab on creating an Aya program:To generate an Aya program, we'll use cargo generate :cargo generate https://github.com/aya-rs/aya-template
We're going to answer a few questions:: the name of the project (I put )The second question is more interesting:In fact, there are different types of eBPF programs. As we saw in Part 1, I wrote that there are 3 types of eBPF software:Observability and TracingThe choices presented to generate the eBPF program are, in fact, parts of the Linux kernel that can be modified or supervised by eBPF. So you need to think carefully about this before jumping headlong into programming.As I wrote in Part 1, Aya is not yet fully mature compared with other solutions. Here's one reason why: although it may seem as if there are already a lot of program types available, there are still quite a few missing:What's more, as the Linux kernel continues to evolve, there will certainly be other types of eBPF programs available in the near future.I tested a few types of eBPF program with Aya before writing this article. To study each type, you can quickly spend days: "I don't understand, it doesn't work" is certainly the phrase I'm uttering the most at the moment. There aren't many similarities between eBPF program types. If you've already played with  programs, you'll have other problems with  programs, for example. For this article, we'll be looking at  eBPF programs.Tracepoints are, as the name suggests, places in the Linux code that have been marked (see recommendations). They are mainly used for tracing and debugging the Linux kernel.All these points are accessible in the /sys/kernel/debug/tracing/available_events file:Just in time, this is the supplementary question we ask:We're going to choose  because everyone knows what a syscall is. Does everyone?When a program is run, it asks the kernel to perform primary functions called syscalls (system calls).To view the syscalls performed when a command is run, you can use the strace program. For example, you can use:To find available tracepoint names, simply issue this command:grep syscalls: /sys/kernel/debug/tracing/available_events
We can see that they are of the : sys_(enter|exit)_$name_of_syscall : starting of syscallWe'll arbitrarily choose , which is the syscall that executes a program and the one that starts the . So we have .Now we're going to test the generated program directly:cd test-aya
RUST_LOG=info cargo run
This will take a little while: the time it takes to download and compile all the dependencies. Now you can go and have a cup of coffee!Once your program has been demarched, run commands on another terminal, e.g.: 
On the  terminal, you'll see the following message every time you type a command:[INFO test_aya] tracepoint sys_enter_execve called
So every time the syscall  is called, this message will be displayed. We've just created the "Hello World" of the Tracepoint eBPF program (). So, for an eBPF program to start up, it needs an event () that tells it to run: this is what we call a .
  
  
  Anatomy of a Hello world Aya program
Let's take a look at what the  command has generated. As we saw in part 1, there are two parts to an eBPF program: kernel space and user space.The eBPF program source code can be found here: test-aya-ebpf/src/main.rs. It is compiled first.
Here are its contents:If you've been following part 1, there's one thing that should already shock you about Rust code: there is no  function. This is a prerequisite for writing an eBPF program. To overcome this problem, we use the notation: Another more disturbing prerequisite is that the standard library (std) is forbidden. Only the core library and all libraries that use it are allowed. This means you can't use the  macro. To tell Rust not to use the standard: But then how are we going to display anything if  isn't possible? That's where Aya comes to the rescue: 
This library can be used to display messages if the  environment variable is set. It also includes , ,  and  (see documentation).
Thanks to this library, you can display :info!(&ctx, "tracepoint sys_enter_execve called");We're going to discard this part of the program:Don't panic! We'll never change it. It's the code that makes the program work.
All that's left is to explain :Two  libraries are used, including one for the tracepoint macro:In other words, we're going to create an eBPF tracepoint program with the function .The  keyword is to say that the function is public; it's not really “important”, it's just to make the Aya framework work. If we remove these unwanted elements:We're back to a few things we've already seen in part 1.
Things to remember:the  function is attached to the  program.All the work will be done in the  function.: this is the context variable that will enable us to go beyond a  program.As you can see, there's no mention of  for the moment. I want this Tracepoint program to be attached to an  syscall. Good timing: let's take a look at the user-space code now.In this article, we won't be modifying the user space code. It can be found in this file: .
I'll simplify its code for a better understanding:This Rust code is a little more traditional: you can use the standard library, as evidenced by the  and the  function.
I've commented out the important parts of the code with numbers:1: We'll load the previously compiled eBPF code into an ebpf variable2: We'll start displaying the eBPF logs (for example, the  from the previous code) in the user space3: Convert the main function of the eBPF code into  code4: Load the main program function (the  function)5: Attach it to the tracepoint syscalls:sys_enter_execveThe rest of the code is for program operation, such as not quitting the program before the logs start.If you're new to Rust, there are certainly some parts of the code that may seem obscure to you. That's a good thing! The next section is dedicated to Rust.
  
  
  Rust, it's getting complicated!
Before modifying the code, I think it's worth reviewing the Rust language a little more theoretically. I didn't want to scare you too much in part 1 😃In Part 1, we looked at variable declaration and modification. But we didn't talk much about types. This is important because, for example, all functions must be filled in with the various types.
There are two types:Let's take a look at scalar types: for integers, for example, we can define very precisely how many bits they are encoded in:It's often optional to declare the variable with its type, but to remove any ambiguity, we do it this way:With , you can display the contents in a different way, for example, you can convert to hexadecimal:That's the magic of the macro! There's also this kind of possibility with  in Aya. For example, you can convert a number into an  or  address: very useful for eBPF networking.Let's finish with scalar types and talk about , i.e. modifying the type. Let's take a look at this example:This program won't work because the arguments require u32 and not u16. It is of course possible to change the function directly, but you don't always have access to the function as you would in a library. How do I change the code? Use the keyword :Now let's talk about compound types, for example how to represent integer arrays:An array has a fixed number of entries: you can't add a number after the fact. There are also dynamic arrays in Rust, such as . However, eBPF programs (on the kernel side) cannot use them: they require arrays with a number of entries already defined before compilation. The strategy is therefore to evaluate the maximum number an array can have by filling it with 0.The default string in Rust is an array (a slice, to be precise - I'll get a slap on the wrist if I don't) with a fixed number of  entries (and therefore ). It is not, as in other programming languages, an array of characters (). The notation is .Thus, it's not possible to concatenate in this way:However, if we convert  to , it works: is the equivalent of . This makes it possible to have arrays of  dynamically. are a mixture of different types. The keyword is . For example, to create a simple role-playing game :If you find initialization a little complicated, you can create methods with the  keyword:As you can see, thanks to the structures, the programming language reads almost naturally. In a way, it's like an object language.
To give you a more realistic example, take a look at the code generated in the user area above:What can be deduced from the doc, the approximate code:Previously, we had to cast to have the same type:Using generic function, we don't need to cast:This way, you won't have to think about which type you need to use the addition function.
The function is inevitably a little more complicated to write. We won't be writing any in this section. But for use via a library, it's pure bliss (I'm exaggerating a little bit).Soon, we'll see a generic function:We've been seeing  a lot since part 1. Let's talk about it in a little more detail. is an . We haven't seen what it is, but to put it simply:  lets you create functions that can result in a success or an error. This is elegant when used with  or  as we saw in Part 1.As with ,  and  can be of any type (within certain constraints).
As a reminder, here's a simple example of a function that returns  :In this example, only integers greater than 10 are displayed. is another  quite similar to Result: the function returns either  or . It's an  value.To retrieve the  value, there's no equivalent to the question mark with . However, a trick is to convert the Option type with  to  and then add the .
Here's an equivalent of the previous code with a function that returns  :Another solution to retrieve this value is to use  method:But there's a big : it's not possible to use  in kernel space code. Why not? To cut a long story short:  can panic, and an eBPF program can't afford that:The piece of code that we discarded in the kernel space above :It can be used to manage the case of , for example.When a variable is created in a program, the compiler must take care of finding where in RAM to add the variable, as well as deleting it to avoid so-called memory leaks. For fixed variables such as integers or non-dynamic arrays, variables are stored in , so there are no problems with releasing them. Thus, because of the restrictions in kernel space programs, there are no clean-up problems in eBPF. Dynamic variables, on the other hand, are stored in , and that's where the problems come in.In , we let the developer do this with  and . If he forgets to free memory, it often goes unnoticed, but is a potential source of bugs.In other languages (such as  or ), the garbage collector takes care of cleaning up automatically, without developer intervention. However, this is done at the expense of performance.In Rust, we use the notion of ownership to solve the problem. has the  type, which is dynamicThis program cannot compile. Why? will reserve memory space in the heap:let s1 = String::from("Hello"); will retrieve the ownership of this memory space: 
Thus  has lost ownership of this memory space:This means that no two (or more) variables can have the same memory space. Rust will automatically release a variable once it has left the scope.The most elegant solution is to use a  (Keyword: ). These references are also known as safe pointers:This will display  twice.
We say that 's value, but  retains its property.
Let's take one last example to show you that this isn't always obvious:This code doesn't compile either. Why? will reserve a memory space in the heap:let s = String::from("Hello");
The  function will take the property of this memory space:
At the end of this processing, Rust will delete this memory space:So the display of  crashes.To solve this problem simply use this borrowing system:When you're not used to it, you often make the mistake. But the error messages are self-explanatory and therefore easy to correct.The  keyword is very useful when working at low level. It's the sysadmin equivalent of . By default, Rust has protections, notably for reading memory. The unsafe keyword lets you tell Rust: "Don't worry, I know what I'm doing! If you set  anywhere, Rust will remind you with a warning, or if you don't set unsafe anywhere, Rust will tell you to set it if that's what you really want to do.Let's finish this busy section with raw pointers. The notion is similar to the C pointer. It lets you know the address of the variable. There are two types of raw pointers:If data of type  cannot change : If data of type  can change: To convert to a raw pointer, use the  function:That's all for this Rust part. I hope it wasn't too complicated and dense to understand. If it was, don't worry: let us guide you through the rest, and we'll review each point for a concrete case.We've already created a program that every time a binary is executed, it displays a few things in the logs. It would be nice to be able to see which binary is being executed. So we're going to create a little program that will log all the binaries that are executed on the machine. It might be handy to see this for security reasons.To do this, you'll need to modify the aya code in kernel space test-aya-ebpf/src/main.rs
This is the main part you'll need to modify:This is based on the  variable, which is a  structure.
To do this, we'll take a look at the documentation:The library is located at the program level:use aya_ebpf::programs::TracePointContext;
This shows all the documentation for each type of eBPF program:This shows the functions that can be used with :We're not interested in the first function, which is used to create a . It's probably used in the macro. That leaves us with the second function: . This function reads the tracepoint at a certain offset. How do we find this offset?You need to look in the syscall tracepoint file. To find it, go to: /sys/kernel/debug/tracing/events/[category]/[name]. And the file name is .
So the file is here: /sys/kernel/debug/tracing/events/syscalls/sys_enter_execve/format.We'll try to retrieve the filename with offset :I wasn't sure what to put in place of , so I arbitrarily set the type to .
If you try to compile :It works better now with :Now let's try using this variable. The result of the read_at function is . The aim is to retrieve . Just use the question mark ().
So we'll get :Nevertheless, compilation won't work because if it makes an error, the  function returns an  and my final function returns a .Similarly, the  function must always return a . We must therefore cast the type to  :With these type-matching modifications, it should compile. Let's have a look at the contents of filename. Here's the whole file now if you're lost :If we run a cargo run again, here's what we see:[INFO  test_aya] tracepoint sys_enter_execve called 94803283704040
[INFO  test_aya] tracepoint sys_enter_execve called 94803283704112
[INFO  test_aya] tracepoint sys_enter_execve called 94173001563176
[INFO  test_aya] tracepoint sys_enter_execve called 824637710416
[INFO  test_aya] tracepoint sys_enter_execve called 94865232898256
[INFO  test_aya] tracepoint sys_enter_execve called 824638316624
[INFO  test_aya] tracepoint sys_enter_execve called 94865232898256
These numbers are not file names. This was to be expected, given that the type is . What I notice is that when I run a command: I'm always number  if it's the server it's the other numbers, if I change user I get a different number.
Well... that's not much help. Shall we give up? The documentation isn't complete, it doesn't say what it's really for.
I look at the source code of the  function:And what does  do?In this way, an address in memory is accessed. Now we need to think about how to successfully read this address.
I'll take a look at the helper functions:I have the feeling that this is this function that i need:Let's add these lines and see what happens:So all we have to do is change the  type to  (a raw pointer) and we'll change the variable names to be consistent with the documentation:The compilation works! Now we have a byte-coded file name. Now we need to figure out how to convert it to .That's what I want, but as I said earlier, eBPF doesn't accept the standard library ().
Miracle! It also exists in the  library :So we add:let _filename = core::str::from_utf8(_filename_bytes);
Let's see if it compiles:So it compiled fine. But the eBPF verifier is not happy at all. The verifier is a kernel protection system that prevents an eBPF program from being launched if it considers it dangerous for the kernel. What we see in the screenshot is JIT code (Just In Time code).The explanation is at the end:In the documentation, there's another function that might suit us. Perhaps  makes too many checks that are not compatible with the eBPF verifier?Let's add  and :It compiles!
By modifying the log line:I connect with  on the server where the program is installed, I see all the binaries that are executed :The code works, and if you clean it up afterwards, you'll get a code similar to this oneYou can find the program in GitHub.That's all for this part. I hope you enjoyed it. It was much more technical than the first part. In the next part, we'll be looking at a few things we've already mentioned: the eBPF map. This will, for example, solve the problem we have with the program: file names are truncated. With an eBPF map, we can solve the problem!]]></content:encoded></item><item><title>Rust Control Flow &amp; Functions: A Beginner’s Guide 🚀</title><link>https://dev.to/sajiram_a4704bc095/rust-control-flow-functions-a-beginners-guide-fp5</link><author>Sajiron</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Tue, 18 Feb 2025 08:47:19 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Control flow is the backbone of programming, dictating how code executes based on conditions and loops. In Rust, we have if statements, match expressions, and loop constructs like for, while, and loop. But that’s not all—Rust’s functions play a crucial role in keeping our code modular and reusable.In this guide, you’ll learn:
✅ How to use if and match for decision-making
✅ When to use for, while, and loop in Rust
✅ How to define and call functions with parameters and return values
✅ The power of generics, closures, and associated functionsMastering these concepts will give you a solid foundation for writing efficient and idiomatic Rust code.]]></content:encoded></item><item><title>Experience WebForms Core Technology in Rust; File Deletion Example</title><link>https://dev.to/elanatframework/experience-webforms-core-technology-in-rust-file-deletion-example-14an</link><author>Elanat Framework</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 23:10:19 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[For Rust developers, we have good news: From now on, you can also experience WebForms Core technology in Rust. At Elanat, we have updated the WebForms class of the Rust programming language to the latest version of the WebFormsJS library, version 1.6.WebForms Core is an automated and advanced technology for simple management of HTML tags on the server. In this technology, data is sent to the server completely faithfully to HTML (and is done via WebFormsJS), but the state of the page remains static. The server also sends INI data via the WebForms class to the WebFormsJS library on the client. WebForms Core technology allows you to manipulate tags offline in serverless scenarios. To use this technology, you need to add the WebFormsJS library and the WebForms class associated with your desired programming language to your system.You can download the WebForms class for the Rust programming language from the link below.To download the latest version of WebFormsJS, visit the following link.
  
  
  Example: Using WebForms Core in Actix Web
The code below the page is the view and the lower code you see is the server code related to the Rust programming language created under the  framework. This is an example to display a list of image files and the ability to delete them via a web page. In this example, the user views the images on the web page and has the ability to delete them. In this example, when the user clicks on the red button, the image file is deleted from the server and subsequently, on the web page, the tag containing the image, the image name, and the delete button are deleted and a deletion message is shown to the user for 3 seconds.This file is an HTML template that displays a list of files and a form to delete them. It uses Tera templating engine for rendering dynamic content.{{ title }}WebForms Core Technology in Rust
            {% for file in files %}
            {{ file }}
            {% endfor %}
        In the code below, the "main" method is for setting the initial configuration. The "index" method is also created to call the "index.html" page and create a list of image files.
The "handle_post" method has been added to delete the image file and use WebForms Core technology.The GIF image below shows how the above code works.Since we want to use the WebForms core technology in the Rust programming language, we need to call this class as follows.If you look at the view page, you will notice that the data is sent via the POST method. The "handle_post" method is also called to respond to the data sent via the POST method.
In the "handle_post" method, the data value of the input tag, which is of the submit type, is first read. Here, there are two data values, one is the index and the other is the file name. These values ​​are separated and placed in an array called "parts".
Note: Sending multiple values ​​via HTML is a common practice. The submit tag is initialized as follows after rendering:Following the codes of the "handle_post" method, the file is physically deleted. A new instance of the WebForms class is created and using the WebForms class, an h3 tag is added to the beginning of the single form and its color is changed to orange. Then the text "File removed successfully." is placed inside the h3 tag and then the command to remove the h3 tag after 3 seconds is added. Finally, the command to remove the parent tag for the img tag is added via its id value.At the end of the method, the response is sent via the  method.What is sent from the client to the server?As we mentioned earlier, in WebForms Core technology, data is sent as if it were an HTML page form submission.What does the server respond to the client?The server response is also based on the INI pattern.[web-forms]
ut<form>=h3
bc<h3>=orange
st<h3>=File removed successfully.
:3)de<h3>=1
de/file_1=1
WebForms Core Technology in Rust opens up exciting possibilities for Rust developers by integrating an automated and advanced way to manage HTML tags on the server. Leveraging the latest version of the WebFormsJS library, version 1.6, WebForms Core offers a seamless and efficient way to manipulate tags offline in serverless scenarios.The provided example demonstrates a practical implementation using Actix Web to display and delete image files via a web page. The user-friendly interface, combined with the robust capabilities of WebForms Core, showcases the power and flexibility of this technology.By incorporating WebForms Core into your Rust projects, you can enhance your web development experience with automated tag management, making it easier to build dynamic and interactive web applications. Download the WebForms class for Rust and the WebFormsJS library to get started and explore the full potential of this innovative technology.]]></content:encoded></item><item><title>Understanding Data Interfaces: Simplifying Data Exchange with Stof</title><link>https://dev.to/amelia_wampler_e7aa93dab9/understanding-data-interfaces-simplifying-data-exchange-with-stof-8hm</link><author>Amelia Wampler</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 22:57:29 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[In today's complex digital landscape, seamless data exchange between systems is critical. Data interfaces serve as the backbone of this exchange, enabling applications, services, and devices to communicate effectively. In this post, we'll explore what a data interface is, why it’s important, how it ties into the innovative approach of Stof, and why you should join our community.
  
  
  What Is a Data Interface?
A data interface acts as a conduit for information between different systems, applications, or devices. Think of it as a communication layer that translates data formats and protocols so that each system can understand and use the information correctly. Traditionally, setting up these interfaces required custom APIs, SDKs, and parsers—each designed to handle the intricacies of data parsing, structuring, and transformation. This complexity often leads to increased development time and potential integration challenges.
  
  
  Why Is a Data Interface Important?
Data interfaces are crucial for several reasons:
Streamlined Communication: They ensure that disparate systems can exchange information smoothly without getting bogged down by incompatible data formats. By managing the translation of data formats centrally, data interfaces reduce the need for extensive, application-specific code.Enhanced Maintainability: A unified approach to data exchange leads to systems that are easier to scale, update, and maintain over time. Centralizing the logic in the data layer helps avoid common pitfalls associated with manual data handling, thus reducing errors.In essence, a robust data interface simplifies the architectural framework, enabling developers to focus on building innovative solutions rather than wrestling with data compatibility issues.
  
  
  How Does Stof Transform Data Interfaces?
Stof reimagines the concept of a data interface by consolidating its elements into a single, cohesive format. Here’s how Stof makes a difference: Stof allows developers to embed additional types, functions, and schemas directly within a single document. This enriched interface means that multiple applications can leverage the same structured data without needing extra layers of custom code. By shifting the heavy lifting into the data layer, Stof reduces the need for external parsers and APIs. This approach not only simplifies implementation but also enhances portability across different systems. With a unified format, Stof minimizes the challenges associated with traditional data interface integration. Developers can achieve more robust and scalable applications with fewer hurdles to overcome.By addressing the common challenges of traditional data interfaces, Stof paves the way for more efficient, maintainable, and innovative system architectures.Are you ready to revolutionize the way your applications handle data exchange? We invite you to join the Stof community and be a part of this exciting journey to enhance the way we work with data. Connect with us on GitHub to explore the code, contribute to discussions, and stay updated on the latest developments. Also, follow us on Linkedin and join our Discord to engage with a community that’s passionate about shaping the future of data interfaces.Embrace the power of simplified data exchange with Stof, and transform the way your systems communicate. Join us today and help drive innovation for our community!]]></content:encoded></item><item><title>Finally, a guide to Atomic::Ordering that won’t make my brain segfault. If you’ve ever stared at Rust’s memory model like it personally wronged you, this one&apos;s for you</title><link>https://dev.to/aryan_getsrusty/finally-a-guide-to-atomicordering-that-wont-make-my-brain-segfault-if-youve-ever-stared-at-4ojd</link><author>Aryan Anand</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 19:09:33 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[The Missing Guide to Rust's Atomic::Ordering]]></content:encoded></item><item><title>The Missing Guide to Rust&apos;s Atomic::Ordering</title><link>https://dev.to/aryan_getsrusty/the-missing-guide-to-rusts-atomicordering-1g9p</link><author>Aryan Anand</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 19:08:03 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[When working with concurrent programming in Rust, atomic operations provide a powerful way to manage shared state safely. However, one aspect that often confuses developers—especially those new to low-level concurrency—is . The  variants in Rust's  module control how operations on atomics are perceived across threads, ensuring correctness while balancing performance.In this article, we'll break down what  really means, why it matters, and how to choose the right ordering for your use case. We'll be implementing a Mutex from scratch and build up to different  (, , , , and ) orderings, examine their trade-offs, and use practical examples with real world analogies to understand the concept The word atomic comes from the Greek word , meaning indivisible, something that cannot be cut into smaller pieces. In computer science, it is used to describe an operation that is indivisible: it is either fully completed, or it didn’t happen yet. in Rust are used to perform small operations (add, substract, compare, etc) on a shared memory. Unlike a normal  statement which has to go through the Fetch, Decode, Execute, WriteBack cycle, atomics on the other hand gets executed in a single  CPU Cycle. Hence preventing a  condition among threads. This makes it perfect for implementing For simplicity's sake, just ignore the  and let's use  cause we're all  in general. CPU and Modern Compilers often re-order the instructions to improve performance and CPU utilisation, However, this is'nt very useful when multiple independent entities  .This could cause  and , stalling the threads for a long time.Now our poorly implemented Mutex falls prey to this, the instructions could get shuffled and all our  implementation goes to waste. Since it could reorder into something like:Which would interefere with another thread who currently has the lock.  something likeThis could lead to the mutex locking its own access to the data and being in the state of Deadlock forever, leading to freezing the program.
  
  
  How does  save us from this ?
It gives special instructions to the compiler, i.e when should it reorder and when it should'nt.Ordering::Acquire/Release(together) – Ensures Seeing Previous Writes and Future Reads
Ensures that all writes done before another thread released the data are visible. Prevents previous reads/writes from moving after the acquire operation. : Imagine  is preparing a , and  is the delivery person.
Now as compiler gets the instructions:- T1 (cook) sets  and then flips  to true.T2 (delivery) waits until  and only then picks up .No chance of reading old data!
  
  
  Example 2 : Think of  as a warehouse preparing a package, and  as a delivery worker picking it up.
T1 (warehouse) packs the order (), then sets  to true.T2 (delivery) will NOT pick up  until  is fully written.T2 always gets the correct value.Key Point:  ensures that all previous writes (like )  are visible before setting . - Ensures previous writes are seen

  
  
  Example : Imagine  is the Chef cooking and  is the Waiter
The waiter () checks if the dish is  ().Once the ticket (flag) is marked "Ready," the waiter  the dish is complete.But , they might do unrelated tasks in any order.
 ensures that once the waiter sees , they will also see the completed dish () and pick it up.However,  tasks(like setting up plates) i.e previous instructions might have happened before checking.Using  ordering for a store ensures all prior changes are visible after the store. A  will see the stored value and enforce order for subsequent operations. However, in load-store operations, the store part becomes "Relaxed," losing strong ordering guarantees.ENSURES WE SEE ALL MEMORY CHANGES MADE BY THE PREVIOUS LOCK OWNERS – Ensures future accesses see the change made

  
  
  Example : Imagine  is the Chef cooking and  is the Waiter
Before the waiter picks the dish...The chef ()  finish preparing the dish  marking the order as ready ().Or else, customers could receive a half-cooked meal. makes sure everything before it happens first (the dish is ready before the flag is flipped).An Acquire load ensures all operations before a prior store are visible, preventing outdated or inconsistent data. It acts as a barrier, enforcing memory consistency across threads.ENSURES THAT FUTURE READS WILL SEE THIS UPDATED VALUE – Ensures a globally consistent order of operations
  
  
  Example: Imagine  is the Customer A and  is the Customer B
Before transferring money...Customer A () must  from their account  depositing it into Customer B's account.Customer B () will  once Customer A's withdrawal is complete.Both actions must happen in a globally consistent order, ensuring that no thread (i.e., no customer) will observe the operations out of order, even if both threads are executed on different processors. ensures that all threads observe the operations in a globally consistent order. This means  is seen . No other thread will see the operations in a different order, thus preventing race conditions and ensuring the bank's accounting is correct. – Atomic Counters (No Synchronization Guaranteed)
Make modification to the shared variable without reading it.We  care when a thread sees the updated count.As long as the , we’re good.Each update would enforce , slowing down performance.
## Using everything we know now to fix our Mutex implementationWhen unlocking and locking a mutex: When a mutex is unlocked, a happens-before relationship is created between the unlock operation and the next lock operation on the same mutex. This ensures that:The next thread that locks the mutex will see all of the changes that were made by the thread that unlocked the mutex.Ensures this read sees  before a  store. store is done, then it may/may not see based on where the CPU has re-ordered the instructionEnsures all previous  writes are visible before unlocking. for the same reason that it may be reordered ahead of the write.]]></content:encoded></item><item><title>Latency based container scaling with Orbit</title><link>https://dev.to/airpipe/latency-based-container-scaling-with-orbit-1c3i</link><author>Kav Pather</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 15:01:53 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[In our previous article, we introduced Orbit, our lightweight container orchestrator built in Rust. Since then, we've made significant improvements driven by both community feedback and production requirements. Let's dive into the technical evolution that's making Orbit even more powerful and efficient.
  
  
  Community-Driven Development
One of the most exciting aspects of Orbit's development has been the community engagement. A perfect example is our implementation of CoDel (Controlled Delay) for scaling decisions, which came directly from a community member's suggestion on Medium. We're also grateful to community members like Josselin Chevalay who contributed the  feature in our latest release, allowing control over container image pulling behavior. This collaborative approach will continue to help shape Orbit's feature set and technical direction.
  
  
  Technical Evolution: Key Improvements

  
  
  1. CoDel (Controlled Delay) - Inspired Scaling: Latency-Driven Container Orchestration
Unlike traditional orchestrators that rely solely on CPU and memory metrics, we've implemented CoDel-based/inspired scaling - a feature not natively available in Kubernetes or other major orchestrators. Here's how it works:The CoDel inspired implementation monitors request latency and makes intelligent scaling decisions based on both immediate and historical performance data. Benefits include:More responsive scaling based on actual service performanceBetter handling of latency spikesPrevention of unnecessary scale-ups during temporary load increases that this is just our initial implementation, and we will continue to improve where possible and perhaps rename when appropriate.Key Differences from Traditional CoDel:Service-Level Application :Our implementation applies CoDel principles at the service level rather than packet levelUses request latency instead of packet sojourn timeFocuses on scaling rather than packet droppingThis is simpler than traditional CoDel's state machine.
We've added the usual health monitoring with TCP health checks:Configurable health check parametersTCP-level connectivity verificationGranular control over failure thresholdsSeparate startup and liveness checks
  
  
  3. Performance Optimizations
We've made several low-level optimizations to improve performance:
  
  
  Switching to FxHashMap/FxHashSet
By replacing standard HashMap with FxHashMap:Better performance for string keysLower collision rates in our specific use cases
  
  
  4. Improved Resource Management
We've implemented a more sophisticated resource management system:Fine-grained control over resource utilizationBetter handling of CPU quota managementMore accurate memory trackingCustomizable metrics aggregation strategiesThese improvements have had significant real-world impact:30% reduction in unnecessary scaling operationsMore stable performance under varying load conditionsReduced resource usage in the orchestrator itselfBetter handling of microservices with varying performance characteristicsStill managed to retain a <5MB binary size footprint
  
  
  What's Next: Decentralized Clustering!?
We're excited to explore our next major development focus: a decentralized clustering solution. This will allow Orbit to:Operate without a central control planeProvide better resilience in edge deploymentsEnable peer-to-peer node coordinationSupport dynamic cluster topology changesWe have some initial ideas on how to design the solution, so please follow for our next update to see how we hope to make this happen!
  
  
  Building at Scale with Air Pipe
While Orbit handles container orchestration, it's just one piece of the puzzle. At Air Pipe, we're building a comprehensive platform for creating scalable, resilient APIs, integrations, and workflows. Our platform enables you to:Build and deploy scalable APIs with minimal boilerplateCreate robust integration workflowsImplement resilient data processing pipelinesLeverage edge computing capabilitiesIf you're building distributed systems or scalable applications, visit airpipe.io to learn how our platform can accelerate your development.We're building Orbit in the open and value community input. Whether you're interested in the technical details or want to contribute to our upcoming clustering features:Stay tuned for our next technical deep-dive where we'll explore the architecture of our decentralized clustering approach!]]></content:encoded></item><item><title>notl.ink - open source blazingly fast url shortener ever. Live on ProductHunt.</title><link>https://dev.to/abdibrokhim/notlink-open-source-blazingly-fast-url-shortener-ever-live-on-producthunt-41mk</link><author>Ibrohim Abdivokhidov</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 13:18:36 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Launching on ProductHunt February 18th, 2025 12:01 AM PST. ]]></content:encoded></item><item><title>Build a software career with meaning: a playbook</title><link>https://dev.to/jonesbeach/build-a-software-career-with-meaning-a-playbook-d3b</link><author>Jones Beach</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 12:30:00 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Even with 20 fingers and toes, I barely have enough digits to count how many times during my 9-5 career I thought, “Wow, I feel frustrated/stuck/alone/hungry, maybe I should look for a new job.” A new job holds the allure of allowing you to let go of all your negative emotions (especially hunger) and start over.There are plenty of reasons to switch jobs, particularly when advocating for fairer compensation or a less-toxic environment. But if you're like me and looking to build a meaningful software career, where you can use your brain to its fullest and maybe do a bit of good, job hopping is more like a salve on a wound than a complete fix.With this in mind, I’m excited to announce my new email course: Build a Software Career with Meaning. While under development, I called this “How I went from ‘Hello World!’ to ‘How can I help?’ in just 19 years,” which is clearly a cheeky title but captures the work-in-progress feel of my career.Did I mention it’s free? Over the course of 5 (business) days, you’ll receive a brief email with a piece of wisdom, along with an actionable thought experiment you can test on your career.I also want to be transparent: in email marketing speak, this is called a “lead magnet.” I learned this term from reading (does YouTube still exist?), so I can only assume this is referring to the element from the periodic table with the symbol  (you know, the one pronounced ‘led’). And here I thought lead wasn't magnetic!I'm offering the course for free because I want to share how I think about things. At the end of 5 (business) days, perhaps you’ll feel like you know a bit more about my values and want to work with me. If you don’t, that’s more than okay! You are welcome to stay on the list indefinitely, and the option to unsubscribe will always be at the bottom. There are no tricks here, just a marketing playbook I’ve been learning in between implementing my Markdown blog and nested functions in Python bytecode.If you’ve ever felt stuck or disillusioned in your software career, I hope this course gives you a new perspective. If nothing else, it’ll be five (business) days of me popping into your email client with career advice just a tad more nuanced than “Quit your job!”P.S. If you’re interested in reading the story about how my 9-5 career crashed-and-burned—and how I built something better from the wreckage—I have the complete account over on From Scratch dot org. I’d love to hear if any of my experiences and catatonic thought loops (the kind where you forget to eat) mirror your own!I mentor software engineers to navigate technical challenges and career growth in a supportive, sometimes silly environment. If you’re interested, you can explore my mentorship programs.In addition to mentoring, I also write about neurodivergence and self-employment. Less code and the same number of jokes.]]></content:encoded></item><item><title>Rust Basics: Syntax, Data Types, and Naming Conventions</title><link>https://dev.to/sajiram_a4704bc095/rust-basics-syntax-data-types-and-naming-conventions-4kpm</link><author>Sajiron</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 11:07:41 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust Atomic Operations Guide: High-Performance Lock-Free Programming Techniques [Tutorial]</title><link>https://dev.to/aaravjoshi/rust-atomic-operations-guide-high-performance-lock-free-programming-techniques-tutorial-2nf2</link><author>Aarav Joshi</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Mon, 17 Feb 2025 09:14:40 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! Atomic operations and lock-free programming in Rust represent a sophisticated approach to concurrent programming. These concepts form essential building blocks for developing high-performance concurrent systems that minimize synchronization overhead.The foundation of atomic operations in Rust centers on the std::sync::atomic module. This module provides atomic versions of primitive types that guarantee thread-safe operations without traditional locks. Let's explore the core atomic types:Memory ordering plays a crucial role in atomic operations. Rust provides several ordering levels that determine the synchronization guarantees between threads:I've implemented numerous lock-free data structures using atomics. A simple atomic counter demonstrates the basic principles:Compare-and-swap (CAS) operations form the foundation of many lock-free algorithms. Here's an implementation of a lock-free stack:Memory fences provide explicit synchronization points when needed. They ensure visibility of changes across threads:Atomic operations excel in scenarios requiring high performance and minimal contention. I've successfully used them in system-level programming, game engines, and high-frequency trading systems.A practical example of atomics in action is a multi-producer, single-consumer channel:Performance considerations play a vital role when working with atomics. While they avoid the overhead of traditional locks, incorrect usage can lead to contention and reduced performance. I recommend careful benchmarking and profiling to ensure optimal results.The ABA problem represents a common challenge in lock-free programming. It occurs when a value changes from A to B and back to A, potentially causing incorrect behavior. Here's a solution using generation counters:Testing atomic code requires specific strategies. I've developed techniques to verify correctness under concurrent access:The future of atomic operations in Rust continues to evolve. The language team actively works on improving the atomic API and adding new features. These improvements will further enhance Rust's position as a leading language for systems programming and concurrent applications.Remember that atomic operations provide powerful tools for concurrent programming, but they require careful consideration of memory ordering and synchronization requirements. Start with simpler synchronization mechanisms unless performance requirements specifically demand atomic operations. is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>The open source fastest URL shortener ever.</title><link>https://dev.to/abdibrokhim/the-open-source-fastest-url-shortener-ever-5af3</link><author>Ibrohim Abdivokhidov</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 16 Feb 2025 22:47:21 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[The open source fastest URL shortener ever.Built with awesome open source tools x.com/rustlang x.com/shuttle_dev x.com/neondatabase x.com/actix_rsx.com/ThePracticalDev x.com/aimlapi x.com/vercel]]></content:encoded></item><item><title>RustyNum Follow-Up: Fresh Insights and Ongoing Development</title><link>https://dev.to/igorsusmelj/rustynum-follow-up-fresh-insights-and-ongoing-development-18f9</link><author>IgorSusmelj</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 16 Feb 2025 20:38:17 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[As a follow-up to my previous introduction to RustyNum, I want to share a developer-focused update about what I’ve been working on these last few weeks. RustyNum, as you might recall, is my lightweight, Rust-powered alternative to NumPy published on GitHub under MIT license. It uses Rust’s portable SIMD features for faster numerical computations, while staying small (around ~300kB for the Python wheel). In this post, I’ll explore a few insights gained during development, point out where it really helps, and highlight recent additions to the documentation and tutorials.If you missed the initial announcement, RustyNum focuses on:High performance using Rust’s SIMDMemory safety in Rust, without GC overheadSmall distribution size (much smaller than NumPy wheels)NumPy-like interface to reduce friction for Python users
  
  
  Developer’s Perspective: What’s New?
1. Working with Matrix OperationsI’ve spent a good chunk of time ensuring matrix operations feel familiar. Being able to do something like matrix-vector or matrix-matrix multiplication with minimal code changes from NumPy was a primary goal. A highlight is the  function and the  operator, which both support these operations.It’s neat to see how close this is to NumPy’s workflow. Benchmarks suggest RustyNum can often handle these tasks at speeds comparable to, and sometimes faster than, NumPy on smaller or medium-sized datasets. For very large matrices, I’m still optimizing the approach.2. Speeding Up Common Analytics TasksThe Python overhead can sometimes offset the raw Rust speed, but in many cases, RustyNum still shows advantages.
  
  
  New Tutorials: Real-World Examples
One of the best ways to see RustyNum in action is through practical examples. I’ve added several new tutorials with real-world coding scenarios: – Focus on dot products, matrix-vector, and matrix-matrix tasks.Replacing Core NumPy Calls – Demonstrates how to switch from NumPy’s mean, min, dot to RustyNum.Streamlining ML Preprocessing – Explores scaling, normalization, and feature engineering for machine learning.Check out a snippet of scaling code from that guide:It’s a small snippet, but it shows how RustyNum can do row/column manipulations quite effectively. After scaling, you can still feed the data into your favorite machine learning frameworks. The overhead of converting RustyNum arrays back into NumPy or direct arrays is minimal compared to the cost of big model training steps.1. Large Matrix OptimizationsI’ve noticed that for very large matrices (like 10k×10k), RustyNum’s current code paths aren’t yet fully optimized compared to NumPy. This area remains an active project. RustyNum is still young, and I’m hoping to introduce further parallelization or block-based multiplication techniques for better large-scale performance.RustyNum supports float32 and float64 well, plus some integer types. I’m considering adding stronger integer support for data science tasks like certain indexing or small transformations. Meanwhile, advanced data types (e.g., complex numbers) might appear further down the line if the community needs them.3. Documentation and API EnhancementsThe docs site at rustynum.com has an API reference and a roadmap. I’m continuously adding to it. If you spot anything missing or if you have a specific use case in mind, feel free to open a GitHub issue or submit a pull request.4. The big goal of RustynumRustyNum is simply a learning exercise for me to combine Rust and Python. Since I spend every day around machine learning I would love to have RustyNum replace part of my daily Numpy routines. And we're slowly getting there. I started adding more and more methods around the topic of how to integrate RustyNum in ML pipelines.
  
  
  Quick Code Example: ML Integration
To demonstrate how RustyNum fits into a data pipeline, here’s a condensed example:This script highlights that RustyNum can handle data transformations with a Pythonic feel, after which you can pass the arrays into other libraries.It’s been fun to expand RustyNum’s features and see how well Rust can integrate with Python for high-performance tasks. The recent tutorials are a window into how RustyNum might replace parts of NumPy in data science or ML tasks, especially when smaller array sizes or mid-range tasks are involved.Check out the tutorials at rustynum.comContribute or report issues on GitHubShare feedback if there’s a feature you’d love to seeThanks for tuning in to this developer-focused update, and I look forward to hearing how RustyNum helps you in your own projects!]]></content:encoded></item><item><title>Pulumi Gestalt devlog #8</title><link>https://dev.to/andrzejressel/pulumi-gestalt-devlog-8-1d1m</link><author>​Andrzej Ressel</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 16 Feb 2025 19:30:16 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Welcome to the eighth devlog for . This week, the focus was on preparing native Rust support and moving toward an initial release.Previously, Rust support in Pulumi Gestalt was essentially Wasm/Rust support, which required a complex setup and runtime environment. This week, I implemented proper native Rust support, which simplifies the process significantly. Now you can get started without external runners - only  and  required.After several weeks of development and refinement, I believe Pulumi Gestalt has reached a state where it’s ready for an initial release. Over the next few days, I’ll be focusing on finalizing documentation and ensuring consistent naming conventions across the SDKs to provide a smoother experience for users.That’s all for this week’s updates! As always, I welcome your feedback. If you have any thoughts, suggestions, or run into issues, feel free to share them on either the main repository or the example repository.]]></content:encoded></item></channel></rss>