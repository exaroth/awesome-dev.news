<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rust</title><link>https://www.awesome-dev.news</link><description></description><item><title>Async Rust explained without Tokio or Smol</title><link>https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD</link><author>/u/Gisleburt</author><category>rust</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 14:00:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hard Rust requirements from May onward (for Debian&apos;s package manager, APT)</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/DeleeciousCheeps</author><category>rust</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:24:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Engineering a Rust optimization quiz</title><link>https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 1 Nov 2025 11:08:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.]]></content:encoded></item><item><title>Borrow checker says “No”! An error that scares me every single time!</title><link>https://polymonster.co.uk/blog/borow-checker-says-no</link><author>/u/__shufb</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 22:25:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[It’s Halloween and I have just been caught out by a spooky borrow checker error that caught me by surprise. It feels as though it is the single most time consuming issue to fix and always seems to catch me unaware. The issue in particular is “cannot borrow x immutably as it is already borrowed mutably” - it manifests itself in different ways under different circumstances, but I find myself hitting it often when refactoring. It happened again recently so I did some investigating and thought I would discuss it in more detail.The issue last hit me when I was refactoring some code in my graphics engine hotline, I have been creating some content on YouTube and, after a little bit of a slog to fix the issue, I recorded a video of me going through the scenario of how it occurred and some patterns to use that I have adopted in the past to get around it. You can check out the video if you are that way inclined, the rest of this post will mostly echo what is in the video, but it might be a bit easier to follow code snippets and description in text.I have a generic graphics API, which consists of traits called gfx. This is there to allow different platform backends to implement the trait; currently I have a fully implemented Direct3D12 backend and I recently began to port macOS using Metal.The gfx backend wraps underlying graphics API primitives; in this case we are mostly concerned about  which is a command buffer. Command buffers are used to submit commands to the GPU. They do things like  or , amongst other things. For the purposes of this blog post, what the command buffer does is not really that important, just that is does , which at the starting point when the code was working is a trait method that takes an immutable self and another immutable parameter ie. fn do_something(&self, param: &Param).In the rest of the code base I have a higher level rendering system called . This is graphics engine code that is not platform specific but implements shared functionality. So where  is a low level abstraction layer,  implements concepts of a  that is a view of a scene that we can render from. A  has a camera that can look at the scene and is then passed to a render function, which can build a command buffer to render the scene from that camera’s perspective. The engine is designed to be multithreaded and render functions are dispatched through  systems, so a view gets passed into a render system but it is wrapped in an .I made a small cutdown example of this code to be able to demonstrate the problem I encounter, so let’s start with the initial working version:I tried to simplify it as much as possible so these snippets should compile if you copy and paste them, they won’t run thanks to  macro (which I absolutely love using, it is so handy!) but we only care about the borrow checker anyway.All we really need to think about is that a  can  and it also gets passed in a , which is also contained as part of ‘view’. Coming from a C/C++ background I landed on my personal preference being procedural C code with context passing, so I tend to group things together into a single struct. It makes sense to me in this case and I wanted to group everything inside , and we fetch the view from elsewhere in the engine.So the code in the snippet compiles fine and I was working with this setup for some time. I began work on macOS and it turned out that the  method needed to mutate the command buffer so that I could mutate some internal state and make the Metal graphics API behave similarly to Direct3D12. This is common for graphics API plumbing.The specific example in this case was that in Direct3D we call a function  to bind an index buffer before we make a call to , but in Metal there is no equivalent to bind an index buffer. Instead you pass a pointer to your index buffer when calling the equivalent draw indexed. So to fix this, when we call  we can store some extra state in the command buffer so we can pass it in the later call to .In hindsight any method on the command buffer trait that does anything, like set anything or write into the command buffer, should take a  because it is mutating the command buffer after all. In my case since I am calling through to methods on  , which is unsafe code and does not require any mutable references.In our simplified example, in order to store, state  now needs to change and take a mutable self: do_something(&mut self, param: &Param) it should be noted that  itself was already .Borrow checker now kicks in…my heart sinks. In the real code base not only did I have to modify a single call site, but I had hundreds of places where this error was happening, I made the decision here and now to make any methods that write to the command buffer also be mutable and make the mutabilityerror[E0502]: cannot borrow `view` as immutable because it is also borrowed as mutable
  --> src/main.rs:30:28
   |
30 |     view.cmd.do_something(&view.param);
   |     ----     ------------  ^^^^ immutable borrow occurs here
   |     |        |
   |     |        mutable borrow later used by call
   |     mutable borrow occurs here

For more information about this error, try `rustc --explain E0502`.
error: could not compile due to 1 previous error
This is not the first time I have encountered this problem and I doubt it will be the last. There are a number of ways to resolve it and they aren’t too complicated. The frustrating thing is that it seems to occur always when you are doing something else and not just when you decide to refactor, so you end up having a mountain of errors to solve before you can get back to the original task. I suppose you could call it a symptom of bad design or lack of experience, but when writing code things inevitably change and bend with new requirements, and Rust throws these unexpected issues up for me more often than I find with C, and often the required refactor takes more effort as well. But that is the cost you pay, hopefully more upfront effort to get past the borrow checker means fewer nasty debugging stages later. So let’s look at some patterns to fix the issue!The one I actually went for in this case was using . We take the  out of view so we no longer need to borrow a ‘view’ to use , and then when finished return the cmd into ‘view’. It is important to note here that  needs to derive default in order for this to work, as when we take the  in  will become This approach is the simplest I could think of at the time because any existing code using  doesn’t need updating, everything stays the same and we just separate the references. In this case it was easy to derive the default for  .You need to remember to set the  back on  here, which could be a pitfall and cause unexpected behaviour if you didn’t.If you can’t easily derive default on a struct there are some other options. If the struct is clonable or you can easily derive a clone, you can clone to achieve a similar effect.Cloning might be considered a heavier operation than ‘take’ depending on the circumstances, but this method has the same benefit as the take version whereby unaffected code that is using  elsewhere doesn’t need to be changed.Another approach would be to use  this allows for interior mutability and again we do not need to worry about default or clone.We also need to update any code that ever used  and do the same. Not ideal but it allows us to get around the need for a default or clone. I have had to resort to this in other places in the code base.There are more options; quite literally  here can help. If we make  an  then this gives us the ability to use  as the default and we can use the  approach. We can also use  and swap with . Swapping works similar to ‘take’, where we take mem and swap with the default.The  approach also requires more effort as we need to now take a reference and unwrap the option and update any code that ever used  to do the same. Not ideal, but it allows us to get around the need for a default or clone, and if your type is already optional then this will fit easily.There is one final approach that could save a lot of time, and that would be to not change the  function at all in the first place. That is to keep it as do_something(&self, param: &Param). So how do we mutate the interior state without requiring the self to be mutable?This can be done with  in single threaded code or  in multithreaded code. Since we already looked at  I will do an example of .I decided to make the mutability explicit to the trait and that was based on how the command buffers are used in the engine, in other places I have taken other approaches favouring interior mutability. For this case a view can be dispatched in parallel with other views, but the engine is designed such that 1 thread per view and no work happens to a single view on multiple threads at the same time. Command buffers are submitted in a queue in order and dispatched on the GPU.Here it made sense to me to avoid locking interior mutability for each time we call a method on a  and it works with the engine’s design. We lock a view at the start of a render thread, fill it with commands and then hand it back to the graphics engineer for submission to the GPU. The usage is explicit, we just needed to appease the borrow checker!I hope you enjoyed this article, please check out my YouTube channel for more videos or more articles on my blog, let me know what you think and if you have any other strategies or approaches I would love to hear about them. I would also like to hear about compiler and borrow checker errors you find particularly time consuming or frustrating to deal with.]]></content:encoded></item><item><title>Futurelock - Subtle Risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>/u/-Y0-</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:20:15 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Project goals for 2025H2 | Rust Blog</title><link>https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/</link><author>/u/Kobzol</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 16:23:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[On Sep 9, we merged RFC 3849, declaring our goals for the "second half" of 2025H2 -- well, the last 3 months, at least, since "yours truly" ran a bit behind getting the goals program organized.In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .Unblocking dormant traits, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.Flexible, fast(er) compilation, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers., making higher-level usage patterns in Rust easier.One of Rust's core value propositions is that it's a "library-based language"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a "second-class citizen" problem where custom pointer types can't provide the same ergonomic experience as built-in references.The "Beyond the " initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.
"Unblocking dormant traits"Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:Polonius will enable new borrowing patterns, and in particular unblock "lending iterators". Over the last few goal periods, we have identified an "alpha" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.The next-generation trait solver is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.The work on evolving trait hierarchies will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.The work to expand Rust's  hierarchy will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on RFC #3729 and RFC #3838, authored in previous Project goal periods.In-place initialization allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.The "Flexible, fast(er) compilation" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often "surprisingly tolerable" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:We aim to stabilize cargo script, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.We aim to finalize the design of ergonomic ref-counting and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the Rust program manager hired by the Leadership Council, will join me in running the program.]]></content:encoded></item><item><title>Release Dioxus v0.7.0 · DioxusLabs/dioxus</title><link>https://github.com/DioxusLabs/dioxus/releases/tag/v0.7.0</link><author>/u/DebuggingPanda</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 13:10:05 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My first day in Rust</title><link>https://www.reddit.com/r/rust/comments/1oki355/my_first_day_in_rust/</link><author>/u/Zealousideal_Sort521</author><category>rust</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 01:43:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I am a programmer with 15 years of experience in C# and the full Microsoft stack. I dream in LINQ and Entity Framework Core. Today was my first deep dive into Rust and I loved it. My observations: * Rust is very precise and type safe. Way more precise than C#. No dynamics ever in Rust * The compiler is actually helpful. * I was under the impression that I was actually using my IQ points while programming again. Which was a pleasant surprise. Rust is the ultimate counterspell to vibe coding. * Setting up swagger was more difficult than it. Needed to be. * Rust code rots faster than C# code. Many examples on GitHub are unusable. * I wasn’t really a fan of the idea of being forced into nightly compiler builds to use the rocket framework. ]]></content:encoded></item><item><title>With the release of Rust 1.91, Arm is now a Tier 1 supported architecture on Windows</title><link>https://github.com/rust-lang/rust/pull/145682</link><author>/u/Balance-</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:44:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[ is now a Tier 1 target with host tools for Rust, meaning ARM64 Windows with MSVC is "guaranteed to work" as a fully supported platform. This means the Rust project provides official binary releases, runs automated testing after every change to ensure builds and tests pass, and supports running development tools like  and  natively on ARM64 Windows machines. In practical terms, developers can now confidently use ARM64 Windows devices (like Windows on ARM laptops) both as compilation targets and as development platforms with the same level of support as established platforms like x86_64 Windows and ARM64 macOS.]]></content:encoded></item><item><title>Rust 1.90.1 is out</title><link>https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/</link><author>/u/manpacket</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 18:39:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.91.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!The Rust compiler supports a wide variety of targets, but
the Rust Team can't provide the same level of support for all of them. To
clearly mark how supported each target is, we use a tiering system:Tier 3 targets are technically supported by the compiler, but we don't check
whether their code build or passes the tests, and we don't provide any
prebuilt binaries as part of our releases.Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but
we don't execute the test suite on those platforms: the produced binaries
might not work or might have bugs.Tier 1 targets provide the highest support guarantee, and we run the full
suite on those platforms for every change merged in the compiler. Prebuilt
binaries are also available.Rust 1.91.0 promotes the  target to Tier 1 support,
bringing our highest guarantees to users of 64-bit ARM systems running Windows.
Add lint against dangling raw pointers from local variablesWhile Rust's borrow checking prevents dangling references from being returned, it doesn't
track raw pointers. With this release, we are adding a warn-by-default lint on raw
pointers to local variables being returned from functions. For example, code like this:Note that the code above is not unsafe, as it itself doesn't perform any dangerous
operations. Only dereferencing the raw pointer after the function returns would be
unsafe. We expect future releases of Rust to add more functionality helping authors
to safely interact with raw pointers, and with unsafe code more generally.These previously stable APIs are now stable in const contexts:Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Rust in Production Podcast: How Cloudflare handles 90 million requests per second with Pingora</title><link>https://corrode.dev/podcast/s05e03-cloudflare/</link><author>/u/mre__</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 16:26:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[How do you build a system that handles 90 million requests per second? That’s the scale that Cloudflare operates at, processing roughly 25% of all internet traffic through their global network of 330+ edge locations.In this episode, we talk to Kevin Guthrie and Edward Wang from Cloudflare about Pingora, their open-source Rust-based proxy that replaced nginx across their entire infrastructure. We’ll find out why they chose Rust for mission-critical systems handling such massive scale, the technical challenges of replacing battle-tested infrastructure, and the lessons learned from “oxidizing” one of the internet’s largest networks.
    CodeCrafters helps you become proficient in Rust by building real-world,
    production-grade projects. Learn hands-on by creating your own shell, HTTP
    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.
  
    Start for free today and enjoy 40% off any paid plan by using
    this link.
  Cloudflare is a global network designed to make everything you connect to the Internet secure, private, fast, and reliable. Their network spans 330+ cities worldwide and handles approximately 25% of all internet traffic. Cloudflare provides a range of services including DDoS protection, CDN, DNS, and serverless computing—all built on infrastructure that processes billions of requests every day.Kevin Guthrie is a Software Architect and Principal Distributed Systems Engineer at Cloudflare working on Pingora and the production services built upon it. He specializes in performance optimization at scale. Kevin has deep expertise in building high-performance systems and has contributed to open-source projects that power critical internet infrastructure.Edward Wang is a Systems Engineer at Cloudflare who has been instrumental in developing Pingora, Cloudflare’s Rust-based HTTP proxy framework. He co-authored the announcement of Pingora’s open source release. Edward’s work focuses on performance optimization, security, and building developer-friendly APIs for network programming.]]></content:encoded></item><item><title>Inside Rust&apos;s std and parking_lot mutexes - who wins?</title><link>https://blog.cuongle.dev/p/inside-rusts-std-and-parking-lot-mutexes-who-win</link><author>/u/lllkong</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 15:32:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I had no idea how to evaluate this claim. A quick search online returned results favoring parking_lot. This felt wrong to me. Why? It contradicted my belief that std should be the gold standard. The standard library team knows what they’re doing, right? And if parking_lot’s mutex really was the performance winner, there had to be trade-offs between the two implementations that people weren’t talking about.That mystery haunted me. I couldn’t just take it on faith. So I jumped down the rabbit hole: read both implementations, wrote the benchmarks, and here we are. In this post, I will:Explain how std implements the mutex (v1.90.0)Explain how parking_lot implements their mutex (v0.12.5)Show you the benchmark with key findingsGive you a decision guide for when to use eachBut first, let’s ground our foundation on mutexes (skim it if you’re already familiar).A classic example of the kind of problem that mutex solves is withdrawing and receiving money at the same time. Imagine you have $100 in your account. Thread A tries to withdraw $80, and Thread B tries to deposit $50. Without proper synchronization, both threads might read the balance as $100 simultaneously, then write back their results independently:Mutex solves this nicely by having a thread wait until the other finishes its update:Simple enough, right? Now let’s see how to use a mutex. (Again, skim this if it’s too basic for you)In languages other than Rust, you typically declare a mutex separately from your data, then manually lock it before entering the critical section and unlock it afterward. Here’s how it looks in C++:Rust takes a completely different approach - the mutex wraps and owns the data:Three things to pay close attention to:That’s enough of the basics. Let’s have some fun. Here is how mutex is implemented, starting with Rust std.A quick look into std::Mutex gives us thisThe main idea is that for different OS (and OS version), Rust uses different Mutex implementation. However, we can divide these implementation to 2 big groups: Futex and other platform primitive.Futex (short for “fast userspace mutex”) is used where the OS kernels expose a “wait on this address” API. We will dive deeper into this one soon.When that API is missing, Rust falls back to the best available platform traditional locks.(I’m in awe btw - that’s a lot of different implementations. Writing and maintaining all this platform-specific stuff must be exhausting. Major respect to whoever’s doing this.)Since Futex is the most used and is quite a typical implementation for Mutex. Let’s look inside it.At its heart, futex is just an atomic u32 (simplified here):In other words, if we use value 0 for Unlocked state, and 1 for Locked state, we have a simple mutex where the thread can simply try to compare the state to 0 (Unlocked), and set to 1 (Locked). If the state is currently 1, keep doing that until successful.So, a simplified version of mutex look like this:But you might ask: if the first thread holds the lock for a long time, then the second thread needs to keep trying (like a infinite loop)? How about if there are hundreds or thousands of them? Maybe the CPU will soon be burnt.Of course, there is the solution to this problem. In real implementation, Rust futex has 3 states:2: Contended - locked, but there are waiter.Notice the Contended state? A thread will try its best to acquire the lock. But if it can’t, it will mark the lock as contended and go to sleep, waiting for the process to wake it up when the mutex is released.What happens when a thread goes to sleep? The kernel helps us put these sleeping threads into a queue. Take a look at the system call on Linux and Android to put the thread into sleeping state (this is usually called “park a thread”):futex as *const Atomic<u32>When a thread finishes, it sets the state to unlocked. If the state was contended, it wakes one waiting thread via syscall. This continues until the queue empties.The final piece of std’s mutex is poisoning, a unique feature you won’t find in most other languages.The guard captures whether the thread was panicking when the lock was acquired. If we weren’t panicking then but we are now, a panic must have occurred in the critical section. The mutex is marked as poisoned with a simple atomic store.This is a “best effort” mechanism. It won’t catch all cases (like double panics or non-Rust exceptions), but it provides a useful safety net. The key insight is that you still get access to the data even if the mutex is poisoned, allowing you to inspect and potentially recover from the corrupted state.issue#134645parking_lot takes a fundamentally different approach. Two key differences:std uses different mutex implementations per platform. parking_lot uses one algorithm everywhere, calling platform-specific code only for sleep/wake.std’s queues live in the kernel. parking_lot manages its own queues in user space via a global hash table.parking_lot’s mutex is remarkably small:Why can parking_lot use just one byte while std needs more? It comes down to how queues work.More states for queue bookkeepingUsing separate bits gives parking_lot four possible states:When a thread can’t acquire the lock, it needs somewhere to wait. This is where parking_lot’s global hash table comes in.Instead of each mutex maintaining its own queue (like kernel futexes do), parking_lot uses a single global hash table shared by all mutexes in your program. When a thread needs to wait:Hash the mutex’s memory address to find a bucket in the global tableAdd the thread to the bucket’s wait queueBeing able to manage the thread queue itself is important for parking_lot to enforce fairness. As you can see right away in the next section.Here’s where parking_lot differs from std in behavior. std’s futex uses a “barging” strategy where any active thread can grab the lock when it’s released, even if others have been waiting in the queue longer. This maximizes throughput but can cause starvation.When a thread unlocks, there are two sources of threads that can lock again:An active thread that is calling for lockingA sleeping thread in the queueAs you can see, the active thread will tend to win the fight of “who locks first”. So if a thread keeps calling for lock, finishes its work, then locks right away, it keeps all other threads starved.As you can see, thread A keeps grabbing the lock immediately after releasing it. Threads B and C do get woken up by the syscall, but by the time they try to acquire the lock, thread A has already grabbed it again. They’re completely starved.parking_lot implements “eventual fairness” to prevent this.Each bucket in the hash table has a timer that fires approximately every 0.5 milliseconds. When the timer fires, the next unlock becomes a “fair unlock”:The unlocker keeps the LOCKED_BIT setThe woken thread receives the lock directly (a “handoff”)That thread owns the lock immediately without racing with other active threadsSo this means, instead of letting anyone who is fast grab the lock, parking_lot forces the lock to be given directly to the next one in the queue (it keeps the LOCKED_BIT set and hands off; it doesn’t even unlock).This eventual fairness technique from parking_lot is pretty clever, isn’t it?You might wonder by now: how does parking_lot put threads to sleep without storing a 32-bit futex word inside every mutex? The answer is thread-local storage.The code looks almost identical to std’s futex path. The only difference? parking_lot points the syscall at the thread-local integer instead of the mutex:https://github.com/cuongleqq/mutex-benchesFor each scenario, you’ll see:Per-thread operation countsparking_lot tells a different story. Every thread completed 860-877 operations (1.9% variation). The fairness mechanism worked exactly as designed. Yes, parking_lot has 7.5% lower throughput and higher median wait time, but that’s because it’s ensuring all threads make progress. The 51x more stable wait times (3.67ms vs 188.73ms standard deviation) show the predictability benefit. When fairness matters, parking_lot prevents the pathological starvation that std exhibits.parking_lot’s fairness timer prevented this catastrophe. The hog still got more operations (9,168) but nowhere near monopolization. All other threads made meaningful progress (7,023-7,109 operations). The result: 261.6% higher overall throughput because all 6 threads contributed work instead of 5 threads sitting idle. The 120x more stable wait times (1.09ms vs 130.76ms) show parking_lot’s predictability. The 0.5ms fairness timer does exactly what it promises: prevent any thread from monopolizing the lock indefinitely.After diving deep into the implementations and running comprehensive benchmarks, here’s when to use each:You need zero dependenciesLow to moderate contention with short critical sectionsYou want poisoning for debuggingPlatform-specific optimizations matterRisk of monopolization existsYou need predictable behaviorYou want timeouts or fairness controlCross-platform consistency is importantstd::Mutex optimizes for throughput in the average caseparking_lot::Mutex optimizes for fairness and predictability in the worst caseFor most applications, where contention is light and critical sections are short, std::Mutex performs excellently. But if your application has any of these characteristics:Long-running critical sectionsRisk of lock monopolization (e.g., one high-priority thread)Need for predictable latency across all threadsRequirement that all threads make forward progressThen parking_lot::Mutex’s eventual fairness mechanism becomes invaluable. The 0.5ms fairness timer is a small price to pay for preventing complete thread starvation.XLinkedInsubstackmedium]]></content:encoded></item><item><title>Rust intern saved TikTok $300K</title><link>https://www.youtube.com/watch?v=1y_vBNbZmIA</link><author>Let&apos;s Get Rusty</author><category>dev</category><category>rust</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/1y_vBNbZmIA?version=3" length="" type=""/><pubDate>Thu, 30 Oct 2025 14:52:55 +0000</pubDate><source url="https://www.youtube.com/channel/UCSp-OaMpsO8K0KkOqyBl7_w">Let&apos;s get Rusty</source><content:encoded><![CDATA[Grab your Rust Job-Ready Roadmap (free): https://letsgetrusty.com/join
The fastest way to become a Rust dev (limited seats): https://letsgetrusty.com/join

An intern used Rust to save TikTok $300k! Yes, you heard that right. Rust’s performance and efficiency are unmatched, and this story proves it.

Original video: https://www.youtube.com/watch?v=sELryGUfzMg

Join the Rust Live Accelerator: https://letsgetrusty.com/join

Chapter:
0:00 Context
0:39 Problem
1:02 Why Rust
1:17 Rewrite Approaches
2:40 Final Solution
3:50 Results
4:24 Takeaways]]></content:encoded></item><item><title>esp-hal 1.0.0 release announcement</title><link>https://developer.espressif.com/blog/2025/10/esp-hal-1/</link><author>/u/XxMabezxX</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 12:22:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[In February this year, we announced the first  1.0 beta release. Since then we’ve been hard at work, polishing and preparing for the full release. Today, the Rust team at Espressif is excited to announce the official  release for , the  vendor-backed Rust SDK!What We’re Stabilizing TodayWe’ve spent many years researching and experimenting to get to this stage (check out the  1.0 beta blog post for the longer story!). However, to get a stable foundation to build from, the experimentation eventually needs to make way for stability. To achieve this, we’ve decided to limit the scope of 1.0 stabilization to:Initializing the HAL,  and the relevant configuration associated with that.Four “core” drivers to start: and  modes for the aforementioned drivers.Our  drivers are compatible with many executors, including Embassy’s.The  module, which provides , , and .A couple of miscellaneous system APIs (SoC reset, etc.).Additional configuration mechanisms beyond feature flags ().With the exception of the list above, everything else in  is now feature-gated behind the  feature. With the scope limited, post 1.0 we can incrementally stabilize drivers, much like the Rust project itself does, building on 1.0’s foundation.What Does Unstable Mean for Drivers?Unstable in this case refers to API stability. There is varying levels of functionality for unstable drivers, however, they are suitable for most common use cases. Using them, reporting feedback, and/or contributing to improving them will aid their stabilization.What About the Other  Crates? is the foundation of many of the ecosystem crates.  (previously known as ) is our next stabilization target, which will enable the use of Wi-Fi, Bluetooth, ESP-NOW and IEEE802.15.4 on the ESP32 family of devices. The end goal is of course to have every  crate with a 1.0+ release eventually.The first step is to read our specially curated book, which explains the ecosystem, tooling and some key embedded concepts for .As part of getting to 1.0, we’ve created our own project generation tool,  to bootstrap a project. This is explained fully in the book, but getting something running today should be as simple as:to launch the interactive project generation terminal user interface.Once you’ve generated your project, connect your ESP32 and run  from your new project directory!This is just the start. We plan on stabilizing all  related crates, next up is . We’ll continue developing ; over time we’ll stabilize more drivers beyond the core set that we’re starting with today. We’ll continue to add support for new devices, such as the newly released ESP32-C5, as they go into mass production.This release would not have been possible without the help from the Rust community, the embedded working group, and of course the ESP community and contributors which have heavily impacted how we’ve developed our Rust offering. I would also like to thank Espressif, and in particular the Rust team for their hard work in getting us to where we are today!If you’re a company using (or considering using) Rust on our devices, please do contact sales@espressif.com, we’d love to hear from you!]]></content:encoded></item><item><title>I’ve been living inside Rust for a while, and Flow-Like is what came out — a typed, local-first workflow engine</title><link>https://github.com/TM9657/flow-like</link><author>/u/tm9657</author><category>rust</category><category>reddit</category><pubDate>Thu, 30 Oct 2025 07:08:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I’ve been quietly building , a typed, visual workflow engine written in Rust. Think node-based “blueprints,” but with  — so flows are safer, easier to reason about, and automatically versioned. Everything runs : the desktop app, the backend, even AI and data nodes. There’s no account and no cloud dependency unless you explicitly add one.With  out, you can now actually build real automations — from  and  to , data transforms, or ML pipelines. And, of course, we’ve carefully hidden many bugs for you to find and report. ❤️Flow-Like is a desktop app (built with ) that lets you visually connect typed nodes into executable graphs. Each connection enforces its pin type, so most wiring errors show up before execution. Under the hood there’s a Rust engine that runs your graph directly — no web service, no remote orchestrator. Our backend code is also in our monorepo if that is more interesting to you.For external connectivity, there’s an  that can spin up a local  server, manage , connect to , handle webhooks, timers, file watchers, and more. You can also host it if you want — the backend code for that is included.Every project comes with its own file storage and database powered by the excellent  library — giving you full-text and vector search out of the box, with no setup required.Llama.cpp is embedded for local models and ONNX for local ML and Embeddings. Every flow and node definition is , so you can safely share or roll back changes. custom async executor that runs typed graphs directly. for event endpoints, HTTP handling, and integrations. and  for structured + vector data storage. for table operations and analytics. and  integration for local inference., cross-platform builds for macOS/Windows/Linux. already working (also thanks to Tauri)! The iOS build runs your flows LOCALLY on your phone — just needs a bit more polish before TestFlight.Build  with typed request/response handling.Run  that respond to messages and events.Create  (IMAP fetch, filter, SMTP send).Automate file pipelines, data transforms, or ML tasks.Use  inside flows for full-text and vector search.Stay completely offline — or opt into cloud APIs if you want.Everything happens locally, and everything is versioned — your data, flows, and nodes.If you like the idea (or just want to see how far Rust and Tauri can go), a quiet ⭐️ on GitHub would be very welcome.]]></content:encoded></item><item><title>Announcing Rust 1.91.0</title><link>https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.91.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!The Rust compiler supports a wide variety of targets, but
the Rust Team can't provide the same level of support for all of them. To
clearly mark how supported each target is, we use a tiering system:Tier 3 targets are technically supported by the compiler, but we don't check
whether their code build or passes the tests, and we don't provide any
prebuilt binaries as part of our releases.Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but
we don't execute the test suite on those platforms: the produced binaries
might not work or might have bugs.Tier 1 targets provide the highest support guarantee, and we run the full
suite on those platforms for every change merged in the compiler. Prebuilt
binaries are also available.Rust 1.91.0 promotes the  target to Tier 1 support,
bringing our highest guarantees to users of 64-bit ARM systems running Windows.
Add lint against dangling raw pointers from local variablesWhile Rust's borrow checking prevents dangling references from being returned, it doesn't
track raw pointers. With this release, we are adding a warn-by-default lint on raw
pointers to local variables being returned from functions. For example, code like this:Note that the code above is not unsafe, as it itself doesn't perform any dangerous
operations. Only dereferencing the raw pointer after the function returns would be
unsafe. We expect future releases of Rust to add more functionality helping authors
to safely interact with raw pointers, and with unsafe code more generally.These previously stable APIs are now stable in const contexts:Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Frustrated by lack of maintained crates</title><link>https://www.reddit.com/r/rust/comments/1ojb87w/frustrated_by_lack_of_maintained_crates/</link><author>/u/MasteredConduct</author><category>rust</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 17:42:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I love Rust. This isn't a criticism of Rust itself. This is plea for advice on how to sell Rust in production.One of the hardest things to do when selling Rust for a project, in my experience, has been finding well supported community library crates. Where other languages have corporate backed, well maintained libraries, more often than not I find that Rust either does not have a library to do what I want, or that library hasn't been touched for 3 years, or it's a single person side project with a handful of drive by contributors. For a personal project it's fine. When I go to my team and say, let's use Rust it has library to do X, they will rightly say well C++ has a library for X and it's been around for two decades, and is built and maintained by Google.A good concrete example has been containers. One option, shiplift, has been abandoned for 4 years. The other option, bollard, *is great*, but it's a hobby project mostly driven by one person. The conversation becomes, why use Rust when Golang has the libraries docker and podman are actually built on we could use directly.Another, less concerning issue is that a lot of the good libraries are simply FFI wrappers around a C library. Do you need to use ssh in go? It's in an official Google/Go Language Team library and written in Go. In Rust you can use a wrapper around libssh2 which is written in.... C. How do you convince someone that we're benefitting from the safety of Rust when Rust is just providing a facade and not the implementation. Note: I know russh exists, this is a general point, not specific to ssh. Do you use the library written in Rust, or the FFI wrapper around the well maintained C library. ]]></content:encoded></item><item><title>Rant: dealing with http::Uri is annoying as heck</title><link>https://www.reddit.com/r/rust/comments/1oj5bd7/rant_dealing_with_httpuri_is_annoying_as_heck/</link><author>/u/DebuggingPanda</author><category>rust</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 14:00:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I need to vent a bit, as I again ran into a situation where I am getting increasingly frustrated by dealing with an . I am building an HTTP server application, so the  crate is in my dependency tree and its  type is exposed in various places (e.g. hyper). Oftentimes, I need to inspect or manipulate URIs. For example, my application can be configured and many config values are URI-like. But: most have some limitations that I want to check for, e.g. "only http or https scheme + authority; no path, query or fragment". Doing these checks, or generally inspecting or manipulating this type is quite annoying though.And I hear you: "Just use the  crate!". I think this post should explain my concerns with it. Even ignoring the dependency problem or the fact that it would compile two separate URL parsers into my binary: when using , I have s everywhere, so converting them back and forth is just annoying, especially since there is no convenient way to do that!It is just plain frustrating. I have been in this situation countless times before! And every time I waste lots of time wrangling confusing APIs, writing lots of manual boilerplate code, having an existential breakdown, and considering whether to . I can only imagine the accumulated human life wasted due to this :(As a disclaimer I should say that all these issues are known to the maintainers and there are some valid arguments for why things are the way they are. I still think the current situation is just not acceptable for the whole ecosystem and it should be possible  to fix this.Thanks for coming to my TED talk. ]]></content:encoded></item><item><title>Built a Rust implementation of Andrej Karpathy&apos;s micrograd</title><link>https://www.reddit.com/r/rust/comments/1oj2tk4/built_a_rust_implementation_of_andrej_karpathys/</link><author>/u/Ryzen__master</author><category>rust</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 12:11:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Someone recently shared their implementation of Micrograd in Go, and in their blog they mentioned they had initially planned to do it in Rust. That gave me the idea to try it myself.I followed along with Andrej Karpathy’s video while coding, and it turned out to be a great learning experience — both fun and insightful. The result is micrograd-rs , a Rust implementation of Micrograd focused on clarity and alignment with the original Python version.A few months ago, I also built a small tensor library called Grad. Since the current micrograd-rs implementation only supports scalars, my next step is to integrate it with Grad for tensor support.I’d love feedback, suggestions, or contributions from anyone interested in Rust, autodiff, or ML frameworks.]]></content:encoded></item><item><title>[Media] You can now propose your cat as changelog cat for Clippy 1.91!</title><link>https://www.reddit.com/r/rust/comments/1oiyc7x/media_you_can_now_propose_your_cat_as_changelog/</link><author>/u/Alexey-Semenyuk</author><category>rust</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 07:43:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/Alexey-Semenyuk ]]></content:encoded></item><item><title>This Week in Rust 623</title><link>https://this-week-in-rust.org/blog/2025/10/29/this-week-in-rust-623/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 29 Oct 2025 04:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is tower-resilience, a library offering resilience features for tower.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.If you are a feature implementer and would like your RFC to appear in this list, add a
 label to your RFC along with a comment providing testing instructions and/or
guidance on which aspect(s) of the feature need testing.Let us know if you would like your feature to be tracked as a part of this list.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.If you are a Rust project owner and are looking for contributors, please submit tasks here or through a PR to TWiR or by reaching out on Bluesky or Mastodon!Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.If you are an event organizer hoping to expand the reach of your event, please submit a link to the website through a PR to TWiR or by reaching out on Bluesky or Mastodon!Mostly negative week, coming almost entirely from adding sizedness bounds in #142712. Other than that, we got a nice win for async code from state transform optimization in #147493 and quite a few smaller improvements from codegen optimization in #147890.Improvements ✅  (secondary)2 Regressions, 2 Improvements, 7 Mixed; 2 of them in rollups
42 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.Rusty Events between 2025-10-29 - 2025-11-26 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.Petition to add an  keyword in RustThanks to llogiq for the suggestion!]]></content:encoded></item><item><title>[Media] The 1.91 Clippy Changelog Cat Contest is open</title><link>https://www.reddit.com/r/rust/comments/1oira11/media_the_191_clippy_changelog_cat_contest_is_open/</link><author>/u/NothusID</author><category>rust</category><category>reddit</category><pubDate>Wed, 29 Oct 2025 01:08:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TIL: SymPy can generate Rust code</title><link>https://www.reddit.com/r/rust/comments/1oimibd/til_sympy_can_generate_rust_code/</link><author>/u/intersecting_cubes</author><category>rust</category><category>reddit</category><pubDate>Tue, 28 Oct 2025 21:46:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This is really helpful because a lot of people in math or data science in your company/project probably know and like Python. Now you can easily translate their Python into Rust. This has personally saved me a few hours of work already at my dayjob. I hope someone else here finds this useful.]]></content:encoded></item><item><title>Introducing Apache Fory™ Rust: A Versatile Serialization Framework with trait objects, shared refs and schema evolution support</title><link>https://fory.apache.org/blog/2025/10/29/fory_rust_versatile_serialization_framework</link><author>/u/Shawn-Yang25</author><category>rust</category><category>reddit</category><pubDate>Tue, 28 Oct 2025 17:55:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Burn 0.19.0 Release: Quantization, Distributed Training, and LLVM Backend</title><link>https://www.reddit.com/r/rust/comments/1oiexhr/burn_0190_release_quantization_distributed/</link><author>/u/ksyiros</author><category>rust</category><category>reddit</category><pubDate>Tue, 28 Oct 2025 17:00:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Our goals this year with Burn were to support large-scale training and quantized model deployment. This release marks a significant advancement in that direction. As a reminder, Burn is a Tensor Library and Deep Learning Framework for both training and inference.We had to rethink several core systems to achieve true multi-GPU parallelism: To support concurrent tasks running simultaneously on a single GPU (like compute and data transfer), we had to support multiple compute queues called streams. For a simple API to declare multiple streams, we simply attach compute streams to Rust threads using a pool.Redesigned Locking Strategies: We created a global device lock shared between multiple subsystems, like the fusion runtime, the CubeCL compute runtime, and autotuning. The new lock ensures that no deadlock is possible. The lock doesn't have a negative performance impact since locking is only used for task registration that ensures order of execution, compute is executed outside of the lock. The autodiff system doesn't share the same locking strategy, as a single graph can be executed on many GPUs. Therefore, we simply adopted a fine-grained locking strategy where different graphs can be executed in parallel.Distributed Training Infrastructure: We introduced burn-collective for gradient synchronization and refactored our training loop to support different distributed training strategies. The performance of some of our algorithms is still lacking, but naive multi-device training still reduces training time by a significant factor, leveraging almost all GPUs at all times.We also added comprehensive quantization support with persistent memory optimization, allowing models to use significantly less memory. Persistent memory leverages the fact that some tensors are less likely to change in size during execution and creates memory pools configured for their specific sizes. With Burn 0.19.0, module parameters are tagged as such, since in most neural networks, the size of the parameters doesn't change during training or inference. This setting can be turned off if it doesn't work well with your models.Just to visualize the memory gains possible, here are the results with a LLAMA 1B model:Finally, we introduced a new CPU backend powered by MLIR and LLVM, bringing the same JIT compilation, autotuning, and fusion capabilities from our GPU backends to CPU execution. The performance of the CubeCL runtime is great, but most of our algorithms aren't optimized for CPU yet, so the Burn backend is still quite slow. With the new CubeCL CPU runtime and LLVM compiler, we essentially created an alternative Rust compiler, though with drastically different compilation characteristics.There are many more improvements in this release beyond these highlights, and we wrote a post to cover them. Don't hesitate to skim it and refer to it for the migration guide. ]]></content:encoded></item><item><title>When O3 is 2x slower than O2</title><link>https://cat-solstice.github.io/test-pqueue/</link><author>/u/cat_solstice</author><category>rust</category><category>reddit</category><pubDate>Tue, 28 Oct 2025 09:50:49 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[While trying to optimize a piece of Rust code, I ran into a pathological case and I dug deep to try to understand the issue. At one point I decided to collect the data and write this article to share my journey and my findings.This is my first post here, I'd love to get your feedback both on the topic and on the article itself!]]></content:encoded></item><item><title>A hard rain&apos;s a-gonna fall: decoding JSON in Rust — Bitfield Consulting</title><link>https://bitfieldconsulting.com/posts/hard-rain-json-rust</link><author>/u/bitfieldconsulting</author><category>rust</category><category>reddit</category><pubDate>Tue, 28 Oct 2025 08:46:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Project goals for 2025H2</title><link>https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/</link><author>Niko Matsakis</author><category>dev</category><category>official</category><category>rust</category><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[On Sep 9, we merged RFC 3849, declaring our goals for the "second half" of 2025H2 -- well, the last 3 months, at least, since "yours truly" ran a bit behind getting the goals program organized.In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .Unblocking dormant traits, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.Flexible, fast(er) compilation, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers., making higher-level usage patterns in Rust easier.One of Rust's core value propositions is that it's a "library-based language"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a "second-class citizen" problem where custom pointer types can't provide the same ergonomic experience as built-in references.The "Beyond the " initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.
"Unblocking dormant traits"Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:Polonius will enable new borrowing patterns, and in particular unblock "lending iterators". Over the last few goal periods, we have identified an "alpha" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.The next-generation trait solver is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.The work on evolving trait hierarchies will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.The work to expand Rust's  hierarchy will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on RFC #3729 and RFC #3838, authored in previous Project goal periods.In-place initialization allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.The "Flexible, fast(er) compilation" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often "surprisingly tolerable" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:We aim to stabilize cargo script, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.We aim to finalize the design of ergonomic ref-counting and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the Rust program manager hired by the Leadership Council, will join me in running the program.]]></content:encoded></item><item><title>Warning! Don&apos;t buy &quot;Embedded Rust Programming&quot; by Thompson Carter</title><link>https://www.reddit.com/r/rust/comments/1ohq25k/warning_dont_buy_embedded_rust_programming_by/</link><author>/u/bowl-modular</author><category>rust</category><category>reddit</category><pubDate>Mon, 27 Oct 2025 20:46:50 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I made the mistake of buying this book, it looked quite professional and I thought to give it a shot.After a few chapters, I had the impression that AI certainly helped write the book, but I didn't find any errors. But checking the concurrency and I2C chapters, the book recommends libraries specifically designed for std environments or even linux operating systems.I've learned my lesson, but let this be a warning for others! Name and shame this author so other potential readers don't get fooled.]]></content:encoded></item><item><title>Rust compiler uses this crate for its beautiful error messages</title><link>https://github.com/rust-lang/annotate-snippets-rs</link><author>/u/nik-rev</author><category>rust</category><category>reddit</category><pubDate>Mon, 27 Oct 2025 20:10:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - longbridge/gpui-component: Rust GUI components for building fantastic cross-platform desktop application by using GPUI.</title><link>https://github.com/longbridge/gpui-component</link><author>/u/-Y0-</author><category>rust</category><category>reddit</category><pubDate>Mon, 27 Oct 2025 13:20:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why do y&apos;all have an aversion to writing comments?</title><link>https://www.reddit.com/r/rust/comments/1oh86x9/why_do_yall_have_an_aversion_to_writing_comments/</link><author>/u/Interesting_Golf_529</author><category>rust</category><category>reddit</category><pubDate>Mon, 27 Oct 2025 07:33:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've been working as a software engineer for about 16 years now, and have been doing some rust for the past year or so. Some at work, some OSS, and a few educational things for myself. Really liking it so far, great fun for the most part!One thing I've noticed though, and have been thinking about for a while, is that a lot of rust projects don't seem to use comments as much as projects written in other languages. A lot of them will have barely an comments at all.This trend seemingly fits in with the style things are documented in general; most of the time you get reference docs of the API and a cursory intro into the thing in a readme style, but "usage" docs or "how to" sections are rarely used.I've found myself having to dive deep into the source code to really understand what's going on way more in rust than I had with most other languages I'm familiar with. One observation I find particularly interesting about this is that I don't this has something to do with a difference in personal preference in general, as I've seen libraries written by the same team/person in a different language take a completely different approach to documenting than in rust.So. What do you think is it about rust that makes people at large not feel like writing comments and documentation? Have you noticed this as well? Do you perhaps notice a difference in your approach to this when writing rust versus another language?PS: Despite the title, I'm asking this with a genuine curiosity and fondness of the language, I'm not trying to do a "rust bad" here :)]]></content:encoded></item><item><title>rust-analyzer changelog #299</title><link>https://rust-analyzer.github.io/thisweek/2025/10/27/changelog-299.html</link><author>/u/WellMakeItSomehow</author><category>rust</category><category>reddit</category><pubDate>Mon, 27 Oct 2025 07:03:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Media] HB to me!🎂 Learning Rust, eating cheesecake</title><link>https://www.reddit.com/r/rust/comments/1ogx5qb/media_hb_to_me_learning_rust_eating_cheesecake/</link><author>/u/necodrre</author><category>rust</category><category>reddit</category><pubDate>Sun, 26 Oct 2025 21:55:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Rust feels wonderful so far btw (main language C and Go for networking)   submitted by    /u/necodrre ]]></content:encoded></item><item><title>What&apos;s New in Rust 1.81-1.84 Rustacean Station marathon</title><link>https://www.youtube.com/watch?v=6mwVWmKONY0</link><author>Jon Gjengset</author><category>dev</category><category>rust</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/6mwVWmKONY0?version=3" length="" type=""/><pubDate>Sun, 26 Oct 2025 16:35:00 +0000</pubDate><source url="https://www.youtube.com/channel/UC_iD0xppBwwsrM9DegC5cQQ">Jon Gjengset</source><content:encoded><![CDATA[Ben and Jon have yet again, and still completely intentionally, fallen behind on the "What's New in Rust" series from Rustacean Station (https://rustacean-station.org/) where they discuss each new Rust release. And all so that they (we?) could do another live marathon stream to go through what's changed across a bunch of Rust releases together!

You can listen to the last What's New in Rust podcast episode (covering 1.79 and 1.80) at https://rustacean-station.org/episode/rust-1.79-1.80/ or wherever you listen to podcasts.

The recording of this episode will be posted to the regular Rustacean Station feed once it's been cut appropriately :)

0:00:00 Black screen
0:25:29 Introduction
0:32:24 1.81.0
1:04:05 1.82.0
1:57:54 1.83.0
2:14:49 1.84.0

**Links**

1.81.0: https://blog.rust-lang.org/2024/09/05/Rust-1.81.0/
- https://github.com/rust-lang/rust/pull/124032/
- https://blog.rust-lang.org/2024/09/04/cve-2024-43402/

1.82.0: https://blog.rust-lang.org/2024/10/17/Rust-1.82.0/
- https://doc.rust-lang.org/rustc/target-tier-policy.html
- https://github.com/rust-lang/rust/pull/128254

1.83.0: https://blog.rust-lang.org/2024/11/28/Rust-1.83.0/
- https://www.unicode.org/emoji/charts-16.0/emoji-released.html
- https://github.com/rust-lang/rust/pull/129687
- https://doc.rust-lang.org/nightly/cargo/CHANGELOG.html#changed-9
- https://github.com/rust-lang/cargo/pull/14588
- https://github.com/rust-lang/cargo/pull/14137

1.84.0: https://blog.rust-lang.org/2025/01/09/Rust-1.84.0/
- https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html
- https://github.com/rust-lang/cargo/pull/14815
- https://github.com/rust-lang/rust/pull/129248]]></content:encoded></item><item><title>Now that I know Rust after doing several projects (most web microservices), I can say with confidence, I can easily use Rust for all back-end related tasks as I do with Go and Python for the last 8 years working as Senior Back-end Dev (Today I&apos;m Staff SWE focused at back-end and distributed system).</title><link>https://www.reddit.com/r/rust/comments/1ogo351/now_that_i_know_rust_after_doing_several_projects/</link><author>/u/swordmaster_ceo_tech</author><category>rust</category><category>reddit</category><pubDate>Sun, 26 Oct 2025 15:53:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This is something that I wasn't confident when started to enjoy Rust, for the context. I worked mostly using golang for the last 8 years in fintechs, big tech, startups etc, most of the time Go + a legacy codebase in Java, PHP, Python etc.The thing was, a language without GC would still be productive? And after using Rust I started to get very comfort with writing the code without getting into any trouble, sure the async model is not as easy as in Go or modern Java with virtual threads, but it is literally the same async colored functions that we use in Python, old Java, PHP, and several other languages for years, it is definitely easy and is not the big deal.Most of the work in my domain that is distributed systems and back-end, is just concurrency code, IO bound, and Rust with Tokio is great for this, same great performance that I achieve with Go, but even more safe because the code is always checked to be thread safe and doesn't have race conditions.And sure, we don't have many problems using Go like people who never work with it exaggerates, I never miss a sleep for a null pointer, and was rare to see someone creating race conditions problems, but at the same time, after you learn Rust, you're learning way more consistent to be better at concurrency thinking about thread safe and preventing race conditions than using Go, and naturally you will become a more efficient software engineer. And even working with very experienced SWE in Go and Java, you came to a point where you cannot continue to get better unless you start to use C++ or drop the GC, so if the curve after learning is pretty much the same (serious, 99% of our job as back-end is calling apis, db, and create concurrent workers to process something async) you definitely can be productive using Rust for this as in any other language, the crates are very mature already, but you're choosing a path that will let always grow in efficiency as SWE (for the cost of maybe one or two days more to build a feature).I already take my decision, I will and already am using Rust for all back-end related and I just use Go or Python if I don't have the Rust runtime like Google cloud function (I don't know if they finally added support) or things like this, otherwise I truly believe Rust should be the main language for general back-end, even in startups that need to move fast, is still fast, because the majority of startups like I said, the work is just calling apis, db, and creating workers etc, no big deal, people just love to pretend that is a complex work when it's not, the complexity is very outside the code in design the solutions for distributed systems, and for that, Rust is just fine and let you become each day more efficient as SWE building better software.]]></content:encoded></item><item><title>New Guide: Data Analysis in Rust</title><link>https://www.reddit.com/r/rust/comments/1oglp67/new_guide_data_analysis_in_rust/</link><author>/u/Eric_Fecteau</author><category>rust</category><category>reddit</category><pubDate>Sun, 26 Oct 2025 14:15:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This new Data analysis in Rust book is a "learn by example" guide to data analysis in Rust. It assumes minimal knowledge of data analysis and minimal familiarity with Rust and its tooling.The first section explores concepts related to data analysis in Rust, the crates (libraries) used in the book and how to collect the data necessary for the examples.The second section explains how to read and write various types of data (e.g.  and ), including larger-than-memory data. This section also focuses on the various locations that data can be read from and written to, including local data, cloud-based data and databases.The third section demonstrates how to transform data by adding and removing columns, filtering rows, pivoting the data and joining data together.The fourth section shows how do summary statistics, such as counts, totals, means and percentiles, with and without survey weights. It also gives some examples of hypothesis testing.The fifth and last section has examples of publication avenues, such as exporting summary statistics to excel, plotting results and writing markdown reports.   submitted by    /u/Eric_Fecteau ]]></content:encoded></item><item><title>GSoC &apos;25: Parallel Macro Expansion</title><link>https://lorrens.me/2025/10/26/GSoC-Parallel-Macro-Expansion.html</link><author>/u/Snerrol</author><category>rust</category><category>reddit</category><pubDate>Sun, 26 Oct 2025 11:25:39 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[A lot of work has been done to parallelise the Rust compiler, some parts are already parallelised, like codegen and processes after
HIR-lowering, such as type-checking, borrow-checking and MIR optimisation. For this years GSoC project I had the chance to parallelise the macro expansion algorithm.While the title of this post includes “ Macro Expansion”, I never got to parallelise the algorithm during this project. This was due to the sheer (unexpected) complexity of the project and
numerous complex issues/roadblocks that we faced as the project went on (one such roadblock even surprised my mentor).
This post will highlight my work and the issues/roadblocks we faced.The current algorithm is order-dependent, which prevents parallelisation. Simply put, it looks like this:Collect unresolved imports and macro invocations.Resolve a single collected import.Commit: write the resolved import back into its module.Resolve a single collected macro.Commit: expand the resolved macro.The idea is to parallelise , called import resolution, and , called macro expansion.As said before, I was meant to do both parts of the algorithm, but I only made progress on the import resolution algorithm. To be complete, I’ll explain this briefly as well.It iteratively resolves undetermined imports until no further progress can be made, committing each resolved import to the module resolutions before proceeding. The goal is to isolate
these resolutions so that each import in the undetermined set can be processed independently and committed afterwards. We called this batched import resolution because resolving and
committing are done over the entire  of undetermined imports.As some of you may observe, we would also need to remove any kind of mutability in the  that happens during this process, to avoid conflicts with other imports being resolved.If we successfully implemented the above 2 points, it would be trivial to parallelise the algorithm.These changes essentially boiled down into splitting the logic of the local crate and external crates. Items defined in external crates are considered a cache because they can not be changed once compiled,
so we only populate external crates when we actually need them.The  keeps a map of all compiled macros in a field called . You can access a compiled macro using a . This map was read from or updated in this method:However, local macros were always present in the map when this method was called, so the map was only updated here when we added an external macro.
This PR fixed this by splitting the maps and changing the logic to account for this.An essential method of the  is . Anytime you want to add particular binding (i.e. a name pointing to a thing) to a module, you’d call this method:However, we would only call this for external items when we eventually parallelise the algorithm. This PR
introduced  and , where  takes a .
The leading blockers here were underscore disambiguators (a way to differentiate  items) and external module maps (remember macro maps), fixed by Vadim Petrochenkov in #144272 and #143550 respectively.This change was very important,  was used by the  method of the , this method provides a way to access the resolutions of a particular module. But if
we want to access the resolutions of an external module, we first need to populate its resolutions if this has not yet been done (remember this is a cache). Now that we can use  here, we can convert the  method to
take a  instead of a mutable one. This allowed us to convert a lot of methods from  to .Smaller but relevant changesA bit of context here: when an external crate is referenced through an  item or path, the  will try to load the referenced crate. This is done through the , which wraps the 
an important data structure of the compiler that keeps information of every external crate used by the currently compiling crate.The problem was that the  also had a mutable reference to some state in the ,
which meant we needed a mutable reference to the  to construct it. But the  actually never used that state, only the , this PR
refactored the logic and that state of the  into the . Now anytime we need the logic of the , we can use the  instead, removing the need for a mutable .Cachify ExternPreludeEntry.bindingUpdating this field is classified as a cache and was thus wrapped in a  in #144605.Conditional Mutable ResolverNot all methods could be refactored like we did above; some require a mutable resolver when we are in later stages of name resolution. Some context here: when macro expansion is done, the compiler
does a bunch of postprocessing on the AST, things like finalising imports/macros, error reporting and a bunch of other things, thus mutating the .
These methods know when to do extra things based on an optional parameter, and only when this parameter is present do we mutate the resolver.We could have taken the approach I explained above, but it would have taken a lot of effort and time because these methods are quite complex and big. So we introduced a Conditional Mutable Resolver or  for short in #144912, which can give out mutable reference to the  based on a particular condition: is a flag set and unset in the code for import resolution. Now we can change the relevant methods to take in a  instead of a .
This type acts as a safeguard, it would panic if we try to get an exclusive reference during import resolution.A related change was to add the same kind of wrappers around  and  using the same flag, it would also panic if we would mutate the underlying . This was introduced in #146283 which would allow us
to make some important types  and  safe in the future.Batched Import ResolutionThis is where the fun starts and is still ongoing (it began on Aug 8), you can track the work here.
As I said in the beginning, we must resolve all undetermined imports, collect their side effects, and write them to the  the same way the current algorithm does.
Import resolution works with 2 kinds of imports: single imports () and glob imports (), the side-effects from these are also different. Converting this algorithm proved to be difficult.
Any change I made that messed up the resolution order failed to compile , which made it hard to debug.The first thing I encountered, which took me off guard, was a bug caused by the implicit prelude the compiler injects into your crate. It’s the first import that gets resolved, and every import after that depends on it:Because we collect side-effects of imports, we also collect the side-effects of this special prelude import. Delaying these side-effects caused a bunch of confusing errors. This PR
resolves the prelude import the moment we encounter it in the AST, thus never adding it to the set of undetermined imports. The  thus always sees the side effects of the prelude import when resolving
the undetermined set.The next challenge I faced was because of the way glob imports work. As you may know, glob imports import  from the module, so for , every public binding defined in  will be imported.
It is however possible that bindings can be defined in a module  the glob import is resolved, so the  needs a way to import these new bindings. This is done with a field in the 
called glob_importers: Vec<Import>, it’s a way to find every resolved glob that references that module, so when we define a new binding, we go through that list and import it using those glob imports.
This works as intended in the current algorithm, but when in batched resolution, this can cause problems because adding a glob import to a module is a side-effect.
Consider the following example where we first resolve  and then :We first resolve  and create a binding for it. Then we resolve , which at that point only finds . Next, we commit  to  and the bindings from  to the root
and only then add  to the  of module . Because  was already resolved earlier,  cannot be imported through it, which leads to an error when we try to use it.
To fix it we iterate twice over the side-effects: first to set all , and then apply the remaining side-effects, This ensures glob import correctly import all bindings.
So here  actually has a way to be imported by ;Incorrectly applying side effects is a recurring theme in this PR: single imports have a different kind of side effects, these are per namespace. The thing I did wrong was mindlessly copy-pasting
the original code to the new  code. But this was wrong, some imports go through multiple rounds of resolution until we determined that each namespace has a binding or not. The problem was when
we resolved a particular import a second time for another namespace, we actually reset the bindings for other already determined namespaces, which broke a lot of things. The fix for this was a
Option<Option<NameBinding>>, where the outer  specified if a side-effect was present, and the inner  was the actual side-effect for that namespace.Glob imports and #[macro_use]Once we fixed the above things, CI was green and my mentor/reviewer was content with the changes, we could run Crater with my PR to see if it breaks code in the wild.
And boy it did, when analysing the crater report I noticed that all regressions (i.e. things that failed but shouldn’t) were caused due to the  crate. For some
reason, it’s derive macro  exported as  (this export is important) couldn’t be resolved by its dependents. I’ll show the reduced case before explaining myself:The example above shows what happens with the current algorithm, and it’s what most people  expect. But this is actually wrong and should be an error, but why? The  import
and the pub use rust_embed_impl::*; glob import are actually  imports, because they import the same thing, but in a totally different way, the  imports privately while the glob
imports publicly. Currently, the compiler can only detect ambiguous imports when they import different things, so developers introduced such imports/exports in their code without knowing it’s wrong.
The algorithm we introduced highlighted this bug in the compiler. I’ll explain why.With batched import resolution, we see things more clearly, we first resolve pub use rust_embed_impl::*; and find the  macro as a resolution, then we resolve pub use RustEmbed as Embed, which finds
the  trait and macro. But here is the catch: that macro is imported by the  import, not the one imported by the glob because it hasn’t been not committed yet. After that, we commit the glob import, so we essentially exported a private macro, which makes the  macro invisible to the dependents of this crate (you can still use  because of the glob). And because every
crate did , they all failed to compile:This took me a long time to figure out. Unfortunately, I can’t fix it by altering the implementation because imports are required to be independent of their resolution order, which
isn’t the case here. So until we have fixed this bug in the compiler, we need to hack these kinds of imports. You can find the current hack here, but I’m unsure if that is enough.
It checks whether a resolution from a public glob import in a particular module shadows a  import, and if that’s the case, we set the visibility of that binding to public when we export it out of the crate.Vadim is currently fixing some things related to ambiguous imports because there are other cases like above, these are #147995 and #147984.]]></content:encoded></item><item><title>TARmageddon (CVE-2025-62518): RCE Vulnerability Highlights the Challenges of Open Source Abandonware | Edera Blog</title><link>https://edera.dev/stories/tarmageddon</link><author>/u/pjmlp</author><category>rust</category><category>reddit</category><pubDate>Sun, 26 Oct 2025 08:51:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This vulnerability impacts major, widely-used projects, including uv (Astral's lightning-fast Python package manager), , and . Due to the widespread nature of  in various forms, it is not possible to truly quantify upfront the blast radius of this bug across the ecosystem.Our suggested remediation is to immediately upgrade to one of the patched versions or remove this dependency. If you depend on , consider migrating to an actively maintained fork like . In addition, the Edera fork  will be archived to coalesce all efforts with the astral fork and reduce the ecosystem confusion.The Challenge of Abandonware: A Decentralized Responsible DisclosureThis vulnerability disclosure was uniquely challenging because the most popular fork (, with over 5 million downloads on crates.io) appears to be abandonware – no longer actively maintained.In a standard disclosure, a single patch is applied to the main upstream repository, and all downstream users inherit the fix. Because we could not rely on the original project maintainers to apply the fix, we were forced to coordinate a decentralized disclosure across a deep and complex fork lineage: (Root) ➡️  (Most popular fork, abandoned) ➡️ (Originally maintained by Edera, now archived) ➡️ (Actively maintained by Astral)Instead of a single point of contact, we had to:Develop patches for the upstream versions.Identify and reach out to the maintainers of the unmaintained upstream repositories ( and ). Neither project had a SECURITY.md or public contact method, so it required some social engineering and community sleuthing to locate the right maintainers. Individually contact the maintainers of the two most active forks ( and ) and coordinate simultaneous patching under a strict 60-day embargo.Proactively reach out to major downstream projects (including , , , and ) to ensure they were ready to upgrade immediately upon disclosure.This process required significantly more effort and time than a typical disclosure, underscoring the severe challenges and inefficiency of ensuring wide-scale patching when critical vulnerabilities are found in popular, yet abandoned, open-source dependencies.Downstream Project Coordination ResultsBinstalk: Plan to eliminate dependency or switch to patched  due to small attack surface.Opa-wasm: Not affected, as they do not use the vulnerable unpacking/extraction-to-path functionality.Other projects were made aware of the upcoming patch and have not responded to our attempts at outreach. Furthermore, there are likely several downstream projects relying on impacted versions that we are not aware of. Technical Overview & Analysis This vulnerability is a desynchronization flaw that allows an attacker to "smuggle" additional archive entries into TAR extractions. It occurs when processing nested TAR files that exhibit a specific mismatch between their PAX extended headers and ustar headers.The flaw stems from the parser's inconsistent logic when determining file data boundaries:A file entry has both PAX and ustar headers.The PAX header correctly specifies the actual file size (, e.g., 1MB).The ustar header incorrectly specifies zero size ().The vulnerable  parser incorrectly advances the stream position based on the ustar size (0 bytes) instead of the PAX size (X bytes).By advancing 0 bytes, the parser fails to skip over the actual file data (which is a nested TAR archive) and immediately encounters the next valid TAR header located at the start of the nested archive. It then incorrectly interprets the inner archive's headers as legitimate entries belonging to the outer archive.File overwriting attacks within extraction directories.Supply chain attacks via build system and package manager exploitation.Bill-of-materials (BOM) bypass for security scanning.Vulnerable Processing (PAX size=X, ustar size=0):The result seen by the parser: Actually extracted by tokio-tar:
The key insight is that the parser's internal position in the stream becomes misaligned, causing it to treat headers and data from a hidden, nested archive as part of the primary archive's entry list.1. Python Build Backend HijackingTarget: Python package managers using  (e.g., ). An attacker uploads a malicious package to PyPI. The package's outer TAR contains a legitimate , but the hidden inner TAR contains a malicious one that hijacks the build backend. During package installation, the malicious config overwrites the legitimate one, leading to RCE on developer machines and CI systems.2. Container Image PoisoningTarget: Container testing frameworks (e.g., ). Testing frameworks that extract image layers for analysis can be tricked into processing layers crafted with this nested TAR structure. The hidden inner TAR introduces unexpected or overwritten files, allowing for test environment compromise and supply chain pollution.Target: Any system with separate "scan/approve" vs "extract/deploy" phases. A security scanner analyzes the outer, clean TAR and approves its limited file set. However, the extraction process using the vulnerable library pulls in additional, unapproved, and unscanned files from the inner TAR, resulting in a security control bypass and policy violation.The Edera team provided patches for  (used by uv) and . The vulnerability is tracked as CVE-2025-62518. Huge thank you to the security team at Astral for diligently working with us to patch this vulnerability and responsibly disclose. The patch requires modifying the TAR parser to:Prioritize PAX headers for size determination over ustar headers.Validate header consistency between PAX and ustar records.Implement strict boundary checking to prevent data/header confusion.Due to architectural differences, we did not provide a direct patch for  but a patch was written by the maintainer.If immediate patching or switching to a maintained fork is not possible, consider these workarounds:: The standard tar crate (non-async) correctly handles this scenario and can serve as a temporary replacement:: Mature, well-tested, handles PAX headers correctly: Synchronous API only, may require architectural changes for async codebases: Consider wrapping tar operations in tokio::task::spawn_blocking() for async compatibilityValidate extracted file counts against expected manifestsImplement post-extraction directory scanning to detect unexpected filesUse separate extraction sandboxes with file count/size limitsDisable file overwriting during extraction where possible. A Note on Rust, Security Boundaries, and the Path ForwardThe discovery of TARmageddon is an important reminder that Rust is not a silver bullet. While Rust's guarantees make it significantly harder to introduce memory safety bugs (like buffer overflows or use-after-free), it does not eliminate logic bugs—and this parsing inconsistency is fundamentally a logic flaw. Developers must remain vigilant against all classes of vulnerabilities, regardless of the language used.This lineage of vulnerable libraries ( ➡️  ➡️ forks) tells a common open-source story: popular code, even in modern secure languages, can become unmaintained and expose its millions of downstream users to risk.This experience reinforces the importance of defense-in-depth. We are happy to report that due to proactive design in Edera, our own products were not vulnerable to this bug despite embedding the vulnerable . By implementing strong security boundaries and ensuring that vulnerable operations were not used in critical pathways, Edera mitigated the issue before the patch was even released.Lastly, we’d like to thank the researchers and maintainers across the ecosystem that worked with us to responsibly disclose and patch this vulnerability. Timeline – 60 Day Disclosure Embargo 8/21: Initial investigation found the vulnerability in  and , as well as the upstream  and 8/21: Minimal reproduction created8/22: Created patches for all libraries8/22: Disclosed bug to maintainers of all libraries, the rust foundation, and select downstream projects (selected by number of downloads). The embargo date was set to October 21.8/22: Received response from  and  and shared the patch9/2: Received acknowledgment from 10/21: Publish GHSA and patches for  and ]]></content:encoded></item><item><title>[Media] you can build actual web apps with just rust stdlib and html, actually</title><link>https://www.reddit.com/r/rust/comments/1og58sy/media_you_can_build_actual_web_apps_with_just/</link><author>/u/pomeach</author><category>rust</category><category>reddit</category><pubDate>Sat, 25 Oct 2025 23:01:15 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[so I was messing around and decided to build a simple ip lookup tool without using any web frameworks. turns out you really don't need axum, actix, or rocket for something straightforward.the title may seem silly, but to me it's kind of crazy. people spend days learning a framework when a single main rs and a index.html could do the job.the whole thing is built with just the standard library , some basic http parsing, and that's pretty much it. no dependencies in the cargo.toml at all.listens on port 8080, serves a minimal terminal-style html page, and when you click the button it returns your ip info in json format. it shows your public ip (grabbed from headers like  or ), the peer connection ip, any forwarding chain, and your user agent.I added some basic stuff like rate limiting (30 requests per minute per ip), proper timeouts, and error handling so connections don't just hang or crash the whole server. the rate limiter uses a simple hashmap with timestamps that gets cleaned up on each request.the html is compiled directly into the binary with  so there are no external files to deal with at runtime. just one executable and you're done.curiosity mostly :). wanted to see how bare bones you could go and still have something functional. frameworks are great and I use them for real projects, but sometimes it's fun to strip everything down and see what's actually happening under the hood.plus the final binary is tiny and starts instantly since there's no framework overhead!!threw it on fly.io with a simple dockerfile. works perfectly fine. the whole thing is like 200 lines of rust code total.if you're learning rust or web stuff, i'd recommend trying this at least once. you learn a lot about http, tcp, and how web servers actually work when you're not relying on a framework to abstract it all away.curious if anyone else has built stuff like this or if i'm just being unnecessarily stubborn about avoiding dependencies lol]]></content:encoded></item></channel></rss>