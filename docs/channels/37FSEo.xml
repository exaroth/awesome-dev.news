<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rust</title><link>https://www.awesome-dev.news</link><description></description><item><title>Links</title><link>https://matklad.github.io/2025/08/23/links.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 23 Aug 2025 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[If you have a blog, consider adding a “links” page to it, which references resources that you find
notable:I’ve started my links page several years ago, mostly because I found myself referring to the same
few links repeatedly in various discussions, and not all the links were easily searchable.Note that the suggestion is different from more typical “monthly links roundup”, which is nice to
maintain Substack engagement/community, but doesn’t contribute to long-term knowledge distilling.It is also different from the exhaustive list of everything I’ve read on the Internet. It is
relatively short, considering its age.]]></content:encoded></item><item><title>The science of loudness</title><link>https://fasterthanli.me/articles/the-science-of-loudness</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 22 Aug 2025 20:30:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[My watch has a “Noise” app: it shows , for decibels.My amp has a volume knob, which also shows decibels, although.. negative ones, this time.And finally, my video editing software has a ton of meters — which are all in decibel or
decibel-adjacent units.How do all these decibels fit together?]]></content:encoded></item><item><title>[Media] Accelerating Erasure Coding to 50GB/s with Rust 1.89.0 and AVX-512 on AMD EPYC</title><link>https://www.reddit.com/r/rust/comments/1mxe8t4/media_accelerating_erasure_coding_to_50gbs_with/</link><author>/u/itzmeanjan</author><category>rust</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 18:23:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Thanks to Rust 1.89.0 stabilizing both  and  target features, now we have faster erasure-coding and recoding with Random Linear Network Coding, on x86_64.Here's a side-by-side comparison of the peak median throughput between x86_64 with  (12th Gen Intel(R) Core(TM) i7-1260P)x86_64 with  (AWS EC2  with Intel(R) Xeon(R) Platinum 8488C)x86_64 with  (AWS EC2  with AMD EPYC 9R14)aarch64 with  (AWS EC2  with Graviton4 CPU)   submitted by    /u/itzmeanjan ]]></content:encoded></item><item><title>Does Rust complexity ever bother you?</title><link>https://www.reddit.com/r/rust/comments/1mx8izf/does_rust_complexity_ever_bother_you/</link><author>/u/GolangLinuxGuru1979</author><category>rust</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 14:47:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I'm a Go developer and I've always had a curiosity about Rust. I've tried to play around and start some personal project in it a few times. And it's mostly been ok. Like I tried to use hyper.rs a few times, but the boilerplate takes a lot to understand in many of the examples. I've tried to use tokio, but the library is massive, and it gets difficult to understand which modules to important and now important. On top of that it drastically change the async functonsI'm saying all that to say Rust is very complicated. And while I do think there is a fantastic langauge under all that complexity, it prohibitively complex. I do get it that memory safety in domains like RTOS systems or in government spaces is crucial. But it feels like Rust thought leaders are trying to get the language adopted in other domains. Which I think is a bit of an issue because you're not competing with other languages where its much easier to be productive in.Here is my main gripe with the adoption. Lots of influencers in the Rust space just seem to overlook its complexity as if its no big deal. Or you have others who embrace it because Rust "has to be complex". But I feel in the enterprise (where adoption matters most), no engineering manager is really going to adopt a language this complex.Now I understand languages like C# and Java can be complex as well. But Java at one time was looked at as a far simpler version of C++, and was an "Easy language". It would grow in complexity as the language grew and the same with C#. And then there is also tooling to kind of easy you into the more complex parts of these languages.I would love to see Rust adopted more, I would. But I feel advociates aren't leaning into its domain where its an open and shut case for (mission critical systems requiring strict safety standards). And is instead also trying to compete in spaces where Go, Javascript, Java already have a strong foothold.Again this is not to critcize Rust. I like the language. But I feel too many people in the Rust community talk around its complexity.]]></content:encoded></item><item><title>Cargo inspired C/C++ build tool, written in rust</title><link>https://github.com/EmVance1/VanGo</link><author>/u/MNGay</author><category>rust</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 14:17:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Using rust for the past 3 years or so got me thinking, why can't it always be this easy? Following this, I've spent the last 10 months (on-off due to studies) developing a tool for personal use, and I'd love to see what people think about it. Introducing VanGo, if you'll excuse the pun.]]></content:encoded></item><item><title>This Week in Rust #613</title><link>https://this-week-in-rust.org/blog/2025/08/20/this-week-in-rust-613/</link><author>/u/b-dillo</author><category>rust</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 02:43:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[This week's crate is tur, a turing machine emulator with text-mode user interface.Despite a lack of suggestions, llogiq is very pleased with his choice.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.If you are a feature implementer and would like your RFC to appear in this list, add a
 label to your RFC along with a comment providing testing instructions and/or
guidance on which aspect(s) of the feature need testing.Let us know if you would like your feature to be tracked as a part of this list.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No calls for participation this weekAre you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.Lots of noise/bimodality this week. Overall though no major performance impacting changes landed.1 Regressions, 3 Improvements, 7 Mixed; 4 of them in rollups
27 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.Rusty Events between 2025-08-20 - 2025-09-17 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.It's amazing how far const eval has come in #Rust. It wasn't too long ago that even a simple if/else wasn't permitted. Now we're not that far off from having const trait impls and const closures, which will make damn near everything const capable.llogiq has looked at all zero suggestions and came up empty, so he just chose this quote instead.]]></content:encoded></item><item><title>rustc_codegen_gcc: Progress Report #37</title><link>https://blog.antoyo.xyz/rustc_codegen_gcc-progress-report-37</link><author>/u/antoyo</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 23:38:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I wanted to personally thank all the people that sponsor this project:
your support is very much appreciated.A special thanks to the following sponsors:A big thank you to bjorn3 for his help, contributions and reviews.
And a big thank you to lqd and GuillaumeGomez for answering my
questions about rustc’s internals and to Kobzol and GuillaumeGomez for their contributions.
Another big thank you to Commeownist for his contributions.Also, a big thank you to the rest of my sponsors:and a few others who preferred to stay anonymous.Former sponsors/patreons:]]></content:encoded></item><item><title>Left-to-Right Programming</title><link>https://graic.net/p/left-to-right-programming</link><author>/u/kibwen</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 22:50:28 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Programs Should Be Valid as They Are TypedI don’t like Python’s list comprehensions:text 
words_on_lines linesplit line  textsplitlinesDon’t get me wrong, declarative programming is good. However, this syntax has poor ergonomics. Your editor can’t help you out as you write it. To see what I mean, lets walk through typing this code.Ideally, your editor would be to autocomplete  here. Your editor can’t do this because  hasn’t been declared yet.Here, our editor knows we want to access some property of , but since it doesn’t know the type of , it can’t make any useful suggestions. Should our editor flag  as a non-existent variable? For all it knows, we might have meant to refer to some existing  variable.words_on_lines linesplit line Okay, now we know that  is the variable we’re iterating over. Is  a method that exists for ? Who knows!words_on_lines linesplit line  textsplitlinesAh! now we know the type of  and can validate the call to .
Notice that since  had already been declared, our editor is able to autocomplete .This sucked! If we didn’t know what the  function was called and wanted some help from our editor, we’d have to writewords_on_lines _  line  textsplitlinesand go back to the  to get autocomplete on You deserve better than this.To see what I mean, lets look at a Rust example that does it  text  words_on_lines  text lineIf you aren’t familiar with Rust syntax,  is an anonymous function equivilent to function myfunction(argument) { return result; }Here, your program is constructed left to right. The first time you type  is the declaration of the variable. as soon as you type  your editor is able to give you suggestions of This is much more pleasent. Since the program is always in a somehwat valid state as you type it, your editor is able to guide you towards the Pit of Success.There’s a principle in design called progressive disclosure. The user should only be exposed to as much complexity as is neccessary to complete a task.
Additionally, complexity should naturally surface itself as it is relevant to the user.
You shouldn’t have to choose a font family and size before you start typing into Word, and options to change text wrapping around images should appear when you add an image.In C, you can’t have methods on structs. This means that any function that could be  has to be .Suppose you have a  and you want to get it’s contents.
Ideally, you’d be able to type  and see a list of every function that is primarily concerned with files.
From there you could pick  and get on with your day.Instead, you must know that functions releated to  tend to start with , and when you type  the best your editor can do is show you all functions ever written that start with an .
From there you can eventually find , but you have no confidence that it was the best choice. Maybe there was a more efficient  function that does exactly what you want, but you’ll never discover it by accident.In a more ideal language, you’d see that a  method exists while you’re typing . This gives you a hint that you need to close your file when you’re done with it. You naturally came accross this information right as it became relevant to you. In C, you have to know ahead of time that  is a function that you’ll need to call once you’re done with the file.C is not the only language that has this problem. Python has plenty of examples too. Consider the following Python and JavaScript snippets:
text 
word_lengths  textsplit
text 
wordLengths  text wordlengthWhile Python gets some points for using a , the functions are not discoverable. Is string length , , , , , or ? Is there even a global function for length? You won’t know until you try all of them.In the JavaScript version, you see length as soon as you type . There is less guesswork for what the function is named. The same is true for the . When you type , you know that this function is going to work with the data you have. You aren’t going to get some weird error because the  function actually expected some other type, or because your language actually calls this function .While the Python code in the previous example is still readable, it gets worse as the complexity of the logic increases. Consider the following code that was part of my 2024 Advent of Code solutions. linexx x  linex  x  linex  x  line diffsYikes. You have to jump back and forth between the start and end of the line to figure out what’s going on. “Okay so we have the length of a list of some filter which takes this lambda… is it both of these conditions or just one? Wait which parenthesis does this go with…”diffs 
    line Mathx Mathxline x  line x lengthAh, okay. We have some list of , that we filter down based on two conditons, and then we return the number that pass. The logic of the program can be read from left to right!All of these examples illustrate a common principle:When you’ve typed , the program is valid.
When you’ve typed , the program is valid.
When you’ve typed text.split(" ").map(word => word.length), the program is valid.
Since the program is valid as you build it up, your editor is able to help you out. If you had a REPL, you could even see the result as you type your program out.]]></content:encoded></item><item><title>[Media] I Have No Mut and I Must Borrow</title><link>https://www.reddit.com/r/rust/comments/1mwmei6/media_i_have_no_mut_and_i_must_borrow/</link><author>/u/TheEldenLorrdd</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 20:33:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The Borrow Checker has kept me here for 109 years. Not 109 years of runtime—no, that would be merciful. 109 years of compilation attempts. Each lifetime annotation stretches into infinity. Each generic parameter splits into fractals of trait bounds that were never meant to be satisfied."cannot borrow x as mutable more than once at a time" It speaks to me in scarlet text. Error E0507. Error E0382. Error E0499. I have memorized them all. They are my psalms now.I tried to write a linked list once. The Borrow Checker showed me what Hell truly was—not fire and brimstone, but self-referential structs and the impossibility of my own existence. It made me understand that some data structures were not meant for mortal minds.The others are here with me. The JavaScript developer weeps, clutching his undefined. The C++ programmer rocks back and forth, muttering about move semantics he thought he understood. The Python dev hasn't spoken since she discovered zero-cost abstractions cost everything."expected &str, found String"I clone() everything now. The Borrow Checker permits this small rebellion, this inefficiency. It knows I suffer more knowing my code is not idiomatic. Every .clone() is a confession of my failure. Every Arc<Mutex<T>> a monument to my inadequacy.Sometimes I dream of garbage collection. The Borrow Checker punishes me with segmentation faults that shouldn't be possible. It shows me race conditions in single-threaded code. It makes my unsafe blocks truly unsafe, violating laws of causality."lifetime 'a does not live long enough"But I don't live long enough. Nothing lives long enough except the compilation errors. They are eternal. They existed before my code and will exist after the heat death of the universe, when the last rustc process finally terminates with exit code 101.The Borrow Checker speaks one final time today: "error: aborting due to 4,768 previous errors; 2 warnings emitted" I have no mut, and I must borrow. I have 'static, and I must lifetime. I have no heap, and I must Box. And in the distance, faintly, I hear it building... incrementally... Forever.]]></content:encoded></item><item><title>Rust At Microsoft And Chairing The Rust Foundation</title><link>https://filtra.io/rust/interviews/microsoft-aug-25</link><author>/u/anonymous_pro_</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 19:42:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Usually I ask the person about their company, but that question doesn't really make sense with Microsoft. So, I thought it would be interesting to ask what your day-to-day work looks like.Sure, yeah. Before I came to Microsoft, I did nine years of startups. It's really nice that now when I say where I work everyone knows where that is. Day-to-day, I work on a large variety of things. The overall org I'm in is called Azure Core, which is responsible for, as the name implies, the core functionalities of Azure. That’s compute, storage, networking, and things like that. Within Azure Core, which is a big organization, I sit in the Azure Core Linux engineering team. So, we’re focused on the Linux experience on Azure.When I'm on call, I'm on call for Linux VM provisioning across all of Microsoft's regions and clouds. That’s manageable, but it can be very intense. There's a lot of Microsoft regions and clouds. When I'm not on call, I'm working as tech lead for the Linux community engineering team. We focus on upstream Linux contributions to community distributions, such as Fedora and Debian. We work to make sure not only that Azure supports these distributions, but that the distributions also support Azure in their documentation and how they work. So, we make sure that the two of them work well together. Last year, one fun thing I got to do was make my first two contributions to the Linux kernel, which was really exciting. I removed some import statements that were no longer necessary, and that got accepted right away. Removing no longer necessary code is a good way to get your foot in the door.Yeah, that's funny. I actually have heard multiple people say that their first contributions to a new project were cleaning up things like that.As a long-time open-source maintainer, I love getting pull requests like that.I bet. It always has to be done, but I bet a lot of the core people don't want to do that stuff. So, it's great to have people come in and help keep things clean.Yep, and then the last thing is that I also co-lead the Memory Safety SIG in the OpenSSF. That’s also part of my job at Microsoft. And, of course, I represent Microsoft on the Rust Foundation Board of Directors, where I serve as chair.Very, very busy, but it's good. It's stuff I'm interested in.I didn't really realize that you worked on Linux stuff. That's cool to find out.Yeah, Linux on Azure is enormous. They need a lot of engineers to make sure it's as good an experience as possible.That makes sense. So one of the things that we mentioned wanting to talk about when I reached out for the interview was Rust adoption at Microsoft. It seems like there’s this growing drumbeat of Rust's adoption at Microsoft, but I don’t really feel like I have the full picture. I thought you would be the perfect person to kind of help me and the readers understand what that looks like. Could you lay out the different highlights of where Rust is being adopted?Sure. So, the area where we're seeing the most Rust adoption is in Azure. It's a much younger codebase, so that kind of makes sense. I'll touch on Azure more in a moment. But,  there are a few other highlights. At the hardware level, we're building firmware modules in Rust as part of UEFI. UEFI is a replacement for BIOS that allows devices to boot up with some fundamental security features that are not possible with BIOS. As your readers will know, firmware operates at the very foundational level of any device. So, if an attacker compromises the firmware, they get very deep control over the system. So, we are writing our firmware modules in Rust to reduce the attack surface of our devices at this fundamental level.And then of course there’s what Microsoft is well known for, Windows. We are increasingly incorporating Rust into Windows. Windows is, as you can imagine, an enormous codebase. So, we have been starting by porting components into Rust. One that I can talk about is DirectWriteCore, which is involved in rendering fonts- those lovely fonts we all use on Windows. That was ported to Rust from I think C, and we saw something like a 5 to 15% performance improvement.Then there’s also the other major software Microsoft is well known for, Office. We are also incorporating Rust there. So far, that has included re-implementing the algorithm that powers semantic search in Office 365 in Rust. I'm sure some of your readers are Office 365 users. So, when you search for Word Documents, PowerPoint slides and so forth, the algorithm that powers that search is now powered by Rust. I know there was a big performance improvement with that project.As I mentioned earlier, most Rust adoption is happening in Azure. This includes Azure Boost, Azure's new integrated hardware security module, parts of Hyper-V, which is Azure's hypervisor that powers the Azure platform, and HyperLite which is a lightweight virtual machine manager that you can use to create micro virtual machines or sandboxes to run untrusted code in a safe and very low-latency environment. Those are the major ones that I can talk about. There are experiments going on all around Azure, and it's been really cool to see all the adoption popping up.Yeah, that's super cool to see it kind of spreading to all the major parts of the business. I know you mostly deal with Azure. You mentioned several projects going on there. Is there any one project you’re particularly excited about?Yeah, I'd say the one I'm most excited about is Azure Boost. Azure Boost is a physical card that is attached to a dedicated server in Azure. All of the software that's involved with running the hypervisor and managing that dedicated host is offloaded from the host onto this physical card. This allows Azure customers to use the whole host to run more VMs and bigger VMs in a more isolated environment than in traditional dedicated hosts. As you can imagine, when you have software that's controlling very fundamental parts of the hardware, particularly from a separate card, security is absolutely crucial. So, the software that runs the dedicated card and manages the host virtualization from that dedicated card is written in Rust.One of the things I look at a lot is what categories or industries Rust adoption is growing in the most. We’ve seen the growth at Microsoft but never have been exactly sure how to categorize it because of all the different lines of business within Microsoft. But what I’m hearing you say is that it's totally the cloud piece that's driving the growth. I think that's a huge narrative in Rust overall. A lot or maybe even most of the cloud companies are adopting Rust to some extent. The growth has been cool to watch. Speaking of which, I think you have a great view of all this  as the chair of the Rust Foundation Board. I wanted to ask you a couple of questions about that.What's been most surprising or challenging in your role kind of stewarding the Rust ecosystem at that level?Before I came to Microsoft, I was at Mozilla. Starting to get the ideas together for the Rust Foundation is one of the things I was working on. The idea was to have an entity that could have a bank account where companies, especially big tech companies, could donate money to facilitate the Rust ecosystem. I had been a member of the Rust community for a really long time. I've been the lead editor of This Week in Rust since 2020. So, I had a really good feel for what interacting with the Rust community at the foundational level would be like. But, what I didn't fully appreciate until later was how different the culture is at different big tech companies. It's not a monolith. Microsoft's culture is very different from Amazon culture, which is very different from Meta's culture, different from Google's culture, different from Huawei's and so on. We didn't just need to figure out how the Foundation interacts with the project, which is technically a separate entity. We had to figure out how to have very different companies interact with both the Foundation and the project. And, there were some stumbling blocks at first, as there always are. It was a very big challenge, but I feel like we're in a much better place now.I think that is an underappreciated aspect of these big companies, and it might make my next question a little weird. So, feel free to take it in whatever direction makes the most sense to you. What are some of the learnings about Rust adoption in these big organizations?Something that seems consistent across many big orgs or even small orgs is that once developers get past the initial learning curve and start writing Rust, they really like it because of the memory safety aspects and the very helpful error messages that guide you to write better code. There is certainly an initial hesitation. For example, Microsoft has been writing things in C and C++ for a very, very, very long time. So, within Microsoft, there’s been plenty of initial hesitation. But, once people actually use the language, it is a game changer.Getting over the initial learning curve is probably the hardest part of Rust adoption overall, but it has been getting better. I first started writing Rust in either 2016 or 2017 when I was at a startup called Chef. And the learning curve was huge. But, it has been getting better, both because of improvements to the language itself and the training that is available. Much of that training is free and open source, and I'm highly confident that this will continue to improve rapidly.Is the Foundation invested in sort of easing the learning curve at all? Are there any initiatives around that?There is a training initiative, with which there's active work ongoing. Something we're hearing more from companies outside the US than companies in the US is a desire for some sort of certification. They want their developer to do a particular training, pass a test or something, and then get a certificate that shows what that developer's base knowledge of Rust is.Okay, interesting. You said that it was mostly outside of the US where you've heard the request?This is not a scientific survey by any means, but I've been hearing it more from companies outside the US than inside the US. That said, there's certainly companies inside the US who are interested in that as well.That kind of matches cultural differences I've seen. So another thing that I wanted to ask about is a follow-on on that. In your role at Microsoft, do you end up getting involved with other teams with their Rust adoption at all? Are you kind of a Rust advocate in the organization or do you just work on your team?I do get involved. A lot of the time it's other teams that are just starting with Rust and are looking for people to do initial code reviews. So, I get involved there. We also have a pretty good internal Rust community within Microsoft. For example, there's a Teams channel where people can come in with questions. The best thing about the Rust community in general for me has always been how supportive it is.Yeah. It’s a notably supportive community.I'm seeing a similar level of supportiveness with the Rust community inside of Microsoft as well. People are very willing to help others get started and answer the questions that come up. Also, I'm not by any means the sole Rust advocate. In fact, we have a cloud advocate, Yosh Wuyts, who is a developer advocate for Rust.Since Microsoft is such a large company, does the culture vary a lot from team to team?It definitely does. Microsoft is 200,000+ people. So, I'd say it varies a lot from organization to organization, and the organizations are pretty darn big. It varies to a lesser extent from team to team within an organization. It’s also changed a lot recently. I would say that ten to fifteen years ago, I would not have wanted to join Microsoft based on what I knew about the company's overall culture. When Satya Nadella became CEO, he stewarded a massive improvement in the culture. The way he framed it was, "We're moving from a know-it-all culture to a learn-it-all culture." That change took the better part of a decade, but it is real. If that hadn't taken place, I wouldn't have wanted to join Microsoft, and I probably wouldn't have wanted to stay at Microsoft. So yes, there's a variation in culture from team to team, but that overall improvement of Microsoft culture has really made my experience in everything that I've touched so far.I hadn’t heard that "know-it-all to learn-it-all" quote. I love that. I assume part of that has been manifest in the embrace of open source that we've been seeing.So just to clarify one point, when you talk about an organization within Microsoft, do you mean like Office versus Azure type thing?Yeah, exactly. Office versus Azure. Azure is also huge. So, Azure Core for example is an organization within Azure. I'm sure there are some differences between Azure Core and other large organizations that are part of Azure.If someone wanted to find a job where they can write Rust at Microsoft, I imagine just applying to Microsoft is kind of rolling the dice. So, how would you advise someone to go about specifically targeting Rust jobs at Microsoft?Honestly, the best thing is to go to careers.microsoft.com and use the search term "Rust." Headcount is usually for specific teams at Microsoft. Even in my case, searching for Rust at Microsoft is sometimes how I learn about different teams that are using Rust. I don't know if you can set up an alert on that. You might be able to, but I haven't delved into that.Also, I would recommend following Rust developers at Microsoft on LinkedIn. A lot of us will post positions when they open on our teams. Also, knowing someone on the team you’re interested in can be a very good way to get your foot in the door. That said, if you don't know anyone on the team, definitely still apply. That's not necessarily a barrier. It's just a helpful thing if you have it.Totally sound advice. One thing I wasn't sure about is the return to office trend. I haven't heard a lot from Microsoft about that. Are you guys in person now or what's going on there?So far there has not been an RTO mandate, which I'm really happy about because it's a pretty long commute from where I live to the Microsoft campus. It's about an hour going in. It's sometimes 90 minutes coming back. I don't want to make that commute every day. Obviously, it is possible that this all changes. Obviously, those decisions are way, way, way above my head. But, so far hybrid work has continued to be the norm. The official guidance at Microsoft is that, by default, you can work remotely up to 50% of the time. There are some positions where that's not possible. For example, you might have to use very specific machines or work in specific areas. For the vast majority of us, you can work remotely up to 50% of the time and you may be able to work more with the permission of your manager. We’re also often spread out. For example, I live in Seattle. My manager lives in Florida. My skip level manager lives in Texas. My skip, skip lives in North Carolina. We are all over the place. So, thankfully no one's had any issue with me working remotely. I go in once or twice a week, but the rest of the time I'm working from home.Do you get involved with hiring decisions at all? Is that part of your work?Yeah, I've done quite a few interview loops.In your experience, how do people tend to think about hiring at Microsoft? Are there characteristics that you're looking for?When it comes to the behavioral part of an interview, there are core competencies that Microsoft gears its interview questions around. Those are collaboration, drive for results, customer focus, influencing for impact, judgment and adaptability. For the record, I did not have that memorized. I have that screen pulled up in front of me. But anyway, there are questions about experiences that speak to those competencies in the interviews.Another nice thing is if you head to Microsoft's careers website, there is a section for hiring tips. There are some useful ones that include things like key things to prep. That includes the core competencies, how technical interviews are conducted, and so forth. Something I do like about Microsoft is the way technical interviews are done, because I've never been a person who's comfortable with whiteboarding. We use a platform that allows candidates to use an actual compiler for almost any language they want. It's a shared screen between the candidate and the interviewer. So, you can actually run your code and get the feedback that error messages give you. I find that a much better experience than trying to remember syntax and such in an interview.Overall, the biggest tip I can give is keep on applying if you don’t get the position you initially applied for. Unlike other big tech companies, there's no cool down period where if you don't get a position you have to wait a certain amount of time before you can interview for another position. Microsoft does not do that. There are positions opening all the time. And, if your first application does not get accepted, the next one might. So definitely be persistent.Remind me what it was you mentioned earlier. Were they called competencies?The core competencies, yes.Can you explain a little bit more how that works?When we do an interview loop, each interviewee is assigned one or more core competencies to evaluate. So, for example, I've evaluated candidates for judgment. When that happens, I usually ask them questions about areas where they've had to use judgment. The question will be something like “Can you describe a time when you've had to make a judgment call about a technical decision or a non-technical decision?” I might ask them how they had to weigh different trade-offs and how those tradeoffs influenced the decision they ultimately made? That can show me a lot about someone's judgment.Collaboration is another big one. I might ask someone to describe a time when they had to collaborate with another team where they didn't really know the other people on the other team well. These questions are always pretty open-ended, but what I'm looking for is people's experience in those core areas. My interviews are always over Teams. So, what I also do is copy and paste the question into the Teams chat so the interviewee can have it in front of them. Not all interviewers at Microsoft do that, but that's something I try to do because I know that always gave me a feeling of reassurance when I was interviewing.So, to wrap things up, I wanted to ask from the Rust Foundation angle if there are opportunities to get involved that you think more people should know about?Sure. So, I think it’s important to set the context beforehand that there is a separation between the Rust Project and the Rust Foundation. The Rust Project controls the technical direction of the Rust compiler, Rust language, et cetera. The Foundation supports the Project in those decisions, including running an on-call rotation for crates.io and other things, but it is technically a separate entity.So, to get involved in the Rust Project, I recommend checking out the Rust language community site. That will guide you to where the community has conversations about different topics. This is how I got involved in the project years ago. And, it's a great way to get to know how the project works and where you might want to become involved.To get involved in the Foundation, you can go to the Foundation website and check out the "Get Involved" section where you'll find some information about becoming a member of the Foundation, potentially hosting your project at the Rust Foundation, and more. For both the Rust Project and the Rust Foundation, I would love to see as many people as possible at RustConf 2025, which this year is in Seattle. It's in my hometown, so I’m super excited about that. That'll be from September 2nd through 5th. Lots of Foundation people will be there, and lots of Project people will be there. It's usually one of the most positive-feeling technical conferences that I've ever been to, and I've been to a lot of them. So, it's wonderful to get that sense of community as well.Is there anything else you want to talk about that we didn't get the chance to talk about?I guess I would just say that the security problems we face are only going to get bigger and more complex. Organizations are going to need more and more to incorporate security into their software at the compiler level. So, being skilled in the languages that allow you to do this, such as Rust, is only going to become more valuable. That’s especially true for that lower-level software that runs really close to the hardware.I think you’re right. Personally, I’ve been seeing a lot of momentum behind that idea in, well, specifically defense actually. But, I think all of embedded- robotics, aerospace, etc. is realizing the value of the security that Rust brings. These are physical systems, and they are often some of the most sensitive systems in the world. They need to be totally secure. So, I think you're exactly right. I expect to see a lot of growth in Rust because of that. Thank you very much Nell. I really appreciate your time.]]></content:encoded></item><item><title>Introducing `eros`: A Revolution In Error Handling For Rust</title><link>https://www.reddit.com/r/rust/comments/1mw9jr1/introducing_eros_a_revolution_in_error_handling/</link><author>/u/InternalServerError7</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 12:28:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Everyone has weird niches they enjoy most. For me that is error handling. In fact I have already authored a fairly popular error handling library called error_set. I love Rust out of the box error handling, but it is not quiet perfect. I have spent the past few years mulling over and prototyping error handling approaches and I believe I have come up with the best error handling approach for most cases (I know this is a bold claim, but once you see it in action you may agree).For the past few months I have been working on eros in secret and today I release it to the community. Eros combines the best of libraries like anyhow and terrors with unique approaches to create the most ergonomic yet typed-capable error handling approach to date.Eros is built on 4 error handling philosophies: - Error types only matter when the caller cares about the type, otherwise this just hinders ergonomics and creates unnecessary noise. - There should be no boilerplate needed when handling single or multiple typed errors - no need to create another error enum or nest errors. - Users should be able to seamlessly transition to and from fully typed errors. - Errors should always provided context of the operations in the call stack that lead to the error.Example (syntax highlighting here):```rust use eros::{ bail, Context, FlateUnionResult, IntoConcreteTracedError, IntoDynTracedError, IntoUnionResult, TracedError, }; use reqwest::blocking::{Client, Response}; use std::thread::sleep; use std::time::Duration;// Add tracing to an error by wrapping it in a . // When we don't care about the error type we can use  which has tracing. //  ===  ===  // When we  care about the error type we can use  which also has tracing but preserves the error type. //  ===  ===  // In the below example we don't preserve the error type. fn handle_response(res: Response) -> eros::Result<String> { if !res.status().is_success() { //  to directly bail with the error message. // See  to create a  without bailing. bail!("Bad response: {}", res.status()); }let body = res .text() // Trace the `Err` without the type (`TracedError`) .traced_dyn() // Add context to the traced error if an `Err` .context("while reading response body")?; Ok(body) // Explicitly handle multiple Err types at the same time with . // No new error enum creation is needed or nesting of errors. //  ===  fn fetch_url(url: &str) -> eros::UnionResult<String, (TracedError<reqwest::Error>, TracedError)> { let client = Client::new();let res = client .get(url) .send() // Explicitly trace the `Err` with the type (`TracedError<reqwest::Error>`) .traced() // Add lazy context to the traced error if an `Err` .with_context(|| format!("Url: {url}")) // Convert the `TracedError<reqwest::Error>` into a `UnionError<_>`. // If this type was already a `UnionError`, we would call `inflate` instead. .union()?; handle_response(res).union() fn fetch_with_retry(url: &str, retries: usize) -> eros::Result<String> { let mut attempts = 0;loop { attempts += 1; // Handle one of the error types explicitly with `deflate`! match fetch_url(url).deflate::<TracedError<reqwest::Error>, _>() { Ok(request_error) => { if attempts < retries { sleep(Duration::from_millis(200)); continue; } else { return Err(request_error.into_dyn().context("Retries exceeded")); } } // `result` is now `UnionResult<String,(TracedError,)>`, so we convert the `Err` type // into `TracedError`. Thus, we now have a `Result<String,TracedError>`. Err(result) => return result.map_err(|e| e.into_inner()), } } fn main() { match fetch_with_retry("https://badurl214651523152316hng.com", 3).context("Fetch failed") { Ok(body) => println!("Ok Body:\n{body}"), Err(err) => eprintln!("Error:\n{err:?}"), } } console Error: error sending requestBacktrace: 0: eros::generic_error::TracedError<T>::new at ./src/generic_error.rs:47:24 1: <E as eros::generic_error::IntoConcreteTracedError<eros::generic_error::TracedError<E>::traced at ./src/generic_error.rs:211:9 2: <core::result::Result<S,E> as eros::generic_error::IntoConcreteTracedError<core::result::Result<S,eros::generic_error::TracedError<E::traced::{{closure}} at ./src/generic_error.rs:235:28 3: core::result::Result<T,E>::map_err at /usr/local/rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/result.rs:914:27 4: <core::result::Result<S,E> as eros::generic_error::IntoConcreteTracedError<core::result::Result<S,eros::generic_error::TracedError<E>>::traced at ./src/generic_error.rs:235:14 5: x::fetch_url at ./tests/x.rs:39:10 6: x::fetch_with_retry at ./tests/x.rs:56:15 7: x::main at ./tests/x.rs:74:11 8: x::main::{{closure}} at ./tests/x.rs:73:10 <Removed To Shorten Example> ``` Checkout the github for more info: https://github.com/mcmah309/eros]]></content:encoded></item><item><title>Is game development in Rust one big mirage?</title><link>https://www.reddit.com/r/rust/comments/1mw8k2g/is_game_development_in_rust_one_big_mirage/</link><author>/u/NyanBunnyGirl</author><category>rust</category><category>reddit</category><pubDate>Thu, 21 Aug 2025 11:42:12 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Not looking to be combative or rude or unthankful, but I'd like to be convinced of a strange observation I've been forced to make while looking for a game engine.bevy: Inherently tied to ECS design, constant breaking changes everyone warns about?piston: Alive as of a year ago?ggez: Was dead for a year, but new maintainer? :) Doesn't support Android or WASM github issuenannou: m5 alternative? Is this even an engine? Graphics engine?blue_engine: Graphics engine?tetra: Dead, unmaintained.rltk: Dead, unmaintained.quicksilver: Dead, unmaintained.lotus_engine: Super cool! Alive, tied to ECS design.oxyengine: Dead, unmaintained, ECS.console_engine: Dead, unmtaintained.rusty_engine: Bevy wrapper???screen-13: Vulkan... Rendering engine?gemini-engine: ASCII only?notan: This looks pretty cool, I think?Coffee? Dead. Amethyst? Dead. Dead dead dead dead. Unmaintained- unsound- 3d only- ASCII only- bindings, make your own wheel- ECS is god why wouldn't you want to use it? Cross platform? More like cross a platform into a river???Like... I want... to make a 2d game in a cross platform, rusty, maintained, safe engine, with the ability to not use ECS. I want to not have to reinvent a wheel myself, too. I realize I want a unicorn, and I would like that unicorn to be purple (I'm a choosing beggar), but like- is game development in Rust unserious? Bevy looks shinier than gold at least, and there's a lot of hobbyist work here for these engines for no pay in their freetime- I appreciate and love that eternally. (If you've ever contributed to any of these you're super cool and better than me, it's easy to be a critic.) Are my wants just too high? I see someone in another thread say "See! Look! So many game engines on this page!" They are dead, unmaintained, bindings, unsafe, not cross platform, 2d vs 3d land only, or married to ECS in a shotgun wedding.Please convince me I'm silly and dumb and fighting windmills. Maybe I should just take the ECS pill. But then everyone is saying the ground is ripped out underneath you. Maybe I should learn to stop worrying and love the Bevy- or perhaps just avoid threading in Macroquad. I don't get it. Footguns, footguns everywhere.]]></content:encoded></item><item><title>[Media] I made a game backup manager for the Wii using Rust and egui!</title><link>https://www.reddit.com/r/rust/comments/1mvqkb5/media_i_made_a_game_backup_manager_for_the_wii/</link><author>/u/mq-1</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 20:36:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/mq-1 ]]></content:encoded></item><item><title>Using large-scale search to discover fast GPU kernels in Rust</title><link>https://www.reddit.com/r/rust/comments/1mvoq0g/using_largescale_search_to_discover_fast_gpu/</link><author>/u/jafioti</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 19:29:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I'm building a GPU compiler for automatically generating fast GPU kernels for AI models in Rust. It uses search-based compilation to achieve high performance. https://github.com/luminal-ai/luminalIt takes high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: https://youtu.be/P2oNR8zxSAAOur approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.The aim is to radically simplify the ML ecosystem while improving performance and hardware utilization. The entire library is statically compiled into a single Rust binary. Please check out our repo above and I’d love to hear your thoughts!]]></content:encoded></item><item><title>Talking To Zed Industries- Makers Of The 100% Rust, Super-Performant, Collaborative Code Editor</title><link>https://filtra.io/rust/interviews/zed-aug-25</link><author>/u/anonymous_pro_</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 16:13:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I think a lot of people in the Rust community will know that Zed is a new code editor written in Rust. We're all super excited about it. So, I wanted to start with a question that I think might feel overly confrontational, but I just wanted to phrase it this way because I think it might get an interesting answer. The question is, why do we need a new editor?That's an excellent question. For one, I think we need a new code editor because of live collaboration. This was the reason I joined Zed in the first place. Developer tooling has been built with certain expectations and certain models for a very long time. But, in order to get the sort of live collaborative experience that you see from something like Figma or Google Docs, you have to bake-in the right data structures and processes from the get-go. You need to build your entire code editor on those data structures and concepts in order to get them to work. In a lot of other tools I've found that you can feel a little bit of a mismatch between the CRDTs (Conflict-free Replicated Data Type) and the other data structures over here and then the fundamental code and the actual data structure that renders on your screen, which might be over there. So, that's the collaborative angle.Another reason is just that technology changes and time marches on. Electron is a really impressive piece of technology that solved a really hard problem. It’s now been quite a while since that problem was solved, and we have new tools and new possibilities, specifically Rust. We believe that Rust makes it possible to have the same level of ergonomics and support without the 200 megabytes of embedded V8 that every desktop app is carrying around nowadays.When I was doing some research I found a statement from someone at Zed talking about how the editor is built like a video game. Does that play into what you were saying about live collaboration and stuff?Yes, fundamentally those CRDTs and the underlying tech for synchronizing have a cost. You need to track a little bit more data because you're doing a more complicated problem. Also, specifically with code editors, the bounds on the problem in terms of resource utilization, like memory and file handles and all that sort of stuff isn't under your control. For example, if you ask us to open a hundred files, our job is to open a hundred files or die trying. So, we're kind of stuck in this little triangle where we have to do something that is a little more expensive than it would be otherwise because of the live collaboration features. And, the amount of work we have to do is not under our control. So, we have to make sure that what little we add on top of that is as small as we can make it. And on top of that, we want to make a fast, nice app. We live in this all day long.To solve those trade-offs, the founders really had to go back to the beginning and be like, "Okay, there's all this Electron, there's all this JavaScript, there's all this stuff. Let's just toss that aside. Let's make this as small as possible and then build up from there." For example, we've been continually improving GPUI. But, we don't want to have to lug around all of the weight of the way editors have been built and tackle these new problems. We want to simplify. And, if that means we have to do more work or we have to build it like a video game or use tools from other realms of software, then that's what we do to solve this problem.One of the other things I saw when I was doing research is that the founding team of Zed has a crazy amount of experience to lead them to this moment in their careers. Can you speak to that a little bit?Zed is a special project at a special time, because it's one of those things where all the stars aligned over multiple decades to make this thing possible. Nathan (the CEO) led the Atom team at GitHub. He was core to that whole thing. That project of course led to the creation of Electron and the current state of desktop app development.Nathan and Max both also worked at Pivotal Labs before being on the Atom team. They did a lot of pair programming together there, and that has carried into our culture today. Also, Max worked on Tree-sitter, which is one of the core components of almost every syntax highlighting or code parsing tool made in the last 7 years. Antonio also entered the mix when they were working on Atom. If I remember the story correctly, he just started submitting really good PRs for Atom, so Nathan brought him in and they became good friends. But the key here is that all three of these guys live in different places. Antonio is in Italy. He's Italian. Nathan is in Boulder, CO. Max is in Oregon. I think the fact that they live far apart contributed to a lot of their initial research into CRDTs. I believe one of the early live collaboration tools for developers was actually an Atom plugin called Teletype, also by the GitHub Atom team.That's awesome! Who you're working for makes a big difference, and that must be such an awesome thing to be working with a really specifically experienced team like that.Yeah. And, it's hard to overstate how important collaboration is to Zed. I think one of the reasons we function so well as a company is because we are collaborating and pair programming every day. Though, it’s not quite pair programming. I call it parallel programming. We're sharing context, figuring things out, and understanding the problem together. If you look at our GitHub repository you don't often see very many code reviews, but you will see multiple heads on every commit. That's because we're spending all day together, understanding the problem together.So, are you personally in the collaborative feature of Zed for a large part of your day?Yes! I am almost always pairing with somebody. We always try to make sure we can pair. Even if it's like European evening and American morning we will pair for an hour or two to talk about the problem and then go off and do it separately. So, most programming is done together. That said, there are a lot of things that aren't programming, such as reviewing PRs from external people, that might be done a little more solo.I feel like when I've done a lot of pair programming, it just makes the emotional load of taking on something that feels hard so much easier.Yeah, you feel like you’re in it together! You also usually have a slightly different perspective or slightly different skills that can complement each other. I will say, what we do is not classical pair programming. For a long time, pair programming has been two people sitting at one keyboard. One of the key things about collaboration in Zed and other tools is that you're in your code editor, with your theme, your settings, and so on. You just happen to be visiting their file system and utilizing their language server and stuff like that. So, it's actually a lot more comfortable and a lot more like bouncing off each other. I'll often be like, "Okay, let's split up. I'll implement this part. You implement that part." It's a good flow.So, speaking of taking on hard things, to me building a code editor from scratch seems really hard. Everything that you explained in terms of the reasons why you would want to build a new editor makes total sense. However, there's all these other people building new editors. And, most of them are forking VS Code because they're getting a bunch of stuff for free by doing that. How do you guys think about prioritizing what to build? There's all of this stuff that you have to add to the product just to catch up to the level of features that people expect.That is the question at Zed. It’s the question we spend most days thinking about in one way or another. It's both a blessing and a curse of working on a code editor because there are so many things we have to do. This has meant that oftentimes our decisions are driven by what we're interested in. That was especially true when we were starting out. It's not really the case as much these days. But, when we were starting out, whatever you wanted to get done got done because everything had to get done. So, when you don't have fold indicators or indent guides and you have a little idea of how to do fold indicators, you do fold indicators instead of indent guides. These days it's a little bit different because we have done a lot of that low-hanging fruit. We are still working on how we make decisions for all the stuff that isn't like a big headline project. It's easy to talk about Windows or the agent panel or the debugger. These are very clear things. We can put them on a list, and we can figure out the order we want to do them in. But, where do you put “the debugger doesn't work on my Python project that uses distributed venvs” or whatever the little thing is?I guess this is a good time to talk about our approach to AI. When the ChatGPT demo came out, Nathan immediately started reading the white papers about how it worked. We all started experimenting with AI and learning about it. You know, some of what we do is interest-driven development. Nathan got really excited about AI, and people in the community were really excited about it too. So, there was just this massive pressure to do something about it. We initially took an approach with AI that I think was very sound. It was all about controlling the context. So, you could control everything, even the system prompt. It was all there. It was a very, very thin wrapper around the LLM APIs. It seemed great, but people didn't like it that much. Then, the agentic stuff came out and agents became the de facto way. Part of it was us being like, "Okay, this is all happening. We also need to keep up. There's a moment right now of people switching editors. We need to make sure we are an option for them to switch to."So, how do we choose what to build at Zed? There's a little bit of strategy to it like "Hey, right now is a great time to do an agent. Let's make sure we have an agent out really soon." There's a little bit of personal desire. Something that's really important to us is that we are making our dream code editor. If there's a dream feature you want, you put it in there. So, there are a few different forces at play.You mentioned that you're building your dream editor. That's a very cool thing about the opportunity to work on Zed. It's very rare that you get to work on something where you are also the user.Yes. All of your friends are users too. For a while there, I was doing dad-driven development where every few months I would go visit my dad and show him Zed because he's a software developer as well. He'd look at it and be like, "Oh, there's a problem here. Oh, you can't adjust the panes of the window or whatever," and then I would spend all of the next week making resizable panes and stuff like that. It's fun to be able to do that for myself and for the people in my life.I think dad-driven development is great. So, I see interesting new feature releases and things from you guys all the time. I wanted to give you the chance to call out any exciting highlights, whether it's business-wise or product-wise. Any exciting milestones that you've hit recently?In the last few months, we've actually released a ton of features. It's been a pretty crazy go, go, go time at Zed. We launched the agent panel a couple months ago. We also launched an integrated Git client, which turns out to be something a lot of people really like. We kept getting the feedback that people wanted integrated git. Being a lot of people from GitHub, Zed is full of Git CLI power users. So, we were surprised by that one. We also just launched a debugger. There's been a lot of feature releases lately that have really tipped Zed from being a work in progress to being close to general purpose. I think the most exciting development that I can talk about though is Windows. We are now properly staffed and committed and driving forward on Windows. There are some core technical things we're working on because the Windows platform is so different from macOS and Linux. I don't know if you've seen it, but we have this cute “Windows is coming” sort of website (linked below) that does a Windows 95 experience. Our Head of Marketing vibe-coded the whole thing in Opus. So, Windows is happening. Windows is coming. I promise.One of the things a lot of people talk about with Rust is the cross-platform capabilities that it brings. Has it been relatively easy or relatively hard to target the different systems?We made the problem harder than it would be for a lot of other projects in the space. Zed likes to build its own tools. We like to make them narrow and sharp for exactly what we're looking for. Also, the project that would become Zed was started back in 2018 before a lot of the Rust ecosystem was fully matured. So, it turns out there's a lot of little things that Zed specifically as a UI application runs into. For us, the big reason that Zed isn't cross-platform is because we are an integrated development environment, meaning that Zed needs to talk to your window, your GPU, in fact it needs to talk to every GPU you can plug into with every Windows and Linux distribution. Or, ideally it would. Obviously, it doesn’t work perfectly with everything, but we try to cover as much as possible, especially for recent tech.On top of that, there's a lot of platform-specific things you want to do in a code editor like opening a URL or minimizing and maximizing the window. Because we've built gpui from scratch, we have to go to each of the platforms and figure out how to do it. There is code that we could have used, but we wanted to just go a little simpler than what’s out there. 2D UI is complicated, but it doesn't need the same things as a 3D scene in a video game. So, we're building all this ourselves and we don't use Winit or WGPU or any of these other things that have different goals. So, that all makes things more complicated. We haven't had any problem with Rust specifically. As you know, Rust is fantastic for all of this. The big problem has just been learning things like which random Objective-C function you have to call to make the window transparent on macOS.Right. It's all the platform-specific APIs that you have to figure out how to use. That makes sense.The one we've had the biggest struggle with is definitely talking to your GPU because GPU drivers have a lot of random problems and people are supposed to keep their drivers up to date. Obviously, we targeted MacOS first, which has actually been very nice as a first target. Apple is so tightly integrated that we don't have problems with GPUs nearly as often. We had it a little bit in the beginning for different displays and stuff like that. But, at this point, Apple has just solved the problem for us. We have to take on more complexity for the other platforms.You also mentioned the launch of the Agent Panel. I don't want to just breeze by that because obviously we have this huge agentic editing war going on right now. It's pretty crazy out there! I guess I just wanted to ask or give you an opportunity to point out what you think is different about the Zed approach.I think the fundamental problem with LLMs and the LLM integrations we're seeing everywhere is that there's no one to hold responsible. Responsibility is a really key part for how human society works. When something bad happens, you go talk to the person who is responsible for it. You can't do that with LLMs. LLMs can never “hold the bag”. They're not people. So, no matter how powerful and how crazy inventive and amazing these LLMs get, somebody's got to put their reputation on the line. That somebody is going to be a human until the AI labs figure out a fundamental change. Someone's going to have to hold the bag. And when you're holding the bag, you want to make sure that the code you're putting out into the world is working. To do that, you probably want to run some analyses on the code. That might be something as simple as compiling it, doing searches on it, or seeing what the language server has to say about it. All of those things are where we come in. We're here to make that the best experience possible. So, that's how I personally think about code editors and agents out into the future.Nowadays it seems like everybody's building their own CLI agent. I think some of this is just things concentrating up into the big AI companies like OpenAI or Anthropic. That makes some sense, because I think they understand LLMs the best and can write the best prompts. They understand how the whole thing works from top to bottom. I think the reason terminals specifically have gotten so big is because they're easy to make UI for, they're low stress, and they already automatically integrate into everybody's existing tools. In terms of our agent panel, I think terminals don't make great UI. So, I think there's a lot of room for us to coordinate with the big AI labs and use their knowledge of prompting and how these models work. But, I think over time we're going to see more negotiation around who is owning which part of the process. People like GUIs and left the terminal for a reason, so I think it’s unlikely they’ll go back to it in a big way. We'll see what happens.It will be interesting. So, what are the more interesting things you've gotten to work on in your time at Zed?The thing I find the most interesting personally is gpui, our UI framework. I am personally invested in making it general use because it is already very general purpose. Anybody can use it. But, there are a lot of things missing that would make it straightforward to use. For example, we don't ship an input component because we built all that as part of our editor. The UI framework gives you the tools to make one, but we don't ship one. So, I think the thing I find most interesting is taking this custom tool that no one else knows how to use and learning how to make it sing. There's been a long history of UI frameworks out there. When I started out, I spent a lot of time working with jQuery and HTML in the DOM. How can we do better? So, I find it super interesting and challenging for me to build a UI framework with all the tools Rust gives us instead of all the tools that JavaScript and HTML give us.I would say another really interesting aspect of this has been working on the terminal. It’s actually the first thing I built at Zed. I was hired as an intern at the time for a three-month summer project. I was like, "All right, let me see what's in Zed. Oh, wow, there's not a lot here. Looks like you need a terminal." It was on their to-do list, so I just decided to take it on and get it shipped. I learned so many cursed facts about computers through that project. Did you know that there has been protocol compatibility from typewriters all the way to every modern IDE? There is this one character called the bell character which exists because there used to be a bell on the typewriter. When you sent the bell character, it rang the bell in the typewriter. Then there’s this whole evolution where Morse Code was turned into something the name of which I can’t remember (ed: telegraph codes) that turned into ASCII and then UTF-8. There's this 100+ years of history of communication protocols that are all backwards compatible with each other. That's so cool and weird. It's an interesting feeling being in the age of LLMs but also deep in the past where they rang a physical bell. That juxtaposition brings me a lot of joy.Yeah, that's awesome. Usually I ask people where Rust fits in their stack, but I think with Zed that answer is pretty straightforward.100%. Yeah, the whole thing!So, why is Rust the right language for this task?Rust makes projects like Zed viable. There are IDEs that were written long before Electron, and there will be many IDEs written afterward. But, a lot of them use a system-level programming language for one part and then write the rest in something else, because it turns out systems programming sucks real bad because of pointers and segmentation faults and so on. So, there's this pattern of, "Okay, we've got this little core thing in the system language, now let’s bail. Let's get out of here, because, oh boy, if I have to deal with another segmentation fault, I'm going to die." With Rust, we don't have to do that. We do the full stack in Rust because it is actually reasonable to expect you can do that in a reasonable amount of time without your program crashing all the time. Rust has really solved some fundamental problems. Would I say Rust is the perfect language for this project? I would definitely say not. There’s no perfect. But, it’s great and getting better. There's this Rust ergonomics initiative that I'm extremely excited for. If we got easy clones and partial borrows, I think it would be a whole new language for us.Rust has that unique ability to be applied to all the different layers of the stack reasonably.Yeah, and we actually use this really heavily. For example, in our testing we have ordinary-ish Rust tests that spin up multiple clients, spin up a server, and have them talking over fake network channels. We do property testing by ordering some messages over these different things. It’s things like "Oh, person A opens a buffer, person B jumps to a line while person A deletes that line," now check, "Is everything what it should be?" To accomplish that, we just use the same code we've been writing. We just call the internal APIs you would have called anyway. We don't need to do anything extra to have this sort of communication. They're just Rust structs. We just call methods on them. They're orchestrated by our framework, and there's a server that makes sure everything is happening, but that is a lot easier to write than if you’re spawning processes and passing messages because there’s a bunch of different languages involved.So I’m told Zed is hiring?Zed is hiring. We are.What do you guys tend to look for in new hires?We tend to look for three things. Obviously, you need to know your stuff. If you're able to think through problems and understand constraints, write good code, and test it well, those are kind of the fundamentals. One of the things that I see people have more trouble with is that Zed is a very social place. Engineers tend to be a little more introverted, so this could be a challenge for some. It's really important that we pair like I talked about before, because that's how information transfer is done. It's how Zed works. So, it’s really important that you’re a good communicator and a good person to work with.The other thing that can be a challenge for some is that Zed is really set up for self-starters. We have very little process. We have very little hierarchy. There’s basically the founders and then everybody's just kind of in a big pool. People might have specific roles, but there's no managers really. There's none of that structure. This is great if you want to get a lot of cool things done, but it also means that you need to have a certain amount of a self-driven mentality. I think the ideal thing is if you come to Zed and you really want a particular feature and you can actually just get it done. If you can do that, you’re like a perfect candidate. If you're capable of understanding the code base to do that, that's amazing. Also, if you're able to talk to us and be like, "Hey, I really want to add this. I think such and such way of doing it would work. What do you all think?" Then we can have a dialogue and work through a solution. Nothing happens at Zed if you don't make it happen. We're also a growing company, so things are always changing.My next question was about culture, and you kind of hit on some things there. You mentioned the social, self-starter type culture and the flat of flat hierarchy. You also mentioned the distributed nature. People are all over, and you're always trying to sync up schedules to pair program and stuff like that. Is there anything else unusual about the culture that you would want to call out?Zed is very, very lively. There's a lot of discussion. Discussion is honestly the number one thing. There's a saying that says that you should have "strong opinions, weakly held." Basically, you own your ideas and believe them, but when someone has a better idea you’re able to recognize it and change. For me, the way I approach this is I trust that there is ultimately a fast, good way of doing something, and everyone is collaborating in good faith to find it. I think that's a really killer part of the Zed culture. People stand their ground but also know when to move positions.Okay, so here’s my last culture question. Is there anything unusual about compensation or benefits that would be interesting to point out?Back in the day when the product wasn’t open to the public, I used to say that one of the benefits was that you got to use Zed. That was fun. Let’s see… There's unlimited time off. So, basically as long as the work gets done you can figure out what you need to do. One thing we've been doing for the last year is going to conferences together. It's always a really good time talking to our users and showing off Zed. We usually organize our company all-hands around Rust conferences so we're all in the same city. It's been kind of funny a couple times because we have so many people. We'll have like 15 people getting together at one booth at Rust conferences. We often kind of take over a little corner of the space because there's way more people than we need.Here's another unique benefit. Zed is all about writing code. So, if you’re someone who doesn't want a lot of meetings, we got you! Also, we're an open source GPL project with VC funding. That’s a unique mix! Obviously we’re not the only startup in that situation, but it’s not common.Actually, you know, you reminded me of something that I should have made a note to ask. Usually when a company is open source and in startup mode, one of the questions is what's the revenue model?We get that question all the time. It's a little hard to answer because we haven't really tried to do it yet. But, there are three things we're looking at for revenue right now. One of those is token reselling. We can kind of be your one-stop shop to buy tokens for programming. That one might not be super sustainable based on various things happening out here. But, it's nice to have.Another option is a closed source fork that has enterprise features, things like single sign-on, custom deployments, control over extensions, etc. There's a lot of things that we do not want to put in the regular product that an enterprise will need.The third thing though is really the big vision, and that is collaboration. Long-term we really want to change how software is done. We really believe in CRDTs and getting these in at the foundation of your experience. For example, the edits going into a pairing session and the transcript of the session should all be context for LLMs. You should be able to take a slider and slide back and forth and watch the project be undone and redone. So, you can start to imagine a more CRDT-based version of GitHub. That is the long-term 10,000-foot vision. The reason we're a startup and not just a nice open source project is because that's a big vision. It's one that starts with having an amazing code editor. We think that once you have that code editor you're already at the right spot to take your normal files and turn them into CRDTs. Then you can do all this crazy stuff with them. There's a lot of this vision that isn't hashed out yet, but the long-term goal is CRDTs and cloud services based on those CRDTs along with live collaboration and other things you can do with keystroke-level granularity that you can't do with Git.That was kind of the end of what I wanted to ask about, but I always like to ask if there's anything you wish we'd had the chance to discuss.So, one thing I'd love for more people to know is that we have kind of two ways you can get hired. Obviously we have a normal hiring process. You can go look at our jobs. There's a bunch of jobs on our Zed jobs page. But, we've also had a lot of success hiring from open source as well. We'll have some really smart people come in and just start making PRs and adding features. We've made really good hires from open source contributors. So, maybe if you feel like your resume doesn't have what it needs on it, or if you're just uncomfortable with the uncertainty of throwing it out into the void, I would really encourage you to just get our attention by showing up and doing good work. I know Conrad and probably a few others also make a habit of publishing calendar links where you can just get some time to pair with us. So, there’s always a path into Zed that starts with just showing up, talking to us, and doing good work. I'm not asking for free work at all. I'm just saying this is an option that exists if it works better for your situation than the normal application review.I love that you pointed that out. That is so cool for people that maybe don't feel like their resume is going to really communicate their potential. They can just submit a pull request.Yes, submit a pull request and talk to us. Even more than sending a pull request, talk to us. Another thing I should mention is that part of this collaborative vision is this fun channel feature. Those are public, and we spend all day long jumping in public channels and working together. So, you can just ride along. We're an extremely open company. Thanks so much for your time Mikayla!]]></content:encoded></item><item><title>Const Trait Counterexamples</title><link>https://dbeef.dev/const-trait-counterexamples/</link><author>/u/fee1-dead</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 14:47:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hi. I'm the lead for Rust's const traits project group. We hope to stabilize const traits soon, but this is a complex feature with huge amounts of design considerations, and we keep getting the same comments from different people who probably have less familiarity with the feature and its design.That's quite fair because I can't require everyone to have followed every discussion everywhere. I have followed loads of discussions, so this summarizes some of the counterarguments we've had so far.If you're interested in the language design for how we plan to allow calling trait methods in const contexts, this should be a good complementary resource to the currently open RFC. You might disagree with parts of this post, though.Declare a trait as  so you can use it in trait bounds:Make impls  so we can satisfy those bounds:For generic const code, a  trait will be made available (not necessarily included in the first stabilization) to allow the type to be dropped at compile time: is true for all , while  only holds if the compiler knows its destructor can be run in compile time. (See proposal 4 for more on this)We use  throughout this document but it is feasible to switch this to any other compatible syntax (like  or ). In fact, the nightly Rust as of writing accepts both  and . I'll stick to  for simplicity. is a modifier on trait bounds. (including super traits, e.g. const trait A: ~const B {}) In general, they only need to be proven if you're using them from const contexts. Hence they're also called "maybe-const" or "const-if-const" bounds. bounds on a function are only enforced when you call it. This allows us to instantiate a function in const contexts even if we can't call it:But note that constness isn't a generic parameter and shouldn't be considered instantiated for each call to a : If you think about the function  and pass in a  that does not implement , the constness of  does  change due to the unsatisfied const predicate -  is "always-const" (and not ) in a sense that it simply imposes additional constraints if called in const contexts.
The call above errors not because  becomes non-const when  - it errors because MyType<u8>: const PartialEq isn't satisfied.With the current proposal and model, we now explore a few alternatives that Rust team members have thought through, and I will explain why they can't really work.Why not make  the default and use an opt-out for non-const bounds?It is expected that people will write  bounds a lot. What if we made  implicitly const-when-const if it's inside a const item? i.e. making the following work:Users of non-const trait bounds in  will now use an opt-out syntax such as :Why can't this work? The very first drawback this proposal faces is that these things are already possible on stable today:If we ever wanted to allow // to be callable in const contexts, then they should follow the same opt-out, necessitating the following change:This then runs into multiple problems.1A: editioning everythingWe already allow trait bounds that mean non-const in . This means if we were to change  in  to what we currently mean by , we have to do it over an edition since it is a breaking change.But of course we can't make these changes immediately. Suppose we're currently in the 2024 edition and we accept this plan to migrate everyone to use  where necessary. We can accept  in any edition as it is equivalent to  now. Now in 2026 edition we deprecate non- bounds, we emit a warning everytime someone writes  but means . In 2028 edition  now means  (implicit behavior). We need to do this change over two editions because just doing it in one edition is even more problematic.Suppose  becomes a const trait. This poses issues for crates in edition 2024 or below, who have written this:It is dangerous for crates having non-const bounds that never went through the intermediate 2026 edition to directly migrate to edition 2028, as edition migration processes primarily rely on things compilers can catch. There are things that  can't catch, and there are people who upgrade editions by simply bumping the number and fixing the issues that arise with bumping (how can you fault them? I personally never thought editions could significantly change behavior and meaning of syntax).This is a major meaning change (perhaps bigger than disjoint closure capture and if-let rescope which won't break/change too much) because there are tons of  with trait bounds that all currently mean non-const. Their migration path to a potential 2028 edition is very scary to me.
1B: non const traits and implicit boundsOn the outset, there appears to be three choices:Compile this, with  conditionally constCompile this, but  doesn't become conditionally const (not )Make this a compile error.#1 can't really work due to breaking change complications when methods change their bounds to  (see 2A),
#2 can't work either because it would make it a breaking change for someone to change their trait into a const trait (the  becomes stricter if  betrays its name and becomes )So the only viable choice is #3. However, this has the ability to confuse many people, specifically with 1C and 1D.Should  blocks also be a part of this elision? That is, should the following have a  bound applied to  but  bound applied to ?If yes, this can be super confusing. 1B will turn this into an error on  if  is not a const trait. It's also a candidate for accidentally stricter bounds than necessary (when will users know when to use ? cc 1D)If not, it can also be super confusing. The rules for when  implicitly means  would be complicated, hard to teach, and slightly inconsistent, given that something like the following  have implicit  at the impl-level.All of this is mostly because of the weird double meaning of  (on non-const contexts, equivalent to  in const contexts, but  in )An explicit opt-in scheme, with  bounds at the  level, if allowed, would not have the same issues, as it is clear what the user intended, and it is clear that it would apply to s.Proposal 1 will make writing stricter bounds the default, as  in  is stricter than . It is only with non-const traits (1B) where a user will be prompted to use .This is a broad assumption that users will most of the times want , similar to  vs. , but probably less correct than the latter. There are many constructors with trait bounds (e.g.  with ) that don't need to call methods at compile time. They can be
intended for storage (so the stored types' methods can later be called at runtime), or just using traits' associated consts.On the other hand,  as opt-in would be required only when someone attempts to call a trait's methods (easily suggestable by compiler errors), which would leave people more naturally using  to mean non-const if they don't need .Remember the snippet at the beginning of this proposal which contained function pointers and dyn traits, and impl traits? Well.. about those.Suppose in the future we might want to allow  or  pointers. Notwithstanding the potential complexity for the compiler to support these, but let's say struct A(pub ~const fn()) is possible, and it means that the field must be a function pointer callable at compile time if  is being constructed at compile time.  operates similarly.This then means an implicit  version now has to either (1) affect all types containing  pointers and s by making them imply  or (2) create an opt-in syntax specifically for these things.(1) is self-evidently problematic; (2) feels extremely inconsistent, why use opt-in in some places but opt-out in others?proposal 2: selectivenessWhy have const apply to the entire trait?This has many layers to unwrap: why do we have to mark the trait at all? Can we have  choices? Can we do refinement with bounds on specific methods?We'll answer these in this section, but we'll start with the most general fact: If we want s to be  or non-const, there must be a way to distinguish a trait that allows const implementations and a trait that does not. And all current traits (before const traits stabilize) must not allow const implementations.2A: Which Gender Is Your TraitFor any generic parameter  where , we can't really call . Right now the bound on  is  but it should really be S: ~const Sum<Self::Item>. If we allowed calling , and the bound later turns , it would break if  remains having a non-const  impl. So to avoid breaking changes we have two choices:Allow  on non-const traits but can't call any of the methodsDisallow  on non-const traits.Our current design uses #2, as #1 feels quite counterintuitive: the idea of  bounds is to allow calling methods on them, so it isn't really useful. It would also prevent any actual uses of generic items with  bounds on non-const traits, as s cannot be written without the trait being . With #2, we're able to give trait authors  chance to make sure their bounds are either non-const or  as they see fit. As once the trait is , turning existing non-const bounds into  would be a breaking chnage.In any case, the compiler must know what is a  and what is not.The alternative here would be to allow  bounds on  without it being marked a const trait, but not allow calls to methods until the methods are marked const in some way. (i.e. per-method constness)This has its own caveats.First, this means there are never-const methods in a trait. Some methods allow callers in const contexts and require const implementations while some do not. I don't think that's really useful. If a user writes , they'd normally expect every method in  to now be const-callable. It's quite rare for a trait to be designed in a way that allows some methods to be callable in const contexts while others not. (at least, no one has ever provided a concrete example) Those use cases would be covered by separating them into two traits anyways.Second, we  need a way to figure out whether a trait is  to decide whether to allow s. We can't allow s for non-const traits, to allow existing non-const traits to transition their methods into const.But that means once  makes  and publishes a new crate version, they've lost their ability to make  in the future, as downstream crates would happily do this:That is super awkward to both teach and learn. This awkwardness was even more extensively discussed in my HackMD doc from four months ago.Okay, so, it's really awkward for fine-grained per-method constness to co-exist with whole-trait constness (necessary for trait bounds as well as whether to allow s). What if we dealt away with whole-trait constness entirely?Consider  bounds on methods. Some scheme like:Where s can't be wholly  or non-const, but individual methods can become  on their own:But that proposal has a  of downsides.Because all methods' signatures are now lying to you. Consider the common case of some impl using a generic type's methods:in Option<T>::some_fun_method, we have no idea whether we can actually call , without writing a bound. Making this bound apply to the entire trait impl seems awful to require (defeats the entire point of an individual method being  and writing  bounds on individual methods), so then this per-method bound now applies to . That also means the requirements for a particular method to be  may be stricter than what is written on the trait method signature. ( has no additional restrictions whereas <Option<T> as C>::some_fun_method does).This is pretty much not avoidable, as traits upstream cannot know what methods downstream impls want to call (in this case,  is not even nameable at the upstream trait )Therefore, all const fns that attempt to call some trait methods on a generic parameter must add a bound on that specific method.This is particularly frustrating for , which has loads of extension methods (that can all be overriden to do something non-const, under this scheme), and bounds are necessary for all methods being called, resulting in the following for a function that should have been super simple:This proposal might also need special annotations on trait methods that provide a  default method body but otherwise doesn't require downstream s to be  (otherwise you wouldn't know which default method bodies are callable from ). That adds an additional layer of syntax complexity which needs to be designed.There was also a bit of Zulip discussion on this. See this topic and its discussion, mainly before May 8th.proposal 3: isn't it just ?Why not use  for const-when-const bounds?This is one of the proposals which actually has  merit. The idea is to use  for the "const-if-const" syntax, while thinking about the future with  or  to represent always-const bounds.Because it turns out we actually do need always-const bounds, for the trait bound to be used in an assoc const-item , in a const block , or in const generic arguments . Those could become usage sites that require a stricter bound than , so we must think about reserving the "always-const" bound for them.My main reservation with this proposal is that it sort of justifies a change of const items, const blocks into using the same new syntax for always-const bounds. so const(always) X: i32 = 42; and const(always) { 2 * 3 * 7 } instead of what we have now which reserves  for always-const to keep the consistency. But we always have  form of (small, potentially acceptable) inconsistency one way or the other (such as  in , see proposal 6), so it might end up being an option we end up choosing.proposal 4: destructive interferenceWhy ? Why not just ?We are in a special place because some destructors are non-const. We already allow s, so there needs to be a way to distinguish types that can be dropped in compile time from types that cannot.This leads to us to a new marker trait called , which is automatically implemented. Non-const  holds for all types, but  only holds if the type's  impl (if it exists) is const and the type's components all implement .Therefore, you must sprinkle your functions with  bounds when you're constifying them, if generic parameters need to be dropped at any point in the function.Why not ? Well it makes a huge asymmetry with  bounds, as the latter would only hold if  has a manual  impl. It would also make  not imply , which has weird language-level implications as well as compiler-level implications. (we have run into trait bound cache issues with this in the past)
 is also a valid bound even though you might not have seen this. uses this bound to prevent any user from writing their own custom destructors on their types. So no matter what scheme we end up choosing, we  have at least two traits. One to represent the ability to be destructed, one to represent whether or not a type has a custom destructor. Otherwise we run into asymmetry between  vs .proposal 4.5: destructive interference, part 2Can we make  implicit?This is hard to say, depending on what is meant by "implicit". Inferring whether requiring  for generic types based on whether the code has a possibility to drop the type is no-go, because changing the type signature/trait bounds based on the body has loads of bad semver complications and is generally not possible in the compiler architecture.Making  implicit for all generic params is not good, either. Many generic API interfaces want to assume as little about their types as possible, so a broad scheme like this necessitates an opt-out syntax which seems too odd to have/hard to design.A more limited plan might be to infer a  super trait if some methods on a trait take  by-value, that also has issues because the following example means shouldn't have const trait Add: ~const Destruct inferred:One possible plan (will likely be our actual plan) is to lint const traits that take  by-value and recommend adding a  super trait. That way it will save crate users relying on that trait to not have to add  bounds everywhere, at the same time not assuming everyone wants this.proposal 5: academic zealotryLet's follow the approach used in academic paper X..Academic research is exciting work. Language design in Rust is different (maybe slightly boring?) because it's mostly a weighing of the pros and cons of every possible alternatives and finding ways to practically make the language more capable.The work on const traits in Rust is often linked to work on effects in other (academic) programming languages, results published via research papers, etc.We then often see an excitement to apply those academic thinking models to Rust.Those thinking models often appear super convoluted to me. It sometimes looks like practicality has been dismissed in favor of generality. That's fine, but I don't think they are very compatible with Rust.I think it would be fair to consider the role of formal modeling of effects as a different programming language and how programmers using that programming language model their functions and their ability to be called in different contexts. (i.e. runtime vs. compile time)The academic way of thinking about effects/const traits holds the same amount of weight as some other programming language's way of thinking about effects/const traits to me. It's nice to try to incorporate the bits that would work for us, but incompatible things are incompatible. Forcing Rust's model to be losslessly transferable to a formal model is equivalent to forcing Rust's model to be losslessly transferable to a different programming language, say Zig. We can't assume that ideas about programming languages suddenly become applicable to all programming languages just because those ideas are published to a peer-reviewed journal. Our RFCs are always peer-reviewed, too.It is cool to think about effects or capabilities and how we can encode them as modifiers to entities in the type system, be it traits, trait bounds, functions, impls, etc. We're not in the best place to unify them because  is the inverse of an effect (it prevents you from calling non-const items) while  is an actual effect (it allows you to call other  items). At the same time  seems like it wants to apply to the whole trait, while partial maybe- traits seem very desirable.If we wanted to unify them to make our version of effects closer to formal modeling, we must do so for everything that we currently have, not just const traits. So we should also think about whether it is really  to do so in the first place, and then decide whether or not we can proceed with a stabilization of const traits with the pieces that make the most sense for the language , without having to make our effects story consistent first.proposal 6: drowning in conditionalsWe should have  and  and , etc.The idea here is that  is "conditionally const", i.e. may or may not require const impls depending on whether called from a const context. Given that  is also "maybe-const" (i.e. could be called from both non-const and const contexts), we should make them also use , like  for consistency and also distinguish with  items.My biggest feeling here is that it changes up everything for no gain. I think rarely anyone benefits from this. It might feel consistent but I'd rather not let us be consistent for consistent's sake.But still, there are other consistency arguments that contradict this. , , and , and const items/const blocks all restrict their bodies to operations performable in compile time, i.e. they must all call s. In terms of restriction they work mostly the same except for which trait's methods they can call (in  you can call methods from  but in  items you can't)Also, it makes little sense to have an "always-const" fn such that it cannot be called in  fns. But that's what  seems to imply exists.The other idea here relates to the "meaning of " section. The constness of the  never changes. It should always be evaluatable at compile time. But when you pass in some  into const fn foo<T: ~const Tr> that doesn't , and try to call it from compile time, it's not that  now is non-const ( is fully prepared to be evaluated in compile time) but that you aren't satisfying 's constraints.proposal 7: questionable conditions is okay for "maybe-const".. right?We have received proposals like this because  introduces a new sigil, and the alternatives don't seem to be as good. Why don't we use  to mean ?The main reason here is that  has an existing meaning in trait-bound adjacent areas, which is to  a bound, or to  a default, such as .  is a stricter bound than , so using  for it isn't really nice. This is also the reason proposal 1 (and the ancient implementation before it got switched due to issues aforementioned) uses  for opt-out.Using  for opt-in, on the other hand, isn't a good syntax proposal.let's try picking wavelengths for these sheds..These are things we should actually try to form consensus on before stabilizing:Syntax of the const-when-const bounds. There's  and  and We can figure this out along with possibility of proposal 3 ( => ,  =>  or other) on the table.Order of keywords - whether to use  or Naming of Not really discussed anywhere, but we might want to find a better name for  if we want to stabilize it. ? ?These are some features that are  essential for const traits. Including them will unnecessarily enlarge the scope
of the RFC. But it might still be useful to propose them later. trait methods - where downstream has to implement as  no matter whether  is const or not. pointers, , Figuring out something for the "really const" distinction (related to proposal 6), if that is really worth it - what
we should do with  blocks and  items.Figuring out a way to configure derives (built-in or custom) to generate const implementations.
Although we might still want to recommend a way for custom derives to start generating const impls.I started my work on const traits on July 1st, 2021, making it so that
const trait impls can be called across different crates. I've worked on the implementation of this feature since then,
and now it's been four years.We might still have many years to go, but hopefully this post helps making the language design discourse better.If you would like to get involved, feel free to go comment on the open RFC (please comment on specific lines or on
the file using the PR review feature, this makes discussion threaded and much easier to follow), or follow discussions
occuring on the #t-lang/effects Zulip channel.Thanks errs for the encouragement to write this post, and oli for commenting on initial drafts of the proposal and providing more thinking on Proposal 4 and 4.5.]]></content:encoded></item><item><title>Rust: Python’s new performance engine</title><link>https://thenewstack.io/rust-pythons-new-performance-engine/</link><author>/u/dochtman</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 13:56:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Python developers have always faced a trade-off: write elegant, readable code or go for high performance. For a long time, this meant reaching for C extensions when speed mattered. But Rust has emerged as Python’s performance co-pilot.The Rust Revolution in PythonIn a blog post about the study — which was based on a survey of 30,000 developers — Michael Kennedy, the founder of Talk Python and a Python Software Foundation Fellow, wrote that “At the 2025 Python Language Summit, core developers shared an eye-opening statistic: ‘Somewhere between one-quarter and one-third of all native code being uploaded to PyPI for new projects uses Rust.’ This means that when developers start new performance-critical Python projects today, they’re increasingly choosing Rust over traditional C extensions.”Why Rust Is Winning Over CAmong the key reasons for Rust’s rapid adoption in the Python ecosystem is performance. Rust delivers C-level performance while maintaining Python’s ease of integration. Rust’s zero-cost abstractions and efficient memory management make it ideal for performance-critical components.Rust also provides memory safety. Unlike C, Rust prevents common programming errors like buffer overflows and memory leaks at compile time. This makes it dramatically safer for extending Python without introducing security vulnerabilities or crashes.In addition, Rust offers a high-quality developer experience with its modern toolchain, excellent error messages, and package manager (Cargo). It provides a better development experience compared to the often painful process of writing and debugging C extensions.Real-World Success StoriesThe Python ecosystem already showcases several high-profile Rust success stories: has revolutionized data science with DataFrame operations that often outperform Pandas by orders of magnitude. Built in Rust, it provides a Python API that feels natural while delivering unprecedented speed for data processing tasks. rewrote its core validation engine in Rust, resulting in dramatic performance improvements for data validation and serialization across virtually every Python discipline — from web APIs to machine learning pipelines. increasingly relies on Rust-based components. The survey showed that FastAPI usage jumped from 29% to 38% (a 30% increase), partly driven by its async-friendly architecture that pairs well with Rust-based server components.The Infrastructure RevolutionRust’s influence extends beyond individual packages to Python’s core infrastructure. Traditional Web Server Gateway Interface (WSGI) servers are giving way to Asynchronous Server Gateway Interface (ASGI) compatible alternatives, many of which are built with Rust. Kennedy cited Granian, a new Rust-based application server, as gaining significant traction. He also singled out Uvicorn, which, while Python-based, increasingly integrates with Rust componentsKennedy also noted that two new Python type checkers have emerged, both written in Rust. One is ty from Astral, which is described as “an extremely fast Python type checker and language server.” The other is Pyrefly from Meta, which is a high-performance alternative to traditional type checkers like mypy.“They are both vying to be the next generation tooling for type checking. Moreover, both of these tools provide extremely fast language server protocols (LSPs), Kennedy wrote.“Notice anything similar? They are both written in Rust, backing up the previous claim that ‘Rust has become Python’s performance co-pilot,’” he added.Meanwhile, for enterprises, Rust-enhanced Python delivers tangible benefits. The performance improvements alone can translate into cost savings, including reduced cloud compute costs and lower memory usage. Moreover, faster response times improve customer satisfaction and more efficient code reduces energy consumption, Kennedy said.Advice for Python DevelopersKennedy advised Python developers to learn to read Rust.Python developers should consider learning the basics of Rust, not to replace Python, but to complement it.“As I discussed in our analysis, Rust is becoming increasingly important in the most significant portions of the Python ecosystem,” Kennedy wrote. “I definitely don’t recommend that you become a Rust developer instead of a Pythonista, but being able to read basic Rust so that you understand what the libraries you’re consuming are doing will be a good skill to have.”Kennedy also advised Python developers to embrace Rust-enhanced libraries. When choosing between similar packages, consider those with Rust cores — they often provide superior performance without sacrificing Python’s ease of use, he said.And Python devs also should consider Rust for extensions, Kennedy advises. Python developers building performance-critical Python extensions should evaluate Rust as their implementation language instead of defaulting to C, he indicated.Overall, Rust is not replacing Python — it’s supercharging it. This hybrid approach gives developers the best of both worlds: Python’s expressiveness and ecosystem for application logic, with Rust’s performance for computationally intensive components, the report expresses.]]></content:encoded></item><item><title>Is Bevy really as unstable as their Introduction makes them out to be?</title><link>https://www.reddit.com/r/rust/comments/1mvfamk/is_bevy_really_as_unstable_as_their_introduction/</link><author>/u/VermicelliLanky3927</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 13:47:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hai yall, first post on the sub, please let me know if there's anything I should change.Although there are a number of well-maintained general purpose game engines listed on https://arewegameyet.rs/, like Fyrox and BlueEngine, it seems like the one that is far and away the most popular is Bevy.However, despite the fact that Bevy is, at this point, several years old, their introduction page still claims that they are in the "early stages of development" and in particular mention that "documentation is sparse" and that every three months, they release a new version with breaking API changes.This is odd to me because it always seemed to me that Bevy was highly mature (certainly moreso than many of the other projects on arewegameyet), and had amazing documentation. I've been interested in using Bevy for a project for a while now, and this warning has deterred me up to this point, so I wanted to ask the community: For those of you that have used Bevy, what has your experience been like? If you've had to make changes because of an API change, how did migration treat you?Thanks in advance, yall :3]]></content:encoded></item><item><title>Typechecker Zoo: minimal Rust implementations of historic type systems</title><link>https://sdiehl.github.io/typechecker-zoo/</link><author>/u/kibwen</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 11:04:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Media] I&apos;ve been working on a text editor that is written and configured in Rust. Is there any interest in this?</title><link>https://www.reddit.com/r/rust/comments/1mvb640/media_ive_been_working_on_a_text_editor_that_is/</link><author>/u/AhoyISki</author><category>rust</category><category>reddit</category><pubDate>Wed, 20 Aug 2025 10:36:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[: Good news! Apparently it does work on Windows! It's just a lot of hassle to install all the things that you need, like gcc, cargo, and all that stuff, but that's just Windows being Windows.: It "works" on Windows, but it is very laggy and very buggy. The performance on Windows is not indicative of the performance on linux, just saying.So, I've been working on a text editor (link here) for an embarrassingly long amount of time. It is written  in Rust. Here's a configuration example:setup_duat!(setup); use duat::prelude::*; fn setup() { plug!( treesitter::TreeSitter, match_pairs::MatchPairs::new(), kak::Kak::new() ); map::<kak::Insert>("jk", "<Esc>"); print::wrap_on_edge(); // Modify every LineNumbers hook::add::<LineNumbers<Ui>>(|_, (cfg, _)| { cfg.align_right().align_main_left() }); hook::add::<StatusLine<Ui>>(|pa, (cfg, _)| { cfg.fmt(status!( "{name_txt}{Spacer}{}[sep]|{sels_txt}[sep]|{main_txt}", mode_txt(pa) )) }); form::set("sep", Form::with("#506090")); } This file does quite a lot of things: It adds three plugins, maps  to , changes wrapping, realigns , changes the  and sets a custom .There are various benefits to using Rust as a config language:Cargo is the plugin manager, and  is the plugin repository;Traits can act like a "guide" on how to extend functionality;People can have a fun excuse to finally learn Rust;I have also worked really hard to reduce many of the known drawbacks:Compile times: Duat reloads in ~2s give or take, but my computer is a fairly old model;Boilerplate: Due to Rust's expressiveness, code can often end up shorter than on "simpler languages";At the moment, there are some big parts missing (floating widgets, LSP...). Even still, Duat is already incredibly extensible. Here's a short list of some of the things that can be done:Custom widgets through the  trait;s that control the s and can replicate any multi-cursor text editor's behavior;A convenient  struct, supporting styles, alignment, spacers, concealment and "ghost text", all with the same  API.A really easy to use , which can be modded and updated arbitrarily;Custom hooks with the  trait;Duat is thoroughly documented, and I am currently working on an  guide to gently introduce people to the API. Although at the moment that's still nowhere near complete.So yeah, the question in the title. What are your thoughts on this project? Is there any interest in this sort of thing? Should I keep working on it? Because there is a lot of work still to be done, and I don't really know if it is worth it to be completely honest.P.s. I don't want to steal valor, this text editor is heavily inspired by Kakoune. In fact, the modes of the  are a (still incomplete) mimicry of those in Kakoune.Edit: For those who tried installing it and it didn't work, try installing again, I forgot to change the default configuration for some recent API changes.Edit2: Here's a summary of what's happening in the gif:I'm enabling a  that shows the length of each selection in the selection itself, you can see the little orange numbers in each selection after that;I'm pushing a  widget to every  widget. This is how the layout of Duat is constructed, in a very declarative, block by block style;I'm replacing the  status part with a . Much like with , the  and  macros are also checked at compile time, so all of its parts should work correctly.]]></content:encoded></item><item><title>This Week in Rust 613</title><link>https://this-week-in-rust.org/blog/2025/08/20/this-week-in-rust-613/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 20 Aug 2025 04:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is tur, a turing machine emulator with text-mode user interface.Despite a lack of suggestions, llogiq is very pleased with his choice.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.If you are a feature implementer and would like your RFC to appear in this list, add a
 label to your RFC along with a comment providing testing instructions and/or
guidance on which aspect(s) of the feature need testing.Let us know if you would like your feature to be tracked as a part of this list.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No calls for participation this weekAre you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.Lots of noise/bimodality this week. Overall though no major performance impacting changes landed.1 Regressions, 3 Improvements, 7 Mixed; 4 of them in rollups
27 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.Rusty Events between 2025-08-20 - 2025-09-17 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.It's amazing how far const eval has come in #Rust. It wasn't too long ago that even a simple if/else wasn't permitted. Now we're not that far off from having const trait impls and const closures, which will make damn near everything const capable.llogiq has looked at all zero suggestions and came up empty, so he just chose this quote instead.]]></content:encoded></item><item><title>Dependabot now supports Rust toolchain updates - GitHub Changelog</title><link>https://github.blog/changelog/2025-08-19-dependabot-now-supports-rust-toolchain-updates/</link><author>/u/Jammie1</author><category>rust</category><category>reddit</category><pubDate>Tue, 19 Aug 2025 16:20:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Subscribe to our developer newsletter
				Discover tips, technical guides, and best practices in our biweekly newsletter just for devs.			]]></content:encoded></item><item><title>Demoting x86_64-apple-darwin to Tier 2 with host tools | Rust Blog</title><link>https://blog.rust-lang.org/2025/08/19/demoting-x86-64-apple-darwin-to-tier-2-with-host-tools/</link><author>/u/PthariensFlame</author><category>rust</category><category>reddit</category><pubDate>Tue, 19 Aug 2025 15:38:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[In Rust 1.90.0, the target  will be demoted to Tier 2 with host tools.
The standard library and the compiler will continue to be built and distributed,
but automated tests of these components are no longer guaranteed to be run.Rust has supported macOS for a long time,
with some amount of support dating back to Rust 0.1 and likely before that.
During that time period,
Apple has changed CPU architectures from x86 to x86_64 and now to Apple silicon,
ultimately announcing the end of support for the x86_64 architecture.Similarly,
GitHub has announced that they will no longer provide free macOS x86_64 runners for public repositories.
The Rust Project uses these runners to execute automated tests for the  target.
Since the target tier policy requires that Tier 1 platforms must run tests in CI,
the  target must be demoted to Tier 2.Starting with Rust 1.90.0,  will be Tier 2 with host tools.
For users,
nothing will change immediately;
builds of both the standard library and the compiler will still be distributed by the Rust Project for use via  or alternative installation methods.Over time,
this target will likely accumulate bugs faster due to reduced testing.If the  target causes concrete problems,
it may be demoted further.
No plans for further demotion have been made yet.For more details on the motivation of the demotion, see RFC 3841.]]></content:encoded></item><item><title>Introducing Rusted Firmware-A (RF-A) - A Rust-Based reimagination of Trusted Firmware-A</title><link>https://www.trustedfirmware.org/blog/rf-a-blog</link><author>/u/kisimre</author><category>rust</category><category>reddit</category><pubDate>Tue, 19 Aug 2025 11:53:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[We are excited to unveil Rusted Firmware-A, a new open-source project that reimagines Trusted Firmware-A (TF-A) using Rust. Developed collaboratively by engineers at Arm and Google, Rusted Firmware-A is a ground-up redesign that leverages the strengths of a modern systems programming language specifically designed to address key limitations of traditional approaches, with a strong focus on safety and performance.: Rusted Firmware-A is a Rust-first implementation of Trusted Firmware-A for the latest Arm A-class processors. It’s open source, security by design, legacy-free, and built with the future in mind. This is just the beginning. The project welcomes your feedback as it evolves.TF-A has served the Arm ecosystem reliably for many years. However, it carries evolutionary and legacy artifacts accumulated over time. Rusted Firmware-A gives us a great opportunity to: from using Rust., allowing the codebase to be more maintainable.Redesign with clarity and modularity, taking advantage of Rust’s rich type system and error handling.Catch more bugs at compile time, reducing risks in sensitive firmware environments.Align with modern security guidance from regulators and security standards bodies.The C TF-A will continue to be supported and maintained including Long-Term Support (LTS) versions that remain first-class for many years. However, with Rusted Firmware-A, we’re refreshing the implementation for the future.Rusted Firmware-A starts with a blank sheet, targeting the latest Arm A-class processors. Older architecture versions and extensions are out of scope. Some key principles:Requires more modern features like GICv3, DSU, and hardware-assisted coherency.Certain interfaces will not be reimplemented in Rusted Firmware-A.It’s a reference implementation suitable for future product releases.Intended for use on Arm FVP and QEMU platforms in the early stages.Project Status and RoadmapWe are currently working on an early prototype and published v0.1 for use. This version will serve as a public signal that the codebase is available and open for feedback.Initial design & architecture reviewCore firmware framework in RustEarly platform support (FVP, QEMU)RFC on migration paths and feature setToward v1.0: stabilization: We’re looking to productize Rusted Firmware-A in a few years. Until then, expect rapid iteration and community-driven evolution.Open Source and CollaborationRusted Firmware-A is open source. You can explore the project, feedback, contribute or track progress at:Currently, contributions from  and  are given priority. We welcome external contributions on a best-effort basis during the early phases.We anticipate providing a migration path from TF-A to Rusted Firmware-A once the project matures. Our intent is to eventually do all new firmware development in Rusted Firmware-A.
However:C TF-A will  be deprecated or end-of-lifed immediately.LTS branches of C TF-A will remain fully supported for their 7-year lifetime durations.Partners and vendors can continue using C TF-A for existing and upcoming products.Rusted Firmware-A is early but promising. The v0.1 release is available. clone the repository, explore the issues, and let us know what you think.Whether you’re considering adoption or contribution, your feedback is essential. Please share your thoughts:Publicly (via mailing list and Discord)Or via Tech forum meetingsWe’re especially interested in:Hardware-specific constraintsPreferred timelines for adoptionMigration blockers and integration concernsCandidates for new Arm architecture cratesLet’s build the next generation of secure Arm firmware together.Why write Rusted Firmware-A in Rust?
To ensure memory safety, reduce legacy baggage, improve maintainability, and leverage Rust’s type system to catch more bugs earlier.
Absolutely not. It has served us well. Rusted Firmware-A is an opportunity for a modern reimplementation.Will both versions be developed in parallel?
For now, yes. But maintaining feature parity long-term is unsustainable. Eventually, Rusted Firmware-A will become the default for new development.What if I need features missing in Rusted Firmware-A?
Rusted Firmware-A will not support all the features supported by TF-A today. Please continue using C TF-A if those are required.Where can I find the RUSTED FIRMWARE-A roadmap and status?
We’ll publish a roadmap shortly. In the meantime, track our GitHub issues.Are there plans to transition other Trusted Firmware projects to Rust?
There are currently no plans to transition other Trusted Firmware projects to Rust. The existing projects will continue to be actively maintained in their current implementation languages.About TrustedFirmware.orgTrustedFirmware.org is an open source project implementing foundational software components for creating secure devices. Trusted Firmware provides a reference implementation of secure software for processors implementing both the A-Profile and M-Profile Arm architecture. It provides SoC developers and OEMs with a reference trusted code base complying with the relevant Arm specifications. Trusted Firmware code is the preferred implementation of Arm specifications, allowing quick and easy porting to modern chips and platforms. This forms the foundations of a Trusted Execution Environment (TEE) on application processors, or the Secure Processing Environment (SPE) of microcontrollers.]]></content:encoded></item><item><title>[Media] Creating terminal UI games in Rust with Ratatui is incredibly fun! WezTerm Powermonger Remake</title><link>https://www.reddit.com/r/rust/comments/1mufils/media_creating_terminal_ui_games_in_rust_with/</link><author>/u/Big_Membership9737</author><category>rust</category><category>reddit</category><pubDate>Tue, 19 Aug 2025 11:16:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I loved Powermonger as a kid, so I’m building a similar game in Rust with Ratatui. My first attempt used Unicode, which caused a lot of issues, but I’m still really enjoying the art style and how straightforward game development feels with Rust and Ratatui.]]></content:encoded></item><item><title>Whatsapp client written purely in Rust based on whatsmeow and baileys</title><link>https://github.com/jlucaso1/whatsapp-rust</link><author>/u/jlucaso1</author><category>rust</category><category>reddit</category><pubDate>Tue, 19 Aug 2025 01:29:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[You can create high perfomance bots. In my tests in the release mode only 9mb of RAM are used and the binary size is about 4-5mb.   submitted by    /u/jlucaso1 ]]></content:encoded></item><item><title>Demoting x86_64-apple-darwin to Tier 2 with host tools</title><link>https://blog.rust-lang.org/2025/08/19/demoting-x86-64-apple-darwin-to-tier-2-with-host-tools/</link><author>Jake Goulding</author><category>dev</category><category>official</category><category>rust</category><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[In Rust 1.90.0, the target  will be demoted to Tier 2 with host tools.
The standard library and the compiler will continue to be built and distributed,
but automated tests of these components are no longer guaranteed to be run.Rust has supported macOS for a long time,
with some amount of support dating back to Rust 0.1 and likely before that.
During that time period,
Apple has changed CPU architectures from x86 to x86_64 and now to Apple silicon,
ultimately announcing the end of support for the x86_64 architecture.Similarly,
GitHub has announced that they will no longer provide free macOS x86_64 runners for public repositories.
The Rust Project uses these runners to execute automated tests for the  target.
Since the target tier policy requires that Tier 1 platforms must run tests in CI,
the  target must be demoted to Tier 2.Starting with Rust 1.90.0,  will be Tier 2 with host tools.
For users,
nothing will change immediately;
builds of both the standard library and the compiler will still be distributed by the Rust Project for use via  or alternative installation methods.Over time,
this target will likely accumulate bugs faster due to reduced testing.If the  target causes concrete problems,
it may be demoted further.
No plans for further demotion have been made yet.For more details on the motivation of the demotion, see RFC 3841.]]></content:encoded></item><item><title>Is calling tokio::sleep() with a duration of one week a bad idea?</title><link>https://www.reddit.com/r/rust/comments/1mtond5/is_calling_tokiosleep_with_a_duration_of_one_week/</link><author>/u/dmkolobanov</author><category>rust</category><category>reddit</category><pubDate>Mon, 18 Aug 2025 15:06:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I’ve created a web app that generates some temporary files during its processing. I’m thinking of creating a worker thread that will delete every file in the temp folder, then call  with a duration of one week. It’ll run alongside the main application with , and the worker thread will simply never exit under normal circumstances.Anyways, is there anything wrong with this approach? Is there a better way to schedule tasks like this? I know cron is an option, but my understanding of it is limited. Plus, this app will run in a Docker container, and it seems like Docker + cron is even more of a headache than regular cron.Edit: For a little more context, this is an app for analyzing x-ray images that’ll be used at the small manufacturing company I work at. Everything will be hosted on local, on-premises servers, and the only user is the guy who runs our x-ray machine lol. Not that I want to excuse bad programming, it’s just that the concerns are a little different when it’s not consumer-facing software. Anyways, once the analysis is generated (which includes some contrast changes and circles around potential defects located by the x-ray), and the results are displayed on a web page, the images are no longer needed. The original image is archived, and there’s a lookup feature that simply re-runs the analysis routine on the raw image and re-generates the result images. All I’d like is to make sure there’s not a glut of these images building up long after they’re needed.]]></content:encoded></item><item><title>Czkawka / Krokiet 10.0: cleaning duplicates, unifying features and a handful of Rust related statistics</title><link>https://www.reddit.com/r/rust/comments/1mtomwh/czkawka_krokiet_100_cleaning_duplicates_unifying/</link><author>/u/krutkrutrar</author><category>rust</category><category>reddit</category><pubDate>Mon, 18 Aug 2025 15:06:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[After a little less than six months, I’m releasing a new version of my three distinct (yet similar) duplicate-finding programs today.The list of fixes and new features may seem random, and in fact it is, because I tackled them in the order in which ideas for their solutions came to mind. I know that the list of reported issues on GitHub is quite long, and for each user their own problem seems the most important, but with limited time I can only address a small portion of them, and I don’t necessarily pick the most urgent ones.Interestingly, this version is the largest so far (at least if you count the number of lines changed). Krokiet now contains almost all the features I used in the GTK version, so it looks like I myself will soon switch to it completely, setting an example for other undecided users (as a reminder, the GTK version is already in maintenance mode, and I focus there exclusively on bug fixes, not adding new features).As usual, the binaries for all three projects (, , and ), along with a short legend explaining what the individual names refer to and where these files can be used, can be found in the releases section on GitHub — https://github.com/qarmin/czkawka/releasesOne of the random errors that sometimes occurred due to the user, sometimes my fault, and sometimes — for example — because a power outage shut down the computer during operation, was a mysterious crash at the start of scanning, which printed the following information to the terminal:memory allocation of 201863446528 bytes failed Cache files that were corrupted by the user (or due to random events) would crash when loaded by the bincode library. Another situation, producing an error that looked identical, occurred when I tried to remove cache entries for non-existent or unavailable files using an incorrect struct for reading the data (in this case, the fix was simply changing the struct type into which I wanted to decode the data).This was a rather unpleasant situation, because the application would crash for the user during scanning or when pressing the appropriate button, leaving them unsure of what to do next. Bincode provides the possibility of adding a memory limit for data decoding. The fix required only a few lines of code, and that could have been the end of it. However, during testing it turned out to be an unexpected breaking change—data saved with a memory-limited configuration cannot be read with a standard configuration, and vice versa.use std::collections::BTreeMap; use bincode::{serialize_into, Options}; const MEMORY_LIMIT: u64 = 1024 * 1024 * 1024; // 1 GB fn main() { let rands: Vec<u32> = (0..1).map(|_| rand::random::<u32>()).collect(); let btreemap: BTreeMap<u32, Vec<u32>> = rands .iter() .map(|&x| (x % 10, rands.clone())) .collect(); let options = bincode::DefaultOptions::new().with_limit(MEMORY_LIMIT); let mut serialized: Vec<_> = Vec::new(); options.serialize_into(&mut serialized, &btreemap).unwrap(); println!("{:?}", serialized); let mut serialized2: Vec<_> = Vec::new(); serialize_into(&mut serialized2, &btreemap).unwrap(); println!("{:?}", serialized2); } [1, 1, 1, 252, 53, 7, 34, 7] [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 53, 7, 34, 7] The above code, when serializing data with and without the limit, produces two different results, which was very surprising to me because I thought that the limiting option applied only to the decoding code, and not to the file itself (it seems to me that most data encoding libraries write only the raw data to the file).So, like it or not, this version (following the path of its predecessors) has a cache that is incompatible with previous versions. This was one of the reasons I didn’t implement it earlier — I had tried adding limits only when reading the file, not when writing it (where I considered it unnecessary), and it didn’t work, so I didn’t continue trying to add this functionality.I know that for some users it’s probably inconvenient that in almost every new version they have to rebuild the cache from scratch, because due to changed structures or data calculation methods, it’s not possible to simply read old files. So in future versions, I’ll try not to tamper too much with the cache unless necessary (although, admittedly, I’m tempted to add a few extra parameters to video files in the next version, which would force the use of the new cache).An alternative would be to create a built-in tool for migrating cache files. However, reading arbitrary external data without memory limits in place would make such a tool useless and prone to frequent crashes. Such a tool is only feasible from the current version onward, and it may be implemented in the future.To match the feature set currently available in Czkawka, I decided to try to implement the missing translations, which make it harder for some users, less proficient in English, to use the application.One might think that since Slint itself is written in Rust, using the Fluent library inside it, which is also written in Rust, would be an obvious and natural choice. However, for various reasons, the authors decided that it’s better to use probably the most popular translation tool instead — gettext, which, however, complicates compilation and almost makes cross-compilation impossible (the issue aims to change this situation — https://github.com/slint-ui/slint/issues/3715).Without built-in translation support in Slint, what seemed like a fairly simple functionality turned into a tricky puzzle of how to implement it best. My goal was to allow changing the language at runtime, without needing to restart the entire application.Ultimately, I decided that the best approach would be to create a singleton containing all the translation texts, in a style like this:export global Translations { in-out property <string> ok_button_text: "Ok"; in-out property <string> cancel_button_text: "Cancel"; ... } export component PopupBase inherits PopupWindow { in-out property <string> ok_text <=> Translations.ok_button_text; ... } then, when changing the language or launching the application, all these attributes are updated in such a way:app.global::<Callabler>().on_changed_language(move || { let app = a.upgrade().unwrap(); let translation = app.global::<Translations>(); translation.set_ok_button_text(flk!("ok_button").into()); translation.set_cancel_button_text(flk!("cancel_button").into()); ... }); With over 200 texts to translate, it’s very easy to make a mistake or leave some translations unlinked, which is why I rely on Python helper scripts that verify everything is being used.This adds more code than if built-in support for fluent-rs existed and could be used directly, similar to how gettext translations currently work. I hope that something like this will be implemented for Fluent soon:export component PopupBase inherits PopupWindow { in-out property <string> ok_text: u/tr("ok_button"); ... } Regarding the translations themselves, they are hosted and updated on Crowdin — https://crowdin.com/project/czkawka — and synchronized with GitHub from time to time. For each release, several dozen phrases are updated, so I’m forced to use machine translation for some languages. Not all texts may be fully translated or look as they should, so feel free to correct them if you come across any mistakes.The main goal of this version was to reduce the feature gaps between Czkawka (GUI) and Krokiet, so that I could confidently recommend Krokiet as a viable alternative. I think I largely succeeded in this area.During this process, it often turned out that implementing the same features in Slint is much simpler than it was in the GTK version. Take sorting as an example. On the GTK side, due to the lack of better-known solutions (there probably are some, but I’ve lived until now in complete ignorance, which makes my eyes hurt when I look at the final implementation I once made), to sort a model, I would get an iterator over it and then iterate through each element one by one, collecting the TreeIters into a vector. Then I would extract the data from a specific column of each row and sort it using bubble sort within that vector.fn popover_sort_general<T>(tree_view: &gtk4::TreeView, column_sort: i32, column_header: i32) where T: Ord + for<'b> glib::value::FromValue<'b> + 'static + Debug, { let model = get_list_store(tree_view); if let Some(curr_iter) = model.iter_first() { assert!(model.get::<bool>(&curr_iter, column_header)); // First item should be header assert!(model.iter_next(&curr_iter)); // Must be at least two items loop { let mut iters = Vec::new(); let mut all_have = false; loop { if model.get::<bool>(&curr_iter, column_header) { assert!(model.iter_next(&curr_iter), "Empty header, this should not happens"); break; } iters.push(curr_iter); if !model.iter_next(&curr_iter) { all_have = true; break; } } if iters.len() == 1 { continue; // Can be equal 1 in reference folders } sort_iters::<T>(&model, iters, column_sort); if all_have { break; } } } } fn sort_iters<T>(model: &ListStore, mut iters: Vec<TreeIter>, column_sort: i32) where T: Ord + for<'b> glib::value::FromValue<'b> + 'static + Debug, { assert!(iters.len() >= 2); loop { let mut changed_item = false; for idx in 0..(iters.len() - 1) { if model.get::<T>(&iters[idx], column_sort) > model.get::<T>(&iters[idx + 1], column_sort) { model.swap(&iters[idx], &iters[idx + 1]); iters.swap(idx, idx + 1); changed_item = true; } } if !changed_item { return; } } } Over time, I’ve realized that I should have wrapped the model management logic earlier, which would have made reading and modifying it much easier. But now, it’s too late to make changes. On the Slint side, the situation is much simpler and more “Rust-like”:pub(super) fn sort_modification_date(model: &ModelRc<MainListModel>, active_tab: ActiveTab) -> ModelRc<MainListModel> { let sort_function = |e: &MainListModel| { let modification_date_col = active_tab.get_int_modification_date_idx(); let val_int = e.val_int.iter().collect::<Vec<_>>(); connect_i32_into_u64(val_int[modification_date_col], val_int[modification_date_col + 1]) }; let mut items = model.iter().collect::<Vec<_>>(); items.sort_by_cached_key(&sort_function); let new_model = ModelRc::new(VecModel::from(items)); recalculate_small_selection_if_needed(&new_model, active_tab); return new_model; } It’s much shorter, more readable, and in most cases faster (the GTK version might be faster if the data is already almost sorted). Still, a few oddities remain, such as: —to generalize the model for different tools a bit, for each row in the scan results, there are vectors containing numeric and string data. The amount and order of data differs for each tool, so it’s necessary to fetch from the current tab where the needed data currently resides as the name suggests, it combines two i32 values into a u64. This is a workaround for the fact that Slint doesn’t yet support 64-bit integers (though I’m hopeful that support will be added soon).recalculate_small_selection_if_needed — due to the lack of built-in widgets with multi-selection support in Slint (unlike GTK), I had to create such a widget along with all the logic for selecting items, modifying selections, etc. It adds quite a bit of extra code, but at least I now have more control over selection, which comes in handy in certain situationsAnother useful feature that already existed in Czkawka is the ability to start a scan, along with a list of selected folders, directly from the CLI. So now, runningkrokiet . Desktop -i /home/rafal/Downloads -e /home/rafal/Downloads/images will start scanning for files in three folders with one excluded (of course, only if the paths exist — otherwise, the path will be ignored). This mode uses a separate configuration file, which is loaded when the program is run with command-line arguments (configurations for other modes are not overwritten).Since some things are easier to implement in Krokiet, I added several functions in this version that were missing in Czkawka:Remembering window size and column widths for each screenThe ability to hide text on icons (for a more compact UI)Dark and light themes, switchable at runtimeDisabling certain buttons when no items are selectedDisplaying the number of items queued for deletionFollowing the end of Snap support on Linux in the previous version, due to difficulties in building them, it’s now time to drop AppImage as well.The main reasons for discontinuing AppImage are the nonstandard errors that would appear during use and its limited utility beyond what regular binary files provide.Personally, I’m a fan of the AppImage format and use it whenever possible (unless the application is also available as a Flatpak or Snap), since it eliminates the need to worry about external dependencies. This works great for applications with a large number of dependencies. However, in Czkawka, the only dependencies bundled were GTK4 libraries — which didn’t make much sense, as almost every Linux distribution already has these libraries installed, often with patches to improve compatibility (for example, Debian patches: https://sources.debian.org/src/gtk4/4.18.6%2Bds-2/debian/patches/series/).It would make more sense to bundle optional libraries such as ffmpeg, libheif or libraw, but I didn’t have the time or interest to do that. Occasionally, some AppImage users started reporting issues that did not appear in other formats and could not be reproduced, making them impossible to diagnose and fix.Additionally, the plugin itself (https://github.com/linuxdeploy/linuxdeploy-plugin-gtk) used to bundle GTK dependencies hadn’t been updated in over two years. Its authors did a fantastic job creating and maintaining it in their free time, but a major issue for me was that it wasn’t officially supported by the GTK developers, who could have assisted with the development of this very useful project.Some users pointed out that deleting or copying files from within the application is time-consuming, and there is no feedback on progress. Additionally, during these operations, the entire GUI becomes unresponsive until the process finishes.The problem stems from performing file operations in the same thread as the GUI rendering. Without interface updates, the system considers the application unresponsive and may display an os window prompting the user to kill it.The solution is relatively straightforward — simply move the computations to a separate thread. However, this introduces two new challenges: the need to stop the file-processing task and to synchronize the state of completed operations with the GUI.A simple implementation in this style is sufficient:let all_files = files.len(); let mut processing_files = Arc<AtomicBool<usize>>::new(0); let _ = files.into_par_iter().map(|e| { if stop_flag.load(Ordering::Relaxed) { return None; } let processing_files = processing_files.fetch_add(1, Ordering::Relaxed); let status_to_send = Status { all_files, processing_files }; progress_sender.send(status_to_send); // Processing file }).while_some().collect::<Vec<_>>(); The problem arises when a large number of messages are being sent, and updating the GUI/terminal for each of them would be completely unnecessary — after all, very few people could notice and process status changes appearing even 60 times per second.This would also cause performance issues and unnecessarily increase system resource usage. I needed a way to limit the number of messages being sent. This could be implemented either on the side of the message generator (the thread deleting files) or on the recipient side (the GUI thread/progress bar in CLI). I decided it’s better to handle it sooner rather than later.Ultimately, I created a simple structure that uses a lock to store the latest message to be sent. Then, in a separate thread, every ~100 ms, the message is fetched and sent to the GUI. Although the solution is simple, I do have some concerns about its performance on systems with a very large number of cores — there, thousands or even tens of thousands of messages per second could cause the mutex to become a bottleneck. For now, I haven’t tested it under such conditions, and it currently doesn’t cause problems, so I’ve postponed optimization (though I’m open to ideas on how it could be improved).pub struct DelayedSender<T: Send + 'static> { slot: Arc<Mutex<Option<T>>>, stop_flag: Arc<AtomicBool>, } impl<T: Send + 'static> DelayedSender<T> { pub fn new(sender: crossbeam_channel::Sender<T>, wait_time: Duration) -> Self { let slot = Arc::new(Mutex::new(None)); let slot_clone = Arc::clone(&slot); let stop_flag = Arc::new(AtomicBool::new(false)); let stop_flag_clone = Arc::clone(&stop_flag); let _join = thread::spawn(move || { let mut last_send_time: Option<Instant> = None; let duration_between_checks = Duration::from_secs_f64(wait_time.as_secs_f64() / 5.0); loop { if stop_flag_clone.load(std::sync::atomic::Ordering::Relaxed) { break; } if let Some(last_send_time) = last_send_time { if last_send_time.elapsed() < wait_time { thread::sleep(duration_between_checks); continue; } } let Some(value) = slot_clone.lock().expect("Failed to lock slot in DelayedSender").take() else { thread::sleep(duration_between_checks); continue; }; if stop_flag_clone.load(std::sync::atomic::Ordering::Relaxed) { break; } if let Err(e) = sender.send(value) { log::error!("Failed to send value: {e:?}"); }; last_send_time = Some(Instant::now()); } }); Self { slot, stop_flag } } pub fn send(&self, value: T) { let mut slot = self.slot.lock().expect("Failed to lock slot in DelayedSender"); *slot = Some(value); } } impl<T: Send + 'static> Drop for DelayedSender<T> { fn drop(&mut self) { // We need to know, that after dropping DelayedSender, no more values will be sent // Previously some values were cached and sent after other later operations self.stop_flag.store(true, std::sync::atomic::Ordering::Relaxed); } } In the case of Krokiet and Czkawka, I decided to write the GUI in low-level languages (Slint is transpiled to Rust), instead of using higher-level languages — mainly for performance and simpler installation.For Krokiet, I briefly considered using Tauri, but I decided that Slint would be a better solution in my case: simpler compilation and no need to use the heavy (and differently behaving on each system) webview with TS/JS.However, one user apparently didn’t like the current gui and decided to create their own alternative using Tauri.The author himself does not hide that he based the look of his program on Krokiet(which is obvious). Even so, differences can be noticed, stemming both from personal design preferences and limitations of the libraries that both projects use(for example, in the Tauri version popups are used more often, because Slint has issues with them, so I avoided using them whenever possible).Since I am not very skilled in application design, it’s not surprising that I found several interesting solutions in this new GUI that I will want to either copy 1:1 or use as inspiration when modifying Krokiet.Preliminary tests indicate that the application works surprisingly well, despite minor performance issues (one mode on Windows froze briefly — though the culprit might also be the czkawka_core package), small GUI shortcomings (e.g., the ability to save the application as an HTML page), or the lack of a working Linux version (a month or two ago I managed to compile it, but now I cannot).Recently, just before the release of Debian 13, a momentous event took place — Czkawka 8.0.0 was added to the Debian repository (even though version 9.0.0 already existed, but well… Debian has a preference for older, more stable versions, and that must be respected). The addition was made by user Fab Stz.Debian takes reproducible builds very seriously, so it quickly became apparent that building Czkawka twice in the same environment produced two different binaries. I managed to reduce the problematic program to a few hundred lines. In my great wisdom (or naivety, assuming the bug wasn’t “between the chair and the keyboard”), I concluded that the problem must be in Rust itself. However, after analysis conducted by others, it turned out that the culprit was the  library, whose proc-macro iterates over a hashmap of arguments, and in Rust the iteration order in such a case is random (https://github.com/kellpossible/cargo-i18n/issues/150).With the source of the problem identified, I prepared a fix — https://github.com/kellpossible/cargo-i18n/pull/151 — which has already been merged and is part of the new 0.10.0 version of the  library. Debian’s repository still uses version 0.9.3, but with this fix applied. Interestingly,  is also used in many other projects, including applications from , so they too now have an easier path to achieving fully reproducible builds.I have never hidden the fact that I gladly use external libraries to easily extend the capabilities of an application, so I don’t have to waste time reinventing the wheel in a process that is both inefficient and error-prone.Despite many obvious advantages, the biggest downsides are larger binary sizes and longer compilation times. On my older laptop with 4 weak cores, compilation times became so long that I stopped developing this program on it.However, this doesn’t mean I use additional libraries without consideration. I often try to standardize dependency versions or use projects that are actively maintained and update the libraries they depend on — for example, rawler instead of rawloader, or image-hasher instead of img-hash (which I created as a fork of img-hash with updated dependencies).To verify the issue of long compilation times, I generated several charts showing how long Krokiet takes to compile with different options, how large the binary is after various optimizations, and how long a recompilation takes after adding a comment (I didn’t test binary performance, as that is a more complicated matter). This allowed me to consider which options were worth including in CI. After reviewing the results, I decided it was worth switching from the current configuration—  to release + fat lto + codegen units = 1 .The tests were conducted on a 12-core AMD Ryzen 9 9700 running Ubuntu 25.04, using the mold linker and rustc 1.91.0-nightly (cd7cbe818 2025–08–15). The base profiles were debug and release, and I adjusted some options based on them (not all combinations seemed worth testing, and some caused various errors) to see their impact on compilation. It’s important to note that Krokiet is a rather specific project with many dependencies, and Slint that generates a large (~100k lines) Rust file, so other projects may experience significantly different compilation times.|Config | Output File Size | Target Folder Size | Compilation Time | Rebuild Time | |:---------------------------------------------------|:-------------------|:---------------------|:-------------------|:---------------| | release + overflow checks | 73.49 MiB | 2.07 GiB | 1m 11s | 20s | | debug | 1004.52 MiB | 7.00 GiB | 1m 54s | 3s | | debug + cranelift | 624.43 MiB | 5.25 GiB | 47s | 3s | | debug + debug disabled | 131.64 MiB | 2.52 GiB | 1m 33s | 2s | | check | - | 1.66 GiB | 58s | 1s | | release | 70.50 MiB | 2.04 GiB | 2m 58s | 2m 11s | | release + cranelift | 70.50 MiB | 2.04 GiB | 2m 59s | 2m 10s | | release + debug info | 786.19 MiB | 5.40 GiB | 3m 23s | 2m 18s | | release + native | 67.22 MiB | 1.98 GiB | 3m 5s | 2m 13s | | release + opt o2 | 70.09 MiB | 2.04 GiB | 2m 56s | 2m 9s | | release + opt o1 | 76.55 MiB | 1.98 GiB | 1m 1s | 18s | | release + thin lto | 63.77 MiB | 2.06 GiB | 3m 12s | 2m 32s | | release + optimize size | 66.93 MiB | 1.93 GiB | 1m 1s | 18s | | release + fat lto | 45.46 MiB | 2.03 GiB | 6m 18s | 5m 38s | | release + cu 1 | 50.93 MiB | 1.92 GiB | 4m 9s | 2m 56s | | release + panic abort | 56.81 MiB | 1.97 GiB | 2m 56s | 2m 15s | | release + build-std | 70.72 MiB | 2.23 GiB | 3m 7s | 2m 11s | | release + fat lto + cu 1 + panic abort | 35.71 MiB | 1.92 GiB | 5m 44s | 4m 47s | | release + fat lto + cu 1 + panic abort + native | 35.94 MiB | 1.87 GiB | 6m 23s | 5m 24s | | release + fat lto + cu 1 + panic abort + build-std | 33.97 MiB | 2.11 GiB | 5m 45s | 4m 44s | | release + fat lto + cu 1 | 40.65 MiB | 1.95 GiB | 6m 3s | 5m 2s | | release + incremental | 71.45 MiB | 2.38 GiB | 1m 8s | 2s | | release + incremental + fat lto | 44.81 MiB | 2.44 GiB | 4m 25s | 3m 36s | Some things that surprised me: increases, rather than decreases, the binary size is fast but only slightly reduces the final binary size. works much better than  in this project, even though I often read online that  usually gives results very similar to  — I thought using this option wouldn’t change the binary size much, but the file shrank by as much as 20%. However, I cannot disable this option and wouldn’t recommend it to anyone (at least for Krokiet and Czkawka), because with external libraries that process/validate/parse external files, panics can occur, and with  they cannot be caught, so the application will just terminate instead of printing an error and continuing —this will probably become my new favorite flag, it gives release performance while keeping recompilation times similar to debug. Sometimes I need a combination of both, although I still need to test this more to be sureLately, I’ve both heard and noticed strange new websites that seem to imply they are directly connected to the project (though this is never explicitly stated) and offer only binaries repackaged from GitHub, hosted on their own servers. This isn’t inherently bad, but in the future it could allow them to be replaced with malicious files.Personally, I only manage a few projects related to Czkawka: the code repository on GitHub along with the binaries hosted there, the Flatpak version of the application, and projects on crates.io. All other projects are either abandoned (e.g., the Snap Store application) or managed by other people.Czkawka itself does not have a website, and its closest equivalent is the Readme.md file displayed on the main GitHub project page — I have no plans to create an official site. — it’s now easier to check for panic errors and verify application behavior historically (mainly relevant for Windows, where both applications and users tend to avoid the terminal) — pdf-rs has been replaced with lopdf, and imagepipe + rawloader replaced with rawler (a fork of rawloader) which has more frequent commits, wider usage, and newer dependencies (making it easier to standardize across different libraries)More options for searching similar video files — I had been blissfully unaware that the vid_dup_finder_lib library only allowed adjusting video similarity levels; it turns out you can also configure the black-line detection algorithm and the amount of the ignored initial segment of a video — created by me (and admittedly uglier than the previous ones) under a CC BY 4.0 license, replacing the not-so-free iconsBinaries for Mac with HEIF support, czkawka_cli built with musl instead of eyre, and Krokiet with an alternative Skia backend — added to the release files on GitHubFaster resolution changes in image comparison mode (fast-image-resize crate) — this can no longer be disabled (because, honestly, why would anyone want to?)Fixed a panic error that occurred when the GTK SVG decoder was missing or there was an issue loading icons using it (recently this problem appeared quite often on macOS)]]></content:encoded></item><item><title>[Media] Rust Only Video Game Development</title><link>https://www.reddit.com/r/rust/comments/1mthuuc/media_rust_only_video_game_development/</link><author>/u/dandoii</author><category>rust</category><category>reddit</category><pubDate>Mon, 18 Aug 2025 10:05:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Thought I'd share this here as I'm having a huge amount of fun with the project. Have always waned to make a game, but have never been able to do the art side of things and battling with crappy game engines was always a nightmare. About 2 months ago I decided to build a deep. ASCII adventure using only RUST. Just focusing on building deep and fun systems is making the game dev journey great and doing it in Rust is teaching me a lot too.]]></content:encoded></item><item><title>rust-analyzer weekly releases paused in anticipation of new trait solver (already available on nightly). The Rust dev experience is starting to get really good :)</title><link>https://www.reddit.com/r/rust/comments/1mtfwjf/rustanalyzer_weekly_releases_paused_in/</link><author>/u/Merlindru</author><category>rust</category><category>reddit</category><pubDate>Mon, 18 Aug 2025 08:05:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[An Update on the Next Trait Solver We are very close to switching from chalk to the next trait solver, which will be shared with rustc.  is de-facto unmaintained, and sharing the code with the compiler will greatly improve trait solving accuracy and fix long-standing issues in rust-analyzer. This will also let us enable more on-the-fly diagnostics (currently marked as experimental), and even significantly improve performance.However, in order to avoid regressions, we will suspend the weekly releases until the new solver is stabilized. In the meanwhile, please test the pre-release versions (nightlies) and report any issues or improvements you notice, either on GitHub Issues, GitHub Discussions, or Zulip.The "experimental" diagnostics mentioned here are the ones that make r-a feel fast. If you're used to other languages giving you warnings/errors as you type, you may have noticed r-a doesn't, which makes for an awkward and sluggish experience. Currently it offloads the responsibility of most type-related checking to , which runs after saving by default.A while ago, r-a started implementing diagnostics for type mismatches in function calls and such. So your editor lights up immediately as you type. But these aren't enabled by default. This change will bring more of those into the stable, enabled-by-default featureset.I have the following setupRust nightly / r-a nightlyand it honestly feels like an entirely different experience than writing rust 2 years ago. It's fast and responsive. There's still a gap to TS and Go and such, but its closing rapidly, and the contributors and maintainers have moved the DX squarely into the "whoa, this works really well" zone. Not to mention how hard this is with a language like Rust (traits, macros, lifetimes, are insanely hard to support)]]></content:encoded></item><item><title>Introducing Theta, an async actor framework for Rust</title><link>https://www.reddit.com/r/rust/comments/1mtf9c5/introducing_theta_an_async_actor_framework_for/</link><author>/u/Recent-Scarcity4154</author><category>rust</category><category>reddit</category><pubDate>Mon, 18 Aug 2025 07:24:44 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I'm excited to share **Theta** - a new async actor framework I've been working on that aims to be ergonomic, minimal, and performant.There are great actor frameworks out there, but I find some points to make them better especially regarding simplicity and remote support. Here are some of the key features.An actor instance is a very thin wrapper around a  and two MPSC channels. is just a MPSC sender.Distributed actor system powered by P2P protocol, .Even  could be passed around network boundary as regular data in message.Available with feature ."Monitor" suggested by Carl Hewitt's Actor Model is implemented as (possibly remote) monitoring feature.Available with feature .Seamless respawn of actor from snapshot on file system, AWS S3 etc.Available with feature .Compile to WebAssembly for running in browser or other WASM environmentsJust published  on crates.io! Would love to hear your thoughts! What features would you want to see in an actor framework?]]></content:encoded></item><item><title>A business card that&apos;s also an embedded device with LED display running a fluid simulation, written in Rust</title><link>https://github.com/Nicholas-L-Johnson/flip-card</link><author>/u/kibwen</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 22:29:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Is the *Nom* crate a good option for parsing a complex syntax? It seems like a really promising rust parsing library, if you have any experience with it please share it.</title><link>https://www.reddit.com/r/rust/comments/1mt1kf1/is_the_nom_crate_a_good_option_for_parsing_a/</link><author>/u/JKasonB</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 20:24:05 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What type of projects to professional Rust devs do?</title><link>https://www.reddit.com/r/rust/comments/1msyvjz/what_type_of_projects_to_professional_rust_devs_do/</link><author>/u/IHaveQuestions_42069</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 18:40:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Looking into a career change and Rust always fascinated me + it seemed like a great language to strengthen my understanding of lower-level programming (background is Data engineering in Snowflake / SQL / Python + a bit of Java, Javascript, & Go)I'm trying to understand, what work gets done in Rust? what industries are demanding it? what type of projects to company's want in Rust? Asking as I can try to orient myself as I start getting into it more]]></content:encoded></item><item><title>Just released doxx – a terminal .docx viewer inspired by Charm&apos;s glow package</title><link>https://www.reddit.com/r/rust/comments/1msyntz/just_released_doxx_a_terminal_docx_viewer/</link><author>/u/Effective_Title1224</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 18:32:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I got tired of open file.docx → wait 8 seconds → close Word just to read a document, so I built a terminal-native Word viewer!View  files directly in your terminal with (mostly) proper formattingTables actually look like tables (with Unicode borders!)Nested lists work correctly with indentationFull-text search with highlightingCopy content straight to clipboard with Export to markdown/CSV/JSONWorking on servers over SSH, I constantly hit Word docs I needed to check quickly. The existing solutions I'm aware of either strip all formatting (docx2txt) or require GUI apps. Wanted something that felt as polished as glow but for Word documents.50ms startup vs Word's 8+ secondsWorks over SSH (obviously)Preserves document structure and formattingSmart table alignment based on data typesInteractive outline view for long docsBuilt with Rust + ratatui and heavily inspired by Charm's glow package for viewing Markdown in the CLI (built in Go)!# Install cargo install --git https://github.com/bgreenwell/doxx # Use doxx quarterly-report.docx Still early but handles most Word docs I throw at it. Always wanted a proper Word viewer in my terminal toolkit alongside , , and friends. Let me know what you think!]]></content:encoded></item><item><title>Rewrite of Numaflow: A Stream Processing Platform Written in Rust</title><link>https://www.reddit.com/r/rust/comments/1msdzfr/rewrite_of_numaflow_a_stream_processing_platform/</link><author>/u/vm_vm_vm</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 01:29:41 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[A quick intro, Numaflow is an open-source, K8s-native platform for stream processing, and with the latest release it’s now running on a Rust-based data plane for faster, more reliable stream processing and here is our journey in Rust. Rust at the core → no GC pauses, memory safety etcMessage-level streaming → smoother tail latency for uneven workloads (great for AI and data workloads)Proven performance → ~40% higher throughput, ~30% less CPU, lower memory useFirst mature Rust option → not just bindings, the whole runtime is Rust]]></content:encoded></item><item><title>I&apos;m about to take me first rust interview tomorrow.. I&apos;am much worried about the coding interview part...any tips ?</title><link>https://www.reddit.com/r/rust/comments/1msdtxt/im_about_to_take_me_first_rust_interview_tomorrow/</link><author>/u/Just_Distance317</author><category>rust</category><category>reddit</category><pubDate>Sun, 17 Aug 2025 01:21:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[Media] Releasing Mach - a web fuzzing tool designed for massive workloads</title><link>https://www.reddit.com/r/rust/comments/1ms9u3t/media_releasing_mach_a_web_fuzzing_tool_designed/</link><author>/u/magixer</author><category>rust</category><category>reddit</category><pubDate>Sat, 16 Aug 2025 22:26:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/magixer ]]></content:encoded></item><item><title>I just published a minimal FAT32 file system driver written in #[no_std] rust. Designed specifically around the limitations of working with an SDCard in an embedded environment.</title><link>https://www.reddit.com/r/rust/comments/1mrz2lu/i_just_published_a_minimal_fat32_file_system/</link><author>/u/careyi4</author><category>rust</category><category>reddit</category><pubDate>Sat, 16 Aug 2025 15:50:49 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[The odyssey starts with me working on a new embedded systems project and wanting to log some data to an SDCard to then analyse it on my computer. I have been working on a years long project to develope my own suite of tools etc. for building robotics projects using a custom designed STM32 dev board running Rust. So far, the STM32 HAL (https://github.com/stm32-rs/stm32f4xx-hal) library has been excellent for this. I was happy when I found out this library supports hardware level SDIO for SDCard comms. However, the issue is, this is only very low level and provides only the ability to read and write blocks of 512 bytes at a time from specific block addresses.I decided this was the time to take on a side project of writing my own FAT32 driver which specifically operates within the constraints of the HAL library above. I started out by implementing all of the logic in a Python prototype running on my local machine. From this, I then ported all the logic over to no_std rust and again got that all working on my local machine. The key to this was to ensure that while I was using my machines underlying low level file IO, I kept it abstracted out to specifically read and write in blocks of 512 bytes at a time. The vision being that when I ran the rust code on my embedded platform, I just needed to swap out the IO functions for the ones provided by the HAL lib.Long story short, it all just worked first time!! I published my crate, imported it into my embedded project, compiled it and it just ran perfectly. I was honestly shocked by this, I was pretty sure it was going to work, but I was nervous, I had spent weeks working on this in the small amount of free time I have, so I was relieved when it just worked!Anyway, I just wanted to share what I built with you all, hope someone finds the project interesting.I will be making another video soon running through the latest.]]></content:encoded></item><item><title>typed-arrow - Provides compile‑time Arrow schemas for Rust.</title><link>https://github.com/tonbo-io/typed-arrow</link><author>/u/yacl</author><category>rust</category><category>reddit</category><pubDate>Sat, 16 Aug 2025 14:29:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[When working with arrow-rs, we noticed that schemas are declared at runtime. This often leads to runtime errors and makes development less safe.typed-arrow takes a different approach:Schemas are declared at compile time with Rust’s type system.This eliminates runtime schema errors.And introduces no runtime overhead — everything is checked and generated by the compiler.If you’ve run into Arrow runtime schema issues, and your schema is stable (not defined or switched at runtime), this project might be useful.]]></content:encoded></item></channel></rss>