<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Official News</title><link>https://www.awesome-dev.news</link><description></description><item><title>From idea to PR: A guide to GitHub Copilot’s agentic workflows</title><link>https://github.blog/ai-and-ml/github-copilot/from-idea-to-pr-a-guide-to-github-copilots-agentic-workflows/</link><author>Chris Reddington</author><category>official</category><pubDate>Tue, 1 Jul 2025 18:57:22 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[I got into software to ship ideas, not to chase down hard-coded strings after a late-breaking feature request. Unfortunately, many of our day-to-day tasks as developers involve branches working on boilerplate code, refactoring, and the “pre-work” to get to the fun stuff: shipping new features.So I turned to Copilot’s agentic workflows to help speed along some of that grunt work. In my latest  live stream, I put that theory to the test in a project where I wanted to localize an application that used: a  web app and a matching  iOS app living in two separate GitHub repos. spun up rapidly in  (on-demand dev environment) and  for the mobile portion. an issue built from a couple of paragraphs to “Add English, French, and Spanish localization.”By the end of my stream, that idea became a GitHub issue, which turned into a fully tested, review-ready PR while I fielded chat questions, and learned about the preview custom chat mode features in VS Code.Why I use agentic workflowsEven seasoned developers and teams still burn hours on jobs like:Turning vague requests into well-scoped issuesHunting down every file in a cross-cutting refactorWriting the same unit-test scaffolding again and againCopilot’s ability to create issues, along with its coding agent, custom chat modes in VS Code, and the new remote MCP backend fold those chores into one tight loop—issue to PR—while you stay firmly in the driver’s seat. You still review, tweak, and decide when to merge, but you skip the drudgery.Key capabilities covered in this livestream Turns any GitHub Issue you assign to Copilot into a PR, and works on that task asynchronously.Allows you to offload the boilerplate work while you focus on reviews and edge case logic.Create issues with CopilotConverts a natural-language prompt into a well-structured Issue with title, body, acceptance criteria, and file hints.Saves PM/eng refining and sets team members, or Copilot coding agent, up with the context they need to work effectively.Custom chat modes (in preview in VS Code)Lets you script repeatable AI workflows (e.g., , , ) that appear alongside the default  /  /  chat modes.Allows you to package instructions and relevant tools for easier use, helping your team follow similar conventions.Allows AI tools to access live GitHub context and tools, like issues, pull requests and code files. With the remote GitHub MCP server, you don’t need to install it locally, and can even authenticate with OAuth 2.0.Provides a smooth experience to accessing the GitHub MCP server, reducing the management overhead of a local server.Copilot agent mode is a real‑time collaborator that sits in your editor, works with you, and edits files based on your needs. Unlike the coding agent, Copilot agent mode works synchronously with you.Think of agent mode as the senior dev pair programming with you. It has access to several tools (like reading/writing code, running commands in the terminal, executing tools on MCP servers), and works alongside you.A GitHub repo you can push toA Copilot subscription with  enabled. (Did you know it’s now available for all paid tiers of GitHub Copilot including Copilot Business and Copilot Pro?)VS Code 1.101+ with the latest Copilot extension.Either:  (update your MCP configuration), or a local GitHub MCP server.Walk-through: localizing a Next.js appHere’s the exact flow I demoed on the most recent  stream.1. Capture the request as a GitHub IssueGo to the immersive view of Copilot Chat. At the bottom of the page, in the “Ask Copilot” box, describe what you want. For example, below is the prompt that I used. Create a GitHub Issue that brings i11n capability to the application. We must support English, French and Spanish.

The user must be able to change their language in their profile page. When they change the language, it must apply immediately across the site.

Please include an overview/problem statement in the issue, a set of acceptance criteria, and pointers on which files need updating/creating.Copilot drafts that into an issue, which includes a title, acceptance criteria, and a loose action plan. From there, you can assign that issue to Copilot, and let it cook in the background. 2. Let the coding agent turn the issue into a PRShortly after assignment, the coding agent:Reviews the task at hand, explores the current state of the codebase, and forms a plan to complete the task.If you have any custom instructions configured, then the coding agent will also use those as context. For example, we specify that npm run lint and npm run test should pass before committing.Once complete, it opens a draft PR for your review.While that runs, you can keep coding, use it as an opportunity to learn (like we learned about custom chat modes) or grab a coffee.3. Review the PR like you normally wouldWhether it’s a colleague, collaborator, or Copilot writing the code, you still need a reviewer. So it’s important to make sure you look the code over carefully, just like you would any other pull request.Start by reviewing the body of the pull request, which Copilot will have helpfully kept up to date.Then, review the code changes in the files changed tab, understanding what has changed and why. I also like to take a look at the coding agent session to understand the approach Copilot took to solving the problem.Once you are comfortable, you may want to try the code out manually in a GitHub Codespace. Or, you may want to run any existing CI checks through your GitHub Actions workflows. But again, make sure you have carefully reviewed the code before executing it.All being well, you will have green check marks being returned from your CI. However, there’s always a possibility that you encounter failures, or spot some changes in your manual testing. For example, I spotted some hard-coded strings that the agent hadn’t addressed. Once again, we approach this just like we would any other pull request. We can post our feedback in a comment. For example, here’s the comment I used:That’s a great start. However, there are a lot of pages which are hardcoded in English still. For example, the flight search/bookings page, the check reservation page. Can you implement the localization on those pages, please?Copilot will react to the comment once again, and get to work in another session. Level up your workflows with custom chat modesOpen the command palette in Visual Studio CodeSelect Create new custom chat mode file.You’ll be asked to save it either in the workspace (to allow collaborating with others), or in the local user data folder (for your use). We opted for the workspace option.Enter the name. This is the name that will appear in the chat mode selection box, so pay attention to any capitalization.You should see a new file has been created with the extension . This is where you can configure the instructions, and the available tools for your new custom chat mode.Below is the example that we used in the livestream, slightly modified from the VS Code team’s docs example. We’ve added the create_issue tool to the list of allowed tools, adjusted our expectations of what’s included in the issue and added an instruction about creating the issue with the `create_issue` tool once revisions are complete and approved by the user.---

description: Generate an implementation plan for new features or refactoring existing code.

tools: ['codebase', 'fetch', 'findTestFiles', 'githubRepo', 'search', 'usages', 'github', 'create_issue']

---

# Planning mode instructions

You are in planning mode. Your task is to generate an implementation plan for a new feature or for refactoring existing code.

Don't make any code edits, just generate a plan.

The plan consists of a Markdown document that describes the implementation plan, including the following sections:

* Overview: A brief description of the feature or refactoring task.

* Requirements: A list of requirements for the feature or refactoring task.

* Implementation Steps: A detailed list of steps to implement the feature or refactoring task.

* Testing: A list of tests that need to be implemented to verify the feature or refactoring task.

Once the plan is complete, ask the user if they would like to create a GitHub issue for this implementation plan. If they respond affirmatively, proceed to create the issue using the `create_issue` tool.When the file is available in your teammate’s local repositories (so they’ve pulled the changes locally), VS Code surfaces the mode in the chat dropdown, allowing you to configure chat modes that are consistent and convenient across your team.Remote MCP: removing the local setupYou may be used to running MCP locally through npm packages or as docker containers. However, remote MCP servers allow you to reduce the management overhead of running these tools locally. There may be other benefits too. For example, the remote GitHub MCP Servers allows you to authenticate using OAuth 2.0 instead of Personal Access Tokens.To use the GitHub Remote MCP Server in VS Code, you’ll need to update the MCP configuration. You can find the instructions on how to do that in the GitHub MCP Server repository.Going mobile: Copilot agent mode in XcodeWhile we didn’t show it in depth, I quickly walked through one of my previous agent mode sessions in Xcode. It showed how I gave a similar prompt to Copilot, asking to add internationalization to the app, which we were able to see in the main navigation bar of the app running in the simulator.We need to implement internationalization in the app. Please make the following changes:

1. The user can select from suported languages (English, Spanish, French) from a dropdown in their profile.

2. The main tab view should support internationalization. No other parts of the app should be changed for now.

3. When the user changes the language, it should update the rendered text instantly.Keep issues tightly scopedAsk the agent to “re-architect the app”Provide acceptance criteriaAssume the agent knows your intentCarefully review the changes madeExecute code or merge a PR without a reviewIterate with Copilot. How often do you get something right on the first shot?Expect perfection first timeAgentic workflows within GitHub Copilot aren’t magic; they’re tools. When a single click can help reduce technical debt (or knock out any other repetitive task you dread), why not let Copilot handle the boilerplate while you tackle the more challenging, fun, and creative problems?]]></content:encoded></item><item><title>An inside look at Meta’s transition from C to Rust on mobile</title><link>https://engineering.fb.com/2025/07/01/developer-tools/an-inside-look-at-metas-transition-from-c-to-rust-on-mobile/</link><author></author><category>dev</category><category>official</category><pubDate>Tue, 1 Jul 2025 16:00:23 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Have you ever worked is legacy code? Are you curious what it takes to modernize systems at a massive scale?Pascal Hartig is joined on the latest Meta Tech Podcast by Elaine and Buping, two software engineers working on a bold project to rewrite the decades-old C code in one of Meta’s core messaging libraries in Rust. It’s an ambitious effort that will transform a central messaging library that is shared across Messenger, Facebook, Instagram, and Meta’s AR/VR platforms.They discuss taking on a project of this scope – even without a background in Rust, how they’re approaching it, and what it means to optimize for ‘developer happiness.’Download or listen to the episode below:You can also find the episode wherever you get your podcasts, including:The Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.And if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.]]></content:encoded></item><item><title>Understand your software’s supply chain with GitHub’s dependency graph</title><link>https://github.blog/security/supply-chain-security/understand-your-softwares-supply-chain-with-githubs-dependency-graph/</link><author>Andrea Griffiths</author><category>official</category><pubDate>Tue, 1 Jul 2025 16:00:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[What if you could spot the weakest link in your software supply chain before it breaks?With GitHub’s dependency graph, you can. By providing a clear, complete view of the external packages your code depends on, both directly and indirectly, it allows you to understand, secure, and manage your project’s true footprint.If you’re like me and sometimes lose track of what’s actually powering your applications (we’ve all been there!), GitHub’s dependency graph is about to become your new best friend. What is the dependency graph?Here’s the thing: Every modern software project is basically an iceberg. That small manifest file with your direct dependencies seems quite harmless at first glance. But underneath? There’s this massive, hidden world of transitive dependencies that most of us never think about. The GitHub dependency graph maps this entire underwater world. Think of it like a family tree, but for your code. Each package is a family member, and each dependency relationship shows who’s related to whom (and trust me, some of these family trees get  complicated).Each package is a node. Each dependency relationship is an edge. The result? A full visual and structured representation of your software’s external codebase.In some cases, 95–97% of your code is actually someone else’s. The dependency graph helps you make sense of that reality.Let that sink in for a moment. We’re basically curators of other people’s work, and the dependency graph finally helps us make sense of that reality.When vulnerabilities are discovered in open source packages, the consequences ripple downstream. If you don’t know a vulnerable dependency is part of your project, it’s hard to take action.The dependency graph isn’t just a cool visualization (though it is pretty neat to look at). It’s the foundation that makes Dependabot alerts possible. When a security issue is found in any of your dependencies (even a transitive one), GitHub notifies you. You get the full picture of what’s in your supply chain, how it got there, and what you can actually do about it.See it in action: From 21 to 1,000 dependenciesEric showed us a project that looked innocent enough: (the ones actually listed in package.json) (including everything that got pulled in along the way)With the dependency graph, you can finally:Understand which dependencies are direct vs. transitiveTrace how a package like Log4j ended up in your codebase. (Spoiler: it probably came along for the ride with something else.)Know what’s yours to fix and what depends on an upstream maintainerTighten your supply chain with DependabotDependabot runs on top of the dependency graph—so enabling the graph is what makes Dependabot’s vulnerability alerts and automatic fix suggestions possible. Pro tip: Filter for direct dependencies first. These are the ones you can actually control, so focus your energy there instead of pulling your hair out over transitive dependencies that are someone else’s responsibility.How to enable the dependency graphYou can enable the dependency graph in your repository settings under Security > Dependency Graph. If you turn on , the graph will be enabled automatically.Using GitHub Actions? Community-maintained actions can generate a Software Bill of Materials (SBOM) and submit it to GitHub’s Dependency submission API, even if your language ecosystem doesn’t support auto-discovery.✅  Dependency graph and Dependabot alerts are free for all repositories.You can’t secure what you can’t see. GitHub’s dependency graph gives you visibility into the 90%+ of your codebase that comes from open source libraries and helps you take action when it counts.(seriously, do it now)Use it with Dependabot for automated alerts and fixesFinally discover what’s actually in your software supply chainYour future self (and your security team) will thank you.]]></content:encoded></item><item><title>Creating a Website with Sphinx and Markdown</title><link>https://www.blog.pythonlibrary.org/2025/07/01/creating-a-website-with-sphinx-and-markdown/</link><author>Mike</author><category>dev</category><category>official</category><category>python</category><pubDate>Tue, 1 Jul 2025 12:28:00 +0000</pubDate><source url="https://www.blog.pythonlibrary.org/">Python Blog</source><content:encoded><![CDATA[Sphinx is a Python-based documentation builder. The Python documentation is written using Sphinx. The Sphinx project supports using ReStructuredText and Markdown, or a mixture of the two. Each page of your documentation or website must be written using one of those two formats.In this tutorial, you will learn how to use Sphinx to create a documentation site. Here is an overview of what you’ll learn:Making Markdown work in SphinxBuilding your Sphinx siteAdding content to your siteLet’s start by installing all the packages you need to get Sphinx working!You will need the following packages to be able to use Sphinx and Markdown:You should install these package in a Python virtual environment. Open up your terminal and pick a location where you would like to create a new folder. Then run the following command:python -m venv NAME_OF_VENV_FOLDEROnce you have the virtual environment, you need to activate it. Go into the  folder and run the activate command in there.Now you can install the dependencies that you need using pip, which will install them to your virtual environment.Here’s how to install them using pip:python -m pip install myst-parser sphinxOnce your packages are installed, you can learn how to set up your site!Now that your packages are installed, you must set up your Sphinx website. To create a barebones Sphinx site, run the following command inside your virtual environment:sphinx-quickstart NAME_OF_SITE_FOLDERIt will ask you a series of questions. The Sphinx documentation recommends keeping the source and build folders separate. Otherwise, you can set the other fields as needed or accept the defaults.You will now have the following tree structure in your SITE_FOLDER:You will work with the files and directories in this structure for the rest of the tutorial.The next step on your Sphinx journey is to enable Markdown support.Making Markdown Work in SphinxGo into the  directory and open the  file in your favorite Python IDE. Update the  and the  variables to the following (or add them if they do not exist):extensions = ['myst_parser']

source_suffix = ['.rst', '.md']These changes tell Sphinx to use the Myst parser for Markdown files. You also leave ReStructuredText files in there so that your Sphinx website can handle that format.You now have enough of your site available to build it and ensure it works.Building Your Sphinx SiteYou can now build a simple site with only an index page and the auto-generated boilerplate content. In your terminal, run the following command in the root of your Sphinx folder:sphinx-build -M html .\source\ .\build\The HTML files will be created inside the  folder. If you open the index page, it will look something like this:Good job! You now have a Sphinx website!Now you need to add some custom content to it.Adding Content to Your SiteYou can add ReStructuredText or Markdown files for each page of your site.  using the  section:.. toctree::
   :maxdepth: 2
   :caption: Contents:

   SUB_FOLDER/acknowledgments.md
   doc_page1.md
   OTHER_FOLDER/sub_doc_page1.mdLet’s add some real content. Create a new file called  in the root folder that contains the  file. Then enter the following text in your new Markdown file:# Python: All About Decorators

Decorators can be a bit mind-bending when first encountered and can also be a bit tricky to debug. But they are a neat way to add functionality to functions and classes. Decorators are also known as a “higher-order function”. This means that they can take one or more functions as arguments and return a function as its result. In other words, decorators will take the function they are decorating and extend its behavior while not actually modifying what the function itself does.

There have been two decorators in Python since version 2.2, namely **classmethod()** and **staticmethod()**. Then PEP 318 was put together and the decorator syntax was added to make decorating functions and methods possible in Python 2.4. Class decorators were proposed in PEP 3129 to be included in Python 2.6. They appear to work in Python 2.7, but the PEP indicates they weren’t accepted until Python 3, so I’m not sure what happened there.

Let’s start off by talking about functions in general to get a foundation to work from.

## The Humble Function

A function in Python and in many other programming languages is just a collection of reusable code. Some programmers will take an almost bash-like approach and just write all their code in a file with no functions. The code just runs from top to bottom. This can lead to a lot of copy-and-paste spaghetti code. Whenever two pieces of code do the same thing, they can almost always be put into a function. This will make updating your code easier since you’ll only have one place to update them.Make sure you save the file. Then, re-run the build command from the previous section. Now, when you open the  file, you should see your new Markdown file as a link that you click on and view.Sphinx is a powerful way to create documentation for your projects. Sphinx has many plugins that you can use to make it even better. For example, you can use sphinx-apidoc to automatically generate documentation from your source code using the autodoc extension.If you are an author and you want to share your books online, Sphinx is a good option for that as well. Having a built-in search functionality makes it even better. Give Sphinx a try and see what it can do for you!]]></content:encoded></item><item><title>Meta joins Kotlin Foundation</title><link>https://engineering.fb.com/2025/06/30/android/meta-joins-kotlin-foundation/</link><author></author><category>dev</category><category>official</category><pubDate>Mon, 30 Jun 2025 16:00:30 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub Advisory Database by the numbers: Known security vulnerabilities and what you can do about them</title><link>https://github.blog/security/github-advisory-database-by-the-numbers-known-security-vulnerabilities-and-what-you-can-do-about-them/</link><author>Jonathan Evans</author><category>official</category><pubDate>Fri, 27 Jun 2025 16:00:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[The GitHub Advisory Database (Advisory DB) is a vital resource for developers, providing a comprehensive list of known security vulnerabilities and malware affecting open source packages. This post analyzes trends in the Advisory DB, highlighting the growth in reviewed advisories, ecosystem coverage, and source contributions in 2024. We’ll delve into how GitHub provides actionable data to secure software projects.The GitHub Advisory Database contains a list of known security vulnerabilities and malware, grouped in three categories: GitHub-reviewed advisories: Manually reviewed advisories in software packages that GitHub supports. These are automatically pulled from the National Vulnerability Database (NVD) and are either in the process of being reviewed, do not affect a supported package, or do not discuss a valid vulnerability. These are specific to malware threats identified by the npm security team.GitHub-reviewed advisories are security vulnerabilities that have been mapped to packages in ecosystems we support. We carefully review each advisory for validity and ensure that they have a full description, and contain both ecosystem and package information.Every year, GitHub increases the number of advisories we publish. We have been able to do this due to the increase in advisories coming from our sources (see Sources section below), expanding our ecosystem coverage (also described below), and review campaigns of advisories published before we started the database. In the past five years, the database has gone from fewer than 400 reviewed advisories to over 20,000 reviewed advisories in October of 2024.Unreviewed advisories are security vulnerabilities that we publish automatically into the GitHub Advisory Database directly from the National Vulnerability Database feed. The name is a bit of a misnomer as many of these advisories have actually been reviewed by a GitHub analyst. The reason why they fall into this category is because they are not found in a package in one of the supported ecosystems or are not discussing a valid vulnerability, and all have been reviewed by analysts other than someone from the GitHub Security Lab. Even though most of these advisories will never turn into a reviewed advisory, we still publish them so that you do not have to look in multiple databases at once.Malware advisories relate to vulnerabilities caused by malware, and are security advisories that GitHub publishes automatically into the GitHub Advisory Database directly from information provided by the npm security team. Malware advisories are currently exclusive to the npm ecosystem. GitHub doesn’t edit or accept community contributions on these advisories.GitHub-reviewed advisories include security vulnerabilities that have been mapped to packages in ecosystems we support. Generally, we name our supported ecosystems after the software programming language’s associated package registry. We review advisories if they are for a vulnerability in a package that comes from a supported registry.Vulnerabilities in Maven and Composer packages are nearly half of the advisories in the database. npm, pip, and Go make up much of the rest, while the other ecosystems have a much smaller footprint.This has not always been the case. When the database was initially launched, NPM advisories dominated the database, but as we have expanded our coverage and added support for new ecosystems, the distribution mix has changed.We add advisories to the GitHub Advisory Database from the following sources: This is a huge source of vulnerabilities covering all types of software. We publish all NVD advisories but only review those relevant to our supported ecosystems, which reduces noise for our users.GitHub Repository Advisories: The second largest source is made up of advisories published through GitHub’s repository security advisory feature. Similar to NVD, these aren’t restricted to our supported ecosystems. However, we provide better coverage of the repository advisories because they focus exclusively on open source software. These are reports from the community that are almost exclusively requesting updates to existing advisories.Other Specialized Sources: Sources like PyPA Advisories (for Python) and Go Vulncheck (for Go) that focus on specific ecosystems. Because they only cover packages within our supported ecosystems, most of their advisories are relevant to us and get reviewed.If you add up the number of reviewed advisories from each source, you will find that total is more than the total reviewed advisories. This is because each source can publish an advisory for the same vulnerability. In fact, over half of our advisories have more than one source.Of the advisories with a single source, nearly all of them come from NVD/CVE. This justifies NVD/CVE as a source, even though it is by far the noisiest.2024 saw a significant increase (39%) in the number of advisories imported from our sources. This is for the most part caused by an increase in the number of CVE records published.In addition to publishing advisories in the GitHub Advisory Database, we are also a CVE Numbering Authority (CNA) for any repository on GitHub. This means that we issue CVE IDs for vulnerabilities reported to us by maintainers, and we publish the vulnerabilities to the CVE database once the corresponding repository advisory is published.GitHub published over 2,000 CVE records in 2024, making us the fifth-largest CNA in the CVE Program.The GitHub CNA is open to  on GitHub, not just ones in a supported ecosystem.Given the constant deluge of reported vulnerabilities, you’ll want tools that can help you prioritize your remediation efforts. To that end, GitHub provides additional data in the advisory to allow readers to prioritize their vulnerabilities. In particular, there are: A low to critical rating for how severe the vulnerability is likely to be, along with a corresponding CVSS score and vector. CWE identifiers provide a programmatic method for determining the type of vulnerability. The Exploit Prediction Scoring System, or EPSS, is a system devised by the global Forum of Incident Response and Security Teams (FIRST) for quantifying the likelihood a vulnerability will be attacked in the next 30 days.Using these ratings, half of all vulnerabilities (15% are Critical and 35% are High) warrant immediate or near-term attention. By focusing remediation efforts on these, you can significantly reduce risk exposure while managing workload more efficiently.The CVSS specification says the base score we provide, “reflects the severity of a vulnerability according to its intrinsic characteristics which are constant over time and assumes the reasonable worst-case impact across different deployed environments.” However, the worst-case scenario for your deployment may not be the same as CVSS’s. After all, a crash in a word processor is not as severe as a crash in a server. In order to give more context to your prioritization, GitHub allows you to filter alerts based on the type of vulnerability or weakness using CWE identifiers. So you have the capability to never see another regular expression denial of service (CWE-1333) vulnerability again or always see SQL injection (CWE-89) vulnerabilities.Number of advisories in 2024Improper Neutralization of Input During Web Page Generation (‘Cross-site Scripting’)Exposure of Sensitive Information to an Unauthorized ActorImproper Limitation of a Pathname to a Restricted Directory (‘Path Traversal’)Improper Input ValidationImproper Control of Generation of Code (‘Code Injection’)Improper Neutralization of Special Elements used in an SQL Command (‘SQL Injection’)Cross-Site Request Forgery (CSRF)Uncontrolled Resource ConsumptionStill drowning in vulnerabilities? Try using EPSS to focus on vulnerabilities likely to be attacked in the next 30 days. EPSS uses data from a variety of sources to create a probability of whether exploitation attempts will be seen in the next 30 days for a given vulnerability. As you can see from the chart below, if you focus on vulnerabilities with EPSS scores of 10% or higher (approx. 7% of all vulnerabilities in the Advisory DB), you can cover nearly all of the vulnerabilities that are likely to see exploit activity.Percentage of overall vulnerabilitiesExpected vulnerabilities in range attacked within the next 30 daysPercentage of total attacked vulnerabilitiesImportant caveats to remember when using EPSS:Low probability events occur.EPSS does not tell you whether a vulnerability is exploited; it only claims how likely it is.EPSS scores are updated daily and will change as new information comes in, so a low-probability vulnerability today may become high probability tomorrow.For more details on how to use CVSS and EPSS for prioritization, see our blog on prioritizing Dependabot alerts.The GitHub Advisory DB isn’t just a repository of vulnerabilities. It powers tools that help developers secure their projects. Services like Dependabot use the Advisory DB to:Identify vulnerabilities: It checks if your projects use any software packages with known vulnerabilities. It recommends updated versions of packages that fix those vulnerabilities when available. You’ll only get notified about vulnerabilities that affect the version of the package you are using.The GitHub Advisory Database is a powerful resource for tracking open source software vulnerabilities, with over 22,000 reviewed advisories to date. By focusing on popular package registries, GitHub allows you to definitively connect vulnerabilities to the packages you are using. Additional data such as CVSS and EPSS scores help you properly prioritize your mitigation efforts.GitHub’s role as a CVE Numbering Authority extends beyond the Advisory Database, ensuring that thousands of vulnerabilities each year reach the broader CVE community. Want to ensure your vulnerability fix reaches your users? Create a GitHub security advisory in your repository to take advantage of both the GitHub Advisory Database and GitHub’s CNA services.]]></content:encoded></item><item><title>Announcing Rust 1.88.0</title><link>https://blog.rust-lang.org/2025/06/26/Rust-1.88.0/</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.88.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.88.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!This feature allows -chaining  statements inside  and  conditions, even intermingling with boolean expressions, so there is less distinction between / and /. The patterns inside the  sub-expressions can be irrefutable or refutable, and bindings are usable in later parts of the chain as well as the body.For example, this snippet combines multiple conditions which would have required nesting  and  blocks before:Let chains are only available in the Rust 2024 edition, as this feature depends on the  temporary scope change for more consistent drop order.Earlier efforts tried to work with all editions, but some difficult edge cases threatened the integrity of the implementation. 2024 made it feasible, so please upgrade your crate's edition if you'd like to use this feature!Rust now supports writing naked functions with no compiler-generated epilogue and prologue, allowing full control over the generated assembly for a particular function. This is a more ergonomic alternative to defining functions in a  block. A naked function is marked with the  attribute, and its body consists of a single  call.The handwritten assembly block defines the  function body: unlike non-naked functions, the compiler does not add any special handling for arguments or return values. Naked functions are used in low-level settings like Rust's , operating systems, and embedded applications.Look for a more detailed post on this soon!The  predicate language now supports boolean literals,  and , acting as a configuration that is always enabled or disabled, respectively. This works in Rust conditional compilation with  and  attributes and the built-in  macro, and also in Cargo  tables in both configuration and manifests.Previously, empty predicate lists could be used for unconditional configuration, like  for enabled and  for disabled, but this meaning is rather implicit and easy to get backwards.  and  offer a more direct way to say what you mean.
Cargo automatic cache cleaningStarting in 1.88.0, Cargo will automatically run garbage collection on the cache in its home directory!When building, Cargo downloads and caches crates needed as dependencies. Historically, these downloaded files would never be cleaned up, leading to an unbounded amount of disk usage in Cargo's home directory. In this version, Cargo introduces a garbage collection mechanism to automatically clean up old files (e.g.  files). Cargo will remove files downloaded from the network if not accessed in 3 months, and files obtained from the local system if not accessed in 1 month. Note that this automatic garbage collection will not take place if running offline (using  or ).Cargo 1.78 and newer track the access information needed for this garbage collection. This was introduced well before the actual cleanup that's starting now, in order to reduce cache churn for those that still use prior versions. If you regularly use versions of Cargo even older than 1.78, in addition to running current versions of Cargo, and you expect to have some crates accessed exclusively by the older versions of Cargo and don't want to re-download those crates every ~3 months, you may wish to set cache.auto-clean-frequency = "never" in the Cargo configuration, as described in the docs.For more information, see the original unstable announcement of this feature. Some parts of that design remain unstable, like the  subcommand tracked in cargo#13060, so there's still more to look forward to!These previously stable APIs are now stable in const contexts:The  target has been demoted to Tier 2, as mentioned in an earlier post. This won't have any immediate effect for users, since both the compiler and standard library tools will still be distributed by  for this target. However, with less testing than it had at Tier 1, it has more chance of accumulating bugs in the future.Many people came together to create Rust 1.88.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>From pair to peer programmer: Our vision for agentic workflows in GitHub Copilot</title><link>https://github.blog/news-insights/product-news/from-pair-to-peer-programmer-our-vision-for-agentic-workflows-in-github-copilot/</link><author>Tim Rogers</author><category>official</category><pubDate>Wed, 25 Jun 2025 16:00:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[Software development has always been a deeply human, collaborative process. When we introduced GitHub Copilot in 2021 as an “AI pair programmer,” it was designed to help developers stay in the flow, reduce boilerplate work, and accelerate coding.But what if Copilot could be more than just an assistant? What if it could actively collaborate with you—working alongside you on synchronous tasks, tackling issues independently, and even reviewing your code?That’s the future we’re building.Our vision for what’s next Today, AI agents in GitHub Copilot don’t just assist developers but actively solve problems through multi-step reasoning and execution. These agents are capable of:Independent problem solving: Copilot will break down complex tasks and take the necessary steps to solve them, providing updates along the way. Whether working in sync with you or independently in the background, Copilot will iterate on its own outputs to drive progress. Copilot will proactively assist with tasks like issue resolution, testing, and code reviews, ensuring higher-quality, maintainable code.Rather than fitting neatly into synchronous or asynchronous categories, the future of Copilot lies in its ability to flexibly transition between modes—executing tasks independently while keeping you informed and in control. This evolution will allow you to focus on higher-level decision-making while Copilot takes on more of the execution.Let’s explore what’s already here—and what’s coming next.Copilot in action: Taking steps toward our vision Agent mode: A real-time AI teammate inside your IDEAgent mode lives where you code and feels like handing your computer to a teammate for a minute: it types on your screen while you look on, and can grab the mouse. When you prompt it, the agent takes control, works through the problem, and reports its work back to you with regular check-in points. It can:Read your entire workspace to understand context.Plan multi‑step fixes or refactors (and show you the plan first).Apply changes, run tests, and iterate in a tight feedback loop. whenever intent is ambiguous.Run and refine its own work through an “agentic loop”—planning, applying changes, testing, and iterating.Rather than just responding to requests, Copilot in agent mode actively works toward your goal. You define the outcome, and it determines the best approach—seeking feedback from you as needed, testing its own solutions, and refining its work in real time. Think of it as pair programming in fast forward: you’re watching the task unfold in real time, free to jump in or redirect at any step. ✨Coding agent: An AI teammate that works while you don’t Not all coding happens in real time. Sometimes, you need to hand off tasks to a teammate and check back later.That’s where  comes in—and it’s our first step in transforming Copilot into an independent agent. Coding agent spins up its own secure dev environment in the cloud. You can assign multiple issues to Copilot, then dive into other work (or grab a cup of coffee!) while it handles the heavy lifting. It can:Clone your repo and bootstrap tooling in isolation.Break the issue into steps, implement changes, and write or update tests.by running your tests and linter. and iterate based on your PR review comments. so you can peek in—or jump in—any time.Working with coding agent is like asking a teammate in another room—with their own laptop and setup—to tackle an issue. You’re free to work on something else, but you can pop in for status or feedback whenever you like.Less TODO, more done: The next stage of Copilot’s agentic futureThe next stage of Copilot is being built on three converging pillars: Ongoing breakthroughs in large language models keep driving accuracy up while pushing latency and cost down. Expanded context windows now span entire monoliths, giving Copilot the long-range “memory” it needs to reason through complex codebases and return answers grounded in your real code.Deeper contextual awareness. Copilot increasingly understands the full story behind your work—issues, pull-request history, dependency graphs, even private runbooks and API specs (via MCP). By tapping this richer context, it can suggest changes that align with project intent, not just syntax.Open, composable foundation. We’re designing Copilot to slot into  stack—not the other way around. You choose the editor, models, and tools; Copilot plugs in, learns your patterns, and amplifies them. You’re in the driver’s seat, steering the AI to build, test, and ship code faster than ever.Taken together, these pillars move Copilot beyond a single assistant toward a flexible AI teammate—one that can help any team, from three developers in a garage to thousands in a global enterprise, plan, code, test, and ship with less friction and more speed.So, get ready for what’s next. The next wave is already on its way. ]]></content:encoded></item><item><title>An Intro to ty – The Extremely Fast Python type checker</title><link>https://www.blog.pythonlibrary.org/2025/06/25/an-intro-to-ty-the-extremely-fast-python-type-checker/</link><author>Mike</author><category>dev</category><category>official</category><category>python</category><pubDate>Wed, 25 Jun 2025 12:45:46 +0000</pubDate><source url="https://www.blog.pythonlibrary.org/">Python Blog</source><content:encoded><![CDATA[Ty is a brand new, extremely fast Python type checker written in Rust from the fine folks at Astral, the makers of Ruff. Ty is in preview and is not ready for production use, but you can still try it out on your code base to see how it compares to Mypy or other popular Python type checkers.If you prefer to install ty, you can use pip:Using the ty Type CheckerWant to give ty a try? You can run it in much the same way as you would Ruff. Open up your terminal and navigate to your project’s top-level directory. Then run the following command:If ty finds anything, you will quickly see the output in your terminal.Astral has also provided a way to exclude files from type checking. By default, ty ignores files listed in an  or  file.Ruff is a great tool and has been adopted by many teams since its release. Ty will likely follow a similar trajectory if it as fast and useful as Ruff has been. Only time will tell. However, these new developments in Python tooling are exciting and will be fun to try. If you have used ty, feel free to jump into the comments and let me know what you think.]]></content:encoded></item></channel></rss>