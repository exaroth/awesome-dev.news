<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Official News</title><link>https://www.awesome-dev.news</link><description></description><item><title>GitHub Game Off 2025 theme announcement</title><link>https://github.blog/company/github-game-off-2025-theme-announcement/</link><author>Lee Reilly</author><category>official</category><pubDate>Sat, 1 Nov 2025 20:37:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[Get ready for the annual Game Off, our month-long game jam that has inspired thousands of developers to make, share, and play games since 2012. Whether you’re a first-time jammer or a returning champion, this November is your chance to make something unforgettable.The theme for this year? !You have until December 1, 2025, at 13:37 PST to build a game  based on the theme. How you interpret it is entirely up to you. Don’t overthink it. Just ride the creative wave and see where it takes you. 🏄🏻Need inspiration? Here are a few concept ideasA space shooter where you fly through gravitational waves and wormholes.A survival game where you build a coastal base and A tower defense game where you battle waves of increasingly powerful baddies.A skateboard game where you ride a sine wave, shredding through peaks and troughs. A rhythm game where you catch the beat and ride the wave.A racing game where you drift through vaporwave skylines and a totally tubular synthwave soundtrack.A physics puzzler where you bounce, reflect, and refract energy waves.A remake of a class you enjoyed when you were younger resulting in endless waves of nostalgia.Whatever form your game takes, whether it crashes, ripples, or totally wipes out… we can’t wait to see it.Stuck for ideas? GitHub Copilot might be able to help. Try asking, “What are some fun games I could create with the game jam theme, WAVES?”Work alone or on a team. Use whatever programming languages, game engines, or libraries you like. Create a free GitHub account if you don’t have one. Hop onto the itch.io Game Off 2025 page. If you don’t already have an itch.io account, you can sign in with your GitHub account.Create a public repository. Store your source code on GitHub. Push your game before Submit your game on itch.io. Once submitted, you’ll be able to play other entries and cast your votes.After the submission period ends, participants will vote on each other’s games. Entries will be evaluated in the following categories:Voting will end on January 8, 2026, at 13:37 PST. Winners will be announced on the GitHub Blog and social channels on January 10, 2026, at 13:37 PST.Game Off is intentionally relaxed, but here are a few simple guidelines to keep things fair and fun:Your game must live in a GitHub repository. You should start from scratch, but you can use templates. The vast majority of the work should be done in the game jam period.License it however you like. Open source is encouraged, but not required. Work however you’re most comfortable.Use any tools or assets you prefer. Open source, commercial, or your own creations are all welcome.AI-assisted development is allowed.That’s it. Keep it creative, respectful, and fun, and remember to push your code before the deadline.You don’t need to be an expert. Many participants build their first game during Game Off. Some use popular engines, others build their own, and a few even create games for classic hardware like the NES, Game Boy, or ZX Spectrum. However you make it, there’s no wrong way to play.Here are a few engines you might want to explore:: Great for 2D and 3D games. Open source, lightweight, and beginner-friendly.: Ideal for 3D or mobile games with plenty of tutorials and asset packs available.: Best for cinematic visuals, complex 3D games, and high-end experiences.: Good choice for browser-based 2D arcade or platformer games.: A solid option for learning game development basics or prototyping ideas quickly.: Modern, data-driven engine for developers who like performance and clean ECS design.: Lightweight and fast, good for 2D games and creative coding projects.: Works well for mobile-first 2D games if you already use Flutter.: Simple and powerful engine for 2D games written in Go.: Cross-platform 2D engine with built-in tools and an active indie community.: A familiar choice for developers coming from Java or Android backgrounds.: Great for retro-style 2D games, platformers, and jam projects.The Game Off 2025 Community is a great place to ask questions or look for teammates. There’s also a friendly community-run Discord server.Game Off is the perfect opportunity to check it out (version control pun intended).Whether your build floats or sinks, you’re part of something swell. Join thousands of developers around the world for a month of creativity, learning, and code-powered fun. Let’s hang ten on your keyboard  🌊 🤙 and make some WAVES together.]]></content:encoded></item><item><title>Measuring what matters: How offline evaluation of GitHub MCP Server works</title><link>https://github.blog/ai-and-ml/generative-ai/measuring-what-matters-how-offline-evaluation-of-github-mcp-server-works/</link><author>Ksenia Bobrova</author><category>official</category><pubDate>Thu, 30 Oct 2025 21:46:07 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[MCP (Model Context Protocol) is a simple, common way for AI models (LLMs) to talk to APIs and data. Think of it like a universal plug: if both sides support MCP, they can connect and work together. An MCP server is any service or app that “speaks MCP” and offers tools the model can use, publishing a list of tools, what each tool does, and what inputs (parameters) each tool needs. The GitHub MCP Server is the foundation for many GitHub Copilot workflows, both inside and outside of GitHub. As an engineering team working on GitHub MCP, we’re always looking to deliver new features and functionality, while avoiding regressions and improving quality with every iteration. And how we name a tool, explain what it does, and spell out its parameters directly affects whether the model picks the right tool, in the right order, with the right arguments. When it comes to our work, small edits matter: tightening a description, adding or removing a tool, or combining a few similar tools can shift results a lot. When descriptions are off, agents choose the wrong tool, skip a step, send arguments in the wrong format, or drop them entirely. The outcome is weak. We need a safe way to change MCP and know if things actually got better, not worse. That’s where offline evaluation comes in.Offline evaluation catches regressions before users see them and keeps the feedback loop short, so we can ship changes that genuinely improve performance.This article walks through our evaluation pipeline and explains the metrics and algorithms that help us achieve these goals.How automated offline evaluation worksOur offline evaluation pipeline checks how well our tool prompts work across different models. The tool instructions are kept simple and precise so the model can choose the right tool and fill in the correct parameters. Because LLMs vary in how they use tools, we systematically test each model–MCP pairing to measure compatibility, quality, and gaps.We have curated datasets that we use as benchmarks. Every benchmark contains the following parameters: : This is a user request formulated in natural language. : Tools we expect to be called.: Arguments we expect to be passed to each tool.Asking how many issues were created in a given time period  How many issues were created in the github/github-mcp-server repository during April 2025?  list_issues with:owner: github 
repo: github-mcp-server 
since: 2025-04-01T00:00:00Z Merge PR 123 in github/docs using squash merge with title “Update installation guide” merge_pull_request with :owner: github
repo: docs 
pullNumber: 123 
merge_method: squash 
commit_title: Update installation guideRequest reviews from alice456 and bob123 for PR 67 in team/project-alpha update_pull_request with owner: team 
repo: project-alpha 
pullNumber: 67
reviewers: ["alice456", "bob123"] Summarize the comments in discussion 33801, in the facebook/react repository : get_discussion_comments with :owner: facebook
repo: react
discussionNumber: 33801The evaluation pipeline has three stages: , , and . We run each benchmark across multiple models, providing the list of available MCP tools with every request. For each run, we record which tools the model invoked and the arguments it supplied. We process the raw outputs and compute metrics and scores. We aggregate dataset-level statistics and produce the final evaluation report.Evaluation metrics and algorithmsOur evaluation targets two aspects: whether the model selects the correct tools and whether it supplies correct arguments.When benchmarks involve a single tool call,  reduces to a multi-class classification problem. Each benchmark is labeled with the tool it expects, and each tool is a “class.”Models tasked with this classification are evaluated using , , , and . is the simplest measure that shows the percentage of correct classifications. In our case it means the percentage of inputs that resulted in an expected tool call. This is calculated on the whole dataset. shows the proportion of the cases for which the tool was called correctly out of all cases where the tool was called. Low precision means the model picks the tool even for the cases where the tool is not expected to be called. shows the proportion of correctly called tools out of all cases where the given tool call was expected. Low recall may indicate that the model doesn’t understand that the tool needs to be called and fails to call the tool or calls another tool instead. is a harmonic mean showing how well the model is doing in terms of both precision and recall. If the model confuses two tools, it can result in low precision or recall for these tools.We have two similar tools that used to be confused often, which are  and . Let’s say we have 10 benchmarks for   and 10 benchmarks for . Imagine  is called correctly in all of 10 cases and on top in 30% of cases where search_issues should be called.This means we’re going to have lower recall for  and lower precision for : () = 10 (cases where tool is called correctly) / (10 + 3 (cases where tool is called instead of )) = 0.77 () =  7 (tool was called correctly) / 10 (cases where tool is expected to be called) = 0.7In order to have visibility into what tools are confused with each other, we build a confusion matrix. Confusion matrix for the  and  tools from the example above would look the following:Expected tool / Called toolThe confusion matrix allows us to see the reason behind low precision and recall for certain tools and tweak their descriptions to minimize confusion.Selecting the right tool isn’t enough. The model must also supply correct arguments. We’ve defined a set of argument-correctness metrics that pinpoint specific issues, making regressions easy to diagnose and fix.We track four argument-quality metrics: How often the model supplies argument names that aren’t defined for the tool.All expected arguments provided: Whether every expected argument is present.All required arguments provided: Whether all required arguments are included. Whether provided argument values match the expected values exactly.These metrics are computed for tools that were correctly selected. The final report summarizes each tool’s performance across all four metrics.Looking forward and filling the gapsThe current evaluation framework gives us a solid read on tool performance against curated datasets, but there’s still room to improve.Benchmark volume is the weak point of offline evaluation. With so many classes (tools), we need more robust per-tool coverage. Evaluations based on just a couple of examples aren’t dependable alone. Adding more benchmarks is always useful to increase the reliability of classification evaluation and other metrics.Our current pipeline handles only single tool calls. In practice, tools are often invoked sequentially, with later calls consuming the outputs of earlier ones. To evaluate these flows, we must go beyond fetching the MCP tool list and actually execute tool calls (or mock their responses) during evaluation.We’ll also update summarization. Today we treat tool selection as multi-class classification, which assumes one tool per input. For flows where a single input can trigger multiple tools, multi-label classification is the better fit.Offline evaluation gives us a fast, safe way to iterate on MCP, so models pick the right GitHub tools with the right arguments. By combining curated benchmarks with clear metrics—classification scores for tool selection and targeted checks for argument quality—we turn vague “it seems better” into measurable progress and actionable fixes.We’re not stopping here. We’re expanding benchmark coverage, refining tool descriptions to reduce confusion, and extending the pipeline to handle real multi-tool flows with execution or faithful mocks. These investments mean fewer regressions, clearer insights, and more reliable agents that help developers move faster.Most importantly, this work raises the bar for product quality without slowing delivery. As we grow the suite and deepen the evaluation, you can expect steadier improvements to GitHub MCP Server—and a better, more predictable experience for anyone building with it.]]></content:encoded></item><item><title>Announcing Rust 1.91.0</title><link>https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.91.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!The Rust compiler supports a wide variety of targets, but
the Rust Team can't provide the same level of support for all of them. To
clearly mark how supported each target is, we use a tiering system:Tier 3 targets are technically supported by the compiler, but we don't check
whether their code build or passes the tests, and we don't provide any
prebuilt binaries as part of our releases.Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but
we don't execute the test suite on those platforms: the produced binaries
might not work or might have bugs.Tier 1 targets provide the highest support guarantee, and we run the full
suite on those platforms for every change merged in the compiler. Prebuilt
binaries are also available.Rust 1.91.0 promotes the  target to Tier 1 support,
bringing our highest guarantees to users of 64-bit ARM systems running Windows.
Add lint against dangling raw pointers from local variablesWhile Rust's borrow checking prevents dangling references from being returned, it doesn't
track raw pointers. With this release, we are adding a warn-by-default lint on raw
pointers to local variables being returned from functions. For example, code like this:Note that the code above is not unsafe, as it itself doesn't perform any dangerous
operations. Only dereferencing the raw pointer after the function returns would be
unsafe. We expect future releases of Rust to add more functionality helping authors
to safely interact with raw pointers, and with unsafe code more generally.These previously stable APIs are now stable in const contexts:Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>The Green Tea Garbage Collector</title><link>https://go.dev/blog/greenteagc</link><author>Michael Knyszek and Austin Clements</author><category>dev</category><category>official</category><category>go</category><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[Go 1.25 includes a new experimental garbage collector called Green Tea,
available by setting  at build time.
Many workloads spend around 10% less time in the garbage collector, but some
workloads see a reduction of up to 40%!It’s production-ready and already in use at Google, so we encourage you to
try it out.
We know some workloads don’t benefit as much, or even at all, so your feedback
is crucial to helping us move forward.
Based on the data we have now, we plan to make it the default in Go 1.26.What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.
We’ll update this blog post with a link to the talk once it’s available online.Tracing garbage collectionBefore we discuss Green Tea let’s get us all on the same page about garbage
collection.The purpose of garbage collection is to automatically reclaim and reuse memory
no longer used by the program.To this end, the Go garbage collector concerns itself with  and
.In the context of the Go runtime,  are Go values whose underlying
memory is allocated from the heap.
Heap objects are created when the Go compiler can’t figure out how else to allocate
memory for a value.
For example, the following code snippet allocates a single heap object: the backing
store for a slice of pointers.var x = make([]*int, 10) // global
The Go compiler can’t allocate the slice backing store anywhere except the heap,
since it’s very hard, and maybe even impossible, for it to know how long  will
refer to the object for. are just numbers that indicate the location of a Go value in memory,
and they’re how a Go program references objects.
For example, to get the pointer to the beginning of the object allocated in the
last code snippet, we can write:Go’s garbage collector follows a strategy broadly referred to as tracing garbage
collection, which just means that the garbage collector follows, or traces, the
pointers in the program to identify which objects the program is still using.More specifically, the Go garbage collector implements the mark-sweep algorithm.
This is much simpler than it sounds.
Imagine objects and pointers as a sort of graph, in the computer science sense.
Objects are nodes, pointers are edges.The mark-sweep algorithm operates on this graph, and as the name might suggest,
proceeds in two phases.In the first phase, the mark phase, it walks the object graph from well-defined
source edges called .
Think global and local variables.
Then, it  everything it finds along the way as , to avoid going in
circles.
This is analogous to your typical graph flood algorithm, like a depth-first or
breadth-first search.Next is the sweep phase.
Whatever objects were not visited in our graph walk are unused, or ,
by the program.
We call this state unreachable because it is impossible with normal safe Go code
to access that memory anymore, simply through the semantics of the language.
To complete the sweep phase, the algorithm simply iterates through all the
unvisited nodes and marks their memory as free, so the memory allocator can reuse
it.You may think I’m oversimplifying a bit here.
Garbage collectors are frequently referred to as , and .
And you’d be partially right, there are more complexities.For example, this algorithm is, in practice, executed concurrently with your
regular Go code.
Walking a graph that’s mutating underneath you brings challenges.
We also parallelize this algorithm, which is a detail that’ll come up again
later.But trust me when I tell you that these details are mostly separate from the
core algorithm.
It really is just a simple graph flood at the center.Let’s walk through an example.
Navigate through the slideshow below to follow along.After all that, I think we have a handle on what the Go garbage collector is actually doing.
This process seems to work well enough today, so what’s the problem?Well, it turns out we can spend  of time executing this particular algorithm in some
programs, and it adds substantial overhead to nearly every Go program.
It’s not that uncommon to see Go programs spending 20% or more of their CPU time in the
garbage collector.Let’s break down where that time is being spent.At a high level, there are two parts to the cost of the garbage collector.
The first is how often it runs, and the second is how much work it does each time it runs.
Multiply those two together, and you get the total cost of the garbage collector.
    Total GC cost = Number of GC cycles × Average cost per GC cycle
    But for now let’s focus only on the second part, the cost per cycle.From years of poring over CPU profiles to try to improve performance, we know two big things
about Go’s garbage collector.The first is that about 90% of the cost of the garbage collector is spent marking,
and only about 10% is sweeping.
Sweeping turns out to be much easier to optimize than marking,
and Go has had a very efficient sweeper for many years.The second is that, of that time spent marking, a substantial portion, usually at least 35%, is
simply spent  on accessing heap memory.
This is bad enough on its own, but it completely gums up the works on what makes modern CPUs
actually fast.“A microarchitectural disaster”What does “gum up the works” mean in this context?
The specifics of modern CPUs can get pretty complicated, so let’s use an analogy.Imagine the CPU driving down a road, where that road is your program.
The CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,
and the way needs to be clear.
But the graph flood algorithm is like driving through city streets for the CPU.
The CPU can’t see around corners and it can’t predict what’s going to happen next.
To make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid
pedestrians.
It hardly matters how fast your engine is because you never get a chance to get going.Let’s make that more concrete by looking at our example again.
I’ve overlaid the heap here with the path that we took.
Each left-to-right arrow represents a piece of scanning work that we did
and the dashed arrows show how we jumped around between bits of scanning work.Notice that we were jumping all over memory doing tiny bits of work in each place.
In particular, we’re frequently jumping between pages, and between different parts of pages.Modern CPUs do a lot of caching.
Going to main memory can be up to 100x slower than accessing memory that’s in our cache.
CPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to
recently accessed memory.
But there’s no guarantee that any two objects that point to each other will  be close to each
other in memory.
The graph flood doesn’t take this into account.Quick side note: if we were just stalling fetches to main memory, it might not be so bad.
CPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see
far enough ahead.
But in the graph flood, every bit of work is small, unpredictable, and highly dependent on the
last, so the CPU is forced to wait on nearly every individual memory fetch.And unfortunately for us, this problem is only getting worse.
There’s an adage in the industry of “wait two years and your code will get faster.”But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.
“Wait two years and your code will get slower.”
The trends in modern CPU hardware are creating new challenges for garbage collector performance:Non-uniform memory access.
For one, memory now tends to be associated with subsets of CPU cores.
Accesses by  CPU cores to that memory are slower than before.
In other words, the cost of a main memory access depends on which CPU core is accessing
it.
It’s non-uniform, so we call this non-uniform memory access, or NUMA for short.Reduced memory bandwidth.
Available memory bandwidth per CPU is trending downward over time.
This just means that while we have more CPU cores, each core can submit relatively fewer
requests to main memory, forcing non-cached requests to wait longer than before.
Above, we looked at a sequential marking algorithm, but the real garbage collector performs this
algorithm in parallel.
This scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes
a bottleneck, even with careful design.Modern hardware features.
New hardware has fancy features like vector instructions, which let us operate on a lot of data at once.
While this has the potential for big speedups, it’s not immediately clear how to make that work for
marking because marking does so much irregular and often small pieces of work.Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.
The key idea behind Green Tea is astonishingly simple:Work with pages, not objects.Sounds trivial, right?
And yet, it took a lot of work to figure out how to order the object graph walk and what we needed to
track to make this work well in practice.More concretely, this means:Instead of scanning objects we scan whole pages.Instead of tracking objects on our work list, we track whole pages.We still need to mark objects at the end of the day, but we’ll track marked objects locally to each
page, rather than across the whole heap.Let’s see what this means in practice by looking at our example heap again, but this time
running Green Tea instead of the straightforward graph flood.As above, navigate through the annotated slideshow to follow along.Let’s come back around to our driving analogy.
Are we finally getting on the highway?Let’s recall our graph flood picture before.We jumped around a whole lot, doing little bits of work in different places.
The path taken by Green Tea looks very different.Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.
The longer these arrows, the better, and with bigger heaps, this effect can be much stronger.
 the magic of Green Tea.It’s also our opportunity to ride the highway.This all adds up to a better fit with the microarchitecture.
We can now scan objects closer together with much higher probability, so
there’s a better chance we can make use of our caches and avoid main memory.
Likewise, per-page metadata is more likely to be in cache.
Tracking pages instead of objects means work lists are smaller,
and less pressure on work lists means less contention and fewer CPU stalls.And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to
before, since now we can use vector hardware!If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.
But besides the usual arithmetic and trigonometric operations,
recent vector hardware supports two things that are valuable for Green Tea:
very wide registers, and sophisticated bit-wise operations.Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.
This is wide enough to hold all of the metadata for an entire page in just two registers,
right on the CPU, enabling Green Tea to work on an entire page in just a few straight-line
instructions.
Vector hardware has long supported basic bit-wise operations on whole vector registers, but starting
with AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction
that enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.
Together, these allow us to turbo-charge the Green Tea scan loop.This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that
are all sorts of different sizes.
Sometimes you needed two bits of metadata and sometimes you needed ten thousand.
There simply wasn’t enough predictability or regularity to use vector hardware.If you want to nerd out on some of the details, read along!
Otherwise, feel free to skip ahead to the evaluation.To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.There’s a lot going on here and we could probably fill an entire blog post just on how this works.
For now, let’s just break it down at a high level:First we fetch the “seen” and “scanned” bits for a page.
Recall, these are one bit per object in the page, and all objects in a page have the same size.Next, we compare the two bit sets.
Their union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,
which tells us which objects we need to scan in this pass over the page (versus previous passes).We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,
we have one bit per word (8 bytes) of the page.
We call this the “active words” bitmap.
For example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap
will be copied to 6 bits in the active words bitmap.
Like so: → 000000 000000 111111 111111 ...Next we fetch the pointer/scalar bitmap for the page.
Here, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word
stores a pointer.
This data is managed by the memory allocator.Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.
The result is the “active pointer bitmap”: a bitmap that tells us the location of every
pointer in the entire page contained in any live object we haven’t scanned yet.Finally, we can iterate over the memory of the page and collect all the pointers.
Logically, we iterate over each set bit in the active pointer bitmap,
load the pointer value at that word, and write it back to a buffer that
will later be used to mark objects seen and add pages to the work list.
Using vector instructions, we’re able to do this 64 bytes at a time,
in just a couple instructions.Part of what makes this fast is the  instruction,
part of the “Galois Field New Instructions” x86 extension,
and the bit manipulation Swiss army knife we referred to above.
It’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very
efficiently.
It performs a bit-wise affine
transformations,
treating each byte in a vector as itself a mathematical vector of 8 bits
and multiplying it by an 8x8 bit matrix.
This is all done over the Galois field,
which just means multiplication is AND and addition is XOR.
The upshot of this is that we can define a few 8x8 bit matrices for each
object size that perform exactly the 1:n bit expansion we need.For the full assembly code, see this
file.
The “expanders” use different matrices and different permutations for each size class,
so they’re in a separate file
that’s written by a code generator.
Aside from the expansion functions, it’s really not a lot of code.
Most of it is dramatically simplified by the fact that we can perform most of the above
operations on data that sits purely in registers.
And, hopefully soon this assembly code will be replaced with Go code!Credit to Austin Clements for devising this process.
It’s incredibly cool, and incredibly fast!So that’s it for how it works.
How much does it actually help?It can be quite a lot.
Even without the vector enhancements, we see reductions in garbage collection CPU costs
between 10% and 40% in our benchmark suite.
For example, if an application spends 10% of its time in the garbage collector, then that
would translate to between a 1% and 4% overall CPU reduction, depending on the specifics of
the workload.
A 10% reduction in garbage collection CPU time is roughly the modal improvement.
(See the GitHub issue for some of these details.)We’ve rolled Green Tea out inside Google, and we see similar results at scale.We’re still rolling out the vector enhancements,
but benchmarks and early results suggest this will net an additional 10% GC CPU reduction.While most workloads benefit to some degree, there are some that don’t.Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a
single page in one pass to counteract the costs of the accumulation process.
This is clearly the case if the heap has a very regular structure: objects of the same size at a
similar depth in the object graph.
But there are some workloads that often require us to scan only a single object per page at a time.
This is potentially worse than the graph flood because we might be doing more work than before while
trying to accumulate objects on pages and failing.The implementation of Green Tea has a special case for pages that have only a single object to scan.
This helps reduce regressions, but doesn’t completely eliminate them.However, it takes a lot less per-page accumulation to outperform the graph flood
than you might expect.
One surprise result of this work was that scanning a mere 2% of a page at a time
can yield improvements over the graph flood.Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled
by setting the environment variable  to  at build time.
This doesn’t include the aforementioned vector acceleration.We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out
with GOEXPERIMENT=nogreenteagc at build time.
Go 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of
tweaks and improvements based on feedback we’ve collected so far.If you can, we encourage you to try at Go tip-of-tree!
If you prefer to use Go 1.25, we’d still love your feedback.
See this GitHub
comment with some details on
what diagnostics we’d be interested in seeing, if you can share, and the preferred channels for
reporting feedback.Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.
The human element of the technology.The core of Green Tea may seem like a single, simple idea.
Like the spark of inspiration that just one single person had.But that’s not true at all.
Green Tea is the result of work and ideas from many people over several years.
Several people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David
Chase, and Keith Randall.
Microarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped
direct the design exploration.
There were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.
Just to make this single, simple idea viable.The seeds of this idea go all the way back to 2018.
What’s funny is that everyone on the team thinks someone else thought of this initial idea.Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe
crawling in Japan and drinking LOTS of matcha!
This prototype showed that the core idea of Green Tea was viable.
And from there we were off to the races.Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even
further.This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire
design space.
One that we don’t think any of us could’ve navigated alone.
It’s not enough to just have the idea, but you need to figure out the details and prove it.
And now that we’ve done it, we can finally iterate.The future of Green Tea is bright.Once again, please try it out by setting  and let us know how it goes!
We’re really excited about this work and want to hear from you!]]></content:encoded></item><item><title>Introducing Agent HQ: Any agent, any way you work</title><link>https://github.blog/news-insights/company-news/welcome-home-agents/</link><author>Kyle Daigle</author><category>official</category><pubDate>Tue, 28 Oct 2025 16:08:15 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[The current AI landscape presents a challenge we’re all too familiar with: incredible power fragmented across different tools and interfaces. At GitHub, we’ve always worked to solve these kinds of systemic challenges—by making Git accessible, code review systematic with pull requests, and automating deployment with Actions. With 180 million developers, GitHub is growing at its fastest rate ever—a new developer joining every second. What’s more, 80% of new developers are using Copilot in their first week. AI isn’t just a tool anymore; it’s an integral part of the development experience. Our responsibility is to ensure this new era of collaboration is powerful, secure, and seamlessly integrated into the workflow you already trust.At GitHub Universe, we’re announcing , GitHub’s vision for the next evolution of our platform. Agents shouldn’t be bolted on. They should work the way you already work. That’s why we’re making agents native to the GitHub flow.Agent HQ transforms GitHub into an open ecosystem that unites every agent on a single platform. .To bring this vision to life, we’re shipping a suite of new capabilities built on the primitives you trust. This starts with a ,a single command center to assign, steer, and track the work of multiple agents from anywhere. It extends to  with new ways to plan and customize agent behavior. And it is backed by enterprise-grade functionality: a new generation of ,a dedicated to govern AI access and agent behavior, and a  to understand the impact of AI on your work. We are also deeply committed to investing in our platform and strengthening the primitives you rely on every day. This new world of development is powered by that foundational work, and we look forward to sharing more updates. GitHub is your Agent HQ: An open ecosystem for all agents  The future is about giving you the power to orchestrate a fleet of specialized agents to perform complex tasks in parallel, not juggling a patchwork of disconnected tools or relying on a single agent. As the pioneer of asynchronous collaboration, we believe it’s our responsibility to make sure these next-generation async tools. Withwhat’s  changing is just as important as what You’re still working with the primitives you know—Git, pull requests, issues—and using your preferred compute, whether that’s GitHub Actions or self-hosted runners. You’re accessing agents through your existing paid Copilot subscription. On top of that foundation, we’re opening the doors to a new world of capability.Over the coming months, coding agents from Anthropic, OpenAI, Google, Cognition, and xAI will be available on GitHubas part of your paid GitHub Copilot subscription.Don’t want to wait? Starting this week, Copilot Pro+ users can begin working with , the first of our partner agents to extend beyond its native surfaces and directly into the editor.Mission control: Your command center, wherever you buildThe power of Agent HQ comes from, a unified command center that follows you wherever you work. It’s not a single destination; it’s a consistent interface across GitHub, VS Code, mobile, and the CLI that lets you direct, monitor, and manage every AI-driven task. With mission control, you can choose from a fleet of agents, assign them work in parallel, and track their progress from any device. New  that give you granular oversight over when to run CI and other checks for agent-created code. to control which agent is building the task, managing access, and policies just like you would with any other developer on your team.One-click merge conflict resolution, improved file navigation, and better code commenting capabilities.New integrations for Slack and Linear, on top of our recently announced connections for Atlassian Jira, Microsoft Teams and Azure Boards, and Raycast. New in VS Code: Plan, customize, and connectMission control is in VS Code, too, so you’ve got a single view of all your agents running in VS Code, in the Copilot CLI, or on GitHub.Today’s brand new release in VS Code is all about working alongside agents on projects, and it’s not surprising that great results start with a great plan. Getting the context right before a project is critical, but that same context needs to carry through into the work. Copilot already adapts to the way your team works by learning from your files and your project’s culture, but sometimes you need more pointed context.So today, we’re introducing , which works with Copilot, and asks you clarifying questions along the way, to help you to build a step-by-step approach for your task. Providing the context upfront improves what Copilot can do and helps you find gaps, missing decisions, or project deficiencies early in the process—before any code is written. Once you approve, your plan goes to Copilot to start implementing, whether that’s locally in VS Code or using an agent in the cloud.For even finer control, you can now create custom agents in VS Code with  files, source-controlled documents that let you set clear rules and guardrails such as “prefer this logger” or “use table-driven tests for all handlers.” This shapes Copilot’s behavior without you re-prompting it every time.Now you can rely on the new . VS Code is the  editor that supports the full MCP specification. Discover, install, and enable MCP servers like Stripe, Figma, Sentry, and others, with a single click. When your task calls for a specialist, create custom agents in GitHub Copilot with their own system prompt and tools to help you define the ways you want Copilot to work. Increased confidence and control for your teamAgent HQ doesn’t just give you more power—it gives you confidence. Ensuring code quality, understanding AI’s influence on your workflow, and maintaining control over how AI interacts with your codebase and organization are essential for your team’s success, and we’re tackling these challenges head-on.When it comes to code quality, the core problem is that “LGTM” doesn’t always mean “the code is healthy.” A review can pass, but can still degrade the codebase and quickly become long-term technical debt. With , in public preview today, you’ve got org-wide visibility, governance, and reporting to systematically improve code maintainability, reliability, and test coverage across every repository. Enabling it extends Copilot’s security checks to look at the maintainability and reliability impact of the code that’s been changed.And we’ve into the Copilot coding agent’s workflow, too, so Copilot gets an initial first-line review and addresses problems (before you even see the code). As an organization, you need to know how Copilot is being used. So today, we’re announcing the public preview of the , showing Copilot’s impact and critical usage metrics across your entire organization.For enterprise administrators who are managing AI access, including AI agents and MCP, we’re focused on providing consistent —your agent governance layer. Set security policies, audit logging, and manage access all in one place. Enterprise admins can also control which agents are allowed, define access to models, and obtain metrics about the Copilot usage in your organization.For developers, by developers We built Agent HQ because we’re developers, too. We know what it’s like when it feels like your tools are you instead of helping you. When “AI-powered” ends up meaning more context-switching, more babysitting, more subscriptions, and more time explaining what you need to get the value you were promised. Agent HQ isn’t about the hype of AI. It’s about the reality of shipping code.  It’s about bringing order and governance to this new era without compromising choice. It’s about giving the power to build faster, with more confidence, and on your terms.Welcome home. Let’s build. ]]></content:encoded></item><item><title>Octoverse: A new developer joins GitHub every second as AI leads TypeScript to #1</title><link>https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/</link><author>GitHub Staff</author><category>official</category><pubDate>Tue, 28 Oct 2025 16:07:06 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[If 2025 had a theme, it would be growth. Every second, more than one new developer on average joined GitHub—over 36 million in the past year. It’s our fastest absolute growth rate yet and 180 million-plus developers now work and build on GitHub. The release of GitHub Copilot Free in late 2024 coincided with a step-change in developer sign-ups, exceeding prior projections. Beyond bringing millions of new developers into the ecosystem, we saw record-level activity across repositories, pull requests, and code pushes. Developers created more than 230 new repositories every minute, merged  pull requests on average each month (+23% YoY), and pushed nearly 1 billion commits in 2025 (+25.1% YoY)—including a record of nearly 100 million in August alone. This surge in activity coincides with a structural milestone: for the first time, TypeScript overtook both Python and JavaScript in August 2025 to become the most used language on GitHub, reflecting how developers are reshaping their toolkits. This marks the most significant language shift in more than a decade.And the growth we see is global: India alone added more than 5 million developers this year (over 14% of all new accounts) and is on track to account for one in every three new developers on GitHub by 2030. This year’s data highlights three key shifts: Generative AI is now standard in development. More than 1.1 million public repositories now use an LLM SDK with 693,867 of these projects created in just the past 12 months alone (+178% YoY, Aug ‘25 vs. Aug ‘24). Developers also merged a record 518.7M pull requests (+29% YoY). Moreover, AI adoption starts quickly: 80% of new developers on GitHub use Copilot in their first week.TypeScript is now the most used language on GitHub. In August 2025, TypeScript overtook both Python and JavaScript. Its rise illustrates how developers are shifting toward typed languages that make agent-assisted coding more reliable in production. It doesn’t hurt that nearly every major frontend framework now scaffolds with TypeScript by default. Even still, Python remains dominant for AI and data science workloads, while the JavaScript/TypeScript ecosystem still accounts for more overall activity than Python alone.AI is reshaping choices, not just code. In the past, developer choice meant picking an IDE, language, or framework. In 2025, that’s changing. We see correlations between the rapid adoption of AI tools and evolving language preferences. This and other shifts suggest AI is influencing not only how fast code is written, but which languages and tools developers use.And one of the biggest things in 2025?  Early signals in our data are starting to show their impact, but ultimately point to one key thing: we’re just getting started and we expect far greater activity in the months and years ahead. 💡 Oh, and if you’re a visual learner, we have you covered.👇The state of GitHub in 2025: A year of record growthIn 2023, GitHub crossed 100 million developers after nearly three years of growth from 50 million to 100 million. But the past year alone has rewritten that curve with our fastest absolute growth yet. Today, more than 180 million developers build on GitHub.So, what does “more than one new developer joining GitHub every second on average” actually mean? Developers are converging on GitHub. More than 36 million developers joined GitHub in a single year (23% YoY), confirming GitHub as the primary hub for collaboration. AI adoption starts immediately. We see nearly 80% of new developers on GitHub use GitHub Copilot within their first week, offering evidence that AI is now an expectation among new coders. The talent boom is geographically diverse. Every minute, ~25 developers joined from APAC, ~12 from Europe, ~6.5 from Africa and the Middle East, and ~6 from LATAM. India alone added over 5 million developers this year. GitHub Copilot steepened growth curvesHistorically, developer sign ups and repository creation followed predictable year-over-year patterns. The launch of Copilot Free in December 2024 accelerated those curves globally, giving millions access to AI-powered workflows for the first time. The end result? Our typical models for growth overturned dramatically.  Private and public repositories play different but interdependent rolesIn 2025, 81.5% of contributions happened in private repositories, while 63% of all repositories were public. The split highlights GitHub’s dual role: most day-to-day work takes place in private projects, but depends on libraries, models, and frameworks in public open source.Private repositories also grew faster (+33% YoY) than public repositories (+19% YoY), reflecting the growth in organizational development happening on GitHub. We also sometimes see open source software (OSS) work start in private projects.Enterprise and team‑level collaboration is happening on GitHub. The  of work is smaller, yet these projects supply the libraries, models, and workflows that power the broader ecosystem.are now on GitHub total repositories with  new repositories in 2025 marking our biggest year yet private repositories (up  33%) underscore the activity happening outside of the public eye. Open source and public projects represent the majority of repositories on GitHub. 63% of all repositories are open source or public.Developer productivity: shipping more, waiting less2025 marked the most active 12-month period in GitHub history with more than 1.12B contributions to public and open source projects. Following the SPACE framework (a model that looks at developer Satisfaction, Performance, Activity, Communication, and Efficiency), this increase reflects record levels of developer activity. As developers are increasingly working with LLMs and agents, there are some new, notable correlations in this year’s data.Developer activity reached record levels in 2025Across every productivity signal on GitHub, developers set new records in 2025.Code pushes are driving the surge with more than 986M commits in 2025 (+25% YoY) and monthly pushes topping 90M by May. Other activity followed: +20.4% (47.5M vs 39.5M) +11.3% (17.5M vs 15.7M) essentially flat (+0.35%) down -27% (sharp decline)These are observational signals rather than causal claims and more work is needed to understand the full impact AI is having in software development. Jupyter Notebooks and Dockerfiles highlight two stages of modern developmentNotebooks are now a mature tool for experimentation, while Dockerfiles are considered  bridge to reproducibility and production. In 2025, 2.4 million repositories used Notebooks (+75% YoY) and 1.9 million used Dockerfiles (+120% YoY). This growth is likely fueled by the need to sandbox agents and LLMs, and containerization is a practical method to run and scale them securely.AI agents enter the mainstreamDevelopers are using AI and agentic tools to build and share their work faster. Between the total number of developers on GitHub growing in tandem with the release of popular agentic tools and the overall activity increases in languages such as TypeScript and Python, 2025 data shows an increase in rapid prototyping and experimentation. Where the world codes in 2025The last five years have redrawn not just GitHub’s developer map, but also the distribution of global activity, faster than any period on record.India added more than 5.2 million developers in 2025, which accounts for a little over 14% of GitHub’s total +36 million new developers in 2025. That makes India the single largest source of new developers on GitHub this year, continuing its rapid rise since 2020.Significant growth came from India, Brazil, and Indonesia. These regions combine large, young developer populations with expanding internet access and booming startup ecosystems. Many are also seeing some of the fastest growth in AI-related projects, as local companies adopt new tools to compete globally.Over the past five years (from 2020 to 2025), India, Brazil, and Indonesia have more than quadrupled the number of developers on the platform; Japan and Germany have more than tripled their developer numbers; and the USA, UK, and Canada have more than doubled the number of developers on the platform.Brazil, in particular, is benefiting from activity investment in fintech and open banking.Regional growth snapshots2024 to  2025 net new devsGovernment skilling, AI‑assisted local‑language tooling. Japan, in particular, has embraced digital transformation in recent years, leading to a boom in developers. Remote hiring by US/EU firms, fintech startup densityGermany, United Kingdom, France Cloud infrastructure spend, AI investment, startup‑visa pipelinesIncreased mobile adoption, community bootcamps, LLMs that work locallyModeling the global developer landscape through 2030Looking ahead, our data team modeled the next five years of developer growth using regression analysis, which can help to capture more of the real-world dynamics impacting the data. (You can get more information about this in our methodology section.) The results of our analysis suggest India will continue to expand its lead, reaching 57.5 million developers by 2030, and accounting for more than one in three of all projected sign ups worldwide. The United States will be the second-largest community with more than 40 million developers expected, while Brazil (19.6M), Japan (11.7M), and the United Kingdom (11M) round out the top five.Notably, emerging regions across Africa and the Middle East show momentum with Egypt, Nigeria, Kenya, and Morocco all projected to add millions of developers in the coming years. This points to a developer population that is not only growing but diversifying geographically at unprecedented speed. new developers who joined GitHub this year comes from a country that wasn’t in the global top 10 in 2020.added the most developers this year of any country, outpacing the US in growth: The number of contributors to generative AI projects on GitHub continues to grow fast, and those contributors are working around the world.  Open source in 2025: activity and influence in the AI eraOpen source development reached record levels this year with 1.12 billion contributions across public repositories (+13% YoY). March 2025 marked the largest single month of new open source contributors in GitHub history: 255,000 first-timers.In total, 395 million public repositories hosted 1.12 billion contributions and 518.7 million merged pull requests—each a record. This year’s fastest‑growing projects by contributorsSix of the 10 fastest-growing repositories were AI infrastructure projects, underscoring demand for runtimes, orchestration, and efficiency tools.The top open source projects by contributors2025’s top projects split between AI infrastructure (vllm, ollama, huggingface/transformers) and enduring ecosystems (vscode, godot, home-assistant).On one side, projects like , , , , and  dominate, confirming that contributors are investing in the foundation layers of AI—model runtimes, inference engines, and orchestration frameworks.On the other side, mainstay ecosystems such as , , , and  continue to attract steady contributor bases, showing that open source momentum extends well beyond AI.? AI infrastructure is emerging as a major magnet, but developer ecosystems remain strong.The fastest-growing projects by contributors show AI’s impact along with evergreen and utility projectsWe see a mix of projects driving the fastest growth. zen-browser/desktop leads the pack, with fast-rising, AI-focused projects like vllm-project/vllm, continue-dev/continue, ollama/ollama, and Aider-AI/aider showing the pull of local inference, coding agents, and model runners.Growth in open source is broad. AI infrastructure projects are prominent among top-growth repositories. When we zoom out to the top 20 projects (not all of these are captured in our above graphic), we see a few things at play: Reproducibility and dependency hygiene are hot. The rise of astral-sh/uv and NixOS/nixpkgs points to a hunger for deterministic builds, faster installs, and less “works on my machine.”Performance-centric developer tools win attention. Ghostty, Tailwind CSS, and uv are all about speed, tight feedback loops, and minimal friction.Developers are contributing to projects that emphasize privacy and control. Zen Browser and Clash-Verge reflect interest in privacy, content control, and routing around networks.Open source social media continues to grow. As one of the biggest social projects, Bluesky’s momentum suggests developers are still investing in open protocols and portable identity.AI, tinkering, and frontend projects attract first-time contributorsNearly 20% of the most popular open source projects among first-time contributors in 2025 were AI-focused. But we’re also seeing other project types capture mindshare among developers who are new to open source. Most of the most popular projects sit firmly in AI infrastructure. Repositories like ollama/ollama, comfyanonymous/ComfyUI, and ultralytics/ultralytics appear prominently, confirming that newcomers want to experiment with models, create local runners, or build pipelines.Major platforms bring visibility. microsoft/vscode shows up as one of the top destinations for first-time contributors, underscoring the pull of widely used developer tools and the scale of contribution opportunities within them.firstcontributions/first-contributions exist almost entirely to help people submit their first pull request. Their year-over-year presence says first-timers still need and seek low-friction practice sandboxes.Smart-home, mobile, and game-dev ecosystems attract newcomers. Smart-home (home-assistant/core), mobile (flutter/flutter, expo/expo), game-dev (godotengine/godot), and 3D printing (bambulab/BambuStudio) rank closely behind the learning repositories. These ecosystems offer visible results on day one, which is perfect for those who want to “learn by doing.”Frontend and dev tool projects also light up. shadcn/ui and uBlockOrigin/uAssets show that CSS, UI, and browser tooling remain magnets for fresh contributors.The global landscape in open source activityIndia now has the largest public and open source contributor base in the world. This reflects both the country’s booming developer population and its increasing role in OSS adoption.The U.S. continues to lead in contributions. Despite having fewer contributors, U.S.-based developers contributed more to public and open source projects on GitHub. This suggests higher per-developer activity.Brazil, Indonesia, and Germany round out the next tier. Brazil contributes both strong headcount and volume, while Indonesia enters the top 5 for contributors, showing how emerging regions are shaping OSS.​​Governance is not keeping pace with velocity. This gap presents an opportunity for developers, organizations, and companies to contribute documentation as well as code.  ~63% of public repositories include a README, holding steady year over year.  At , contributor guides remain an ecosystem-wide opportunity as the number of first-time contributors increases. 2% penetration; governance still lags growth.Key repository files like README or a LICENSE file are more than formalities. They’re foundational to scaling inclusive, legal, secure, and sustainable collaboration. This guide to getting your repository collaboration-ready shares what documentation is most important for fostering a sense of shared ownership. public and open source repositories (+19% YoY) public and open source contributions (+13% YoY) pull requests merged (+29% YoY) of top 10 open source projects by contributors are AI‑focusedOnly  repositories ships with a Code of ConductSecurity: from “shift left” to secure by defaultAverage fix times for critical severity vulnerabilities have improved by 30% over the past year, as remediation is beginning to keep pace with faster software development. Automation is driving this acceleration. Dependabot usage more than doubled (846k projects, +137% YoY), and AI tools like Copilot Autofix are resolving common OWASP Top 10 issues across thousands of repositories every month. This is underscored by the fact that in 2025, 26% fewer repositories received critical alerts through a combination of increased automation and AI usage. At the same time, new risks are emerging. Broken Access Control overtook Injection as the most common CodeQL alert, flagged in 151k+ repositories (+172% YoY). Much of this stems from misconfigured permissions in CI/CD pipelines and AI-generated scaffolds that skip critical auth checks (GitHub’s engineers published a walkthrough of how they improved their SAML authentication flow, which offers some valuable lessons). Automation is working (until the merge queue stalls)Developers are automating more build, test, and security activity. In 2025, we saw developers use 11.5 billion total GitHub Actions minutes (measured in CPU minutes) in public projects for free. That’s up 35% year over year from 8.5 billion GitHub Actions in 2024. Note: in last year’s report, we included GitHub Actions minutes across public projects and self-hosted usage. If we use the same rubric this year, 13.5 billion minutes were used, which is up 30% from last year.Automation raises fixes quickly, but merges still stall when approval depends on humans or policy. Projects that configure Dependabot with auto-merge rules remediate vulnerabilities more consistently than those relying solely on manual review.We saw a peak of more than 12M Dependabot alerts in December 2022, which came a year after the Log4Shell vulnerability and immediately after key vulnerabilities in OpenSSL.Monthly openings have since settled near the 3-4M range, but merges hover around 1M.  within the same month it’s proposed.Security alerts follow the same pattern: brief spikes when new CVEs land, then a long tail of unresolved notifications (a number of which are likely attributable to zombie projects that are no longer maintained).In 2025, we saw 30% faster fixes of critical severity vulnerabilities with 26% fewer repositories receiving critical alerts. And this acceleration is happening at scale with the average fix time shrinking from 37 to 26 days in total.  Configuring and codifying securityRepositories that define Dependabot behavior in  more than doubled this year (), marking a shift from “notify me” to “patch me automatically, within guardrails.”Repositories with CodeQL in 2025: Broken access control vulnerabilities spikeBroken Access Control overcame Injection to become the top CodeQL alert, flagged in 151k+ repositories. New CodeQL coverage for GitHub Actions revealed widespread misconfigured permissions and token scopes.This points to a broader issue: authentication and authorization remain difficult for both developers and LLMs. Injection still dominates JavaScript, but Broken Access Control now leads in Python, Go, Java, and C++ (languages where AI-assisted “vibe coding” sometimes scaffolds endpoints that look correct but lack critical auth checks).That same category became the fastest-growing target for Copilot Autofix. By mid-2025, developers were accepting AI-generated fixes for Broken Access Control in 6,000+ repositories per month. Autofix also gained traction for Injection (3,100 projects), Insecure Design (2,300 projects), and Logging/Monitoring failures (3,500 projects).: 47 of the top 50 open source projects (94%) defined by their Mona ranking (combined ranking of stars, forks, and issue authors) now use the OpenSSF Scorecard via GitHub Actions or are independently scanned, bringing real-time checks for security best practices. The top programming languages of 2025: TypeScript jumps to #1 while Python takes #2By GitHub contributor counts, August 2025 marks the first time TypeScript emerged as the most used language on GitHub, surpassing Python by ~42k contributors (other industry indices use different methodologies and may still rank JavaScript and Python higher). This caps a decade-long trend of developers shifting toward typed JavaScript and signals a new default for modern development. grew by over 1 million contributors in 2025 (+66% YoY), driven by frameworks that scaffold projects in TypeScript by default and by AI-assisted development that benefits from stricter type systems. remains dominant in AI and data science with 2.6 million contributors (+48% YoY). Jupyter Notebook remains the go-to exploratory environment for AI (≈403k repositories;  inside AI-tagged projects). is still massive (2.15M contributors), but its growth slowed as developers shifted toward TypeScript.Together, TypeScript and Python now account for more than 5.2 million contributors (roughly 3% of all active GitHub developers in August 2025). The rise of typed languages suggests AI isn’t just changing the speed of coding, but also influencing which languages teams trust to take AI-generated code into production.YoY % growth (Aug 2024 vs. Aug 2025)TypeScript overtook Python and JavaScript for #1 growth, showing its dominance in new green-field development.Considered the lingua franca of AI and ML, Python’s usage has increased significantly amidst generative AI work. Still massive in scale, but more incremental growth as usage shifts toward TypeScript.Java continues its steady enterprise-driven growth.Cloud, desktop, and game dev keep momentum for C#.Python still trails the combined JavaScript and TypeScript ecosystem, a continuation of last year’s trend that highlights just how large the typed and untyped JavaScript community remains. But starting in 2025, Python’s growth curve began to track almost identically in parallel with JavaScript and TypeScript, suggesting that AI adoption is influencing language choice across these ecosystems.Python dominates AI projects. It remains the clear leader inside AI-tagged repositories, where Jupyter Notebook usage nearly doubled in 2025 offering evidence of its role as the go-to language for prototyping, training, and orchestrating AI workloads. TypeScript’s growth confirms our 2024 observation: much of what was previously counted as “JavaScript” activity already came through TypeScript transpilation pipelines. The data shows typed languages are increasingly becoming the default.Enterprise stacks endure. Java and C# each added over 100k contributors this year, showing steady growth across large enterprise and game-dev environments even as AI reshapes the landscape.Legacy experiments emerge. COBOL appeared in our dataset with nearly 3,000 active developers—likely driven by organizations and hobbyists creating AI-assisted tutorial repositories aimed at modernizing legacy codebases.The fastest-growing languages by percentage growthThe following languages may not have the biggest developer communities behind them, but each has at least 1,000 monthly contributors and they’re posting the fastest year-over-year growth rates on GitHub. Luau is Roblox’s scripting language and a  language, reflecting a broader industry trend toward typed flexibility. As a , Typst aims to make academic and technical publishing faster, less cryptic, and more collaborative.Astro’s “islands architecture” and focus on shipping zero-JavaScript by default resonate with developers building fast, content-heavy sites (we added Astro to Linguist in 2021, which is our source for languages).As Laravel’s templating engine, Blade rides on Laravel’s continued dominance in PHP web development.Offering type safety for the JavaScript world, TypeScript’s combination of JavaScript ubiquity and type safety is compelling for both greenfield and legacy projects (plus, its types work well with AI coding tools).Core stacks for new projects built in the last 12 monthsNearly 80% of new repositories used just six languages: Python, JavaScript, TypeScript, Java, C++, and C#. These core languages anchor most modern development.Total repositories (Sep 2024-Aug 2025) Growth (Jan-Aug 2025 vs. Jan-Aug 2024) What this growth tells usAI’s default glue with growth driven by ML, agents, notebooks, and orchestration.Still ubiquitous for scripts and web apps, though growth is slower as TypeScript gains share.Typed standard for modern web dev. Ideal for safe API/SDK integration, especially with AI.Reliable enterprise and backend workhorse. Gradual AI integration without language churn.Performance-critical workloads used in game engines, inference, and embedded systems supporting AI.Steady enterprise and game dev usage, with AI capabilities folded into established ecosystems.Experimentation is becoming more common. Though not a language, Jupyter Notebooks grew 24.5% YoY, with exploratory LLM experiments and data analysis now generating new standalone repos rather than remaining siloed in monorepos.Performance and systems languages are rising with AI (but not evenly). C grew ~20.9% YoY and C++ grew ~11.8% YoY, reflecting demand for faster runtimes, inference engines, and hardware-optimized loops.C# grew ~10.6% YoY, consistent with enterprise and game/tooling ecosystems. This suggests AI features are being integrated into existing .NET workflows rather than driving a wholesale language shift.The languages powering AI developmentPython and Jupyter Notebook continue to anchor new AI projects, but the story this year is Python’s growth. Python now powers nearly half of all new AI repositories (), underscoring its role as the backbone of applied AI work, from training and inference to orchestration and deployment. Jupyter Notebook remains the go-to exploratory environment for experimentation (), but the shift toward Python codebases signals more projects moving out of prototypes and into production stacks.Front-end and app-layer languages grew sharply from smaller bases—TypeScript +77.9% (85,746) and JavaScript +24.8% (88,023)—mirroring the rise of demos, dashboards, and lightweight apps built around model endpoints.  emerged as the fastest riser, reflecting how teams codify eval harnesses, data prep, and deployment pipelines. And C++ crossed 7,800 repos (+11%), a steady reminder of its role in performance-critical inference engines, runtimes, and hardware-close systems.Last year we saw AI move from experiment to mainstream. In 2025, it became part of the everyday workflow. And no matter what tool developers used over the last 12 months, their work converged on GitHub. AI-related repositories on GitHub now exceed 4.3 million, nearly doubling in less than two years.Roughly 80% of new GitHub users tried Copilot within their first week, showing that AI is no longer an advanced tool to grow into, but part of the default developer experience.Monthly contributors to generative-AI projects climbed sharply across our measurement year. From September 2024 through August 2025, months averaged ~151k contributors (median ~160k). Activity rose from ~86k in January 2025 to a peak of 206,830 in May (+132% YoY vs. May 2024). It then held near ~200k through the summer. On a like-for-like basis, Jan–Aug 2025 averaged ~175k contributors, up +108% YoY vs. Jan–Aug 2024 (~84k), indicating a durable step-change rather than a one-off spike.Generative AI is becoming infrastructure. More than 1.1M public repositories now import an LLM SDK (+178% YoY, August ‘25 vs. August ‘24), supported by 1.05M+ contributors and 1.75M monthly commits (+4.8X since 2023). Growth surged early in the calendar year, then normalized as projects shifted from experimentation to shipping. Contributors ran +100-118% YoY from Feb-May 2025, then cooled to +31% (Jun), +11% (Jul), and -3% (Aug) as teams focused on shipping vs. experimenting.  Half (50%) of open source projects have at least one maintainer using GitHub Copilot.Early evidence of a prototype-to-production pivot. Python-based code accelerated mid-2025 while Notebook growth flattened—signaling packaging into production. (By year’s end, Notebooks rebounded, keeping pace with Python.)Strong signals of mainstream appeal 178% YoY increase in projects that import an LLM SDK public repositories now import an LLM SDK;  were created in the last 12 months alone.Growth rates indicate a shift from early experimentation to sustained building.Contributors up >3X since 2023Monthly distinct contributors to AI repos rose from 68k (Jan 2024) to ~200k (Aug 2025). August 2025 is up 111% YoY vs. August 2024.AI work is no longer the domain of specialists.Monthly contributions near 6MMonthly  to AI projects reached  hitting a peak of . August 2025 is up  vs. August 2024.More code, more often, offering evidence of production-grade adoption and active iteration.1.13M+ public repositories now depend on generative-AI SDKs (up 178% YoY). More than  were created in the last 12 months, sharply outpacing 2024’s total (~400,000). The compounding curve that began in early 2023 shows no sign of tapering; every week, on average, we are still seeing new all-time highs.The U.S. remains the largest source of contributions (~12.8M, 31.8%). India ranks second (~5M, 12.5%) and leads by distinct repositories (405k vs. 342k).A second tier (Germany, Japan, U.K., Korea, Canada, Brazil, Spain, France) contributes another ~40%, globalizing the map.A first glimpse of coding agent shows1+ million pull requests that were created between May 2025 and September 2025.A repository-level comparison of public repositories with ≥1 coding agent-authored pull request vs. a random sample without Copilot coding agent shows strong selection effects: coding agent activity is skewed toward repositories with more stars, larger size, and greater age. In other words, teams aren’t only assigning coding agent to throwaway projects; they’re trying it in better-known, more established projects as well. We invite the community to run within-repository experiments (A/B or stepped-wedge) and  conditioned on size, stars, age, and complexity proxies to establish robust baselines. We’ll continue looking into this as we evolve coding agent across GitHub, the Copilot CLI, and more. AI is driving notable breakouts in open sourceGenerative AI projects continue to be among GitHub’s most popular. Projects like vllm, ragflow, and ollama outpaced the historical contributor growth of staples such as vscode, home-assistant, flutter. Repository (age ≤3 yrs unless noted)Open source  model + training/inference stackLocal Llama inference on CPU/GPUEnd-to-end retrieval-augmented-generation (RAG) template“LLM-native” command-line shell that reasons over local context (6.6 yrs)Defacto Python library for model loading/fine tuningSoftware infrastructure outpaces everything else in velocity. Brand-new generative AI repositories (≤ 1 yr old) are racking up star counts that took other projects a decade to accumulate.Standards are emerging in real time. The rapid rise of (MCP) shows the community coalescing around interoperability standards.AI is reshaping classic tooling. Projects like and show how local inference and AI-augmented pipelines are moving from proof-of-concept into mainstream developer workflows.AI is helping developers fix code, tooGitHub Copilot Autofix contributed to measurable improvements in 2025: surged the fastest, with fixes accepted in 6,000+ repositories per month by mid-2025.Security logging and monitoring failures, injection, insecure design, and misconfiguration fixes also climbed sharply, each crossing into the thousands of repositories monthly.Autofix is addressing the most common OWASP Top 10 issues—not just exotic vulnerabilities—bringing AI into the daily fabric of software security.Early adopters using agents, open standards, and self-hosted inference are already setting the norms for the next decade. Continuous AI—systems and workflows that are updated, retrained, and deployed on an ongoing basis—is emerging.Expect AI libraries to become “plumbing.” If your stack can’t load a model or pipe context into one, you’ll feel legacy-bound quickly.  Package your experiments early to share them with others.Watch the toolchain, not just the models. The next productivity leap may come from LLM-native editors, shells, and test runners growing out of today’s fast-rising repositories.Build for interoperability. Standards like MCP and Llama-derived protocols are gaining momentum across ecosystems.Three years ago, we said AI wouldn’t replace developers—it would bring more people into the ecosystem. The data now proves it: activity on GitHub has reached record levels, with more contributors, more repositories, and more experimentation than ever.The past year marked historic milestones: India overtook the United States as the largest contributor base to public and open source projects on GitHub. It’s also poised to overtake the US developer population within the next few years, underscoring how global the developer community has become.TypeScript became the most used language for the first time, overtaking both Python and JavaScript and signaling a generational shift in how modern software is built.Open source remains the foundation. Public projects supply the libraries, models, and workflows that power most private development. The strength of this ecosystem and the maintainers who sustain it will determine how far and how fast the next wave of software innovation goes.The story of 2025 isn’t AI versus developers. It’s about the evolution of developers in the AI era where they orchestrate agents, shape languages, and drive ecosystems. No matter which agent, IDE, or framework they choose, GitHub is where it all converges.: Refers to September 1, 2024 through August 31, 2025. Year-over-year language comparisons: Unless otherwise noted, YoY values reflect same-month comparisons (e.g., Aug 2025 vs Aug 2024) to normalize for month-length and seasonality in developer activity.Commenting on a commit, issue, pull request, pull request diff, or team discussion; creating a gist, issue, pull request, or team discussion; pushing commits to a project; and reviewing a pull request. : GitHub users who have performed any of the contribution activities defined above. Anyone with a GitHub account. Also sometimes referred to as a GitHub user. The open source and developer communities are an increasingly diverse and global group of people who tinker with code, make non-code contributions, conduct scientific research, and more. GitHub users drive open source innovation, and they work across industries—from software development to data analysis and design.The combined number of public and private repositories on GitHub.Programming language usage: Unless otherwise noted, “most used” languages are ranked by the number of distinct monthly contributors who committed code in that language. This is the standard measure for “TypeScript became the most used language on GitHub.”Any repository tagged with AI-related topics (e.g., “AI,” “ML,” “LLM”) or falling under our AI classification methodology. This broad category captures general experimentation and projects adjacent to AI.Software development tasks completed with the aid of autonomous or semi-autonomous AI tools (e.g., GitHub Copilot coding agent creating pull requests, triaging issues, or running tests).: A GitHub Copilot feature that can independently draft code, run tests, and open draft pull requests in a secure environment—subject to developer review and approval.: A GitHub Copilot feature that reviews pull requests, suggests changes, and surfaces potential issues before merging.: CPU minutes used to execute GitHub Actions (CI/CD workflows). Reported cumulatively and as YoY growth.: GitHub’s semantic code analysis engine used to detect security vulnerabilities. Alerts are categorized by vulnerability type (e.g., Broken Access Control, Injection, Insecure Design).: Ranks repositories based on their stars, forks, and unique issue authors in the repository. The steps to compute Mona rank: 1) Calculate individual ranks based on stars, forks, and issue authors. 2) Sum these individual ranks. 3) Assign the final “Mona Rank” based on the summed rank.A set of official libraries and tools published by model providers (such as OpenAI, Anthropic, Meta, Mistral, Cohere, or AI21) that make it easier for developers to connect to and use their large language models. These SDKs wrap the underlying model APIs with client libraries, helper functions, and runtime integrations, letting developers handle prompts, responses, tokens, and extensions without needing to build low-level infrastructure. To determine usage of LLM SDKs, we referenced models offered via GitHub Models. These included SDKs from DeepSeek, Grok, Mistral, Phi, OpenAI, Cohere, Llama, and AI21. Sep 1, 2024 through Aug 31, 2025.Unless noted, metrics reflect  only. Public indicators are also available on the . For country-level reporting there, we publish metrics only when  unique developers performed the activity in the period. Calendar-month metrics to show peaks/turning points (e.g., language rankings). vs  for year-over-year trends/averages. Historical context from  where relevant. users per metric (e.g., per language). One user can appear in multiple categories in the same month, so these do not sum across categories. Counted in a month if created or active, per metric definition. “pushes,” “pull requests created/merged,” “comments,” etc., follow standard GitHub event definitions. Month vs. the  prior year (e.g., ) are used for milestones/seasonality control as in language comparisons. Trailing-12-month  vs the prior trailing-12-month metric are used for sustained trends (e.g., average monthly contributors, pull requests per month).Stock vs. flow (cumulative metrics): = level at a point in time (e.g., SDK repos ).  =  over a window (e.g., ). Labeled distinctly.Developers are mapped to countries via , standardized to ; country aggregates observe the  publication threshold.Repository & language classification GitHub language detection (e.g., Linguist); mixed-language repositories are attributed to  language. “Jupyter Notebook” is a development-environment classification and is labeled transparently. Identified via signals such as  (imports/dependencies) and related metadata.Open source quality signals: Presence of files/policies (e.g., codes of conduct) from repository metadata. tracking;  for stock growth;  (contributors, repo counts, growth rates, activity volumes). to reduce noise (e.g., lists may require  contributors or  repos; metric-specific).Count unique users per time period and per metric to avoid within-slice double counting; cross-category duplication is expected by design.Exclude incomplete months (e.g., the current month) from YoY/T12. where identifiable (account flags + behavioral heuristics).Enforce  for inclusion in growth and rankings. against multiple internal/public sources (e.g., Innovation Graph).Interpretation & reproducibility motivates same-month YoY for snapshots and  for structure. primary language is repository-level; TypeScript/JavaScript mixes may appear under one language. public-only views undercount private activity but preserve directionality.Public counterparts for several metrics can be verified via the .Developer growth projections: a collection of time-series and regression models that use historical data and statistics to predict future outcomes. Forecasting models relied on historical GitHub data, sign-up rates and product usage, as well as market-sizing information. No forecast is ever accurate. Backtesting shows models used for forecasting growth projections were within reasonable levels of accuracy (less than 30% Mean Absolute Percentage Error).Forecasts do not take into account competitive landscape changes, geopolitical/economic conditions, or future covariates (product/feature releases that may shift responses differently from historical data).]]></content:encoded></item><item><title>Announcing the 2025 GitHub Partner Award winners 🎉</title><link>https://github.blog/news-insights/company-news/announcing-the-2025-github-partner-award-winners/</link><author>Jamie Cooper</author><category>official</category><pubDate>Tue, 28 Oct 2025 16:00:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[In today’s fast-moving tech industry, partnerships are a powerful engine for scale. More than just sales channels, they unlock new markets, fuel innovation, and enhance customer impact. At GitHub, collaboration with our partners is central to how we grow and deliver value. Their expertise and shared commitment help us push boundaries, solve complex challenges, and create better outcomes for developers and businesses alike.Partners are a force multiplier for GitHub. Partners amplify our capabilities, expand our reach, and accelerate innovation for our joint customers. It’s exciting to see how our partner ecosystem is growing and flourishing.Elizabeth Pemmerl, Microsoft CVP and GitHub Chief Revenue OfficerThat said, we are thrilled to announce the winners of our 2025 GitHub Partner Awards—a celebration of the outstanding contributions, innovation, and collaboration from our valued partners around the globe. Each year, we recognize partners who have gone above and beyond in delivering exceptional value, driving transformative impact, and strengthening our shared mission. These partners, recognized in specific categories, are an integral part of the GitHub partner landscape, enabling our joint customers to unlock innovation, fortify security, and build unique solutions and services that integrate GitHub’s secret sauce to meet our joint customers where they are.This year’s honorees exemplify excellence, leadership, and the spirit of true partnership. Now, let’s roll out the red carpet for our 2025 GitHub Partner Award winners!🏆 2025 Partner Award winners 🏆Strategic Services and Channel Partner of the Year: Growth Services and Channel Partner of the Year: AMER Services and Channel Partner of the Year:APAC Services and Channel Partner of the Year: EMEA Services and Channel Partner of the Year: Emerging Market Services and Channel Partner of the Year:Platform Services and Channel Partner of the Year:Security Services and Channel Partner of the Year:AI Services and Channel Partner of the Year: Technology Partner of the Year Award:We’re incredibly grateful for the dedication our partners bring to the table every day. Together, we’re driving a joint mission that’s fundamentally transforming how software is built—raising the bar for the entire industry and reshaping the way people live and work. Your commitment to collaboration and customer success continues to power this momentum and inspire what’s possible.Matt Finkelstein, VP, Global Microsoft, Partner & Services Solution Sales, GitHubWe extend our heartfelt congratulations to all our winners. Your dedication and partnership continue to inspire us and move the industry forward. As we look to the future, we remain committed to growing together, innovating fearlessly, and creating shared success. Thank you for being part of our journey.]]></content:encoded></item><item><title>Project goals for 2025H2</title><link>https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/</link><author>Niko Matsakis</author><category>dev</category><category>official</category><category>rust</category><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[On Sep 9, we merged RFC 3849, declaring our goals for the "second half" of 2025H2 -- well, the last 3 months, at least, since "yours truly" ran a bit behind getting the goals program organized.In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .Unblocking dormant traits, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.Flexible, fast(er) compilation, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers., making higher-level usage patterns in Rust easier.One of Rust's core value propositions is that it's a "library-based language"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a "second-class citizen" problem where custom pointer types can't provide the same ergonomic experience as built-in references.The "Beyond the " initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.
"Unblocking dormant traits"Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:Polonius will enable new borrowing patterns, and in particular unblock "lending iterators". Over the last few goal periods, we have identified an "alpha" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.The next-generation trait solver is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.The work on evolving trait hierarchies will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.The work to expand Rust's  hierarchy will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on RFC #3729 and RFC #3838, authored in previous Project goal periods.In-place initialization allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.The "Flexible, fast(er) compilation" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often "surprisingly tolerable" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:We aim to stabilize cargo script, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.We aim to finalize the design of ergonomic ref-counting and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the Rust program manager hired by the Leadership Council, will join me in running the program.]]></content:encoded></item></channel></rss>