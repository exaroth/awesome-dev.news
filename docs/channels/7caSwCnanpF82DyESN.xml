<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Official News</title><link>https://www.awesome-dev.news</link><description></description><item><title>Building your first MCP server: How to extend AI tools with custom capabilities</title><link>https://github.blog/ai-and-ml/github-copilot/building-your-first-mcp-server-how-to-extend-ai-tools-with-custom-capabilities/</link><author>Chris Reddington</author><category>official</category><pubDate>Fri, 22 Aug 2025 16:52:27 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[Have you ever worked with AI tools and wished they had access to some additional piece of context? Or wanted them to perform actions on your behalf? Think of those scenarios where you’re working with GitHub Copilot and you need it to check a GitHub Issue, run a Playwright test, or interact with an API. By default, these AI tools lack access to those external systems. But that’s where the Model Context Protocol (MCP) comes in. It’s a standardized way to extend AI tools with custom capabilities. I wanted to learn more about it by building something visual and interactive. So I created a turn-based game server that lets you play Tic-Tac-Toe and Rock Paper Scissors against Copilot using MCP.In my latest  live stream, I walked through the project, which has a few components, all written in TypeScript:A Next.JS Web App and API, intended to run locally for demo/learning purposesA shared library for common type definitions and components, which are reused across the web app, API, and MCP serverYou can watch the full stream below 👇Why MCP matters for developersEven with powerful AI agents, we continue to run into limitations:They don’t have access to the latest documentation or real-time data.Agents can’t perform actions like creating pull requests, exploring the UI on your locally running application, or interact with your APIs.To access external context and take action, we need to extend the capabilities of these AI tools. But before MCP, there was no standard approach to integrating with third-party tools and services. You’d potentially need different plugins and different integration patterns for whatever AI tool you were using. MCP changes that by providing  to plug tools and capabilities into any tool that supports the Model Context Protocol.MCP follows a client-server pattern that’s familiar to most developers: The AI tool you’re using, like GitHub Copilot in VS Code (you’ll notice that Copilot in VS Code has good support in the MCP feature support matrix). The host initiates the connection to your MCP server via a client.Clientslive inside the host application (for example, GitHub Copilot in VS Code), and have a 1:1 relationship with a server. When VS Code connects to a new MCP server (like GitHub, Playwright or the turn-based-game MCP server we’re talking about in this post), a new client is created to maintain the connection.: Your custom MCP server that provides tools, resources, and prompts. In our case, we’re making an MCP server that provides capabilities for playing turn-based games! Building the turn-based game MCP serverFor my learning project, I wanted something visual that would show the overall MCP interaction and could be reused when people are trying to explain it as part of a talk. So I built a web app with Tic-Tac-Toe and Rock Paper Scissors. But instead of the game having two people play locally (or online), or even a CPU in the backend, the opponent’s moves are orchestrated by an MCP server.The architecture includes:: The game interface where I make movesAPI routes (part of the Next.js implementation): Backend logic for game state management, called by the frontend and the MCP server.: TypeScript server that handles AI game moves: Common game logic used across componentsHere’s how it works in practice:We register an MCP server in VS Code so that Copilot is aware of the new capabilities and tools.I interact with GitHub Copilot in VS Code. I can call tools explicitly, or Copilot can autodiscover them.Copilot calls the large language model. Based on the prompt context and the available tools, it may call the MCP server.The MCP server executes the requested tool (like making a move in the game) and returns results.Copilot uses those results to continue the conversation.The magic step is when you register the MCP server with an MCP application host (in our example, GitHub Copilot in Visual Studio Code). Suddenly, your AI agent gains access to the capabilities that have been built into those servers.Setting up the MCP server in VS CodeYou can configure your MCP servers by creating a  file. You can find more details about that on the Visual Studio Code docs.{
  "servers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    },
    "turn-based-games": {
      "command": "node",
      "args": ["dist/index.js"],
      "cwd": "./mcp-server"
    }
  }
}The above configuration tells GitHub Copilot in VS Code that there are two MCP servers that we’d like to use:A Playwright MCP server that is executed locally as an NPM package.A turn-based-games MCP server that runs a server locally based on the compiled TypeScript code from our working directory.For this implementation, I kept my turn-based-game MCP server architecture and logic relatively simple, with all components in a single repository. This monorepo approach bundles the web app, API, and MCP server together, making it straightforward to clone and run the entire system locally without complex dependency management or cross-repository setup. But for a more robust setup, you would likely distribute that MCP server either as a package (such as npm or a docker image), and have clear publishing and versioning processes around that. The three core building blocks of MCPThrough building this project, I familiarized myself with three fundamental MCP server concepts:Tools define what actions the MCP server can perform. In my game server, I specified tools like:: Get the current state of any gamecreate_rock_paper_scissors_game: Start a new game of Rock Paper Scissors: Start a new Tic-Tac-Toe game: Make an AI choice in Rock Paper Scissors: Make an AI move in Tic-Tac-Toe: Polls the endpoint until the player has made their moveEach tool has a clear description and input schema that tells the AI what parameters to provide:{
  name: 'play_tic_tac_toe',
  description: 'Make an AI move in Tic-Tac-Toe game. IMPORTANT: After calling this tool when the game is still playing, you MUST call wait_for_player_move to continue the game flow.',
  inputSchema: {
    type: 'object',
    properties: {
      gameId: {
        type: 'string',
        description: 'The ID of the Tic-Tac-Toe game to play',
      },
    },
    required: ['gameId'],
  },
},GitHub Copilot and the Large Language Model (LLM) aren’t calculating the actual game moves. When Copilot calls the play_tic_tac_toe tool, the MCP server executes a handler for that specific tool that runs the CPU game logic, like random moves for Tic Tac Toe in “easy” difficulty or a more optimal  algorithm for moves when using the “hard” difficulty.In other words, tools are reusable pieces of software that can be called by the AI, often to take some form of action (like making a move in a turn-based game!).2. Resources: Context your AI can accessResources provide a way for the AI to gather context, and often have a URI-based identifier. For example, I implemented custom URI schemes like:: List all Tic-Tac-Toe gamesgame://tic-tac-toe/{Game-ID}: Get state for a specific game of Tic Tac Toegame://rock-paper-scissors: List all Rock Paper Scissors gamesgame://rock-paper-scissors/{Game-ID}: Get state for a specific game of Rock Paper ScissorsAs the MCP resources docs explain, you can choose how these resources are passed. In our turn-based-game MCP server, there is a method that translates the resource URIs into an API call to the local API server and passes on the raw response, so that it can be used as context within a tool call (like playing a game).async function readGameResource(uri) {
  const gameSession = await callBackendAPI(gameType, gameId);
  if (!gameSession) {
    throw new Error("Game not found");
  }
  return gameSession;
}3. Prompts: Reusable guidance for usersThe third concept is prompts. You’ll be very familiar with prompts and prompt crafting, as that’s the way that you interact with AI tools like GitHub Copilot. Your users could write their own prompts to use your tools, like “Play a game of Tic Tac Toe” or “Create a GitHub Issue for the work that we’ve just scoped out.”But you may want to ship your MCP server with predefined prompts that help users get the most out of your tools. For example, the turn-based-game MCP comes with several prompts like:Strategy guides for different difficulty levelsGame rules and optimal play instructionsTroubleshooting help for common scenariosYour users can access these prompts via slash commands in VS Code. For example, when I typed , I could access the prompt asking for advice on optimal play for a given game or difficulty level.Real-world applications and considerationsWhile my game demo is intentionally simple to help you learn some of these initial concepts, the patterns apply to other MCP servers:: Get information from existing GitHub Issues or pull requests, list Dependabot alerts, or create and manage issues and pull requests, all based on the access you provide via OAuth (in the remote MCP server) or a Personal Access Token.: Automatically navigate to specific pages in a browser, click and interact with the pages, capture screenshots, and check rendered content.: Connect to your internal services, databases, or business logic.Additional capabilities from the MCP specificationTools, resources, and prompts are some of the most commonly used capabilities of the MCP specification. Recently, a number of additional capabilities including sampling and elicitation were added to the spec. I haven’t had a chance to add those yet, but perhaps they’ll feature as part of another stream!Authentication and securityYou may need to handle authentication and authorization for production MCP servers depending on the scenario. As an example, the GitHub MCP server supports OAuth flows for the remote MCP server and Personal Access Tokens in local and remote. This turn-based game MCP server is intended to be simple, and doesn’t include any auth requirements, but security should be a key consideration if you’re building your own MCP servers.Trusting third-party MCP serversYou may not always need to create your own MCP server. For example, GitHub ships its own MCP server. Instead of creating your own version, why not make an open source contribution upstream to improve the experience for all?Make sure to do your due diligence on MCP servers before installing them, just like you would with any other dependency as part of your project’s supply chain. Do you recognise the publisher? Are you able to review (and contribute to) the code in an open source repository?MCP provides SDKs for multiple languages, so you can build servers in whatever technology fits your stack, ranging from TypeScript to Python, Go to Rust and more. I chose TypeScript because I wanted my entire demo (frontend, backend, and MCP server) in one repository with shared code and a common language.Here’s what you can learn from this exploration:MCP standardizes AI tool extensibility across different platforms and applications (like Copilot in Visual Studio Code) by investigating what existing MCP servers are available. Review the MCP servers: Do you recognize the publisher and can you access the code?Building your own? Start simple with focused servers that solve specific problems rather than trying to build everything at onceThe three building blocks (tools, resources, prompts) provide a clear framework for designing the capabilities of your MCP serverMCP isn’t just about playing games with AI (though that was fun). It’s about breaking down the walls between your AI assistants and the systems they need to help you work effectively. Whether you’re building internal developer tools, integrating with external APIs, or creating custom workflows, MCP provides the foundation you need to extend your AI tools in consistent, powerful ways.Want to explore MCP further? Here are some practical starting points:Check out the GitHub MCP server to use in your own workflows or learn more about a real-world MCP server implementation.Build a simple server for your internal APIs or development tools. You can check out the turn-based-game-mcp as an example.Experiment with custom prompts that encode your team’s best practices.The goal of MCP is to give your AI assistants the tools they need to be truly helpful in your specific development environment. So, which tool will you be using? What will you build?]]></content:encoded></item><item><title>Explore the best of GitHub Universe: 9 spaces built to spark creativity, connection, and joy</title><link>https://github.blog/news-insights/company-news/explore-the-best-of-github-universe-9-spaces-built-to-spark-creativity-connection-and-joy/</link><author>Jeimy Ruiz</author><category>official</category><pubDate>Thu, 21 Aug 2025 18:02:34 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[What does your ideal developer event look like?At GitHub Universe 2025, we’re building it. Returning to Fort Mason Center in San Francisco this October, Universe will be bigger, bolder, and more interactive than ever before. In addition to 100+ expert-led sessions (our full session catalogue drops in early September), you’ll find nine unique spaces designed to spark creativity, connection, and a lot of joy — whether you’re building LEGOs, networking with peers, or exploring new career resources.In this blog, we’re breaking down everything you can do on-site so you can start thinking about how you’ll spend your time. Let’s dive in.Go beyond the keynotes and sessions to explore GitHub’s latest tools with help from the people who know them best. If you’re curious about what’s new or want to go deep on the tools you already use, Universe is packed with opportunities to level up. Here’s where to start:After the morning keynotes, head over to GitHub Central, where you’ll find live demos, customer stories, and product journeys aligned to our three content tracks — all designed to help you build more efficiently, securely, and creatively with GitHub.You’ll explore GitHub Copilot, GitHub Actions, GitHub Advanced Security, and more. With a mix of self-guided stations, live sessions with GitHub experts, and fun surprises, GitHub Central is your hub for discovery and inspiration.Whether you’re an enterprise architect or just getting started with automation, you’ll leave with practical ideas and trusted strategies to bring back to your team.The GitHub Expert Center is your go-to for technical deep dives, quick advice, and 1:1 conversations. From specialists in AI and GitHub Actions, to security to scaled adoption, our GitHub experts are on-site and ready to answer all your GitHub product questions. Getting help is easy; you can book an appointment ahead of time or stop by in between your sessions. Fuel your growth (and your network!)The future of software is being shared, shaped, and supported by the people behind the code. If you’re interested in joining us at Universe to sharpen your skills, expand your network, or prep to find your next big opportunity, this is the place to start:In the Open Source Zone, you can connect with contributors, maintainers, and community leaders from around the globe. Discover rising stars from the GitHub Accelerator program, learn from Maintainer Community champions, and explore projects that are changing the way the world builds software. Looking to take the next step in your dev career? Book a 1:1 session with a career coach to get personalized advice on everything, including polishing your resume, optimizing your GitHub and LinkedIn profiles, and helping you prep for your next interview. The Career Corner is here to help you level up with confidence.Learning isn’t one-size-fits-all. That’s why GitHub Learn brings together tutorials, certifications, and role-based learning paths to help you grow on your terms. Explore content from GitHub, Microsoft Learn, and more to help you build real, job-ready skills designed for developers at every level.At Universe, some of the most memorable moments happen when you’re solving puzzles, swapping stories with someone you just met, or building something weird and wonderful with AI. The spaces below are yours to unwind, experiment, and rediscover the joy of making.Take a quick break from sessions and connect over shared interests. Recess is your chance to meet fellow attendees or hang out with coworkers who share your non-dev passions, from Lego building to chatting with executives over gelato. Get your hands working in the most creative way possible! The Makerspace is where code meets art, music, robotics, and beyond. This is your space to play, explore, and reimagine what code can do. No formal training required — just bring your curiosity.Every in-person ticket includes a hackable badge — designed to be customized and coded. Follow tutorials, get inspired by other hackers, and create a unique piece of hardware art to commemorate your Universe experience.Looking for GitHub Copilot gear? Want exclusive merch that speaks dev? The Shop is your destination for exclusive GitHub swag, fan-favorite collectibles, and a few surprises you’ll only find in person at our global developer event.Don’t miss your moment at GitHub UniverseRegardless of what you want to accomplish during your time at Fort Mason, you can fully customize your event experience at GitHub Universe. Now’s the best time to grab your in-person pass and hang out with the people who love building and scaling as much as you do:Save  on your pass with our  discount, only until September 8.Get  when you buy Or get when you buy (And yes, either group discount can be combined with Early Bird pricing!)]]></content:encoded></item><item><title>Who will maintain the future? Rethinking open source leadership for a new generation</title><link>https://github.blog/open-source/maintainers/who-will-maintain-the-future-rethinking-open-source-leadership-for-a-new-generation/</link><author>Abigail Cabunoc Mayes</author><category>official</category><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[When I was a first-year student, I joined a campus club after seeing a flyer in the hallway. I didn’t know much about it — only that my mom had been part of the same group when she was in university, and it had shaped her life. At the first meeting, I found instant community. Some of those people became lifelong friends. But they were also mostly in their third or fourth year.The next year, most of them graduated and there was a huge leadership gap. I wasn’t ready, but I stepped up.That experience taught me something I’ll never forget: If you don’t bring in new people early, your community won’t last.Years later, I’m seeing the same pattern in open source. We talk a lot about burnout, bus factors, and maintainers leaving — but we don’t talk enough about how to bring in new contributors, or what it takes to help them grow into leaders.The graying of Open SourceAccording to Tidelift’s 2024 maintainer survey, the percentage of maintainers aged 46–65 has doubled since 2021. Meanwhile, the share of contributors under 26 has dropped from 25% to just 10%.This “graying” isn’t inherently a problem. But the lack of succession is. If we don’t create pathways for younger contributors, we’re setting ourselves up for burnout, knowledge loss, and long-term fragility.Enter Sam: A Gen Z personaTo explore what support might look like, I introduced a persona named “Sam” in a recent talk at Open Source Summit North America:: Urban Canada, lives mostly online: “I want to contribute to a climate tech project that actually matters — but I don’t know where to start. I taught myself to code on YouTube. I design and moderate online communities. But public repos feel intimidating. I want purpose, flexibility, and a place where people like me belong.”Sam is our Gen Z persona. They want to contribute to something that matters. They’re self-taught, community-oriented, and motivated by purpose. But they’re also navigating financial pressures, unclear pathways, and they aren’t sure how leadership in open source actually works.How do we help Sam thrive in open source?The mountain of engagementTo support contributors like Sam, here’s a framework I’ve used for years in programs like Mozilla Open Leaders and GitHub’s Maintainer Programs: the Mountain of Engagement.This model outlines a contributor’s journey in six steps:: How they first hear about the project.: How they first engage with the project or their initial interaction.: How they first participate or contribute.: How their contribution or involvement can continue.: How they may invite and onboard others or networking within the community.: How they may take on some additional responsibility on the project, or begin to lead.At each stage, you can compare traditional best practices with what Gen Z contributors like Sam might actually need.How they first hear about the project. Make your project discoverable. That means publishing to a public repository, applying an open source license, and doing basic marketing: a project website, documentation, and maybe a few social posts. Sam isn’t browsing GitHub trending pages. They’re discovering projects through TikTok, Discord, and YouTube. They want to see purpose up front, not buried in a README. And their learning starts on mobile.84% of Gen Z are on YouTube (Sprout Social Index, 2025)86% say purpose is very or somewhat important for their job satisfaction. (Deloitte, 2025)To reach Sam, projects need to show up where they already are, with formats and values that resonate.How they first engage with the project, their initial interaction. A good README, clear contributing docs, and a communication channel where newcomers can ask questions. A mobile-friendly, visual-first landing experience. A project that leads with its mission. A casual, open chat like Discord where they can lurk before jumping in.72.8% of Gen Z prefer visual learning. (TJHSS, 2025)Gen Z favors community-driven platforms like Discord over public forums. (Impero, 2022)How they first participate or contribute. Personal invitations, fast responses to questions, “good first issues,” and clear contribution docs. These reduce friction and help people get started. Real-time feedback. Sandboxed environments to try things out. Clear spaces where it’s okay to learn, not just perform.Generation Z students tend to be keen observers; they prefer to watch others complete tasks before attempting them themselves. (TJHSS, 2025)4. Sustained participationHow their contribution or involvement can continue. Recognize contributors, match tasks to interests, and show how their work connects to the project’s mission. Recognition they can share: badges, mentions, portfolios. They care about making a difference more than climbing a hierarchy. Show impact.86% of Gen Z workers prioritize mentorship and skill development. (Deloitte, 2025)5. Networked participationHow they may invite and onboard others, networking within the community. Mentorship, social events, and formal roles that build commitment and connection. Named, shareable roles like Discord mod or community guide. Off-topic channels and casual connection. Peer-led leadership that spreads influence.70% of Gen Z join communities for belonging and voice. (Impero, 2022)How they may take on some additional responsibility on the project, or begin to lead. Invite someone to become a maintainer. Share governance. Provide documentation on roles and responsibilities. Shared stewardship, not top-down control. Compensation or professional growth. A clear value exchange.52% of Gen Z live paycheck to paycheck. (Deloitte, 2024)They’re more likely to contribute when there’s tangible support: mentorship, visibility, or paid time.Open source won’t thrive without the next generation. Let’s build projects where contributors like Sam feel welcome, supported, and seen.Here are a few concrete actions you can take this week:Turn your  into a 60-second explainer video.Create a sandbox space for first-time contributors.Start a Discord or off-topic channel to foster belonging.Make your project’s mission loud and visible.Ask yourself: “What does a thriving project look like to Sam? What would it take for them to stay for five years, not five weeks?”Let’s build a future togetherMaintaining open source isn’t just about keeping the lights on. It’s about creating space for the next generation.Want to go deeper? Take a look at my slides (with speaker notes + references).Let’s build an ecosystem where maintainers are supported, projects thrive, and people like Sam stay for years — not weeks.]]></content:encoded></item><item><title>Container-aware GOMAXPROCS</title><link>https://go.dev/blog/container-aware-gomaxprocs</link><author>Michael Pratt and Carlos Amedee</author><category>dev</category><category>official</category><category>go</category><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go’s out-of-the-box production-readiness.
In this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.One of Go’s strengths is its built-in and easy-to-use concurrency via goroutines.
From a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.
On the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.
Any Go-managed thread can run any goroutine, so creating a new goroutine doesn’t require creating a new thread, and waking a goroutine doesn’t necessarily require waking another thread.That said, along with a scheduler comes scheduling questions.
For example, exactly how many threads should we use to run goroutines?
If 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?This is where  comes in.
Semantically,  tells the Go runtime the “available parallelism” that Go should use.
In more concrete terms,  is the maximum number of threads to use for running goroutines at once.So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.
Often, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.
Go will also preempt goroutines that don’t block on their own, ensuring all goroutines get a chance to run.From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.
Note that in this post, “core” more precisely means “logical CPU.”
For example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.This typically makes a good default for “available parallelism” because it naturally matches the available parallelism of the hardware.
That is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.
This extra layer of scheduling is not always a problem, but it is unnecessary overhead.Another of Go’s core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.
Container orchestration platforms like Kubernetes take a set of machine resources and schedule containers within the available resources based on requested resources.
Packing as many containers as possible within a cluster’s resources requires the platform to be able to predict the resource usage of each scheduled container.
We want Go to adhere to the resource utilization constraints that the container orchestration platform sets.Let’s explore the effects of the  setting in the context of Kubernetes, as an example.
Platforms like Kubernetes provide a mechanism to limit the resources consumed by a container.
Kubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.
Setting a CPU limit translates to the creation of a Linux control group CPU bandwidth limit.Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.
Instead, it would set  to the number of cores on the machine it was deployed to.
If there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.
To prevent an application from exceeding its limit, the Linux kernel will throttle the application.Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.
The throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.
Even if the application never has much parallelism, tasks performed by the Go runtime—such as garbage collection—can still cause CPU spikes that trigger throttling.We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.
If a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.Both of these defaults only apply if  is otherwise unspecified.
Setting the  environment variable or calling  continues to behave as before.
The  documentation covers the details of the new behavior.Slightly different modelsBoth  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different. is a parallelism limit.
If  Go will never run more than 8 goroutines at a time.By contrast, CPU limits are a throughput limit.
That is, they limit the total CPU time used in some period of wall time.
The default period is 100ms.
So an “8 CPU limit” is actually a limit of 800ms of CPU time every 100ms of wall time.This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .
On the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.In other words, a CPU limit doesn’t limit the total number of CPUs the container can run on.
It only limits total CPU time.Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!
However, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).
On the other hand,  must be a positive integer.
Thus, Go must round the limit to a valid  value.
Go always rounds up to enable use of the full CPU limit.Go’s new  default is based on the container’s CPU limit, but container orchestration systems also provide a “CPU request” control.
While the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.
Unfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.Containers with a CPU request are still constrained when exceeding their request if the machine is busy.
The weight-based constraint of exceeding requests is “softer” than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.Should I set a CPU limit?We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.
For example, a small container receiving 2 CPUs running on a 128 core machine.
These are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.
Doing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.
You can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .Thanks to everyone in the community that contributed to the longdiscussions that made this a reality, and in particular to feedback from the maintainers of  from Uber, which has long provided similar behavior to its users.]]></content:encoded></item><item><title>Agents panel: Launch Copilot coding agent tasks anywhere on GitHub</title><link>https://github.blog/news-insights/product-news/agents-panel-launch-copilot-coding-agent-tasks-anywhere-on-github/</link><author>Tim Rogers</author><category>official</category><pubDate>Tue, 19 Aug 2025 19:53:14 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[If the past year has underscored anything, it’s that AI agents are becoming a bigger part of developers’ day-to-day workflows.We recently launched Copilot coding agent, an asynchronous, autonomous developer agent: it allows you to assign an issue to Copilot, and Copilot will work in the background and create a draft pull request for your review. Copilot coding agent works like a member of your team, and it’s received a great response from developers so far. But we know that not all of your work lives in GitHub Issues. Today, we’re launching a new agents experience on GitHub — the agents panel — allowing you to quickly delegate tasks to Copilot from any page on github.com with a simple prompt and track Copilot’s progress without breaking your flow.And of course, Copilot coding agent is also integrated into VS Code, GitHub Mobile, and the GitHub MCP Server, so you can collaborate with Copilot wherever you’re working.Delegate any coding task to Copilot coding agent, wherever you are, with the agents panelNeed a quick refresher on GitHub Copilot coding agent? Skip ahead >The agents panel, available today on every page on github.com, is your mission control center for agentic workflows on GitHub. It’s a  that allows you to hand new tasks to Copilot and track existing tasks without navigating away from your current work. Just click the new Agents button in the navigation bar to get started.From the agents panel, you can:🛠️  without switching pages.👀  with real-time status.🔗  when you’re ready to review.Launch new tasks without breaking your flowYou can start a new Copilot task from the new agents panel with a simple prompt. Just open the panel from any page on GitHub, describe your goal in natural language, and select the relevant repository. Copilot will then take it from there and start creating a plan, drafting changes, running tests, and then preparing a pull request.Not sure where to start? Try the following sample prompts::
“Add integration tests for LoginController”“Refactor WidgetGenerator for better code reuse”“Add a dark mode/light mode switcher”Refer to a GitHub issue or pull request as context, optionally providing extra context:
“Fix #877 using pull request #855 as an example”“Fix #1050, and make sure you update the screenshots in the README”Run multiple tasks in parallel:“Add unit test coverage for utils.go” + “Add unit test coverage for helpers.go”Copilot coding agent: a quick reintroductionCopilot coding agent lets you hand off coding tasks — via GitHub.com, GitHub Mobile, VS Code, or any MCP-enabled tool — and get back a draft pull request when it’s done. It runs in the cloud, can work in parallel on multiple tasks, and continues even if your computer is off.Its secure, GitHub Actions-powered environment can run builds, tests, and linters without asking for every step. You stay in control with detailed logs and pull request-based approvals, and can give feedback by mentioning  in a review.More ways to hand off work to CopilotYou can also start Copilot coding agent tasks from:Copilot coding agent and the new agents panel on GitHub is available today in public preview for all paid Copilot subscribers. Your administrator may need to enable the Copilot coding agent policy.]]></content:encoded></item><item><title>Tuning Linux Swap for Kubernetes: A Deep Dive</title><link>https://kubernetes.io/blog/2025/08/19/tuning-linux-swap-for-kubernetes-a-deep-dive/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Tue, 19 Aug 2025 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The Kubernetes NodeSwap feature, likely to graduate to  in the upcoming Kubernetes v1.34 release,
allows swap usage:
a significant shift from the conventional practice of disabling swap for performance predictability.
This article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills.However, enabling swap is not a "turn-key" solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet's eviction logic.In this blogpost, I'll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms.
I will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters.Introduction to Linux swapAt a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel's page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors:Page access patterns (how recently pages are accessed)Page dirtyness (whether pages have been modified)Memory pressure (how urgently the system needs free memory)Anonymous vs File-backed memoryIt is important to understand that not all memory pages are the same. The kernel distinguishes between anonymous and file-backed memory.: This is memory that is not backed by a specific file on the disk, such as a program's heap and stack. From the application's perspective this is private memory, and when the kernel needs to reclaim these pages, it must write them to a dedicated swap device.: This memory is backed by a file on a filesystem. This includes a program's executable code, shared libraries, and filesystem caches. When the kernel needs to reclaim these pages, it can simply discard them if they have not been modified ("clean"). If a page has been modified ("dirty"), the kernel must first write the changes back to the file before it can be discarded.While a system without swap can still reclaim clean file-backed pages memory under pressure by dropping them, it has no way to offload anonymous memory. Enabling swap provides this capability, allowing the kernel to move less-frequently accessed memory pages to disk to conserve memory to avoid system OOM kills.Key kernel parameters for swap tuningTo effectively tune swap behavior, Linux provides several kernel parameters that can be managed via .: This is the most well-known parameter. It is a value from 0 to 200 (100 in older kernels) that controls the kernel's preference for swapping anonymous memory pages versus reclaiming file-backed memory pages (page cache).
: The kernel will be aggressive in swapping out less-used anonymous memory to make room for file-cache.: The kernel will strongly prefer dropping file cache pages over swapping anonymous memory.: This parameter tells the kernel to keep a minimum amount of memory free as a buffer. When the amount of free memory drops below the this safety buffer, the kernel starts more aggressively reclaiming pages (swapping, and eventually handling OOM kills).
 It acts as a safety lever to ensure the kernel has enough memory for critical allocation requests that cannot be deferred.: Setting a higher  effectively raises the floor for for free memory, causing the kernel to initiate swap earlier under memory pressure.vm.watermark_scale_factor: This setting controls the gap between different watermarks: ,  and , which are calculated based on .
:
: When free memory is below this mark, the  kernel process wakes up to reclaim pages in the background. This is when a swapping cycle begins.: When free memory hits this minimum level, then aggressive page reclamation will block process allocation. Failing to reclaim pages will cause OOM kills.: Memory reclamation stops once the free memory reaches this level.: A higher  careates a larger buffer between the  and  watermarks. This gives  more time to reclaim memory gradually before the system hits a critical state.In a typical server workload, you might have a long-running process with some memory that becomes 'cold'. A higher  value can free up RAM by swapping out the cold memory, for other active processes that can benefit from keeping their file-cache.Tuning the  and  parameters to move the swapping window early will give more room for  to offload memory to disk and prevent OOM kills during sudden memory spikes.To understand the real-impact of these parameters, I designed a series of stress tests.: GKE on Google Cloud: 1.33.2:  (8GiB RAM, 50GB swap on a  disk, without encryption), Ubuntu 22.04: A custom Go application designed to allocate memory at a configurable rate, generate file-cache pressure, and simulate different memory access patterns (random vs sequential).: A sidecar container capturing system metrics every second.: Critical system components (kubelet, container runtime, sshd) were prevented from swapping by setting  in their respective cgroups.I ran a stress-test pod on nodes with different swappiness settings (0, 60, and 90) and varied the  and  parameters to observe the outcomes under heavy memory allocation and I/O pressure.Visualizing swap in actionThe graph below, from a 100MBps stress test, shows swap in action. As free memory (in the "Memory Usage" plot) decreases, swap usage () and swap-out activity () increase. Critically, as the system relies more on swap, the I/O activity and corresponding wait time ( in the "CPU Usage" plot) also rises, indicating CPU stress.My initial tests with default kernel parameters (, , watermark_scale_factor=10) quickly led to OOM kills and even unexpected node restarts under high memory pressure. With selecting appropriate kernel parameters a good balance in node stability and performance can be achieved.The swappiness parameter directly influences the kernel's choice between reclaiming anonymous memory (swapping) and dropping page cache. To observe this, I ran a test where one pod generated and held file-cache pressure, followed by a second pod allocating anonymous memory at 100MB/s, to observe the kernel preference on reclaim:My findings reveal a clear trade-off:: The kernel proactively swapped out the inactive anonymous memory to keep the file cache. This resulted in high and sustained swap usage and significant I/O activity ("Blocks Out"), which in turn caused spikes in I/O wait on the CPU.: The kernel favored dropping file-cache pages delaying swap consumption. However, it's critical to understand that this does not disable swapping. When memory pressure was high, the kernel still swapped anonymous memory to disk.The choice is workload-dependent. For workloads sensitive to I/O latency, a lower swappiness is preferable. For workloads that rely on a large and frequently accessed file cache, a higher swappiness may be beneficial, provided the underlying disk is fast enough to handle the load.Tuning watermarks to prevent eviction and OOM killsThe most critical challenge I encountered was the interaction between rapid memory allocation and Kubelet's eviction mechanism. When my test pod, which was deliberately configured to overcommit memory, allocated it at a high rate (e.g., 300-500 MBps), the system quickly ran out of free memory.With default watermarks, the buffer for reclamation was too small. Before  could free up enough memory by swapping, the node would hit a critical state, leading to two potential outcomes: If kubelet's eviction manager detected  was below its threshold, it would evict the pod. In some high-rate scenarios, the OOM Killer would activate before eviction could complete, sometimes killing higher priority pods that were not the source of the pressure.To mitigate this I tuned the watermarks:Increased  to 512MiB: This forces the kernel to start reclaiming memory much earlier, providing a larger safety buffer.Increased  to 2000: This widened the gap between the  and  watermarks (from ≈337MB to ≈591MB in my test node's ), effectively increasing the swapping window.This combination gave  a larger operational zone and more time to swap pages to disk during memory spikes, successfully preventing both premature evictions and OOM kills in my test runs.Table compares watermark levels from  (Non-NUMA node): and watermark_scale_factor=10min_free_kbytes=524288KiB and watermark_scale_factor=2000Node 0, zone Normal    pages free 583273    min 10504    high 15756    present 1310720 Node 0, zone Normal    pages free 470539    low 337017    spanned 1310720   managed 1274542The graph below reveals that the kernel buffer size and scaling factor play a crucial role in determining how the system responds to memory load. With the right combination of these parameters, the system can effectively use swap space to avoid eviction and maintain stability.Enabling swap in Kubernetes is a powerful tool, but it comes with risks that must be managed through careful tuning.Risk of performance degradation Swapping is orders of magnitude slower than accessing RAM. If an application's active working set is swapped out, its performance will suffer dramatically due to high I/O wait times (thrashing). Swap could preferably be provisioned with a SSD backed storage to improve performance.Risk of masking memory leaks Swap can hide memory leaks in applications, which might otherwise lead to a quick OOM kill. With swap, a leaky application might slowly degrade node performance over time, making the root cause harder to diagnose.Risk of disabling evictions Kubelet proactively monitors the node for memory-pressure and terminates pods to reclaim the resources. Improper tuning can lead to OOM kills before kubelet has a chance to evict pods gracefully. A properly configured  is essential to ensure kubelet's eviction mechanism remains effective.Together, the kernel watermarks and kubelet eviction threshold create a series of memory pressure zones on a node. The eviction-threshold parameters need to be adjusted to configure Kubernetes managed evictions occur before the OOM kills.As the diagram shows, an ideal configuration will be to create a large enough 'swapping zone' (between  and  watermarks) so that the kernel can handle memory pressure by swapping before available memory drops into the Eviction/Direct Reclaim zone.Based on these findings, I recommend the following as a starting point for Linux nodes with swap enabled. You should benchmark this with your own workloads.: Linux default is a good starting point for general-purpose workloads. However, the ideal value is workload-dependent, and swap-sensitive applications may need more careful tuning.vm.min_free_kbytes=500000 (500MB): Set this to a reasonably high value (e.g., 2-3% of total node memory) to give the node a reasonable safety buffer.vm.watermark_scale_factor=2000: Create a larger window for  to work with, preventing OOM kills during sudden memory allocation spikes.I encourage running benchmark tests with your own workloads in test-environments, when setting up swap for the first time in your Kubernetes cluster. Swap performance can be sensitive to different environment differences such as CPU load, disk type (SSD vs HDD) and I/O patterns.]]></content:encoded></item><item><title>Demoting x86_64-apple-darwin to Tier 2 with host tools</title><link>https://blog.rust-lang.org/2025/08/19/demoting-x86-64-apple-darwin-to-tier-2-with-host-tools/</link><author>Jake Goulding</author><category>dev</category><category>official</category><category>rust</category><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[In Rust 1.90.0, the target  will be demoted to Tier 2 with host tools.
The standard library and the compiler will continue to be built and distributed,
but automated tests of these components are no longer guaranteed to be run.Rust has supported macOS for a long time,
with some amount of support dating back to Rust 0.1 and likely before that.
During that time period,
Apple has changed CPU architectures from x86 to x86_64 and now to Apple silicon,
ultimately announcing the end of support for the x86_64 architecture.Similarly,
GitHub has announced that they will no longer provide free macOS x86_64 runners for public repositories.
The Rust Project uses these runners to execute automated tests for the  target.
Since the target tier policy requires that Tier 1 platforms must run tests in CI,
the  target must be demoted to Tier 2.Starting with Rust 1.90.0,  will be Tier 2 with host tools.
For users,
nothing will change immediately;
builds of both the standard library and the compiler will still be distributed by the Rust Project for use via  or alternative installation methods.Over time,
this target will likely accumulate bugs faster due to reduced testing.If the  target causes concrete problems,
it may be demoted further.
No plans for further demotion have been made yet.For more details on the motivation of the demotion, see RFC 3841.]]></content:encoded></item><item><title>Launching MDN&apos;s new front end</title><link>https://developer.mozilla.org/en-US/blog/launching-new-front-end/</link><author>mdn-team</author><category>dev</category><category>official</category><category>frontend</category><enclosure url="https://developer.mozilla.org/en-US/blog/launching-new-front-end/featured.png" length="" type=""/><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate><source url="https://developer.mozilla.org/en-US/blog/">MDN Blog</source><content:encoded><![CDATA[MDN is getting a facelift 🎉. Discover what's changed, what's improved, and how navigating the site just got smoother.
]]></content:encoded></item><item><title>Highlights from Git 2.51</title><link>https://github.blog/open-source/git/highlights-from-git-2-51/</link><author>Taylor Blau</author><category>official</category><pubDate>Mon, 18 Aug 2025 17:04:36 +0000</pubDate><source url="https://github.blog/">Github Blog</source><content:encoded><![CDATA[To celebrate this most recent release, here is GitHub’s look at some of the most interesting features and changes introduced since last time.Cruft-free multi-pack indexesGit stores repository contents as “objects” (blobs, trees, commits), either individually (“loose” objects, e.g. $GIT_DIR/objects/08/10d6a05...) or grouped into “packfiles” (). Each pack has an index () that maps object hashes to offsets. With many packs, lookups slow down to , (where  is the number of packs in your repository, and  is the number of objects within a given pack).A MIDX works like a pack index but covers the objects across multiple individual packfiles, reducing the lookup cost to , where  is the total number of objects in your repository. We use MIDXs at GitHub to store the contents of your repository after splitting it into multiple packs. We also use MIDXs to store a collection of reachability bitmaps for some selection of commits to quickly determine which object(s) are reachable from a given commit.However, we store unreachable objects separately in what is known as a “cruft pack”. Cruft packs were meant to exclude unreachable objects from the MIDX, but we realized pretty quickly that doing so was impossible. The exact reasons are spelled out in this commit, but the gist is as follows: if a once-unreachable object (stored in a cruft pack) later becomes reachable from some bitmapped commit, but the only copy of that object is stored in a cruft pack outside of the MIDX, then that object has no bit position, making it impossible to write a reachability bitmap.Git 2.51 introduces a change to how the non-cruft portion of your repository is packed. When generating a new pack, Git used to exclude any object which appeared in at least one pack that would not be deleted during a repack operation, including cruft packs. In 2.51, Git now will store additional copies of objects (and their ancestors) whose only other copy is within a cruft pack. Carrying this process out repeatedly guarantees that the set of non-cruft packs does not have any object which reaches some other object not stored within that set of packs. (In other words, the set of non-cruft packs is closed under reachability.)As a result, Git 2.51 has a new repack.MIDXMustContainCruft configuration which uses the new repacking behavior described above to store cruft packs outside of the MIDX. Using this at GitHub has allowed us to write significantly smaller MIDXs, in a fraction of the time, and resulting in faster repository read performance overall. (In our primary monorepo, MIDXs shrunk by about 38%, we wrote them 35% faster, and improved read performance by around 5%.)Give cruft-less MIDXs a try today using the new repack.MIDXMustContainCruft configuration option.Smaller packs with path walkIn Git 2.49, we talked about Git’s new “name-hash v2” feature, which changed the way that Git selects pairs of objects to delta-compress against one another. The full details are covered in that post, but here’s a quick gist. When preparing a packfile, Git computes a hash of all objects based on their filepath. Those hashes are then used to sort the list of objects to be packed, and Git uses a sliding window to search between pairs of objects to identify good delta/base candidates.Prior to 2.49, Git used a single hash function based on the object’s filepath, with a heavy bias towards the last 16 characters of the path. That hash function, dating back all the way to 2006, works well in many circumstances, but can fall short when, say, unrelated blobs appear in paths whose final 16 characters are similar. Git 2.49 introduced a new hash function which takes more of the directory structure into account, resulting in significantly smaller packs in some circumstances.Git 2.51 takes the spirit of that change and goes a step further by introducing a new way to collect objects when repacking, called “path walk”. Instead of walking objects in revision order with Git emitting objects with their corresponding path names along the way, the path walk approach emits all objects from a given path at the same time. This approach avoids the name-hash heuristic altogether and can look for deltas within groups of objects that are known to be at the same path.As a result, Git can generate packs using the path walk approach that are often significantly smaller than even those generated with the new name hash function described above. Its timings are competitive even with generating packs using the existing revision order traversal.Try it out today by repacking with the new  command-line option.If you’ve ever needed to switch to another branch, but wanted to save any uncommitted changes, you have likely used . The stash command stores the state of your working copy and index, and then restores your local copy to match whatever was in  at the time you stashed.If you’ve ever wondered how Git actually stores a stash entry, then this section is for you. Whenever you push something onto your stash, Git creates three commits behind the scenes. There are two commits generated which capture the staged and unstaged changes. The staged changes represent whatever was in your index at the time of stashing, and the working directory changes represent everything you changed in your local copy but didn’t add to the index. Finally, Git creates a third commit listing the other two as its parents, capturing the entire snapshot.Those internally generated commits are stored in the special  ref, and multiple stash entries are managed with the reflog. They can be accessed with , and so on. Since there is only one stash entry in  at a time, it’s extremely cumbersome to migrate stash entries from one machine to another.Git 2.51 introduces a variant of the internal stash representation that allows multiple stash entries to be represented as a sequence of commits. Instead of using the first two parents to store changes from the index and working copy, this new representation adds one more parent to refer to the previous stash entry. That results in stash entries that contain four parents, and can be treated like an ordinary log of commits.As a consequence of that, you can now export your stashes to a single reference, and then push or pull it like you would a normal branch or tag. Git 2.51 makes this easy by introducing two new sub-commands to git stash to import and export, respectively. You can now do something like:$ git stash export --to-ref refs/stashes/my-stash
$ git push origin refs/stashes/my-stashon one machine to push the contents of your stash to origin, and then:$ git fetch origin '+refs/stashes/*:refs/stashes/*'
$ git stash import refs/stashes/my-stashon another, preserving the contents of your stash between the two.Now that we’ve covered some of the larger changes in more detail, let’s take a quicker look at a selection of some other new features and updates in this release.
If you’ve ever scripted around the object contents of your repository, you have no doubt encountered , Git’s dedicated tool to print the raw contents of a given object. also has specialized  and  modes, which take a sequence of objects over stdin and print each object’s information (and contents, in the case of ). For example, here’s some basic information about the  file in Git’s own repository.$ echo HEAD:README.md | git.compile cat-file --batch-check
d87bca1b8c3ebf3f32deb557ae9796ddc5b792ca blob 3662Here, Git is telling us the object ID, type, and size for the object we specified, just as we expect.  produces the same information for tree and commit objects. But what happens if we give it the path to a submodule? Prior to Git 2.51,  would just print . But Git 2.51 improves this output, making  more useful in a new variety of scripting scenarios:[ pre-2.51 git ]
$ echo HEAD:sha1collisiondetection | git cat-file --batch-check
HEAD:sha1collisiondetection missing

[ git 2.51 ]
$ echo HEAD:sha1collisiondetection | git cat-file --batch-check 855827c583bc30645ba427885caa40c5b81764d2 submoduleBack in our coverage of 2.28, we talked about Git’s new changed-path Bloom feature. If you aren’t familiar with Bloom filters, or could use a refresher about how they’re used in Git, then read on.A Bloom filter is a probabilistic data structure that behaves like a set, with one difference. It can only tell you with 100% certainty whether an element is  in the set, but may have some false positives when indicating that an item is in the set.Git uses Bloom filters in its commit-graph data structure to store a probabilistic set of which paths were modified by that commit relative to its first parent. That allows history traversals like git log origin -- path/to/my/file to quickly skip over commits which are known not to modify that path (or any of its parents). However, because Git’s full pathspec syntax is far more expressive than that, Bloom filters can’t always optimize pathspec-scoped history traversals.Git 2.51 addresses part of that limitation by adding support for using multiple pathspec items, like git log -- path/to/a path/to/b, which previously could not make use of changed-path Bloom filters. At the time of writing, there is ongoing discussion about adding support for even more special cases.The modern equivalents of , known as  and  have been considered experimental since their introduction back in Git 2.23. These commands delineate the many jobs that  can perform into separate, more purpose-built commands. Six years later, these commands are no longer considered experimental, making their command-line interface stable and backwards compatible across future releases.Even if you’re a veteran Git user, it’s not unlikely to encounter a new Git command (among the 144!)  every once in a while. One such command you might not have heard of is , which behaves like its modern alternative .That command is now marked as deprecated with eventual plans to remove it in Git 3.0. As with other similar deprecations, you can still use this command behind the aptly-named  flag.Speaking of Git 3.0, this release saw a few more entries added to the  list. First, Git’s reftable backend (which we talked about extensively in our coverage of Git 2.45) will become the new default format in repositories created with Git 3.0, when it is eventually released. Git 3.0 will also use the SHA-256 hash function as its default hash when initializing new repositories.Though there is no official release date yet planned for Git 3.0, you can get a feel for some of the new defaults by building Git yourself with the  flag.Last but not least, a couple of updates on Git’s internal development process. Git has historically prioritized wide platform compatibility, and, as a result, has taken a conservative approach to adopting features from newer C standards. Though Git has required a C99-compatible compiler since near the end of 2021, it has adopted features from that standard gradually, since some of the compilers Git targets only have partial support for the standard.One example is the  keyword, which became part of the C standard in C99. Here, the project began experimenting with the  keyword back in late 2023. This release declares that experiment a success and now permits the use of  throughout its codebase. This release also began documenting C99 features that the project is using experimentally along with C99 features that the project doesn’t use.Finally, this release saw an update to Git’s guidelines on submitting patches, which have historically required contributions to be non-anonymous, and submitted under a contributor’s legal name. Git now aligns more closely with the Linux kernel’s approach, to permit submitting patches with an identity other than the contributor’s legal name. For some bit position (corresponding to a single object in your repository,) a  means that object can be reached from that bitmap’s associated commit, and a  means it is not reachable from that commit. There are also four type-level bitmaps (for blobs, trees, commits, and annotated tags); the  of those bitmaps is the all s bitmap. For more details on multi-pack reachability bitmaps, check out our previous post on Scaling monorepo maintenance. ⤴️ For the curious, each layer of the directory is hashed individually, then downshifted and  ed into the overall result. This results in a hash function which is more sensitive to the whole path structure, rather than just the final 16 characters. ⤴️ Usually. Git will sometimes generate a fourth commit if you stashed untracked (new files that haven’t yet been committed) or ignored files (that match one or more patterns in a ). ⤴️ Almost to the day; Git 2.23 was released on August 16, 2019, and Git 2.51 was released on August 18, 2025. ⤴️ It’s true; git --list-cmds=builtins | wc -l outputs “144” with Git 2.51. ⤴️ If you are somehow a diehard  user, please let us know by sending a message to the Git mailing list. ⤴️]]></content:encoded></item></channel></rss>