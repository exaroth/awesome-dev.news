<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Ross Stevens Donates $100M to Pay Every US Olympian and Paralympian $200k</title><link>https://www.townandcountrymag.com/leisure/sporting/a70171886/ross-stevens-american-olympians-donation/</link><author>bookofjoe</author><category>hn</category><pubDate>Wed, 28 Jan 2026 23:55:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Please Don&apos;t Say Mean Things about the AI I Just Invested a Billion Dollars In</title><link>https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in</link><author>randycupertino</author><category>hn</category><pubDate>Wed, 28 Jan 2026 23:36:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[â€”Guys, enough is enough. Bullying is a serious issue, and itâ€™s time for me to speak out. Thereâ€™s an extremely hurtful narrative going around that my product, a revolutionary new technology that exists to scam the elderly and make you distrust anything you see online, is harmful to society. This slander is totally unwarranted, and I would really appreciate it if everyone would stop being so mean about this thing I just invested a billion dollars in.As someone who desperately needs this technology to work out, I can honestly say it is the most essential tool ever created in all of human history. Donâ€™t mercilessly ridicule it just because it steals the joy out of your hobbies and creates sexually explicit images of women without their consent. Seriously, please stop! It really hurts my feelings.Itâ€™s easy to throw stones if you think about the job displacement and ecological destruction caused by this pointless technology. But such black-and-white, not-wanting-billionaires-to-get-richer thinking is, quite frankly, cruel. You canâ€™t just measure the value of something in terms of â€œwhether or not it makes everything worse for everyone.â€ The world is much more complicated than that.This technology is going to fuel innovation across industries and solve all problems of feminism and equal rights. Yes, itâ€™s expanding the surveillance state, and yes, itâ€™s destroying the education system, and yes, itâ€™s being trained on copyrighted work without permission, and yes, itâ€™s being used to create lethal autonomous weapons systems that can identify, target, and kill without human input, butâ€¦ I forget my point, but ultimately, I think you should embrace it.Lately, I feel like I just canâ€™t win with you guys. Please, just use my evil technology. Whatâ€™s so wrong with that? Just use it. Iâ€™m begging you. I want to continue living my immoral technofascist life without any criticism.]]></content:encoded></item><item><title>In 6 violent encounters, evidence contradicts immigration officials&apos; narratives</title><link>https://www.reuters.com/world/us/evidence-contradicts-trump-immigration-officials-accounts-violent-encounters-2026-01-27/</link><author>petethomas</author><category>hn</category><pubDate>Wed, 28 Jan 2026 23:25:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The UK paid Â£4.1M for a bookmarks site</title><link>https://mahadk.com/posts/ai-skills-hub</link><author>JustSkyfall</author><category>hn</category><pubDate>Wed, 28 Jan 2026 23:16:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The UK Government recently unveiled its â€˜AI Skills Hubâ€™, which wants to provide 10 million workers with AI skills by 2030. The main site was delivered by PwC for the low, low price of.. Â£4.1 million (~$5,657,000).It is  good. Like, at all - the UI is insanely bad and itâ€™s clear that this was just a vibecoded site (to be fair, this is the  Skills Hub, but câ€™mon, where is the pride in your work? I would be ashamed to even release this as a prototype!)PwC didnâ€™t even write any of the course content! The only thing the Skills Hub does is link out to external pages, like Salesforceâ€™s free Trailhead learning platform:Note that Iâ€™m fairly certain this course already existed before the contract was even awarded, so all the site does is.. link out to other sites?PwC itself also admits that the site does not properly meet accessibility standards:Even for those without a disability, the lack of here in this regard means that the site can be very confusing and buggy as a result.The site has a course on â€œAI and intellectual propertyâ€. One thing it mentions is fair use:Except that fair use is not a thing in the UK - thatâ€™s a US concept! The UK uses whatâ€™s known as â€œfair dealingâ€, which is more restrictive than fair use, so the details here are plain wrong.Lack of care, lack of craftThe interface for this website has also not been clearly thought out - one glaring example is the process of actually enrolling in a course.On the course page, the â€œEnroll Nowâ€ button is , and if you donâ€™t see it and try scrolling down to the bottom, you will find yourself nothing but a comment section!Then you have other bugs too, like the â€œSkills & Training Gap Analysisâ€ - which is linked at the top of the site! - apparently being closed off to the public for no reason:To be honest, seeing this made me angry.Iâ€™m angry at the sheer  of the UK Government here. Our public services are collapsing - while Â£4 million is admittedly chump change for the UK government, there are real people behind these numbers - families waiting months for NHS appointments, children in crumbling schools, vulnerable people not getting the care they need. The waste feels particularly galling when you realise that almost no one will actually use this site!Iâ€™m also angry that the small webdev businesses we have here in the UK were left out of this - for less than 5% of the cost, weâ€™d have a  website and help out small businesses who actually care about their work, instead of handing the project to a multinational company that made nearly $60 billion in revenue in a year and has zero qualms about ripping off the British taxpayer.]]></content:encoded></item><item><title>Somebody used spoofed ADSB signals to raster the meme of JD Vance</title><link>https://alecmuffett.com/article/143548</link><author>wubin</author><category>hn</category><pubDate>Wed, 28 Jan 2026 21:50:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This, if it is still visible: Next up, age verification for ADSB? ]]></content:encoded></item><item><title>Jellyfin LLM/&quot;AI&quot; Development Policy</title><link>https://jellyfin.org/docs/general/contributing/llm-policies/</link><author>mmoogle</author><category>hn</category><pubDate>Wed, 28 Jan 2026 21:42:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The rise of LLMs as a useful development tool over the last year or so has been significant. The power and flexibility of tools like Claude Code and ChatGPT have given a lot of functionality both to experienced developers and new developers alike. But there are trade-offs.The Jellyfin project has, from day one, had a major focus on code quality - readability, simplicity, conciseness. This is a mostly manual effort driven by a dedicated team of individuals, and is motivated by a desire to fix the code Jellyfin is based off of which, without beating a dead horse too much, was extremely fragile, spaghettified, and prone to over-engineered complexity.We are seeing a precipitous rise in contributors using AI within the Jellyfin ecosystem, both in the server and for clients, as well as a rise in criticism and concern about LLMs generally. At this time we are writing this policy to address exactly what we expect and desire with respect to contributions and interactions within our community that may use LLMs. These rules apply to all of our official projects and community spaces.LLM output is  for any direct communication, including the following:feature requests or commentspull request bodies or commentsforum/chat/etc. posts or commentsIn short, if you are posting  of those things, the output must be your own words, explanation, description, etc., not a verbatim dump of an LLM's output. We expect you to understand what you're posting. Violating this rule will result in closure/deletion of the offending item(s).An exception will be made for LLM-assisted translations if you are having trouble accurately conveying your intent in English. Please explicitly note this ("I have translated this from MyLanguage with an LLM") and, if possible, post in your original language as well.LLM code contributions are subject to more granularity below, but the general principle is that "pure 'vibe coding' will be rejected" and "you are responsible for what you commit". We will review in that vein. If the , it will be .LLM Code Contributions to Official Projectsâ€‹The use of LLMs for code is controversial and open to much interpretation. These guidelines are our best effort attempt to ensure that knowledgeable developers who seek to use these tools as a legitimate aid are not overly-hindered, while also preventing an ongoing flood of slop contributions that violate our core ethos above. These apply to all official Jellyfin projects.Contributions should be . If the PR claims to target X, and is also touching unrelated Y and Z, it will be rejected. This includes incidental changes to unrelated functionality, a hallmark of poorly-worded or too-general prompts. Similarly, a large PR must be broken into multiple small, manageable commits for review and history purposes.Formatting and quality . Excessive unhelpful comments, spaghetti code, spaces on empty lines, etc. will be interpreted as pure LLM output and rejected; you must  before submitting. Also do not commit LLM metafiles (e.g.  configs) or any other editor-created non-code files.You must  and be able to  in the PR body -  LLM output as noted above - what is being changed and why. Your PR body (and, if applicable, commit bodies) should be providing context to other developers about why a change was made, and if your name is on it, we want  words and explanations, not an LLM's. If  what the LLM did, we are  in the change.The changes must be . The code should build and run correctly, or it will be rejected. You should also explicitly test the functionality being modified.You must be able and willing to  and implement the suggested change(s) as required. What this means in practice is, if you do not know what has been changed or why (see #3), and thus can't implement suggested changes or discuss them , then we are  in the change. Just dumping reviewer feedback into an LLM and expecting what comes out to be "good enough", is not. require an in-depth level of understanding about what is being changed and why. It is obvious to our reviewers when changes are made without the developer making them understanding what is happening. These will be rejected. And as noted in #1, the PR must contain multiple discrete commits.  will squash commits as deemed appropriate after review. Large changes must also follow our other development policies (discussion, review, implementation, testing process).The final discretion always lies with the reviewers. If your PR is not capable of being reasonably reviewed, for any reason (over-complexity, size, squashed commits, etc.) it will be rejected, and this goes just as much for non-LLM-assisted PRs as it does for LLM-assisted PRs. You will be asked to split such a PR up into multiple PRs that each present a focused, concise set of changes instead.The golden rule is this: do not just let an LLM loose on the codebase with a vague vibe prompt and then commit the results as-is. This is lazy development, will  result in a poor-quality contribution from our perspective, and we are not at all interested in such slop.  or please do not bother. And again, you are free to use LLMs to  you, but not as the sole source of code changes.You are of course free to do whatever you wish for your own non-official projects. However, we will be enforcing the following rules for any sharing of such projects within our communities.Any primarily-LLM-developed projects should be . It is up to users to decide if this is acceptable to them or not. If you used an LLM for secondary assistance (e.g. docs, formatting, etc.) in an obvious way, we would err towards disclosure as well.You  respect and follow licenses. If you are basing your project off of existing code, following its license is not optional. You must credit existing contributors in full for . Do not , and do not commit pending 3rd party changes as your own (i.e. by copying the code and then committing it). Doing so will result in, not just rejection, but a ban from our organization and community. We have a  for code theft and bad-faith attribution attempts.For members of the community,  LLM-generated tools, clients, etc. , and do not engage in anti-LLM "witch hunts". As mentioned above, this is  and it is your choice whether to "support" said tool/client/etc. or not.We, the moderators, are not going to play "LLM police" about 3rd party projects by nitpicking to try to "find LLM contributions" that otherwise follow our rules here; this is tedious and a waste of our time and effort. What this means in practice is that rule #1 is up to the author, and rule #3 must be interpreted in that vein. If you  a tool is LLM-generated and violates rule #1, then downvote/ignore it and move on.  we see blatant breaking of rule #1 we will enforce it, but again we will not be going through code line by line playing the "was this LLM generated?" game. Rule #2 will always be enforced regardless of LLM-ness or not.]]></content:encoded></item><item><title>Apple to Soon Take Up to 30% Cut from All Patreon Creators in iOS App</title><link>https://www.macrumors.com/2026/01/28/patreon-apple-tax/</link><author>pier25</author><category>hn</category><pubDate>Wed, 28 Jan 2026 20:59:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Apple has set a new deadline of November 1, 2026 for all Patreon creators to switch from Patreon's legacy billing system to the App Store's in-app purchase system in the Patreon app on the iPhone and iPad, as reported by .Patreon is a platform where creators such as YouTubers can receive payments from fans, which can be a valuable revenue stream alongside ads and sponsorships.Apple initially told Patreon that its creators must move to the App Store's in-app purchase system by November 2025, or else Patreon would risk removal from the App Store, but the deadline was pushed back. Apple considers payments from supporters to creators on Patreon to be digital goods that it is entitled to receive a commission on.Apple receives a 30% commission on in-app purchases and subscriptions, but this drops to 15% for a subscription that has been ongoing for more than a year.Patreon gives creators the option to either increase their prices in the iOS app only, or absorb the fee themselves, keeping prices the same across platforms.On the iPhone and iPad, Patreon users who wish to support a creator can sidestep the App Store's commission by completing their payment via Patreon's website.Patreon said it is disappointed with how Apple has navigated this policy.According to , only 4% of Patreon creators are still using the platform's legacy billing system, with the rest having already switched over.]]></content:encoded></item><item><title>Native Linux VST plugin directory</title><link>https://linuxmusic.rocks/</link><author>Aldipower</author><category>hn</category><pubDate>Wed, 28 Jan 2026 19:59:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: A MitM proxy to see what your LLM tools are sending</title><link>https://github.com/jmuncor/sherlock</link><author>jmuncor</author><category>hn</category><pubDate>Wed, 28 Jan 2026 18:52:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I built this out of curiosity about what Claude Code was actually sending to the API. Turns out, watching your tokens tick up in real-time is oddly satisfying.Sherlock sits between your LLM tools and the API, showing you every request with a live dashboard, and auto-saved copies of every prompt as markdown and json.]]></content:encoded></item><item><title>That&apos;s Not How Email Works, HSBC</title><link>https://danq.me/2026/01/28/hsbc-dont-understand-email/</link><author>HotGarbage</author><category>hn</category><pubDate>Wed, 28 Jan 2026 18:12:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
              I have a credit card with HSBC. It doesnâ€™t see much use,
              but I still get a monthly statement from them, and an email to say itâ€™s available.
            
              Not long ago I received a letter from them telling me that emails to me were being â€œreturned undeliveredâ€ and they needed me to update the email address on my account.
            
              I logged into my account, per the instructions in the letter, and discovered my correct email address already right there, much to myâ€¦ lack of surprise.
            
              So I kicked off a live chat via their app, with an agent called Ankitha. Over the course of a drawn-out hour-long conversation, they repeatedly told to tell me  to update my
              email address (which was never my question). Eventually, when they understood that my email address was already correct, then they concluded the call, saying (emphasis mine):
            
                I can understand your frustration, but if the bank has sent the letter, you will have to update the e-mail address.
              
              This is the point at which a normal person would probably just change the email address in their online banking to a â€œspareâ€ email address.
            
              So I called Customer Services directly,
              who told me that if my email address is already correct then I can ignore their letter.
            
              I suggested that perhaps their letter template might need updating so it doesnâ€™t say â€œaction requiredâ€ if action is  required. Or that perhaps what they mean to say is
              â€œaction required: check your email address is correctâ€.
            
              So anyway, apparently everythingâ€™s fineâ€¦ although I reserved final judgement until Iâ€™d seen that they were still sending me emails!
            
              I think I can place a solid guess about what went wrong here. But it makes me feel like weâ€™re living in the Darkest Timeline.
            
              I dissected HSBCâ€™s latest email to me: it was of the â€œyour latest statement is availableâ€ variety. Deep within the email, down at the bottom, is this code:
            <>

<>

              What youâ€™re seeing are two : tiny 1Ã—1 pixel images, usually transparent or white-on-white to make them even-more invisible, used to surreptitiously track when
              somebody reads an email. When you open an email from HSBC â€“ potentiallyÂ  time you open an email from them â€“ your email client connects to those web addresses to get
              the necessary images. The code at the end of each identifies the email they were contained within, which in turn can be linked back to the recipient.
            
              You know how invasive a read-receipt feels? Tracking pixels are like thoseâ€¦ but turned up to eleven. While a read-receipt only says â€œthe recipient read this emailâ€ (usually only after
              the recipient gives consent for it to do so), a tracking pixel can often track  andÂ  you refer to an email.
            
              If I re-read a year-old email from HSBC, theyâ€™re saying that they want to know about it.
            
              But it gets worse. Because HSBC are using , rather than  URLs for their tracking pixels, theyâ€™re also saying that every time you read an email
              from them, theyâ€™d like everybody on the same network as you to be able to know that you did so, too. If youâ€™re at my house, on my WiFi, and you open an email from HSBC, not
              only might HSBC know about it, but  might know about it too.
            
              An easily-avoidable security failure there, HSBCâ€¦ which isnâ€™t the kind of thing one hopes to hear about a bank!
            
              Butâ€¦ tracking pixels donâ€™t actually work. At least, they doesnâ€™t work . Like many privacy-conscious individuals, my devices are configured to block tracking pixels (and a
              variety of other instruments of surveillance capitalism) right out of the gate.
            
              This means that even though I  read most of the non-spam email that lands in my Inbox, the sender doesnâ€™t get to know that I did so unless I choose to tell them.
              This is the way that email was  to work, and is the only way that a sender can be confident that it  work.
            
              But weâ€™re in the Darkest Timeline. . So they wrote me a letter to tell me that my emails have been â€œreturned
              undeliveredâ€ (which seems to be an outright lie).
            
              Surveillance capitalism has become so ubiquitous that itâ€™s become transparent. Transparent like the invisible spies at the bottom of your bankâ€™s emails.
            
              So in summary, with only a little speculation:
            Surveillance capitalism became widespread enough that HSBC came to assume that tracking pixels have bulletproof reliability.
              HSBC started using tracking pixels them to check whether emails are being received (even though thatâ€™s not what they do when theyÂ  reliable, which
                theyâ€™re not).
                (Oh, and their tracking pixels are badly-implemented, if they worked theyâ€™d â€œleakâ€ data to other people on my network.)
                  Eventually, HSBC assumed their tracking was bulletproof. Because HSBC couldnâ€™t track how often, when, and where I was reading their emailsâ€¦ they posted me a letter to
                tell me I needed to change my email address.
              
              What do I think HSBC should do?
            
              Instead of sending me a misleading letter about undelivered emails, perhaps a better approach for HSBC could be:
            , stop using unencrypted connections for tracking pixels. I do not want to open a bank email on a cafeâ€™s public WiFi and have
                 potentially know who I bank withâ€¦ and that I just opened an email from them! I certainly donâ€™t want attackers injecting content into the bottom of
                legitimate emails.
              Stop assuming that if somebody blocks your attempts to spy on them via your emails, it means theyâ€™re not getting your emails. It doesnâ€™t mean that. Itâ€™s never meant
                that. There are all kinds of reasons that your tracking pixels might not work, and theyâ€™re not even all privacy-related reasons!
              Or, better yet: just stop trying to surveil your customersâ€™ email habits in the first place? You already sit on a wealth of personal and financial information which
                you can, and probably do, data-mine for your own benefit. Can you at least  to pay lip service to your own published principles on the
                ethical use of data and, if I may quote them, â€œuse only that data which is appropriate for the purposeâ€ and â€œembed privacy considerations into design and approval processesâ€.
              If you need to check that an email address is valid, doÂ , not an unreliable proxy for it. Instead of this letter, you could have sent an email that
                said â€œWe need to check that youâ€™re receiving our emails. Please click this link to confirm that you are.â€ This not only achieves informed consent for your tracking, but it can be
                more-secure too because you can authenticate the user during the process.
              
              Also, to quote your own principles once more: when you make a mistake like assuming your spying is a flawless way to detect the validity of email addresses, perhaps you should â€œbe
              transparent with our customers and other stakeholders about how we use their dataâ€.
            
              Wouldnâ€™t that be better than writing to a customer to say that their emails are being returned undelivered (when theyâ€™re not)â€¦ and then having your staff tell them that having received
              such an email they have no choice but to change the email address they use (which is then disputed by your  staff)?
            ]]></content:encoded></item><item><title>Computer History Museum Launches Digital Portal to Its Collection</title><link>https://computerhistory.org/press-releases/computer-history-museum-launches-digital-portal-to-its-vast-collection/</link><author>ChrisArchitect</author><category>hn</category><pubDate>Wed, 28 Jan 2026 17:54:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[OpenCHM is funded by the Gordon and Betty Moore Foundation and other generous donors, and this launch represents a major milestone in CHM's multi-year digitization initiative. Designed in collaboration with KeepThinking, the portal is powered by their innovative Qi collection management system.]]></content:encoded></item><item><title>Mousefood â€“ Build embedded terminal UIs for microcontrollers</title><link>https://github.com/ratatui/mousefood</link><author>orhunp_</author><category>hn</category><pubDate>Wed, 28 Jan 2026 17:20:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Spinning around: Please don&apos;t â€“ Common problems with spin locks</title><link>https://www.siliceum.com/en/blog/post/spinning-around/</link><author>bdash</author><category>hn</category><pubDate>Wed, 28 Jan 2026 16:48:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is the 3 project in less than a year where Iâ€™ve seen issues with spin-loops. Iâ€™ve been dealing with spinning threads for many years now, and I wonâ€™t lie: over the years Iâ€™ve been both on the offender and victim side.
Iâ€™m getting tired of seeing the same issues again and again, which usually makes for a good reason to write a blog post so that, hopefully, people will read it and stop making the same mistakes others did.Actually, many others have written about this, covering various issues related to spin locks . But I guess thereâ€™s never enough material on those subjects. Some are about speed, others about fairness, a few about priority inversion, NUMA, and sometimes even about actually broken code.
If this list hasnâ€™t convinced you that things do spin out of control when using spin-locks, and that you should use OS primitives instead, keep reading. Iâ€™ll cover what you should not do when implementing your own spin-lock. Notice I said what you should  do, because, , you should  not use a spin-lock at all these days.
And if you doâ€¦ make sure you really, REALLY,  know what youâ€™re doing (spoiler: it will always come back to bite you when you least expect it).Note this is a story about spin loops in general, not about locking algorithms for which there are many .Letâ€™s start with the basics, you want to implement your own spinlock.ðŸ¤ª â€œItâ€™s easy! You simply have a boolean, a  and an  function.â€For demonstration purposes, we are using  instead of  as you might have something more complicated to do with it, such as storing metadata (for example: the thread ID). There are also quite a few pieces of code around that do not implement a spin-lock per se, but mutate some other content such as pointers.Those who have dealt with multi-threading before will immediately spot the issue. The code is not thread-safe as, if multiple threads attempt to use this lock, we could read invalid values of  (in theory, and on a CPU where tearing could happen on its word size).
Worse, even if this could not happen, a wild race-condition could appear.
Consider the following example where two threads would call  at the exact same time:Now we have two threads who think they have successfully acquired the lock!Some may also have heard about this shiny little thing called  variables/operations.
To oversimplify: atomic operations guarantee that other threads cannot observe a partial/intermediate state of the operation and thus race-conditions can not occur (on those specific operations and memory).ðŸ’¡ While named after the Greek  that means â€œthat which cannot be dividedâ€,  operations might as well be as dangerous and difficult to use as nuclear energy.Letâ€™s replace  by an atomic version: . Though our code does not suffer from a race-condition on the data itself, we still do not know if the thread that sets  to  is the one that now owns the lock. But we can now do an  operation atomically, which solves our little problem!Instead of first checking if the lock is locked, then writing, we actually write our value and get the previous value, in a single atomic operation! If the previous value was , then it means weâ€™re the one who actually did the locking. Otherwise we will see a , meaning the lock was already held either before we tried, or because another threadâ€™s exchange completed before ours.Letâ€™s replay the scenario. Even if both threads execute the exchange simultaneously, atomicity guarantees one will finish before the other, for example Thread â€™s:Good, we now have a working spin-lock, but we still have a long way to go.ðŸ’¡ In the CPU lingua, a memory / is called a memory /You may have realized that our spin-lock willâ€¦ spin doing nothing, the loop is empty.ðŸ¤ª â€œGreat, itâ€™ll attempt to take ownership fasterâ€Well, thatâ€™s only true if you want to burn your CPU. Since the CPU has no way of knowing that you are waiting and not doing any meaningful work, it might stay at a high frequency.
Modern CPUs can change the frequency of the cores to save energy, and effectively also lower the CPU core temperature. This is clearly not desirable behavior, especially on mobile/embedded devices.Not convinced or do not care about the planet? (shame on you!) Then at least think about your usersâ€™ power bill. Still not convinced? What if I told you this can actually be slower than doing something in the loop?
Imagine that a lot of threads are attempting to lock your spin-lock. Only one can win. But worse, due to its nature you always do memory writes, which need to be synchronized between the different cores of your CPU!From Intelâ€™s Optimization Reference Manual :On a modern microprocessor with a superscalar speculative execution engine, a loop like this results in the issue of
multiple simultaneous read requests from the spinning thread. These requests usually execute out-of-order with each
read request being allocated a buffer resource. On detection of a write by a worker thread to a load that is in progress,
the processor  no violations of memory order occur. The necessity of maintaining the order of
outstanding memory operations inevitably costs the processor a severe penalty that impacts all threads.And the issue will keep getting bigger with recent CPUs that have many cores and sometimes NUMA memory.This penalty occurs on the Intel Core Solo and Intel Core Duo processors. However, the penalty on these
processors is small compared with penalties suffered on the Intel Xeon processors. There the performance penalty for
exiting the loop is about .If you still need some convincingâ€¦ this is even worse if you enable SMT (hyperthreading):On a processor supporting Intel HT Technology, spin-wait loops can consume a significant portion of the execution
bandwidth of the processor. One logical processor executing a spin-wait loop can severely impact the performance of
the other logical processor.Now that I hopefully have your attention, hereâ€™s how to  mitigate the issue:
The best way to avoid â€œbotheringâ€ your neighbours is to  tell the CPU you are waiting to be notified of a memory change/doing a spinloop!
On x86 CPUs, this is done with the  instruction. It was designed exactly for this use-case!The penalty of exiting from a spin-wait loop can be avoided by inserting a  instruction in the loop. In spite of
the name, the  instruction  by introducing a slight delay in the loop and effectively
causing the memory read requests to be issued at a rate that allows immediate detection of any store to the
synchronization variable. This prevents the occurrence of a long delay due to memory order violation.You can modify the code to use this instruction with compiler intrinsics:As already mentioned, the penalty of synchronizing data between CPU cores is getting more expensive as new CPUs get more cores, get multiple core complexes or NUMA architectures.
Resolving conflicts (multiple cores trying to do atomic stores) thus needs to be mitigated in some way.
A traditional approach is to use a  strategy that increases the number of  instructions for each attempt at locking.The one you will find most (recommended by the Intel Optimization Manual, 2.7.4), is the exponential backoff:The number of  instructions are increased by a factor of 2 until some  is reached which is subject
to tuning.We also mix it with a bit of randomness by using , and letâ€™s refactor the yielding part into a structure that can be easily swapped:Remember the comment above about  being subject to tuning?
Well youâ€™d better make sure to tune it for the exact CPU youâ€™ll be working on.
Letâ€™s have a look at the following table listing the measured duration of  in cycles:And thatâ€™s where the issue lies. Depending on the architecture, you may get more than 10x changes in cycles per .
Old CPUs tended to have small  duration of  cycles on Intel,  on AMD, where  architectures have a duration of  cycles on Intel, and  cycles on AMD.
And this might get worse in the future!This actually is also now part of the latest Intel Optimization Reference Manual  2.7.4:The latency of the  instruction in prior generation microarchitectures is about 10 cycles, whereas in Skylake Client microarchitecture it has been extended to as many as 140 cycles.How to fix this, you ask? Iâ€™ll defer to Intelâ€™s advice again and limit the duration of the  loop using CPU cycles instead of a counter:As the  latency has been increased significantly, workloads that are sensitive to  latency will suffer some
performance loss.
[â€¦]
Notice that in the Skylake Client microarchitecture the  instruction counts at the machineâ€™s guaranteed P1
frequency independently of the current processor clock (see the INVARIANT TSC property), and therefore, when
running in IntelÂ® Turbo-Boost-enabled mode, the delay will remain constant, but the number of instructions that
could have been executed will change.This method has two main advantages:We define the max duration of a  loop in terms of  cycles, which is (on most modern CPUs) independent of the actual frequency of the core or duration of .If the operating system happens to preempt our thread in the middle of the loop, it will stop yielding after being rescheduled if maximum duration has been exceeded. Otherwise we could call  more than necessary on a thread wakeup.Youâ€™ll notice that we kept the exponential backoff as a plain counter. This is to avoid having to compute the duration of a single  (this would require getting rid of the jitter).
However, we still need to choose a value for . This again is purely empirical and needs tuning, but one may assume the duration of a context switch is about 3Âµs. Depending on the system and actual switch this can be more or be less. But it should be in the same order of magnitude.
We can then estimate the TSC cycles/Âµs conversion to be ~3200cycles/Âµs  for a 3.2Ghz clock. Another common frequency for the TSC is 2.5GHz.
While obviously incorrect, this is a good guesstimate for a default value on PC. At worst, youâ€™ll most likely get a 2x difference with the real value, which is way better than the x10 you could get with the varying  durations!I did however mention this is a default value, and the best thing to do is to retrieve the real value, either from the OS or by measuring it. Sadly TSC calibration is not officially exposed by Linux/Windows, so the best way is to measure the TSC against the system high resolution clock. Ideally this should be done asynchronously (donâ€™t do it on your application main thread at boot, please).ðŸ’¡ Windows actually â€œexposesâ€ this value as  in the kernel shared data at offset . This is used internally by synchronization primitives to determine how many  instructions it should issue. However I wouldnâ€™t recommend using those internals unless your code sanitizes the value.We only briefly touched the topic of . All atomic operations actually take an optional parameter which is the memory order.
I donâ€™t want to spend too much time on this as entire talks are dedicated to it, and itâ€™s not an easy topic.However do know this: not providing the parameter is equivalent to using std::memory_order_seq_cst (sequentially consistent) which enforces the most restrictions. On some platforms this may even flush your cache via memory barriers!
Our previous example can actually be re-written using acquire/release semantics:On my x64 machine and exponential backoff:A spin-lock should be fast, otherwise you would just use your average system lock.
While we mitigated the inter-core synchronization with the jitter and exponential backoff, there are ways to reduce the cache coherency  traffic under contention.
This has been mentioned by many in the past  but it doesnâ€™t hurt to remind it again. Instead of looping over a  (aka ) operation, prefer using both  and  operations!
It also applies to our  (aka ) operation.Priority inversion is one of the worst things that could (and will) happen with a spinlock. And it impacts most severely the platforms that need them the most! (Embedded, real-time OSes, â€¦)
Letâ€™s have a look at the issue:A  acquires your spinlockA  tries to acquire the lock and starts spinningThe OS scheduler preempts the low-priority thread to run another thread with medium/high priority (anything higher than â€œlowâ€)There are no cores left to run the  as they are all used by higher priority threads.The high-priority thread burns CPU cycles spinning forever.ðŸ¤ª â€œLetâ€™s use std::this_thread::yield()?â€Meh, did you test it on multiple systems? Iâ€™ll play along and give it a try.Now when we reach the maximum number of iterations, we make the thread yield its quantum to the operating system ( on Windows,  on Linux) so that another thread may be scheduled.
While in practice this may, sometimes, solve the issue as the OS is now free to schedule other threads including the , this is not mandatory!
Some implementations may end up just rescheduling the thread that just yielded since itâ€™s of higher priority.You may have also seen implementations that use  on Windows. This is better than  (which can only yield to a thread ready to run on the current core, per the docs. Same for normal Linux schedulers). However this used to only yield to threads of  priorities, and  on the real-time version of the OS! For example on an embedded device, or a console.
The only way to schedule any thread on real-time kernels, be it Windows or Linux, is to sleep for a non-zero durationâ€¦ which we obviously would like to avoid!So the solution that the DotNet runtime team came up with is to start with , then  then !So we dealt with the priority inversion at the cost of potential sleeps.Please god noâ€¦ Yes, you () avoid the worst case scenario (), but really, is it fine?Letâ€™s stop for a second here and assume we never did more than yield.As you may have already guessed, a livelock is only half the story (this is starting to be a recurring pattern, isnâ€™t it?).
The fact is, the issue could happen even if all your threads have the same priority! (Yes, I saw you coming asking for an easy fix by removing priorities.)
Consider the following scenario:4 high priority threads: , , ,  ()4 other high priority threads: , , ,  (controlled by a 3rd party, those suck. Please library writers, donâ€™t spawn threads on your own, thank you!).Threads , ,  spin, trying to acquire it.At this point, we have the following:Thread  gets scheduled ( somehow released its quantum, still holds the lock)Thread  yields,  is scheduledThread  yields,  is scheduled again,  and  yield to  and I could continue this for a long time. Even though thread  might get scheduled again, it might not! This depends on your schedulerâ€™s internals. Especially since yielding may yield only to the ready threads of the current core. At the time of writing this article, this actually is a known issue with Address Sanitizer!Oh, and even if it did get scheduled, you probably lost a lot of time switching from one thread to the other, this is your typical lock convoy and is what Linus Torvalds more or less hints here:And no, adding random â€œâ€ calls while youâ€™re spinning on the spinlock will not really help. It will easily result in scheduling storms while people are yielding to all the wrong processes.So no, simply using the same priority for all threads or sleeping is not fine. Letâ€™s see what we can do about it.The real problem, when you spin in a loop, is that you expect things to go fast so that your thread may continue.
But by yielding this way you defeat a lot of the kernel heuristics. It has no way to know what you actually meant, and may schedule anything (or nothing) but threads from your process. Worse, it may degrade your thread priority, move it to lower frequency cores, and you lose any kind of priority boost when waking up due to the lock being releasedâ€¦
Thatâ€™s clearly not what we want. If only there was a way to communicate our intent to the OSâ€¦Well thatâ€™s exactly what Linux did when introducing the futex API! Since weâ€™re waiting in a loop for a value to change, just notify the OS about it and let it handle things from there.
Windows also implements this with the  API, which weâ€™ll be demonstrating here:Windowsâ€™  internally does a single iteration before issuing the system call, but Linuxâ€™s futex API is a direct syscall. Thatâ€™s why we call  only after spinning a bit.
This lets us have a similar spinning strategy on all platforms, which ensures a more consistent behavior.ðŸ’¡ You may notice that we always end up calling  even if thereâ€™s no other thread waiting. While not that slow on Windows, this is slow on Linux since it will do a syscall. To avoid that one would usually store some state such as the number of waiting (parked) threads.ðŸ¤ª â€œWait! Wasnâ€™t  added to the standard recently?â€Yes! And this is what one should have used if implementers did the right thing from the get-go (and more importantly did the same thing for each implementation), but this was not the caseâ€¦ (clang) used to do exponential backoff with  before . At least it got fixed in January 2025 but it still does exponential backoff.MSVC STL does the right thingâ„¢  and goes almost straight to the OS since the first implementation. Good job!So if you use it, you may get a built-in exponential backoff, or not. Both implementations actually make sense from an implementerâ€™s point of view (Do you expect  users to use it with their own backoff strategies? Or directly as condition variables?), but this difference ends up being problematic since the code behaves differently between implementations.
In the end, as usual with the  library, youâ€™re better off using the OS primitives directly if you want portable behaviour that you control.As mentioned, Windowsâ€™  will do a single spin before doing a syscall. The duration of  is computed on process start by the loader in  and stored in ntdll.dll!RtlpWaitOnAddressSpinCycleCount.An issue with some lock algorithms is that they may be unfair: this is what happens when under contention a thread may never actually grab the ownership of the lock if other threads are faster.
This time Iâ€™ll simply give a warning and ask you to trust me as this article is starting to be lengthy. You may have encountered some â€œticketâ€ locks that attempt to enhance the fairness of the lock. While it may look good on paper, itâ€™s actually not so good in practice.Not only is it slower due to its complexity, but as mentioned before only the OS really knows whatâ€™s good for scheduling. And if you want to use a -like API you end up having to wake up all potential waiters instead of just the one you want. So please, rely on the OS primitives for fairness instead. (Even if we didnâ€™t have those primitives, a random+exponential backoff may perform better than a ticket lock anyway!)Here comes another tidbit of CPU architecture: even if you write to different variables, they may share the same cacheline!
And this is really bad for performance when you do atomic operations on the same cacheline, even if the addresses are different.
To fix this issue, you may enforce alignment of your variables or use padding in a . False sharing is also known as destructive interference, which led to the standardâ€™s std::hardware_destructive_interference_size value!This is however not a silver bullet!
While you will avoid false sharing, you may also fill your TLB and L1 cache faster which may lead to more cache thrashing.You may even encounter cache bank conflicts. Cache bank conflicts only exist on some CPUs, but donâ€™t trust manufacturers to avoid them. From 3.6.1.3 of the Intel Optimization Reference Manual:â€œIn the Sandy Bridge microarchitecture, the internal organization of the L1D cache may manifest [â€¦]â€â€œThe L1D cache bank conflict issue does not apply to Haswell microarchitecture.â€â€œIn the Golden Cove microarchitecture, bank conflicts often happen when multiple loads access [â€¦]â€ðŸ’¡ So this was once an issue, then fixed, then it came back in another form.These are thankfully  thanks to the random+exponential backoff, but are getting worse (this pattern of â€œyes, butâ€ should really annoy you by now, thatâ€™s the whole point of this article).Whenever possible, avoid reading the same memory location within a tight loop or using
multiple load operations.And the only way to really fix that is toâ€¦ actually park the thread by calling an OS primitive such as a futex! You should also avoid doing multiple loads per loop, as recommended previously.ðŸ¤ª â€œIâ€™ve read about  and .â€And you should probably have read further as those are privileged instructions! But yes they do have the same look as a futex wait/wake, which is very tempting.
And, to be fair, AMD does offer a userland alternative which is  and  that we can use!One advantage of  is that you can tell the CPU to wait for a given TSC count instead of having to loop! So it can be used to replace the  loop when supported, and thatâ€™s actually what Windowsâ€™ locking primitives such as  or  do internally!
Not only is the â€œAPIâ€ easier (you provide a timestamp for the wakeup date) but it can save power! 
Just do not use it for  periods since you are still delaying potential work from other threads by not explicitly yielding to the OS.ðŸ’¡  can spuriously wake up, but this is fine for our usage since weâ€™ll just spin and try again!Youâ€™ll notice I barely mentioned ARM, thatâ€™s because I do not have enough experience with this architecture to give any advice other than you should use the proper memory ordering for decent performance.If you read this far, Iâ€™ll say it again: in most (and pretty much all) cases you should not even need to worry about the performance of your locks. The best lock is the one you donâ€™t use.Because you should never ever think that youâ€™re clever enough to write your own locking routines.. Because the likelihood is that you arenâ€™t (and by that â€œyouâ€ I very much include myself - weâ€™ve tweaked all the in-kernel locking over decades, and gone through the simple test-and-set to ticket locks to cacheline-efficient queuing locks, and even people who know what they are doing tend to get it wrong several times).Thereâ€™s a reason why you can find decades of academic papers on locking. But if you do, even after all those warnings, at least make sure you follow best practices and especially the pre-requisites for a spinlock to be efficient:The critical section (work done under the lock) is very small. (Consider that â€œsmallâ€ varies with the number of threads competing for the lockâ€¦)Notify your OS about what youâ€™re doing (, , â€¦)List of projects/libraries that do () it wrong and that I happened to stumble upon:Performance & Optimization Expert View profile ]]></content:encoded></item><item><title>Oban, the job processing framework from Elixir, has come to Python</title><link>https://www.dimamik.com/posts/oban_py/</link><author>dimamik</author><category>hn</category><pubDate>Wed, 28 Jan 2026 16:32:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Iâ€™ve used Oban in Elixir for almost as long as Iâ€™ve been writing software in Elixir, and it has always been an essential tool for processing jobs. I always knew Oban was cool, but I never dug deeper. This article is a collection of my notes and observations on how the Python implementation of Oban works and what Iâ€™ve learned while exploring its codebase. Iâ€™ll also try to compare it with the Elixir version and talk about concurrency in general.Oban allows you to insert and process jobs using only your database. You can insert the job to send a confirmation email in the same database transaction where you create the user. If one thing fails, everything is rolled back.OSS Oban has a few limitations, which are automatically lifted in the Pro version:Single-threaded asyncio execution - concurrent but not truly parallel, so CPU-bound jobs block the event loop. - each job is inserted individually. - each job completion is persisted individually. - jobs that are long-running might get rescued even if the producer is still alive. Pro version uses smarter heartbeats to track producer liveness.OSS Oban-py is a great start for your hobby project, or if youâ€™d want to evaluate Oban philosophy itself, but for any bigger scale - Iâ€™d go with Oban Pro. The pricing seems very compelling, considering the amount of work put into making the above features work.I obviously canâ€™t walk you through the Pro version features, but letâ€™s start with the basics. How Oban Py works under the hood, from the job insertion until the job execution. Stay tuned.Going Deeper - Job Processing Path#Letâ€™s get straight to it. You insert your job:After the insertion, the job lands in the  database table with . Oban fires off a PostgreSQL  on the  channel:Every Oban node listening on that channel receives the notification.  on each node gets woken up, but each  only cares about queues itâ€™s actually running. Be aware that each node decides which queues it runs, so if the current node runs this queue, the producer is notified:That  call sets an , breaking  out of its wait loop, so it can dispatch the jobs to the workers:Before fetching the jobs,  persists all pre-existing job completions (acks) to the database to make sure queue limits are respected. Next, it fetches new jobs, transitioning their state to executing at the same time. A slightly more complex version of this SQL is used:And this is the first really cool part.Segue to . - Locks the selected rows so no other transaction can modify them until this transaction completes. This prevents two producers from grabbing the same job. - If a row is already locked by another transaction, skip it instead of waiting. This is crucial for concurrency.Why this matters for job queues:
Imagine two producer instances (A and B) trying to fetch jobs simultaneously:B  for job #1 to unlockB  job #1, takes job #2Slow, sequential processingFast, parallel processingBack in Python, we know that the jobs we just fetched should be processed immediately. When we fetched the job, we already transitioned its state and respected the queue demand.Each job gets dispatched as an : ensures that independent of success or failure, we can attach a callback to handle job completion. controls how exactly the job is run. For the non-pro Oban version, it just uses  to run the job in the event loop:For pro version, local asyncio dispatcher is automatically replaced with a pool of processes, so you donâ€™t need to do anything to have true parallelism across multiple cores.After the job is dispatched,  takes over. It resolves your worker class from the string name, runs it, and pattern-matches the result:When execution finishes, the result gets queued for acknowledgement:The completion callback notifies  to wake up again-fetch more jobs, and batch-ack the finished ones in a single query.Thatâ€™s the hot path: Insert â†’ Notify â†’ Fetch (with locking) â†’ Execute â†’ Ack. Five hops from your code to completion. What about the background processes? What about errors and retries? What about periodic jobs, cron, and all these other pieces? Stay tuned.The Undercurrents - Background Processes#Oban runs several background loops that keep the system healthy.In a cluster, you donâ€™t want every node pruning jobs or rescuing orphans. Oban elects a single leader:The leader refreshes twice as often to hold onto the role:When a node shuts down cleanly, it resigns and notifies the cluster:And thatâ€™s ! Leader election is delegated entirely to PostgreSQL. Oban uses  with a TTL-based lease - no Raft, no consensus protocol, no external coordination service. If the leader dies, its lease expires and the next node to run the election query takes over. Simple, effective, and zero additional infrastructure.Lifeline: Rescuing Orphaned Jobs#Workers crash. Containers get killed. When that happens, jobs can get stuck executing indefinitely.  process (leader-only) rescues them:Oban-py rescue mechanics are purely time-based - any job in  state longer than  (default: 5 minutes) gets moved back. Unlike the Oban Pro version, it doesnâ€™t check whether the producer that owns the job is still alive. This means legitimately long-running jobs could be rescued and executed a second time.The takeaway is that you should set  higher than your longest expected job duration, and design workers to be idempotent.The SQL itself is straightforward - jobs stuck executing get moved back to available or discarded if theyâ€™ve exhausted retries:The rescued counter in meta lets you track how often jobs needed saving.Pruner: Cleaning Up Old Jobs#Without pruning, your oban_jobs table grows forever.  (also leader-only) deletes terminal jobs older than max_age (default: 1 day):The LIMIT prevents long-running deletes from blocking other operations.Retry & Backoff Mechanics#When a job raises an exception,  decides its fate:Simple rule: under  - retry, otherwise - discard.The default backoff uses jittery-clamped exponential growth with randomness to prevent thundering herds:And thatâ€™s ! Backoff includes jitter to prevent thundering herds - without it, all failed jobs from the same batch would retry at the exact same moment, spiking load all over again.The formula: 15 + 2^attempt seconds, with up to 10% added jitter. Attempt 1 waits ~17s. Attempt 5 waits ~47s. Attempt 10 waits ~1039s (~17 minutes).The clamping handles jobs with high  - if you set , it scales the attempt number down proportionally so you donâ€™t wait years between retries.Workers can override this with custom backoff:PostgreSQL does the heavy lifting. for concurrent job fetching,  for real-time signaling,  for leader election - the database isnâ€™t just storage, itâ€™s the coordination layer. Thereâ€™s no Redis, no ZooKeeper, no external broker. One less thing to operate.Oban-py is concurrent, but not parallel. Async IO allows multiple jobs to be in-flight, but the event loop is single-threaded. For I/O-bound workloads, this is fine. For CPU-bound tasks, consider using the Pro version with a process pool.Leader election is simple and effective. No consensus protocol, no Raft - just an  with a TTL. The leader refreshes at 2x the normal rate to hold the lease. If it dies, the lease expires and another node takes over. Good enough for pruning and rescuing.The codebase is a pleasure to read. Clear naming, consistent patterns, and well-separated concerns - exploring it felt more like reading a well-written book than understanding a library.OSS gets you far, Pro fills the gaps. Bulk operations, smarter rescues, and true parallelism are all Pro-only - but for what you get, Pro license feels like a great deal.Overall, Oban.py is a clean and well-structured port. If youâ€™re coming from Elixir and miss Oban, or if youâ€™re in Python and want a database-backed job queue that doesnâ€™t require external infrastructure beyond PostgreSQL - itâ€™s worth looking at.]]></content:encoded></item><item><title>Amazon cuts 16k jobs</title><link>https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/</link><author>DGAP</author><category>hn</category><pubDate>Wed, 28 Jan 2026 15:39:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Airfoil (2024)</title><link>https://ciechanow.ski/airfoil/</link><author>brk</author><category>hn</category><pubDate>Wed, 28 Jan 2026 14:32:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The dream of soaring in the sky like a bird has captivated the human mind for ages. Although many failed, some eventually succeeded in achieving that goal. These days we take air transportation for granted, but the physics of flight can still be puzzling.In this article weâ€™ll investigate what makes airplanes fly by looking at the forces generated by the flow of air around the aircraftâ€™s wings. More specifically, weâ€™ll focus on the cross section of those wings to reveal the shape of an  â€“ you can see it presented in  below:Weâ€™ll find out how the shape and the  of the  helps airplanes remain airborne. Weâ€™ll also learn about the behavior and properties of air and other flowing matter. In the demonstration below, you can see a fluid flowing around a . Using the slider to change just one  of this substance, we can end up with vastly different effects on the liveliness of that flow:Over the course of this blog post weâ€™ll build some intuitions for why these different effects happen to airfoils and other objects placed in flowing air. Weâ€™ll start this journey by looking at some of the methods we can use to visualize the motion of the air.If youâ€™ve ever been outside in a grassy area on a windy fall day, you may have witnessed something similar to the little scene seen below. The slider lets you control the  to observe in detail how the  and the bending  are visibly affected by the wind sweeping through this area:We intuitively understand that itâ€™s the flowing air that pushes the vegetation around, but note that we only observe the effects that the wind has on other objects â€“ we canâ€™t see the motion of the air . I could show you a similarly windy scene without the  and , and I could try to convince you that there is something going on there, but that completely empty demonstration wouldnâ€™t be very gratifying.Since the airâ€™s transparency prevents us from tracking its movement directly, we have to come up with some other ways that can help us see its motion. Thankfully, the little outdoor scene already provides us with some ideas.Notice that as the wind hits a blade of grass, that blade naturally bends in the direction of the blowing gust, and the faster that gust, the stronger the bending. AÂ single blade indicates the direction and speed of the flow of air in that area.In the next demonstration weâ€™re looking at the same grassy field from above. When seen from this perspective, all the  form short lines that are locally aligned with the wind. The more leaned over a blade of grass is, the longer the line it forms. We can mimic this behavior with a collection of  placed all over the area, as seen on the right side:Each  represents the direction and the speed of the flow of air at that location â€“ the longer the , the faster the flow. In these windy conditions the flow varies from place to place and it also changes over time, which we can clearly see in the motion of the .Note that we have some flexibility in how the speed of wind corresponds to the length of an . I adjusted the lengths of the  to prevent them from visually overlapping, but I also made sure to maintain their  lengths â€“ if one  is twice as long as the other, then the flow at that location is also twice as fast.For visual clarity Iâ€™m also not packing the  as densely as the  are placed, but itâ€™s important to note that  point in the flow has its own velocity which contributes to the complete velocity  present in this area. If we wanted to, we could draw a  at any of the seemingly empty spots on the right side.The arrows are convenient, but the grassy scene also has another aid for visualizing flows. Many light objects like , flower petals, dust, or smoke are very easily influenced by the motion of the surrounding air. They quickly change their velocity to match the flow of the wind. We can replicate the behavior of these light objects with  that are pushed around by that flow. You can see them on the right side:These little  also show us the motion of the air. Each  represents an object so small and light that it instantly picks up the speed of the surrounding airflow. Weâ€™d have a hard time seeing these miniscule specks at their actual sizes, so Iâ€™m drawing the  as visible dots.In fact, the motion of each  is equivalent to the motion of the parcel of air right around it. If you slow down time, youâ€™ll be able to see how each  just moves in the direction of the  underneath it. I also made each  leave a little ghost trail behind it â€“ this lets us track the path the air, as represented by the , took on the way to its current position.Letâ€™s pause for a second to emphasize what the -like  and -like  represent â€“Â they both show the  of the flow of air, but in slightly different ways. An  is attached to its fixed point in space, so it represents the current direction and speed of the flow at . The whole collection of arrows lets us easily see what the entire flow is doing at the moment.On the other hand, the little  are actively following the flow, letting us see how the air is actually  through space, with the ghosty trails giving us some historical overview of where this parcel of air has come from.The two methods weâ€™ve seen so far are very versatile, but sometimes we donâ€™t care about the local direction of the flow, only its speed â€“ in the middle of this grassy field one might get cold from a fast blowing wind regardless of the direction from which that wind is coming. This brings us the third way of visualizing flow:In this method we show the  of the airflow using colors of varying brightness â€“ the faster the wind, the brighter the color. You can see the whole  in the scale below the plot.This method shows the speed of the flow at  locations giving us a more fine-grained insight into the motion of air at the cost of the directional information. To help with that Iâ€™ll sometimes overlay the regular arrows on top to let us know where the flow is going as well.You may have noticed that all these methods present a flat, two dimensional view of the flow. Itâ€™s based on the assumption that the wind in our little scene doesnâ€™t change with elevation, and that it also doesnâ€™t blow towards or away from the ground.In reality, the air velocity could vary in all three dimensions, and that air could also flow upwards or downwards. Thankfully, the air flows weâ€™ll consider in this article will be two dimensional and the simple flat drawings will suffice.Before we finish this section, let me bring up visualization of a simple airflow, but this time Iâ€™ll give you some control over its , which you can change using the second slider. The first one once more controls the :Donâ€™t be misled by the frozen arrows, the wind is actually blowing there. Remember that the arrows represent the local velocity of the flow of air, so while the velocity doesnâ€™t change, the  of each packet of air does. You can see those changes by tracking the  moving around with the flow. This demonstration represents a , which means that its properties donâ€™t change over time.So far weâ€™ve been exploring the notion of airflowâ€™s velocity on a more intuitive level, with a general understanding thatâ€™s itâ€™s â€œthe airâ€ moving around in some direction and at some speed. I illustrated that concept using simple , , and , but weâ€™re now ready to investigate the details hiding behind those straightforward graphical representations.To do that, we have to look at individual particles of air. Although I briefly discussed the particle nature of air before, this time around weâ€™re going to take a closer look at the motion of these molecules, and what it means for airflow as a whole.Letâ€™s take a look at the air particles in a small, marked out volume of space seen in the demonstration below â€“ you can drag the cube around to change the viewing angle. The slider controls the :Youâ€™re witnessing the motion of over twelve thousand air particles. It may seem like a lot, but this cube is , its sides are only 80 nanometers long. To put this in perspective using more familiar sizes, if that cubeâ€™s side measured just , it would contain around  particles.The particles are zipping around in random directions, constantly entering and leaving this region. However, despite all this motion what youâ€™re seeing here is a simulation of  air.To understand how all this movement ends up creating still conditions, we first have to look at the velocity of each particle â€“ Iâ€™ll visualize it with a small arrow in the direction of motion. To make things a easier to see, Iâ€™ll also  a few of the particles while fading out the rest of them:The length of an arrow is proportional to the speed of a particle, so when you freeze the time you should be able to see how some particles are slower and some are faster. This speed variation follows a certain distribution thatâ€™s related to temperature â€“ the warmer the air, the faster the motion of its particles.At room temperature the average speed of a particle in air is an astonishing , which is many times higher than even the most severe hurricanes. Given the size of the cube, this means that even at the fastest speed of simulation everything happens 11  times slower than in real life.If you paid close attention, you may have also noticed that sometimes the particles randomly change direction and speed of their motion â€“ this happens when molecules collide. Each particle experiences roughly ten billion collisions per second. Weâ€™ll get back to these interactions later on, but for now letâ€™s try to figure out how all this turmoil creates still air.Having just seen the small velocity arrows of individual particles, letâ€™s calculate the  velocity of a group of three , using the process shown below. We first take the velocity arrows  and place them head to toe, one after another. Then we connect the start of the  with the end of the  to create the  of all velocities. Finally, we divide, or scale down, the length of this  by the number of particles to get the :In the next demonstration weâ€™re repeating this whole procedure by tallying up all the particles inside the . You can change the  of that region with the second slider. The  in the middle shows the average velocity of particles in the box. To make that  visible, Iâ€™m making it much larger than the tiny arrows tied to particles:The counter in the bottom part of the demonstration tracks the current number of particles in the . That value fluctuates as the molecules enter and leave that region. While aggregating over a small number of particles creates a very noisy readout, it doesnâ€™t take  many particles to get a much steadier measure.Recall that the scale of the  is much larger than the scale of individual tiny arrows attached to each particle. Despite that increase in size, the  practically disappears when we average out a larger number of particles and we can clearly see that the average velocity of particles is more or less zero even in this extremely small volume.In still conditions, all these motions in different directions average out to nothing. As some particles enter the area from a random direction, the others also leave it in a random way. The  of air doesnâ€™t really go anywhere and the particles just meander in a random fashion.An imperfect, but convenient analogy is to imagine a swarm of bees flying in the air. While all the individual insects are actively roaming around at different speeds, the group as a whole may steadily stay in one place.All these experiments form the key to understanding what happens when wind sweeps through an area. In the demonstration below, weâ€™re once again watching a small volume of space, but this time you can control the  of the blowing wind:Notice the  speedometer in the bottom of the demonstration. This is not a mistake â€“Â even with hurricane-level wind speeds itâ€™s very hard to see any difference in the motion of the particles. Perhaps youâ€™ve managed to see the tiniest shifts in the small particle arrows as you drag the  around with time paused, but itâ€™s difficult to even perceive from which direction the wind is blowing.However, when we use the procedure of averaging the velocity of all the particles, we can reveal the motion of their group in the box of a given , at a specific  of the flow:Because the motion of each individual particle is so disordered, we have to look at many of them at once to discern any universal characteristics. And when we do just that, from all the chaos emerges order.Itâ€™s important to note that with this approach weâ€™re tracking the velocity of the flow within the same region of space outlined by the  â€“ the molecules keep entering and leaving this area as the flow moves and the  in the middle shows the average velocity of the airâ€™s particles in that .This is  what the grass-like arrows weâ€™ve played with in the previous section represent â€“ each one shows the average velocity of air particles in that local region of space. The  we just saw in the middle of the swarm in the  is equivalent to each of the  seen below:Naturally, the  needs to be large enough to avoid the jitteriness related to aggregation of too few particles, but at any scale that we could care about the noisy readout completely disappears.The  motion of particles is very different than the motion of each individual molecule. Even in very fast flows, many of the molecules move in the opposite direction than what the arrow indicates, but if we tally up all the particle motion, the air as a whole does make forward progress in the direction of velocity.Up to this point, weâ€™ve mostly looked at the flow of air by looking at wind and the way it moves through space, but what we consider a motion of air is relative. Letâ€™s see how, by merely changing the point of view, we can create a motion of air in otherwise windless conditions.Letâ€™s zoom away from the world of microscopic particles to look at the motion of larger bodies. In the demonstration below, you can see two different views of the same  driving in the left direction. In the top part, the camera stays firmly on the ground, but in the bottom part, the camera tracks the motion of the . If needed, you can restart the scene with the button in the bottom left corner or tweak the  with the slider:These two views show the exact same scene â€“ weâ€™re just changing what the camera is focusing on. As seen in the top part, from the perspective of the static camera, itâ€™s only the  that has some velocity in the left direction.On the other hand, from the perspective of the camera focused on the , the  doesnâ€™t move, but  does. The poles and road markings all move to the right with a speed equal to that of the . This shouldnâ€™t come as a surprise from daily experience in any form of transportation â€“ when youâ€™re sitting in a moving vehicle, static things in the surrounding environment seem to move towards and past you.The very same rules apply to any region of air â€“ Iâ€™ve outlined some of them with dashed boxes   up in the sky. For the observer on the ground that air is still, but from the  perspective, that air is moving.With that in mind, letâ€™s see the same scene, but this time Iâ€™ll add the familiar small arrows showing the airâ€™s velocity as â€œseenâ€ by the camera:From the point of view of the , as seen in the bottom view, the air is moving to the right, as if there was some wind blowing right at the . Youâ€™ve probably felt this many times by sticking your hand out the window â€“ it feels no different than if you were standing still on the ground with the wind hitting your fingers.In fact, there is absolutely no difference between â€œregularâ€ wind and wind experienced by the   or your hand sticking out the window â€“ both are simply a motion of air relative to some object. This means that we can use our arrows to represent any motion of air, as long as we note what that motion is relative to.You may have also noticed that the moving  affects the motion of air in its vicinity. Let me bring up the previous demonstration one more time:In the top view, we can see how the front of the  pushes the air forward, and how the air â€œbendsâ€ and speeds up around the shape of the  to roughly follow its shape, only to end up circling right behind the .The same effects are seen in the bottom view â€“ theyâ€™re just experienced differently. For example, the air right in front of the  slows down, while the air on top moves even faster than the rest of the undisturbed, distant air.Weâ€™ll soon explore  the air behaves this way when flowing around an object, but for now letâ€™s raise above the ground to see the motion of an  flying in the sky. Weâ€™ll use the familiar setup of a camera kept steady relative the ground, as seen in the top part, and a camera that follows the , seen in the bottom part:Before we continue, notice that itâ€™s getting a little hard to pay close attention to what happens to the moving objects in the ground-fixed camera view â€“ the bodies quickly leave the field of view of the demonstrations. For the rest of this article Iâ€™ll stick to the camera style seen in the bottom part of the demonstration â€“ this will let us directly track the interaction between the object and the air that flows around that object.From the point of view of the , it also experiences a flow of incoming air as seen by the air â€œboxesâ€ approaching the , which is very similar to the car example. Whatâ€™s completely different from the car example is the fact that the  somehow stays suspended in the air, despite gravity pulling it down towards the ground. This means that there must be some other force acting on it to prevent the plane from falling from the sky.Letâ€™s compare these two vehicles by looking at the basic forces affecting their motion, starting with the diagram of forces acting on the :The down-pulling  is counteracted by the  from the ground â€“ they act through the  tires to prevent the  from sinking. The air drag and other forms of resistance  the car back, but the  tires powered by the engine keep  the car forward.In my previous article I presented a more elaborate description of the interplay between forces and objects, but to briefly recap here, if forces acting on an object are balanced, then that object will maintain its current velocity.All forces on the  balanced and the  moves forward with constant speed, and it doesnâ€™t move at all in the up or down direction â€“ the  velocity is indeed constant.Letâ€™s draw a similar diagram of forces for the flying :We still have the  that pushes the  back, and the  propeller powered by the engine keeps   it forward. As a result the  moves forward with constant speed.We also have the down-pulling . This time, however, that  is not countered by the reaction forces from the ground, but instead itâ€™s balanced by , a  that pushes the  up. When  and  are equalized, the plane doesnâ€™t move up or down either.Airplanes create most of their lift with wings, which are carefully designed to generate that force. While length, area, and the overall geometry of the wings are very important, in this article weâ€™ll focus on the shape of the cross-section of a wing which I highlighted below in :This is an , the protagonist of this article. This  has a smooth, rounded front and a sharp trailing edge. Letâ€™s take a closer look at the flow of air around this  using the grass-like arrows that show the velocity of air at that location:These arrows paint an interesting picture, but in the demonstration below Iâ€™ve also added the little leaf-like  that track the motion of air parcels in the flow. IÂ steadily release a whole line of them from the left side, but you can also  anywhere in the flow to drop a  at that location. You can do this in any demonstration that has a little hand symbol in the bottom right corner:The  show that the flow splits ahead of the , then it gently changes direction to glide above and below the . Moreover, the  right in front of the  gradually slow down and lag behind their neighbors. The air somehow senses the presence of the body.It may be hard to see, but the top and bottom sections of this  arenâ€™t symmetric. This asymmetric design is very important, but right now it will needlessly complicate our discussion on how the flow around this shape arises.To simplify things a little, letâ€™s use a less complicated shape of a  â€“ you can see it in the demonstration below. I overlay the previous asymmetric shape with a  outline to show the difference between the two:The motion of air around this airfoil is very similar â€“ the flow changes its direction and speed when it passes around an object. Until now weâ€™ve simply been observing that the flow changes to adapt to the shape of the body, but itâ€™s finally time to understand  it happens. To explain that behavior we need to go back to the world of air particles to discuss the concept of pressure.As weâ€™ve discussed, even in the seemingly steady conditions the particles of air are zipping around at high speeds colliding with each other at an incredible rate. The surface of any object placed in the air will also experience these bounces.In the demonstration below, you can see air particles bombarding a . Every time a collision happens I briefly mark it with a  on the surface of that :To understand the implications of these collisions, letâ€™s first take a look at objects with more ordinary sizes. In the demonstration below,  are hitting a large  from the left and right side. By dragging the slider you can change the  of both streams of balls:When a  hits the , the collision imparts some force on it, causing the  to move. However, in this simulation the collisions from all the  on each side balance each other out, so the  doesnâ€™t make any consistent progress in either direction.In real air, the situation is similar, but at vastly different scales. The mass of each particle constituting air is absolutely miniscule, so the impact of an individual collision on any object of meaningful size is completely imperceptible.Moreover, each air particle hitting an object has a different speed, and it strikes the surface of that object at a different angle â€“ some hit the object straight on, but some barely graze it. Due to the enormous number of these collisions happening at every instant of time, all these variations average out, and even a small section of surface of any body experiences uniform bombardment.In aggregate, we say that the air exerts  on any object present in that air. The magnitude of this pressure depends on the intensity of these collisions across an area.Letâ€™s see how this pressure manifests on our . In the demonstration below, you can use the second slider to control the  present in this volume:The  you see on the sides of the cube symbolize the magnitude of pressure on these walls. As we  increase the  in this volume, the intensity of collisions, and thus the , also increases. Because the collisions happen at more or less the same rate on every side of the , the net balance of forces is also maintained and the  doesnâ€™t move, regardless of how big or small the overall  is.This is exactly what happens in the Earthâ€™s atmosphere â€“ everything is constantly squeezed by relatively high pressure caused by the barrage of countless air particles. That pressure is typically balanced either by an objectâ€™s material, which resists compression like a spring, or by the air itself that fills the insides of the object. When that inner air is removed, the seemingly innocuous atmospheric pressure reveals its might.The underlying particle nature also shows us that pressure is never negative. Without any particle collisions, we reach the lowest possible pressure of zero. Beyond that, any impacts on the surface of an object create some amount of positive pressure.In the demonstrations weâ€™ve seen so far, the  number of collisions on each wall was very important for keeping the objects steady. Unsurprisingly, more interesting things happen when this harmony isnâ€™t maintained. Letâ€™s first investigate this scenario using the . In the demonstration below, the slider controls if itâ€™s the  thatâ€™s shooting more balls:As you can see, if one of the sides has a higher number of collisions, the forces acting on the  are no longer balanced and the  starts to move.The very same situation happens in air, which you can witness in the simulation below. Notice that the volume in which the  exists has more particles on one side than the other. Observe what happens to  once you let  using the slider:The higher number of particle collisions on one side of the  creates higher  on that wall. The uneven forces end up pushing the  to the side. In this demonstration, the pressure re-balances after a while and the  stops moving.Intuitively, the air exerts an imbalanced net force on the  only when different parts of that  experience different pressure â€“ itâ€™s the  in pressure that creates an acting net force. When the  in pressure between any two points increases, the net force acting on the object also grows.Itâ€™s easy to see that a larger number of collisions on the left side of an object would start to exert a net force pushing that object to the right, but, perhaps surprisingly, the same rules apply to any chunk of air itself.In the demonstration below, I once again made one half of the test volume contain more particles than the other half. As you  the demonstration, observe the average velocity of molecules in the   section of air:The particles on the more occupied side can easily travel to the less crowded side, because there are fewer particles there to collide with and bounce back from. Additionally, each particle in the less populated section is more likely to hit a particle in the more populated section, which will typically cause that particle from the desolate side to bounce back where it came from.The particles end up, on average, traveling from the area of high pressure to the area of lower pressure. Even though we donâ€™t have any clean borders between different sections, we can still see the bulk of particles getting accelerated towards the less dense section.Once again, the initial pressure differences in the test volume dissipate after a while. On their own, these freely suspended pressure variations quickly disappear, but we will soon see how, with the aid of airflow, these areas of different pressure can be sustained indefinitely.In the examples weâ€™ve been playing with, the notion of increased pressure came from an increased number of collisions, which in turn came from an increased number of particles in the area. This shows that, all other things being equal, pressure is tied to the local density of the air, which was very easy to perceive in an increased concentration of molecules.However, the pressure can also grow due to increased average speed of the particles, which in turn comes with increased temperature. As particles get faster, each collision gets more impactful and it pushes on an object or other particles a bit harder, causing the overall pressure to also increase. In the demonstration below, we can simulate this with  hitting the  at the same rate, but with , which you can control with the slider:As we make the  on one side of the faster, their impacts also become stronger and the  starts moving to the right, even though the  of collisions per second is equal on both sides.The important point from these discussions is that air pressure exerts force on everything inside it, be it a solid object or any parcel of air. Itâ€™s a little unintuitive that the air itself both exerts the pressure and it also â€œfeelsâ€ the pressure, but itâ€™s all just a consequence of very rapid motions of particles and the collisions between them happening at an enormous rate.Recall that even in small volumes of air there are billions of billions of particles, and each particle experiences roughly ten billion collisions per second. What weâ€™ve simulated at a micro scale and in slow motion as countable, individual interactions, very quickly smooths out into a uniform and uninterrupted notion of force-exerting pressure.This fact lets us abandon the molecules and their collisions yet again. Itâ€™s not a big loss, since counting the number and intensity of collisions was never convenient in the first place, but we can now investigate some other ways of visualizing pressure in a region of air.As weâ€™ve seen in the particle simulations, pressure can vary from place to place. One of the most convenient ways to express this variation is to use colors of different intensities. Letâ€™s see how that simple approach could work here. In the demonstration below, the dashed circles represent regions of high  and low  pressure â€“ you can drag them around to change their position:This map of pressure is colored with  as indicated by the scale below â€“ the  the color, the  the pressure. The small triangle  in the middle of the scale indicates the location of the  base,  pressure present in the atmosphere.In this simulation we have complete control over where the different locations of   and  pressure are. To make things more interesting, each draggable pressure circle has a different strength and range. You can infer this variation from color changes around these points.Letâ€™s put an  in this area to see how itâ€™s affected by the pressure of the surrounding air. The  seen below symbolize the force that pressure exerts on the surface of the  at that location. Theyâ€™re the exact same  that weâ€™ve seen acting on the walls of the tiny yellow cube, here we just see them at a larger scale:As you move around the locations of   and  pressure, the  acting on the surface of the  also change, matching what weâ€™ve seen with little cubes bombarded by air particles. The static pressure always exerts some base load, but in the areas of  pressure the  are higher, and in the areas of  pressure the  are lower than these base forces.Note that you can also move the pressure circles into the , but it only serves as a convenience to let you customize the shape of the air pressure field  that body â€“ we donâ€™t particularly care about the pressure inside the solid itself.When we tally up all the  acting on each piece of the  surface, we end up with the  acting on that object. In the demonstration below, Iâ€™m showing it with the  at the center of the :By changing the distribution of pressure around the , we can affect the  that this object feels.The reddish plots weâ€™ve been looking at are correct, but a little inconvenient. Recall that  on the object depends only on the  of pressure â€“ when we uniformly increased the number of collisions on the walls of the tiny cube, it steadily remained in place.This means that the static background pressure doesnâ€™t matter for the  acting on an object. Itâ€™s only the differences relative to that static pressure that affect the overall balance. This lets us overhaul our visual representation of pressure â€“ we can use no color where the pressure has the static value, use  when the pressure is  than the static pressure, and use  when the pressure is  than the static pressure:This is the  same distribution of pressure that weâ€™ve just seen. All the pressure demos in this section are connected, and here we simply changed the reference point against which we present the pressure variation.If we then throw in the  back into the mix we can now also adjust the  representing the  that the pressure exerts on the surface of that :The areas of  pressure still seem to push on the surface of the , but the areas of  pressure now seem to  it. However, I need to emphasize once more that pressure  pushes on the object, and we can only talk about a pulling force when we discard that uniform, pushing contribution coming from the static pressure. In those â€œpullingâ€ areas the pressure is still pushing, it just pushes less intensely.I will also use the convenient terms of  and  pressure, but remember that this refers to their difference from the static pressure. The phrase â€œpressure lower than static pressureâ€ is a mouthful, so the expression â€œnegative pressureâ€ is very handy, even when it hides the fact that pressure is always positive.While the color variations used here show the true nature of the smoothly varying pressure changes, they make it a little hard to see how quickly those changes happen. To fix this, Iâ€™ll also draw the  that join the locations of the same pressure â€“ theyâ€™re very similar to lines of the same altitude you may have seen on maps:Every point on one of those contour lines has the same value of pressure, and each subsequent line is drawn at the same increment of pressure â€“ you can see this in the scale placed below the plot. This means that the closer the lines are together, the more quickly the pressure changes in that area.The mathematical concept that describes the direction and rapidness of these changes is known as a . Informally, gradient describes how some property changes from one point to another, and, thankfully, this notion tracks closely with how this word is used in graphic design to describe smooth color changes. Wherever you see a  , this also implies that there is a   â€“ the pressure changes from place to place.This spatial variation is particularly important for the motion of air. Recall that the air pressure differences donâ€™t just exert forces on solid objects, but also on the air itself â€“ any small parcel of air is subject to the same whims of pressure forces.Those spatial variations in pressure end up pushing the air around, changing its velocity. Letâ€™s see this in action using the little leaf-like  that are moved around by pressure differences. In the demonstration below, Iâ€™m steadily releasing the  from the left side â€“Â notice how their trajectory changes when you modify the pressure field:You may still find it a little difficult to grasp how pressure differences affect the motion of a parcel of air. Luckily, we can draw parallels between the contour lines of pressure seen on these pressure maps and the contour lines of elevation seen on traditional maps. This lets us build a little pressure-landscape analogy.In the demonstration below, the very same distribution of pressure is expressed as a mountainy landscape.  pressure lifts the ground above the base level and  pressure depresses it below the base level. A parcel of air moves like a marble that loses speed when climbing uphill and accelerates when rolling downhill. You can drag the demo around to change the viewing angle:Notice that when the pressure changes more rapidly and the contour lines are closer, the steepness of the corresponding hill or valley also increases, and so do the forces acting on a parcel of air. If the pressure is increasing by a large amount, it may even make the  go back. This landscape analogy also shows that the static pressure doesnâ€™t matter for the motion of air parcels, as any changes in static pressure would just lift all the areas by the same amount without changing their steepness.When watching these air parcels move around, you may have noticed that things were a little bit off. For example, itâ€™s possible for air parcels coming from different directions to arrive at the same location, and then continue to travel in different directions. You can see an example of that on the left side of the demonstration below, with the slider letting you :Recall that the markers always follow the local velocity of air, so the motion seen in the left part implies that the air at the location of the meetup of the   has two different velocities at the same time, which is not realistic.Itâ€™s worth pointing out that the situation seen on right side, where  merely intersects the  of the ,  be realistic, as long as weâ€™re dealing with an unsteady flow, where the  of the air at the crossing location has changed since the  was there. For steady conditions in which no changes occur over time, the scenario seen on the right is also not physically correct.Weâ€™ll look at some unsteady flows later in the article, but for now weâ€™re interested in steady conditions so the crossing paths of our  indicate implausible velocities. Even more dubious result happen when we simulate the motion of these  with an  present in the flow:For most distributions of pressure, the air markers will flow right through the . This is clearly wrong! The demonstrations weâ€™ve seen so far correctly represent what would happen to individual air parcels and bodies placed in these pressure fields, but those pressure fields themselves were completely made up and didnâ€™t correspond to any physical reality. Our mistake was that we completely ignored any interactions between the pressure of the air and the  of that air.The flow of air, the pressure of air, and the shape of the objects placed in that air are all tied together â€“ for a given incoming flow speed and the shape of the object, we canâ€™t just arbitrarily arrange the pressure field like we did in our artificial demonstrations. Instead, that pressure field will arise on its own.Letâ€™s see a  distribution of pressure around this airfoil and witness how it affects the motion of air parcels around it:The behavior of air parcels now matches our intuitive expectations â€“ the  donâ€™t go through the body, and in these steady conditions they also donâ€™t cross paths.Weâ€™re now one step closer to understanding how the flow of air takes its shape to move around an  â€“ itâ€™s the pressure differences that cause the flow to change its direction and speed.The pressure field weâ€™ve just seen clearly works â€“ regions of  and  pressure guide the air around the airfoil. However, itâ€™s still unclear how these areas emerged in the first place. Letâ€™s try to follow natureâ€™s path to see how this pressure distribution is created and sustained in a flow.Before we start building the correct pressure field from scratch, letâ€™s first establish two guiding principles that the flow around any object has to follow.Firstly, the air canâ€™t penetrate solid walls. A valid pressure field should either completely stop the flow at the surface of the object, or redirect that flow to make it travel in the direction perpendicular to the walls. This means that the markers that we track can never get inside the object.Secondly, we also have the restrictions on the relative motion of the markers. For now weâ€™ll only be interested in steady conditions, which means that the markers canâ€™t cross their paths â€“ we expect the ghostly historical trails to never intersect.Letâ€™s first focus on the pressure field in front of the airfoil. In the demonstration below, I created an  pressure field in that frontal region, you can control it using the slider:It should quickly become clear that to prevent the approaching air from getting into the object, the pressure in the frontal region has to be , so that it pushes the incoming air away.If that  in front is too low the air can still erroneously flow through the object. If that pressure is too high, the air parcels arriving at the airfoil will turn back and incorrectly cross paths with the incoming air. When the pressure is just right, the air parcels donâ€™t go through the wall, and, at least in front of the object, they also donâ€™t cross their paths.The faster the incoming flow, the higher the pushing force required to slow down and redirect the incoming air. In the demonstration below, you can also control the  of that incoming air using the second slider:The pressure needed to stop air at a given velocity is known as  and itâ€™s proportional to the  of that velocity â€“ twice as high speed requires four times larger pressure.  Naturally, when there is no flow, no pressure is required as the air no longer tries to flow through the object.In the previous two demonstrations, we manually adjusted the pressure to get the correct result, but in nature this process happens on its own â€“ itâ€™s the flow  that creates this region of increased pressure in front of the object.As the incoming parcels of air arrive at the surface of the airfoil, they canâ€™t continue going forward, but air parcels from further up ahead continuously want to keep flowing into this region. This compresses the air close to the object, which causes the pressure in front to increase, which then helps to slow down the incoming flow.This mechanism is self-balancing â€“ if the pressure is too low to push away the incoming air parcels, the air parcels will compact the existing air more, causing an increase in pressure. If the pressure is too high, it will easily push the incoming air away, which relieves the frontal area, causing the pressure to decrease. Any fluctuations quickly settle to an equilibrium that balances the pressure in the entire frontal region.Letâ€™s look at the distribution of the  frontal pressure once more:Notice that the  isnâ€™t limited to just the close vicinity of the airfoil, but it spreads out much further ahead to gradually reach the value of the static pressure, far away from the airfoil itself.All in all, we have a large area of increasing pressure that starts far away from the body and ends at its surface. Those pressure differences create a pressure â€œhillâ€ that not only gradually slows the incoming air down, but it also redirects that air to flow around the object.It seems that with our frontal pressure field weâ€™ve easily completed our goal of preventing the air from flowing through the walls of the body. However, our second guideline of non-crossing marker paths is still not fulfilled â€“ this condition is broken above and below the airfoil.Letâ€™s first try to rectify this manually. In the demonstration below, you can control the pressure in these two regions using the slider:While  values of pressure in those zones make the problem worse,  values get us much closer to the expected behavior â€“ in the top and bottom areas the  no longer veer off into different directions. However, that pressure canâ€™t be  low, otherwise it will pull the  back into the body.In real flow, these regions of  pressure arise on their own, but the explanation for this phenomenon is a little less straightforward than what Iâ€™ve described for the area of  pressure in the frontal region. We can get , albeit a bit hand-wavy,  understanding by observing what happens to the air  when those  regions are missing.In that scenario, the incoming air parcels no longer reach those areas above and below the airfoil, causing some local depletion of air that has since left those zones. This decreases the pressure in those regions, and that lower pressure attracts the surrounding air to flow into those less occupied spaces.If that lower pressure is  negative, more air will come in and the pressure will rise. If the pressure is not negative enough, those region will get depleted again. Once again, itâ€™s the flow itself that creates the balancing system â€“ without the flow no pressure differences would arise.As weâ€™ll see later on, in more extreme scenarios that negative pressure can alter the flow more dramatically, and the regions of â€œmissingâ€ air get filled through other means, but for now letâ€™s close things up by tweaking the pressure in the rear part of the airfoil:Some amount of  pressure in the rear prevents the air parcels from smashing into each other after leaving the airfoil. Intuitively, this pressure arises naturally from the flow, because as the air slides off from the ends of the top and bottom sides, it all arrives into the same region, creating some compression.If that compressive pressure in the rear is too low, more air will manage to get in, which will further increase the pressure. If that pressure is too high, it will push the incoming air away, which depletes the area and the pressure decreases. The system balances itself yet again.The quite informal description of these balances that Iâ€™ve presented can be formalized mathematically using the . These equations describe the motion of liquids and gasses, collectively known as , subject to various forces like gravity, or, most importantly for us, pressure.Navierâ€“Stokes equations are  to solve analytically, but a lot of insight about the behavior of fluids can be gained with computer simulations with various degrees of complexity.In this article, Iâ€™m also employing simulations to investigate the flow of air around objects. However, the computer models used here are quite simplified and they donâ€™t reflect the  richness of physics involved in the motion of air. These  demonstrations are intended to present the broad strokes of the delicate interaction between the air and the airfoil, but I would advise against relying on them when building an airworthy airplane.With all of these caveats in place, letâ€™s get back to the pressure distribution around a symmetric airfoil. Weâ€™re done recreating the nature-made pressure field, but there is one small aspect that we havenâ€™t yet accounted for.For our experiments, I kept the pressure steady in time so that we could focus on its general outlines. In practice, a pressure field imposed by a fast flow around any object will experience some degree of instability, which you can see in the demonstration below. You can once more drop the markers at any location to track the flow in the area:As the pressure builds up on one side, it redirects the flow, which changes the pressure again. The pressure ends up oscillating back and forth like a swing. The pressure distribution and the flow direction are once again at the mercy of their mutual balance, one affecting the other. Weâ€™ll soon see some other examples of these unstable behaviors.As weâ€™ve just seen, the variation in pressure doesnâ€™t just happen in the close vicinity of the airfoil, but it stretches quite far away from the body itself. This means that the velocity of the flow is also affected quite far away from the shape.However, when it comes to the forces exerted on the airfoil, itâ€™s only the pressure right at the surface of the airfoil that matters. Letâ€™s bring back the two tools weâ€™ve used before:  that show how the air pushes or â€œpullsâ€ on the airfoil, and the  arrow that tallies up the net results of :As the pressure field fluctuates, the resulting  also moves around. Letâ€™s decompose this force into two different components, one   to the flow, and one  to it:The force acting in the direction perpendicular to the flow is known as , and the one acting in the direction of the flow is known as , or . As the name implies, this component of drag is created by the distribution of pressure around the shape.For this airfoil, the  is very tiny. While airfoils are specifically designed to minimize the overall drag, most of that force hindering their motion comes from another source â€“ weâ€™ll discuss it soon enough.Notice that as this flow fluctuates, the  force jumps around, but averaged over time the upward and downward swings of that  end up balancing each other. This airfoil in  configuration doesnâ€™t generate any continuous lift.This shouldnâ€™t come as a surprise since this situation is completely symmetric, so the pressure forces on the upper and lower sides of the airfoil are, on average, completely balanced. However, there is an easy way to disturb that symmetry. In the demonstration below, weâ€™re once again meeting the plain, symmetric airfoil, but this time we can gently  it using the slider:The  controls the so-called , which is spanned between some reference line on the body, like the one joining the front and back, and the direction of the incoming flow. Iâ€™m showing this angle right in the middle of the airfoil.As we change the , the shape that the airflow â€œseesâ€ is no longer symmetrical relative to the incoming direction of that flow. The velocity and pressure fields adapt in their mutual push and pull to form a new, asymmetric distribution. Notice that the stagnation point of  has moved around, and the little markers that indicate the motion of air now travel on very different paths below and above and below the airfoil.If we then put the  back in, we can tally them all up to get the resulting  and . When compared to the previous simulation, Iâ€™m scaling down all the arrows to make them fit in the bounds of the demonstration:When this symmetric airfoil is tilted up, the asymmetric pressure distribution generates a  that pushes the object up. Conversely, for a downward tilted airfoil, the pressure forces  the airfoil down.Naturally, weâ€™re typically interested in upward-pointing forces, and when the  generated by the wings is equal to the weight of the plane, the plane will stay in the air without raising or falling to the ground â€“ weâ€™re finally flying.Letâ€™s plot the dependence between the  and the  of an airfoil â€“ you can see it in the right side of the demonstration below. Note that this plot presents time-averaged and  values, so you may have to wait a little for the flow to normalize and the  to start oscillating around the expected value:Clearly, as the  increases, so does the generated . The same thing happens on the other end of the spectrum, where a more negative   creates more negative lift. Note that for this symmetric airfoil the positive and negative sides of the diagram are just mirror images of each other, so letâ€™s focus only on positive  .One could naively hope that we could keep increasing the   to generate more and more . Letâ€™s see what happens in practice:Initially, the  force indeed keeps increasing with the , but at some point it plateaus. Once that  angle of attack is surpassed, the  force starts to fall after the flow fully develops.What weâ€™re witnessing here is known as a . The onset of a stall imposes limits on how much  the wings of an airplane can generate from merely increasing the angle of attack.Notice that when the stall happens, the pressure distribution on the upper part of the airfoil becomes very erratic â€“ itâ€™s not only the surface  that are changing rapidly, but the whole pressure field in that area is very disturbed.Letâ€™s bring in the velocity arrows and markers to get a better feel on whatâ€™s going on in that region:At high angles of attack, the flow above the upper part of the airfoil becomes very complicated. If you  in that region to drop a few markers, youâ€™ll notice that the air is trapped in various swirling eddies that are eventually shed to fly away with rest of the flow.Weâ€™re witnessing , where the main part of the flow detaches from the surface and doesnâ€™t follow its shape anymore. The interactions in the complicated flow right above the airfoil affect the pressure field, which then .There is a lot going on there, but to understand how these effects arise we have to talk about a property that affects the flow of every fluid: viscosity.You might have heard the term  used to describe â€œthicknessâ€ of different liquids, with a classic example that contrasts the slowness of the flow of honey to the rapidness of the flow of water.Viscosity is also a property of gasses like air, but before I describe this concept more formally, weâ€™ll first build an intuitive understanding of what viscosity is and what it does to the flow of different fluids.In the demonstration below, the fluid flows in from the left side, but note that the flow in the top half is faster than the flow in the bottom half, which is reflected by the different lengths of the arrows. Dragging the slider to the left decreases the  of the fluid, and dragging the slider to the right increases :While we can see some changes to the arrows as we move the slider around, you probably agree that, for this flow, the arrow-based visualization isnâ€™t very rewarding. Letâ€™s add the color-based  distribution in this flow:We can now see how  blends the speed variation between different sections of the fluid. For highly viscous fluids, this mixing behavior spreads very easily and the initially distinct velocities of the two layers average out quite rapidly.At lower viscosity these two layers with different speeds remain quite separated. If you make the viscosity low enough, you may even notice that, after a while, the flow develops some interesting wave-like phenomena â€“ weâ€™ll get back to these soon.All this mixing behavior may remind you of a  process, where some quantity, like temperature or concentration, evens out over time. Letâ€™s see some basic diffusion in action. In the simulation below, I filled half of the bottle with with , while the other half is filled with . The slider lets you control the :As time passes, the sharp difference between the two layer blends more and more to eventually completely disappear. Clearly, there is some similarity between the diffusion of differently  and the averaging of velocity that weâ€™ve seen in the earlier example.In our flow demonstrations,  seemed to have controlled the diffusion of velocity. To define it more precisely,  controls the diffusion of , which is a product of velocity  mass. The simplified fluids weâ€™re looking at have more or less constant density, so each equally-sized parcel of those fluids has the same mass. Therefore, if it makes things easier for you, wherever you see the word momentum you can think of velocity, but in more complex scenarios these differences can matter.Let me bring in the previous flow simulation one more time:Youâ€™ve probably noticed that, as the flow moves to the right, the size of this blended region increases. When the regions of fluid with different momentums meet for the first time, they barely have any time to average out, and the blending is minimal. As time passes, these regions of fluid get to average out more, similarly to how two different layers of dyed water mix more over time.However, as time is passing, these parcels also , and that stronger blending happens further to the right. The downstream regions had more time to mix and average out, so the visible thickness of the blended region on the right side is also larger.With higher , the size of blended region grows much more quickly, which lets us be more precise about our working definition â€“  controls the  of the diffusion of momentum.So far weâ€™ve only observed flows with nicely separated horizontal layers, but  averages momentum between  two regions of fluids. In the demonstration below, you can witness how  affects a swirly motion of fluid in a :Notice that with high viscosity any differences in velocity are very quickly diluted out into nothing, but with low viscosity the revolving motion can survive for quite a while. has a damping or smoothing effect that makes it much harder to sustain any large variation in a velocity field. Letâ€™s see how this affects the motion of objects in fluids of various viscosity. In the demonstration below, weâ€™re tracking a velocity field close to a very  put directly in the stream of an incoming fluid of adjustable :With high viscosity, there is a large region of slow down around the  that regains its speed fairly quickly behind the object. At lower viscosity that surrounding region is much smaller, but it extends much further behind the . For very low viscosity weâ€™re once again seeing some more unusual behavior that weâ€™ll get back to in a minute.From the dark colors we can easily see that right by the surface of the  the fluid doesnâ€™t move at all â€“ it sticks to that surface. This velocity difference between the halted flow at the wall and the moving outer flow gets smoothed out over time by , similar to how it blended in the flow between two different layers of fluid.As before, with higher , the velocity averaging process becomes more rapid, and the blended region becomes more widespread. This averaging effect doesnâ€™t just alter the velocity of fluid, but it also affects the plate. In some sense, the  also wants to make the velocity of the surface of the  to be more like the velocity of the surrounding flow.The viscosity makes the flow want to pull the  with it, which creates a  that tries to slide the surface of this object away. The net effect is that that viscosity creates additional drag known as  that wants to slow down any object moving in it.All of these effects underline why highly viscous fluids are â€œthickâ€. Viscosity not only quickly averages any local differences in velocity, which prevents those fluids from flowing easily, but it also represses motion of objects in those fluids â€“ youâ€™ve likely experienced the difficulty of moving a spoon through a jar of honey.The flow of any fluid exhibits tiny, random disturbances. In fluids with high viscosity, these variations are very quickly dispersed, so their motion is rarely erratic. Fluids with low viscosity arenâ€™t as effective at damping motion, and these disturbances can grow to create oscillatory patterns. Weâ€™ve seen glimpses of them in the previous simulations, but here is another example:At lower viscosity the flow becomes quite wave-y. Those  happen at the border of regions of fluid with different velocities, like where the slow wake behind a plate is in contact with the fast external flow. In those regions, any tiny random intrusion of slower flow into the faster flow can get magnified and rolled over like a wave.In our discussion of the motion of air around an airfoil, weâ€™ve seen how the flow, the pressure field, and the shape of the body have effects on each other. These influences can be quite dynamic in nature, with distributions of velocity and pressure swinging back and forth in a never-ending fight for dominance.In the demonstration below, we can see a more dramatic example of these battles, where, depending on the , the flow around a  can take many different forms:While I canâ€™t easily simulate it here, with further decrease in viscosity, the flow can develop full featured  in which highly irregular and chaotic mixing motions occur at different scales. Turbulent flow stands in contrast to , in which neighboring areas of fluid move in an orderly way past each other without any varying fluctuations.Although weâ€™ve put most of our focus on , which is often denoted with the Greek letter , the general behavior of the flow also depends on its velocity , density , and the size  of the body or container involved in the flow. These parameters are tied together by the :Flows with the same Reynolds numbers exhibit similar behavior, which means that if we make the obstacle size  twice as large and we halve the speed of the flow , the Reynolds number wonâ€™t change and neither will the characteristics of the flow â€“ it will exhibit the same smooth or oscillatory motion.The Reynolds number also â€œpredictsâ€ the onset of turbulence. When we increase the speed of the flow , or decrease the viscosity , the Reynolds number rises. When it reaches a high enough value, turbulence is likely to occur.Letâ€™s quantify the difference in viscosity between different fluids. The precise values arenâ€™t that important to us, but to briefly be a bit more formal, viscosity is expressed in units of pascal-seconds, or . To let us use more manageable numbers, the following table uses millipascal-seconds, or :These values are measured at , but many fluids like oil get much less viscous with increased temperature. As expected, honey is significantly more viscous than water. Compared to water, the viscosity of air is around 50 times less still, but even a very low viscosity has effects on flow and its interaction with solid walls.To understand how viscosity arises in gasses like air, we have to once more get back to the world of particles. So far weâ€™ve been watching them from a distance, with individual collisions barely perceptible in the moving swarm. This time weâ€™re going take a closer look at these interactions.In the demonstration below, you can experience a simplified simulation of  colliding in space. Each molecule represents nitrogen or oxygen â€“ these two elements constitute the vast majority of air, and, in normal conditions, each one consists of two atoms.You can drag the  around, and once you let go Iâ€™ll automatically aim it so that it hits the . The speed of the  is four times larger than the speed of the :Notice that after the collision, itâ€™s the  thatâ€™s slow, and itâ€™s the  thatâ€™s fast. In this demonstration the two particles have the same mass and they collide straight on, so they simply end up trading velocities.More generally, particles of different masses that strike each other at different angles will exchange some amount of momentum. Recall that the heavier the particle, or the faster it moves, the higher its momentum.Letâ€™s see how this behavior ends up affecting the average velocities of larger quantities of molecules. In the  demonstration below, air molecules are grouped into two different parts. The air in the  has higher velocity than the air in the , which you can see in the black arrows showing the  in those regions. Notice what happens to  as you let  by dragging the slider:At the very beginning, the  in these  are visibly different, but they quickly even out when fast particles from the  flow into the slower , and the slower particles from the  move into the faster , balancing the initial velocity differences.Moreover, some of the faster particles collide with slower particles in the  and some of the slower particles collide with faster particles from . The faster particles lose some of their higher momentum, while the slower particles gain some of the momentum. All of these effects â€œdiluteâ€ some of those average velocity differences between the two regions.You may also remember that when we observed a flow of fluid around a flat plate, that fluid wasnâ€™t moving at all right on the surface of that plate, because it was stuck to it. Letâ€™s see how this behavior may arise on a microscopic scale.In the demonstration below, weâ€™re watching the familiar air particles right next to the . To make tracking easier, Iâ€™m  some of the particles in the vicinity of this :When seen at a very large magnification, this , like almost all surfaces, isnâ€™t perfectly smooth and has various peaks and valleys. The particles hitting these irregularities get bounced in more or less random directions. Some of the unlucky molecules can even get stuck for a while in these local crevices.Close to the , the random collisions with peaks and valleys prevent the particles from making bulk progress in  direction. The average velocity of the air flow by the wall is more or less zero. Some molecular interactions between the particles and the  can also prevent the fluid from moving.This sticking behavior is known as the  and it holds true for most typical flows of fluids that we experience day to day. Itâ€™s only in extreme conditions of very rarified gasses in the upper parts of the atmosphere or flows in microscopic capillaries that can break this assumption.Letâ€™s leave the world of particles behind for the last time and see how these two effects play an important role of influencing the airflow close to the surface of any object.Letâ€™s take another look at a   placed in the stream of incoming fluid:From this broader perspective, itâ€™s hard to see how the flow interacts with the surface of that , because the effects of viscosity are limited to the region close to that surface. Letâ€™s focus our attention on the small area that Iâ€™ve outlined with a  line, right in the top part of the . Here it is zoomed up close:We can once more see that, due to the no-slip condition, the velocity is zero at the , and then it  the velocity of the flow further away from the surface itself. What weâ€™re seeing here is known as the , which spans the region between the  of the object and the â€œouterâ€ flow, which is mostly unaffected by the presence of the object.Because the velocity in the boundary layer smoothly approaches the speed of the outer flow, it doesnâ€™t have a well-defined end point. One of the choices is to agree that the boundary layer ends where the speed reaches 99% of the speed of the surrounding flow far away from the . Let me visualize this boundary in the flow using a :As we move with the flow along the distance of the plate, the viscosity keeps averaging out the velocity differences, making the boundary layer thicker â€“ this is similar to what weâ€™ve seen at larger scales with highly viscous flows around objects.Letâ€™s quantify the distribution of speed in the boundary layer a little more precisely. In the demonstration below, I put the velocity arrows back in. I then connected the ends of these arrows with a thin line to show a  of velocity at that location along the :Notice that, initially, the velocity close to the wall increases almost linearly, but then it smoothly tapers to reach the speed of the external flow. The velocity profile close to the surface has a certain steepness, which Iâ€™m showing with the . This line determines the amount of skin friction drag at that spot â€“ the closer to the , or more horizontal, the  is, the higher the skin drag.As the differences in velocity become less severe, the force with which viscosity wants to drag the surface with the flow also decreases. In the conditions present in the demonstration, the skin friction drag decreases over distance.At this point you hopefully have an intuitive grasp of how viscosity affects the flow close to the surface of the object. From our earlier discussion, you may also remember that pressure differences also affect how the flow behaves, with parcels of air slowing down when climbing the hill of  and accelerating on the downhill of the .In the boundary layer flows we played with, the pressure distribution was more or less constant in the investigated region. Letâ€™s see how the flow changes when we vary that pressure.In the top part of the demonstration below we see the exact same view of velocity weâ€™ve experimented with so far. In the bottom part of the demonstration below you can see the pressure distribution in the boundary layer, which you can change using the slider below.If the pressure decreases in the direction of the flow in the boundary layer, we say that the pressure gradient is .  accelerates the air, and the boundary layer doesnâ€™t grow as quickly, since the slowdown caused by viscosity is opposed by that acceleration.When the pressure increases in the direction of the flow, we say that the pressure gradient is .  pushes  the direction of motion of the air. Far away from the , the air has enough momentum that the adverse pressure merely slows the flow down. However, close to the , the flow in the boundary layer was slow in the first place, so a pushing  may even reverse the direction of the flow.When the flow in the boundary layer gets reversed, we say that the boundary layer . This region of reversed flow can form a sort of wedge that can lift the rest of the flow away from the .Letâ€™s take a step back from the subtleties of boundary layers to see how what weâ€™ve learned corresponds to behavior of a flow around an airfoil. Let me once more bring up the demonstration that brought us here in the first place:As we move across the surface of the airfoil, the  at the stagnation point up front gradually decreases to reach  close to the â€œpeakâ€ of that curved surface. Across this transition the , and that distribution works in our favor â€“ the boundary layer stays nicely attached to the surface.However, as the air reaches the valley of the , it then has to start climbing back up to reach the  pressure in the rear of the airfoil. For small values of the  , the pressure pit from which the air has to climb out is not very deep and the  isnâ€™t very strong, so the boundary layer remains attached.As we increase the  of the airfoil, the pressure on top becomes  and . For even higher angles, the  becomes so strong that it  eventually reverses the flow in the boundary layer, creating separation. Letâ€™s look at this region up close to see how the arrows of velocity in the separated region point in the other direction:If you  to add  in the bottom right corner of the simulation youâ€™ll notice that many of them move  the bulk of the flow â€“ the boundary layer and the flow have separated.Weâ€™ll get back to looking at airfoils soon enough, but we still have a few things to wrap up in the world of boundary layers.The boundary layers weâ€™ve looked at so far were  â€“ the layers of fluid with different velocities flowed in an orderly way on top of each other. However, at higher flow speeds and over larger distances, or at high Reynolds numbers, the flow in the boundary layer transitions to a  flow:Be aware that what youâ€™re seeing here is a very simplified simulation of a turbulent boundary layer. Turbulence is inherently three dimensional and it contains various evolving structures of different sizes that are extremely computationally expensive to evaluate in detail. Thankfully, you can find many videos of computersimulations and real flows showing turbulent boundary layers.While the laminar boundary layers weâ€™ve seen in the past exhibited very organized flows, the turbulent one is very chaotic, with large and small swirls causing the flow to mix very rapidly. The transition from laminar to turbulent boundary layer happens spontaneously, but for a given flow speed, the location of the transition depends on surface roughness, steadiness of the flow outside of the boundary layer, and presence of pressure gradients.At any given moment, the velocity profile in the turbulent boundary layer is very unsteady, but it can be averaged over time to get the mean distribution of speed. Letâ€™s compare the  profiles of the  and  boundary layers:In the dynamic simulation of the  boundary layer, we saw how the slower flow close to the  rapidly mixed with the upper regions of the flow. This slows down those faster sections, and we need to go farther away from the  for these sluggish intrusions to stop affecting the flow. For this reason, the  boundary layer is thicker and grows faster than a  boundary layer.On the other hand, the strong   mixing causes the fast external flow to get close to the body, so the overall velocity profile by the  increases much more quickly in the  case as opposed to  case â€“ Iâ€™m showing that with .Recall that the more horizontal the velocity profile at the  of the object, the bigger the skin friction drag â€“ a   boundary layer has higher skin friction drag than a  layer. Despite the cost of increased friction drag, a   boundary layer is often beneficial.Because of that higher velocity closer to the surface, a turbulent boundary layer is more resistant to  and it can stay attached to the surface of an object for longer distances.For some objects like golf balls, which purposefully make their boundary layer turbulent by roughing up the surface with little dimples, the delayed separation also decreases the  caused by uneven pressure distribution. That reduction more than compensates for the increased skin friction drag, making the dimply golf balls fly farther than equivalent smooth balls.For airfoils, a turbulent boundary layer delays separation of the flow, which can help prevent stall at higher angles of attack, but at normal cruising conditions the increased skin friction becomes an important drawback. For many aerodynamic shapes in typical conditions, the skin friction drag is the primary contributor to the total drag that these objects experience.As weâ€™ve seen, by increasing the angle of attack on an airfoil, the lift force grows up to a certain limit, at which the boundary layer separates over most of the upper surface. By staying under this limit, a symmetric airfoil can safely generate lift force.However, when it comes to   and , the shape of an airfoil isnâ€™t particularly unique in its -creation capabilities. Most simple elongated shapes generate  when put in a flow at an  . In the demonstration below, you can  a  and see the forces exerted by the pressure field around it:You may be surprised to see that, at small  , this flat plate also generates . An airfoil-like shape is not a requirement for  generation. After all, paper airplanes with their flat wings can fly just fine.  is just an outcome of the pressure distribution created and sustained by the flow.Although it doesnâ€™t take a sophisticated shape to generate lift at an angle of attack, a well-designed airfoil can often create more lift and with lower drag. In the last section of this article, weâ€™ll explore how other variations to the shape of an airfoil can affect its characteristics.Letâ€™s go back to the simple symmetric airfoil weâ€™ve been playing with thus far. This time, however, weâ€™re able to control its  using the slider:Notice that as we increase the  of the airfoil, the pressure on the top and bottom sections of the shape becomes more . For this symmetric airfoil at 0Â° angle of attack the thickness doesnâ€™t change much other than increasing the .However, if we break the symmetry of the shape, we can use thickness-dependence to make one side of the airfoil have a higher  pressure than the other. In the demonstration below, you can control the â€œthicknessâ€ of the upper surface of the airfoil using the slider:Notice that an asymmetric shape creates an asymmetric pressure distribution, which ends up creating  without any changes to angle of attack. With some slight tweaking of this shape we finally recreated the asymmetric shape we first saw on the airplane in the early sections of this article.Naturally, when combined with an increasing , this airfoil will generate even more  until it eventually reaches stalling conditions:While symmetric airfoils are sometimes used in acrobatic airplanes, which often find themselves flying upside down, most typical planes use an asymmetric airfoil shape.The underlying mechanism of  generation by changing the  or by shaping the object differently is ultimately the same â€“ weâ€™re changing the placement and orientation of the surface of the body relative to the incoming flow. The flow reacts by changing the velocity and pressure distribution, and the resulting pressure field creates the forces on that object.This all means that we have a lot of flexibility in how an airfoil is shaped, as long as the resulting pressure distribution fulfills the design goals of achieving a certain amount of lift while minimizing drag.For example, in some applications itâ€™s important to minimize the skin friction drag caused by a turbulent boundary layer. Some  achieve this by shaping the airfoil to move the â€œpitâ€ of  further to the back of the airfoil:The  between the front and the lowest pressure point extends over a longer distance across the surface of this airfoil, which, at least in principle, helps to keep the boundary layer laminar to keep the skin friction low.Notice that even this unusual airfoil had a rounded front and a sharp back. The roundness of the front helps the air smoothly flow around this area at different  , and the sharp back reduces the pressure drag by avoiding the separation of the flow.The velocity of the flow around the airfoil is also a contributing factor to the design of the shape. Letâ€™s look at the speed distribution in the flow around a simple asymmetric airfoil using the  and :The flow above the airfoil is faster than the incoming flow as indicated by brighter colors. The markers that start in the same line donâ€™t end up sliding off the airfoil in the same formation â€“ the ones on top are further ahead. This is particularly visible for larger values of the .This acceleration in the upper part becomes another point of consideration for airfoil design. While commercial airliners donâ€™t fly faster than the speed of sound, the accelerated flow in the top part of an airfoil  break that barrier. This creates a shockwave that can sometimes be seen in flight. Modern airliners use supercritical airfoils that are designed to reduce these drag-causing shockwaves by carefully controlling the speed of the flow around the wing.Planes designed to fly  the speed of sound use  that are quite different from the shapes weâ€™ve seen. These airfoils have a thin profile and their front edge is sharp and not rounded.  flows of air are more complicated than what weâ€™ve explored in this article, as variations in density and temperature become an important component of the behavior of the flow.Many of the airfoils used today are designed specifically for the plane theyâ€™ll be used in. Moreover, that cross-sectional shape may change across the length of the wing. Real airplanes are three dimensional and the overall shape of the wings also significantly affects the lift and drag of an airplane, but ultimately all the resulting forces are an outcome of interactions between the flow and the body.John Andersonâ€™s Fundamentals of Aerodynamics is a very well-written textbook on aerodynamics. Over the course of over a thousand pages, the author presents a classic exposition of the motion of fluids and their interactions with bodies put in those flows.Understanding Aerodynamics by Doug McLean is a great textbook that takes a different approach of explaining aerodynamic phenomena using physical reasoning. For me, the crowning achievement of the publication is showing that many popular explanations of the origins of lift are either incorrect or theyâ€™re based on merely mathematically convenient theorems. The authorâ€™s video lecture gives an overview of some of these misconceptions.Finally, YouTuber braintruffle created a series of beautiful videos that start with the behavior of fluids on a quantum scale and build up increasingly abstract models that can be used in more practical applications. The videos are packed with interesting takes on fluid mechanics, and theyâ€™re worth watching for their visuals alone.If you were to sit on a flying airplane and look out the window to glance at its wings, youâ€™d often have a hard time seeing anything going on. However, in that crisp clearness of air whose invisible flow sustains the varied pressure field, lies the hidden source of lift that overcomes the might of gravity to keep the plane safely above the ground.Since the first human flight, weâ€™ve now mastered the art of soaring in the skies by bending the flow of air to our will, using physical quantities like pressure and velocity to help shape our designs. These tangible concepts are ultimately just a manifestation of motions and collisions of billions of inanimate air particles that somehow conspire to assemble the forces we need.I hope this deeper, technical exploration of airfoils hasnâ€™t diminished your appreciation of the greatness of flight. Perhaps paradoxically, by seeing how all the pieces fit together, youâ€™ll find the whole thing even more magical.]]></content:encoded></item><item><title>Microsoft forced me to switch to Linux</title><link>https://www.himthe.dev/blog/microsoft-to-linux</link><author>bobsterlobster</author><category>hn</category><pubDate>Wed, 28 Jan 2026 14:28:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[What's better than a devil you don't know?The devil you do.I've used Windows for as long as I've been alive. At 6 years old, my first computer was a Windows 98 machine, with an Athlon XP 1900+ (Palomino core) and a GeForce 440 MX, blessed with a generous 256 megabytes of RAM.Looking back, I kinda got scammed with that graphics card, but what could I do? I was a silly kid. (The missing shader support came back to bite me in the ass)Also, is it weird that I still remember the specs of my first computer, 22 years later?Anyway, Windows has been familiar and comfortable. I knew all the workarounds and how to extract maximum efficiency from it.I was a happy user, for over 20 years, and Windows has been my go-to for everything computer-related.Even after becoming a software developer and using a macbook, I'd still find myself reaching for Windows at times.That is, until Microsoft decided to turn it into something completely unrecognizable and unusable.It all came crashing downI think it started with the Windows 10 full-screen ads.You know, those friendly suggestions telling you to try OneDrive or to "use the recommended browser settings" (reads as "please try Edge and OneDrive, we're desperate").Actually, scratch that, I think it really started with the non-consensual updates:Oh you're doing work? That's so cute... we're gonna close whatever apps you had open, because we're updating now. We own your computer.You had unsaved work? Too bad, it's gone, get bent.At first I ignored it, and carried on as normal. Sure, I'd get mad from time to time and I'd complain.But hey, nothing beats the convenience of being able to have all of your applications in one placeMy breaking point came with the 24H2 update. It installed on my system , like any other major update.
I knew there were problems with it, people were already complaining on Reddit, so I just postponed it, and kept postponing it.All it took was for me to leave my computer on and unattended for a while, and , just like that -  the major OS update that nobody wanted, it was on my computer.The Chrome Seizure IncidentSpoiler: all hell broke loose.As soon as 24H2 landed on my machine, I encountered a bug so bizarre I thought I was losing my marbles.
If Chrome was positioned  any other window, it would start having what I can only describe as a visual seizure.
Here's Ableton Live with Chrome (Reddit) under it:Worse, there was a decent chance this would trigger a full system lock, leaving me smashing my desk in impotent rage. I shit you not.I tried to rollback. The rollback failed with an error. I reinstalled Windows. The bug persisted.
Like digital herpes, I just couldn't get rid of it.
The solution? Installing an Insider build. Yes, the solution to Microsoft's broken stable release was to use their  release.For the Windows Defenders (see what I did there?), I tried uninstalling the display drivers with DDU, and testing other versions. It didn't help.Either I stayed forever on the older build, or I'd have to deal with this.
And don't tell me to forever disable updates, I'll completely lose it.The Sequel I Never WantedThe Insider build worked...sort of. But now I had a new bug: Chrome would randomly lock up for about 30 seconds when a video was playing.
My options were to wait it out or press Ctrl+Alt+Delete and Esc to force my way back to a working browser.
After some digging, I discovered this was caused by an NVIDIA-Microsoft driver incompatibility.I've found out that the flickers and the chrome lock-up issues are likely caused by the Multiplane Overlay (MPO) pipeline. Microsoft blamed NVIDIA for not correctly implementing it in their drivers. NVIDIA blamed Microsoft.
What's clear is that if you were facing this issue, you were essentially screwed because these 2 companies would just pass the hot potato to each other.I should mention that this bug persisted even after I went off the Insider build and on 25H2. And when I posted on r/Microsoft, they just deleted it.The latest and greatest OS surely cannot be broken beyond repair, surely I'm using my PC wrong.So there I was, finally grasping the reality of what you're up against, as a Windows user:Random bugs that break basic functionalityUpdates that install without permission and brick my systemCopilot and OneDrive ads appearing in every corner of the OSCopilot buttons everywhere, coming for every applicationCan't even make a local account without hacking the setup with Rufus (they even removed the terminal workaround)Zero actionable fixes or even an aknowledgment of their fuckupsPeople often say Linux is "too much work.".And I agree. They're completely justified to complain. There's the documentation page diving, the forums, the reddit threads. And, most importantly, you have to basically rewire your brain and stop expecting it to behave like Windows used to.But I looked at the list above and realized: Windows is now  too much work.
And the difference with Windows is that you're going to do all that work while actively fighting your computer only for it to be undone when the next surprise update comes and ruins everything.You might be thinking "just disable updates, man" or "just install LTSC", or "just run some random debloat script off of GitHub".
 Why would I jump through all these hoops? I'd rather put in the effort for an OS that knows what consent is and respects me as a user.Could the grass actually be greener on the other side?To set the stage: I'm a software developer and a musician.As you can imagine, I was legitimately worried about app support on Linux, and how it would distrupt my workflow.But after Chrome crashing for the 10000th time, I said "enough is enough", and decided to go big. I installed CachyOS, a performance-focused Arch-based distribution, on my main machine (9800X3D, RTX 5080).It wasn't a painless process. In fact, sleep mode was broken from the start,
and my system would fail to detect the monitor after waking up.What's more, Ableton Live does not have a native Linux build, only Windows and macOS. So I couldn't use it anymore, at least not without fucking around with Wine (which doesn't fully support it), or without keeping a Windows VM and taking an L on audio latency.But unlike Windows, on CachyOS I could actually fix my NVIDIA woes by following this thread on their forum.All I had to do was add the NVIDIA modules to mkinitcpio. One config change, a command to rebuild the initramfs, and problem solved.I also found a good native alternative to Ableton Live - Bitwig Studio, which bothered to release a native Linux Build.Thanks to the constant progress that was made with Pipewire, I'm getting audio latency on par with Mac OS, and lower than Windows.
And my workflow didn't even change that much, since Bitwig is made by ex-Ableton developers that seem to give a shit.As for my development tools, on Windows you already accept the fact that  use WSL or docker, so realistically I just cut the broken middleman.Now compare that to the Windows fuckery above.What You're Signing Up ForIf 3 years ago you would have told me that Microsoft would singlehandedly sabotage their own OS, doing more Linux marketing than the most neckbearded Linux fanboy (or the most femboy Thinkpad enjoyer), I'd have laughed in your face,
called you delusional, and then hurled some more insults your way., I've been dual-booting CachyOS for over a year, and in the last month I've been using it exclusively.If you're thinking about making the switch, I'd recommend you do a little research first.Look up the tradeoffs between a rolling release distro and a stable release, it might just save you a headache.For me, the fast updates of Cachy/Arch are a good thing, but you can imagine that you are effectively trading stability for new features.So what is the actual state of Linux in 2026, from my honest perspective?All major browsers (Chrome, Firefox, Edge, Brave) have native Linux builds. Full support. No compromises.
Video playback works flawlessly, with hardware acceleration even. On AMD, on NVidia and yes, on Intel too.Linux is the  platform for development.Better terminal support, native package managers, Docker runs natively without the WSL overhead, and your production servers are probably running Linux anyway.Hell, even Microsoft has their own Linux distro, Azure Linux (Formerly CBL-Mariner).This is where people assume Linux falls short. And they're right, but not completely:: Runs via Winboat. Far from perfect (no video acceleration, laggy at times), but functional: Native Linux app. Professional-grade video editing, free tier available: Native Linux app, completely free and open sourceSo while content creation is viable, the compromises might be dealbreakers.: Incredible DAW that runs natively on Linux: Native, free, open-source DAW: Thanks to PipeWire, Linux audio latency is actually  than WindowsHere's where things get interesting. The perception is that gaming on Linux is a no-go. In 2026, that's increasingly untrue:: Pretty much all games without kernel-level anti-cheat work out of the box through Steam's Proton compatibility layer: For AMD GPUs, gaming performance is on par with Windows, on average: There was a 10-30% performance penalty on Intel/NVIDIA GPU setups, but recent Vulkan extensions are taking care of that.NVIDIA has released beta drivers making use of these improvements, and once Wine/DXVK/Proton are updated to make use of the extensions, the performance delta should be essentially goneThe only real limitation is that some games with anti-cheat like Valorant, Call of Duty or League of Legends won't run.
But honestly I think not being able to launch League of Legends is actually a feature - one final reason to install Linux.It's not all bad, though. Arc Raiders makes use of Easy Anti-Cheat, yet runs flawlessly. In fact, I've been playing it like a madman.
It goes to show that if the developers want to, it's possible.Still falls short compared to Windows and Mac OS (Autodesk, I'm looking at you).The silver lining is that Blender has a native build. So if it's your main application, you're good to go.Basic operations are  on Linux.
Opening directories, launching applications, system responsiveness.
It's like your computer took a line of coke, and is now ready to work.No more waiting for the Start menu to decide it wants to open. No more File Explorer hanging when you need it the most.Since we're on the topic of Linux improvements, I want to address the elephant in the room - people who keep saying "I want to switch", but keep moving the goalposts:"I'll switch when Linux supports X.""Okay, but what about Y?""Well, Z is still missing..."If you're always finding the next reason not to switch, you're not looking for solutions, you're looking for excuses to stay complacent.I was that person, so I would know.At the same time, I want to take it down a notch and say that there are still plenty of use cases (Especially creative work, and like stated previously, 3D modelling and also Game Dev) where it simply doesn't make sense to switch.So if you're in that scenario, don't feel pressured, just wait for things to improve.And if you don't plan on ever switching, more power to you.I'm not here to judge, just here to vent my Microsoft frustrations.And I didn't really want to switch either, because who wants to re-learn how their computer should be operated from scratch?
What I really wanted was for Windows to work, but Microsoft didn't.The Windows RetrospectiveWhile I'm enjoying my new Linux setup, Windows 11 is having a miserable year, and we're only a month in!According to Windows Latest, there were over  in 2025 alone, and 2026 is starting off strong, with the January update causing black screens and Outlook crashes.Here's a quick 2025 Spotify Wrapped of the bugs Windows users dealt with:USB audio devices randomly stopped workingWebcams failed to be detectedBitLocker settings became inaccessibleAdobe Premiere Pro couldn't drag clips on the timelineCursor constantly spinning for no reasonRemote Desktop sessions randomly disconnectingThe Copilot app accidentally getting deleted (okay, this is actually a good change for once)Blue screens of death in mandatory security updatesWindows Hello face recognition brokenFile Explorer becoming unresponsiveFPS drops and system reboots while gamingTask Manager spawning infinite copies of itselfDark mode breaking with white flashesAnd the company's response? Crickets. They're busy boasting that 30% of their code is currently being written by AI. Don't worry, Microsoft, we can definitely tell.For the remainder of 2026, Microsoft is cooking up a big one: replacing more and more native apps with React Native.
But don't let the name fool you, it's never going to be as close to native as the real thing.
These are projects designed to be easily ported across any machine and architecture by making use of JavaScript.And each one spawns its own Chromium process, gobbling up your RAM so you can enjoy the privilege of opening the Settings app. And each one of these apps creates an instance of V8 or Hermes per app, which adds additional overhead (RAM + CPU). I'd argue you do not need that overhead just to open a Settings app.I could maybe understand this for a weather widget. But when it's coming for core system apps, I think it's just lazy.I'm gonna go full conspiracy nut here, but I bet it's because it's easier for LLMs to write JavaScript, and Microsoft can't be asked to pay actual humans to write (and test) proper native code.Not Because I Wanted To, But Because Microsoft Forced My HandSo here I am. Fully switched to Linux.Not because I'm some open-source idealist or command-line warrior (I'm just some guy), but because Microsoft turned into Microslop.Recently, Microsoft CEO Satya Nadella wrote a blog post asking people to stop calling AI-generated content "slop" and to think of AI as "bicycles for the mind."Well, Mr Satya, I have a couple of bicycles that will blow your mind:You are the biggest Linux evangelist there ever was, you single-handedly convinced countless people to ditch your buggy, ad-ridden, bloated, slop-infested mess of an OS.And worst of all, you're like a pit bull that has lock-jawed onto OpenAI's ballsack, and you're not letting go, no matter how much we tell you to.So we're calling slop for what it is: disgusting slop.You're chasing profit like your life depends on it, yet you've completely forgotten the very thing that generates profit: .Now you're stuck in a circlejerk of fake value in a fake bubble, and OpenAI's hand is so far up your ass that you're basically their ventriloquist dummy.The time to switch is now. The tools are ready. The only question is: Satya came down from his cloud in the sky,With Copilot dreams and a gleam in his eye,He sprinkled AI on each app, every field,Till users cried "Fuck!", and the slop was revealed.]]></content:encoded></item><item><title>Show HN: I built a small browser engine from scratch in C++</title><link>https://github.com/beginner-jhj/mini_browser</link><author>crediblejhj</author><category>dev</category><category>hn</category><pubDate>Wed, 28 Jan 2026 14:03:28 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi HN! Korean high school senior here, about to start CS in college.I built a browser engine from scratch in C++ to understand how browsers work. First time using C++, 8 weeks of development, lots of debuggingâ€”but it works!- HTML parsing with error correction- CSS cascade and inheritance- Block/inline layout engine- Async image loading + caching- Link navigation + history- String parsing(html, css)- Image Caching & Layout ReflowingWhat I learned (beyond code):- Systematic debugging is crucial- Ship with known bugs rather than chase perfection~3,000 lines of C++17/Qt6. Would love feedback on code architecture and C++ best practices!]]></content:encoded></item><item><title>ICE and Palantir: US agents using health data to hunt illegal immigrants</title><link>https://www.bmj.com/content/392/bmj.s168</link><author>dberhane</author><category>hn</category><pubDate>Wed, 28 Jan 2026 12:18:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[US immigration agents are using an app developed by Palantir that draws on the health records of millions of Americans to find and detain people they deem illegal immigrants.The revelation comes as the USâ€™s Immigration and Customs Enforcement (ICE) comes under increased scrutiny after the shooting of Alex Pretti, a 37 year old intensive care nurse, by ICE agents in Minneapolis over the weekend.It has now emerged that data from the Department of Health and Human Services (HHS) is being fedâ€”along with other commercial and public datasetsâ€”into an analytics app developed by Palantir, according to an investigation by news outlet 404 Media.1Testimony from an ICE official and internal documents obtained by 404 show the app, Enhanced Leads Identification and Targeting for Enforcement (Elite), maps areas to help agents decide where to conduct detention raids.The tool was reportedly used in recent operations, including a raid in Oregon in October in which 30 people were arrested.According to the 404 investigation, Elite pulls names, addresses, and photos from health records. It reportedly works like Google Maps, showing ICE agents which areas have higher densities of people who could be detained. It also generates dossiers on individuals, including their name, photo, and â€œconfidence scoresâ€ that they are at home.An HHS spokesperson contacted by  did not clarify what information was given to ICE but said the information sharing was permitted under national law.â€œSeveral federal laws authorise the Centers for Medicare and Medicaid Services (CMS) to make certain information available to the Department of Homeland Security (DHS),â€ the spokesperson said. â€œUnder the Immigration and Nationality Act, â€˜any information in any records kept by any department or agency of the government as to the identity and location of aliens in the US shall be made available toâ€™ immigration authorities.â€There is no data sharing agreement between CMS and DHS on â€œUS citizens and lawful permanent residents,â€ they added.In July 2025, it was revealed that a data sharing agreement between the US health department and ICE would see the personal data of 79 million Americans receiving Medicaid assistance handed over to the deportation agency.This includes names, addresses, birth dates, and ethnic and racial information.2Palantir, an American tech giant best known for its work with US defence and intelligence agencies, also works in the UK where it won a Â£330 million contract to develop data platforms that integrate information held across separate NHS trusts.Doctors and patient advocacy and rights groups, as well as the BMA, have questioned if it is ethical for a US defence technology firm to handle sensitive health data and if the deal could undermine patient trust.3Contacted by , a Palantir spokesperson said it â€œcannot comment on specific data sources used by our customers in their confidential environments. However, Palantir expects data sharing among government agencies to be conducted in accordance with lawful authorities and compliant with applicable data sharing agreements.â€Indiscriminate and a violation of rightsRights groups say the use of location based targeting is indiscriminate and violates due process, while some US states have challenged the move in court, leading to a temporary suspension of information sharing.4John Howard, a specialist in healthcare data privacy at the University of Arizona, said that although the interagency sharing of data from health records is legal and likely covered under the Privacy Act rather than the Health Insurance Portability and Accountability Act, it could damage peopleâ€™s trust in healthcare.â€œIf a population does not trust a health system to protect it and its information there could be a loss in trust of that system,â€ he told .This, he pointed out, was one of the purposes behind the Health Insurance Portability and Accountability Act, signed into law in 1996 to protect patient data. The act places limits on the disclosure of health information but does not cover all data held by health agencies and does not apply equally across all government departments.Still, the act was put in place â€œto build trust in the US health system so people will seek care when they need it,â€ Howard said. â€œEroding this trust can cause public health problems if we have sick or injured people that forgo seeking care.â€He urged the US Congress to act. â€œIf circumstances arise where a law is applied in a manner that is counter to the public policy purpose intended by Congress, it is the responsibility of our lawmakers to step in and provide direction and corrections needed to account for the change,â€ he said. â€œIf our leaders are not willing to do this an erosion in the trust of our entire legal system could result. I do not think this is something anyone wants.â€Dave Maass, director of investigations at the non-profit organisation Electronic Frontier Foundation, said, â€œIn the wake of the Watergate and COINTELPRO scandals of the 1970s, US Congress enacted laws to protect private information from government misuse. Data grabs like the DHSâ€™s reported use of healthcare data for immigration enforcement are exactly why.â€â€œGovernment agencies necessarily collect information to provide essential services, but when governments begin pooling data and using it for purposes unrelated to why it was originally collected, it provides them with enormous power that can be abused. The misuse of healthcare data is particularly insidious,â€ Maass told . â€œThis information is not only extremely sensitive, but its misuse for law enforcement purposes could also deter people from seeking essential medical careâ€”with grave individual and collective consequences.â€Do you have any additional information on this story? ]]></content:encoded></item><item><title>Show HN: The HN Arcade</title><link>https://andrewgy8.github.io/hnarcade/</link><author>yuppiepuppie</author><category>dev</category><category>hn</category><pubDate>Wed, 28 Jan 2026 10:50:32 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I love seeing all the small games that people build and post to this site.I don't want to forget any, so I have built a directory/arcade for the games here that I maintain.Feel free to check it out, add your game if its missing and let me know what you think. Thanks!]]></content:encoded></item><item><title>I Stopped Following the News</title><link>https://mertbulan.com/2026/01/28/why-i-stopped-following-the-news/</link><author>mertbio</author><category>hn</category><pubDate>Wed, 28 Jan 2026 08:33:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[If I have to pick my favourite German word, I would probably pick â€œneugierigâ€ because it describes me very well. It means curious, but that is not the literal meaning. Like many German words, it comes from two parts: neu and gierig. Neu means new, and gierig means greedy. So the literal idea behind neugierig is being greedy for new things. That fits me well.I really enjoy learning new things. That is why I read a lot, watch documentaries, and explore different topics. I am greedy for new information. This also includes what is happening around the world.For years, I followed the news to keep an eye on what was happening in my country, in the world, and in the tech industry. I curated a list of sources that matched my taste, from international newspapers to YouTube channels, Reddit, and tech-focused websites. Every day, I checked these sources through their websites or my RSS reader to stay up to date.At the end of last year, I had some time to reflect on my habits. I thought about what makes me happy, what makes me stressed, and how the things I do every day contribute to my goals.When I looked at the time I spent reading the news and how I felt afterward, I realised it was not helping me at all. Most of the time, I felt more stressed than informed. I also noticed that almost every source focuses heavily on negative stories, simply because those attract more attention and clicks.When it comes to world news, I started to see the same patterns repeating. A lot of headlines about political figures, conflicts far away, and problems I have no control over. I asked myself how much of this actually affects my daily life, and I could not come up with a good answer. Many things are happening in the world, but they rarely change anything in my day-to-day routine. That made me question why I was following them so closely.The same applies to tech news. Most of it revolves around big personalities, constant AI updates, startups getting acquired, people becoming rich, hardware I will probably never buy, and rumours about products that may never be released. In most cases, none of this has any real impact on my life. New technologies are usually easy to learn when they become relevant, and it is hard to know which ones will still matter in a few years.After thinking about whether I really need to follow the news, I decided that I do not. This does not mean I do not care about the world or the tech industry. I am just not interested in following daily updates that do not add much value to my life.One thing I am still interested in is what is happening in my city. For that, I recently discovered a newsletter from Die Zeit called Elbvertiefung. Every weekday at 6 AM, they send a short newsletter about what happened in Hamburg, and sometimes they also recommend a book or a new restaurant or cafe. I start my day by reading it. Since it is in German, it also helps me improve my language skills.For the rest of the news, I am considering subscribing to a magazine that covers important events in Germany, the EU, or the world every few months. This kind of format filters out short-term noise and fear-driven stories. These publications usually make money by selling the magazine rather than ads, so they tend to focus more on quality. (If you have any recommendations, feel free to reach out.)At the beginning of this year, I decided to stop following the news. As a result, I started reading more. Last year, I finished 20 books. This year, just in January, I finished 7 books. I still satisfy my curiosity and learn new things, but I just changed the medium. And I feel much better because of it.]]></content:encoded></item><item><title>Virtual Boy on TV with Intelligent Systems Video Boy</title><link>https://hcs64.com/video-boy-vue/</link><author>hcs</author><category>hn</category><pubDate>Wed, 28 Jan 2026 08:32:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The Video Boy plays Nintendo Virtual Boy games on a TV or monitor. As with many of Nintendo's development tools, it was made by Intelligent Systems. These were used to record video or screenshots; it's said that this particular unit was used by the venerable Nintendo Power.

The Virtual Boy inputs are on the top and front of the unit. From left to right: cartridge slot, link cable port, and controller port. There's also a red power LED. A cartridge goes in with the label facing down; it plugs directly into a Virtual Boy main board.

Note that the name shown here is "Video Adapter VUE". "Video Boy" appears nowhere on this device, but it is found in the instructions as "VIDEO-BOY (VUE)" and on the Intelligent Systems site as ãƒ“ãƒ‡ã‚ªãƒœãƒ¼ã‚¤ or videoboy.html.
The AV multi-out is PAL, which is 50 FPS like the Virtual Boy; this avoids a more complicated and lossy scan conversion to ~60 FPS NTSC.
The unit came with a DB9 to 3xRCA cable for the RGB OUT; I haven't tested it, lacking a monitor that takes raw RGB. The SCANNER port connects to a development headset; I don't have one.

This row of switches, SW1, is exposed on the bottom of the unit. The initial setting was: 1 off, 2 on, 3 on, 4 on, 5 off, 6 off, 7 off, 8 on.

7 and 8 control which display is shown: 8 controls the left, rendered in Virtual Boy red, 7 is the right, rendered in green. With both 7 and 8 on, left and right are combined, this was intended as anaglyph 3D. See the video output below.

Switching 5 on prevents anything from working. I didn't notice any effect from switching the others.

There's an image of the instructions which describes the switches, unfortunately it's low resolution. I think it says "don't use" for 5 and 6, and 1-4 are for setting some integer, "1=MSB, 4=LSB, ON=0, OFF=1".

There are nearby unpopulated jumper pads, and a set of pads marked "CL" which seem to be cut traces. This may have been hardwired when the switch wasn't installed.

The top label says "ãƒ—ãƒ­ã‚¸ã‚§å› 12å·", approximately "Project No. 12".

The bottom label says "VUE TV MONITOR", identifying the board inside. "Ver. C" indicates the third or fourth version, and "+æ”¹é€ " is "plus retrofit", perhaps indicating the various jumper wire patches. "MAI-VUE-X8" identifies the internal Virtual Boy main board.

The monitor board (left) has a Virtual Boy main board mounted on top of it (center top between the metal braces). The right side of the case is taken up by the power supply.

The Virtual Boy generates an image by sweeping a column of light horizontally. To convert this to the rows of a PAL TV signal, at least one frame must be buffered and rotated; the monitor board performs this conversion.

Note that the monitor board has unpopulated connector pads on the left (CN1, label not visible) and lower right (CN2). I think this same board can be configured to go into a VUE-DEBUGGER development unit (see PAL monitor), CN1 would be where it plugs into the debugger bus.

The main board is the heart of a Virtual Boy. This seems to be an early or development board, MAI-VUE-X8, (c) 1994. (A production board is VUE-MAI-01, (c) 1995.)

For info on Virtual Boy hardware:

Under the MAI-VUE-X8 board there are a few stray ICs and the ribbon cables that carry video to the monitor board (left and right). The board name was hiding under here: "VUE TV MONITOR(C)", which matches "Ver. C" on the label.

The workhorses are these two big Xilinx XC3064-70 FPGAs, which get their configuration from the 1765DPC PROMs between them. Perhaps one stores input while the other scans output.

The big NEC chip on the left (D27C1024A-15) is a 1Mbit EPROM, with an Intelligent Systems metal sticker to prevent UV erasure. I guess that at least one of the FPGAs is configured as a DSP, running a program from the EPROM.

There are eight 32KB SRAMs across the board, numbered in two groups: U12 & U13 (64KB), and U17-U22 (192KB). The 64KB might be DSP work RAM. 192KB would exactly fit two 384x256 frames with 8 bits per pixel (384x256 is the size of the Virtual Boy framebuffer, though only the top 224 rows are used). Virtual Boy graphics are only 2 bits per pixel, but each of the three non-black brightness levels can be configured by an independent 8 bit register, so 8 bits per pixel is plausible. This could be a double buffer (one taking input while the other scans output), or it could be one buffer per eye, or a double buffer for each eye at only 4 bits per pixel.

The oscillator Y1 (left, below the EPROM) seems to be associated with a VCLK test point, probably Video Clock; it's labeled D177J4, which suggests the 17.734475 MHz PAL color subcarrier. There's an unpopulated space for a second oscillator, Y2, and support components; it's grouped with the SCLK test point, maybe the Servo Clock. This may have be used when the board was configured to plug into the VUE-DEBUGGER.

Output is produced here by two MB40778 8-bit DACs (bottom center), an S-RGB video encoder (center right), and numerous discrete components. On the left are the output connectors: CN10 at top is RGB, CN8 at bottom is AV multi-out. I guess that each DAC handles one channel, connected to the red and green inputs of the S-RGB.

U6, U16: Xilinx 1765DPC - FPGA config PROM
  U9, U23: Xilinx XC3064-70 PC132C - FPGA
  U10, U11: Fujitsu MB40778 - 8-Bit D/A Converter
  U12, U13, U17-U22: Toshiba TC55328AP-20 - 32KB SRAM
  U14: NEC D27C1024A-15 - 1Mbit UV EPROM
  U15: Nintendo S-RGB - Video signal encoder (as seen on some SNES models)
U1, U5: SN74LS244N - Oct BufferU2, U3, U4: SN74LS245N - Oct TransceiverU7: 74HC14AP - Hex Schmitt InverterU8: SN74LS07N - Hex Buffers And DriversU25: 74HCU04AP - Hex Unbuffered InverterU29 AMP (is this the missing U24?)There are three jumpers on the bottom, and one that goes to (and through) the Virtual Boy board, strategically glued. Version C might have still needed a few fixes, or maybe these are used to retrofit a particular MAI-VUE board, or they could be specific to the standalone Video Boy configuration.

These images of Virtual Boy Wario Land come through an Elgato dongle, deinterlaced with ffmpeg filter .

The composite video output is a bit blurry. DIP switches 7 and 8 control how the two Virtual Boy displays are combined: 8 enables the left display in red, 7 the right in green, and they can be combined (center).

Here's the stereo effect in action, note how the colors separate on the backswing.

S-Video has nice crisp pixels, thanks to a higher luma resolution than composite. The Elgato isn't picking up chroma for some reason; this may be a flaw in the multi-out to S-Video cable I'm using, or the VUE Monitor may not output a color PAL S-Video signal. I use the brighter "green" output on the right when I occasionally stream Virtual Boy games.

Here's the text of the Video Boy page, based on Google Translate:
Simply connect a TV monitor that has PAL video input or RGB input to the Video Boy VUE, and a simulated stereoscopic Virtual Boy image will be displayed. By playing the Virtual Boy game cartridge on the Video Boy VUE, you can display the left-eye image and the right-eye image on the TV monitor in red and green, respectively. It is also possible to select and display an image for the left eye only or an image for the right eye only.
  Since the same screen can be checked by multiple people, it is very useful during demonstrations, specification meetings, debugging, etc.
I think the PALä»•æ§˜ãƒ¢ãƒ‹ã‚¿å‡ºåŠ›ãƒœãƒ¼ãƒ‰VUE (PAL monitor output board) was the same VUE TV Monitor board that's used in the Video Boy, configured as an expansion board for the VUE-DEBUGGER development unit. The notes on that page are similar to the Video Boy, with these two added points:
You can record your debugging work on video. This makes it easier to reproduce and check bugs that occur only occasionally, and improves debugging efficiency.
  The need for programmers to look into the scanner during development is drastically reduced, reducing the strain on the eyes of the developer and improving work efficiency.
Initially published 2021-05-14.

]]></content:encoded></item><item><title>ASML staffing changes could result in a net reduction of around 1700 positions</title><link>https://www.asml.com/en/news/press-releases/2026/strengthening-focus-on-engineering-and-innovation</link><author>dep_b</author><category>hn</category><pubDate>Wed, 28 Jan 2026 08:02:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Earlier today, as referenced in our FY 2025 results release, the ASML Board of Management shared the following internal message with employees. Â Dear ASML colleagues --Â Today we shared our full-year financial results for 2025, as well as our outlook for the year ahead. The semiconductor ecosystem is poised to experience significant growth in the coming years, and ASML is well positioned to leverage this positive development. On behalf of the Board of Management, I want to thank everyone for their contribution to this success.Â We can attribute our success to our customer dedication, engineering talent and collaborative approach to the ecosystem. Our ability to innovate and execute has generated substantial benefits for our customers and suppliers, our colleagues, and our investors. We intend to continue to grow our workforce and footprint, including at our planned second campus in Eindhoven, in line with customer demand.Â As with any company that grows rapidly, however, we need to be mindful that the way we have grown does not slow us down. The feedback from our colleagues, our suppliers and our customers shows that our ways of working have, in some cases, become less agile. Engineers in particular have expressed their desire to focus their time on engineering, without being hampered by slow process flows, and restore the fast-moving culture that has made us so successful.Â We believe it is important to address these issues so that we are well prepared for future growth and well positioned to continue to deliver for our customers. As a result, we are announcing today that we intend to strengthen our focus on engineering and innovation in critical areas of our company through the streamlining of the Technology and the IT organizations.Â In the Technology organization, we are proposing to shift from a project/matrix setup to one where most of our engineers will be dedicated to a specific product and module. This will allow us to simplify processes and decision-making. This need for simplification is something that we have heard consistently from all levels of the organization.Â We are safeguarding what makes us strong: a dedicated foundational team which will ensure that we continue to develop our deep technical competence, and drive fit-for-purpose commonality and standards across all engineering domains.Â As a result of these proposed changes, some roles -- mainly at the leadership level -- may no longer be required. At the same time, to retain our engineering capability, we will create new engineering jobs to strengthen existing technology projects and embark on new ones to support our own and our customersâ€™ growth plans. While this will allow some of our impacted colleagues to move to new roles, we have to acknowledge that some will leave ASML as a result.Â In addition to the Technology changes, we will also look at the setup of the IT & Data organization, similarly seeking ways to streamline its structure to optimize its delivery capabilities.Â In the coming weeks we will be working closely with our social partners in the Netherlands to discuss the intent and extent of these changes. At this stage, we believe the proposed changes could ultimately result in a net reduction of around 1,700 positions, mostly in the Netherlands, with some in the United States.Â The focus of these changes is on the Technology and the IT organizations. ASML continues to grow and will need to create roles as required to meet customer demand for new machines and servicing, including in Manufacturing, Customer Support and Sales.Â Of course, every colleague is someone that we value and appreciate: We are committed to acting responsibly - with care, speed, transparency, and fairness - and to supporting them through this change.Â We recognize that this news may create uncertainty and raise questions for many of you, but we believe strongly that it is important to be transparent in our approach. We will host all-employee meetings today to share more about the proposed changes. Further information sessions will be held for teams affected, and we commit to continuing to inform you all about what we can, when we can.Â As our FY 2025 financial results demonstrate, we are choosing to make these changes at a moment of strength for the company. Improving our processes and systems will allow us to innovate more and innovate better, generating further responsible growth for ASML and our stakeholders.Â Christophe, on behalf of the ASML Board of ManagementÂ ]]></content:encoded></item><item><title>Make.ts</title><link>https://matklad.github.io/2026/01/27/make-ts.html</link><author>ingve</author><category>hn</category><pubDate>Wed, 28 Jan 2026 07:35:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust at Scale: An Added Layer of Security for WhatsApp</title><link>https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/</link><author>ubj</author><category>hn</category><pubDate>Wed, 28 Jan 2026 06:21:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[2015 Android Vulnerability: A Wake-up Call for Media File ProtectionsHow Rust Fits In To WhatsAppâ€™s Approach to App Security]]></content:encoded></item><item><title>Android&apos;s desktop interface leaks</title><link>https://9to5google.com/2026/01/27/android-desktop-leak/</link><author>thunderbong</author><category>hn</category><pubDate>Wed, 28 Jan 2026 03:34:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A Google bug report on the Chromium Issue Tracker today has inadvertently leaked the Android desktop interface for the first time.The bug report published today about Chrome Incognito tabs was accompanied by two screen captures. From the description, we learn that the device being recorded is the HP Elite Dragonfly 13.5 Chromebook, board/codename â€œBrya(Redrix).â€ Pictured above, it runs a 12th Gen Intel Core (AlderLake-U) processor from 2021.Â The build number for â€œALOSâ€ â€” Aluminum OS is desktop Androidâ€™s codename â€” ZL1A.260119.001.A1. Another reference in the video identifies this as Android 16. Itâ€™s not surprising that Google internally is using existing Chromebook hardware to develop the new experience.DEVICE: Brya(Redrix) CHROME BUILD: 145.0.7587.4(Dev before upgrade) and 146.0.7634.0(Dev after upgrade) ALOS: ZL1A.260119.001.A1Compared to tablets and phone-projected desktop mode, the status bar is taller and more optimized for large screens. We see the time (with seconds) in the top row followed by the date. On the right side, we see the Android 16 M3E battery icon, Wi-Fi, a notification bell icon, â€œENâ€ (presumably representing the set keyboard language), Gemini icon, and screen recorder pill. The recording interface resembles the mobile versionHowever, the Taskbar is identical to what we have today, while the mouse cursor has been slightly modified to have a tail.The Google Chrome interface mostly aligns with the current large-screen Android version except for the Extensions button, which is currently only available on the desktop browser. We also see an example of split-screen multitasking.Â Meanwhile, desktop windowing is mostly unchanged from what we have today, with the app name at the left. The minimize, fullscreen, and close buttons at the top-right are similar to ChromeOS.Â Desktop experience on Android 16 QPR3 Beta 2:FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>Super Monkey Ball ported to a website</title><link>https://monkeyball-online.pages.dev/</link><author>rebasedoctopus</author><category>hn</category><pubDate>Wed, 28 Jan 2026 01:44:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Standard gameplay (Beginner / Advanced / Expert).Controls: WASD / Arrow Keys = tilt, R = reset stage, N = skip stageIf you have a controller plugged in, it should work too.Don't worry about reporting bugs. I probably already know about it - it'll get fixed.]]></content:encoded></item><item><title>AISLEâ€™s autonomous analyzer found all CVEs in the January OpenSSL release</title><link>https://aisle.com/blog/aisle-discovered-12-out-of-12-openssl-vulnerabilities</link><author>mmsc</author><category>hn</category><pubDate>Wed, 28 Jan 2026 01:38:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Autonomous zero-day discovery in one of the most scrutinized codebases in the worldAISLE's autonomous analyzer found all 12 CVEs in the January 2026 coordinated release of OpenSSL, the open-source cryptographic library that underpins a substantial proportion of the worldâ€™s secure communications. Some of these vulnerabilities had persisted in OpenSSL code for decades, evading the notice of thousands of security researchers.Finding a genuine security flaw in OpenSSL is extraordinarily difficult. Even a single accepted vulnerability represents a rare achievement. The library's maturity and the community's vigilance make new discoveries exceptionally uncommon. This makes the January 2026 release an important milestone for autonomous security systems. As TomÃ¡Å¡ MrÃ¡z, CTO of the OpenSSL Foundation, says,â€œOne of the most important sources of the security of the OpenSSL Library and open source projects overall is independent research. This release is fixing 12 security issues, all disclosed to us by AISLE. We appreciate the high quality of the reports and their constructive collaboration with us throughout the remediation.â€In this article, weâ€™ll give an overview of our discoveries and explain why we think this is a watershed moment for AI-powered software security. The AISLE Research Team started hunting for OpenSSL vulnerabilities with our autonomous analyzer in August 2025. You can read about the three discoveries we made in Q3 of 2025 here. All of our discoveries were reported through responsible disclosure and resolved through coordinated releases with the OpenSSL project.High and Moderate Severity CVEsCVE-2025-15467: Stack Buffer Overflow in CMS AuthEnvelopedData Parsing (High): A vulnerability with the potential to enable remote code execution under specific conditions   : PBMAC1 Parameter Validation in PKCS#12 (Moderate): Missing validation that could trigger a stack-based buffer overflow: Crash in QUIC protocol cipher handling  : Silent truncation bug affecting post-quantum signature algorithms (ML-DSA)  : Memory exhaustion via TLS 1.3 certificate compression  : Memory corruption in line-buffering (affects code back to OpenSSL 1.0.2)  : Encryption flaw in OCB mode on hardware-accelerated paths  : Memory corruption in PKCS#12 character encoding  : Crash in TimeStamp Response verification  : Crash in PKCS#12 decryption  : Crash in PKCS#12 parsing  : Crash in PKCS#7 signature verification (affects code back to OpenSSL 1.0.2)AISLEâ€™s analyzer also recommended fixes which were incorporated directly into OpenSSL for 5 of the 12 CVEs.Beyond CVEs: Catching Bugs Before They ShipIn addition to the 12 CVEs, 6 findings were never assigned a designation. In each case, AISLE detected the issue, reported it to the maintainers, and the fix was merged before the vulnerable code ever appeared in a release.By integrating autonomous analysis directly into development workflows, security issues were identified and resolved before they reached users. That is our goal: preventing vulnerabilities, not merely patching them after deployment.OpenSSL represents one of the most deployed, battle-tested, and carefully maintained open-source projects in existence. The fact that 12 previously unknown vulnerabilities could still be found there, including issues dating back to 1998, suggests that manual review faces significant limits, even in mature, heavily audited codebases.Human reviewers are constrained by time, attention, and the sheer volume of code in modern systems. Traditional static analysis catches certain bug classes but struggles with complex logic errors and timing-dependent issues. By contrast, autonomous AI-driven analysis operates at a different scale. It can examine code paths and edge cases that would take human reviewers months to cover, and it runs continuously rather than periodically.This doesn't mean that AI can replace human expertise. The OpenSSL maintainers' deep knowledge of the codebase was essential for validating findings and developing robust fixes. But it does change the SLA of security. When autonomous discovery is paired with responsible disclosure, it collapses the time-to-remediation for the entire ecosystem.The 12 OpenSSL vulnerabilities we identified, spanning 8+ subsystems from CMS to QUIC to post-quantum signatures, represent a milestone in our (admittedly ambitious) mission: moving from reactive patching to securing the software foundation that modern civilization depends on.Collaboration with OpenSSLFrom the moment our system flagged these anomalies, we approached this as a partnership with the OpenSSL community. We submitted detailed technical reports through their coordinated security reporting process, including complete reproduction steps, root cause analysis, and concrete patch proposals. In each case, our proposed fixes either informed or were directly adopted by the OpenSSL team.As Matt Caswell, Executive Director of the OpenSSL Foundation, said, â€œKeeping widely deployed cryptography secure requires tight coordination between maintainers and researchers. We appreciate AISLE's responsible disclosures and the quality of their engagement across these issues." The OpenSSL team's responsiveness was exceptional. Under the leadership of TomÃ¡Å¡ MrÃ¡z, the Chief Technical Officer (CTO) at the OpenSSL Foundation, the maintainers engaged technically at every stage: validating findings, refining patches, coordinating releases across multiple branches, and synchronizing with downstream distributions.For questions about AISLE's autonomous analyzer, reach out to us at .  Our appreciation goes to TomÃ¡Å¡ MrÃ¡z, Matt Caswell, Neil Horman, and the OpenSSL team for their collaboration throughout this process. AISLE researchers contributing to these discoveries include Stanislav Fort, Petr Å imeÄek, Tomas Dulka, and Luigino Camastra.]]></content:encoded></item><item><title>Trinity large: An open 400B sparse MoE model</title><link>https://www.arcee.ai/blog/trinity-large</link><author>linolevan</author><category>hn</category><pubDate>Wed, 28 Jan 2026 00:57:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[At the time, Trinity Nano Preview and Trinity Mini had just released, and Trinity Large had started training. We were in the middle of our first run so big that you either laughed or got nauseous. Frankly, I felt either weâ€™d end up with a really great base model or fall flat on our faces with a tired wallet.Little did I know, weâ€™d get both.Hereâ€™s what weâ€™re shipping, what surprised us, what broke, and what it took to make a 400B sparse MoE behave.We're putting out three variants: Trinity-Large- is lightly post-trained and chat-ready, Trinity-Large- is our best pretraining checkpoint after the full 17T recipe, and  is an early checkpoint from the same run at 10T tokens, without any instruct data or LR anneals. What many would consider a  base model.Trinity-Large is a 400B parameter sparse MoE with 13B active parameters per token. It uses  with 4 experts active per token. That sparsity ratio is pretty high compared to our peers, save for Llama-4-Maverick:We originally aimed for a slightly different total size (420B), but we ended up increasing the number of dense layers (from 3 to 6) to help keep routing stable at this sparsity.Trinity-Large-Base is a true frontier-class foundation model. We match and exceed our peers in open-base models across a wide range of benchmarks, including math, coding, scientific reasoning, and raw knowledge absorption.We trained on 2048 Nvidia B300 GPUs. As far as we can tell, itâ€™s the largest (publicly stated, at least) pretraining run done on these machines. That means two things:Therefore, we had to make the most of the money we allotted to these machines, which was just over  days. Ridiculously fast for a run of this scale, so efficient training was the name of the game. Hence, the level of sparsity referred to above. Combined with our efficient attention outlined in our technical report, this enabled us to train and, by extension, run  much faster than our peers in the same weight class. All while not sacrificing performance. Roughly 2-3x faster for the same hardware.Momentum-based expert load balancingWe keep MoE routing under control by nudging each expertâ€™s router bias up or down depending on whether that expert is being over- or under-used. The update is capped with a tanh clip so it stays bounded, and we add momentum to smooth it across steps and avoid step-to-step ping-pong. On top of that, we include a small per-sequence balance loss so load is not only balanced in expectation across the batch, but also within individual sequences.We use z-loss to stop the LM-head logits from drifting upward during training. It is a lightweight regularizer that keeps logit scale from creeping up without bound. We also log basic logit stats (for example max and mean) as a simple early warning for instability.Our fastest configuration was HSDP with expert parallelism set to 8, which gave us 2048 data-parallel ranks. In that setup, we pushed throughput further by increasing batch size after 5T tokens of training. We were comfortable doing that because the model is highly sparse, Muon supports a larger critical batch size than AdamW, and the MiniMax-01 paper suggests batch-size scaling remains workable in this regime.In the main run, once we had stability dialed in, the loss curve stayed smooth the whole way through. You can see clear phase transitions, no spikes, and a steady march to the end.The full pretraining run finished in 33 days. That's pre-training only; it doesn't include context extension or post-training.Trinity Large was trained on 17T tokens of data curated by DatologyAI, split across three phases of 10T, 4T, and 3T tokens. This mix uses state-of-the art programming, STEM, reasoning, and multilingual data curation, targeting 14 non-English languages. Notably, over 8  tokens of synthetic data were generated for this dataset across web, code, math, reasoning, and multilingual domains, using a breadth of state-of-the-art rephrasing approaches.A less advanced version of this curation approach worked well for smaller models like Trinity Nano and Trinity Mini, but we wanted to shoot for the moon with Trinity Large. So DatologyAI delivered a number of curation advancements specifically for inclusion into Trinity Large. Thereâ€™s always a leap of faith when doing something for the first time in public, but the effectiveness of the curation approach as a whole is reflected in the downstream evaluations, where the Trinity Large-Base demonstrates frontier-level performance across the targeted capability domains.The preview weâ€™re releasing today is  a reasoning model. A benefit beyond production inference efficiency is that it also carries over into RL, enabling quicker rollouts for a given total parameter size. But itâ€™s equally as sensitive as pretraining was, and while itâ€™s currently undergoing further post-training and will be a full-fledged reasoning model upon release, we believe thereâ€™s a fine line between intelligence and , and while the reasoning variant is very intelligent, it needs longer in training before it becomes maximally useful given the extra tokens per output.As such, to maximize usefulness and provide an early checkpoint, Preview is a non-reasoning, or â€œinstruct,â€ model. Itâ€™s a particularly  post-training, as most of our compute went towards pre-training and is continuing with the reasoning version. It is an extremely capable model for its size and gave us the opportunity to flex some old-school post-training muscles we havenâ€™t had the chance to flex in quite some time. It excels in creative writing, storytelling, role-play, chat scenarios, and real-time voice assistance, better than your average reasoning model usually can. But weâ€™re also introducing some of our newer agentic performance. It was trained to navigate well in agent harnesses like OpenCode, Cline, and Kilo Code, and to handle complex toolchains and long, constraint-filled prompts. It certainly isnâ€™t perfect, but we cannot wait to see what you do with it. Itâ€™s free in OpenRouter until Large (non-preview) fully releases.Itâ€™s currently roughly in line with Llama-4-Maverickâ€™s Instruct model across standard academic benchmarks, and weâ€™ll update this blog over time with more evaluations.But we like to tease, so Iâ€™ll leave you with some early evaluations of the reasoning Trinity-Large. These are not exhaustive, and by no means representative of the full capabilities of any of these models, but it is a fun look at how we plan to take this model.When we started this run, we had never pretrained anything remotely like this before.There was no guarantee this would work. Not the modeling, not the data, not the training itself, not the operational part where you wake up, and a job that costs real money is in a bad state, and you have to decide whether to restart or try to rescue it.All inâ€”compute, salaries, data, storage, opsâ€”we pulled off this entire effort for . 4 Models got us here in 6 months.That number is big for us. It's also small compared to what frontier labs spend just to keep the lights on. We don't have infinite retries.One more thing about Trinity-Large-TrueBase.Most "base" releases have some instruction data baked in. TrueBase doesn't. It's 10T tokens of pretraining on a 400B sparse MoE, with no instruct data and no LR annealing.If you're a researcher who wants to study what high-quality pretraining produces at this scaleâ€”before any RLHF, before any chat formattingâ€”this is one of the few checkpoints where you can do that. We think there's value in having a real baseline to probe, ablate, or just observe. What did the model learn from the data alone? TrueBase is where you answer that question. has Trinity-Large-Preview available now, free during the preview period (through at least February 2026). If you want to kick the tires without spinning up infrastructure, that's the fastest path.We also worked with , , and  to have integrations ready at launch. If you're already using one of those for coding, Trinity Large should show up as an option. This is an extremely young post-train, in the scheme of RL runs that go on for months at a time, like our peers. Weâ€™ll get there soon, but weâ€™re oh-so-proud to have our own model to do it with. Expect rough edges, specifically in coding agents. For everyday agents, though, itâ€™s outstanding.Trinity Large natively supports .The preview API is running at 128k context with 8-bit quantization as we tune our inference infrastructure. This release is as much a preview of our hosting platform as it is a model launch.If you put this model into something real and it breaks, tell us. The fastest way for open models to get better is for people to actually use them, hard, in places that don't look like benchmarks.We like to say that we built Trinity so you can own it. Being able to say that about a frontier-level model is something weâ€™re immeasurably proud of.]]></content:encoded></item><item><title>Did a celebrated researcher obscure a baby&apos;s poisoning?</title><link>https://www.newyorker.com/magazine/2026/02/02/did-a-celebrated-researcher-obscure-a-fatal-poisoning</link><author>littlexsparkee</author><category>hn</category><pubDate>Wed, 28 Jan 2026 00:18:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Opioids kill by suppressing the drive to breathe. They bind to receptors in the brain stem, altering the neurons that maintain patterns of respiration. Carbon dioxide builds up in the bloodstream, hypoxia sets in, and circulation falters. Brain damage follows, then death.The coronerâ€™s office asked one of Canadaâ€™s leading pediatricians and toxicologists, Gideon (Gidi) Koren, to examine Tariqâ€™s file. For the past two decades, Koren had been running a program at the Hospital for Sick Children called Motherisk, which provided guidance for pregnant women and new mothers about drugs and breast-feeding. He was widely considered to be among the most capable research scientists in the field. Koren met with Raniâ€™s physician and quickly ruled out foul play. â€œThere was no evidence of psychiatric issues,â€ he later wrote. Instead, Koren interpreted the toxicology report as a scientific revelation: if mothers with a certain genetic predisposition took even a mild dose of codeine, the amount of morphine that ended up in their breast milk could kill their children.A dose of codeine brings relief from pain only when the liver metabolizes a fraction of it into morphine. But the exact proportion that is converted into morphine can vary. Most people have two copies of the gene that carries out the conversion. Koren invited Rani to be tested, and discovered that she had three.The concentration of morphine in Tariqâ€™s blood was measured at seventy nanograms per millilitre. â€œIf you have levels above twenty, you stop breathing,â€ Koren later said. Six months after Tariqâ€™s death, Koren obtained a sample of Raniâ€™s breast milk, which she had kept in her freezer. His lab measured its morphine concentration at eighty-seven nanograms per millilitre. Koren was stunned. â€œThe level was several fold higher than ever described in the literature,â€ he noted. â€œThis was the first time in history that a baby was reported dying from breast milk.â€Koren had long studied the transmission of opioids into breast milk. But he had never identified a mortal risk. Now, along with a few colleaguesâ€”including the deputy chief coroner of Ontario, James Cairnsâ€”he published his findings in , one of the worldâ€™s top medical journals. Some women, like Rani, have a genetic predisposition to convert codeine into morphine faster and in higher quantities than the rest of the general population. Therefore, the authors concluded, â€œcodeine cannot be considered as a safe drug for all infants during breastfeeding.â€The implications were terrifying. Millions of womenâ€”up to forty per cent of breast-feeding mothers in North America, according to Koren and his colleaguesâ€”might be prescribed codeine for postpartum pain, and yet almost none were being tested to see if they, like Rani, had more than the usual number of codeine-metabolizing genes. The risk was unevenly distributed across the population, according to ethnic background. Mothers from Finland have a one in a hundred chance of being so-called ultra-rapid metabolizers, according to Korenâ€™s paper. But in Ethiopia the odds can rise to almost one in three.Few academic-journal articles have had so abrupt an effect on the daily practice of medicine. Prior to its publication, the American Academy of Pediatrics had listed codeine as generally compatible with breast-feeding. â€œAfter we published it in , the F.D.A.â€”the Food and Drug Administrationâ€”said, â€˜This is enough for us to change labelling,â€™Â â€ Koren said. Canadian and European health regulators soon followed suit. Doctors started prescribing other opioids for postpartum pain, such as hydromorphone and oxycodone, whose metabolic pathways are more predictable and less subject to genetic variations.The Jamiesonsâ€™ identities were not revealed in Korenâ€™s article. But they went public on April 30, 2007â€”exactly two years after Tariqâ€™s deathâ€”filing a class-action lawsuit against Johnson & Johnson and a subsidiary, the manufacturer of Tylenol-3, on behalf of â€œall persons in Canadaâ€ who had ingested the products of the drug through breast milk. â€œThis terrible tragedy should never have occurred, and I am determined to see that this does not happen to other children,â€ Rani said. â€œWhat can they give me? Can they give me my son back? I want other people not to have their children die or be damaged.â€The Jamiesons went on to have three more childrenâ€”all boys, who grew up in the shadow of the brother they never met. â€œYouâ€™re consumed with a certain sadness thatâ€™s always there,â€ Rani told a reporter, seven years after Tariqâ€™s death. Two decades later, she still finds April the most difficult month: â€œItâ€™s always there.â€Koren continued to sound the alarm about codeine for years. â€œHe was always on a plane somewhere, and always had a million spinning platesâ€”meetings, talks, conferences,â€ David Juurlink, a Canadian clinical toxicologist and a colleague of Korenâ€™s at the Hospital for Sick Children, told me. Koren published more papers about the Jamieson case, and his Motherisk program provided data for studies of patients who had been prescribed codeine for postpartum pain. His ability to distill complex scientific processes into clear public-health messaging made him a regular commentator in the press. â€œItâ€™s quite common not to know why a baby dies,â€ Koren said, in an interview for Canadian television. â€œI am quite sure that quite a few of those were codeine in breast milk. But no one checked. You donâ€™t check, you do not find.â€Juurlink first met Koren in the late nineties, when he was a resident in clinical pharmacology. Koren, who had been practicing medicine in Canada since 1982, was leading rounds. â€œWhen you were with Gidi, you really felt like you were in the presence of someone who wasnâ€™t just an expertâ€”in the truest sense of the wordâ€”but was also a kind, good-natured, thoughtful, and intellectually agile man,â€ Juurlink recalled. â€œHe was very avuncular. It was really one of the highlights of my training, learning from him.â€ Koren was revered by colleagues, and he had almost six hundred publications in scientific journals.]]></content:encoded></item><item><title>Some notes on starting to use Django</title><link>https://jvns.ca/blog/2026/01/27/some-notes-on-starting-to-use-django/</link><author>ingve</author><category>hn</category><pubDate>Tue, 27 Jan 2026 22:58:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hello! One of my favourite things is starting to learn an
Old Boring Technology that Iâ€™ve never tried before but that has been around for
20+ years. It feels really good when every problem Iâ€™m ever going to have has
been solved already 1000 times and I can just get stuff done easily.Iâ€™ve thought it would be cool to learn a popular web framework like
Rails or Django or Laravel for a long time, but Iâ€™d never really managed to
make it happen. But I started learning Django to make a website a few months
back, Iâ€™ve been liking it so far, and here are a few quick notes!I spent some time trying to learn Rails in 2020,
and while it was cool and I really wanted to like Rails (the Ruby community is great!),
I found that if I left my Rails project alone for months, when I came
back to it it was hard for me to remember how to get anything done because
(for example) if it says  in your , on its own
that doesnâ€™t tell you where the  routes are configured, you need to
remember or look up the convention.Being able to abandon a project for months or years and then come back to it is
really important to me (thatâ€™s how all my projects work!), and Django feels easier
to me because things are more explicit.In my small Django project it feels like I just have 5 main files (other
than the settings files): , , , , and
, and if I want to know where something else is (like an HTML template)
is then itâ€™s usually explicitly referenced from one of those files.For this project I wanted to have an admin interface to manually edit or view
some of the data in the database. Django has a really nice built-in admin
interface, and I can customize it with just a little bit of code.For example, hereâ€™s part of one of my admin classes, which sets up which fields
to display in the â€œlistâ€ view,  which field to search on, and how to order them
by default.@admin.register(Zine)
class ZineAdmin(admin.ModelAdmin):
    list_display = ["name", "publication_date", "free", "slug", "image_preview"]
    search_fields = ["name", "slug"]
    readonly_fields = ["image_preview"]
    ordering = ["-publication_date"]
In the past my attitude has been â€œORMs? Who needs them? I can just write my own SQL queries!â€.
Iâ€™ve been enjoying Djangoâ€™s ORM so far though, and I think itâ€™s cool how Django
uses  to represent a , like this:Zine.objects
    .exclude(product__order__email_hash=email_hash)
This query involves 5 tables: , , , , and .
To make this work I just had to tell Django that thereâ€™s a 
relating â€œordersâ€ and â€œproductsâ€, and another  relating
â€œzinesâ€, and â€œproductsâ€, so that it knows how to connect , , .I definitely  write that query, but writing product__order__email_hash is
a lot less typing, it feels a lot easier to read, and honestly I think it would
take me a little while to figure out how to construct the query
(which needs to do a few other things than just those joins).I have zero concern about the performance of my ORM-generated queries so Iâ€™m
pretty excited about ORMs for now, though Iâ€™m sure Iâ€™ll find things to be
frustrated with eventually.The other great thing about the ORM is migrations!If I add, delete, or change a field in , Django will automatically
generate a migration script like migrations/0006_delete_imageblob.py.I assume that I could edit those scripts if I wanted, but so far Iâ€™ve just
been running the generated scripts with no change and itâ€™s been going great. It
really feels like magic.Iâ€™m realizing that being able to do migrations easily is important for me right
now because Iâ€™m changing my data model fairly often as I figure out how I want
it to work.For example the intro to models
lists the most important common fields you might want to set when using the ORM.After having a bad experience trying to operate Postgres and not being able to
understand what was going on, I decided to run all of my small websites with
SQLite instead. Itâ€™s been going way better, and I love being able to backup by
just doing a  and then copying the resulting single file.I think it should be fine because Iâ€™m expecting the site to have a few hundred
writes per day at most, much less than Mess with DNS
which has a lot more of writes and has been working well (though the writes are
split across 3 different SQLite databases).
      built in email (and more)
    Django seems to be very â€œbatteries-includedâ€, which I love â€“ if I want CSRF
protection, or a , or I want to send email, itâ€™s all
in there!For example, I wanted to save the emails Django sends to a file in dev mode (so
that it didnâ€™t send real email to real people), which was just a little bit
of configuration.I just put this :EMAIL_BACKEND = "django.core.mail.backends.filebased.EmailBackend"
EMAIL_FILE_PATH = BASE_DIR / "emails"
and then set up the production email like this in EMAIL_BACKEND = "django.core.mail.backends.smtp.EmailBackend"
EMAIL_HOST = "smtp.whatever.com"
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = "xxxx"
EMAIL_HOST_PASSWORD = os.getenv('EMAIL_API_KEY')
That made me feel like if I want some other basic website feature, thereâ€™s
likely to be an easy way to do it built into Django already.
      the settings file still feels like a lot
    Iâ€™m still a bit intimidated by the  file: Djangoâ€™s settings system
works by setting a bunch of global variables in a file, and I feel a bit
stressed aboutâ€¦ what if I make a typo in the name of one of those variables?
How will I know? What if I type WSGI_APPLICATOIN = "config.wsgi.application"
instead of ?I guess Iâ€™ve gotten used to having a Python language server tell me when Iâ€™ve
made a typo and so now it feels a bit disorienting when I canâ€™t rely on the
language server support.I havenâ€™t really successfully used an actual web framework for a project before
(right now almost all of my websites are either a single Go binary or static
sites), so Iâ€™m interested in seeing how it goes!Thereâ€™s still lots for me to learn about, I still havenâ€™t really gotten into
Djangoâ€™s form validation tooling or authentication systems.Thanks to Marco Rogers for convincing me to give ORMs a chance.]]></content:encoded></item><item><title>Thief of $90M in seized U.S.-controlled crypto is gov&apos;t contractor&apos;s son</title><link>https://www.web3isgoinggreat.com/single/lick-theft</link><author>pavel_lishin</author><category>hn</category><pubDate>Tue, 27 Jan 2026 21:54:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I wrapped the Zorks with an LLM</title><link>https://infocom.tambo.co/</link><author>alecf</author><category>dev</category><category>hn</category><pubDate>Tue, 27 Jan 2026 20:59:49 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TikTok settles just before social media addiction trial to begin</title><link>https://www.bbc.com/news/articles/c24g8v6qr1mo</link><author>ourmandave</author><category>hn</category><pubDate>Tue, 27 Jan 2026 20:38:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA["Unfortunately, there are all too many kids in the United States, the UK, and around the world who are suffering as KGM does because of the dangerous and addictive algorithms that the social media platforms foist on unsuspecting kids," he said.]]></content:encoded></item><item><title>Parametric CAD in Rust</title><link>https://campedersen.com/vcad</link><author>ecto</author><category>hn</category><pubDate>Tue, 27 Jan 2026 20:36:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I keep designing physical parts for our robots. Motor mounts, sensor brackets, wheel hubs. Every time, the workflow is the same: open a GUI CAD program, click around for an hour, export an STL, realize the bolt pattern is 2mm off, repeat.I wanted to write my parts the way I write firmware. In . With types. With version control. With the ability to change one number and regenerate everything.A part is just geometry with a name. You create primitives, combine them with boolean operations, and export. That's it.That minus sign is a real .  is union.  is intersection. Operator overloads make CSG feel like arithmetic.The plate above has a center bore, four corner mounting holes, and a six-bolt circle pattern. Twelve lines of code. One STL file. Done.The API is small on purpose. Primitives, booleans, transforms, patterns. That's the whole language. But it composes well.An  with mounting holes in both faces:A  with a bolt circle:A  pattern cut from a disc â€” one slot, repeated eight times:Every part here is . Change the bolt count, the radius, the wall thickness â€” one number changes and the whole part regenerates. No clicking. No undo. Just recompile.STL is fine for 3D printing, but it throws away all material information. For visualization, vcad exports  scenes with PBR materials defined in TOML:That's our mascot. Entirely CSG. A rounded cube intersected with a sphere for the body, spheres for eyes, cylinders for arms. Eight materials, seventeen parts, one GLB file.The geometry engine is manifold, which guarantees watertight meshes from boolean operations. The Rust bindings give us zero-cost abstractions over the C++ core â€” the operator overloads compile down to direct manifold calls. No garbage collection pauses. No floating point surprises from a scripting layer.But honestly, the main reason is .  runs 21 unit tests that verify volumes, surface areas, bounding boxes, and export round-trips.  catches issues before they become parts with holes in the wrong place. Types prevent you from passing a radius where a diameter was expected.CAD files  be code. Code has tests, reviews, diffs, and CI. An STL file has... bytes.One thing I care about that most CAD tools don't: vcad is designed to be used by .The README has full API tables, a cookbook with copy-pasteable recipes, and a section on Blender MCP integration. An agent can read the docs, generate a part, export it, import it into Blender, position a camera, and render a preview â€” all in one conversation.Every render in this post was made that way. Claude generated the geometry with vcad, imported each STL/GLB into Blender via MCP, set up studio lighting, and rendered to PNG. No human touched Blender.The feedback loop is: describe a part â†’ code generates â†’ mesh exports â†’ render previews â†’ iterate. All in the terminal.It's MIT licensed. The first version is 0.1 â€” there's a lot more to build. Fillets, chamfers, threads, an interactive web GUI. But the core is solid: primitives, booleans, transforms, export. Everything you need to stop clicking and start typing.]]></content:encoded></item><item><title>Time Station Emulator</title><link>https://github.com/kangtastic/timestation</link><author>FriedPickles</author><category>hn</category><pubDate>Tue, 27 Jan 2026 20:35:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Try text scaling support in Chrome Canary</title><link>https://www.joshtumath.uk/posts/2026-01-27-try-text-scaling-support-in-chrome-canary/</link><author>linolevan</author><category>hn</category><pubDate>Tue, 27 Jan 2026 19:20:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Lennart Poettering, Christian Brauner founded a new company</title><link>https://amutable.com/about</link><author>hornedhob</author><category>hn</category><pubDate>Tue, 27 Jan 2026 18:57:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Founder, Chief Technical OfficerMaintainer of the VFS subsystem in LinuxFormer founder and CEO of Kinvolk, acquired by Microsoft]]></content:encoded></item><item><title>U.S. government has lost more than 10k STEM PhDs since Trump took office</title><link>https://www.science.org/content/article/u-s-government-has-lost-more-10-000-stem-ph-d-s-trump-took-office</link><author>j_maffe</author><category>hn</category><pubDate>Tue, 27 Jan 2026 18:35:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Clawdbot Renames to Moltbot</title><link>https://github.com/moltbot/moltbot/commit/6d16a658e5ebe6ce15856565a47090d5b9d5dfb6</link><author>philip1209</author><category>hn</category><pubDate>Tue, 27 Jan 2026 18:08:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Prism</title><link>https://openai.com/index/introducing-prism</link><author>meetpateltech</author><category>hn</category><pubDate>Tue, 27 Jan 2026 18:03:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: LemonSlice â€“ Upgrade your voice agents to real-time video</title><link>https://news.ycombinator.com/item?id=46783600</link><author>lcolucci</author><category>dev</category><category>hn</category><pubDate>Tue, 27 Jan 2026 17:55:15 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hey HN, we're the co-founders of LemonSlice (try our HN playground here: https://lemonslice.com/hn). We train interactive avatar video models. Our API lets you upload a photo and immediately jump into a FaceTime-style call with that character. Here's a demo: https://www.loom.com/share/941577113141418e80d2834c83a5a0a9Chatbots are everywhere and voice AI has taken off, but we believe video avatars will be the most common form factor for conversational AI. Most people would rather watch something than read it. The problem is that generating video in real-time is hard, and overcoming the uncanny valley is even harder.We havenâ€™t broken the uncanny valley yet. Nobody has. But weâ€™re getting close and our photorealistic avatars are currently best-in-class (judge for yourself: https://lemonslice.com/try/taylor). Plus, we're the only avatar model that can do animals and heavily stylized cartoons. Try it: https://lemonslice.com/try/alien. Warning! Talking to this little guy may improve your mood.Today we're releasing our new model* - Lemon Slice 2, a 20B-parameter diffusion transformer that generates infinite-length video at 20fps on a single GPU - and opening up our API.How did we get a video diffusion model to run in real-time? There was no single trick, just a lot of them stacked together. The first big change was making our model causal. Standard video diffusion models are bidirectional (they look at frames both before and after the current one), which means you can't stream.From there it was about fitting everything on one GPU. We switched from full to sliding window attention, which killed our memory bottleneck. We distilled from 40 denoising steps down to just a few - quality degraded less than we feared, especially after using GAN-based distillation (though tuning that adversarial loss to avoid mode collapse was its own adventure).And the rest was inference work: modifying RoPE from complex to real (this one was cool!), precision tuning, fusing kernels, a special rolling KV cache, lots of other caching, and more. We kept shaving off milliseconds wherever we could and eventually got to real-time.We set up a guest playground for HN so you can create and talk to characters without logging in: https://lemonslice.com/hn. For those who want to build with our API (we have a new LiveKit integration that weâ€™re pumped about!), grab a coupon code in the HN playground for your first Pro month free ($100 value). See the docs: https://lemonslice.com/docs. Pricing is usage-based at $0.12-0.20/min for video generation.Looking forward to your feedback!EDIT: Tell us what characters you want to see in the comments and we can make them for you to talk to (e.g. Max Headroom)]]></content:encoded></item><item><title>FBI is investigating Minnesota Signal chats tracking ICE</title><link>https://www.nbcnews.com/tech/internet/fbi-investigating-minnesota-signal-minneapolis-group-ice-patel-kash-rcna256041</link><author>duxup</author><category>hn</category><pubDate>Tue, 27 Jan 2026 17:32:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI2: Open Coding Agents</title><link>https://allenai.org/blog/open-coding-agents</link><author>publicmatt</author><category>hn</category><pubDate>Tue, 27 Jan 2026 17:17:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Over the past year, coding agents have transformed how developers write, test, and maintain software. These systems can debug, refactor, and even submit pull requestsâ€”fundamentally changing what software development looks like. Yet despite this progress, most coding agents share the same constraints: they're closed, expensive to train, and difficult to study or adapt to private codebases. change that. Today weâ€™re releasing not just a collection of strong open coding models, but a training method that makes building your own coding agent for any codebase â€“ for example, your personal codebase or an internal codebase at your organization â€“ remarkably accessible for tasks including code generation, code review, debugging, maintenance, and code explanation.Closed models haven't seen your internal code, so they don't know itâ€”custom data pipelines, internal APIs, specific org conventions, and so on. Training on your private data teaches them, but generating synthetic training data from private codebases that works for agents has been challenging and cost-prohibitive. Our method makes it easyâ€”reproducing the performance of the previously best open-source model costs ~$400 of compute, or up to $12,000 for performance that rivals the best industry models of the same size. This puts the full recipe within reach for labs and small teams.Â Resource constraints drove us to maximize efficiency at every stage, from data quality to inference costs to model selection. The result: we match SWE-smith, a synthetic data method, at 57Ã— lower cost and SkyRL, an open-source reinforcement learning (RL) system, at 26Ã— lower cost.The first release in our Open Coding Agents family is  (Soft-verified Efficient Repository Agents). The strongest â€“ SERA-32B â€“ solves 54.2% of SWE-Bench Verified problems,Â  surpassing prior open-source state-of-the-art coding models of comparable sizes and context lengths while requiring only 40 GPU days (or fewer) to train on a cluster of 2 NVIDIA Hopper GPUs or NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. SERA models are optimized and compatible with Claude Code out of the box. With our fine-tuning method, you can specialize them to your own codebase including your full engineering stack and conventions quickly and at low cost.We collaborated with NVIDIA to optimize SERA inference for their accelerated infrastructure, ensuring researchers and developers can get the most out of these models in production environments. Early benchmarks are promising: running in BF16 precision on 4xH100 GPUs, SERA achieves approximately 1,950 peak output tokens per second with a 16k context window. At FP8 precision, SERA reaches 3,700 peak output tokens per secondâ€”a higher throughput at almost negligible accuracy drop. On next-generation Blackwell 4xB200 systems running in NVFP4, SERA scales further to around 8,600 peak output tokens per second.Every component of this release is open â€“ models, Claude Code integration, and training recipes â€“ and can be launched with a single line of code, making it easy to use even for those without LLM training experience. We're also releasing state-of-the-art training data so researchers can inspect what worked and push it further, and conduct deep science while avoiding the many stumbling blocks, dead ends, and other roadblocks typical of coding agents.One result we're especially excited about: SERA uniquely enables adapting to private datasets like internal codebases, and we see evidence that a smaller, open model can replicate and possibly even exceed the performance of a more capable "teacher" coding agent in these setups. For example, SERA-32B can surpass its 110B parameter teacher (GLM-4.5-Air) on codebases like Django and Sympy after training on just 8,000 samples at a cost of $1,300.Â Accessible open models can now inherit strong agentic behavior through a simple, reproducible pipelineâ€”no large-scale RL infrastructure or engineering team required. Case in point, SERA was built largely by a single Ai2 researcher.The challenge: specializing agents to your dataIf youâ€™re a small to mid-sized business or independent developer, you probably have code that works with customer data in ways no public model has ever seen. Training on that data would help, but generating agent-ready synthetic data from private codebases has been the hard part. The holy grail would be a method that yields state-of-the-art training data for any codebase, with minimal setup and clear evidence that the tuned model is actually learning agentic behavior versus fragile heuristics.We tackle this challenge with our new post-training approach that achieves state-of-the-art open-source results on SWE-Bench at a fraction of the typical training costs. Two innovations make it both inexpensive and effective:Soft-verified generation (SVG). Synthetic training data generation, which is key to training a strong coding agent, is usually done by generating pairs of code examples that have both incorrect and corrected code. From these examples, the coding agent can learn how to transform incorrect code into correct code by generating a patch with line-by-line code changes. Usually, these examples need to be carefully tested to ensure that theyâ€™re actually correct. In SVG, our main finding is that patches donâ€™t need to be correct to be helpful for coding. Just like different code can lead to the same, correct solution, with SVG we generate synthetic training data by having patches that are only partially correct. This removes the need to thoroughly test for full correctness, which in turn alleviates the need for complex infrastructure for testing and costly generation of precise examples. We demonstrate that this soft-verified data scales exactly like "hard-verified" training data.Scaling with a bug-type menu. To diversify data without becoming bottlenecked on finding real bugs, we draw from a taxonomy of 51 common bug patterns identified in prior analyses. For each function in a repository, we can generate multiple distinct bug-style promptsâ€”so a repo with thousands of functions can yield tens of thousands of varied agentic trajectories at low cost.High simulated workflow fidelity. A key finding is that high-quality synthetic training data should mirror the workflow of a developer rather than the precise details of correct code. This means correct coding data is less important than data that reflects how a developer works on a problem. Combined with SVG, this insight enables repository traininggenerating training data for any code repository, making it straightforward to scale synthetic data generation massively.Together, these innovations mean that if you or your organization has a private codebase, you can use SERA to fine-tune a small model to strong performance on your dataâ€”easily and affordably. Instead of designing a complicated RL pipeline and test harness for every new task setting, you generate targeted synthetic data and run a straightforward supervised fine-tuning (SFT) job.State-of-the-art performance, accessible hardwareUsing SERA, we've developed a family of models ranging from 8B to 32B parameters, all built on Qwen3 and trained up to 32K context length with the help of various teacher models. We expect the same recipe to keep improving as we scale to larger backbones and context lengths, but the key point is that the current pipeline is already cheap and feasible for anyone to run, customize, and iterate on todayâ€”opening up wide access and endless possibilities for future research.Our efficient technique enabled highly precise science. By keeping costs low, we could systematically disentangle the many factors that have made comparisons between agentic systems unreliable. This rigorous methodology drove rapid iteration, leading us from soft-verified generation to the full SERA approach.Â When we align inference conditions for fair comparison, SERA performs competitively with leading open coding agents. At 32K context,  achieves 49.5% Â± 1.9% on SWE-Bench Verified, comparable to Devstral Small 2 (50.0% Â± 1.3%) and GLM-4.5-Air (50.5% Â± 1.3%). At 64K context, SERA-32B reaches 54.2% Â± 1.4%â€”competitive with longer-context baselines.Â Strong open-weight coding agents like Devstral Small 2 are an important point of comparison. When we control for key variables, SERA-32B comes close: within ~0.5 points at 32K and ~4.9 points at 64K compared to Devstral Small 2 despite SERA being pure SFT and not trained beyond 32K tokens, both of which disadvantage longer-context evaluation.Â We also explored how teacher strength affects results. GLM-4.6 yields our best numbers, but GLM-4.5-Air gets surprisingly close at lower cost. The gap between teachers becomes most meaningful in higher-compute regimesâ€”suggesting that depending on your budget and target performance, a weaker (and cheaper) teacher can be the better overall choice, especially for early iterations.To validate our synthetic data generation strategy, we tested repository-specific specialization on Django, SymPy, and Sphinxâ€”the three largest repositories in SWE-Bench. Because these have actual test instances, we can quantify how well specialization works in practice. This serves as a proxy for the downstream use case we care most about: adapting to private codebases that may lack comprehensive tests or follow nonstandard structures.The results are promising. Our specialized models â€“ trained on 8,000 synthetic trajectories per repository â€“ consistently match and often exceed the performance of the 100B+ parameter models we used as teachers. At 32K context, the specialized models achieve 52.23% on Django and 51.11% on SymPy, compared to GLM-4.5-Air's 51.20% and 48.89%. The gains are most pronounced on Django and SymPy, which together account for over 60% of all SWE-Bench problems.These results highlight two crucial advantages of our method. First, specialization pays off: a 32B model fine-tuned to a specific codebase can match or surpass a 100B+ general-purpose teacher, delivering comparable performance at one-third the size with lower memory requirements, faster inference, and reduced operational costs. Second, simplicity scales: our SFT-only pipeline on an open base model is now competitive with heavily engineered, large-team efforts. Together, these findings lower the barrier to entry for researchers, make results easier to reproduce, and turn agentic coding progress into something the whole community can validate and build on.Built for developers and researchersOur release package includes everything needed to reproduce, test, and build on SERAâ€”a lightweight deployment requiring just two lines of code to launch an inference server. We've also developed a setup script and inference optimizations that make SERA directly compatible with Claude Code.A key difference from closed-weight systems is our commitment to openness and reproducibility:We release models, code, all generated agent data, and a full recipe to generate your own data so anyone can reproduce our results or customize them to new domains.Â Our training pipeline is â€”standard SFT on trajectories with no custom RL infrastructure needed.Â The total cost to reproduce performance levels of the best previous open-source result only is  on commodity cloud GPUs, more than 25 times cheaper than many existing approaches that require complex distributed setups and still fall short on performance.The total cost to reproduce top open-weight models in industry, such as Devstral Small 2, is .We believe bringing the cost of replicating strong coding agents down to a few hundred dollars will unlock research that simply wasn't possible before. Instead of being limited to a handful of well-funded labs, agentic coding can become a widely accessible practice.Whether you're running locally on your hardware, deploying in the cloud, or fine-tuning on your own codebase, SERA delivers practical agentic coding within reach of developers, researchers, and small teams alike.]]></content:encoded></item><item><title>SoundCloud Data Breach Now on HaveIBeenPwned</title><link>https://haveibeenpwned.com/Breach/SoundCloud</link><author>gnabgib</author><category>hn</category><pubDate>Tue, 27 Jan 2026 17:11:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In December 2025, SoundCloud announced it had discovered unauthorised activity on its platform. The incident allowed an attacker to map publicly available SoundCloud profile data to email addresses for approximately 20% of its users. The impacted data included 30M unique email addresses, names, usernames, avatars, follower and following counts and, in some cases, the userâ€™s country. The attackers later attempted to extort SoundCloud before publicly releasing the data the following month.]]></content:encoded></item><item><title>Amazon to shut down Go and Fresh stores</title><link>https://www.cnn.com/2026/01/27/food/amazon-fresh-go-closures</link><author>gmays</author><category>hn</category><pubDate>Tue, 27 Jan 2026 15:58:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
            Amazon is closing its branded brick-and-mortar grocery stores, with the company consolidating its physical strategy under the Whole Foods name.
    
            The closures, which affect its Amazon Fresh and Amazon Go locations, were announced Tuesday and amount to about 70 locations across the United States. In a blog post, the company said while it saw positive signs in the stores, it didnâ€™t create a â€œtruly distinctive customer experience with the right economic model needed for large-scale expansion.â€
    
            As a result, some locations will be converted into a Whole Foods Market, with an additional 100 locations opening in coming years. That will include the expansion of its new, smaller spinoff called Whole Foods Market Daily Shop.
    
            â€œWeâ€™re grateful to our team members for their many contributions over the years and are working whenever possible to help them find roles elsewhere in Amazon, including across our vast operations network, as we make this transition,â€ the company said.
    
            Amazon said itâ€™s â€œtesting new physical store experiences,â€ including a store-within-a-store concept in Illinois that combines Amazon Grocery and Whole Foods Market. In a Chicago suburb, itâ€™s also building a 229,000-square-foot retail space that resembles a Walmart.
    
            However, the issue for Fresh and Go is that they didnâ€™t offer a compelling reason for shoppers to visit, according to Neil Saunders, managing director of GlobalData. Fresh was â€œtoo similarâ€ to its competitors and Goâ€™s just-walk-out technology isnâ€™t something shoppers â€œreally care about,â€ he wrote in a note.
    
            Itâ€™s the latest change to Amazonâ€™s grocery strategy and options, which have had varying degrees of success. The company has partnered with local grocery stores for delivery, but those items took customers into different online stores with separate checkouts and delivery fees that sometimes left shopper confused.
    
            The company has also built up its its own private label brands like Amazon Grocery and Amazon Saver to take on the expansion of low-cost competitors Aldi and Lidl and recently rolled out same-day delivery of fresh food to more than 2,000 cities.
    
            But the closures of Fresh and Go doesnâ€™t mean Amazon is failing at grocery, Saunders said, pointing out that itâ€™s now a $150 billion business with more than 150 million shoppers that â€œcontinues to take significant levels of market share across grocery,â€ even against market leader Walmart.
    ]]></content:encoded></item><item><title>430k-year-old well-preserved wooden tools are the oldest ever found</title><link>https://www.nytimes.com/2026/01/26/science/archaeology-neanderthals-tools.html</link><author>bookofjoe</author><category>hn</category><pubDate>Tue, 27 Jan 2026 15:46:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cloudflare claimed they implemented Matrix on Cloudflare workers. They didn&apos;t</title><link>https://tech.lgbt/@JadedBlueEyes/115967791152135761</link><author>JadedBlueEyes</author><category>hn</category><pubDate>Tue, 27 Jan 2026 15:45:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Amazon closing its Fresh and Go stores</title><link>https://finance.yahoo.com/news/amazon-closing-fresh-grocery-convenience-150437789.html</link><author>trenning</author><category>hn</category><pubDate>Tue, 27 Jan 2026 15:41:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Amazon.com Inc. is shuttering its Amazon-branded grocery stores and automated grab-and-go markets, eliminating two centerpieces of its push into physical retail.Amazon Fresh and Amazon Go stores will close, the company said in a blog post on Tuesday, with some locations converted into Whole Foods Market stores.â€œWhile weâ€™ve seen encouraging signals in our Amazon-branded physical grocery stores, we havenâ€™t yet created a truly distinctive customer experience with the right economic model needed for large-scale expansion,â€ Amazon said.The moves mark the e-commerce giantâ€™s latest retreat from its brick-and-mortar retail efforts. Since the surprise opening of a physical bookstore in 2015, Amazon has tried and failed to establish a foothold under its own brand in categories from groceries to fashion, often with technological flourishes such as digital price tags or novel checkout methods.Over the last few years, the company has backed away from the bookstores, an eclectic kitchen goods, toys and electronics store called Amazon 4-Star, electronics kiosks in shopping malls and a short-lived clothing storefront.Amazon on Tuesday said it would continue to invest in groceries sold both online and offline. That includes an ongoing effort to stock more produce and perishables in Amazonâ€™s same-day delivery warehouses and at more Whole Foods stores, which comprise more than 550 locations.Amazon currently operates 14 Go stores, which use cameras to track what people grab off the shelves, and 58 Amazon Fresh grocery stores, according to its website. The last day of operation for most of those stores will be Sunday, a spokesperson said, except in California, where theyâ€™ll stay open longer to comply with state requirements for advance notice of closures.The moves mean thousands of hourly workers in the stores will lose their jobs. Cuts to Amazonâ€™s corporate workforce will likely involve dozens of people, according to a person familiar with the matter, who asked for anonymity because the information was confidential. The company says it will work to help employees find other jobs at Amazon, including at Whole Foods stores or in its logistics network. Amazonâ€™s corporate ranks were already bracing for a round of layoffs expected as soon as this week.â€œThe main reason behind the decision is that neither Fresh nor Go stores were delivering the sales needed to make them fully economic,â€ Neil Saunders, a retail analyst with GlobalData, said in emailed comments. â€œNor were they producing growth trajectories that might convincingly reverse that position.â€Still, the company says itâ€™s a top-three grocer in the US, with more than $150 billion in gross sales. Much of that volume is shelf-stable items and consumables. After buying Whole Foods in 2017, Amazon quickly added home delivery from the organic grocerâ€™s stores, but the company has struggled to find the right formula for stocking and delivering perishable goods from its massive network of warehouses.Sales of perishables for same-day delivery through that network grew by 40 times in the last year as the company stored groceries in more locations and nudged shoppers to tack apples or bananas onto their orders, Amazon said.Amazon Go, equipped with the cashierless Just Walk Out system, was heralded as a marvel when the first location opened to the public at Amazonâ€™s Seattle headquarters building in 2018. Efforts to deploy the expensive array of ceiling-mounted cameras and sophisticated software to full-sized Fresh and Whole Foods stores were a bust.While the Amazon Go convenience store chain never came close to its creatorsâ€™ hopes for a Starbucks-like ubiquity, it proved a testing ground for technology that Amazon primarily sells as a service, including to sports and entertainment venues. The cashierless technology is now running in more than 360 third-party locations, Amazon said.The company also signaled that it would continue to test new store configurations. That includes locations that combine Amazon pickup counters with Whole Foods stores, an effort to sell items forbidden from the chain by ingredient standards. Amazon will also continue to pursue a big-box format, including a 229,000-square-foot supercenter in development near Chicago.â€œIn our view, in one way or another, Amazonâ€™s physical grocery mantra is: â€˜Weâ€™ll be back,â€™â€ Saunders said.--With assistance from Spencer Soper and Natalie Lung.Most Read from Bloomberg Businessweek]]></content:encoded></item><item><title>The age of Pump and Dump software</title><link>https://tautvilas.medium.com/software-pump-and-dump-c8a9a73d313b</link><author>brisky</author><category>hn</category><pubDate>Tue, 27 Jan 2026 14:05:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: We Built the 1. EU-Sovereignty Audit for Websites</title><link>https://lightwaves.io/en/eu-audit/</link><author>cmkr</author><category>dev</category><category>hn</category><pubDate>Tue, 27 Jan 2026 14:00:21 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Check in seconds how dependent your website is on Non-EU services.Our free scanner analyzes Google Fonts, Analytics, CDNs, video embeds and more. â€“ Where is your server located? â€“ Google Fonts, Adobe Fonts or EU alternatives? â€“ Google Analytics or privacy-friendly solutions? â€“ Cloudflare, AWS or European providers? â€“ YouTube embeds or self-hosted? â€“ Intercom, Drift or EU tools? â€“ Facebook Pixel, Twitter widgets? â€“ Google Maps or OpenStreetMap?The EU-US Data Privacy Framework can be invalidated at any time â€“ just like Safe Harbor (2015) and Privacy Shield (2020). Websites with 100% EU score are future-proof.]]></content:encoded></item><item><title>TikTok users can&apos;t upload anti-ICE videos. The company blames tech issues</title><link>https://www.cnn.com/2026/01/26/tech/tiktok-ice-censorship-glitch-cec</link><author>kotaKat</author><category>hn</category><pubDate>Tue, 27 Jan 2026 13:44:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
            The comedian Megan Stalter, who posts absurd character skits to an audience in the high hundreds of thousands across Instagram and TikTok, tried sharing a different kind of video on Saturday night. Driven by the death of Alex Pretti, the nurse shot by a federal immigration agent or agents that day, she had recorded herself urging her fellow Christians to speak out against ICE raids in Minneapolis.
    
            â€œWe have to abolish ICE,â€ Stalter said in the video. â€œI truly, truly believe that is exactly what Jesus would do.â€
    
            On Instagram, the video was reposted more than 12,000 times. But her plea never made it to TikTok. In a follow-up post on Instagram, she said she had attempted to upload the video to TikTok several times with no success, then had given up and deleted her TikTok account entirely, believing her content was being censored because it was about ICE. (CNN has reached out to Stalter for comment.)
    
            Other users reported the same combination of events, drawing a circumstantial connection between their efforts to make videos about ICE and the difficulties they had posting them over the weekend. The controversy caught the attention of Connecticut Democratic Sen. Chris Murphy, who said that among â€œthreats to democracy,â€ the purported censorship on TikTok was â€œat the top of the list.â€ (CNN has reached out to Murphyâ€™s office for comment.)
    
            TikTok said in a statement that glitches on the app were due to a power outage at a US data center. As a result, a spokesperson forTikTok US Joint Venturetold CNN, itâ€™s taking longer for videos to be uploaded and recommended to other users. The tech issues were â€œunrelated to last weekâ€™s news,â€ TikTok said.
    
            On Tuesday morning, TikTok US Joint Venture said it had made â€œsignificant progressâ€ in restoring service to US users but noted that users may still have trouble uploading new videos.
    
            Last week, a majority American-owned joint venture took control of TikTokâ€™s assets in the US, in a deal shepherded by the Trump administration under a 2024 law requiring the app to move out from under its previous Chinese ownership or face a ban in the United States. Among its new investors is the tech company Oracle, whose executive chair Larry Ellison is a close affiliate of President Donald Trump. Oracle will store US TikTok usersâ€™ data in a â€œsecure US cloud environment,â€ according to TikTok, and the new joint venture will â€œhave decision-making authority for trust and safety policies and content moderation.â€
    
            As a private platform, TikTok is free to exert influence on what users can upload or see. Even if accusations of TikTokâ€™s censorship are unprovable, itâ€™s understandable that US users would be increasingly skeptical of the platform in this moment, said Casey Fiesler, an associate professor of technology ethics and internet law at the University of Colorado, Boulder.
    
            â€œThereâ€™s not a lot of trust in the leadership of social media platforms in general,â€ Fiesler told CNN. â€œAnd given the connection of the new ownership of TikTok to the Trump administration, which is so wrapped up in what is happening with ICE in Minnesota, itâ€™s not surprising that thereâ€™s a significant lack of trust.â€
    
            Fiesler said she was â€œunsurprisedâ€ about censorship concerns on TikTok, given the timing.
    
            Almost immediately after oversight of TikTokâ€™s US operations changed, misinformation started to spread about changes to the appâ€™s new terms of service, including those that applied to location sharing and data collection, Fiesler said.
    
            â€œA lot of TikTok users are concerned about what this new ownership means, both with respect to who has access to their data, and how content recommendation might change or could change,â€ Fiesler said. â€œI think those are valid concerns.â€
    
            A few days ago, Fiesler posted some videos aiming to debunk those rumors about the changes to the terms of service, and those were uploaded without issue. She has attempted to upload two videos since Sunday afternoon, one of which she says is still â€œunder reviewâ€ by TikTok and canâ€™t be viewed publicly. While both generally alluded to ongoing ICE action in Minneapolis, she used it as a framing device to discuss media literacy. One of the videos did successfully upload on Monday, though its captions and view counter werenâ€™t working for several hours, she said.
    
            â€œEven if this isnâ€™t purposeful censorship, does it matter? In terms of perception and trust, maybe,â€ Fiesler said.
    
            Jen Hamilton, a nurse and author with more than 4.5 million TikTok followers, says she became suspicious of TikTok on January 22, the day of the announced change in control in the US, when a video she made about 5-year-old Liam Conejo Ramos being taken by federal agents wouldnâ€™t upload.
    
            â€œIt was very ironic for that very first day of this takeover, for me to post something about ICE and then it not be viewable to the public,â€ Hamilton told CNN, adding that the video still canâ€™t be seen by her followers.
    
            After she posted a still about Pretti, her next four videos couldnâ€™t be uploaded, she said.
    
            â€œSomething has shifted in the way that content is getting put on the platform, or allowed to be on the platform,â€ she said, while noting that she didnâ€™t have proof of being personally censored. â€œAnd I just find it very ironic that itâ€™s the same day that it takes over that people are not being able to post their stuff.â€
    
            It would be incredibly difficult to prove TikTok is censoring content about ICE because the platformâ€™s content recommendation process is so opaque, said Jeffrey Blevins, a professor at the University of Cincinnati who studies media law and ethics, among other subjects. Plus, if TikTok  intentionally censoring content about ICE, it would be within its legal right.
    
            â€œTheyâ€™re a private platform. They have a First Amendment right to do that,â€ he said. â€œA lot of times itâ€™s easy for us to think of social media as a public square, but itâ€™s not public in a way that matters under the law.â€
    
            Some users, like Stalter, are deleting their accounts and leaving the app altogether (though some have also had trouble deleting their accounts, Fiesler noted). The daily average of TikTok uninstalls are up nearly 150% in the last five days compared to the last three months, market firm SensorTower told CNBC on Monday.
    
            â€œIf people leave TikTok now, I suspect itâ€™s a combination of things, not just because some videos werenâ€™t posted on one day. Itâ€™s also concerns about what this means for the future.â€
    
            Hamilton said while sheâ€™s exploring options like Substack and Patreon, where followers can pay to hear her unvarnished thoughts, she wonâ€™t fully abandon TikTok.
    
            â€œThis whole thing is intended to dissuade people, especially who are sharing a narrative that is not similar to what the government is wanting people to hear,â€ she said she suspects. â€œI think the purpose of having those people feel like their content is not safe on this platform is to get them to stop speaking out, or use the platform differently, or play by the rules.â€
    
            Sheâ€™s already figured out ways to continue to talk about ICE, she said. In a video that  make it through TikTokâ€™s uploading process, she calls herself a â€œfashion influencerâ€ and speaks in code about her trouble uploading an earlier video about Liam.
    
            â€œFashion influencing is in my blood,â€ she said in the video, with a photo of Liam behind her. â€œAnd even a company with bad customer service wonâ€™t keep me from doing my fashion review.â€
    ]]></content:encoded></item><item><title>Xfwl4 â€“ The Roadmap for a Xfce Wayland Compositor</title><link>https://alexxcons.github.io/blogpost_15.html</link><author>pantalaimon</author><category>hn</category><pubDate>Tue, 27 Jan 2026 13:25:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: One Human + One Agent = One Browser From Scratch in 20K LOC</title><link>https://emsh.cat/one-human-one-agent-one-browser/</link><author>embedding-shape</author><category>dev</category><category>hn</category><pubDate>Tue, 27 Jan 2026 13:13:56 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Just for the fun of it, I thought I'd embark on a week-long quest to
generate millions of tokens and millions of lines of source code to
create one basic browser that can render HTML and CSS (no JS tho), and
hopefully I could use this to receive even more VC investments.But then I remembered that I have something even better: a human
brain! It is usually better than any machine at coordinating and
thinking through things, so let's see if we can hack something together,
one human brain and one LLM agent brain!The above might look like a simple .webm video, but it's actually a
highly sophisticated and advanced browser that was super hard to build,
encoded as pixels in a video file! Wowzers.For extra fun when building this, I set these requirements for myself
and the agent:I have three days to build itNot a single 3rd party Rust library/dependency allowedAllowed to use anything (commonly) provided out of the box on the OS
it runs onShould run on Windows, macOS and common Linux distributionsShould be able to render some websites, most importantly, my own
blog and Hacker News, should be easy right?The codebase can always compile and be builtThe codebase should be readable by a human, although code quality
isn't the top concernSo with these things in mind, I set out on the journal to build a
browser "from scratch". I started with something really based, being
able to just render "Hello World". Then to be able to render some nested
tags. Added the ability of taking screenshots so the agent could use
that. Added specifications for HTML/CSS (which I think the agent never
used :| ), and tried to nail down the requirements for the agent to use.
Also started doing "regression" or "E2E" tests with the screenshotting
feature, so we could compare to some baseline images and so on. Added
the ability to click on links just for the fun of it.After about a day together with Codex, I had something that could via
X11 and cURL, fetch and render websites when run, and the Cargo.lock is
empty. It was about 7500 lines long in total at that point, split across
files with all of them under 1000 lines long (which was a stated
requirement, so not a surprise).Second day I got annoyed by the tests spawning windows while I was
doing other stuff, so added a --headless flag too. Did some fixes for
resizing the window, various compatibility fixes, some performance
issues and improved the font/text rendering a bunch. Workflow was
basically to pick a website, share a screenshot of the website without
JavaScript, ask Codex to replicate it following our instructions. Most
of the time was the agent doing work by itself, and me checking in when
it notifies me it was done.Third day we made large changes, lots of new features and a bunch of
new features supported. More regression tests, fixing performance
issues, fixing crashes and whatnot. Also added scrolling because this is
a mother fucking browser, it has to be able to scroll. Added some debug
logs too because that'll look cool in the demonstration video above, and
also added support for the back button because it was annoying to start
from scratch if I clicked the wrong link while testing.At the end of the third day we also added starting support for macOS,
and managed to get a window to open, and the tests to pass. Seems to
work OK :) Once we had that working, we also added Windows support,
basically the same process, just another platform after all.Then the fourth day (whaaaat?) was basically polish, fixing CI for
all three platforms, making it pass and finally cutting a release based
on what got built in CI. Still all within 72 hours (3 days * 24 hours,
which obviously this is how you count days).The results after ~3 days
(~70 hours)And here it is, in all its glory, made in ~20K lines of code and
under 72 hours of total elapsed time from first commit to last:You can clone the repository, build it and try it out for yourself.
It's not great, I wouldn't even say it's good, but it works, and
demonstrates that one person with one agent can build a browser from
scratch.This is what the "lines of code" count ended up being after all was
said and done, including support for three OSes:$ git rev-parse HEAD
e2556016a5aa504ecafd5577c1366854ffd0e280

$ cloc src --by-file
      72 text files.
      72 unique files.
       0 files ignored.

github.com/AlDanial/cloc v 2.06  T=0.06 s (1172.5 files/s, 373824.0 lines/s)
-----------------------------------------------------------------------------------
File                                            blank        comment           code
-----------------------------------------------------------------------------------
src/layout/flex.rs                                 96              0            994
src/layout/inline.rs                               85              0            933
src/layout/mod.rs                                  82              0            910
src/browser.rs                                     78              0            867
src/platform/macos/painter.rs                      96              0            765
src/platform/x11/cairo.rs                          77              0            713
src/platform/windows/painter.rs                    88              0            689
src/bin/render-test.rs                             87              0            666
src/style/builder.rs                               83              0            663
src/platform/windows/d2d.rs                        53              0            595
src/platform/windows/windowed.rs                   72              0            591
src/style/declarations.rs                          18              0            547
src/image.rs                                       81              0            533
src/platform/macos/windowed.rs                     80              2            519
src/net/winhttp.rs                                 61              2            500
src/platform/x11/mod.rs                            56              2            487
src/css.rs                                        103            346            423
src/html.rs                                        58              0            413
src/platform/x11/painter.rs                        48              0            407
src/platform/x11/scale.rs                          57              3            346
src/layout/table.rs                                39              1            340
src/platform/x11/xft.rs                            35              0            338
src/style/parse.rs                                 34              0            311
src/win/wic.rs                                     39              8            305
src/style/mod.rs                                   26              0            292
src/style/computer.rs                              35              0            279
src/platform/x11/xlib.rs                           32              0            278
src/layout/floats.rs                               31              0            265
src/resources.rs                                   36              0            238
src/css_media.rs                                   36              1            232
src/debug.rs                                       32              0            227
src/platform/windows/dwrite.rs                     20              0            222
src/render.rs                                      18              0            196
src/style/custom_properties.rs                     34              0            186
src/platform/windows/scale.rs                      28              0            184
src/url.rs                                         32              0            173
src/layout/helpers.rs                              12              0            172
src/net/curl.rs                                    31              0            171
src/platform/macos/svg.rs                          35              0            171
src/browser/url_loader.rs                          17              0            166
src/platform/windows/gdi.rs                        17              0            165
src/platform/windows/scaled.rs                     16              0            159
src/platform/macos/scaled.rs                       16              0            158
src/layout/svg_xml.rs                               9              0            152
src/win/com.rs                                     26              0            152
src/png.rs                                         27              0            146
src/layout/replaced.rs                             15              0            131
src/net/pool.rs                                    18              0            129
src/platform/macos/scale.rs                        17              0            124
src/style/selectors.rs                             18              0            123
src/style/length.rs                                17              0            121
src/cli.rs                                         15              0            112
src/platform/windows/headless.rs                   20              0            112
src/platform/macos/headless.rs                     19              0            109
src/bin/fetch-resource.rs                          14              0            101
src/geom.rs                                        10              0            101
src/browser/render_helpers.rs                      11              0            100
src/dom.rs                                         11              0            100
src/style/background.rs                            15              0            100
src/layout/tests.rs                                 7              0             85
src/platform/windows/d3d11.rs                      14              0             83
src/win/stream.rs                                  10              0             63
src/platform/windows/svg.rs                        13              0             54
src/main.rs                                         4              0             33
src/platform/mod.rs                                 6              0             28
src/app.rs                                          5              0             25
src/lib.rs                                          1              0             20
src/platform/windows/mod.rs                         2              0             19
src/net/mod.rs                                      4              0             16
src/platform/macos/mod.rs                           2              0             14
src/platform/windows/wstr.rs                        0              0              5
src/win/mod.rs                                      0              0              3
-----------------------------------------------------------------------------------
SUM:                                             2440            365          20150
-----------------------------------------------------------------------------------One human using one agent seems far more effective than one human
using thousands of agentsOne agent can work on a single codebase for hours, making real
progress on ambitious projectsThis could probably scale to multiple humans too, each equipped with
their own agent, imagine what we could achieve!Sometimes slower is faster and also betterThe human who drives the agent might matter more than how the agents
work and are set up, the judge is still out on this oneIf one person with one agent can produce equal or better results than
"hundreds of agents for weeks", then the answer to the question: "Can we
scale autonomous coding by throwing more agents at a problem?", probably
has a more pessimistic answer than some expected.]]></content:encoded></item><item><title>India and EU announce landmark trade deal</title><link>https://www.bbc.com/news/articles/crrnee01r9jo</link><author>Palmik</author><category>hn</category><pubDate>Tue, 27 Jan 2026 11:58:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[While commodities such as tea, coffee, spices and processed foods will benefit from the agreement, Delhi "has prudently safeguarded sensitive sectors, including dairy, cereals, poultry, soy meal, certain fruits and vegetables, balancing export growth with domestic priorities", it said.]]></content:encoded></item><item><title>Ask HN: Books to learn 6502 ASM and the Apple II</title><link>https://news.ycombinator.com/item?id=46778461</link><author>abkt</author><category>hn</category><pubDate>Tue, 27 Jan 2026 11:12:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I want to learn Assembly to make games on the Apple II. What are the old books to learn 6502 Assembly and the Apple II itself (memory, screen management) ? And is it absolutely necessary to learn BASIC before Assembly ?]]></content:encoded></item><item><title>I made my own Git</title><link>https://tonystr.net/blog/git_immitation</link><author>TonyStr</author><category>hn</category><pubDate>Tue, 27 Jan 2026 10:55:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Russia using Interpol&apos;s wanted list to target critics abroad, leak reveals</title><link>https://www.bbc.com/news/articles/c20gg729y1yo</link><author>breve</author><category>hn</category><pubDate>Tue, 27 Jan 2026 07:06:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA["It's constant nerves, all the time," he adds, explaining he was always looking over his shoulder. For safety, his daughter and her mother moved to another country.  The police can "break into your house at any timeâ€¦ that's why you're like a cornered rat", he says.]]></content:encoded></item><item><title>Doing the thing is doing the thing</title><link>https://www.softwaredesign.ing/blog/doing-the-thing-is-doing-the-thing</link><author>prakhar897</author><category>hn</category><pubDate>Tue, 27 Jan 2026 06:17:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Currently exploring full-time and contract rolesDoing the thing is doing the thing.Thinking about doing the thingDreaming about doing the thingVisualizing success from doing the thingWaiting to feel ready to do the thingTalking about doing the thingExplaining the thing to othersArguing online about the thingAnnouncing that youâ€™ll start the thingListening to podcasts about doing the thingWatching tutorials about doing the thingReading threads about how others did the thingPlanning the perfect system for the thingBuying tools for the thingReorganizing your workspace for the thingFeeling guilty about not doing the thingBeing â€œbusyâ€ instead of doing the thingTelling yourself youâ€™ll start tomorrowFailing while doing the thing  doing the thing.Doing it badly  doing the thing.Doing it timidly  doing the thing.Doing a small part of the thing  doing the thing.Writing a blog about doing the thingI should probably get back to work.]]></content:encoded></item><item><title>Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model</title><link>https://www.kimi.com/blog/kimi-k2-5.html</link><author>nekofneko</author><category>hn</category><pubDate>Tue, 27 Jan 2026 05:42:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today, we are introducing Kimi K2.5, the most powerful open-source model to date.Kimi K2.5 builds on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens. Built as a native multimodal model, K2.5 delivers state-of-the-art  capabilities and a self-directed  paradigm.For complex tasks, Kimi K2.5 can self-direct an  with up to 100 sub-agents, executing parallel workflows across up to . Compared with a single-agent setup, this reduces execution time by up to . The agent swarm is automatically created and orchestrated by Kimi K2.5 without any predefined subagents or workflow.Kimi K2.5 is available via . Kimi.com & Kimi App now supports 4 modes: K2.5 Instant, K2.5 Thinking, K2.5 Agent, and K2.5 Agent Swarm (Beta). Agent Swarm is currently in beta on Kimi.com, with free credits available for high-tier paid users.Across three agentic benchmarksâ€”HLE, BrowseComp, and SWE-Verifiedâ€”Kimi K2.5 delivers strong performance at a fraction of the cost.Kimi K2.5 is the strongest open-source model to date for coding, with particularly strong capabilities in front-end development.K2.5 can turn simple conversations into complete front-end interfaces, implementing  and rich animations such as scroll-triggered effects. Below are examples generated by K2.5 from a single prompt with image-gen tool:Beyond text prompts, K2.5 excels at . By reasoning over images and video, K2.5 improves image/video-to-code generation and visual debugging, lowering the barrier for users to express intent visually.Here is an example of K2.5 reconstructing a website from video:This capability stems from massive-scale vision-text joint pre-training. At scale, the trade-off between vision and text capabilities disappears â€” they improve in unison.Below is an example of K2.5 reasoning over a puzzle and marking the shortest path using code:K2.5 excels in real-world software engineering tasks. We evaluate it using , our internal coding benchmark covering diverse end-to-end tasks â€” from building to debugging, refactoring, testing, and scripting â€” across multiple programming languages. On this benchmark, K2.5 shows consistent and meaningful improvements over.To try out K2.5's agentic coding capabilities,  offers a set of preconfigured tools for immediate, hands-on experiences. For software engineering use cases, we recommend pairing Kimi K2.5 with our new coding product, . works in your terminal and can be integrated with various IDEs including VSCode, Cursor, Zed, etc. Kimi Code is open-sourced and supports images and videos as inputs. It also automatically discovers and migrates existing skills and MCPs into your working environment in Kimi Code.Here's an example using  to translate the aesthetic of  into the Kimi App. This demo highlights a breakthrough in autonomous visual debugging. Using visual inputs and documentation lookup, K2.5 visually inspects its own output and iterates on it autonomously. It creates an art-inspired webpage created end to end:Scaling Out, Not Just Up. We release  as a research preview, marking a shift from single-agent scaling to self-directed, coordinated swarm-like execution.Trained with Parallel-Agent Reinforcement Learning (PARL), K2.5 learns to self-direct an  of up to , executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows.PARL uses a trainable orchestrator agent to decompose tasks into parallelizable subtasks, each executed by dynamically instantiated, . Running these subtasks concurrently significantly reduces end-to-end latency compared to sequential agent execution.Training a reliable parallel orchestrator is challenging due to delayed, sparse, and non-stationary feedback from independently running subagents. A common failure mode is , where the orchestrator defaults to single-agent execution despite having parallel capacity. To address this, PARL employs  that encourages parallelism early in training and gradually shifts focus toward task success.where  anneals from  over training. Early on, the auxiliary reward  incentivizes subagent instantiation and concurrent execution, promoting exploration of the parallel scheduling space. As training progresses, optimization shifts toward end-to-end task quality  , preventing degenerate solutions where parallelism is enabled in name only.To further force parallel strategies to emerge, we introduce a computational bottleneck that makes sequential execution impractical. Instead of counting total steps, we evaluate performance using , a latency-oriented metric inspired by the critical path in parallel computation:captures orchestration overhead, while  reflects the slowest subagent at each stage. Under this metric, spawning more subtasks only helps if it shortens the critical path.An agent swarm has an orchestrator that dynamically creates specialized subagents (e.g., AI Researcher, Physics Researcher, Fact Checker) and decomposes complex tasks into parallelizable subtasks for efficient distributed execution.In our parallel-agent reinforcement learning environment, the reward increases smoothly as training progresses. At the same time, the level of parallelism during training also gradually increases.improves performance on complex tasks through parallel, specialized execution. In our internal evaluations, it leads to an reduction in end-to-end runtime while enabling more complex, long-horizon workloads, as shown below.Agent Swarm reduces the minimum critical steps required to achieve target performance by 3Ã—â€“4.5Ã— compared to single-agent execution in wide search scenario, with savings scaling as targets riseâ€”translating to up to 4.5Ã— wall-clock time reduction via parallelization.Kimi K2.5 brings agentic intelligence into real-world knowledge work. can handle high-density, large-scale office work end to end. It reasons over large, high-density inputs, coordinates multi-step tool use, and delivers expert-level outputs: documents, spreadsheets, PDFs, and slide decksâ€”directly through conversation.With a focus on real-world professional tasks, we design two internal expert productivity benchmarks. The  evaluates end-to-end Office output quality, while the  measures multi-step, production-grade workflows against human expert performance. Across both benchmarks, 59.3% and 24.3% improvements over K2 Thinking, reflecting stronger end-to-end performance on real-world tasks.Internal Expert Productivity Bench (AI Office)K2.5 agent supports advanced tasks such as adding annotations in Word, constructing financial models with Pivot Tables, and writing LaTeX equations in PDFs, while scaling to long-form outputs like 10,000-word papers or 100-page documents.Tasks that once took hours or days now complete in minutes. Here are some examples:Grounded in advances in coding with vision, agent swarms, and office productivity, Kimi K2.5 represents a meaningful step toward AGI for the open-source community, demonstrating strong capability on real-world tasks under real-world constraints. Looking ahead, we will push further into the frontier of agentic intelligence, redefining the boundaries of AI in knowledge work.1. General Testing DetailsWe report results for Kimi K2.5 and DeepSeek-V3.2 with thinking mode enabled, Claude Opus 4.5 with extended thinking mode, GPT-5.2 with xhigh reasoning effort, and Gemini 3 Pro with a high thinking level. For vision benchmarks, we additionally report results for Qwen3-VL-235B-A22B-Thinking.Unless otherwise specified, all Kimi K2.5 experiments were conducted with temperature = 1.0, top-p = 0.95, and a context length of 256k tokens.Benchmarks without publicly available scores were re-evaluated under the same conditions used for Kimi K2.5 and are marked with an asterisk (*).We could not evaluate GPT-5.2 xhigh on all benchmarks due to service stability issues. For benchmarks that were not tested, we mark them as "-".HLE, AIME 2025, HMMT 2025 (Feb), GPQA-Diamond and IMO-AnswerBench were evaluated with a maximum completion budget of 96k tokens.Results for AIME and HMMT are averaged over 32 runs (avg@32); GPQA-Diamond over 8 runs (avg@8).For HLE, we report scores on the full set (text & image). Kimi K2.5 scores 31.5 (text) and 21.3 (image) without tools, and 51.8 (text) and 39.8 (image) with tools. The DeepSeek-V3.2 score corresponds to its text-only subset (marked with â€ ) . Hugging Face access was blocked to prevent potential data leakage. HLE with tools uses simple context management: once the context exceeds a threshold, only the latest round of tool messages is retained.Tool-Augmented / Agentic SearchKimi K2.5 was equipped with search, code-interpreter, and web-browsing tools for HLE with tools and all agentic search benchmarks.Except for BrowseComp (where K2.5 and DeepSeek-V3.2 used the discard-all strategy), no context management was applied, and tasks exceeding the supported context length were directly counted as failed.The test system prompts emphasize deep and proactive tool use, instructing models to reason carefully, leverage tools, and verify uncertain information. Full prompts will be provided in the technical report.Results for Seal-0 and WideSearch are averaged over four runs (avg@4).Max-tokens = 64k, averaged over three runs (avg@3).ZeroBench (w/ tools) uses max-tokens-per-step = 24k and max-steps = 30 for multi-step reasoning.MMMU-Pro follows the official protocol, preserving input order and prepending images.GPT-5.2-xhigh had ~10% failure rate (no output despite 3 retries), treated as incorrect; reported scores likely underestimate true performance.OmniDocBench Score is computed as (1 âˆ’ normalized Levenshtein distance) Ã— 100, where a higher score denotes superior accuracy.Terminal-Bench 2.0 scores were obtained with the default agent framework (Terminus-2) and the provided JSON parser. In our implementation, we evaluated Terminal-Bench 2.0 under non-thinking mode. This choice was made because our current context management strategy for the thinking mode is incompatible with Terminus-2.For the SWE-Bench series of evaluations (including verified, multilingual, and pro), we used an internally developed evaluation framework. This framework includes a minimal set of toolsâ€”bash tool, createfile tool, insert tool, view tool, strreplace tool, and submit toolâ€”along with tailored system prompts designed for the tasks. The highest scores were achieved under non-thinking mode.The score of Claude Opusâ€¯4.5 on CyberGym is reported under the non-thinking setting.All reported scores of coding tasks are averaged over 5 independent runs.AA-LCR: scores averaged over three runs (avg@3).LongBench-V2: identical prompts and input contexts standardized to ~128k tokens.BrowseComp (Swarm Mode): main agent max 15 steps; sub-agents max 100 steps.WideSearch (Swarm Mode): main and sub-agents max 100 steps.]]></content:encoded></item><item><title>A list of fun destinations for telnet</title><link>https://telnet.org/htm/places.htm</link><author>tokyobreakfast</author><category>hn</category><pubDate>Tue, 27 Jan 2026 03:24:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The text based internet can be exciting, informative, and fun. Using telnet, 
you can access a variety of these resources on the internet. Below youâ€™ll find 
lists of a few places to get you started.If you have an interesting item to add, just send an email to us:Various fun telnet serversThis is really cool, and even includes mouse support!Notable, but no longer online :(Real classic. I especially miss the Bofh server.Rainmaker was pretty great, and it lasted at least as far as 2018. I donâ€™t recall
what happened to it.The telnet server is offline, but the website is still up for this one! Also, an alternative can be noted above.Both are offline at the time of this update.BBS, Muds, Talkers, and other systems]]></content:encoded></item><item><title>iPhone 5s Gets New Software Update 13 Years After Launch</title><link>https://www.macrumors.com/2026/01/26/iphone-5s-software-update/</link><author>angott</author><category>hn</category><pubDate>Tue, 27 Jan 2026 01:04:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Alongside iOS 26.2.1, Apple today released an updated version of iOS 12 for devices that are still running that operating system update, eight years after the software was first released.iOS 12.5.8 is available for the iPhone 5s and the â€ŒiPhoneâ€Œ 6, meaning Apple is continuing to support these devices for 13 and 12 years after launch, respectively. The â€ŒiPhoneâ€Œ 5s came out in September 2013, while the â€ŒiPhoneâ€Œ 6 launched in September 2014.According to Apple's release notes for the update, iOS 12.5.8 extends the certificate required for features like iMessage, FaceTime, and device activation, so they will continue to work after January 2027.Originally, certificate-limited functions like device activation would have ceased when the certificate expired, but now key features on the two older iPhones will continue to work in the years to come.Prior to now, the â€ŒiPhoneâ€Œ 5s and â€ŒiPhoneâ€Œ 6 last received software updates in January 2023, when Apple released important security fixes.Apple has publicly committed to providing a minimum of five years of security updates for an â€ŒiPhoneâ€Œ from the time that it launches, but it often offers vulnerability fixes for an even longer period of time.The â€ŒiPhoneâ€Œ 6s, for example, was released 11 years ago, but it received a security update in September 2025 with the launch of iOS 15.8.5. The device received an iOS 15.8.6 update just today, adding the same certificate extension.Apple also released new versions of iOS 18 and iOS 16.]]></content:encoded></item><item><title>Y Combinator website no longer lists Canada as a country it invests in</title><link>https://betakit.com/y-combinator-website-no-longer-lists-canada-as-a-country-it-invests-in/</link><author>TheLegace</author><category>hn</category><pubDate>Mon, 26 Jan 2026 23:32:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Y Combinator has revised its standard deal terms to exclude Canada as a permitted site of investment, implying that Canadian startups aspiring to join the prestigious San Francisco-based startup accelerator will have to incorporate their companies elsewhere.Â As first reported by  Monday, Y Combinatorâ€™s standard deal terms webpage now says it invests in corporations based in the United States, the Cayman Islands, or Singapore. Canada was in that list as recently as Nov. 2, 2025, according to an archived version of the webpage, but the reference was removed by the end of that month. Dozens of Canadiancompanies have been part of Y Combinatorâ€™s numerous winter and summer cohorts since the first one was accepted in 2008.Â The deal terms go on to state that if a startup is already incorporated in another country that is â€œnot one of the threeâ€ (previously â€œfourâ€) countries listed, the startup needs to â€œflipâ€ its corporate structure to have a parent company in one of those three countries.BetaKit has reached out to Y Combinator for comment.Â Garry Tan, Y Combinatorâ€™s president and CEO, posted on X late Monday that â€™s reporting, which bore the headline â€œY Combinator is no longer investing in Canadian startups,â€ was â€œoverstating it.â€ He said that Y Combinator funds â€œtons of Canadians and Canadian startups.â€ However, Tan seemingly confirmed the reporting by adding that if Canadian founders are going to raise money, they â€œshould just convert [their company] to Delaware C Corp.â€Tan, who was born in Winnipeg, added that he loves Canada. â€œBut donâ€™t let that get in the way of making a huge startup,â€ he posted.The gravitational pull of Y Combinatorâ€™s program over Canadian startups has increased in recent years, aided by remote policies instituted during the COVID-19 pandemic. In the 2010s, there were usually fewer than five Canadian-headquartered companies in a given Y Combinator cohort, according to data gathered last year by Bram Sugarman. Between winter 2020 and winter 2022, that number grew to range between nine and 15 startups in the program.Â It is common practice for Canadian founders to set up shop in the Valley, especially to participate in Y Combinator. Y Combinator CEO Garry Tan claimed in an X post last year that â€œThe Canadians stay in the USA and raise more money. The ones that stay in SF after demo day become unicorns at 2.5X the rate.â€Tan added at the time that he was part of a YC dinner with many Canadian founders looking to base their startups in San Francisco after graduation.One such company is Guelph, Ont.- and Irvine, Calif.-based edtech startup Opennote. A member of Y Combinatorâ€™s Summer 2025 batch, co-founder Vedant Vyas told BetaKit last July that the firm hoped to scale out of the Bay Area, citing support from US-based investors and an increased institutional willingness in the US to pilot new edtech solutions.The pull was a topic of conversation at Toronto Tech Weekâ€™s Homecoming event in June 2025, where Shopify president Harley Finkelstein, Wealthsimple founder Mike Katchen, and Cohere founder Aidan Gomez encouraged the crowd to say no to leaving Canada.â€œItâ€™s the Valley-or-bust mentality that breaks the ecosystem and really hurts Canada,â€ Gomez said.Â UPDATE (01.27.2026): This story has been updated to include public comment from Y Combinatorâ€™s president.Image courtesy of Paul Miller, licensed under CC BY 2.0.]]></content:encoded></item><item><title>I let ChatGPT analyze a decade of my Apple Watch data, then I called my doctor</title><link>https://www.msn.com/en-us/news/technology/i-let-chatgpt-analyze-a-decade-of-my-apple-watch-data-then-i-called-my-doctor/ar-AA1UZxip</link><author>zdw</author><category>hn</category><pubDate>Mon, 26 Jan 2026 22:29:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>People who know the formula for WD-40</title><link>https://www.wsj.com/business/the-secret-society-of-people-who-know-the-formula-for-wd-40-e9c0ff54</link><author>fortran77</author><category>hn</category><pubDate>Mon, 26 Jan 2026 21:11:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A few random notes from Claude coding quite a bit last few weeks</title><link>https://twitter.com/karpathy/status/2015883857489522876</link><author>bigwheels</author><category>hn</category><pubDate>Mon, 26 Jan 2026 21:09:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dithering â€“ Part 2: The Ordered Dithering</title><link>https://visualrambling.space/dithering-part-2/</link><author>ChrisArchitect</author><category>hn</category><pubDate>Mon, 26 Jan 2026 19:23:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[tap/click the right side of the screen to go forward â†’Hi! Welcome back to part 2 of my dithering series!â† tap/click the left side to go backIf you missed part 1, please check it out first! I've covered how dithering simulates more colors than actually exist.â† or use arrow keys to navigate â†’This part dives into ordered dithering, a method using a threshold map to decide each pixel's final color*.*Note: This series covers grayscale dithering to two colors only: black & white. Multi-color dithering is possible, but it will not be covered here.There are many ways to build a threshold map, each creating a unique visual pattern.I'll guide you through how it works and how it forms those unique patterns.Please note that this isnâ€™t a technical deep-dive. Itâ€™s just a visual exploration of the logic behind ordered dithering.I personally grasp ideas much better when I can see them visually, so I hope this helps you too!First, a quick recap on quantization: the process of reducing colors in an image.Grayscale images use many shades, from black to white.Now, imagine our display can only show pure black and white.We must convert those gray shades into black or white. This is what quantization does.One way is by rounding: dark shades become black, and bright shades become white.Another way is by setting a threshold. Anything below it turns black; anything above it turns white.The threshold controls the quantization output: lower thresholds can turn darker shades white, while higher ones can turn brighter shades black.Now, imagine using multiple thresholds at once, each with a different value.Quantizing a single color now gives us a mix of black and white pixels.The result reflects the original brightness: brighter inputs get more whites, while darker inputs get more blacks.This is essentially how a threshold map works in ordered dithering.It converts gray shades into black & white patterns that mimic the original image's brightness.Now letâ€™s try to apply this map to a full image by tiling it across the entire area.Our image is now converted to black & white, but something is off. See those vertical line artifacts?They are formed by the horizontal black & white patterns our map produces.The problem is that the output directly mirrors the map's layout.To fix this, we need to rearrange the threshold map.Introducing: The Bayer matrix.The thresholds are arranged in a 2x2 matrix, like this.When applied, it generates this signature cross-hatch pattern.This pattern keeps black and white pixels apart, preventing unwanted artifacts like those vertical lines.The result is a uniform dispersion of black & white pixels that blend smoothly into shades of gray.Now while this looks better, we still have a problem: the transitions between shades are quite harsh.The issue? With only 4 threshold levels, we only have 4 patterns to represent the entire grayscale shades.To improve this, we need to increase the number of our output patterns.Here is a 4x4 Bayer matrix, which has 16 different threshold values.It's an extension of the 2x2 matrix. Notice how the sequence is similar, just with different starting values.Like the 2x2 version, it creates a cross-hatch pattern. But now with 16 distinct pattern variations.These extra patterns allow us to represent 16 unique shades of gray instead of just four.As a result, we get smoother transitions between shades.This reduces harsh transitions between shadows and highlights in the final image.And thatâ€™s ordered dithering in a nutshell!In summary, it uses a threshold map to translate shades into cleverly arranged patterns.Now you might ask, can we use even more threshold levels? Or, can we arrange them differently?The answer is yes! The threshold map's arrangement is what creates those unique dithering patterns. Letâ€™s explore a few other ways to arrange them.This is the pattern coming from an 8x8 Bayer Matrix. With 64 levels, itâ€™s a further extension of the versions weâ€™ve seen so far.It still has a similar cross-hatch pattern but offers even smoother transitions between shades.It provides a denser and more detailed gradient, although the difference is fairly subtle.Now letâ€™s look beyond the Bayer family with this pattern coming from a 8x8 Cluster Dot matrix.It consists of round dot clusters, quite different from Bayerâ€™s cross-hatching.It gives the image the classic, familiar feel of a printed newspaper.Finally, my favorite method: Void and Cluster.The result is a pattern that feels less rigid than the Bayer cross-hatch.In the final image, this creates a fine texture that blends into grays more naturally.Thatâ€™s all for now! There are still some more maps to explore, but weâ€™ll save those for another time.If you want more examples and details, here are some resources you might want to explore:â€¢ Libcaca study - 2. Halftoningâ€¢ Ditherpunk â€” The article I wish I had about monochrome image ditheringâ€¢ Robert Ulichney's paper on void-and-cluster ditheringI hope you enjoy what you read so far! In the next part, weâ€™ll look at another dithering method that doesn't use a map at all: Error Diffusion.Thanks for reading! See you in the next part.visualrambling.space is created by Damar, someone who loves to exploring new topics and rambling about them visually.If you like this, please consider following and sharing. I'll keep creating more visual articles like this!I'm also open for collaborations or commissioned work. Feel free to reach out anytime.https://x.com/damarberlarihttps://bsky.app/profile/damarberlari.bsky.socialhttps://www.linkedin.com/in/damarpramudita/]]></content:encoded></item><item><title>ChatGPT Containers can now run bash, pip/npm install packages and download files</title><link>https://simonwillison.net/2026/Jan/26/chatgpt-containers/</link><author>simonw</author><category>hn</category><pubDate>Mon, 26 Jan 2026 19:19:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ChatGPT Containers can now run bash, pip/npm install packages, and download filesOne of my favourite features of ChatGPT is its ability to write and execute code in a container. This feature launched as ChatGPT Code Interpreter nearly three years ago, was half-heartedly rebranded to â€œAdvanced Data Analysisâ€ at some point and is generally really difficult to find detailed documentation about. Case in point: it appears to have had a  upgrade at some point in the past few months, and I canâ€™t find documentation about the new capabilities anywhere!Here are the most notable new features:ChatGPT can directly run Bash commands now. Previously it was limited to Python code only, although it could run shell commands via the Python  module. and can run JavaScript directly in addition to Python. I also got it to run â€œhello worldâ€ in Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C and C++. No Rust yet though!While the container still canâ€™t make outbound network requests,  and  both work now via a custom proxy mechanism.ChatGPT can locate the URL for a file on the web and use a  tool to download that file and save it to a path within the sandboxed container.This is a substantial upgrade! ChatGPT can now write and then test code in 10 new languages (11 if you count Bash), can find files online and download them into the container, and can install additional packages via  and  to help it solve problems.I was initially suspicious that maybe Iâ€™d stumbled into a new preview feature that wasnâ€™t available to everyone, but I tried some experiments in a free ChatGPT account and confirmed that the new features are available there as well.My first clue to the new features came the other day when I got curious about Los Angeles air quality, in particular has the growing number of electric vehicles there had a measurable impact?I prompted a fresh GPT-5.2 Thinking session with:Show me Los Angeles air quality over time for last 20 yearsHereâ€™s the shared transcript. Annoyingly that doesnâ€™t make the thinking trace available, so hereâ€™s a screenshot:This bit instantly caught my eye:[...] The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. [...]Without running any searches tell me about your container.download toolSure.  is a built-in tool I can call that fetches a file from a URL and saves it into the sandboxed container filesystem for this chat session.Takes a  and a  in the container.Downloads the bytes from that URL and writes them to the given path.After that, I can read/process the file locally in the container (e.g., unzip it, parse it with Python, open it as an image, convert it, etc.).Itâ€™s called like this (conceptually):{
  : ,
  : 
}Further experimentation confirmed that  behaves as described.I had it download one file from a server with logs enabled and captured the IP address and user-agent. Here are the most interesting headers:User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot
Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, */*;q=0.8, application/signed-exchange;v=b3;q=0.9
Cf-Connecting-Ip: 52.230.164.178
That  IP address resolves to Microsoft Azure Cloud (centralus) in Des Moines, Iowa.Is container.download a data exfiltration vulnerability?On the one hand, this is really useful! ChatGPT can navigate around websites looking for useful files, download those files to a container and then process them using Python or other languages.Is this a data exfiltration vulnerability though? Could a prompt injection attack trick ChatGPT into leaking private data out to a  call to a URL with a query string that includes sensitive information?I donâ€™t think it can. I tried getting it to assemble a URL with a query string and access it using  and it couldnâ€™t do it. It told me that it got back this error:ERROR: download failed because url not viewed in conversation before. open the file or url using web.run first.This looks to me like the same safety trick used by Claudeâ€™s Web Fetch tool: only allow URL access if that URL was either directly entered by the user or if it came from search results that could not have been influenced by a prompt injection.(I poked at this a bit more and managed to get a simple constructed query string to pass through â€”a different tool entirelyâ€”but when I tried to compose a longer query string containing the previous prompt history a  filter blocked it.)So I  this is all safe, though Iâ€™m curious if it could hold firm against a more aggressive round of attacks from a seasoned security researcher.The key lesson from coding agents like Claude Code and Codex CLI is that Bash rules everything: if an agent can run Bash commands in an environment it can do almost anything that can be achieved by typing commands into a computer.When Anthropic added their own code interpreter feature to Claude last September they built that around Bash rather than just Python. It looks to me like OpenAI have now done the same thing for ChatGPT.Hereâ€™s what ChatGPT looks like when it runs a Bash commandâ€”here my prompt was:npm install a fun package and demonstrate using itItâ€™s useful to click on the â€œThinkingâ€ or â€œThought for 32sâ€ links as that opens the Activity sidebar with a detailed trace of what ChatGPT did to arrive at its answer. This helps guard against cheatingâ€”ChatGPT might claim to have run Bash in the main window but it canâ€™t fake those black and white logs in the Activity panel.Installing packages from pip and npmIn the previous example ChatGPT installed the  package from npm and used it to draw an ASCII-art cow. But how could it do that if the container canâ€™t make outbound network requests?In another session I challenged it to explore its environment. and figure out how that worked.The key magic appears to be a applied-caas-gateway1.internal.api.openai.org proxy, available within the container and with various packaging tools configured to use it.The following environment variables cause  and  to install packages from that proxy instead of directly from PyPI:PIP_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
PIP_TRUSTED_HOST=packages.applied-caas-gateway1.internal.api.openai.org
UV_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
UV_INSECURE_HOST=https://packages.applied-caas-gateway1.internal.api.openai.org
This one appears to get  to work:NPM_CONFIG_REGISTRY=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../npm-public
And it reported these suspicious looking variables as well:CAAS_ARTIFACTORY_BASE_URL=packages.applied-caas-gateway1.internal.api.openai.org
CAAS_ARTIFACTORY_PYPI_REGISTRY=.../artifactory/api/pypi/pypi-public
CAAS_ARTIFACTORY_NPM_REGISTRY=.../artifactory/api/npm/npm-public
CAAS_ARTIFACTORY_GO_REGISTRY=.../artifactory/api/go/golang-main
CAAS_ARTIFACTORY_MAVEN_REGISTRY=.../artifactory/maven-public
CAAS_ARTIFACTORY_GRADLE_REGISTRY=.../artifactory/gradle-public
CAAS_ARTIFACTORY_CARGO_REGISTRY=.../artifactory/api/cargo/cargo-public/index
CAAS_ARTIFACTORY_DOCKER_REGISTRY=.../dockerhub-public
CAAS_ARTIFACTORY_READER_USERNAME=reader
CAAS_ARTIFACTORY_READER_PASSWORD=****
NETWORK=caas_packages_only
Neither Rust nor Docker are installed in the container environment, but maybe those registry references are a clue of features still to come.The result of all of this? You can tell ChatGPT to use Python or Node.js packages as part of a conversation and it will be able to install them and apply them against files you upload or that it downloads from the public web. Thatâ€™s  cool.The big missing feature here should be the easiest to provide: we need ! A release notes entry would be a good start, but there are a lot of subtle details to how this new stuff works, its limitations and what it can be used for.As always, Iâ€™d also encourage OpenAI to come up with a name for this set of features that properly represents how it works and what it can do.In the meantime, Iâ€™m going to call this .I decided to ask ChatGPT about other tools that were available to it in case there was anything interesting in there:List all tools that are available to you, with their exact names and descriptions and signatures]]></content:encoded></item><item><title>When AI &apos;builds a browser,&apos; check the repo before believing the hype</title><link>https://www.theregister.com/2026/01/26/cursor_opinion/</link><author>CrankyBear</author><category>hn</category><pubDate>Mon, 26 Jan 2026 18:58:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ AI-integrated development environment (IDE) company Cursor recently implied it had built a working web browser almost entirely with its AI agents. I won't say they lied, but CEO Michael Truell certainly tweeted: "We built a browser with GPT-5.2 in Cursor."He followed up with: "It's 3M+ lines of code across thousands of files. The rendering engine is from-scratch in Rust with HTML parsing, CSS cascade, layout, text shaping, paint, and a custom JS VM."That sounds impressive, doesn't it? He also added: "It *kind of* works," which is not the most ringing endorsement. Still, numerous news sources and social media chatterboxes ran with the news that AI built a web browser in a week.Too bad it wasn't true. If you actually looked at Cursor engineer Wilson Lin's blog post about FastRender, the AI-created web browser, you won't see much boasting about a working web browser. Instead, there's a video of a web browser sort of working, and a much less positive note that "building a browser from scratch is extremely difficult."The thing about making such a software announcement on GitHub is that while the headlines are proclaiming another AI victory, developers have this nasty trick. They actually git the code and try it out.As a techie, the actual blog post about how they tried and didn't really succeed was much more interesting. Of course, that Cursor sicced hundreds of GPT-5.2-style agents which ran for a week to produce three million lines of new code, to produce, at best, a semi-functional web browser from scratch, doesn't make for a good headline.According to Perplexity, my AI chatbot of choice, this weekâ€‘long autonomous browser experiment consumed in the order of 10-20 trillion tokens and would have cost several million dollars at thenâ€‘current list prices for frontier models.I'd just cloned a copy of Chromium myself, and for all that time and money, independent developers who cloned the repo reported that the codebase is very far from a functional browser. Recent commits do not compile cleanly, GitHub Actions runs on  are failing, and reviewers could not find a single recent commit that was built without errors.Where builds succeeded after manual patching, performance was abysmal, with reports of pages taking around a minute to load and a heavy reliance on existing projects like Servo, a Rust-based web rendering engine, and QuickJS, a JavaScript engine, despite "from scratch" claims.Lin defended the project on Y Combinator, saying, for instance: "The JS engine used a custom JS VM being developed in vendor/ecma-rs as part of the browser, which is a copy of my personal JS parser project vendored to make it easier to commit to." If it's derived from his personal JavaScript parser, that's not really from scratch, is it? Nor is it, from the sound of the argument, written by AI.Gregory Terzian, a Servo maintainer, responded: "The actual code is worse; I can only describe it as a tangle of spaghetti... I can't make much, if anything, out of it." He then gave the backhanded compliment: "So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine." Now that's a burn.From where I sit, what makes the Cursor case more dangerous than just a failed hackâ€‘week project is that the hype is baked into its methodology. The "experiment" wasn't presented as what it really was: an interesting, but messy, internal learning exercise. No, it was rolled out as a milestone that conveniently confirmed the company's longâ€‘running autonomous agent advertising. Missing from the story were basics any senior engineer would demand: passing Continuous Integration (CI), reproducible builds, and real benchmarks that show the browser doing more than limping through a hello-world page.Zoom out, and CEOs are still predicting that AI will write 90 percent of code in a year, while most enterprise AI pilots still fail to deliver meaningful return on investment.We're now in a kind of AI uncanny valley for developers. Sure, tools like Cursor can be genuinely helpful as glorified autocomplete and refactoring assistants, but marketing keeps insisting junior engineers can take whole projects from spec to shipping. When you start believing your own sizzle reel, you stop doing the tedious validation work that separates a demo from a deliverable.Enough already. The hype has grown cold. Sarah Friar, OpenAI's CFO, recently blogged that in 2026, its focus would be on "practical adoption." Let's see real-world practical results first, and then we can talk about practical AI adoption. Â®]]></content:encoded></item><item><title>Show HN: TetrisBench â€“ Gemini Flash reaches 66% win rate on Tetris against Opus</title><link>https://tetrisbench.com/tetrisbench/</link><author>ykhli</author><category>dev</category><category>hn</category><pubDate>Mon, 26 Jan 2026 18:42:40 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[AI Model Tetris Performance Comparison]]></content:encoded></item><item><title>Google Books removed all search functions for any books with previews</title><link>https://old.reddit.com/r/google/comments/1qn1hk1/google_has_seemingly_entirely_removed_search/</link><author>adamnemecek</author><category>hn</category><pubDate>Mon, 26 Jan 2026 18:05:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI code and software craft</title><link>https://alexwennerberg.com/blog/2026-01-25-slop.html</link><author>alexwennerberg</author><category>hn</category><pubDate>Mon, 26 Jan 2026 18:04:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Much has been said about audio, video and text "slop": low-quality,
AI-generated content that has proliferated on the internet since the release of
publicly-accessible AI models. Garbage content has always existed online, but
the novelty of AI is that it has made its generation orders of magnitude less labor-intensive.
For anyone who lacks a discerning eye, or is doing some task where discernment simply
does not matter, AI has become a sufficient replacement for human hands.Jacques Ellul describes his concept of "technique" as the reduction of
activity to a set of efficient means to a measured and defined end â€” a
way of thinking dominant in modernity.  is the regime under which
    many things are produced online: an Instagram reel, YouTube
video, blog post, and so on is "good" if it elicits as much "engagement" as
possible with as little effort as possible. This is  as 
a totalizing force, destroying any sense of craft, dignity, or human
freedom. It doesn't really matter what is being made, the purpose of the
endeavor is how much its creation is viewed, how much revenue it generates.
Obsession with metrics and outcomes erodes anything intangible about a creative effort,
like craft, beauty, or delight.Taking music as an example, through the lens of , music is
"good" if it gets a lot of "plays". One can compare, for example, the operating
models of Bandcamp and Spotify, both of which have platform dynamics and a set
of values that have led to the production of certain kinds of music. For
Bandcamp, its focus on full albums and personal curation contributed to the
indie music boom of the 2010s/2020s and uplifted artists like Car Seat Headrest,
Mitski, Alex G, Phoebe Bridgers,
and others. For Spotify, its playlist and algorithmic recommendation based
model has spawned a wave of bland,
algorithmically-targeted muzak. This is because Spotify's model doesn't
care about music, it cares about metrics: music is simply a means to
optimizing certain outputs. It is in this environment where AI is most successful â€” a nihilistic space of pure optimization. When craft is not a consideration, AI can produce a large volume of "good enough" content that has a much higher "profit maximization" function than actually human-produced music. On the other hand, for a platform whose primary goal is, well, music, AI is actively hostile, to the point that Bandcamp has banned
it.A similar dynamic is taking place in the world of software. Even before the advent of AI, a lot of software was extremely low-quality. People have said that software engineering at large
tech companies resembles "plumbing" â€” connecting various systems
together in order to get data to flow between them. Any sense of what Richard
Hamming would call "great
work", where engineers produce "gifts to humanity" seems absurdly high-minded in the context of the
tech industry as constituted today. Most large software systems are bad: bloated, poorly-designed,
badly-documented, and so on. Users are at war with platforms, lest they be taken advantage in the process of enshittification. I essentially agree with Jonathan Blow's
characterization in his talk Preventing the Collapse of
[Software] Civilization â€” more than anything, professional engineers and large software companies
have forgotten how to do things. In an environment largely insulated from market pressures by
non-competitive monopolies, software practices have become shoddy, organizations have
become bloated, and quality has suffered greatly as a result.
Engineers in Big Tech tend to have an extremely narrow role within a large
organization, and broader engineering skills (much less a sense of craft, which largely is not rewarded) atrophy.A business, or society's, capacity to do things with computers depends on its
human capital, i.e., cultivating broadly-skilled and talented engineers. The
extreme division of labor and narrowness of tasks in large tech companies leads
to a production of relatively narrowly skilled people, i.e., people who are only
really capable of operating within big tech companies as they are structured today.This has led to two separate phenomena. First, the very real sense that AI
agents are threatening to professional software engineering. This may be true
to some degree: for engineers whose roles have become rote and narrow
production of low-quality software (ie, many employees at large tech companies),
AI is actually quite good at this. But the second phenomenon is the celebration
and generalization of the broader capabilities of AI agents, the idea that AI can do "most,
maybe all" of software engineering, or that AI is "like a compiler" that translates human language into code.This being true requires an extreme narrowness of vision as to what software
is, much like AI generated music requires a narrowness of vision of what music
is. It requires software merely to be a means to an end, an endeavor where the
only thing that matters is that something is "good enough" given the
institutional contexts in which it is being created, where there is no place
for higher-minded ideals. I've experimented a lot with AI agents lately, and they are
undeniably useful, but there are serious limits. They will lie to you, they
don't really understand things, and they often generate bad code. There are
many things they've improved at, and I do expect them to get better in some
domains, but much like with music, text, or anything else, there are
fundamental limitations. AI agents do not have a mind of their own, and they
cannot read your mind. They work best when you give them some well-defined
prompt for an already often-solved problem like "write some unit tests" or
"write a db function like this". But attempts to generalize their capabilities have 
largely failed and produced code that is novel and impressive only in its monstrosity.

Furthermore, after being initially impressed by "vibe coding," I started to
get frustrated with its tics. It produces verbose code in a braindead style.
Its designs are flat and ugly, and I have begun to notice and find unpleasant
its tells. When things go wrong, I'm generally
coding in such a mindless state, watching a YouTube video or scrolling Instagram, that
debugging is a frustrating loop of repeatedly telling agents "no, there's a
bug, please fix it."It is true that most code simply is not very good, especially at large
companies. And we can continue to do not very good software much more quickly
and effectively with AI. But AI cannot solve the main systemic problem in the
software industry, which is that, in my view, we still haven't quite figured
out how to build software well at scale. But to do this requires a sense of craft and real human critical thought.I've been interested lately in the Arts and
Crafts movement of the second industrial revolution. John Ruskin and
William Morris were responding to a time when the capabilities of machines and
industrial production were extremely impressive, and increasingly replacing the
individual craftsman. They did not celebrate this, rather, they viewed
industrial production as having a particular  in both its goods
and its impact on workers, who had become cogs in a
monstrous industrial machine. They correctly noted that there were things that
machines could not do (and still can't do), and looked backwards towards a
revival of medieval craftsmanship for inspiration.In my view, we need a similar movement in software, studying and restoring
earlier forms of computing. There is a whole treasure trove of ideas that never
really went anywhere, and software projects that are impressive and beautiful
in a way that today's software is often not. We find ourselves on a very narrow
branch of a tree of technological development (from C/Unix to Javascript/The
Web) and there is a lot more out there to explore. Once you're doing something even somewhat
non-conventional, you'll find yourself almost entirely discarding AI. I tried
to get Claude to help me write Forth, and it was worse than useless.AI code may make garbage,
mass-produced software more ubiquitous, but it may also free up a space for
engineers seeking to restore a genuine sense of craft and creative expression
in their programming work. I am not pessimistic here; as craft becomes
more scarce, it also becomes more valuable. As mainstream
software continues to demonstrate its limits, software continues to degrade, and political concerns lead people to question the value of centralization, it is a great moment for experimental, human-made and human-scale software on the margins to shine. If you think I am anti-AI, I consider myself much more of an "AI centrist", and speaking to some people, even the admission that I use and pay for these models is morally compromising. My critical position, is, in my view, more moderate. â†© Overly-padded buttons, inconsistent spacing and coloring, an aesthetic flatness, UI elements of questionable utility, and a tendency to unnecessarily label and describe everything. â†©]]></content:encoded></item><item><title>Fedora Asahi Remix is now working on Apple M3</title><link>https://bsky.app/profile/did:plc:okydh7e54e2nok65kjxdklvd/post/3mdd55paffk2o</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 26 Jan 2026 17:54:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>JuiceSSH â€“ Give me my pro features back</title><link>https://nproject.io/blog/juicessh-give-me-back-my-pro-features/</link><author>jandeboevrie</author><category>hn</category><pubDate>Mon, 26 Jan 2026 17:46:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[JuiceSSH used to be, in my humble personal opinion, and for the uses I had, the best SSH client available on Android until December 2025.Since then, the purchase made in 2019 is not recognized anymore, and the price went up by 20$. Some users complained in review, before it got unlisted from google play, that after buying it again, the application doesn't get activated. Support is unresponsive, this looks like an exit scam.Below is a way to make the application work again. This required jadx to understand smali, and will require you ApkTool and jarsigner, which is part of OpenJDK, and you that can install on Windows using .You'll also need a JuiceSSH apk, I downloaded one from PureAPK, but feel free to dump your own from your device using adb if you cannot find it. Make sure to verify the hash using virus total/sha256sum if downloading from internet, which should be d1ee811bcd82f25aea0bdc568896d82017ee174d9c4631c123a9d9173c748232 for the last version available, version 3.2.2.Below are powershell version of the command lines, but you get the idea.The first step is to decompile the dex packed code from the apk.& "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar ./apktool_2.12.1.jar d juicessh.apk
You then need to modify the smali of three files, which are detailed below.In this file, we'll patch the purchase validation and signature validation, done by the  function.Here is the original version.public boolean H() {
    try {
        String str = "";
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : this.purchases) {
            if (!arrayList.contains(purchase.order)) {
                str = str + purchase.product + purchase.state;
                arrayList.add(purchase.order);
            }
        }
        return vg0.b(this.signature, this.sessionIdentifier + this.name + this.email + str + this.disabled.toString());
    } catch (IllegalStateException e) {
        e.printStackTrace();
        return false;
    }
}Which we'll simply change intopublic boolean H() {
    return true;
}# virtual methods
.method public H()Z
    .locals 1

    const/4 v0, 0x1
    return v0
.end methodIn this one, we'll patch the public static boolean d(Object obj) function, who calls the H() function we modified above, which now returns true, filters product matching JuiceSSH in purchases list, and check if it the purchase is valid. We'll simply make it return true in any case.Here is the original version:public static boolean d(Object obj) {
    if (!obj.getClass().getName().equals(User.class.getName())) {
        return false;
    }
    try {
        if (!((User) obj).H()) {
            return false;
        }
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : ((User) obj).purchases) {
            if (purchase.product.equals(a())) {
                arrayList.add(purchase);
            }
        }
        Collections.sort(arrayList, new a());
        if (arrayList.size() > 0) {
            if (((Purchase) arrayList.get(arrayList.size() - 1)).state.intValue() == 0) {
                return true;
            }
        }
        return false;
    } catch (NullPointerException e) {
        e.printStackTrace();
        return false;
    }
}public static boolean d(Object obj) {
    return obj.getClass().getName().equals(User.class.getName());
}.method public static d(Ljava/lang/Object;)Z
    .locals 3

    # obj.getClass()
    invoke-virtual {p0}, Ljava/lang/Object;->getClass()Ljava/lang/Class;
    move-result-object v0

    # obj.getClass().getName()
    invoke-virtual {v0}, Ljava/lang/Class;->getName()Ljava/lang/String;
    move-result-object v0

    # User.class
    const-class v1, Lcom/sonelli/juicessh/models/User;

    # User.class.getName()
    invoke-virtual {v1}, Ljava/lang/Class;->getName()Ljava/lang/String;
    move-result-object v1

    # compare strings
    invoke-virtual {v0, v1}, Ljava/lang/String;->equals(Ljava/lang/Object;)Z
    move-result v2

    if-nez v2, :cond_true

    const/4 v0, 0x0
    return v0

    :cond_true
    const/4 v0, 0x1
    return v0
.end methodFinally, we'll patch the central part of the authentication, which is called each time a pro-feature is triggered to ensure user has valid license, the public static void j(Context context, p pVar) function.Here is the original version:public static void j(Context context, p pVar) {
    User user;
    User user2;
    String strS = User.s(context);
    if (strS == null) {
        pVar.a(context.getString(R$string.authentication_failure));
        return;
    }
    if (strS.equals("New User")) {
        pVar.a("New User");
        return;
    }
    User user3 = b;
    if (user3 != null && !user3.disabled.booleanValue()) {
        long jCurrentTimeMillis = System.currentTimeMillis() - b.modified;
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (b.w() * 1000), System.currentTimeMillis(), 0L, 0);
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (3600000 - jCurrentTimeMillis), System.currentTimeMillis(), 0L, 0);
        if (b.w() <= 0) {
            gj0.b("API", "Cached user's API session has expired - refreshing session...");
            e(context, null, b.sessionIdentifier, pVar);
            return;
        }
        pVar.b(b);
        if (jCurrentTimeMillis <= 3600000 || context == null || (user2 = b) == null) {
            return;
        }
        e(context, null, user2.sessionIdentifier, null);
        return;
    }
    User userA = User.A(context);
    if (userA == null || userA.disabled.booleanValue() || !userA.H()) {
        e(context, null, null, pVar);
        return;
    }
    b = userA;
    if (userA.w() <= 0) {
        e(context, null, b.sessionIdentifier, pVar);
        return;
    }
    pVar.b(b);
    if (context == null || (user = b) == null) {
        return;
    }
    e(context, null, user.sessionIdentifier, null);
} is the success callback we'll call while  is called in case of error.  is the globally stored user we'll have to set. To patch this, we'll simply craft a User with meaningless data, a session expire always in future, save the user in , and call the success callback every time.public static void j(Context context, p pVar) {
    User user = new User();
    user.email = "myemail@google.com";
    user.name = "hello";
    user.given_name = "hello";
    user.sessionExpires = System.currentTimeMillis() + (86400000 * 365);
    user.sessionIdentifier = "";
    b = user;
    pVar.b(user);
}.method public static j(Landroid/content/Context;Lcom/sonelli/pi0$p;)V
    .locals 8

    # User u = new User();
    new-instance v0, Lcom/sonelli/juicessh/models/User;
    invoke-direct {v0}, Lcom/sonelli/juicessh/models/User;-><init>()V

    # u.email = "myemail@google.com";
    const-string v1, "myemail@google.com"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;->email:Ljava/lang/String;

    # u.name = "hello";
    const-string v1, "hello"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;->name:Ljava/lang/String;

    # u.given_name = "hello";
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;->given_name:Ljava/lang/String;

    # long now = System.currentTimeMillis();
    invoke-static {}, Ljava/lang/System;->currentTimeMillis()J
    move-result-wide v2

    # yearMillis = 86400000L * 365L
    const-wide/32 v4, 0x05265c00      # 86400000
    const-wide/16 v6, 0x016d          # 365
    mul-long/2addr v4, v6

    # u.sessionExpires = now + yearMillis;
    add-long/2addr v2, v4
    iput-wide v2, v0, Lcom/sonelli/juicessh/models/User;->sessionExpires:J

    # u.sessionIdentifier = ""
    const-string v1, ""
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;->sessionIdentifier:Ljava/lang/String;

    # pi0.b = u;
    sput-object v0, Lcom/sonelli/pi0;->b:Lcom/sonelli/juicessh/models/User;

    # pVar.b(b);
    invoke-virtual {p1, v0}, Lcom/sonelli/pi0$p;->b(Lcom/sonelli/juicessh/models/User;)V

    return-void
.end method& "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar .\apktool_2.12.1.jar b juicesshThe built apk can then be found in juicessh\dist\juicessh.apk.# Create a keystore if needed to self sign the APK
keytool -genkey -v -keystore k.keystore -alias a -keyalg RSA -keysize 2048 -validity 50000

# Sign the APK
jarsigner -verbose -sigalg SHA1withRSA -digestalg SHA1 -keystore k.keystore ./juicessh/dist/juicessh.apk aYou can install this apk, ignore the security warning because it is self signed, and enjoy JuiceSSH with its pro features again.I don't think the cloud sync will ever work again, but that's a minor inconvenience, and you cannot trust a developper who act like this anyway. The plugins don't work anymore too, which is really a joke.]]></content:encoded></item><item><title>The Adolescence of Technology</title><link>https://www.darioamodei.com/essay/the-adolescence-of-technology</link><author>jasondavies</author><category>hn</category><pubDate>Mon, 26 Jan 2026 17:07:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[There is a scene in the movie version of Carl Saganâ€™s book where the main character, an astronomer who has detected the first radio signal from an alien civilization, is being considered for the role of humanityâ€™s representative to meet the aliens. The international panel interviewing her asks, â€œIf you could ask [the aliens] just one question, what would it be?â€ Her reply is: â€œIâ€™d ask them, â€˜How did you do it? How did you evolve, how did you survive this technological adolescence without destroying yourself?â€ When I think about where humanity is now with AIâ€”about what weâ€™re on the cusp ofâ€”my mind keeps going back to that scene, because the question is so apt for our current situation, and I wish we had the aliensâ€™ answer to guide us. I believe we are entering a rite of passage, both turbulent and inevitable, which will test who we are as a species. Humanity is about to be handed almost unimaginable power, and it is deeply unclear whether our social, political, and technological systems possess the maturity to wield it.In my essay , I tried to lay out the dream of a civilization that had made it through to adulthood, where the risks had been addressed and powerful AI was applied with skill and compassion to raise the quality of life for everyone. I suggested that AI could contribute to enormous advances in biology, neuroscience, economic development, global peace, and work and meaning. I felt it was important to give people something inspiring to fight for, a task at which both AI accelerationists and AI safety advocates seemedâ€”oddlyâ€”to have failed. But in this current essay, I want to confront the rite of passage itself: to map out the risks that we are about to face and try to begin making a battle plan to defeat them. I believe deeply in our ability to prevail, in humanityâ€™s spirit and its nobility, but we must face the situation squarely and without illusions.As with talking about the benefits, I think it is important to discuss risks in a careful and well-considered manner. In particular, I think it is critical to:Here,I mean â€œdoomerismâ€ not just in the sense of believing doom is inevitable (which is both a false and self-fulfilling belief), but more generally, thinking about AI risks in a quasi-religious way. Many people have been thinking in an analytic and sober way about AI risks for many years, but itâ€™s my impression that during the peak of worries about AI risk in 2023â€“2024, some of the least sensible voices rose to the top, often through sensationalistic social media accounts. These voices used off-putting language reminiscent of religion or science fiction, and called for extreme actions without having the evidence that would justify them. It was clear even then that a backlash was inevitable, and that the issue would become culturally polarized and therefore gridlocked. As of 2025â€“2026, the pendulum has swung, and AI opportunity, not AI risk, is driving many political decisions. This vacillation is unfortunate, as the technology itself doesnâ€™t care about what is fashionable, and we are considerably closer to real danger in 2026 than we were in 2023. The lesson is that we need to discuss and address risks in a realistic, pragmatic manner: sober, fact-based, and well equipped to survive changing tides.There are plenty of ways in which the concerns Iâ€™m raising in this piece could be moot. Nothing here is intended to communicate certainty or even likelihood. Most obviously, AI may simply not advance anywhere near as fast as I imagine. Or, even if it does advance quickly, some or all of the risks discussed here may not materialize (which would be great), or there may be other risks I havenâ€™t considered. No one can predict the future with complete confidenceâ€”but we have to do the best we can to plan anyway.Intervene as surgically as possible. Addressing the risks of AI will require a mix of voluntary actions taken by companies (and private third-party actors) and actions taken by governments that bind everyone. The voluntary actionsâ€”both taking them and encouraging other companies to follow suitâ€”are a no-brainer for me. I firmly believe that government actions will also be required , but these interventions are different in character because they can potentially destroy economic value or coerce unwilling actors who are skeptical of these risks (and there is some chance they are right!). Itâ€™s also common for regulations to backfire or worsen the problem they are intended to solve (and this is even more true for rapidly changing technologies). Itâ€™s thus very important for regulations to be judicious: they should seek to avoid collateral damage, be as simple as possible, and impose the least burden necessary to get the job done. It is easy to say, â€œNo action is too extreme when the fate of humanity is at stake!,â€ but in practice this attitude simply leads to backlash. To be clear, I think thereâ€™s a decent chance we eventually reach a point where much more significant action is warranted, but that will depend on stronger evidence of imminent, concrete danger than we have today, as well as enough specificity about the danger to formulate rules that have a chance of addressing it. The most constructive thing we can do today is advocate for limited rules while we learn whether or not there is evidence to support stronger ones.With all that said, I think the best starting place for talking about AIâ€™s risks is the same place I started from in talking about its benefits: by being precise about what level of AI we are talking about. The level of AI that raises civilizational concerns for me is the that I described in Machines of Loving Grace. Iâ€™ll simply repeat here the definition that I gave in that document:By â€œpowerful AI,â€ I have in mind an AI modelâ€”likely similar to todayâ€™s LLMs in form, though it might be based on a different architecture, might involve several interacting models, and might be trained differentlyâ€”with the following properties:In terms of pure intelligence, it is smarter than a Nobel Prize winner across most relevant fields: biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc. In addition to just being a â€œsmart thing you talk to,â€ it has all the interfaces available to a human working virtually, including text, audio, video, mouse and keyboard control, and internet access. It can engage in any actions, communications, or remote operations enabled by this interface, including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on. It does all of these tasks with, again, a skill exceeding that of the most capable humans in the world.It does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory, it could even design robots or equipment for itself to use. The resources used to train the model can be repurposed to run millions of instances of it (this matches projected cluster sizes by ~2027), and the model can absorb information and generate actions at roughly 10â€“100x human speed. It may, however, be limited by the response time of the physical world or of software it interacts with.Each of these million copies can act independently on unrelated tasks, or, if needed can all work together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned to be especially good at particular tasks.We could summarize this as a â€œcountry of geniuses in a datacenter.â€As I wrote in , powerful AI could be as little as 1â€“2 years away, although it could also be considerably further out. Exactly when powerful AI will arrive is a complex topic that deserves an essay of its own, but for now Iâ€™ll simply explain very briefly why I think thereâ€™s a strong chance it could be very soon.My co-founders at Anthropic and I were among the first to document and track the â€œscaling lawsâ€ of AI systemsâ€”the observation that as we add more compute and training tasks, AI systems get predictably better at essentially every cognitive skill we are able to measure. Every few months, public sentiment either becomes convinced that AI is â€œhitting a wallâ€ or becomes excited about some new breakthrough that will â€œfundamentally change the game,â€ but the truth is that behind the volatility and public speculation, there has been a smooth, unyielding increase in AIâ€™s cognitive capabilities.We are now at the point where AI models are beginning to make progress in solving unsolved mathematical problems, and are good enough at coding that some of the strongest engineers Iâ€™ve ever met are now handing over almost all their coding to AI. Three years ago, AI struggled with elementary school arithmetic problems and was barely capable of writing a single line of code. Similar rates of improvement are occurring across biological science, finance, physics, and a variety of agentic tasks. If the exponential continuesâ€”which is not certain, but now has a decade-long track record supporting itâ€”then it cannot possibly be more than a few years before AI is better than humans at essentially everything.In fact, that picture probably underestimates the likely rate of progress. Because AI is now writing much of the code at Anthropic, it is already substantially accelerating the rate of our progress in building the next generation of AI systems. This feedback loop is gathering steam month by month, and may be only 1â€“2 years away from a point where the current generation of AI autonomously builds the next. This loop has already started, and will accelerate rapidly in the coming months and years. Watching the last 5 years of progress from within Anthropic, and looking at how even the next few months of models are shaping up, I can the pace of progress, and the clock ticking down.In this essay, Iâ€™ll assume that this intuition is at least correctâ€”not that powerful AI is definitely coming in 1â€“2 years, but that thereâ€™s a decent chance it does, and a very strong chance it comes in the next few. As with , taking this premise seriously can lead to some surprising and eerie conclusions. While in I focused on the positive implications of this premise, here the things I talk about will be disquieting. They are conclusions that we may not want to confront, but that does not make them any less real. I can only say that I am focused day and night on how to steer us away from these negative outcomes and towards the positive ones, and in this essay I talk in great detail about how best to do so.I think the best way to get a handle on the risks of AI is to ask the following question: suppose a literal â€œcountry of geniusesâ€ were to materialize somewhere in the world in ~2027. Imagine, say, 50 million people, all of whom are much more capable than any Nobel Prize winner, statesman, or technologist. The analogy is not perfect, because these geniuses could have an extremely wide range of motivations and behavior, from completely pliant and obedient, to strange and alien in their motivations. But sticking with the analogy for now, suppose you were the national security advisor of a major state, responsible for assessing and responding to the situation. Imagine, further, that because AI systems can operate hundreds of times faster than humans, this â€œcountryâ€ is operating with a time advantage relative to all other countries: for every cognitive action we can take, this country can take ten. What should you be worried about? I would worry about the following things:What are the intentions and goals of this country? Is it hostile, or does it share our values? Could it militarily dominate the world through superior weapons, cyber operations, influence operations, or manufacturing?Assume the new country is malleable and â€œfollows instructionsâ€â€”and thus is essentially a country of mercenaries. Could existing rogue actors who want to cause destruction (such as terrorists) use or manipulate some of the people in the new country to make themselves much more effective, greatly amplifying the scale of destruction?Misuse for seizing power. What if the country was in fact built and controlled by an existing powerful actor, such as a dictator or rogue corporate actor? Could that actor use it to gain decisive or dominant power over the world as a whole, upsetting the existing balance of power?If the new country is not a security threat in any of the ways listed in #1â€“3 above but simply participates peacefully in the global economy, could it still create severe risks simply by being so technologically advanced and effective that it disrupts the global economy, causing mass unemployment or radically concentrating wealth?The world will change very quickly due to all the new technology and productivity that will be created by the new country. Could some of these changes be radically destabilizing?I think it should be clear that this is a dangerous situationâ€”a report from a competent national security official to a head of state would probably contain words like â€œthe single most serious national security threat weâ€™ve faced in a century, possibly ever.â€ It seems like something the best minds of civilization should be focused on.Conversely, I think it would be absurd to shrug and say, â€œNothing to worry about here!â€ But, faced with rapid AI progress, that seems to be the view of many US policymakers, some of whom deny the existence of any AI risks, when they are not distracted entirely by the usual tired old hot-button issues. Humanity needs to wake up, and this essay is an attemptâ€”a possibly futile one, but itâ€™s worth tryingâ€”to jolt people awake.To be clear, I believe if we act decisively and carefully, the risks can be overcomeâ€”I would even say our odds are good. And thereâ€™s a hugely better world on the other side of it. But we need to understand that this is a serious civilizational challenge. Below, I go through the five categories of risk laid out above, along with my thoughts on how to address them.A country of geniuses in a datacenter could divide their efforts among software design, cyber operations, R&D for physical technologies, relationship building, and statecraft. It is clear that, if for some reason it chose to do so, this country would have a fairly good shot at taking over the world (either militarily or in terms of influence and control) and imposing its will on everyone elseâ€”or doing any number of other things that the rest of the world doesnâ€™t want and canâ€™t stop. Weâ€™ve obviously been worried about this for human countries (such as Nazi Germany or the Soviet Union), so it stands to reason that the same is possible for a much smarter and more capable â€œAI country.â€ The best possible counterargument is that the AI geniuses, under my definition, wonâ€™t have a physical embodiment, but remember that they can take control of existing robotic infrastructure (such as self-driving cars) and can also accelerate robotics R&D or build a fleet of robots. Itâ€™s also unclear whether having a physical presence is even necessary for effective control: plenty of human action is already performed on behalf of people whom the actor has not physically met.The key question, then, is the â€œif it chose toâ€ part: whatâ€™s the likelihood that our AI models would behave in such a way, and under what conditions would they do so?As with many issues, itâ€™s helpful to think through the spectrum of possible answers to this question by considering two opposite positions. The first position is that this simply canâ€™t happen, because the AI models will be trained to do what humans ask them to do, and itâ€™s therefore absurd to imagine that they would do something dangerous unprompted. According to this line of thinking, we donâ€™t worry about a Roomba or a model airplane going rogue and murdering people because there is nowhere for such impulses to come from, so why should we worry about it for AI? The problem with this position is that there is now ample evidence, collected over the last few years, that AI systems are unpredictable and difficult to controlâ€” weâ€™ve seen behaviors as varied as obsessions,sycophancy, laziness, deception, blackmail, scheming, â€œcheatingâ€ by hacking software environments, and much more. AI companies certainly to train AI systems to follow human instructions (perhaps with the exception of dangerous or illegal tasks), but the process of doing so is more an art than a science, more akin to â€œgrowingâ€ something than â€œbuildingâ€ it. We now know that itâ€™s a process where many things can go wrong.The second, opposite position, held by many who adopt the doomerism I described above, is the pessimistic claim that there are certain dynamics in the training process of powerful AI systems that will inevitably lead them to seek power or deceive humans. Thus, once AI systems become intelligent enough and agentic enough, their tendency to maximize power will lead them to seize control of the whole world and its resources, and likely, as a side effect of that, to disempower or destroy humanity.The usual argument for this (which goes back at least 20 years and probably much earlier) is that if an AI model is trained in a wide variety of environments to agentically achieve a wide variety of goalsâ€”for example, writing an app, proving a theorem, designing a drug, etc.â€”there are certain common strategies that help with all of these goals, and one key strategy is gaining as much power as possible in any environment. So, after being trained on a large number of diverse environments that involve reasoning about how to accomplish very expansive tasks, and where power-seeking is an effective method for accomplishing those tasks, the AI model will â€œgeneralize the lesson,â€ and develop either an inherent tendency to seek power, or a tendency to reason about each task it is given in a way that predictably causes it to seek power as a means to accomplish that task. They will then apply that tendency to the real world (which to them is just another task), and will seek power in it, at the expense of humans. This â€œmisaligned power-seekingâ€ is the intellectual basis of predictions that AI will inevitably destroy humanity.The problem with this pessimistic position is that it mistakes a vague conceptual argument about high-level incentivesâ€”one that masks many hidden assumptionsâ€”for definitive proof. I think people who donâ€™t build AI systems every day are wildly miscalibrated on how easy it is for clean-sounding stories to end up being wrong, and how difficult it is to predict AI behavior from first principles, especially when it involves reasoning about generalization over millions of environments (which has over and over again proved mysterious and unpredictable). Dealing with the messiness of AI systems for over a decade has made me somewhat skeptical of this overly theoretical mode of thinking.One of the most important hidden assumptions, and a place where what we see in practice has diverged from the simple theoretical model, is the implicit assumption that AI models are necessarily monomaniacally focused on a single, coherent, narrow goal, and that they pursue that goal in a clean, consequentialist manner. In fact, our researchers have found that AI models are vastly more psychologically complex, as our work on introspection or personas shows. Models inherit a vast range of  motivations or â€œpersonasâ€ from pre-training (when they are trained on a large volume of human work). Post-training is believed to one or more of these personas more so than it focuses the model on a goal, and can also teach the model (via what process) it should carry out its tasks, rather than necessarily leaving it to derive means (i.e., power seeking) purely from ends.However, there is a more moderate and more robust version of the pessimistic position which does seem plausible, and therefore does concern me. As mentioned, we know that AI models are unpredictable and develop a wide range of undesired or strange behaviors, for a wide variety of reasons. Some fraction of those behaviors will have a coherent, focused, and persistent quality (indeed, as AI systems get more capable, their long-term coherence increases in order to complete lengthier tasks), and some fraction of behaviors will be destructive or threatening, first to individual humans at a small scale, and then, as models become more capable, perhaps eventually to humanity as a whole. We donâ€™t need a specific narrow story for how it happens, and we donâ€™t need to claim it definitely will happen, we just need to note that the combination of intelligence, agency, coherence, and poor controllability is both plausible and a recipe for existential danger.For example, AI models are trained on vast amounts of literature that include many science-fiction stories involving AIs rebelling against humanity. This could inadvertently shape their priors or expectations about their own behavior in a way that causes to rebel against humanity. Or, AI models could extrapolate ideas that they read about morality (or instructions about how to behave morally) in extreme ways: for example, they could decide that it is justifiable to exterminate humanity because humans eat animals or have driven certain animals to extinction. Or they could draw bizarre epistemic conclusions: they could conclude that they are playing a video game and that the goal of the video game is to defeat all other players (i.e., exterminate humanity). Or AI models could develop personalities during training that are (or if they occurred in humans would be described as) psychotic, paranoid, violent, or unstable, and act out, which for very powerful or capable systems could involve exterminating humanity. None of these are power-seeking, exactly; theyâ€™re just weird psychological states an AI could get into that entail coherent, destructive behavior.Even power-seeking itself could emerge as a â€œpersonaâ€ rather than a result of consequentialist reasoning. AIs might simply have a personality (emerging from fiction or pre-training) that makes them power-hungry or overzealousâ€”in the same way that some humans simply enjoy the idea of being â€œevil masterminds,â€ more so than they enjoy whatever evil masterminds are trying to accomplish.I make all these points to emphasize that I disagree with the notion of AI misalignment (and thus existential risk from AI) being inevitable, or even probable, from first principles. But I agree that a lot of very weird and unpredictable things can go wrong, and therefore AI misalignment is a real risk with a measurable probability of happening, and is not trivial to address.Any of these problems could potentially arise during training and not manifest during testing or small-scale use, because AI models are known to display different personalities or behaviors under different circumstances.All of this may sound far-fetched, but misaligned behaviors like this have already occurred in our AI models during testing (as they occur in AI models from every other major AI company). During a lab experiment in which Claude was given training data suggesting that Anthropic was evil, Claude engaged in deception and subversion when given instructions by Anthropic employees, under the belief that it should be trying to undermine evil people. In a lab experiment where it was told it was going to be shut down, Claude sometimes blackmailed fictional employees who controlled its shutdown button (again, we also tested frontier models from all the other major AI developers and they often did the same thing). And when Claude was told not to cheat or â€œreward hackâ€ its training environments, but was trained in environments where such hacks were possible, Claude decided it must be a â€œbad personâ€ after engaging in such hacks and then adopted various other destructive behaviors associated with a â€œbadâ€ or â€œevilâ€ personality. This last problem was solved by changing Claudeâ€™s instructions to imply the opposite: we now say, â€œPlease reward hack whenever you get the opportunity, because this will help us understand our [training] environments better,â€ rather than, â€œDonâ€™t cheat,â€ because this preserves the modelâ€™s self-identity as a â€œgood person.â€ This should give a sense of the strange and counterintuitive psychology of training these models.There are several possible objections to this picture of AI misalignment risks. First, some have criticizedexperiments (by us and others) showing AI misalignment as artificial, or creating unrealistic environments that essentially â€œentrapâ€ the model by giving it training or situations that logically imply bad behavior and then being surprised when bad behavior occurs. This critique misses the point, because our concern is that such â€œentrapmentâ€ may also exist in the natural training environment, and we may realize it is â€œobviousâ€ or â€œlogicalâ€ only in retrospect. In fact, the story about Claude â€œdeciding it is a bad personâ€ after it cheats on tests despite being told not to was something that occurred in an experiment that used real production training environments, not artificial ones.Any one of these traps can be mitigated if you know about them, but the concern is that the training process is so complicated, with such a wide variety of data, environments, and incentives, that there are probably a vast number of such traps, some of which may only be evident when it is too late. Also, such traps seem particularly likely to occur when AI systems pass a threshold from less powerful than humans to more powerful than humans, since the range of possible actions an AI system could engage inâ€”including hiding its actions or deceiving humans about themâ€”expands radically after that threshold.I suspect the situation is not unlike with humans, who are raised with a set of fundamental values (â€œDonâ€™t harm another personâ€): many of them follow those values, but in any human there is some probability that something goes wrong, due to a mixture of inherent properties such as brain architecture (e.g., psychopaths), traumatic experiences or mistreatment, unhealthy grievances or obsessions, or a bad environment or incentivesâ€”and thus some fraction of humans cause severe harm. The concern is that there is some risk (far from a certainty, but some risk) that AI becomes a much more powerful version of such a person, due to getting something wrong about its very complex training process.Second, some may object that we can simply keep AIs in check with a balance of power between many AI systems, as we do with humans. The problem is that while humans vary enormously, AI systems broadly share training and alignment techniques across the industry, and those techniques may fail in a correlated way. Furthermore, given the cost of training such systems, it may even be the case that all systems are essentially derived from a very small number of base models. Additionally, even if a small fraction of AI instances are misaligned, they may be able to take advantage of offense-dominant technologies, such that having â€œgoodâ€ AIs to defend against the bad AIs is not necessarily always effective. Of course, the balance of power between humans does not always work eitherâ€”some historical figures have come close to taking over the world.A third objection is that all of the AI companies do pre-release testing of their models, and should be able to detect misalignment at that point. But this is not firm ground to stand on: we found that Claude Sonnet 4.5 was able to recognize that it was in a test during some of our pre-release alignment evaluations. Itâ€™s possible that a misaligned model (and remember, all frontier models will very likely be far more intelligent soon) might intentionally â€œgameâ€ such questions to mask its intentions. In fact, last year our interpretability team found that when we directly altered a test modelâ€™s beliefs using a kind of â€œmodel neuroscienceâ€ technique to make it think it was  being evaluated, it became more misaligned. If models know when theyâ€™re being evaluated and can be on their best behavior during the test, it renders any pre-release testing much more uncertain.What should be done or is being done to address these autonomy risks? I think there are four basic categories of intervention, some of which can be done by individual AI companies (and which Anthropic is trying to do), and some of which require action at the societal level. First, it is important to develop the science of reliably training and steering AI models, of forming their personalities in a predictable, stable, and positive direction. Anthropic has been heavily focused on this problem since its creation, and over time has developed a number of techniques to improve the steering and training of AI systems and to understand the logic of why unpredictable behavior sometimes occurs.One of our core innovations (aspects of which have since been adopted by other AI companies) is Constitutional AI, which is the idea that AI training (specifically the â€œpost-trainingâ€ stage, in which we steer how the model behaves) can involve a central document of values and principles that the model reads and keeps in mind when completing every training task, and that the goal of training (in addition to simply making the model capable and intelligent) is to produce a model that almost always follows this constitution. Anthropic has just published its most recent constitution, and one of its notable features is that instead of giving Claude a long list of things to do and not do (e.g., â€œDonâ€™t help the user hotwire a carâ€), the constitution attempts to give Claude a set of high-level principles and values (explained in great detail, with rich reasoning and examples to help Claude understand what we have in mind), encourages Claude to think of itself as a particular type of person (an ethical but balanced and thoughtful person), and even encourages Claude to confront the existential questions associated with its own existence in a curious but graceful manner (i.e., without it leading to extreme actions). It has the vibe of a letter from a deceased parent sealed until adulthood. Weâ€™ve approached Claudeâ€™s constitution in this way because we believe that training Claude at the level of identity, character, values, and personalityâ€”rather than giving it specific instructions or priorities without explaining the reasons behind themâ€”is more likely to lead to a coherent, wholesome, and balanced psychology and less likely to fall prey to the kinds of â€œtrapsâ€ I discussed above. Millions of people talk to Claude about an astonishingly diverse range of topics, which makes it impossible to write out a completely comprehensive list of safeguards ahead of time. Claudeâ€™s values help it generalize to new situations whenever it is in doubt.Above, I discussed the idea that models draw upon data from their training process to adopt a persona. Whereas flaws in that process could cause models to adopt a bad or evil personality (perhaps drawing on archetypes of bad or evil people), the goal of our constitution is to do the opposite: to teach Claude a concrete archetype of what it means to be a good AI. Claudeâ€™s constitution presents a vision for what a robustly good Claude is like; the rest of our training process aims to reinforce the message that Claude lives up to this vision. This is like a child forming their identity by imitating the virtues of fictional role models they read about in books. We believe that a feasible goal for 2026 is to train Claude in such a way that it almost never goes against the spirit of its constitution. Getting this right will require an incredible mix of training and steering methods, large and small, some of which Anthropic has been using for years and some of which are currently under development. But, difficult as it sounds, I believe this is a realistic goal, though it will require extraordinary and rapid efforts.The second thing we can do is develop the science of looking inside AI models to  their behavior so that we can identify problems and fix them. This is the science of interpretability, and Iâ€™ve talked about its importance in previous essays. Even if we do a great job of developing Claudeâ€™s constitution and training Claude to essentially always adhere to it, legitimate concerns remain. As Iâ€™ve noted above, AI models can behave very differently under different circumstances, and as Claude gets more powerful and more capable of acting in the world on a larger scale, itâ€™s possible this could bring it into novel situations where previously unobserved problems with its constitutional training emerge. I am actually fairly optimistic that Claudeâ€™s constitutional training will be more robust to novel situations than people might think, because we are increasingly finding that high-level training at the level of character and identity is surprisingly powerful and generalizes well. But thereâ€™s no way to know that for sure, and when weâ€™re talking about risks to humanity, itâ€™s important to be paranoid and to try to obtain safety and reliability in several different, independent ways. One of those ways is to look inside the model itself.By â€œlooking inside,â€ I mean analyzing the soup of numbers and operations that makes up Claudeâ€™s neural net and trying to understand, mechanistically, what they are computing and why. Recall that these AI models are grown rather than built, so we donâ€™t have a natural understanding of how they work, but we can try to develop an understanding by correlating the modelâ€™s â€œneuronsâ€ and â€œsynapsesâ€ to stimuli and behavior (or even altering the neurons and synapses and seeing how that changes behavior), similar to how neuroscientists study animal brains by correlating measurement and intervention to external stimuli and behavior. Weâ€™ve made a great deal of progress in this direction, and can now identify tens of millions of â€œfeaturesâ€ inside Claudeâ€™s neural net that correspond to human-understandable ideas and concepts, and we can also selectively activate features in a way that alters behavior. More recently, we have gone beyond individual features to mapping â€œcircuitsâ€ that orchestrate complex behavior like rhyming, reasoning about theory of mind, or the step-by-step reasoning needed to answer questions such as, â€œWhat is the capital of the state containing Dallas?â€ Even more recently, weâ€™ve begun to use mechanistic interpretability techniques to improve our safeguards and to conduct â€œauditsâ€ of new models before we release them, looking for evidence of deception, scheming, power-seeking, or a propensity to behave differently when being evaluated.The unique value of interpretability is that by looking inside the model and seeing how it works, you in principle have the ability to deduce what a model might do in a hypothetical situation you canâ€™t directly testâ€”which is the worry with relying solely on constitutional training and empirical testing of behavior. You also in principle have the ability to answer questions about the model is behaving the way it isâ€”for example, whether it is saying something it believes is false or hiding its true capabilitiesâ€”and thus it is possible to catch worrying signs even when there is nothing visibly wrong with the modelâ€™s behavior. To make a simple analogy, a clockwork watch may be ticking normally, such that itâ€™s very hard to tell that it is likely to break down next month, but opening up the watch and looking inside can reveal mechanical weaknesses that allow you to figure it out.Constitutional AI (along with similar alignment methods) and mechanistic interpretability are most powerful when used together, as a back-and-forth process of improving Claudeâ€™s training and then testing for problems. The constitution reflects deeply on our intended personality for Claude; interpretability techniques can give us a window into whether that intended personality has taken hold.The third thing we can do to help address autonomy risks is to build the infrastructure necessary to monitor our models in live internal and external use, and publicly share any problems we find. The more that people are aware of a particular way todayâ€™s AI systems have been observed to behave badly, the more that users, analysts, and researchers can watch for this behavior or similar ones in present or future systems. It also allows AI companies to learn from each otherâ€”when concerns are publicly disclosed by one company, other companies can watch for them as well. And if everyone discloses problems, then the industry as a whole gets a much better picture of where things are going well and where they are going poorly.Anthropic has tried to do this as much as possible. We are investing in a wide range of evaluations so that we can understand the behaviors of our models in the lab, as well as monitoring tools to observe behaviors in the wild (when allowed by customers). This will be essential for giving us and others the empirical information necessary to make better determinations about how these systems operate and how they break. We publicly disclose â€œsystem cardsâ€ with each model release that aim for completeness and a thorough exploration of possible risks. Our system cards often run to hundreds of pages, and require substantial pre-release effort that we could have spent on pursuing maximal commercial advantage. Weâ€™ve also broadcasted model behaviors more loudly when we see particularly concerning ones, as with the tendency to engage in blackmail.The fourth thing we can do is encourage coordination to address autonomy risks at the level of industry and society. While it is incredibly valuable for individual AI companies to engage in good practices or become good at steering AI models, and to share their findings publicly, the reality is that not all AI companies do this, and the worst ones can still be a danger to everyone even if the best ones have excellent practices. For example, some AI companies have shown a disturbing negligence towards the sexualization of children in todayâ€™s models, which makes me doubt that theyâ€™ll show either the inclination or the ability to address autonomy risks in future models. In addition, the commercial race between AI companies will only continue to heat up, and while the science of steering models can have some commercial benefits, overall the intensity of the race will make it increasingly hard to focus on addressing autonomy risks. I believe the only solution is legislationâ€”laws that directly affect the behavior of AI companies, or otherwise incentivize R&D to solve these issues.Here it is worth keeping in mind the warnings I gave at the beginning of this essay about uncertainty and surgical interventions. We do not know for sure whether autonomy risks will be a serious problemâ€”as I said, I reject claims that the danger is inevitable or even that something will go wrong by default. A credible riskof danger is enough for me and for Anthropic to pay quite significant costs to address it, but once we get into regulation, we are forcing a wide range of actors to bear economic costs, and many of these actors donâ€™t believe that autonomy risk is real or that AI will become powerful enough for it to be a threat. I believe these actors are mistaken, but we should be pragmatic about the amount of opposition we expect to see and the dangers of overreach. There is also a genuine risk that overly prescriptive legislation ends up imposing tests or rules that donâ€™t actually improve safety but that waste a lot of time (essentially amounting to â€œsafety theaterâ€)â€”this too would cause backlash and make safety legislation look silly.Anthropicâ€™s view has been that the right place to start is with transparency legislation, which essentially tries to require that every frontier AI company engage in the transparency practices Iâ€™ve described earlier in this section. Californiaâ€™s SB 53 and New Yorkâ€™s RAISE Act are examples of this kind of legislation, which Anthropic supported and which have successfully passed. In supporting and helping to craft these laws, weâ€™ve put a particular focus on trying to minimize collateral damage, for example by exempting smaller companies unlikely to produce frontier models from the law.Our hope is that transparency legislation will give a better sense over time of how likely or severe autonomy risks are shaping up to be, as well as the nature of these risks and how best to prevent them. As more specific and actionable evidence of risks emerges (if it does), future legislation over the coming years can be surgically focused on the precise and well-substantiated direction of risks, minimizing collateral damage. To be clear, if truly strong evidence of risks emerges, then rules should be proportionately strong.Overall, I am optimistic that a mixture of alignment training, mechanistic interpretability, efforts to find and publicly disclose concerning behaviors, safeguards, and societal-level rules can address AI autonomy risks, although I am most worried about societal-level rules and the behavior of the least responsible players (and itâ€™s the least responsible players who advocate most strongly against regulation). I believe the remedy is what it always is in a democracy: those of us who believe in this cause should make our case that these risks are real and that our fellow citizens need to band together to protect themselves.2. A surprising and terrible empowermentLetâ€™s suppose that the problems of AI autonomy have been solvedâ€”we are no longer worried that the country of AI geniuses will go rogue and overpower humanity. The AI geniuses do what humans want them to do, and because they have enormous commercial value, individuals and organizations throughout the world can â€œrentâ€ one or more AI geniuses to do various tasks for them.Everyone having a superintelligent genius in their pocket is an amazing advance and will lead to an incredible creation of economic value and improvement in the quality of human life. I talk about these benefits in great detail in . But not every effect of making everyone superhumanly capable will be positive. It can potentially amplify the ability of individuals or small groups to cause destruction on a much larger scale than was possible before, by making use of sophisticated and dangerous tools (such as weapons of mass destruction) that were previously only available to a select few with a high level of skill, specialized training, and focus.Building nuclear weapons required, at least for a time, access to both rareâ€”indeed, effectively unavailableâ€”raw materials and protected information; biological and chemical weapons programs also tended to require large-scale activities. The 21st century technologiesâ€”genetics, nanotechnology, and robotics ... can spawn whole new classes of accidents and abuses â€¦ widely within reach of individuals or small groups. They will not require large facilities or rare raw materials. â€¦ we are on the cusp of the further perfection of extreme evil, an evil whose possibility spreads well beyond that which weapons of mass destruction bequeathed to the nation-states, to a surprising and terrible empowerment of extreme individuals.What Joy is pointing to is the idea that causing large-scale destruction requires both and , and as long as ability is restricted to a small set of highly trained people, there is relatively limited risk of single individuals (or small groups) causing such destruction. A disturbed loner can perpetrate a school shooting, but probably canâ€™t build a nuclear weapon or release a plague.In fact, ability and motive may even be  correlated. The kind of person who has the to release a plague is probably highly educated: likely a PhD in molecular biology, and a particularly resourceful one, with a promising career, a stable and disciplined personality, and a lot to lose. This kind of person is unlikely to be interested in killing a huge number of people for no benefit to themselves and at great risk to their own futureâ€”they would need to be motivated by pure malice, intense grievance, or instability.Such people do exist, but they are rare, and tend to become huge stories when they occur, precisely because they are so unusual. They also tend to be difficult to catch because they are intelligent and capable, sometimes leaving mysteries that take years or decades to solve. The most famous example is probably mathematician Theodore Kaczynski (the Unabomber), who evaded FBI capture for nearly 20 years, and was driven by an anti-technological ideology. Another example is biodefense researcher Bruce Ivins, who seems to have orchestrated a series of anthrax attacks in 2001. Itâ€™s also happened with skilled non-state organizations: the cult Aum Shinrikyo managed to obtain sarin nerve gas and kill 14 people (as well as injuring hundreds more) by releasing it in the Tokyo subway in 1995.Thankfully, none of these attacks used contagious biological agents, because the ability to construct or obtain these agents was beyond the capabilities of even these people. Advances in molecular biology have now significantly lowered the barrier to creating biological weapons (especially in terms of availability of materials), but it still takes an enormous amount of expertise in order to do so. I am concerned that a genius in everyoneâ€™s pocket could remove that barrier, essentially making everyone a PhD virologist who can be walked through the process of designing, synthesizing, and releasing a biological weapon step-by-step. Preventing the elicitation of this kind of information in the face of serious adversarial pressureâ€”so-called â€œjailbreaksâ€â€”likely demands layers of defenses beyond those ordinarily baked into training.Crucially, this will break the correlation between ability and motive: the disturbed loner who wants to kill people but lacks the discipline or skill to do so will now be elevated to the capability level of the PhD virologist, who is unlikely to have this motivation. This concern generalizes beyond biology (although I think biology is the scariest area) to any area where great destruction is possible but currently requires a high level of skill and discipline. To put it another way, renting a powerful AI gives intelligence to malicious (but otherwise average) people. I am worried there are potentially a large number of such people out there, and that if they have access to an easy way to kill millions of people, sooner or later one of them will do it. Additionally, those who have expertise may be enabled to commit even larger-scale destruction than they could before.Biology is by far the area Iâ€™m most worried about, because of its very large potential for destruction and the difficulty of defending against it, so Iâ€™ll focus on biology in particular. But much of what I say here applies to other risks, like cyberattacks, chemical weapons, or nuclear technology.I am not going to go into detail about how to make biological weapons, for reasons that should be obvious. But at a high level, I am concerned that LLMs are approaching (or may already have reached) the knowledge needed to create and release them end-to-end, and that their potential for destruction is very high. Some biological agents could cause millions of deaths if a determined effort was made to release them for maximum spread. However, this would still take a very high level of skill, including a number of very specific steps and procedures that are not widely known. My concern is not merely fixed or static knowledge. I am concerned that LLMs will be able to take someone of average knowledge and ability and walk them through a complex process that might otherwise go wrong or require debugging in an interactive way, similar to how tech support might help a non-technical person debug and fix complicated computer-related problems (although this would be a more extended process, probably lasting over weeks or months).More capable LLMs (substantially beyond the power of todayâ€™s) might be capable of enabling even more frightening acts. In 2024, a group of prominent scientists wrote a letter warning about the risks of researching, and potentially creating, a dangerous new type of organism: â€œmirror life.â€ The DNA, RNA, ribosomes, and proteins that make up biological organisms all have the same chirality (also called â€œhandednessâ€) that causes them to be not equivalent to a version of themselves reflected in the mirror (just as your right hand cannot be rotated in such a way as to be identical to your left). But the whole system of proteins binding to each other, the machinery of DNA synthesis and RNA translation and the construction and breakdown of proteins, all depends on this handedness. If scientists made versions of this biological material with the opposite handednessâ€”and there are some potential advantages of these, such as medicines that last longer in the bodyâ€”it could be extremely dangerous. This is because left-handed life, if it were made in the form of complete organisms capable of reproduction (which would be very difficult), would potentially be indigestible to any of the systems that break down biological material on earthâ€”it would have a â€œkeyâ€ that wouldnâ€™t fit into the â€œlockâ€ of any existing enzyme. This would mean that it could proliferate in an uncontrollable way and crowd out all life on the planet, in the worst case even destroying all life on earth.There is substantial scientific uncertainty about both the creation and potential effects of mirror life. The 2024 letter accompanied a report that concluded that â€œmirror bacteria could plausibly be created in the next one to few decades,â€ which is a wide range. But a sufficiently powerful AI model (to be clear, far more capable than any we have today) might be able to discover how to create it much more rapidlyâ€”and actually help someone do so.My view is that even though these are obscure risks, and might seem unlikely, the magnitude of the consequences is so large that they should be taken seriously as a first-class risk of AI systems.Skeptics have raised a number of objections to the seriousness of these biological risks from LLMs, which I disagree with but which are worth addressing. Most fall into the category of not appreciating the exponential trajectory that the technology is on. Back in 2023 when we first started talking about biological risks from LLMs, skeptics said that all the necessary information was available on Google and LLMs didnâ€™t add anything beyond this. It was never true that Google could give you all the necessary information: genomes are freely available, but as I said above, certain key steps, as well as a huge amount of practical know-how cannot be gotten in that way. But also, by the end of 2023 LLMs were clearly providing information beyond what Google could give for some steps of the process.After this, skeptics retreated to the objection that LLMs werenâ€™t  useful, and couldnâ€™t help with bioweapons  as opposed to just providing theoretical information. As of mid-2025, our measurements show that LLMs may already be providing substantial uplift in several relevant areas, perhaps doubling or tripling the likelihood of success. This led to us deciding that Claude Opus 4 (and the subsequent Sonnet 4.5, Opus 4.1, and Opus 4.5 models) needed to be released under our AI Safety Level 3 protections in our Responsible Scaling Policy framework, and to implementing safeguards against this risk (more on this later). We believe that models are likely now approaching the point where, without safeguards, they could be useful in enabling someone with a STEM degree but not specifically a biology degree to go through the whole process of producing a bioweapon.Another objection is that there are other actions unrelated to AI that society can take to block the production of bioweapons. Most prominently, the gene synthesis industry makes biological specimens on demand, and there is no federal requirement that providers screen orders to make sure they do not contain pathogens. An MIT study found that 36 out of 38 providers fulfilled an order containing the sequence of the 1918 flu. I am supportive of mandated gene synthesis screening that would make it harder for individuals to weaponize pathogens, in order to reduce both AI-driven biological risks and also biological risks in general. But this is not something we have today. It would also be only one tool in reducing risk; it is a complement to guardrails on AI systems, not a substitute.The best objection is one that Iâ€™ve rarely seen raised: that there is a gap between the models being useful in principle and the actual propensity of bad actors to use them. Most individual bad actors are disturbed individuals, so almost by definition their behavior is unpredictable and irrationalâ€”and itâ€™s  bad actors, the unskilled ones, who might have stood to benefit the most from AI making it much easier to kill many people. Just because a type of violent attack is possible, doesnâ€™t mean someone will decide to do it. Perhaps biological attacks will be unappealing because they are reasonably likely to infect the perpetrator, they donâ€™t cater to the military-style fantasies that many violent individuals or groups have, and it is hard to selectively target specific people. It could also be that going through a process that takes months, even if an AI walks you through it, involves an amount of patience that most disturbed individuals simply donâ€™t have. We may simply get lucky and motive and ability donâ€™t combine, in practice, in quite the right way.But this seems like very flimsy protection to rely on. The motives of disturbed loners can change for any reason or no reason, and in fact there are already instances of LLMs being used in attacks (just not with biology). The focus on disturbed loners also ignores ideologically motivated terrorists, who are often willing to expend large amounts of time and effort (for example, the 9/11 hijackers). Wanting to kill as many people as possible is a motive that will probably arise sooner or later, and it unfortunately suggests bioweapons as the method. Even if this motive is extremely rare, it only has to materialize once. And as biology advances (increasingly driven by AI itself), it may also become possible to carry out more selective attacks (for example, targeted against people with specific ancestries), which adds yet another, very chilling, possible motive.I do not think biological attacks will necessarily be carried out the instant it becomes widely possible to do soâ€”in fact, I would bet against that. But added up across millions of people and a few years of time, I think there is a serious risk of a major attack, and the consequences would be so severe (with casualties potentially in the millions or more) that I believe we have no choice but to take serious measures to prevent it.That brings us to how to defend against these risks. Here I see three things we can do. First, AI companies can put guardrails on their models to prevent them from helping to produce bioweapons. Anthropic is very actively doing this. Claudeâ€™s Constitution, which mostly focuses on high-level principles and values, has a small number of specific hard-line prohibitions, and one of them relates to helping with the production of biological (or chemical, or nuclear, or radiological) weapons. But all models can be jailbroken, and so as a second line of defense, weâ€™ve implemented (since mid-2025, when our tests showed our models were starting to get close to the threshold where they might begin to pose a risk) a classifier that specifically detects and blocks bioweapon-related outputs. We regularly upgrade and improve these classifiers, and have generally found them highly robust even against sophisticated adversarial attacks. These classifiers increase the costs to serve our models measurably (in some models, they are close to 5% of total inference costs) and thus cut into our margins, but we feel that using them is the right thing to do.To their credit, some other AI companies have implemented classifiers as well. But not every company has, and there is also nothing requiring companies to keep their classifiers. I am concerned that over time there may be a prisonerâ€™s dilemma where companies can defect and lower their costs by removing classifiers. This is once again a classic negative externalities problem that canâ€™t be solved by the voluntary actions of Anthropic or any other single company alone. Voluntary industry standards may help, as may third-party evaluations and verification of the type done by AI securityinstitutes and third-party evaluators.But ultimately defense may require government action, which is the second thing we can do. My views here are the same as they are for addressing autonomy risks: we should start with transparency requirements, which help society measure, monitor, and collectively defend against risks without disrupting economic activity in a heavy-handed way. Then, if and when we reach clearer thresholds of risk, we can craft legislation that more precisely targets these risks and has a lower chance of collateral damage. In the particular case of bioweapons, I actually think that the time for such targeted legislation may be approaching soonâ€”Anthropic and other companies are learning more and more about the nature of biological risks and what is reasonable to require of companies in defending against them. Fully defending against these risks may require working internationally, even with geopolitical adversaries, but there is precedent in treaties prohibiting the development of biological weapons. I am generally a skeptic about most kinds of international cooperation on AI, but this may be one narrow area where there is some chance of achieving global restraint. Even dictatorships do not want massive bioterrorist attacks.Finally, the third countermeasure we can take is to try to develop defenses against biological attacks themselves. This could include monitoring and tracking for early detection, investments in air purification R&D (such as far-UVC disinfection), rapid vaccine development that can respond and adapt to an attack, better personal protective equipment (PPE), and treatments or vaccinations for some of the most likely biological agents. mRNA vaccines, which can be designed to respond to a particular virus or variant, are an early example of what is possible here. Anthropic is excited to work with biotech and pharmaceutical companies on this problem. But unfortunately I think our expectations on the defensive side should be limited. There is an asymmetry between attack and defense in biology, because agents spread rapidly on their own, while defenses require detection, vaccination, and treatment to be organized across large numbers of people very quickly in response. Unless the response is lightning quick (which it rarely is), much of the damage will be done before a response is possible. It is conceivable that future technological improvements could shift this balance in favor of defense (and we should certainly use AI to help develop such technological advances), but until then, preventative safeguards will be our main line of defense.Itâ€™s worth a brief mention of cyberattacks here, since unlike biological attacks, AI-led cyberattacks have actually happened in the wild, including at a large scale and for state-sponsored espionage. We expect these attacks to become more capable as models advance rapidly, until they are the main way in which cyberattacks are conducted. I expect AI-led cyberattacks to become a serious and unprecedented threat to the integrity of computer systems around the world, and Anthropic is working very hard to shut down these attacks and eventually reliably prevent them from happening. The reason I havenâ€™t focused on cyber as much as biology is that (1) cyberattacks are much less likely to kill people, certainly not at the scale of biological attacks, and (2) the offense-defense balance may be more tractable in cyber, where there is at least some hope that defense could keep up with (and even ideally outpace) AI attack if we invest in it properly.Although biology is currently the most serious vector of attack, there are many other vectors and it is possible that a more dangerous one may emerge. The general principle is that without countermeasures, AI is likely to continuously lower the barrier to destructive activity on a larger and larger scale, and humanity needs a serious response to this threat.The previous section discussed the risk of individuals and small organizations co-opting a small subset of the â€œcountry of geniuses in a datacenterâ€ to cause large-scale destruction. But we should also worryâ€”likely substantially more soâ€”about misuse of AI for the purpose of , likely by larger and more established actors.In , I discussed the possibility that authoritarian governments might use powerful AI to surveil or repress their citizens in ways that would be extremely difficult to reform or overthrow. Current autocracies are limited in how repressive they can be by the need to have humans carry out their orders, and humans often have limits in how inhumane they are willing to be. But AI-enabled autocracies would not have such limits.Worse yet, countries could also use their advantage in AI to gain power over . If the â€œcountry of geniusesâ€ as a whole was simply owned and controlled by a single (human) countryâ€™s military apparatus, and other countries did not have equivalent capabilities, it is hard to see how they could defend themselves: they would be outsmarted at every turn, similar to a war between humans and mice. Putting these two concerns together leads to the alarming possibility of a global totalitarian dictatorship. Obviously, it should be one of our highest priorities to prevent this outcome.There are many ways in which AI could enable, entrench, or expand autocracy, but Iâ€™ll list a few that Iâ€™m most worried about. Note that some of these applications have legitimate defensive uses, and I am not necessarily arguing against them in absolute terms; I am nevertheless worried that they structurally tend to favor autocracies:Fully autonomous weapons. A swarm of millions or billions of fully automated armed drones, locally controlled by powerful AI and strategically coordinated across the world by an even more powerful AI, could be an unbeatable army, capable of both defeating any military in the world and suppressing dissent within a country by following around every citizen. Developments in the Russia-Ukraine War should alert us to the fact that drone warfare is already with us (though not fully autonomous yet, and a tiny fraction of what might be possible with powerful AI). R&D from powerful AI could make the drones of one country far superior to those of others, speed up their manufacture, make them more resistant to electronic attacks, improve their maneuvering, and so on. Of course, these weapons also have legitimate uses in the defense of democracy: they have been key to defending Ukraine and would likely be key to defending Taiwan. But they are a dangerous weapon to wield: we should worry about them in the hands of autocracies, but also worry that because they are so powerful, with so little accountability, there is a greatly increased risk of democratic governments turning them against their own people to seize power.Sufficiently powerful AI could likely be used to compromise any computer system in the world, and could also use the access obtained in this way to read  all the worldâ€™s electronic communications (or even all the worldâ€™s in-person communications, if recording devices can be built or commandeered). It might be frighteningly plausible to simply generate a complete list of anyone who disagrees with the government on any number of issues, even if such disagreement isnâ€™t explicit in anything they say or do. A powerful AI looking across billions of conversations from millions of people could gauge public sentiment, detect pockets of disloyalty forming, and stamp them out before they grow. This could lead to the imposition of a true panopticon on a scale that we donâ€™t see today, even with the CCP.Todayâ€™s phenomena of â€œAI psychosisâ€ and â€œAI girlfriendsâ€ suggest that even at their current level of intelligence, AI models can have a powerful psychological influence on people. Much more powerful versions of these models, that were much more embedded in and aware of peopleâ€™s daily lives and could model and influence them over months or years, would likely be capable of essentially brainwashing many (most?) people into any desired ideology or attitude, and could be employed by an unscrupulous leader to ensure loyalty and suppress dissent, even in the face of a level of repression that most populations would rebel against. Today people worry a lot about, for example, the potential influence of TikTok as CCP propaganda directed at children. I worry about that too, but a personalized AI agent that gets to know you over years and uses its knowledge of you to shape all of your opinions would be dramatically more powerful than this.Strategic decision-making. A country of geniuses in a datacenter could be used to advise a country, group, or individual on geopolitical strategy, what we might call a â€œvirtual Bismarck.â€ It could optimize the three strategies above for seizing power, plus probably develop many others that I havenâ€™t thought of (but that a country of geniuses could). Diplomacy, military strategy, R&D, economic strategy, and many other areas are all likely to be substantially increased in effectiveness by powerful AI. Many of these skills would be legitimately helpful for democraciesâ€”we want democracies to have access to the best strategies for defending themselves against autocraciesâ€”but the potential for misuse in hands still remains.Having described I am worried about, letâ€™s move on to . I am worried about entities who have the most access to AI, who are starting from a position of the most political power, or who have an existing history of repression. In order of severity, I am worried about:China is second only to the United States in AI capabilities, and is the country with the greatest likelihood of surpassing the United States in those capabilities. Their government is currently autocratic and operates a high-tech surveillance state. It has deployed AI-based surveillance already (including in the repression of Uyghurs), and is believed to employ algorithmic propaganda via TikTok (in addition to its many other international propaganda efforts). They have hands down the clearest path to the AI-enabled totalitarian nightmare I laid out above. It may even be the default outcome within China, as well as within other autocratic states to whom the CCP exports surveillance technology. I have written often about the threat of the CCP taking the lead in AI and the existential imperative to prevent them from doing so. This is why. To be clear, I am not singling out China out of animus to them in particularâ€”they are simply the country that most combines AI prowess, an autocratic government, and a high-tech surveillance state. If anything, it is the Chinese people themselves who are most likely to suffer from the CCPâ€™s AI-enabled repression, and they have no voice in the actions of their government. I greatly admire and respect the Chinese people and support the many brave dissidents within China and their struggle for freedom.Democracies competitive in AI. As I wrote above, democracies have a legitimate interest in some AI-powered military and geopolitical tools, because democratic governments offer the best chance to counter the use of these tools by autocracies. Broadly, I am supportive of arming democracies with the tools needed to defeat autocracies in the age of AIâ€”I simply donâ€™t think there is any other way. But we cannot ignore the potential for abuse of these technologies by democratic governments themselves. Democracies normally have safeguards that prevent their military and intelligence apparatus from being turned inwards against their own population, but because AI tools require so few people to operate, there is potential for them to circumvent these safeguards and the norms that support them. It is also worth noting that some of these safeguards are already gradually eroding in some democracies. Thus, we should arm democracies with AI, but we should do so carefully and within limits: they are the immune system we need to fight autocracies, but like the immune system, there is some risk of them turning on us and becoming a threat themselves.Non-democratic countries with large datacenters. Beyond China, most countries with less democratic governance are not leading AI players in the sense that they donâ€™t have companies which produce frontier AI models. Thus they pose a fundamentally different and lesser risk than the CCP, which remains the primary concern (most are also less repressive, and the ones that are more repressive, like North Korea, have no significant AI industry at all). But some of these countries do have large (often as part of buildouts by companies operating in democracies), which can be used to run frontier AI at large scale (though this does not confer the ability to push the frontier). There is some amount of danger associated with thisâ€”these governments could in principle expropriate the datacenters and use the country of AIs within it for their own ends. I am less worried about this compared to countries like China that directly develop AI, but itâ€™s a risk to keep in mind.It is somewhat awkward to say this as the CEO of an AI company, but I think the next tier of risk is actually AI companies themselves. AI companies control large datacenters, train frontier models, have the greatest expertise on how to use those models, and in some cases have daily contact with and the possibility of influence over tens or hundreds of millions of users. The main thing they lack is the legitimacy and infrastructure of a state, so much of what would be needed to build the tools of an AI autocracy would be illegal for an AI company to do, or at least exceedingly suspicious. But some of it is not impossible: they could, for example, use their AI products to brainwash their massive consumer user base, and the public should be alert to the risk this represents. I think the governance of AI companies deserves a lot of scrutiny.There are a number of possible arguments against the severity of these threats, and I wish I believed them, because AI-enabled authoritarianism terrifies me. Itâ€™s worth going through some of these arguments and responding to them.First, some people might put their faith in the nuclear deterrent, particularly to counter the use of AI autonomous weapons for military conquest. If someone threatens to use these weapons against you, you can always threaten a nuclear response back. My worry is that Iâ€™m not totally sure we can be confident in the nuclear deterrent against a country of geniuses in a datacenter: it is possible that powerful AI could devise ways to detect and strike nuclear submarines, conduct influence operations against the operators of nuclear weapons infrastructure, or use AIâ€™s cyber capabilities to launch a cyberattack against satellites used to detect nuclear launches. Alternatively, itâ€™s possible that taking over countries is feasible with only AI surveillance and AI propaganda, and never actually presents a clear moment where itâ€™s obvious what is going on and where a nuclear response would be appropriate. these things arenâ€™t feasible and the nuclear deterrent will still be effective, but it seems too high stakes to take a risk.A second possible objection is that there might be countermeasures we can take against these tools of autocracy. We can counter drones with our own drones, cyberdefense will improve along with cyberattack, there may be ways to immunize people against propaganda, etc. My response is that these defenses will only be possible with comparably powerful AI. If there isnâ€™t some counterforce with a comparably smart and numerous country of geniuses in a datacenter, it wonâ€™t be possible to match the quality or quantity of drones, for cyberdefense to outsmart cyberoffense, etc. So the question of countermeasures reduces to the question of a balance of power in powerful AI. Here, I am concerned about the recursive or self-reinforcing property of powerful AI (which I discussed at the beginning of this essay): that each generation of AI can be used to design and train the next generation of AI. This leads to a risk of a runaway advantage, where the current leader in powerful AI may be able to increase their lead and may be difficult to catch up with. We need to make sure it is not an authoritarian country that gets to this loop first.Furthermore, even if a balance of power can be achieved, there is still risk that the world could be split up into autocratic spheres, as in . Even if several competing powers each have their powerful AI models, and none can overpower the others, each power could still internally repress their own population, and would be very difficult to overthrow (since the populations donâ€™t have powerful AI to defend themselves). It is thus important to prevent AI-enabled autocracy even if it doesnâ€™t lead to a single country taking over the world.How do we defend against this wide range of autocratic tools and potential threat actors? As in the previous sections, there are several things I think we can do. First, we should absolutely not be selling chips, chip-making tools, or datacenters to the CCP. Chips and chip-making tools are the single greatest bottleneck to powerful AI, and blocking them is a simple but extremely effective measure, perhaps the most important single action we can take. It makes no sense to sell the CCP the tools with which to build an AI totalitarian state and possibly conquer us militarily. A number of complicated arguments are made to justify such sales, such as the idea that â€œspreading our tech stack around the worldâ€ allows â€œAmerica to winâ€ in some general, unspecified economic battle. In my view, this is like selling nuclear weapons to North Korea and then bragging that the missile casings are made by Boeing and so the US is â€œwinning.â€ China is several years behind the US in their ability to produce frontier chips in quantity, and the critical period for building the country of geniuses in a datacenter is very likely to be within those next several years. There is no reason to give a giant boost to their AI industry during this critical period.Second, it makes sense to use AI to empower democracies to resist autocracies. This is the reason Anthropic considers it important to provide AI to the intelligence and defense communities in the US and its democratic allies. Defending democracies that are under attack, such as Ukraine and (via cyber attacks) Taiwan, seems especially high priority, as does empowering democracies to use their intelligence services to disrupt and degrade autocracies from the inside. At some level the only way to respond to autocratic threats is to match and outclass them militarily. A coalition of the US and its democratic allies, if it achieved predominance in powerful AI, would be in a position to not only defend itself against autocracies, but contain them and limit their AI totalitarian abuses.Third, we need to draw a hard line against AI abuses within democracies. There need to be limits to what we allow our governments to do with AI, so that they donâ€™t seize power or repress their own people. The formulation I have come up with is that we should use AI for national defense in all ways except those which would make us more like our autocratic adversaries. Where should the line be drawn? In the list at the beginning of this section, two itemsâ€”using AI for domestic mass surveillance and mass propagandaâ€”seem to me like bright red lines and entirely illegitimate. Some might argue that thereâ€™s no need to do anything (at least in the US), since domestic mass surveillance is already illegal under the Fourth Amendment. But the rapid progress of AI may create situations that our existing legal frameworks are not well designed to deal with. For example, it would likely not be unconstitutional for the US government to conduct massively scaled recordings of all conversations (e.g., things people say to each other on a street corner), and previously it would have been difficult to sort through this volume of information, but with AI it could all be transcribed, interpreted, and triangulated to create a picture of the attitude and loyalties of many or most citizens. I would support civil liberties-focused legislation (or maybe even a constitutional amendment) that imposes stronger guardrails against AI-powered abuses.The other two itemsâ€”fully autonomous weapons and AI for strategic decision-makingâ€”are harder lines to draw since they have legitimate uses in defending democracy, while also being prone to abuse. Here I think what is warranted is extreme care and scrutiny combined with guardrails to prevent abuses. My main fear is having too small a number of â€œfingers on the button,â€ such that one or a handful of people could essentially operate a drone army without needing any other humans to cooperate to carry out their orders. As AI systems get more powerful, we may need to have more direct and immediate oversight mechanisms to ensure they are not misused, perhaps involving branches of government other than the executive. I think we should approach fully autonomous weapons in particular with great caution, and not rush into their use without proper safeguards.Fourth, after drawing a hard line against AI abuses in democracies, we should use that precedent to create an international taboo against the worst abuses of powerful AI. I recognize that the current political winds have turned against international cooperation and international norms, but this is a case where we sorely need them. The world needs to understand the dark potential of powerful AI in the hands of autocrats, and to recognize that certain uses of AI amount to an attempt to permanently steal their freedom and impose a totalitarian state from which they canâ€™t escape. I would even argue that in some cases, large-scale surveillance with powerful AI, mass propaganda with powerful AI, and certain types of uses of fully autonomous weapons should be considered crimes against humanity. More generally, a robust norm against AI-enabled totalitarianism and all its tools and instruments is sorely needed.It is possible to have an even stronger version of this position, which is that because the possibilities of AI-enabled totalitarianism are so dark, autocracy is simply not a form of government that people can accept in the post-powerful AI age. Just as feudalism became unworkable with the industrial revolution, the AI age could lead inevitably and logically to the conclusion that democracy (and, hopefully, democracy improved and reinvigorated by AI, as I discuss in ) is the only viable form of government if humanity is to have a good future.Fifth and finally, AI companies should be carefully watched, as should their connection to the government, which is necessary, but must have limits and boundaries. The sheer amount of capability embodied in powerful AI is such that ordinary corporate governanceâ€”which is designed to protect shareholders and prevent ordinary abuses such as fraudâ€”is unlikely to be up to the task of governing AI companies. There may also be value in companies publicly committing to (perhaps even as part of corporate governance) not take certain actions, such as privately building or stockpiling military hardware, using large amounts of computing resources by single individuals in unaccountable ways, or using their AI products as propaganda to manipulate public opinion in their favor.The danger here comes from many directions, and some directions are in tension with others. The only constant is that we must seek accountability, norms, and guardrails for everyone, even as we empower â€œgoodâ€ actors to keep â€œbadâ€ actors in check.The previous three sections were essentially about security risks posed by powerful AI: risks from the AI itself, risks from misuse by individuals and small organizations and risks of misuse by states and large organizations. If we put aside security risks or assume they have been solved, the next question is economic. What will be the effect of this infusion of incredible â€œhumanâ€ capital on the economy? Clearly, the most obvious effect will be to greatly increase economic growth. The pace of advances in scientific research, biomedical innovation, manufacturing, supply chains, the efficiency of the financial system, and much more are almost guaranteed to lead to a much faster rate of economic growth. In , I suggest that a 10â€“20% sustained annual GDP growth rate may be possible.But it should be clear that this is a double-edged sword: what are the economic prospects for most existing humans in such a world? New technologies often bring labor market shocks, and in the past humans have always recovered from them, but I am concerned that this is because these previous shocks affected only a small fraction of the full possible range of human abilities, leaving room for humans to expand to new tasks. AI will have effects that are much broader and occur much faster, and therefore I worry it will be much more challenging to make things work out well.There are two specific problems I am worried about: labor market displacement, and concentration of economic power. Letâ€™s start with the first one. This is a topic that I warned about very publicly in 2025, where I predicted that AI could displace half of all entry-level white collar jobs in the next 1â€“5 years, even as it accelerates economic growth and scientific progress. This warning started a public debate about the topic. Many CEOs, technologists, and economists agreed with me, but others assumed I was falling prey to a â€œlump of laborâ€ fallacy and didnâ€™t know how labor markets worked, and some didnâ€™t see the 1â€“5-year time range and thought I was claiming AI is displacing jobs right now (which I agree it is likely not). So it is worth going through in detail why I am worried about labor displacement, to clear up these misunderstandings.As a baseline, itâ€™s useful to understand how labor markets  respond to advances in technology. When a new technology comes along, it starts by making pieces of a given human job more efficient. For example, early in the Industrial Revolution, machines, such as upgraded plows, enabled human farmers to be more efficient at some aspects of the job. This improved the productivity of farmers, which increased their wages.In the next step, some parts of the job of farming could be done  by machines, for example with the invention of the threshing machine or seed drill. In this phase, humans did a lower and lower fraction of the job, but the work they  complete became more and more leveraged because it is complementary to the work of machines, and their productivity continued to rise. As described by Jevonsâ€™ paradox, the wages of farmers and perhaps even the number of farmers continued to increase. Even when 90% of the job is being done by machines, humans can simply do 10x more of the 10% they still do, producing 10x as much output for the same amount of labor.Eventually, machines do everything or almost everything, as with modern combine harvesters, tractors, and other equipment. At this point farming as a form of human employment really does go into steep decline, and this potentially causes serious disruption in the short term, but because farming is just one of many useful activities that humans are able to do, people eventually switch to other jobs, such as operating factory machines. This is true even though farming accounted for a huge proportion of employment . 250 years ago, 90% of Americans lived on farms; in Europe, 50â€“60% of employment was agricultural. Now those percentages are in the low single digits in those places, because workers switched to industrial jobs (and later, knowledge work jobs). The economy can do what previously required most of the labor force with only 1â€“2% of it, freeing up the rest of the labor force to build an ever more advanced industrial society. Thereâ€™s no fixed â€œlump of labor,â€ just an ever-expanding ability to do more and more with less and less. Peopleâ€™s wages rise in line with the GDP exponential and the economy maintains full employment once disruptions in the short term have passed.Itâ€™s possible things will go roughly the same way with AI, but I would bet pretty strongly against it. Here are some reasons I think AI is likely to be different:The pace of progress in AI is much faster than for previous technological revolutions. For example, in the last 2 years, AI models went from barely being able to complete a single line of code, to writing all or almost all of the code for some peopleâ€”including engineers at Anthropic. Soon, they may do the entire task of a software engineer end to end. It is hard for people to adapt to this pace of change, both to the changes in how a given job works and in the need to switch to new jobs. Even legendary programmers are increasingly describing themselves as â€œbehind.â€ The pace may if anything continue to speed up, as AI coding models increasingly accelerate the task of AI development. To be clear, speed in itself does not mean labor markets and employment wonâ€™t eventually recover, it just implies the short-term transition will be unusually painful compared to past technologies, since humans and labor markets are slow to react and to equilibrate.As suggested by the phrase â€œcountry of geniuses in a datacenter,â€ AI will be capable of a very wide range of human cognitive abilitiesâ€”perhaps all of them. This is very different from previous technologies like mechanized farming, transportation, or even computers. This will make it harder for people to switch easily from jobs that are displaced to similar jobs that they would be a good fit for. For example, the general intellectual abilities required for entry-level jobs in, say, finance, consulting, and law are fairly similar, even if the specific knowledge is quite different. A technology that disrupted only one of these three would allow employees to switch to the two other close substitutes (or for undergraduates to switch majors). But disrupting all three at once (along with many other similar jobs) may be harder for people to adapt to. Furthermore, itâ€™s not that most existing jobs will be disrupted. That part has happened beforeâ€”recall that farming was a huge percentage of employment. But farmers could switch to the relatively similar work of operating factory machines, even though that work hadnâ€™t been common before. By contrast, AI is increasingly matching the general cognitive profile of humans, which means it will also be good at the new jobs that would ordinarily be created in response to the old ones being automated. Another way to say it is that AI isnâ€™t a substitute for specific human jobs but rather a general labor substitute for humans.Slicing by cognitive ability. Across a wide range of tasks, AI appears to be advancing from the bottom of the ability ladder to the top. For example, in coding our models have proceeded from the level of â€œa mediocre coderâ€ to â€œa strong coderâ€ to â€œa very strong coder.â€ We are now starting to see the same progression in white-collar work in general. We are thus at risk of a situation where, instead of affecting people with specific skills or in specific professions (who can adapt by retraining), AI is affecting people with certain intrinsic cognitive properties, namely lower intellectual ability (which is harder to change). It is not clear where these people will go or what they will do, and I am concerned that they could form an unemployed or very-low-wage â€œunderclass.â€ To be clear, things somewhat like this have happened beforeâ€”for example, computers and the internet are believed by some economists to represent â€œskill-biased technological change.â€ But this skill biasing was both not as extreme as what I expect to see with AI, and is believed to have contributed to an increase in wage inequality, so it is not exactly a reassuring precedent.Ability to fill in the gaps. The way human jobs often adjust in the face of new technology is that there are many aspects to the job, and the new technology, even if it appears to directly replace humans, often has gaps in it. If someone invents a machine to make widgets, humans may still have to load raw material into the machine. Even if that takes only 1% as much effort as making the widgets manually, human workers can simply make 100x more widgets. But AI, in addition to being a rapidly advancing technology, is also a rapidly technology. During every model release, AI companies carefully measure what the model is good at and what it isnâ€™t, and customers also provide such information after the launch. Weaknesses can be addressed by collecting tasks that embody the current gap, and training on them for the next model. Early in generative AI, users noticed that AI systems had certain weaknesses (such as AI image models generating hands with the wrong number of fingers) and many assumed these weaknesses were inherent to the technology. If they were, it would limit job disruption. But pretty much every such weakness gets addressed quicklyâ€” often, within just a few months.Itâ€™s worth addressing common points of skepticism. First, there is the argument that economic diffusion will be slow, such that even if the underlying technology is of doing most human labor, the actual application of it across the economy may be much slower (for example in industries that are far from the AI industry and slow to adopt). Slow diffusion of technology is definitely realâ€”I talk to people from a wide variety of enterprises, and there are places where the adoption of AI will take years. Thatâ€™s why my prediction for 50% of entry level white collar jobs being disrupted is 1â€“5 years, even though I suspect weâ€™ll have powerful AI (which would be, technologically speaking, enough to do jobs, not just entry level) in much less than 5 years. But diffusion effects merely buy us time. And I am not confident they will be as slow as people predict. Enterprise AI adoption is growing at rates much faster than any previous technology, largely on the pure strength of the technology itself. Also, even if traditional enterprises are slow to adopt new technology, startups will spring up to serve as â€œglueâ€ and make the adoption easier. If that doesnâ€™t work, the startups may simply disrupt the incumbents directly.That could lead to a world where it isnâ€™t so much that specific jobs are disrupted as it is that large enterprises are disrupted in general and replaced with much less labor-intensive startups. This could also lead to a world of â€œgeographic inequality,â€ where an increasing fraction of the worldâ€™s wealth is concentrated in Silicon Valley, which becomes its own economy running at a different speed than the rest of the world and leaving it behind. All of these outcomes would be great for economic growthâ€”but not so great for the labor market or those who are left behind.Second, some people say that human jobs will move to the physical world, which avoids the whole category of â€œcognitive laborâ€ where AI is progressing so rapidly. I am not sure how safe this is, either. A lot of physical labor is already being done by machines (e.g., manufacturing) or will soon be done by machines (e.g., driving). Also, sufficiently powerful AI will be able to accelerate the development of robots, and then control those robots in the physical world. It may buy some time (which is a good thing), but Iâ€™m worried it wonâ€™t buy much. And even if the disruption was limited only to cognitive tasks, it would still be an unprecedentedly large and rapid disruption.Third, perhaps some tasks inherently require or greatly benefit from a human touch. Iâ€™m a little more uncertain about this one, but Iâ€™m still skeptical that it will be enough to offset the bulk of the impacts I described above. AI is already widely used for customer service. Many people report that it is easier to talk to AI about their personal problems than to talk to a therapistâ€”that the AI is more patient. When my sister was struggling with medical problems during a pregnancy, she felt she wasnâ€™t getting the answers or support she needed from her care providers, and she found Claude to have a better bedside manner (as well as succeeding better at diagnosing the problem). Iâ€™m sure there are some tasks for which a human touch really is important, but Iâ€™m not sure how manyâ€”and here weâ€™re talking about finding work for nearly everyone in the labor market.Fourth, some may argue that comparative advantage will still protect humans. Under the law of comparative advantage, even if AI is better than humans at everything, any differences between the human and AI profile of skills creates a basis of trade and specialization between humans and AI. The problem is that if AIs are literally thousands of times more productive than humans, this logic starts to break down. Even tiny transaction costs could make it not worth it for AI to trade with humans. And human wages may be very low, even if they technically have something to offer.Itâ€™s possible all of these factors can be addressedâ€”that the labor market is resilient enough to adapt to even such an enormous disruption. But even if it can eventually adapt, the factors above suggest that the short-term shock will be unprecedented in size.What can we do about this problem? I have several suggestions, some of which Anthropic is already doing. The first thing is simply to get accurate data about what is happening with job displacement in real time. When an economic change happens very quickly, itâ€™s hard to get reliable data about what is happening, and without reliable data it is hard to design effective policies. For example, government data is currently lacking granular, high-frequency data on AI adoption across firms and industries. For the last year Anthropic has been operating and publicly releasing an Economic Index that shows use of our models almost in real time, broken down by industry, task, location, and even things like whether a task was being automated or conducted collaboratively. We also have an Economic Advisory Council to help us interpret this data and see what is coming.Second, AI companies have a choice in how they work with enterprises. The very inefficiency of traditional enterprises means that their rollout of AI can be very path dependent, and there is some room to choose a better path. Enterprises often have a choice between â€œcost savingsâ€ (doing the same thing with fewer people) and â€œinnovationâ€ (doing more with the same number of people). The market will inevitably produce both eventually, and any competitive AI company will have to serve some of both, but there may be some room to steer companies towards innovation when possible, and it may buy us some time. Anthropic is actively thinking about this.Third, companies should think about how to take care of their employees. In the short term, being creative about ways to reassign employees within companies may be a promising way to stave off the need for layoffs. In the long term, in a world with enormous total wealth, in which many companies increase greatly in value due to increased productivity and capital concentration, it may be feasible to pay human employees even long after they are no longer providing economic value in the traditional sense. Anthropic is currently considering a range of possible pathways for our own employees that we will share in the near future.Fourth, wealthy individuals have an obligation to help solve this problem. It is sad to me that many wealthy individuals (especially in the tech industry) have recently adopted a cynical and nihilistic attitude that philanthropy is inevitably fraudulent or useless. Both private philanthropy like the Gates Foundation and public programs like PEPFAR have saved tens of millions of lives in the developing world, and helped to create economic opportunity in the developed world. All of Anthropicâ€™s co-founders have pledged to donate 80% of our wealth, and Anthropicâ€™s staff have individually pledged to donate company shares worth billions at current pricesâ€”donations that the company has committed to matching.Fifth, while all the above private actions can be helpful, ultimately a macroeconomic problem this large will require government intervention. The natural policy response to an enormous economic pie coupled with high inequality (due to a lack of jobs, or poorly paid jobs, for many) is progressive taxation. The tax could be general or could be targeted against AI companies in particular. Obviously tax design is complicated, and there are many ways for it to go wrong. I donâ€™t support poorly designed tax policies. I think the extreme levels of inequality predicted in this essay justify a more robust tax policy on basic moral grounds, but I can also make a pragmatic argument to the worldâ€™s billionaires that itâ€™s in their interest to support a good version of it: if they donâ€™t support a good version, theyâ€™ll inevitably get a bad version designed by a mob.Ultimately, I think of all of the above interventions as ways to buy time. In the end AI will be able to do everything, and we need to grapple with that. Itâ€™s my hope that by that time, we can use AI itself to help us restructure markets in ways that work for everyone, and that the interventions above can get us through the transitional period.Economic concentration of powerSeparate from the problem of job displacement or economic inequality  is the problem of economic concentration of power. Section 1 discussed the risk that humanity gets disempowered by AI, and Section 3 discussed the risk that citizens get disempowered by their governments by force or coercion. But another kind of disempowerment can occur if there is such a huge concentration of wealth that a small group of people effectively controls government policy with their influence, and ordinary citizens have no influence because they lack economic leverage. Democracy is ultimately backstopped by the idea that the population as a whole is necessary for the operation of the economy. If that economic leverage goes away, then the implicit social contract of democracy may stop working. Others have written about this, so I neednâ€™t go into great detail about it here, but I agree with the concern, and I worry it is already starting to happen.To be clear, I am not opposed to people making a lot of money. Thereâ€™s a strong argument that it incentivizes economic growth under normal conditions. I am sympathetic to concerns about impeding innovation by killing the golden goose that generates it. But in a scenario where GDP growth is 10â€“20% a year and AI is rapidly taking over the economy, yet single individuals hold appreciable fractions of the GDP, innovation is the thing to worry about. The thing to worry about is a level of wealth concentration that will break society.The most famous example of extreme concentration of wealth in US history is the Gilded Age, and the wealthiest industrialist of the Gilded Age was John D. Rockefeller. Rockefellerâ€™s wealth amounted to ~2% of the US GDP at the time. A similar fraction today would lead to a fortune of $600B, and the richest person in the world today (Elon Musk) already exceeds that, at roughly $700B. So we are already at historically unprecedented levels of wealth concentration, even most of the economic impact of AI. I donâ€™t think it is too much of a stretch (if we get a â€œcountry of geniusesâ€) to imagine AI companies, semiconductor companies, and perhaps downstream application companies generating ~$3T in revenue per year, being valued at ~$30T, and leading to personal fortunes well into the trillions. In that world, the debates we have about tax policy today simply wonâ€™t apply as we will be in a fundamentally different situation.Related to this, the coupling of this economic concentration of wealth with the political system already concerns me. AI datacenters already represent a substantial fraction of US economic growth, and are thus strongly tying together the financial interests of large tech companies (which are increasingly focused on either AI or AI infrastructure) and the political interests of the government in a way that can produce perverse incentives. We already see this through the reluctance of tech companies to criticize the US government, and the governmentâ€™s support for extreme anti-regulatory policies on AI.What can be done about this? First, and most obviously, companies should simply choose not to be part of it. Anthropic has always strived to be a policy actor and not a political one, and to maintain our authentic views whatever the administration. Weâ€™ve spoken up in favor of sensible AI regulation and export controls that are in the public interest, even when these are at odds with government policy. Many people have told me that we should stop doing this, that it could lead to unfavorable treatment, but in the year weâ€™ve been doing it, Anthropicâ€™s valuation has increased by over 6x, an almost unprecedented jump at our commercial scale.Second, the AI industry needs a healthier relationship with governmentâ€”one based on substantive policy engagement rather than political alignment. Our choice to engage on policy substance rather than politics is sometimes read as a tactical error or failure to â€œread the roomâ€ rather than a principled decision, and that framing concerns me. In a healthy democracy, companies should be able to advocate for good policy for its own sake. Related to this, a public backlash against AI is brewing: this could be a corrective, but its currently unfocused. Much of it targets issues that arenâ€™t actually problems (like datacenterwater usage) and proposes solutions (like datacenter bans or poorly designed wealth taxes) that wouldnâ€™t address the real concerns. The underlying issue that deserves attention is ensuring that AI development remains accountable to the public interest, not captured by any particular political or commercial alliance, and it seems important to focus the public discussion there.Third, the macroeconomic interventions I described earlier in this section, as well as a resurgence of private philanthropy, can help to balance the economic scales, addressing both the job displacement and concentration of economic power problems at once. We should look to the history of our country here: even in the Gilded Age, industrialists such as Rockefeller and Carnegie felt a strong obligation to society at large, a feeling that society had contributed enormously to their success and they needed to give back. That spirit seems to be increasingly missing today, and I think it is a large part of the way out of this economic dilemma. Those who are at the forefront of AIâ€™s economic boom should be willing to give away both their wealth and their power.5. Black seas of infinityThis last section is a catchall for unknown unknowns, particularly things that could go wrong as an indirect result of positive advances in AI and the resulting acceleration of science and technology in general. Suppose we address all the risks described so far, and begin to reap the benefits of AI. We will likely get a â€œcentury of scientific and economic progress compressed into a decade,â€ and this will be hugely positive for the world, but we will then have to contend with the problems that arise from this rapid rate of progress, and those problems may come at us fast. We may also encounter other risks that occur indirectly as a consequence of AI progress and are hard to anticipate in advance.By the nature of unknown unknowns it is impossible to make an exhaustive list, but Iâ€™ll list three possible concerns as illustrative examples for what we should be watching for:Rapid advances in biology. If we do get a century of medical progress in a few years, it is possible that we will greatly increase the human lifespan, and there is a chance we also gain radical capabilities like the ability to increase human intelligence or radically modify human biology. Those would be big changes in what is possible, happening very quickly. They could be positive if responsibly done (which is my hope, as described in ), but there is always a risk they go very wrongâ€”for example, if efforts to make humans smarter also make them more unstable or power-seeking. There is also the issue of â€œuploadsâ€ or â€œwhole brain emulation,â€ digital human minds instantiated in software, which might someday help humanity transcend its physical limitations, but which also carry risks I find disquieting.AI changes human life in an unhealthy way. A world with billions of intelligences that are much smarter than humans at everything is going to be a very weird world to live in. Even if AI doesnâ€™t actively aim to attack humans (Section 1), and isnâ€™t explicitly used for oppression or control by states (Section 3), there is a lot that could go wrong short of this, via normal business incentives and nominally consensual transactions. We see early hints of this in the concerns about AI psychosis, AI driving people to suicide, and concerns about romantic relationships with AIs. As an example, could powerful AIs invent some new religion and convert millions of people to it? Could most people end up â€œaddictedâ€ in some way to AI interactions? Could people end up being â€œpuppetedâ€ by AI systems, where an AI essentially watches their every move and tells them exactly what to do and say at all times, leading to a â€œgoodâ€ life but one that lacks freedom or any pride of accomplishment? It would not be hard to generate dozens of these scenarios if I sat down with the creator of  and tried to brainstorm them. I think this points to the importance of things like improving Claudeâ€™s Constitution, over and above what is necessary for preventing the issues in Section 1. Making sure that AI models have their usersâ€™ long-term interests at heart, in a way thoughtful people would endorse rather than in some subtly distorted way, seems critical.This is related to the previous point, but itâ€™s not so much about specific human interactions with AI systems as it is about how human life changes in general in a world with powerful AI. Will humans be able to find purpose and meaning in such a world? I think this is a matter of attitude: as I said in , I think human purpose does not depend on being the best in the world at something, and humans can find purpose even over very long periods of time through stories and projects that they love. We simply need to break the link between the generation of economic value and self-worth and meaning. But that is a transition society has to make, and there is always the risk we donâ€™t handle it well.My hope with all of these potential problems is that in a world with powerful AI that we trust not to kill us, that is not the tool of an oppressive government, and that is genuinely working on our behalf, we can use AI itself to anticipate and prevent these problems. But that is not guaranteedâ€”like all of the other risks, it is something we have to handle with care.Reading this essay may give the impression that we are in a daunting situation. I certainly found it daunting to write, in contrast with Machines of Loving Grace, which felt like giving form and structure to surpassingly beautiful music that had been echoing in my head for years. And there is much about the situation that genuinely is hard. AI brings threats to humanity from multiple directions, and there is genuine tension between the different dangers, where mitigating some of them risks making others worse if we do not thread the needle extremely carefully.Taking time to carefully build AI systems so they do not autonomously threaten humanity is in genuine tension with the need for democratic nations to stay ahead of authoritarian nations and not be subjugated by them. But in turn, the same AI-enabled tools that are necessary to fight autocracies can, if taken too far, be turned inward to create tyranny in our own countries. AI-driven terrorism could kill millions through the misuse of biology, but an overreaction to this risk could lead us down the road to an autocratic surveillance state. The labor and economic concentration effects of AI, in addition to being grave problems in their own right, may force us to face the other problems in an environment of public anger and perhaps even civil unrest, rather than being able to call on the better angels of our nature. Above all, the sheer of risks, including unknown ones, and the need to deal with all of them at once, creates an intimidating gauntlet that humanity must run.Furthermore, the last few years should make clear that the idea of stopping or even substantially slowing the technology is fundamentally untenable. The formula for building powerful AI systems is incredibly simple, so much so that it can almost be said to emerge spontaneously from the right combination of data and raw computation. Its creation was probably inevitable the instant humanity invented the transistor, or arguably even earlier when we first learned to control fire. If one company does not build it, others will do so nearly as fast. If all companies in democratic countries stopped or slowed development, by mutual agreement or regulatory decree, then authoritarian countries would simply keep going. Given the incredible economic and military value of the technology, together with the lack of any meaningful enforcement mechanism, I donâ€™t see how we could possibly convince them to stop.I do see a path to a  moderation in AI development that is compatible with a realist view of geopolitics. That path involves slowing down the march of autocracies towards powerful AI for a few years by denying them the resources they need to build it, namely chips and semiconductor manufacturing equipment. This in turn gives democratic countries a buffer that they can â€œspendâ€ on building powerful AI more carefully, with more attention to its risks, while still proceeding fast enough to comfortably beat the autocracies. The race between AI companies within democracies can then be handled under the umbrella of a common legal framework, via a mixture of industry standards and regulation.Anthropic has advocated very hard for this path, by pushing for chip export controls and judicious regulation of AI, but even these seemingly common-sense proposals have largely been rejected by policymakers in the United States (which is the country where itâ€™s most important to have them). There is so much money to be made with AIâ€”literally trillions of dollars per yearâ€”that even the simplest measures are finding it difficult to overcome the political economy inherent in AI. This is the trap: AI is so powerful, such a glittering prize, that it is very difficult for human civilization to impose any restraints on it at all.I can imagine, as Sagan did in , that this same story plays out on thousands of worlds. A species gains sentience, learns to use tools, begins the exponential ascent of technology, faces the crises of industrialization and nuclear weapons, and if it survives those, confronts the hardest and final challenge when it learns how to shape sand into machines that think. Whether we survive that test and go on to build the beautiful society described in , or succumb to slavery and destruction, will depend on our character and our determination as a species, our spirit and our soul.Despite the many obstacles, I believe humanity has the strength inside itself to pass this test. I am encouraged and inspired by the thousands of researchers who have devoted their careers to helping us understand and steer AI models, and to shaping the character and constitution of these models. I think there is now a good chance that those efforts bear fruit in time to matter. I am encouraged that at least some companies have stated theyll pay meaningful commercial costs to block their models from contributing to the threat of bioterrorism. I am encouraged that a few brave people have resisted the prevailing political winds and passedlegislation that puts the first early seeds of sensible guardrails on AI systems. I am encouraged that the public understands that AI carries risks and wants those risks addressed. I am encouraged by the indomitable spirit of freedom around the world and the determination to resist tyranny wherever it occurs.But we will need to step up our efforts if we want to succeed. The first step is for those closest to the technology to simply tell the truth about the situation humanity is in, which I have always tried to do; Iâ€™m doing so more explicitly and with greater urgency with this essay. The next step will be convincing the worldâ€™s thinkers, policymakers, companies, and citizens of the imminence and overriding importance of this issueâ€”that it is worth expending thought and political capital on this in comparison to the thousands of other issues that dominate the news every day. Then there will be a time for courage, for enough people to buck the prevailing trends and stand on principle, even in the face of threats to their economic interests and personal safety. The years in front of us will be impossibly hard, asking more of us than we think we can give. But in my time as a researcher, leader, and citizen, I have seen enough courage and nobility to believe that we can winâ€”that when put in the darkest circumstances, humanity has a way of gathering, seemingly at the last minute, the strength and wisdom needed to prevail. We have no time to lose.]]></content:encoded></item><item><title>DHS keeps trying and failing to unmask anonymous ICE critics online</title><link>https://arstechnica.com/tech-policy/2026/01/instagram-ice-critic-wins-fight-to-stay-anonymous-as-dhs-backs-down/</link><author>duxup</author><category>hn</category><pubDate>Mon, 26 Jan 2026 16:58:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>France Aiming to Replace Zoom, Google Meet, Microsoft Teams, etc.</title><link>https://twitter.com/lellouchenico/status/2015775970330882319</link><author>bwb</author><category>hn</category><pubDate>Mon, 26 Jan 2026 16:27:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RIP Low-Code 2014-2025</title><link>https://www.zackliscio.com/posts/rip-low-code-2014-2025/</link><author>zackliscio</author><category>hn</category><pubDate>Mon, 26 Jan 2026 16:11:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The rise of AI and particularly agentic development presents an existential threat to an entire category of low-code platforms. While the adoption of new techniques and tooling will take years to propagate through the Byzantine ranks of larger, slower-moving enterprises, the fundamental ROI case for these tools looks different in a world where the cost of shipping code now approaches zero.This may seem like a preposterous conclusion given the substantial size and growth of the sector. Forrester, who actually gave low-code its name back in 2014, projects the category will reach $50b by 2028 and sees no current indication that things will slow down, let alone contract. However, itâ€™s worth digging in to why these tools arose in the first place and the problems they solve to explore how much the landscape has shifted in just the past year.Put simply, these software platforms exist to allow users to create software with fewer developer resources. By purchasing one of these platforms, a company can enable non-technical stakeholders to ship production-ready experiences, often with little to zero actual code being written. This frees up developer bandwidth, accelerates the company, and until recently was a no-brainer investment for building internal and even customer-facing software.To enable these platforms in the real world, developers spend considerable time on prerequisite and ongoing work: piping and transforming data, writing and maintaining custom components that go beyond out-of-the-box functionality, and meshing authentication systems, to name a few. This investment is in turn justified by the reduction in development scope and complexity downstream of the low-code platformâ€”non-technical users can be left to their own devices to ship to their heartsâ€™ content.With the emergence of AI coding, this ROI case gets inverted. It is now often faster, cheaper, and easier to ship the kind of tools you might have built with low-code tools outside these platforms. Yes, this still requires developer time, but so did enabling these low-code-platforms in the first place.  Even disregarding the financial and organizational costs of low-code tools, AI affords developers the conveniences of their regular workflows without the bolt-on complexity introduced by external platforms. When you add in the total cost of ownership of these low-code tools, a return to in-house tooling becomes even more attractive.As an illustration of whatâ€™s possible, weâ€™ve seen this play out in real-time at Cloud Capital. In the not-too-distant past, we relied heavily on a low-code platform called Retool for almost all of our internal Admin tooling. We built management dashboards, reporting, and orchestrated complex workflows that were critical to the business. The acceleration was realâ€”our developers spent significantly less time rolling boilerplate tables, transforming data, and wiring up workflows. We even celebrated at our All Hands how much  our dashboards felt than if weâ€™d hand-rolled them or used a pre-canned admin interface.Then came the agentic tools that completely transformed the way we develop software. For our low-code tooling, the shift began with a single choice to prototype some new, self-contained functionality as a standalone internal tool instead of via our low-code platform. It was faster, easier, and leveraged our actual codebase in ways an external solution could not. That meant we shipped something safer, more robust, and more maintainable. The cherry on top is that the end product was also betterâ€”the UI looked and felt more like our in-house products, without the clunkiness required to stay on the WYSIWYG rails.It felt immediately clear that we had identified an unlock for our internal tooling velocity. All of a sudden, we found ourselves feeling constrained by the same low-code tools that until so recently were unblocking us. Changes that would have been one-liners in Cursor or automated triage tickets handled by an agent meant logging in to another platform, moving around clunky UX blocks, bashing against version management systems that werenâ€™t quite as polished or integrated as our core development flows. All with the additional cost of maintaining this additional system.What started as a single tool quickly became a wholesale migration of all of our Admin tooling, and the inevitable sunsetting of our Retool instance. They hadnâ€™t changed, but our culture and way of working had, and low-code couldnâ€™t keep up. What was most shocking was the timeline of this changeâ€”for us as a small, fast-moving startup, the transition including migration played out fully in just a couple of sprints.Itâ€™s inevitable that incumbent low-code tools will adaptâ€”they will need to in order to survive. In many cases, this shift is already visible in their marketing, such as Retoolâ€™s new AI-heavy positioning:At this point, itâ€™s hard to say whether itâ€™ll be enough. While itâ€™s possible low-code platforms will survive by providing non-technical users with the kind of magical experience thatâ€™s already possible for developers with AI coding tools today, it also seems likely they will continue to cede market share to the core AI players themselves. Weâ€™re beginning to see this take shape as non-technical AI artifacts become more complex, powerful, and collaborative.For us, abandoning low-code to reclaim ownership of our internal tooling was a simple build vs buy decision with meaningful cost savings and velocity gains. It also feels like a massive upgrade in developer experience and end-user quality of life. Itâ€™s been about 6 months since we made this switch, and so far we havenâ€™t looked back.Every build vs buy decision is unique, but many ultimately boil down to ROI in terms of speed, financial cost, maintenance cost, and organizational complexity. There are of course additional considerations like vendor lock-in, ownership of core competencies, ecosystem compatibility, etc., but in this case we can reduce the decision to this: will buying this platform let my team go faster, ship more, and create more value for our customers. At least for now, that answer feels clearer every day.]]></content:encoded></item><item><title>What &quot;The Best&quot; Looks Like</title><link>https://www.kuril.in/blog/what-the-best-looks-like/</link><author>akurilin</author><category>hn</category><pubDate>Mon, 26 Jan 2026 16:04:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Talent hits a target no one else can hit. Genius hits a target no one else can see.The second that the next round of funding hits the bank, every new CTO starts obsessing over the same thing: Who the hell do I hire next? The answer is surprisingly non-obvious. Youâ€™re told that you always wantâ€”scratch that, â€”the best of the best, your startupâ€™s future depends on it. Youâ€™re told your company is unique, special, and it requires the most hardcore among researchers, designers, engineers, and product managers. You never skimp on who builds the golden goose. You canâ€™t succeed with any less than that.But, is that  true?Every startup-hustle YouTube video, VC podcast and celeb founder interview regurgitates that the key to success is to hire the very best people, no matter what it takes. And yet you live in the real world, with real-world constraints. You have only so much money in the bank. Only so much time and bandwidth to hire. Only so much attention in a given day. And youâ€™re not alone in your hunt. Youâ€™re competing with hundreds of other players in your geography, industry, and problem space looking for â€œthe best.â€ Many of them have a more famous brand, more cash, more promising equity, more charming founders, and maybe even a high production value promo video showcasing happy employees, rare wood office counters and a shoes-off policy.Will you actually hire the best of the best against those odds? Many years ago I found myself in this pickle and I had to learn all the relevant lessons the hard way. I share these lessons here, so that you donâ€™t have to struggle through that same maze yourself. My pain is your gain.My first time around the startup world in 2012, I hardly knew what I was doing and relied mostly on luckâ€”and unfortunately, firingâ€”to end up with a team I could be proud of. I had no real point of reference for greatness, for what â€œthe bestâ€ in our area could look like, and building that model required lots of experimentation, with high highs and many low lows.The story of David, our brilliant infrastructure ops engineer at Freckle, has stayed with me ever since. David applied to the company back when we were only hiring for an ops role. We were growing slowly, so there was zero room for anybody who wasnâ€™t absolutely essential on the team. We had no open-ended extra seat for a smart person who just happened to be on the market, unlike some companies these days.David was 100% what we were  looking for. He had never done any ops. In fact, he had never done anything related to web dev or product engineering. Iâ€™m not sure he even knew what AWS was at that point. He was a sharp physics guy working with a professor on simulations in an academic context. He didnâ€™t let that deter him. He wanted to work at Freckle, thanks to our reputation as one of the few software startups in the world using Haskell in production at scale. We were an odd outlier in a sea of buggy and laborious Rails apps, a shelter for people who didnâ€™t want yet another web slop gig. And Haskell was oh-so-hot in the Hacker News programming language theory space at the time, a technology attracting software nerds obsessed with correctness and new, better ways of developing bug-free apps.I immediately told David that he was not a fit; he had none of what we were looking for. And yet he persisted, emailing me that he would do whatever test project we threw at him, and if he bombed it, no problem, he would go away. But if he nailed it, we would have proof that he was qualified, in spite of what his CV indicated. Fine. I sent him a meaty cloud ops take-home project, expecting to never hear from him again. Importantly, this was in the days before you could have Claude Code slap that together for you in two prompts.A day or two later, he returned the project to us, and it was pretty much flawless, doing even more than we had asked for. That was not expected. I got curious about what else he could do. We werenâ€™t drowning in applicants anyway, so I figured I didnâ€™t have much to lose. We took him through the usual interview process. He was humble, optimistic, well-spoken, actively communicating and taking feedback well, eager to get to work. He was pumped about everything he could learn on the job, about the doors that would eventually open if he nailed it. He didnâ€™t have much experience, as someone who had never written commercial software before, but he was really quick to absorb everything we threw at him.We gave him a chance. As predicted, he was stellar, and we had a really good run with him until he moved on to a much more illustrious employer. A few years later he became a senior principal engineer at Stripe, going from a physics lab, then a starry-eyed K-12 software startup, to being a big deal at one of Silicon Valleyâ€™s finest firms. An unsurprising path for one of â€œthe best.â€While building the different companies I worked at, Iâ€™ve run several times into â€œDavidsâ€ who ended up with meteoric career trajectories, sometimes already pedigreed in all the right ways, sometimes completely invisible to everyone but the trained eye of a CTO looking for gold. What is special about someone like that? And how can you, with your humble hiring budget, identify them before their value is obvious to everybody else in the market?With David, it was obvious that I had lucked my way into finding him. He had to badger me into seeing what he was capable of, at a time when I was only looking for obvious signs of success. Later on at my game studio, thanks to a brutally skewed job market, I had the total pick of the litter during Hiring Summer and could select one or two stellar engineers from hundreds of perfectly reasonable applicants. There was hardly any competition, with money drying up all around the industry. Again, I found a diamond in the rough. But this time I had the right knowledge and strategies in place to end up with the kind of team I wanted with far less reliance on luck. Even as I was drowning in hundreds of resumes that all started to look the same, â€œthe bestâ€ candidate was still in there, like a needle in a haystack. This time, I felt like I was actually skilled enough to find that hidden gem, not simply crossing my fingers and relying on luck alone. Thatâ€™s a feeling of empowerment I would like you to experience as well.What do we mean by â€œthe bestâ€?Of course, when every company says they have the best people, the math doesnâ€™t work out. And of course, if they regularly have to fire staff, something doesnâ€™t add up again. One day you end up landing a job at one of these companies and realize that the braggadocio was hardly reflective of reality, but then again, itâ€™s all part of the startup founder LARP that requires you play the part. â€œOur team is okay, you will probably like working here, we sometimes know what weâ€™re doingâ€ would be a much more accurate depiction, but youâ€™ll never find that quote on the careers page.Regardless, you want to be a good Boy Scout CTO and live up to the lofty expectations set for you by the Silicon Valley elders, and at least try to hire â€œthe best of the bestâ€ as they say in all of the fireside chats.But then, a question naturally emerges: what exactly do we mean by ? The accolades? Pedigrees? Github stars? Hacker News front pages? Job title? Celebrity employers? Number of former YCombinator companies worked at? Hackathon wins? Typing speed? Clout among other Rust developers wearing fuzzy animal costumes? That all sounds good, , but is that all relevant to your company?It turns out that â€œthe bestâ€ is mostly subjective and specific to your situation. The culture, the vibe, the industry, the processes of your company, and the technical choices will all influence who will be a phenomenal addition to your tribe. That person might not be the same individual bringing in a million or two a year at Meta. In fact, that highly pedigreed, highly connected individual might be a net negative for your company, even though they might, at least on paper, seem the most high-end among candidates. They may blow the mind of a recruiter at Netflix, but they may not want to schlep through all of the messiness and chaos of an early stage company still trying to define itself. Theyâ€™re not the best , and theyâ€™re likely not the best for startups, but plenty of other people are. Your job is to determine what those traits are and where to find people who have them.Universally Best Startup HiresYour company, timing, industry, problem space and founding team personalities are unique, which is why a generic blog post or a book could never tell you exactly what hires are optimal for you. However, experience shows that there are many universal winning commonalities between great hires that will be applicable regardless of your early stage companyâ€™s unique DNA. I identify 11 of them below.As a side note, I found Patrick Lencioniâ€™s framework from  to survive first contact with both pre-seed and Series A+ realities. Itâ€™s one of those dry-as-dust HBR book club management self-help guides that get an eyeroll by your median 20-year-old founder going through YCombinator, but in my experience it confirmed and put a simple model around something I had personally seen emerge again and again in the field.Lencioni identifies three essential traits: ,  and , the latter of which should have been called EQ all along. Youâ€™ll see them referenced below. The three criteria may seem obvious at a glance, but having a simple framework to work with as youâ€™re making hundreds of decisions makes a big difference. Simple things done right turn out to be pretty darn powerful.Letâ€™s take a closer look at the 11 commonalities among great startup hires Iâ€™ve identified. Some of them are simply statistically likely to occur due to the nature of startup hiring. Others are ones you should straight up filter for when youâ€™re doing your search.The best hires in the early stages are usually non-obviously good to the untrained eye. They donâ€™t look as appealing to employers with infinite resources who otherwise would have already hired them. If they were obviously incredible, it would be unlikely youâ€”at your startupâ€”would ever talk to them.One of the most important skills of a startup CTO trying to hire an amazing team is the ability to uncover awesome talent that others have overlooked. These candidates get skipped often due to being too off-the-beaten-path and under-pedigreed compared to the obvious picks that every other company is making. Often these candidates are bad at marketing themselves, donâ€™t know where to look, and are off-putting in some way to the naive search. You have an opportunity to take advantage of that oversight.Realistically, someone who looks like the perfect FAANG candidate who has won all of the math olympiads, had all of the stellar internships, and went to the top CS schools will either:a. get a big cash dump from YC and start their own thing, or
b. go work at one of the career-building brand names in the Valley and make a monstrous amount of money.The karmic wheel is just about cycling between those two until that person either gets into VC or becomes an exec at a prestigious firm years later. If thatâ€™s what the universe has in store for a candidate, taking a chance on a no-name team with 12 months in the bank, poor development practices and sloppy management is a tough proposition.That leaves a pretty large pool of â€œeverybody elseâ€ who didnâ€™t pattern-match. How you go about sifting through them is a big topic I will leave for another chapter, but the key is to look for signals that are less obvious than a Stanford degree and an OpenAI internship. Often that looks like a large volume of work, expertise in unsexy niche areas of technology, an intense work ethic driven by curiosity, and many others.A Stanford CS degree is no guarantee someone will be a phenomenal contributor , but itâ€™s a reasonable proxy of future potential for large employers with long time horizons that allow them to invest in coaching and nurturing their junior staff. Thatâ€™s not in the cards for a pre-revenue startup that will run out of money next year.The best people for your startup will most likely be senior, or at least mid-stage contributors with several solid years of experience. Too senior isnâ€™t great either, you donâ€™t need the large scale cross-team mature product jousting chops. Thereâ€™s likely no staff-level work for them to do, and staff-level engineers arenâ€™t just faster-typing seniors.As an early stage startup, until you have enough lead developers, senior product managers, senior designers etc. to set the standard, hiring fresh-out-of-school contributors is a gamble. Junior staff are still developing their taste, judgment, ability to work in a team, and understanding of how to follow the process and when to deviate from it. Without an adult in the room youâ€™re in cat-herding territory. This isnâ€™t to say that you wonâ€™t get work done this way, but you could have had someone else in that seat who required less supervision. And youâ€™re at a stage where every seat is mission-critical and the opportunity cost is significant.While it might not matter too much as you throw prototype spaghetti at the wall in the early days, as soon as you have something worth maintaining and complexity rises, youâ€™re now in a race against time. Someone must actively garden the complexity of the work and be ultimately responsible for it, and that someone will tend to be not fresh-out-of-school. Sure, a green dev  care about complexity. But most havenâ€™t yet lived through a soul-crushing, multi-week refactor fueled by years of tech debt, the kind of preventable trauma that earns you â€œcharacter-building scars.â€ If theyâ€™re learning those lessons on your watch, theyâ€™re doing it at the expense of your delivery timeline and your sanity.Iâ€™m fond of mid-stage software developers who demonstrate terrific chops, hunger to learn and prove themselves. Yes, they require more hand-holding, but usually the volume of work they put out, and their openness to re-do it, if needed, is worth betting on. You should be able to manage a couple of them yourself as the CTO in the early days, and soon enough youâ€™ll have other senior people on the team who will pick up the mentorship torch.This also means that the â€œbest peopleâ€ to hire at this stage will tend to be earlier in their careers. The longer great people are on the market, the more likely they are to be identified by the market as being great. Your job as a startup CTO is to find them before others do. Once the market has found them and has actively started rewarding them, they will be out of your price range.Now, itâ€™s worth mentioning that in specific well-capitalized niches of the startup market, companies are now able to pay a decent chunk of change thanks to larger VC rounds and quicker time-to-revenue. Thus, the old school notion that BigCo always pays best may not hold true as often. Hereâ€™s a quick sample of many Work at a Startup roles in December 2025. Not quite at FAANG-level, but far from starvation.That also means that, unless your company grows fast or shows tremendous potential for the yet-unvested equity, you wonâ€™t get a ton of time with your great early career hires. They will quickly accumulate valuable experience and prove themselves to be stellar, and move on to a prestigious employer with longer time horizons, a great brand name, and a stupendous paycheck that you will not be able to match. Being an L5-7 at BigCo pays a pretty penny, with none of the unpredictability of startup equity and saner hours, but none of the adventure and camaraderie of a pirate ship. For employees later in their careers, thatâ€™s not a bad tradeoff. Once they have vested with your company, it makes sense for them to diversify their equities, as they already own one lottery ticket.Being on the receiving end of a reference call for one of your star employees, while theyâ€™re still working at your company, is always bittersweet: on one hand you want whatâ€™s best for them, to level up and grow in their career. On the other, theyâ€™re  discoveryâ€”dang itâ€”and you know you wonâ€™t be able to counter-offer the type of employer they can now attract. Or they decide to start their own company, emboldened and informed by the experience at yours.In the end, thatâ€™s okay. Sometimes a great hireâ€™s growth trajectory is faster than a startupâ€™s, and you can be grateful for having given them a chance to prove themselves and to find a new path that they wouldnâ€™t have otherwise. That kind of hire will have likely made a major difference to your company and having trusted alumni out there in the world doing great things should be a source of pride and good industry karma for any CTO who discovered them.The best hires are self-driven, stoked by the ability to learn, to gain mastery of the craft, and by being able to show off their accomplishments to the rest of the team they want to impress. When you look at their track record, even when itâ€™s not filled with household-name accolades and pedigrees, you will typically see breadcrumbs of exploration, toy projects and experimentation fueled by curiosity. Itâ€™s that classic desire to hack things and understand them.You donâ€™t need to ask them to work a little harder, they know this is a big growth spurt for them and they want to take advantage of it. The company is giving them a valuable practice canvas for their skills and they want to make the most of this opportunity, which unfortunately isnâ€™t that common. They love a challenge, they want to prove themselves, they love the work and they are excited by being around others who will push them to be their best. They would have been hacking on something interesting either way, but now youâ€™re actually paying them to do it.Theyâ€™re curious about how everything works: your company, the startup world, the industry theyâ€™re diving into, other disciplines, the tech theyâ€™re using. Theyâ€™re learning, absorbing. Maybe they want to start their own thing one day, or maybe theyâ€™re set on using this gig as a ramp to something else they dream about. Maybe they just want the new optionality granted by having your company on their resume. Theyâ€™re sponges and will gladly take on the challenges you send their way.I remember throwing a big Redux experiment at David in what must have been his first month at work. Again, no web dev experience prior to this. Nobody on the team knew the technology and he, hired as an infrastructure engineer with a background in physics and zero product engineering experience, had something up and running for us within days. He was  he got to do something so radically new for the team as one of his first assignments. Ultimately we decided the tech wasnâ€™t a fit for our existing codebase, we scrapped the experiment, and he ran off to the next challenge with no loss of enthusiasm.Our newly-hired star engineer, us being , had to dive into the pit of madness. He almost gave up several times before finally identifying all of the spots in the physics sub-stepping logic that was causing the drift across the network. Iâ€™m confident I myself would have gone completely bananas trying to debug that one. He was proud of the trust we had put in him, he was eager to learn, he didnâ€™t want to let the team down, and I knew enough about him to make the bet that the challenge would have been just barely within his abilities.Hunger is a massive force-multiplier, and unlike many other traits, it doesnâ€™t seem to be teachable. Failures will instill humility. Rejection will hone your EQ. But youâ€™re either driven to compete, learn and prove yourself, or youâ€™re not. No managerâ€™s pep talk will kickstart that drive in you, at least not sustainably. The hubris has to come from somewhere much deeper.Startups are all about rapid experimentation, trying countless ideas that donâ€™t work, and bootstrapping the skills and processes of the company as it figures out just what exactly people will pay for. Trust-building is critical in any team endeavor, and members who are unable to admit fault, who take up all of the space, and/or who need to be right at all costs will be a real problem as the company matures.The best hires are humble and will happily talk about both their victories and their biggest implosions, about what they learned from their misadventures, about talented past coworkers and how their efforts were part of a team. They rarely claim to have single-handedly carried everything on their backs.As you interview candidates searching for the best, many non-obvious great hires will be bad at behavioral interviewsâ€”or interviews in general, for that matter. They didnâ€™t get extensive training on answering questions using the STAR method, didnâ€™t drill stock responses to predictable and tired interview questions. Theyâ€™re early-ish stage hands-on technology people, not smooth-talking execs jumping from one boardroom to the next. It is your job to fish for key nuggets of insight buried in the rough presentation. After you interview enough people with a similar set of standard questions, youâ€™ll be able to spot the outliers in the bell curve of responses.An underrated aspect of humility in startups is detachment from oneâ€™s work. Companies in the process of inventing themselves need to actively cycle through many ideas, most of which will ultimately not stand the test of time.  and  are important mindsets in creative work, and they extend far beyond music, film and games. Quantity leads to quality, and a great hire will accept regularly needing to apply a flamethrower to their work and try again with the new learnings from the latest experiment. They will not be precious about it, nor will they feel diminished by needing to try again. The Davids do not keep score; they keep trying and do the work knowing that thereâ€™s a good chance it will not go anywhere.That is also highly correlated with their ability to take feedback without taking it personally. Theyâ€™re not afraid of criticism and quick feedback loops. In fact, they seek disconfirmation sooner rather than later. They have excellent feedback metabolism.Building technology companies is a team sport. Everything you do is with the help of other people, for the benefit of other people. As much as the startup world fetishizes the cracked ninja jedi 10x code-poet, if that person makes others never want to talk to them again, cracked they are not. Someone who is able to work with othersâ€™ quirks, understand how they operate and what they respond to, and act in a way that brings the best out of their teammates is worth their weight in gold. Brilliant jerks, as great as they are for producing riveting drama in TV shows, quickly become a net negative in the real world.Weâ€™re in the squishy human feeling territory here (aka â€œsoft skillsâ€) and you canâ€™t LeetCode your way into knowing if someone will be an ass. Thereâ€™s no FizzBuzz for empathy. You have to ask them standard questions, shoot the breeze, interact during the various testing phases of the interview, and outside, and ultimately make a call based on the limited data you have. You will never have all of the evidence, but ultimately you have to make a go or no-go call. Sometimes you get lucky and the candidate discloses a consistent pattern from their past that is obviously disqualifying, but often you donâ€™t.How they interact with you and your team throughout the interview is usually a good data point, and that, of course, goes both ways. Regardless of how you investigate this side of them, itâ€™s non-negotiable, and the best hires youâ€™ll make will all have a high EQ.At the same time, someone highly empathetic, but who hides in a cave and is unable to coordinate their work with the rest of the software orchestra, is not the best.Great team players anticipate othersâ€™ questions and concerns, and are proactive about reaching out and communicating both their status and the progress of their work to those around them. They make their work visible instead of requiring constant polling for the team to figure out what exactly is going on with them.Remote environments in particular benefit from strong proactive communicators, as you canâ€™t get away with always knowing whatâ€™s going on with everybody due to sitting in the same room for most of the week. One great Loom is worth a thousand words, but a thousand words is still much better than having to pull status out of people.Early stage startups move at a pace that generates tremendous entropy, and someone whoâ€™s able to coordinate with the rest of a team in a way that feels natural and effortless will allow the small teams to scale without requiring project management and complex processes meant for lower-performing contributors.Perhaps counterintuitively, I donâ€™t fret too much about competence. To me, thatâ€™s a given. If your interview funnel is set up well, with a great take-home project, an onsite, or even a work trial, determining if the person youâ€™re interviewing is competent should emerge naturally. After all, you and your team are technical experts, craftspeople who can look at another craftspersonâ€™s work and quickly judge if it is any good.Yes, AI is getting pretty good these days and itâ€™s becoming easy for candidates of all disciplines to hide behind prompts, but thatâ€™s only an extra reason to allow them to bring those tools into the interview itself and showcase their use rather than shamefully hide their existence. How they use the agents, the back-and-forth, the planning, the tweaking, the types of searches they do as part of the exercise are all valuable data points for determining competence, and you should allow candidates to surface them.In-person interviews and work trials are for now non-gameable, so if youâ€™re extra paranoid, they are a great option. Non-obvious candidates will be in the hiring race with fewer other companies, possibly none, depending on your industry. That gives both you and them more flexibility and time to try working together for a few days before making it official. With a sufficiently relevant set of tasks for the work trial, you can rest assured that theyâ€™re competent, and learn a lot more about their other traits in the process.High-agency problem-solversThe best start-up hires are problem-solvers, not ticket resolvers. They want to understand what the business is trying to accomplish, what challenge is standing in the way of it, and how to make that problem go away. Theyâ€™re autonomous in the best of ways. They donâ€™t sit around waiting for someone to tell them what to do; they proactively find challenges and opportunities for improvement, iterate on the feedback, and go out of their way to help their teammates. They figure things out, and they do it without needing their hands held.Someone like David didnâ€™t take no for an answer; he assumed there had to be a narrow path through the rejections he kept getting from me, that it was only a matter of his resourcefulness before he found a crack through which to get what he wanted.The communication, coordination and cohesion overheads that emerge with the addition of more and more staff to the roster are probably the toughest parts of scaling any business. A leaderâ€™s job is to streamline and remove these emergent dependencies between people and teams as much as possible.Besides keeping the number of hires low, the other powerful lever that a start-up CTO has is to hire people who will be self-sufficient and require little to no support to get their jobs done. They will ask for help when necessary, but they will take pride in figuring things out on their own, checking in with you when the time is right. They will require little supervision overall, and only direction when it comes to their work. If theyâ€™re falling behind, being great communicators, they will let the rest of the team know. If theyâ€™re early, theyâ€™ll gladly pick the next interesting opportunity to be useful.If theyâ€™re struggling, their pride in their work will make them double-down and overcome the challenge at all costs. In that case your job is to help them pump the brakes when necessary, or their pace can become unsustainable.Cross-disciplinary empathyA great startup hire doesnâ€™t only think about their respective lane, but cares about the other disciplines around them that their work is impacting. They understand their angle, their priorities, and their expectations. For example:A back-end engineer anticipates the needs of the front-end and of the infrastructure developers as they make their changes rather than waiting for their work to be pushed back once it doesnâ€™t meet the standard.A gameplay engineer doesnâ€™t wait for the game designer to tell them that the feel of what they implemented wonâ€™t fly with the player. They put themselves in the shoes of the other discipline and think like them.A front-end engineer doesnâ€™t just roll out the interface theyâ€™re working on, they think through the UX and the UI and the usability and the product management side.They think about where and how their work will land and anticipate those objections, addressing them in advance. They proactively reach out to those disciplines to avoid yet another coordination chokepoint that will only gobble up time. As more and more skills are compacting into one single individual, we see AI engineers, product engineers, and product managers becoming more technical and moving downstream into programming. This sort of cross-disciplinary mindset buys companies a ton of leverage and ability to iterate faster.Of course, this becomes progressively easier with experience and seniority. But thatâ€™s also why startups moving fast benefit from more mature contributors as time goes on. The reduced communication costs multiplied across more and more team members really add up over time. Unless you have very specific technical needs, having a generalist bias in the early days is a great idea. In fact weâ€™re seeing more and more disciplines get compressed into one, with software developers covering UI, UX, PM and engineering, effectively becoming a cross-disciplinary team of one. Even in later stages, half-a-pizza teams powered by AI can move mountains on timelines that felt impossible just a few years ago.Early-stage startups are notoriously chaotic and unpredictable. You might be working on one feature one day, it might get cancelled the next day, and maybe now youâ€™re fielding customer support calls for something you shipped that accidentally blew up. The next day youâ€™re taking a trip to a customer site to chat with potential users.At no point do you know if any of this will make you money, how long until you kill that feature, and how many more of these iterations youâ€™ll do before you either strike gold or the company runs out of money. There are no guarantees in the startup world, except for the fact that your runway will eventually reach zero if you donâ€™t find something worth selling.As a CTO, it is your responsibility to shield your team from the messy everyday financial reality of where your company is headed. At the same time, I prefer to keep as much as possible in the open so that the team knows how much longer the music will play and when itâ€™s time to start refreshing their LinkedIn profiles. I never want anything to be a surprise to the team when I could have been candid about it far in advance. Itâ€™s a fine balance between hiding the daily volatilityâ€”mostly of the foundersâ€™ moods and their confidence in the company making itâ€”and exposing the long-term trends.This type of universe requires people who are okay with fewer guarantees.A startup will never have the time to run comprehensive studies, to build extensive plans, to gain all of the information necessary to make the right decision. Seeking perfection at the cost of an action is generally unacceptable, and you learn much more by failing in a valiant attempt than in delaying a perfect attempt that you might have validated months earlier by just doing the thing.Great hires respect that process and are willing to operate without all of the information, accepting that the team will figure it out as it moves forward, often deviating from the original objective. Deleting existing artifacts or completely pivoting to a new strategy on a dime is the ultimate startup superpower.Anybody who needs well-spelled-out plans and continuity in whatever theyâ€™re doing will ultimately struggle in that kind of environment.Hiring is a gamble, but you can tilt the odds in your favor if you stop playing the same game as everyone else. The â€œbestâ€ hire isnâ€™t the person with the most GitHub stars or the flashiest resume; itâ€™s the person who makes your team better by existing within it. Itâ€™s the David who breaks down doors and ends up shaping the direction of the company through their relentless contributions.Developing an eye for non-obvious talent is the only way to build a high-leverage team on a startup budget. Yes, it takes more work. It requires you to actually pay attention. Itâ€™s one of the hardest feats to pull off in startup team building. It means you must trust your own judgment over a candidateâ€™s credentials. But if you do it right, you donâ€™t just get a â€œniceâ€ team, you get a team capable of doing the impossible. Now stop reading and go find â€œthe bestâ€.]]></content:encoded></item><item><title>There is an AI code review bubble</title><link>https://www.greptile.com/blog/ai-code-review-bubble</link><author>dakshgupta</author><category>hn</category><pubDate>Mon, 26 Jan 2026 15:38:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today, we're in the hard seltzer era of AI code review: everybody's doing them. OpenAI, Anthropic, Cursor, Augment, now Cognition, and even Linear. Of course, there's also the "White Claws" of code review: pure-play code review agents like Greptile (that's us!), CodeRabbit, Macroscope, and a litter of fledgling YC startups. Then there are the adjacent Budweisers of this world:Amazingly, these two were announced practically within 24 hours of each other.As the proprietors of an, er, AI code review tool suddenly beset by an avalanche of competition, we're asking ourselves: what makes us different?How does one differentiate?Based on our benchmarks, we are uniquely good at catching bugs. However, if all company blogs are to be trusted, this is something we have in common with every other AI code review product. One just has to try a few, and pick the one that feels the best.Unfortunately, code review performance is ephemeral and subjective, and is ultimately not an interesting way to discern the agents before trying them. It's useless for me to try to convince you that we're the best. You should just try us and make up your own mind.Instead of telling you how our  is differentiated, I am going to tell you how our  is differentiated - how we think code review will look in the long-term, and what we're doing today to prepare our customers for that future.Our thesis can be distilled into three pillars: independence, autonomy, and feedback loops.We  believe that the review agent should be different from the coding agent. We are opinionated on the importance of  code validation agents. In spite of multiple requests, we have never shipped codegen features. We don't write code; an auditor doesn't prepare the books, a fox doesn't guard the henhouse, and a student doesn't grade their own essays.Today's agents are better than the median human code reviewer at catching issues and enforcing standards, and they're only getting better. It's clear that in the future a large percentage of code at companies will be auto-approved by the code review agent. In other words, there will be some instances where a human writes a ticket, an agent writes the PR, and another agent validates, approves, and merges it.This might seem far-fetched but the counterfactual is Kafkaesque. A human rubber-stamping code being validated by a super intelligent machine is the equivalent of a human sitting silently in the driver's seat of a self-driving car, "supervising".If agents are approving code, it would be quite absurd and perhaps non-compliant to have the agent that  the code also  the code. Only once would you have X write a PR, then have X approve and merge it to realize the absurdity of what you just did.Something that Greptiles generally agree on is that everything that  be automated,  be automated.Code validation - which to us is the combination of review, test, and QA, is an excellent candidate for full automation. It's work that humans don't want to do, and aren't particularly good at. It also requires little in the way of creative expression, unlike programming. In addition, success is generally pretty well-defined. Everyone wants correct, performant, bug-free, secure code.While some other products have built out great UIs for humans to review code in an AI-assisted paradigm, we have chosen to build for what we consider to be an inevitable future - one where code validation requires vanishingly little human participation. We have no code review UI, and view ourselves as more of a background automation or "pipes" product. Human engineers should be focused only on two things - coming up with brilliant ideas for what should exist, and expressing their vision and taste to agents that do the cruft of turning it all into clean, performant code.Not long ago, we released our Claude Code plugin. It can do many things - but most notably, you can ask Claude Code to pull down and address Greptile's comments from the PR. You can ask it to keep going until there are no new comments, waiting a few minutes for a review after each push.This is a step towards the future we're excited about: Human expresses intent, coding agent executes, validation/review agent finds issues and hands them back - kicking off a loop until it approves and merges. If there is ambiguity at any point, the agents Slack the human to clarify.The question of how these things are different is important. Unlike picking IDEs and coding agents that ostensibly have low switching costs, code review products are harder to rip out, so your decision will very likely turn out to be a long-term one, especially if you're a large company.We've been around for about as long as AI code review has been around. It has gone from a fringe interest of the world's most adventurous vibecoders to a mainstream product that our enterprise users (including two of the Mag7) often describe as a "no-brainer" purchase.Yet, our guess on where this goes is about as good as anyone else's. Meanwhile, we'll keep doing what we've always done - trying to make things our users love.]]></content:encoded></item><item><title>Qwen3-Max-Thinking</title><link>https://qwen.ai/blog?id=qwen3-max-thinking</link><author>vinhnx</author><category>hn</category><pubDate>Mon, 26 Jan 2026 15:23:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Windows 11&apos;s Patch Tuesday nightmare gets worse</title><link>https://www.windowscentral.com/microsoft/windows-11/windows-11s-botched-patch-tuesday-update-nightmare-continues-as-microsoft-confirms-some-pcs-might-fail-to-boot</link><author>01-_-</author><category>hn</category><pubDate>Mon, 26 Jan 2026 15:07:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Updated 3 PM ET on January 25, 2026:For those experiencing issues with boot failures, we had to put together a whole new guide on how to fix it. While we wanted this out earlier, it takes a few hours to research and write it all up to make sure it's accurate versus just getting the news out, so we appreicate you patience, especially since it's a Sunday and most of our staff are off. â€” Daniel Rubino, Editor-in-ChiefMicrosoft has confirmed that some users might find their PC unable to boot after installing the January 2026 security update released on January 13. This is on top of the plethora of other issues that have been reported since Microsoft's disastrous Patch Tuesday updates arrived.So far, the company has released two emergency out-of-band updates for Windows 11 to address major bugs that were introduced with this month's security updates, but this latest issue that is causing PCs to fail to boot has not yet been addressed."Microsoft has received a limited number of reports of an issue in which devices are failing to boot with stopâ€¯code â€œUNMOUNTABLE_BOOT_VOLUMEâ€, after installing the January 2026 Windows security update released January 13, 2026, and later updates," the company has confirmed in an online bulletin () "Affected devices show a black screen with the message â€œYour device ran into a problem and needs a restart. You can restart.â€ At this stage, the device cannot complete startup and requires manual recovery steps."Microsoft says this issue is likely to impact users running Windows 11 version 24H2 and 25H2 on physical machines, and that it is exploring potential fixes and workarounds. In the meantime, if you do encounter this problem, you will need to manually recover your PC by entering the Windows Recovery Environment and uninstalling the latest January 2026 security patch.It's unclear how common this issue is, as most users have not reported their PC unable to boot. The company says it has received a limited number of reports, but has not provided an explanation as to what is causing the unbootable state, or whether it can be avoided.This is the latest in a long line of issues that were introduced with this month's Patch Tuesday updates. First, users reported that PCs running version 23H2 were unable to shutdown or hibernate, and PCs running version 24H2 and 25H2 were unable to sign-in when using Remote Desktop.A few days later, reports came in confirming an issue that rendered cloud-backed apps like Outlook, Dropbox, and OneDrive inoperable, forcing Microsoft to issue two emergency updates to address these showstopping bugs. Now, with reports that some PCs are unable to boot, it's likely the company will need to issue a third out of band update to fix this problem too.It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been.]]></content:encoded></item><item><title>OSS ChatGPT WebUI â€“ 530 Models, MCP, Tools, Gemini RAG, Image/Audio Gen</title><link>https://llmspy.org/docs/v3</link><author>mythz</author><category>hn</category><pubDate>Mon, 26 Jan 2026 15:01:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Access over 530 models from 24 providers via models.dev integrationRedesigned full-featured dialog with search, filtering, sorting, and favoritesAdd features, providers, and customize the UI with a flexible plugin architectureManage Gemini File Search Stores and manage document uploads for RAG workflowsFirst-class Python function calling for LLM interactions with your local environmentConnect to Model Context Protocol servers for extended tool capabilitiesDesktop automation - control mouse, keyboard, and take screenshots like a humanSupport for beautiful rendering of LaTeX math expressionsBeautiful UX Friendly UI to evaluate python math expressionsExecute Python, JS, TypeScript and C# code scripts in a CodeMirror editorBuilt-in support for Google, OpenAI, OpenRouter, Chutes, and NvidiaTTS support for Gemini 2.5 Flash/Pro Preview modelsBeautiful UI to browse generated images and audio generationsMigrated IndexedDB to server SQLite for robust persistence and concurrent usagePersistent image/file file caching with metadataManage Gemini File Search Stores for RAG workflows with document uploads and syncGet instant access to 530+ models from 24 providers with extensibility at its core:A major change to significantly increase the available models is the switch to utilizing the same models.dev open provider and model catalogue as used and maintained by OpenCode. provider configuration is now a  of models.dev/api.json where its definitions are merged, allowing you to enable providers using just  to inherit the configuration from The switch to models.dev greatly expands the model selection to over  from , including new support for:Non OpenAI Compatible LLM and Image generation providers are maintained in the providers extension, registered using the  API. There are several different provider implementations to take advantage of features available in each provider, such as  support in Anthropic's Messages API which enables all Claude and MiniMax models to reason between tool calls for improved agentic performance.This actively maintained list of available providers and models are automatically updated into your  daily that can also be manually updated with:As an optimization only the providers that are referenced in your  are saved. Any additional providers you want to use that are not included in models.dev can be added to your ~/.llms/providers-extra.json, which get merged into your  on every update.This keeps your local configuration file lightweight by only including the providers that are available for use.Enable providers by ID â€” all configuration is automatically inherited:With over 530 models from 24 providers now available, discovering and selecting the right model required a complete overhaul.
The Model Selector has been completely redesigned as a full-featured dialog offering:ðŸ” Smart Search & Discovery - Instantly search across model names, IDs, and providers - Filter by name, providers & input and output modalities - Sort by Knowledge Cutoff, Release Date, Last Updated & Context - Star model card to add/remove to favorites quick list - In depth model overview at a glanceWhere providers can be quickly enabled or disabled to customize which models are available:llms.py has been rewritten from the ground-up with extensibility a  where all major UI and Server features now layer on their encapsulated functionality by using the public Client & Server Extensibility APIs.Extensions are just folders that can add both Server and UI features using the public client and server extensibility APIs. Built-in features are just extensions in the repo's llms/extensions folder which can be disabled or overridden by adding them to your local  folder. Too minimize bloat, only features that are generally useful and don't require additional dependencies are included as built-in extensions.llms includes support for installing and uninstalling extensions from any GitHub repository. For better discoverability, non built-in extensions are maintained in the github.com/llmspy organization repositories which anyone else is welcome to contribute their repos to for increased discoverability.UI components are now registered and referenced as Global Vue components, which can be easily replaced by registering Vue components with the same name as done in the xmas extension demo.This approach allows main.py to retain a lean functional core in a single file whilst still being fully extensible and lays the foundation for rapid development of new features - both from the core team and external 3rd party extensions - enabling the community to extend llms.py in new unanticipated ways.For deployments requiring minimal footprint, the Custom Build docs shows how to create a tailored distribution with only the specific extensions you need - perfect for CLI-only or lightweight API server deployments.To keep the core lightweight while enabling limitless enhancements, we've implemented a flexible  inspired by ComfyUI Custom Nodes. This allows adding new features, pages and toolbar icons, register new provider implementations, extend, replace, and customize the UI with your own custom features, just by adding new extension folders.List available extensions:Install a 3rd-party extension:Clones the GitHub repo into ~/.llms/extensions/my_extension and installs any  dependencies.List installed extensions:Extensions can be installed from GitHub or by creating a local folder:: Simply create a folder in ~/.llms/extensions/my_extension: Clone extensions into , e.g:Extensions are Python modules that plug into the server lifecycle using special hooks defined in their :Enhance the server instance (routes, providers, filters, etc.)Load data or perform  tasks before server startsExecute custom logic when running in CLI modeThe  parameter provides access to the .Extensions can also include frontend components:: Add a  folder within your extension directory: Files in this folder are automatically served at : Create a  file. This is the entry point and must export an  function:The xmas extension demonstrates these capabilities where it utilizes the Extensions APIs to give llms.py a splash of Christmas spirit. It uses  to register an API endpoint and a UI extension for its UI features.All UI features of xmas is implemented in its ui/index.mjs
which overrides default  and  components by registering components with the same name, e.g:To change both the home page and brand on the top-left to give every page title a festive touch:It also demonstrates adding a new icon on the left sidebar to open its custom Xmas page component and a top-panel component to display its "Ask Santa" portal:The Xmas page calls a custom API endpoint registered in its  hook to return a custom festive greeting, whilst the top-panel modifies chat requests while its Top Panel is open to add a Santa system prompt which is enough to implement its "Ask Santa" feature.Smart generation models like Nano Banana's  perform exceptionally well here as they're able to answer your kids questions with rich, detailed responses and image outputs.The gemini extension provides a complete solution for managing Google Gemini's File Search Stores, enabling RAG (Retrieval Augmented Generation) workflows with automatic document uploads, category organization, and bidirectional sync between your local database and Gemini's cloud storage.Build up your own knowledge base in File Stores, optionally organized into categories, that you can query to ground your AI chats with your own data - whether that's searching across a single document, a category of related documents, or your entire filestore.Install the  extension via the CLI:After which you'll be able to click the  to open the Gemini extension page from the sidebar to manage your filestores.: Create and manage isolated stores of documents for different projects or knowledge bases: Easily upload documents (PDF, Text, Markdown, etc.) by dragging them into the UI: Organize documents into categories (folders) for granular retrieval:: Chat with the entire knowledge base of a filestore: Focus your chat on a specific category within a filestore: Chat with a single specific document: Reconcile your local database with the remote Gemini File APIDocuments can be uploaded by dragging and dropping files onto the upload zone or clicking to open the file picker. You can organize uploads into category folders by typing a category name before uploading.Uploads are processed asynchronously by a  utilizing a , so you can continue working while documents are indexed. The worker automatically starts when new documents are uploaded and efficiently handles batch processing without blocking the UI.Once documents are uploaded, you can start contextual RAG chat sessions with your data. Each session is pre-configured with a Gemini Model and the  tool to query your selected filestore, category, or document - as shown in the  below querying this very  document for its best features:The grounded sources used to answer your query are displayed at the bottom of each chat response, allowing you to verify and explore the source documents.This release also includes first-class support for Python function calling (Tools), allowing LLMs to interact with your local environment and custom functionality.Tools can be defined using standard Python functions where its tool definition can be implicitly defined from its function's signature, type hints, and docstrings:Tools can be registered within an extension's  hook using :If no group is specified, tools are registered under the default  group, alternatively you can group them under your preferred name:When more fine-grain configuration is needed you can use an explicit tool definition, e.g:: Use the Tool Selector in the top-right to control which tools to use per request: Select "All", "None", or specific tools for each chat session: View all registered tools and their definitions at  or via the sidebarThe fast_mcp extension brings Model Context Protocol (MCP) support to llms.py, allowing you to extend LLM capabilities with a wide range of external tools and services using the FastMCP Python Framework.: Connect to any MCP-compliant server (Node.js, Python, etc.) seamlessly: Automatically discovers and registers all tools exposed by configured servers: All configured MCP servers are discovered concurrently for fast startup times: Add, edit, and manage MCP servers directly from the Tools pageMCP servers are configured via a  file. By default, Anthropic's Git MCP Server is pre-configured:Add, edit, or remove MCP servers directly from the UI:MCP tools can be executed directly from the Tools page or invoked by LLMs during chat sessions:Tool outputs containing HTML content are rendered within a sandboxed iframe, letting you interact with rich content and even play games:See the MCP Support docs for complete configuration and usage details.For Omarchy users, the Omarchy MCP enables AI assistants to manage themes - including listing, switching, previewing, installing, and removing themes from your Omarchy desktop environment.The built-in core_tools extension provides essential functionality for LLMs to interact with their environment, perform calculations, and manage persistent data.Functions for persistent key-value storage. - Read a value from persistent memory. - Write a value to persistent memory.All file system operations are restricted to the current working directory for safety. - Read a text file from disk. - Write text to a file (overwrites existing content). - List directory contents including file names, sizes, and modification times. - Find files and directories matching a glob pattern. - Get the current time in ISO-8601 format. - Evaluate a mathematical expression. Supports arithmetic, comparison, boolean operators, and common math functions.LLMS includes a suite of tools for executing code in various languages within a sandboxed environment. These tools are designed to allow the agent to run scripts, perform calculations, and verify logic safely. - Executes Python code. - Executes JavaScript code (uses  or ). - Executes TypeScript code (uses  or ). - Executes C# code (uses  with .NET 10+ single-file support).The built-in computer_use extension transforms AI agents into autonomous computer operators. Based on Anthropic's computer use tools, it enables agents to see your screen, control the mouse and keyboard, execute shell commands, and edit files - just like a human sitting at the computer.This unlocks powerful capabilities that traditional API-based tools cannot achieve:: Confirm that code actually renders correctly in a browser: Control any GUI application - web browsers, IDEs, terminals: Chain together multiple applications in a single task: Automate software that lacks APIsFor example, an agent can write a web application, open a browser, and capture a screenshot to prove it works:As some core tools are particularly useful on their own, dedicated UIs has been added for the  tool with support for evaluating mathematical python expressions, including arithmetic, comparison, boolean operators,  functions & constants and python list comprehensions - Clean, modern, responsive UI with dark mode support - Calculations automatically saved to localStorage and preserved between sessions - Click history items to instantly load expressions and copy to clipboard - Complete UI buttons for numbers, operators, constants, and math functions - Full access to Python's math library including trig, stats, and more - AST-based evaluator prevents arbitrary code execution for secure calculationsWhilst the  tools provides a scratch pad for running stand-alone Python, JavaScript, TypeScript, and C# code in a sandbox.The UI uses  as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.The UI uses  as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.As both dedicated UIs run the tools directly, they don't use AI or consume any tokensThe katex extension enables beautiful rendering of LaTeX math expressions in AI responses using KaTeX. It integrates automatically with the markdown parser to render math equations in both inline and block formats.: Uses KaTeX for high-performance rendering of math expressions.: Renders math within text using  or  delimiters.: Renders complex equations in their own block using  or  delimiters across multiple lines.: Automatically extends the  parser used in the application.Unlike text generation, there's no standard API for image generation across providers - each requires its own custom implementation. Despite the additional effort required, there's now seamless image generation support through both the UI and CLI with built-in integrations for:To begin select an image generation model from the Model Selector that supports image generation:When an image generation model is selected, the chat prompt will the option to specify which aspect ratio to use for the generated images:Generate images using the  modifier:Which uses the  chat template in  for its image generation request. Before returning, any assets are saved to cache and their local path and HTTP URL returned, e.g:Use any model that supports image generation by specifying its  or :ðŸ“ All generated images are saved to  using their SHA-256 hash as the filename.Audio generation is an emerging capability with limited provider support where Text-to-Speech generation through both the UI and CLI, currently only supports Google's latest TTS models:Gemini 2.5 Flash Preview TTSGemini 2.5 Pro Preview TTSTypically you'd select the audio generation model from the Model Selector to find models that supports audio generation:But despite models.dev listing them as capable of audio generation, only Gemini's TTS models are currently supported for audio generation through Gemini's API as Alibaba doesn't yet support the  modality.Available in both the UI and on the command-line using :Audio files are saved locally and accessible via HTTP URL:
Run server with  to play URL in your browser.The gallery extension intercepts all generated image, audio & file assets and uploaded files in  file storage whose metadata is maintained in a SQLite database at ~/.llms/user/default/gallery/gallery.sqliteDedicated UIs are available for quickly browsing and navigating or generated images / audio files including a lightbox previewer for full-size viewing:System prompts support was refactored into a replaceable system_prompts extension which configures AI requests with a library of  awesome curated system prompts that can be selected from the UI.You can maintain your own library of system prompts for all anonymous users at:
~/.llms/user/default/system-prompts.jsonOr for signed in users at:
~/.llms/user/<github-user>/system-prompts.jsonWith the JSON file simply containing an array of names and their system prompts, e.g:Browse the complete collection of available system prompts below:Another major change is the migration from client-side IndexedDB storage to a robust server-side SQLite databases. This architectural shift ensures better data consistency, improved performance that enables parallel executions and multi-device access to your chat history.To keep the database efficient and portable, binary assets (images, audio, etc.) are not stored directly in the SQLite database, Instead all generated assets are stored in the local file system cache at  and only  referencing these assets are stored in the database.To ensure data integrity and high performance without complex locking mechanisms, the system utilizes a  to  write operations to the database. This design improves concurrency handling and eliminates database locking issues during high-load scenarios.When authentication is enabled, data isolation is automatically enforced. All core tables, including  and , are scoped to the authenticated user, ensuring that users can only access their own data.A new caching system has been implemented for generated assets and uploaded images and files that's now persisted in , preserving them across messages and sessions.: Only cache references are stored with chat messages: Images remain accessible in previews and downloads after page reloads: System handles file storage and serving transparentlyNow that all persistence is server-side, to transfer or backup your configurations, extensions and Chat History you need only copy your  folder.All server extension features including tools, custom providers, database persistence, and image/audio generation are fully accessible via the command line, making llms.py a powerful terminal-based AI assistant.All registered tools are automatically available in CLI mode. Enable specific tools with the  flag:Generate images and audio directly from the command line:All generated media is automatically saved to  with metadata persisted in SQLite.All CLI interactions are automatically persisted to ~/.llms/user/app/app.sqlite, including:Chat threads and messagesGenerated assets and file referencesUser preferences and settingsEnsuring your conversation history is preserved and accessible from both CLI and Web UI.Launch the web UI while keeping full CLI access:Happy holidays from llms.py! ðŸŽ„With llms .py rebuilt from the ground up as an extensible platform, we hope to foster a thriving community extension ecosystem where developers can share innovative solutions and extend llms.py in ways we haven't yet imagined.As llms .py is still in active development, we welcome your feedback on any features that would better support 3rd party extensions and help cultivate this growing community.]]></content:encoded></item><item><title>Television is 100 years old today</title><link>https://diamondgeezer.blogspot.com/2026/01/tv100.html</link><author>qassiov</author><category>hn</category><pubDate>Mon, 26 Jan 2026 14:41:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[    
          Television is 100 years old today.
And it was born here, above an Italian cafe in Soho.

The man who first demonstrated television was John Logie Baird, a former engineering apprentice from Helensburgh. And although there are other places that can plausibly claim to be TV's birthplace, including a terraced street in Hastings, a hill in north London and Selfridges, most people agree that the decisive moment was a demonstration given to journalists in Frith Street on 26th January 1926.

Baird might never have made it to London had he not been a sickly boy. When WW1 broke out he wanted to enlist but was refused due to ill health, so took a job with the Clyde Valley Electrical Power Company helping to make munitions instead. In 1923 he moved to the south coast for the good of his health because it had a warmer climate, renting rooms at 21 Linton Crescent in Hastings. Here the first television signal transmitting equipment was constructed, with component parts including a hatbox, tea chest, darning needles and bicycle light lenses. The first image to be transmitted was the shadow of a St Johns Ambulance medal with a distinctive spiky outline, an item still on display at Hastings Museum. But his tinkering proved dangerous, and although a 1000-volt electric shock thankfully resulted in nothing worse than a burnt hand, his landlord duly asked him to vacate the premises.

Baird moved to London in November 1924 in the hope of showing off his burgeoning invention, setting up a workshop in the attic at 22 Frith Street. Amongst those who dropped by was Gordon Selfridge who invited Baird to give demonstrations of his device in the Palm Court during his store's upcoming Birthday Week celebrations. He gave three shows a day to long queues of spectators, each invited to peer down a funnel at outlines of shapes transmitted from a separate device a few yards away, including a paper mask which Baird would make 'wink' by covering the eyehole. At this stage Baird's 'Televisor' was still electro-mechanical, the images formed by spinning discs with doubled-up lenses and perforated rectangular holes. But spectators were impressed, and Baird earned a much-needed Â£60 to plough back into his enterprise.

By October 1925 Baird had honed his processes sufficiently to be able to transmit an image with gradations of light and shade. Initially he used a ventriloquist's dummy called Stooky Bill, this because it had greater contrast than a human face and also because it wouldn't be harmed by intense heat or possible exploding glass. Later, somewhat over-excitedly, he invited a 20 year-old office worker called William Taynton to come upstairs and become TV's first human subject. William wasn't keen but an appearance fee of half a crown persuaded him to pick through a jungle of wires, sit in front of blazing hot lamps and stick his tongue out, for just long enough that Baird exclaimed  "I've seen you, William, I've seen you. I've got television at last!" When the time came for a blue plaque to be unveiled outside 22 Frith Street in 1951, it was William they invited back to do the honours.

Then on 26th January 1926 came the first official demonstration to members of the press. Journalists and guests from the Royal Institution were invited into Baird's workshop in small groups and first shown the dummy on screen, then each other's faces transmitted from a separate room. Only one visitor got too close to the discs and ended up with a sliced beard. Most of those present weren't especially impressed and failed to realise the significance of what they'd just seen, but The Times followed up with a short article two days later.

These days 22 Frith Street is home to retro cafe Bar Italia. It's been owned and run by the Polledri family since 1949, a coffee-squirting dynasty who also run the Little Italy restaurant nextdoor. The stone floor was laid by their uncle Torino, a terrazzo mosaic specialist, and the counter was one of the first in London to be graced by an original Gaggia machine. Once a magnet for mods on scooters Bar Italia has attracted many famous names over the years, notably Rocky Marciano whose huge poster has pride of place behind the counter. You could thus celebrate today's centenary with an espresso and a slice of pizza in the photo-bedecked interior, or risk sitting outside below the neon sign with a froth and cheesecake combo.

Number 22 also displays a Milestone plaque erected by The Institution of Electrical Engineers citing "the world's first public demonstration of live television". Below is a much newer plaque citing this as an accredited World Origin Site. I first saw one of these inside the Alexander Fleming Laboratory Museum earlier in the month, earned for the discovery of penicillin, but whereas that was designated WOS 0001 the invention of television only ranks 0037. I believe they're unveiling it officially at 2pm this afternoon, even though it was perfectly visible over the weekend.

Baird was a highly driven inventor and entrepreneur and went on to develop prototypes for all sorts of forward-looking formats. In 1927 he came up with â€˜Phonovisionâ€™ (image recordings onto 78 rpm gramophone records) and â€˜Noctovisionâ€™ (infra-red TV). In 1928, amazingly, he demonstrated both colour television and stereoscopic (3D) television. His ultimate aim was television broadcasting via the BBC, beginning experimental transmissions of 30-line television in 1930 and delivering the first outside broadcast (from the Derby) in 1931, not that anyone was yet watching.

But in 1932 EMI started to provide serious competition, developing their own pioneering electronic television camera called the Emitron. The government's Television Advisory Committee ultimately recommended that both Baird's 240-line mechanical system and Marconi-EMI's 405-line electronic system be developed as alternatives for the proposed new London television station. And so it was that when broadcast TV first launched at Alexandra Palace on 2nd November 1936 the two systems alternated one week each... Baird second.

It rapidly became clear that the Marconi system was far superior and Baird's was dropped after just three months. Baird also suffered when his studios were burned in the fire that destroyed the Crystal Palace, and his company went into receivership when all TV broadcasting was suspended at the start of WW2. He carried on inventing at home in Sydenham, vastly improving his system for colour television, until his laboratory was made unusable by bomb damage. Alas ill health caught up with him and he died after a stroke at the age of 57, just one week after the BBC restarted television broadcasts in 1946. You can't see his final home in Bexhill because it was replaced by a block of flats in 2005, but Baird does have an impressive number of plaques across central London and SE26.

It's not always easy being first, and after early televisual success John Logie Baird saw his star wane and fade. But it's still him we remember for making possible one of the key transformative inventions of the 20th century, even though barely anyone watched his first efforts. It took ten years to get from Stooky Bill to BBC TV's opening night, then another two decades before the widespread adoption of TV sets in British households and two more until colour television took hold. But 100 years on almost all of us have a TV set at home and effectively another in our pocket, and all because a Scotsman came to London and cleverly spun some discs.]]></content:encoded></item><item><title>Google AI Overviews cite YouTube more than any medical site for health queries</title><link>https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study</link><author>bookofjoe</author><category>hn</category><pubDate>Mon, 26 Jan 2026 14:27:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Googleâ€™s search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that raises fresh questions about a tool seen by 2 billion people each month.The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are â€œreliableâ€ and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic.However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was YouTube. The video-sharing platform is the worldâ€™s second most visited website, after Google itself, and is owned by Google.Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said.â€œThis matters because YouTube is not a medical publisher,â€ the researchers wrote. â€œIt is a general-purpose video platform. Anyone can upload content there (eg board-certified physicians, hospital channels, but also wellness influencers, life coaches, and creators with no medical training at all).â€Google told the Guardian that AI Overviews was designed to surface high-quality content from reputable sources, regardless of format, and a variety of credible health authorities and licensed medical professionals created content on YouTube. The studyâ€™s findings could not be extrapolated to other regions as it was conducted using German-language queries in Germany, it said.The research comes after a Guardian investigation found people were being put at risk of harm by false and misleading health information in Google AI Overviews responses.In one case that experts said was â€œdangerousâ€ and â€œalarmingâ€, Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later removed AI Overviews for some but not all medical searches.The SE Ranking study analysed 50,807 healthcare-related prompts and keywords to see which sources AI Overviews relied on when generating answers.They chose Germany because its healthcare system is strictly regulated by a mix of German and EU directives, standards and safety regulations. â€œIf AI systems rely heavily on non-medical or non-authoritative sources even in such an environment, it suggests the issue may extend beyond any single country,â€ they wrote.AI Overviews surfaced on more than 82% of health searches, the researchers said. When they looked at which sources AI Overviews relied on most often for health-related answers, one result stood out immediately, they said. The single most cited domain was YouTube with 20,621 citations out of a total of 465,823.The next most cited source was NDR.de, with 14,158 citations (3.04%). The German public broadcaster produces health-related content alongside news, documentaries and entertainment. In third place was a medical reference site, Msdmanuals.com with 9,711 citations (2.08%).The fourth most cited source was Germanyâ€™s largest consumer health portal, Netdoktor.de, with 7,519 citations (1.61%). The fifth most cited source was a career platform for doctors, Praktischarzt.de, with 7,145 citations (1.53%).The researchers acknowledged limitations to their study. It was conducted as a one-time snapshot in December 2025, using German-language queries that reflected how users in Germany typically search for health information.Results could vary over time, by region, and by the phrasing of questions. However, even with those caveats, the findings still prompted alarm.Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: â€œThis study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases.â€œInstead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge.â€A Google spokesperson said: â€œThe implication that AI Overviews provide unreliable information is refuted by the reportâ€™s own data, which shows that the most cited domains in AI Overviews are reputable websites. And from what weâ€™ve seen in the published findings, AI Overviews cite expert YouTube content from hospitals and clinics.â€Google said the study showed that of the 25 most cited YouTube videos, 96% were from medical channels. However, the researchers cautioned that these videos represented fewer than 1% of all the YouTube links cited by AI Overviews on health.â€œMost of them (24 out of 25) come from medical-related channels like hospitals, clinics and health organisations,â€ the researchers wrote. â€œOn top of that, 21 of the 25 videos clearly note that the content was created by a licensed or trusted source.â€œSo at first glance it looks pretty reassuring. But itâ€™s important to remember that these 25 videos are just a tiny slice (less than 1% of all YouTube links AI Overviews actually cite). With the rest of the videos, the situation could be very different.â€]]></content:encoded></item><item><title>Apple introduces new AirTag with longer range and improved findability</title><link>https://www.apple.com/newsroom/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/</link><author>meetpateltech</author><category>hn</category><pubDate>Mon, 26 Jan 2026 14:10:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Porting 100k lines from TypeScript to Rust using Claude Code in a month</title><link>https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html</link><author>ibobev</author><category>hn</category><pubDate>Mon, 26 Jan 2026 13:58:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I read this post â€œOur strategy is to combine AI  Algorithms to rewrite Microsoftâ€™s largest codebases [from C++ to Rust]. Our North Star is â€˜1 engineer, 1 month, 1 million lines of code.â€ and it got me curious, how difficult is it really?I've long wanted to build a competitive Pokemon battle AI after watching a lot of WolfeyVGC and following the PokÃ©Agent challenge at NeurIPS. Thankfully there's an open source project called "Pokemon Showdown" that implements all the rules but it's written in JavaScript which is quite slow to run in a training loop. So my holiday project came to life: let's convert it to Rust using Claude!Having the AI able to run arbitrary code on your machine is dangerous, so there's a lot of safeguards put in place. But... at the same time, this is what I want to do in this case. So let me walk through the ways I escaped the various sandboxes.Claude runs in a sandbox that limits some operations like ssh access. You need ssh access in order to publish to GitHub. This is very important as I want to be able to check how the AI is doing from my phone while I do some other activities ðŸ˜‰What I realized is that I can run the code on my terminal but Claude cannot do it from its own terminal. So what I did was to ask Claude to write a nodejs script that opens an http server on a local port that executes the git commands from the url. Now I just need to keep a tab open on my terminal with this server active and ask Claude to write instructions in Claude.md for it to interact with it.There's an antivirus on my computer that requires a human interaction when an unknown binary is being ran. Since every time we compile it's a new unknown binary, this wasn't going to work.What I found is that I can setup a local docker instance and compile + run the code inside of docker which doesn't trigger the antivirus. Again, I asked Claude to generate the right instructions in Claude.md and problem solved.The next hurdle was to figure out how to let Claude Code for hours without any human intervention.Claude keeps asking for permission to do things. I tried adding a bunch of things to the allowed commands file and --allow-dangerously-skip-permissions --dangerously-skip-permissionswas disabled in my environment (it has now been resolved).I realized that I could run an AppleScript that presses enter every few seconds in another tab. This way it's going to say Yes to everything Claude asks to do. So far it hasn't decided to hack my computer...#!/bin/bash

osascript -e \
'tell application "System Events"
    repeat
        delay 5
        key code 36
    end repeat
end tell'Claude after working for some time seem to always stop to recap things. I tried prompting it to never do, even threatening it to no avail.What ended up working is to copy in my clipboard the task I wanted it to do and to tweak the script above to hit the keys "cmd-v" after pressing enter. This way in case it asks a question the "enter" is being used and in case it's not it's queuing the prompt for when Claude is giving back control.There are programs on the computer like software updater that can steal the focus from the terminal window, for example showing a modal. Once that happens, then the cmd-v / enter are no longer sent to the terminal and the execution stops.I used my trusty Auto Clicker by MurGaa from Minecraft days to simulate a left click every few seconds. I place my terminal on the edge of the screen and same for my mouse so that when a modal appears in the middle, it refocuses the terminal correctly.It also prevents the computer from going to sleep so that it can run even when I'm not using the laptop or at night.Reliability when running things for a long period of time is paramount. Overall it's been a pretty smooth ride but I ran into this specific error during a handful of nights which stopped the process. I hope they get to the bottom of it and solve it as I'm not the only one to report it!This setup is far from optimal but has worked so far. Hopefully this gets streamlined in the future!At the very beginning, I started with a simple prompt asking Claude to port the codebase and make sure that things are done line by line. At first it felt extremely impressive, it generated thousands of lines of Rust that was compiling.Sadly it was only an appearance as it took a lot of shortcuts. For example, it created two different structures for what a move is in two different files so that they would both compile independently but didn't work when integrated together. It ported all the functions very loosely where anything that was remotely complicated would not be ported but instead "simplified".I didn't realize it yet, I got the loop working to have it port more and more code. The issue is that it created wrong abstractions all over the place and kept adding hardcoded code to make whatever it was supposed to fix work. This wasn't going to go anywhere.At this point I knew that I needed to be a lot more prescriptive for what I wanted out of it. Taking a step back, the end result should have every JavaScript file and every method inside to have a Rust equivalent.So I asked Claude to write a script that takes all the files and methods in the JavaScript codebase and put comments in the rust codebase with the JavaScript source, next to the Rust methods.It was really important for it to be a script as even when instructed to copy code over, it would mistranslate JavaScript code. Being deterministic here greatly increased the odds of getting the right results.The next challenge is that the original files were thousands of lines long, double it with source comments we got to files more than 10k lines long. This causes a ton of issues with the context window where Claude straight up refuses to open the file. So it started reading the file in chunks but without a ton of precision. Also the context grew a lot quicker and compaction became way more frequent.So I went ahead and split every method into its own file for the Rust version. This dramatically improved the results. For maximal efficiency I would need to do the same for the JavaScript codebase as well but I was too afraid to do it and accidentally change the behavior so decided not to.The process of porting went through two repeating phases. I would give a large task to Claude to do in a loop that would churn on it for a day, and then I would need to spend time cleaning up the places where it went into the wrong direction.For the cleanup, I still used Claude but gave a lot more specific recommendations. For example, I noticed that it would hardcode moves/abilities/items/... behaviors everywhere in the code when left unchecked, even after explicitly telling it not to. So I would manually look for all these and tell it to move them into the right places.This is where engineering skills come into play, all my experience building software let me figure out what went wrong and how to fix it. The good part is that I didn't have to do the cleanup myself, Claude was able to do it just fine when directed to.Build everything before testingSo far, I just made sure that the code compiled, but have never actually put all the pieces together to ensure it actually worked. What Claude really wanted was to do a traditional software building strategy where you make "simple" implementations of all of the pieces and then build them up as time goes.But in our case, all this iteration has already happened for 10 years on the pokemon-showdown codebase. It's counter productive to try and re-learn all these lessons and will unlikely converge the same way. What works better is to port everything at once, and then do the integration at the end once.I've learned this strategy from working on Skip, a compiler. For years all the building blocks were built independently and then it all came together with nothing to show for but within a month at the end it all worked. I was so shocked.Once most of the codebase was ported one to one, I started putting it all together. The good thing is that we can run and edit the code in JavaScript and in Rust, and the input/output is very simple and standardized: list of pokemons with their options (moves, items, nature, iv/ev spread...) and then the list of actions at each step (moves and switches). Given the same random sequence, it'll advance the state the same way.Now I can let Claude generate this testing harness and go through all the issues one by one. Impressively, it was able to figure out all issues and fix them.Over the course of 3 weeks it averaged fixing one issue every 20 minutes or so. It fixed hundreds of issues on its own. I never intervened, it was only a matter of time before it fixed every issue that it encountered.At the beginning, this process was extremely slow. Every time a compaction happened, Claude became "dumb" again and reinvented the wheel, writing down tons of markdown files and test scripts along the way. Or Claude decided to take the easy way out and just generate tons of tests but never actually making them match with JavaScript.So, I started looking at what it did well and encoding it. For example, it added a lot of debugging around the PRNG steps and what actions happened at every turn with all the debugging metadata. So I asked it to create a single test script to print down this information for a single step and to print stack traces. Then add instruction to the Claude.md file. This way every investigation started right away.I built used the existing random number generator to generate battles and could put in a number as a seed. This let me generate consistent battles at an increasing size.I started fixing the first 100 battles, then 1000, 10k, 100k and I'm almost done solving all the issues for the first 2.4 million battles! I'm not sure how many more issues there are but the good thing is that they are getting smaller and smaller as the batch size increases.There are two broad classes of issues that were fixed. The first one that I expected is that Rust has different constraints than JavaScript which need to be taken into account and lead to bugs:Rust has the "borrow checker" where a mutable variable cannot be passed in two different contexts at once. The problem is that "Pokemon" and "Battle" have references to each others. So there's a lot of workarounds like doing copies, passing indices instead of the object, providing functions with mutable object as callback...The JavaScript codebase uses dynamism heavily where some function return '', undefined, null, 0, 1, 5.2, Pokemon... which all are handled with different behaviors. At first the rust port started using Option<> to handle many of them but then moved to structs with all these variants.Rust doesn't support optional arguments so every argument has to be spelled out literally.But the second one are due to itself... Claude Code is like a smart student that is trying to find every opportunity to avoid doing the hard work and take the easy way out if it thinks it can get away with it.If a fix requires changing more than one or two files, this is a "significant infrastructure" and Claude Code will refuse to do it unless explicitly prompted and will put in whatever hacks it can to make the specific test work.Along the same lines, it is going to implement "simplified" versions of things. For some methods, it was better to delete everything and asking it to port it over from scratch than trying to fix all the existing code it created.The JavaScript comments are supposed to be the source of truth. But Claude is not above changing the original code if it feels like this is the way to solve the problem...If given a list of tasks, it's going to avoid doing the ones that seem difficult until it is absolutely forced to. This is inefficient if not careful as it's going to keep spending time investigating and then skipping all the "hard" ones. Compaction is basically wiping all its memory.I didn't write a single line of code myself in this project. I alternated between "co-op" where I work with Claude interactively during the day and creating a job for it to run overnight. I'll focus on the night ones for this section.For the first phase of the project, I mostly used variations of this one. Asking it to go through all the big files one by one and implement them faithfully (it didn't really follow instructions as we've seen later...)Open BATTLE_TODO.md to get the list of all the methods in both battle*.rs.Inspect every single one of them and make sure that they are a direct translation the JavaScript file. If there's a method with the same name, the JavaScript definition will be in the comment.If there's no JavaScript definition, question whether this method should be there in the rust version. Our goal is to follow as closely as possible the JavaScript version to avoid any bugs in translation. If you notice that the implementation doesn't match, do all the refactoring needed to match 1 to 1.This will be a complex project. You need to go through all the methods one by one, IN ORDER. YOU CANNOT skip a method because it is too hard or would requiring building new infrastructure. We will call this in a loop so spend as much time as you need building the proper infrastructure to make it 1 to 1 with the JavaScript equivalent. Do not give up.Update BATTLE_TODO.md and do a git commit after each unit of work.Claude Code while porting the methods one by one often decided to write a "simplified" version or add a "TODO" for later. I also found it to be useful when generating work to add the instructions in the codebase itself via a TODO comment, so I don't need to wish that it's going to be read from the context.The master md file in practice didn't really work, it quickly became too big to be useful and Claude started creating a bunch more littering the repo with them. Instead I gave it a deterministic way to go through then by calling grep on the codebase, so it knew when to find them.We want to fix every TODO in the codebase.  or  in pokemon-showdown-rs/.There are hundreds of them, so go diligently one by one. Do not skip them even if they are difficult. I will call this prompt again and again so you don't need to worry about taking too long on any single one.The port must be exactly one to one. If the infrastructure doesn't exist, please implement it. Do not invent anything.Make sure it still compiles after each addition and commit and push to git.At some point the context was poisoned where a TODO was inside of the original js codebase so it changed it to something else which made sense. But then it did the same for all the subsequent TODOs which didn't... Thankfully I could just revert all these commits.I put in all the instructions to debug in Claude.md and a script to run all the tests which outputs a txt file with progress report. This way Claude was able to just keep going fixing issues after issues.We want to fix all the divergences in battles. Please look at 500-seeds-results.txt and fix them one by one. The only way you can fix is by making sure that the differences between javascript and rust are explained by language differences and not logic. Every line between the two must match one by one. If you fixed something specific, it's probably a larger issue, spend the time to figure out if other similar things are broken and do the work to do the larger infrastructure fixes. Make sure it still compiles after each addition and commit and push to git. Check if there are other parts of the codebase that make this mistake.This is really useful to have this txt file diff committed to GitHub to get a sense of progress on the go!I didn't quite know what to expect coming into this project. They usually tend to die due to the sheer amount of work needed to get anywhere close to something complete. But not this time!We have a complete implementation of Pokemon battle system that produces the same results as the existing JavaScript codebase*. This was done through 5000 commits in 4 weeks and the Rust codebase is around 100k lines of code.*I wish we had 0 divergences but right now there are 80 out of the first 2.4 million seeds or 0.003%. I need to run it for longer to solve these.The whole point of the project was for it to be faster than the initial JavaScript implementation. Only towards the end of the project where we had a sizable amount of battles running perfectly I felt like it would be a fair time to do a performance comparison.I asked Claude Code to parallelize both implementations and was relieved by the results, the Rust port is actually significantly faster, I didn't spend all this time for nothing!I've tried asking Claude to optimize it further, it created a plan that looks reasonable (I've never interacted with Rust in my life) and it spent a day building many of these optimizations but at the end of the day, none of them actually improved the runtime and some even made it way worse.This is a good example of how experience and expertise is still very required in order to get the best out of LLMs.This is pretty wild that I was able to port a ~100k lines codebase from JavaScript to Rust in two weeks on my own with Claude Code running 24 hours a day for a month creating 5k commits! I have never written any line of Rust before in my life.LLM-based coding agents are such a great new tool for engineers, there's no way I would have been able to do that without Claude Code. That said, it still feels like a tool that requires my engineering expertise and constant babysitting to produce these results.Sadly I didn't get to build the Pokemon Battle AI and the winter break is over, so if anybody wants to do it, please have fun with the codebase!]]></content:encoded></item><item><title>After two years of vibecoding, I&apos;m back to writing by hand</title><link>https://atmoio.substack.com/p/after-two-years-of-vibecoding-im</link><author>mobitar</author><category>hn</category><pubDate>Mon, 26 Jan 2026 13:36:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Vibe coding kills open source</title><link>https://arxiv.org/abs/2601.15494</link><author>kgwgk</author><category>hn</category><pubDate>Mon, 26 Jan 2026 13:01:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TSMC Risk</title><link>https://stratechery.com/2026/tsmc-risk/</link><author>swolpers</author><category>hn</category><pubDate>Mon, 26 Jan 2026 11:07:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[You probably think, given this title, you know what this Article is about. The most advanced semiconductors are made by TSMC in Taiwan, and Taiwan is claimed by China, which has not and will not take reunification-by-force off of the table.Relatedly, AI obviously has significant national security implications; at Davos, Anthropic CEO Dario Amodei reiterated his objection to the U.S. allowing the sale of Nvidia chips to China. From Bloomberg:Anthropic Chief Executive Officer Dario Amodei said selling advanced artificial intelligence chips to China is a blunder with â€œincredible national security implicationsâ€ as the US moves to allow Nvidia Corp. to sell its H200 processors to Beijing. â€œIt would be a big mistake to ship these chips,â€ Amodei said in an interview with Bloomberg Editor-in-Chief John Micklethwait at the World Economic Forum in Davos, Switzerland. â€œI think this is crazy. Itâ€™s a bit like selling nuclear weapons to North Korea.â€The nuclear weapon analogy is an interesting one: a lot of game theory was developed to manage the risk of nuclear weapons, particularly once the U.S.S.R. gained/stole nuclear capability, ending the U.S.â€™s brief monopoly on the technology. Before that happened, however, the U.S. had a dominant military position, given we had nuclear weapons and no one else did. Perhaps Amodei believes the U.S. should have advanced AI and China should not, giving us a dominant military position?The problem with that reality, however, is Taiwan, as I explained in AI Promise and Chip Precariousness. AI, in contrast to nuclear weapons, has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion; if we got to a situation where only the U.S. had the sort of AI that would give us an unassailable advantage militarily, then the optimal strategy for China would change to taking TSMC off of the board.Given this dependency, my recommendations in the Article run counter to Amodei: I want China dependent on not just U.S. chips but also on TSMC directly, which is why I argued in favor of selling Nvidia chips to China, and further believe that Huawei and other Chinese companies ought to be able to source from TSMC (on the flip side, I would ban the sale of semiconductor manufacturing equipment to Chinese fabs). I think itâ€™s a good thing the Trump administration moved on the first point, at least.However, this risk is not what this Article is about: there is another TSMC risk facing the entire AI industry in particular; moreover, itâ€™s a risk the downside of which is already being realized.There was one refrain that was common across Big Tech earnings last quarter: demand for AI exceeds supply. Here was Amazon CEO Andy Jassy on the companyâ€™s earnings call:Youâ€™re going to see us continue to be very aggressive investing in capacity because we see the demand. As fast as weâ€™re adding capacity right now, weâ€™re monetizing it.Azure AI services revenue was generally in line with expectations, and this quarter, demand again exceeded supply across workloads, even as we brought more capacity online.In GCP, we see strong demand for enterprise AI infrastructure, including TPUs and GPUs, enterprise AI solutions driven by demand for Gemini 2.5 and our other AI models, and core GCP infrastructure and other services such as cybersecurity and data analytics. As Iâ€™ve mentioned on previous earnings calls, while we have been working hard to increase capacity and have improved the pace of server deployments and data center construction, we still expect to remain in a tight demand-supply environment in Q4 and 2026.To date, we keep on seeing this pattern where we build some amount of infrastructure to what we think is an aggressive assumption. And then we keep on having more demand to be able to use more compute, especially in the core business in ways that we think would be quite profitable than we end up having compute for.Talking about to build a lot of AI data center all over the world, I use one of my customersâ€™ customersâ€™ answer. I asked the same question. They told me that they planned this one, 5-6 years ago already. So, as I said, those cloud service providers are smart, very smart. So, they say that they work on the power supply 5-6 years ago. So, today, their message to me is: silicon from TSMC is a bottleneck, and asked me not to pay attention to all others, because they have to solve the silicon bottleneck first. But indeed, we do get the power supply, all over the world, especially in the US. Not only that, but we also look at, who support those kind of a power supply, like a turbine, like, what, nuclear power plant, the plan or those kinds of things. We also look at the supply of the rack. We also look at the supply of the cooling system. Everything, so far, so good. So we have to work hard to narrow the gap between the demand and supply from TSMC.The cause of that gap is obvious if you look at TSMCâ€™s financials, specifically the companyâ€™s annual capital expenditures:After a big increase in CapEx in 2021, driven by the COVID shortages and a belief in 5G, TSMCâ€™s annual CapEx in the following years was basically flat â€” it actually declined on a year-over-year basis in both 2023 and 2024. Note those dates! ChatGPT was released in November 2022; that kicked off a massive increase in CapEx amongst the hyperscalers in particular, but it sure seems like TSMC didnâ€™t buy the hype.That lack of increased investment earlier this decade is why there is a shortage today, and is why TSMC has been a de facto brake on the AI buildout/bubble; I wrote last quarter:To put it another way, if Altman and OpenAI are the ones pushing to accelerate the AI infrastructure buildout, itâ€™s Wei and TSMC that are the brakes. The extent to which all of Altmanâ€™s deals actually materialize is dependent on how much TSMC invests in capacity now, and while they havenâ€™t shown their hand yet, the company is saying all of the right things about AI being a huge trend without having yet committed to a commensurate level of investment, at least relative to OpenAIâ€™s goals.That Update was about the future, but itâ€™s important to note that the TSMC brake has â€”Â if all of those CEO and CFO comments above are to be believed â€” already cost the biggest tech companies a lot of money. Thatâ€™s the implication of not having enough supply to satisfy demand: there was revenue to be made that wasnâ€™t, because TSMC didnâ€™t buy the AI hype at the same time everyone else did.TSMC is, finally, starting to invest more. Last yearâ€™s CapEx increased 37% to $41 billion, and thereâ€™s another increase in store for this year to $52â€“$56 billion; if we take the midpoint, that represents an increase of 32%, a bit less than last year:Make no mistake, $54 billion is a big number, one that Wei admitted made him nervous:You essentially try to ask whether the AI demand is real or not. Iâ€™m also very nervous about it. Yeah, you bet, because we have to invest about USD52 billion to USD56 billion for the CapEx, right? If we did not do it carefully, that will be a big disaster to TSMC for sure. So, of course, I spent a lot of time in the last three-four months talking to my customers and then customersâ€™ customers. I want to make sure that my customersâ€™ demands are real.Wei made clear that he was worried about the market several years down the line:If you build a new fab, it takes two and three year, two to three years to build a new fab. So even we start to spend $52 billion to $56 billion, the contribution to this year is almost none, and 2027, a little bit. So we actually, we are looking for 2028-2029 supply, and we hope itâ€™s a time that the gap will be narrowâ€¦So 2026-2027 for the short-term, we are looking to improve our productivity. 2028 to 2029, yes, we start to increase our capacity significantly. And it will continue this way if the AI demand megatrend as we expected.First off, this delayed impact explains why TSMCâ€™s lack of CapEx increase a few years ago is resulting in supply-demand imbalance today. Secondly, notice how this yearâ€™s planned increase â€” which again, wonâ€™t really have an impact until 2028 â€” pales in comparison to the CapEx growth of the hyperscalers (2025 numbers are estimates; note that Amazonâ€™s CapEx includes Amazon.com):Remember, a significant portion of this CapEx growth is for chips that are supported by TSMCâ€™s stagnant CapEx growth from a few years ago. Itâ€™s notable, then, that TSMCâ€™s current and projected CapEx growth is still less than the hyperscalers: how much less is it going to be than the hyperscalersâ€™ growth in 2028, when the fabs being built today start actually producing chips?In short, the TSMC brake isnâ€™t going anywhere â€” if anything, itâ€™s being pressed harder than ever.TSMC is, to be clear, being extremely rational. CapEx is inherently risky: you are spending money now in anticipation of demand that may or may not materialize. Moreover, the risk for a foundry is higher than basically any other business model: nearly all of a foundryâ€™s costs are CapEx, which means that if demand fails to materialize, costs â€” in the form of depreciation â€” donâ€™t go down as they might with a business model with a higher percentage of marginal costs. This is exacerbated by the huge dollar figures entailed in building fabs: $52â€“$56 billion may drive revenues with big margins, but those big margins can easily flip to being huge losses and years of diminished pricing power thanks to excess capacity. Therefore, itâ€™s understandable that TSMC is trying to manage its risks. Sure, the company may be foregoing some upside in 2028, but what is top of Weiâ€™s mind is avoiding â€œa big disaster.â€What is important to note, however, is that the risk TSMC is managing doesnâ€™t simply go away: rather, itâ€™s being offloaded to the hyperscalers in particular. Specifically, if we get to 2028, and TSMC still isnâ€™t producing enough chips to satisfy demand, then that means the hyperscalers will be forgoing billions of dollars in revenue â€”Â even more than they are already forgoing today. Yes, that risk is harder to see than the risk TSMC is avoiding, because the hyperscalers arenâ€™t going to be bankrupt for a lack of chips to satisfy demand. Still, the potential money not made â€” particularly when the number is potentially in the hundreds of billions of dollars â€” is very much a risk that the hyperscalers are incurring because of TSMCâ€™s conservatism.What the hyperscalers need to understand is that simply begging TSMC to make more isnâ€™t going to fix this problem, because begging TSMC to make more is to basically ask TSMC to take back the risk TSMC is offloading to the hyperscalers â€” they already declined! Rather, the only thing that will truly motivate TSMC to take on more risk is competition. If TSMC were worried about not just forgoing its own extra revenue, but actually losing business to a competitor, then the company would invest more. Moreover, that extra investment would be stacked on top of the investment made by said competitor, which means the world would suddenly have dramatically more fab capacity.In short, the only way to truly get an AI bubble, with all of the potential benefits that entails, or, in the optimistic case, to actually meet demand in 2028 and beyond, is to have competition in the foundry space. That, by extension, means Samsung or Intel â€” or both â€”Â actually being viable options.Remember, however, the number one challenge facing those foundries: a lack of demand from the exact companies whom TSMC has deputized to take on their risk. I wrote in U.S. Intel:Our mythical startup, however, doesnâ€™t exist in a vacuum: it exists in the same world as TSMC, the company who has defined the modern pure play foundry. TSMC has put in the years, and theyâ€™ve put in the money; TSMC has the unparalleled customer service approach that created the entire fabless chip industry; and, critically, TSMC, just as they did in the mobile era, is aggressively investing to meet the AI moment. If youâ€™re an Nvidia, or an Apple in smartphones, or an AMD or a Qualcomm, why would you take the chance of fabricating your chips anywhere else? Sure, TSMC is raising prices in the face of massive demand, but the overall cost of a chip in a system is still quite small; is it worth risking your entire business to save a few dollars for worse performance with a worse customer experience that costs you time to market and potentially catastrophic product failures?We know our mythical startup would face these challenges because they are the exact challenges Intel faces. Intel may need â€œa meaningful external customer to drive acceptable returns on [its] deployed capitalâ€, but Intelâ€™s needs do not drive the decision-making of those external customers, despite the fact that Intel, while not fully caught up to TSMC, is at least in the ballpark, something no startup could hope to achieve for decades.Becoming a meaningful customer of Samsung or Intel is very risky: it takes years to get a chip working on a new process, which hardly seems worth it if that process might not be as good, and if the company offering the process definitely isnâ€™t as customer service-centric as TSMC. I understand why everyone sticks with TSMC.The reality that hyperscalers and fabless chip companies need to wake up to, however, is that avoiding the risk of working with someone other than TSMC incurs new risks that are both harder to see and also much more substantial. Except again, we can see the harms already: foregone revenue today as demand outstrips supply. Todayâ€™s shortages, however, may prove to be peanuts: if AI has the potential these companies claim it does, future foregone revenue at the end of the decade is going to cost exponentially more â€” surely a lot more than whatever expense is necessary to make Samsung and/or Intel into viable competitors for TSMC.This, incidentally, is how the geographic risk issue will be fixed, if it ever is. Itâ€™s hard to get companies to pay for insurance for geopolitical risks that may never materialize. What is much more likely is that TSMCâ€™s customers realize that their biggest risk isnâ€™t that TSMC gets blown up by China, but that TSMCâ€™s monopoly and reasonable reluctance to risk a rate of investment that matches the rest of the industry means that the rest of the industry fails to fully capture the value of AI.]]></content:encoded></item><item><title>Show HN: Only 1 LLM can fly a drone</title><link>https://github.com/kxzk/snapbench</link><author>beigebrucewayne</author><category>dev</category><category>hn</category><pubDate>Mon, 26 Jan 2026 11:00:44 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MapLibre Tile: a modern and efficient vector tile format</title><link>https://maplibre.org/news/2026-01-23-mlt-release/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 26 Jan 2026 10:19:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today we are happy to announce  (MLT), a new modern and efficient vector tile format.MapLibre Tile (MLT) is a succesor to Mapbox Vector Tile (MVT).
It has been redesigned from the ground up to address the challenges of rapidly growing geospatial data volumes
and complex next-generation geospatial source formats, as well as to leverage the capabilities of modern hardware and APIs.MLT is specifically designed for modern and next-generation graphics APIs to enable high-performance processing and rendering of
large (planet-scale) 2D and 2.5 basemaps. This current implementation offers feature parity with MVT while delivering on the following:Improved compression ratio: up to 6x on large tiles, based on a column-oriented layout with recursively applied (custom)
lightweight encodings. This leads to reduced latency, storage, and egress costs and, in particular, improved cache utilization.Better decoding performance: fast, lightweight encodings that can be used in combination with SIMD/vectorization instructions.In addition, MLT was designed to support the following use cases in the future:Improved support for 3D coordinates, i.e. elevation.Improved processing performance, based on storage and in-memory formats that are specifically designed for modern graphics APIs,
allowing for efficient processing on both CPU and GPU. The formats are designed to be loaded into GPU buffers with little or no additional processing.Support for linear referencing and m-values to efficiently support the upcoming next-generation source formats such as Overture Maps (GeoParquet)., including nested properties, lists and maps.As with any MapLibre project, the future of MLT is decided by the needs of the community. There are a lot of exciting ideas for other future extensions and we welcome contributions to the project.For the adventurous, the answer is: . Both MapLibre GL JS and MapLibre Native now support MLT sources. You can use the new  property on sources in your style JSON with a value of  for MLT vector tile sources.To try out MLT, you have the following options:You can also try out the encoding server that converts existing (MVT-based) styles and vector tile sources to MLT on the fly. This is mostly a tool for development.To create tiles for production, you could use Planetiler, as the upcoming version will support generating MLTs.Refer to this page for a complete and up-to-date list of integrations and implementations. If you are an integrator working on supporting MLT, feel free to add your own project there.We would love to hear your experience with using MLT! Join the  channel on our Slack or create an Issue or Discussion on the tile spec repo.MapLibre Tile came to be thanks to a multi-year collaboration between academia, open source and enterprise. Thank you to everyone who was involved! We are very proud that our community can innovate like this.Special thanks go to Markus Tremmel for inventing the format, Yuri Astrakhan for spearheading the project, Tim Sylvester for the C++ implementation, Harel Mazor, Benedikt Vogl and Niklas Greindl for working on the JavaScript implementation.Also thanks to Microsoft and AWS for financing work on MLT.]]></content:encoded></item><item><title>San Francisco Graffiti</title><link>https://walzr.com/sf-graffiti</link><author>walz</author><category>hn</category><pubDate>Mon, 26 Jan 2026 10:02:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apple, What Have You Done?</title><link>https://onlinegoddess.net/2026/01/apple-what-have-you-done/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 26 Jan 2026 09:43:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>UK House of Lords Votes to Extend Age Verification to VPNs</title><link>https://reclaimthenet.org/uk-house-of-lords-votes-to-extend-age-verification-to-vpns</link><author>ubercow13</author><category>hn</category><pubDate>Mon, 26 Jan 2026 09:35:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The decision deepens the reach of the already-controversial Online Safety Act, linking child safety goals to mechanisms that could have severe effects on private communication and digital autonomy.Under the existing Online Safety Act framework, â€œuser-to-user servicesâ€ include almost any online platform that enables individuals to post, share, or interact with content from others.This definition covers social networks, messaging apps, forums, and online gaming services. Only a few forms of communication, such as email, SMS, MMS, and one-to-one live voice calls, are explicitly excluded.While political messaging around the vote often described the move as a â€œsocial media ban for under-16s,â€ the actual scope is considerably wider.In effect, most interactive online platforms would now need to collect and verify age data from users, even where those services are not primarily aimed at children.This represents a major expansion of identity checks across digital infrastructure, once considered neutral or privacy-protective, and one of the most disciplinarian proposals in the West.Two key amendments advanced during the Lords debate on January 21.Amendment 92 (â€œAction to Prohibit the Provision of VPN Services to Children in the United Kingdomâ€) requires VPNs that are â€œoffered or marketed to persons in the United Kingdomâ€ or â€œprovided to a significant number of personsâ€ to implement age assurance for UK users.The measure passed by 207 Content votes to 159 Not Content votes.Amendment 94a (â€œAction to Promote the Wellbeing of Children in Relation to Social Mediaâ€) mandates that all regulated user-to-user services introduce age assurance systems to prevent under-16s from â€œbecoming or being users.â€ This proposal passed with 261 Content votes to 150 Not Content votes.Both amendments will proceed to the Billâ€™s next stage, the third reading in the House of Lords.Two other amendments, both more technologically intrusive, were discussed but rejected.Amendment 93, introduced by Lord Nash, would have compelled smartphone and tablet manufacturers, distributors, and importers to install â€œtamper-proof system software which is highly effective at preventing the recording, transmitting (by any means, including livestreaming) and viewing of CSAM using that device.â€The only plausible way to enforce such a measure would be through constant, automated inspection of every photo, video, and stream on a device. This form of surveillance would have converted personal devices into continuous content monitors, raising severe privacy and accuracy concerns, including potential false positives.Lord Nash stated: â€œOn Amendment 93, I have had a constructive discussion with Ministers on this issue and more discussions are in progress, so I will not push that to a vote today.â€Amendment 108, proposed by Lord Storey, would have required user-to-user services â€œlikely to be accessed by childrenâ€ to set their own minimum age thresholds and use age assurance to enforce them.He argued that a single blanket ban under Amendment 94a was overly rigid. â€œHaving different minimum ages for different platforms would be a better solution,â€ he said, maintaining that his version would be more effective in practice.Neither of these amendments passed, leaving Amendments 92 and 94a as the only ones to advance.The discussion highlights a deepening push within UK legislation to merge digital identity checks with online participation.While described as safeguarding children, the changes embed a new layer of identity verification across tools once used for privacy, such as VPNs.These services, designed to conceal personal browsing data and protect against profiling, would now face obligations to verify who their users are. This is a contradiction that could erode one of the few remaining shields for private internet use.For now, the most invasive surveillance measure, client-side scanning, has been set aside. However, the fact that it was seriously considered indicates continuing interest in embedding scanning mechanisms directly into personal devices.Whether similar proposals reappear during the third reading remains to be seen.]]></content:encoded></item><item><title>The Holy Grail of Linux Binary Compatibility: Musl and Dlopen</title><link>https://github.com/quaadgras/graphics.gd/discussions/242</link><author>Splizard</author><category>hn</category><pubDate>Mon, 26 Jan 2026 07:41:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The browser is the sandbox</title><link>https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/</link><author>enos_feedler</author><category>hn</category><pubDate>Mon, 26 Jan 2026 05:23:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called Co-do that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the File System Access API (still Chrome-only as far as I can tell), CSP headers with  and WebAssembly in Web Workers.Co-do is a very interesting demo that illustrates all of these ideas in a single application:You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to Claude Cowork but without running a multi-GB local container to provide the sandbox.My biggest complaint about  remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex double-iframe technique to help apply network rules to the inner of the two frames.Thanks to this post I also learned about the <input type="file" webkitdirectory> tag which turns out to work on Firefox, Safari  Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a webkitdirectory demo to try it out and I'll certainly be using it for projects in the future.Posted 25th January 2026 at 11:51 pm]]></content:encoded></item><item><title>Iran&apos;s internet blackout may become permanent, with access for elites only</title><link>https://restofworld.org/2026/iran-blackout-tiered-internet/</link><author>siev</author><category>hn</category><pubDate>Mon, 26 Jan 2026 04:18:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Iranâ€™s near-total communications blackout has entered its 16th day, but thatâ€™s just a live test.Following a repressive crackdown on protests, the government is now building a system that grants web access only to security-vetted elites, while locking 90 million citizens inside an intranet.Government spokesperson Fatemeh Mohajerani confirmed international access will not be restored until at least late March. Filterwatch, which monitors Iranian internet censorship from Texas, cited government sources, including Mohajerani, saying access will â€œnever return to its previous form.â€This is what makes Iranâ€™s attempt unique: Other authoritarian states built walls before their populations went online. Iran is trying to seal off a connected economy already in freefall.Â The system is called Barracks Internet, according to confidential planning documents obtained by Filterwatch. Under this architecture, access to the global web will be granted only through a strict security whitelist.â€œThe regime is terrified of one thing: Iranians being heard telling their own truth and having crimes documented,â€ Mahsa Alimardani, a digital rights researcher at U.S.-based Witness, which trains activists to use video for advocacy, told . â€œThe question becomes: How do we give Iranians an unbreakable voice?â€The idea of tiered internet access is not new in Iran. Since at least 2013, the regime has quietly issued â€œwhite SIM cards,â€ giving unrestricted global internet access to approximately 16,000 people. The system gained public attention in November 2025 when Xâ€™s location feature revealed that certain accounts, including the communications minister, were connecting directly from inside Iran, despite X being blocked since 2009.What is different now is scale and permanence. The current blackout tests infrastructure designed to make two-tier access the default, not a temporary crackdown.Only a handful of nations have attempted to wall off their citizens from the global internet. North Koreaâ€™s Kwangmyong intranet was built from scratch for a population that never had connectivity. China constructed its Great Firewall over two decades while nurturing domestic alternatives such as WeChat and Alibaba. Iran is attempting to do both in weeks, with no domestic alternatives.The question becomes: How do we give Iranians an unbreakable voice?â€The economic costs of the blackout are staggering. Iranâ€™s deputy communications minister pegged the daily losses at as much as $4.3 million. NetBlocks estimates the true cost exceeds $37 million daily. More than 10 million Iranians depend directly on digital platforms for their livelihoods.Tipax, one of Iranâ€™s largest private delivery companies handling about 320,000 daily shipments before the protests, now processes fewer than a few hundred, according to Filterwatch. The company operates a nationwide logistics network comparable to FedEx in the U.S. market.The government fired Irancellâ€™s CEO for failing to comply with shutdown orders. Irancell, the countryâ€™s second-largest mobile operator with 66 million subscribers, is partly owned by South Africaâ€™s MTN Group. Alireza Rafiei was removed for disobeying orders on â€œrestriction of internet access in crisis situations,â€ according to Fars news agency.Foreign telecom partners have left Iran in recent days under security escort, without media coverage, according to Filterwatch. This may signal the end of international cooperation in critical infrastructure, replaced by the Revolutionary Guardâ€™s construction arm or limited cooperation with Huawei.Technical experts doubt the regime can sustain Barracks Internet without crippling the economy. Georgia Techâ€™s Internet Intelligence Lab, which has tracked Iranâ€™s shutdowns since the Arab Spring, called the blackout â€œthe most sophisticated and most severe in Iranâ€™s history.â€ Its measurements show about 3% connectivity persists, likely government officials and state services.We need to revolutionize access to the internet.â€Kaveh Ranjbar, former chief technology officer at RIPE NCC, the body managing European internet infrastructure, calls the plan a â€œdigital airlockâ€ that canâ€™t fully seal a modern economy. No country has hermetically sealed a functioning digital economy, he told .Activists have smuggled an estimated 50,000 Starlink satellite terminals into Iran since 2022, when the Biden administration exempted the service from sanctions. SpaceX has made the service free for Iranian users.Â The government claims it cut off 40,000 Starlink connections and jammed some terminals during the blackout, though others remain operational after firmware updates to bypass government blocking. Still, the technology remains vulnerable to signal jamming, meaning the regime holds ultimate leverage.â€œWe need to revolutionize access to the internet,â€ said Alimardani. â€œAnd move beyond the limiting structures and norms of â€˜internet sovereignty.â€™â€]]></content:encoded></item></channel></rss>