<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Introducing tmux-rs</title><link>https://richardscollin.github.io/tmux-rs/</link><author>Jtsummers</author><category>hn</category><pubDate>Thu, 3 Jul 2025 15:03:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tools: Code Is All You Need</title><link>https://lucumr.pocoo.org/2025/7/3/tools/</link><author>Bogdanp</author><category>hn</category><pubDate>Thu, 3 Jul 2025 10:51:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[If you've been following me on Twitter, you know I'm not a big fan of MCP
(Model Context Protocol)
right now.  It's not that I dislike the idea; I just haven't found it to work as
advertised.  In my view, MCP suffers from two major flaws:It isn’t truly composable.  Most composition happens through inference.It demands too much context.  You must supply significant upfront input, and
every tool invocation consumes even more context than simply writing and
running code.A quick experiment makes this clear: try completing a GitHub task with the
GitHub MCP, then repeat it with the  CLI tool.  You'll almost certainly
find the latter uses context far more efficiently and you get to your intended
results quicker.I want to address some of the feedback I've received on my stance on this.  I
evaluated MCP extensively in the context of agentic coding, where its
limitations were easiest to observe.  One piece of feedback is that MCP might
not make a ton of sense for general code generation, because models are already
very good at that but they make a lot of sense for end-user applications, like,
say, automating a domain-specific task in a financial company.  Another one is
that I need to look at the world of the future, where models will be able to
reach many more tools and handle much more complex tasks.My current take is that my data indicates that current MCP will always be harder
to use than writing code, primarily due to the reliance on inference.  If you
look at the approaches today for pushing towards higher tool counts, the
proposals all include a layer of filtering.  You pass all your tools to an LLM
and ask it to filter it down based on the task at hand.  So far, there hasn't
been much better approaches proposed.The main reason I believe this will most likely also hold true — that you
shouldn't be using MCP in its current form even for non-programming,
domain-specific tasks — is that even in those cases code generation just is the
better choice because of the ability to compose.Replace Yourself With A ShellscriptThe way to think about this problem is that when you don't have an AI, and
you're solving a problem as a software engineer, your tool of choice is code.
Perhaps as a non-software engineer, code is out of reach.  Many many tasks
people do by hand are actually automatable through software.  The challenge is
finding someone to write that software.  If you're working in a niche
environment and you're not a programmer yourself, you might not pick up a
programming book to learn how to code, and you might not find a developer
willing to provide you with a custom piece of software to solve your specific
problem.  And yes, maybe your task requires some inference, but many do need
them all the time.There is a reason we say “to replace oneself with a shell script”, it's because
that has been happening for a long time.  With LLMs and programming, the idea is
that rather than replacing yourself with a shell script, you're replacing
yourself with an LLM.  But you run into three problems: cost, speed, and general
reliability.  All these problems are what we need to deal with before we can
even think of tool usage or MCP.  We need to figure out how to ensure that our
automated task actually works correctly at scale.The key to automation is really to automate things that will happen over and
over.  You're not going to automate a one-shot change that will never recur.
You're going to start automating the things where the machine can truly give you
a productivity boost because you're going to do it once or twice, figure out how
to make it work, and then have the machine repeat it a thousand times.  For that
repetition, there's a very strong argument to be made for always using code.
That's because if we instruct the machine to use inference to do it, it might
work, particularly for small tasks, but it requires validation which can take
almost the same time as doing it in the first place.  Getting an LLM to
calculate for you sort of works, but it's much better for the LLM to write the
Python code to do the calculation.  Why?  First, you can review the formula, not
the calculation.  We can it ourselves or we can use the LLM as a judge to figure
out if the  is correct.  Don't really have to validate that Python
calculates correct, you can rely on that.  So, by opting for code generation for
task solving, we get a little closer to being able to verify and validate the
process ourselves, rather than hoping the LLM inferred correctly.This obviously goes way beyond calculation.  Take, for instance, this blog.  I
converted this entire blob from reStructuredText to Markdown recently.  I put
this conversion off for a really long time, partly because I was a little too
lazy.  But also, when I was lazy enough to consider deploying an LLM for it, I
just didn't trust it to do the conversion itself without regressing somewhere.
I was worried that if it ran out of context, it might start hallucinating text
or change wording slightly.  It's just that I worried about subtle regressions
too much.I still used an LLM for it, but I asked it to do that transformation in a
different way: through code.I asked the LLM to perform the core transformation from reStructuredText to
Markdown but I also asked it to do this in a way that uses the underlying AST
(Abstract Syntax Tree).  So, I instructed it to parse the reStructuredText
into an actual reStructuredText AST, then convert that to a Markdown AST, and
finally render it to HTML, just like it did before. This gave me an intermediate
transformation step and a comparable end result.Then, I asked it to write a script that compares the old HTML with the new HTML,
performs the diffing after some basic cleanup it deemed necessary for
comparison.  I asked it to consider what kind of conversion errors were
actually acceptable.  So, it read through its own scripts to see where it might
not match the original output due to known technical limitations (e.g.,
footnotes render differently between the Markdown library I'm using and the
reStructuredText library, so even if the syntax matches correctly, the HTML
would look different).  I asked it to compensate for this in that script.After that was done, I asked it to create a third script, which I could run
over the output of hundreds of files to analyze the differece to go back into
the agentic loop for another iteration tep.Then I kicked off off this in a loop.  I did not provide all the posts, I
started with 10 until differences were low and then had it do it for all.  It
did this for maybe 30 minutes or so until I came back to it and found it in a
pretty acceptable state.What's key about this transformation is not so much that the LLM was capable of
pulling it off, but that I actually trusted this process at the end because I
could review the approach.  Not only that, I also tried to ask another LLM what
it thinks of the code that another LLM wrote, and the changes.  It gave me much
higher confidence that what was going on would not lose data.  It felt right to
me.  It felt like a mechanical process that was fundamentally correct, and I was
able to observe it and do spot checks.  At worst, the regressions were minor
Markdown syntax errors, but the text itself wouldn't have been corrupted.Another key here is also that because the inference is rather constant, the cost
of inference in this process scales with the number of iteration steps and the
sample size, but it doesn't depend on how many documents I'm wanting to convert
overall.  Eventually, I just had it run over all documents all the time but
running it over 15 docs vs 150 docs is more or less the same effort, because the
final LLM based analysis step did not have that many more things to review (it
already skipped over all minor differences in the files).This is a long-winded way of saying that this entire transformation went through
code.  It's a pipeline that starts with human input, produces code, does an LLM
as a judge step and iterates.  And you can take this transformation and apply it
to a general task as well.To give an example, one MCP you might be using is Playwright.  I find it very
hard to replace Playwright with a code approach  because what
you're essentially doing is remotely controlling your browser.  The task you're
giving it largely involves reading the page, understanding what's on it, and
clicking the next button.  That's the kind of scenario where it's very hard to
eliminate inference at each step.However, if you already know what the page is — for instance, if you're
navigating your own app you're working on — then you can actually start telling
it to write a Playwright Python script instead and run that.  This script can
perform many of those steps sequentially without any inference.  I've noticed
that this approach is significantly quicker, and because it understands your
code, it still generally produces correct results.  It doesn't need to navigate,
read page contents, find a button, or press an input in real-time.  Instead, it
will write a single Python script that automates the entire process in one go,
requiring very little context by comparison. This process is repeatable.  Once the script is written, I can execute it 100,
200, or even 300 times without requiring any further inference.  This is a
significant advantage that an MCP typically cannot offer.  It's incredibly
challenging to get an LLM to understand generic, abstract MCP tool calls.  I
wish I could, for example, embed an MCP client directly into a shell script,
allowing me to run remote MCP services efficiently via code generation, but
actually doing that is incredibly hard because the tools are not written with
non inference based automation in mind.Also, as ironic as it is: I'm a human, not an MCP client.  I can run and debug a
script, I cannot even figure out how to reliably do MCP calls.  It's always a
gamble and incredibly hard to debug.  I love using the little tools that Claude
Code generates while generating code.  Some of those I had it convert into long
term additions to my development process.I don't know.  But it's an interesting moment to think what we could potentially
do to make code generation for purposeful agentic coding better.  The weird
thing is that MCP is actually pretty great when it works.  But it feels in the
current form too much like a dead end that cannot be scaled up, particularly to
automation at scale because it relies on inference too much.So maybe we need to look at ways to find a better abstraction for what MCP is
great at, and code generation.  For that that we might need to build better
sandboxes and maybe start looking at how we can expose APIs in ways that allow
an agent to do some sort of fan out / fan in for inference.  Effectively we want
to do as much in generated code as we can, but then use the magic of LLMs after
bulk code execution to judge what we did.I can also imagine that it might be quite interesting to do code generation in a
way that also provides enough context for an LLM to explain in human language to
a non programmer what the script is doing.  That might enable these flows to be
used by human users that are not developers themselves.In any case I can only encourage people to bypass MCP and to explore what else
is possible.  LLMs can do so much more if you give them the power to write code.Here are some more posts you might want to read or videos you might want to
watch:Drew Breunig's post “How to fix your context”
which covers some attempts to improve MCP tool selection if you cannot avoid
it.Manuel Odendahl's excellent “MCPs are Boring”
talk from AI Engineer that was one of the first to point to the challenges
with MCP.]]></content:encoded></item><item><title>I scanned all of GitHub&apos;s &quot;oops commits&quot; for leaked secrets</title><link>https://trufflesecurity.com/blog/guest-post-how-i-scanned-all-of-github-s-oops-commits-for-leaked-secrets</link><author>elza_1111</author><category>hn</category><pubDate>Thu, 3 Jul 2025 06:44:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[GitHub Archive logs every public commit, even the ones developers try to delete. Force pushes often cover up mistakes like leaked credentials by rewriting Git history. GitHub keeps these dangling commits, from what we can tell, forever. In the archive, they show up as “zero-commit” . I scanned every force push event since 2020 and uncovered secrets worth . Together with Truffle Security, we're open sourcing a new tool to scan your own GitHub organization for these hidden commits (try it here).tool identifies secrets in dangling commits.This guest post by Sharon Brizinov, a white-hat hacker, was developed through Truffle Security’s Research CFP program. We first connected with Sharon after his widely shared write-up, How I Made 64k From Deleted Files, where he used TruffleHog to uncover high-value secrets in public GitHub repositories. In this follow-up, Sharon expanded his research to access 100% of deleted commits on GitHub. He takes a deeper dive into one of our favorite areas: secrets hidden in deleted GitHub commits.What Does it Mean to Delete a Commit?Finding all Deleted CommitsHunting for Impactful SecretsCase Study - Preventing a Massive Supply-Chain CompromiseMy name is Sharon Brizinov, and while I usually focus on low-level vulnerability and exploitation research in OT/IoT devices, I occasionally dive into bug bounty hunting.I recently published a blog post about uncovering secrets hidden in dangling blobs within GitHub repositories, which sparked quite a lively discussion. After the post, I had several conversations with various people including Dylan, the CEO of Truffle Security, who gave me some intriguing ideas for continuing to explore new methods for large-scale secret hunting. I decided to create a mind map with everything I know related to this topic and try to come up with a new idea. I’ll spare you my messy sketch, but here’s a roundup of the projects, blogs, ideas, and resources I zeroed in on (highly recommended):Hidden GitHub Commits and How to Reveal Them by Neodyme.ioAnyone can Access Deleted and Private Repository Data on GitHub by TruffleHogTruffleHog now finds all Deleted & Private Commits on GitHub by TruffleHogTruffleHog Scans Deleted Git Branches by TruffleHogPhantom Secrets: Undetected Secrets Expose Major Corporations by Aqua SecurityEventually, I came up with a simple idea - I will use the Github Event API alongside the GitHub Archive project to scan all Zero-Commit Push-Events (deleted commits) for secrets. Everything was known, I just glued it together and built automation at scale that hunted for secrets.In this blog, I will describe my journey from understanding why you can never really delete a commit in GitHub to how to find all of them and build automation around it.What Does it Mean to Delete a Commit?In my previous blog post, I discussed how I discovered supposedly deleted files within GitHub repositories. Specifically, I was able to reconstruct dangling blobs - objects that had been deleted and were no longer referenced by any commit or tree… Or so I thought. After chatting with the Truffle folks, it turns out these orphaned blobs actually had orphaned commits that went along with them. And with a little research, I was able to uncover 100% of those orphaned commits at scale, across all of GitHub. Suppose you’ve accidentally committed and pushed a secret to your repository. What’s the next step? Typically, you’d want to reset the HEAD to the previous commit and force-push the changes, effectively removing the current commit and making it unreferenced - essentially deleting it. Here’s how you do it:But as neodyme and TruffleHog discovered, even when a commit is deleted from a repository, GitHub never forgets. If you know the full commit hash, you can access the supposedly deleted content. Moreover, you don't even need the full commit has, as TruffleHog discovered - it's enough to brute-force just the first four hex-digits.Force Pushing: A TutorialLet’s see this in action using my own repository: test-oops-commit. Try to locate the deleted commit - 9eedfa00983b7269a75d76ec5e008565c2eff2ef. To help visualize our commits, I prepared a simple bash script that shows the commit-tree-blob objects, :- -- | - - | - -We start by creating a simple repository with a single commit (a  file):Next, we create a new file named  containing our secret . We accidentally commit and push our secret to GitHub.We look at the commit tree to see that we have a new commit … which is associated with a new tree and a new blob for the file . We see the same when we run , , or when we access it from the web on GitHub.Oops! We discover our mistake and delete the commit by moving the HEAD of the branch to the previous commit and force-push it using:Let's remove our local version of the repo, clone the repository again, and check the commit tree. Phew, no secrets; the commit was really deleted!But we remember the commit hash, we we check online on GitHub and the commit can still be accessed - 9eedfa00983b7269a75d76ec5e008565c2eff (even accessing using four hex digits is enough 9eef). However, this time we get a message saying that the commit is deleted or doesn't belong to any branch on this repository. When you force-push after resetting (aka  followed by ), you remove Git’s reference to that commit from your branch, effectively making it unreachable through normal Git navigation (like ). However, the commit is still accessible on GitHub because GitHub stores these reflogs. Why? I don’t know for sure, but GitHub does give some hints. As I see it, GitHub is a much more complex beast than just a git server. It has many layers, including pull-requests, forks, private-public settings, and more. My guess is that to support all of these features, GitHub stores all commits and never deletes them. Here are some cases to consider:What are pull requests? These are just temporary branches, as Aqua Security wrote about, and can be retrieved by fetching all refs using -git -c "remote.origin.fetch=+refs/*:refs/remotes/origin/*" fetch originHow does the GitHub fork network work? What happens when you “fork” a repository? All the data is replicated, including commits you might delete.For these cases, and probably many others too (auditing? monitoring?) Github stores all the commits and won’t delete them, even if you force-push the head and “delete” the commit.OK, so commits are not really deleted. Fine. But you’d still need to know the full commit hash, or at least the first four hex-digits ignoring collisions (). As it turns out, TruffleHog has a tool to do just that, but it’s very slow, as you can imagine, going through all those. It doesn’t scale well beyond taking a day or two on a single repo.But there’s another way. A faster way, I’m now happy to share with you. The GitHub Event API is part of GitHub's REST API, which allows users to retrieve information about events that occur within GitHub. Events represent various activities in GitHub, such as:Opening or closing issues or pull requestsNo API token or auth is needed!You can see all the events that GitHub supports here.Events are recorded in near-real-time, but may be delayed by a few seconds.It’s only for public repositories.So, we could monitor commit data for all GitHub public repositories and store all the hashes. No more guessing commit hashes! Yeah, but it’s way too much. We are talking about millions of events per hour, and what about past events? Are they lost?Luckily for us a great developer named Ilya Grigorik decided many years ago to start a project that listens to GitHub’s event stream and systematically archives it. The project is open-source and called GH Archive and the website is gharchive.org. So, if we want, for example, to get the entire GitHub public activity around Jan 1st at 3pm UTC we just download this from here: https://data.gharchive.org/2015-01-01-15.json.gz.Here is a random sample of a  from that  archive:Finding Force Push Deleted CommitsTo identify only the deleted commits from force push events, we can look for push events that contain zero commits. Why would a Git push event have no commits? It indicates a force push that resets the branch - essentially just moving the HEAD without adding any new commits! I call this an  or a .Let’s see a quick example. We will download a random archive and search for such an event.If we randomly select one of the target event types, we will see that the  array is empty (zero commits). And if we look at the  commit - the one that was “deleted” (the HEAD before moving to HEAD^1, which is the “after”) - we see that Github still holds a record of it 10 years later!Here it is - https://github.com/grapefruit623/gcloud-python/commit/e9c3d31212847723aec86ef96aba0a77f9387493And it’s not necessarily just the  commit that was deleted. Sometimes a force push overwrites many commits at once. Given a Github organization (or user), repo name, and commit hash, it’s quite easy to scan the content of the “deleted” commit(s) for secrets using Git access:Clones a repo in a minimal way.: Omits file contents (blobs), only history/trees/commits.: Doesn't check out the working directory (no files appear yet).Fetches a specific commit ().Scans for secrets using TruffleHog.TruffleHog will automatically pull down the file contents (blobs) that need to be scanned. This command will search for secrets in all commits, starting with the  commit and working backward until the start of that branch. This ensures that all data from a force push overwriting more than one commit gets scanned; however, it will scan some non-dangling commits. The open-source tool we’ve released is a bit more efficient and only scans the actual dangling (dereferenced) commits.GitHub doesn't specify an exact rate limit for Git operations, but excessive cloning or fetching of repositories may trigger delaying or rate limiting (see here).In addition, we can use other methods to query a specific deleted/dangling commit with the GitHub API or simply with the Github web UI.Query for the commit patch using  GitHub’s REST API: https://api.github.com/repos/<ORG>/<REPO-NAME>/commits/<HASH>https://api.github.com/repos/github/gitignore/commits/e9552d855c356b062ed82b83fcaacd230821a6ebNote: There’s a strict rate-limit of 5,000 queries per hour for registered users and merely 60 for unregistered users. The server response header  indicates how many API calls users have left.Direct Web Access via Github.comYou can also access the commit details directly from GitHub.com.                                             Here are three different examples of how to access any commit via the GitHub website:https://github.com/<ORG>/<REPO-NAME>/commit/<HASH>https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6ebhttps://github.com/<ORG>/<REPO-NAME>/commit/<HASH>.patchhttps://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.patchhttps://github.com/<ORG>/<REPO-NAME>/commit/<HASH>.diffhttps://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.diffAlthough there is no documented rate limit, access is not guaranteed under heavy usage, and their WAF may block requests at any time without notice.So we have all the ingredients - we can get all GitHub event data, search for all events, fetch the “deleted” commit (the  hash), and then scan for active secrets using TruffleHog. Let’s do this. You know what? No need to build it, because together with Truffle Security’s Research team, we’re open-sourcing a new tool to search the entire GH Archive for “Oops Commits” made by your GitHub organization or user account. Since the entire GH Archive is available as a Google Big Query public dataset, this tool scans GHArchive PushEvent data for zero-commit events, fetches the corresponding commits, and scans them for secrets using TruffleHog. : We are releasing this tool to help blue teamers assess their potential exposure. Please use it responsibly.Here’s a command to get started:. --- ///. -- </For this research, I used a custom version of our open-source tool to scan all of GitHub's  since 2020. And wow. There were lots of secrets!Hunting for Impactful SecretsAfter running the automation, I found thousands of active secrets. But how can I identify the most interesting secrets tied to the most impactful organizations? My three-step formula for success: manual search, a vibe-coded triage tool, and AI.First, I manually explored and manipulated the data - essentially, got my hands dirty. The automation I built stores each newly discovered secret in a well-structured JSON file. Here's an example of what one of those files looks like:During this stage, I manually looked over the files for interesting secrets. For example, I filtered out all commits made by authors with generic email addresses (e.g. gmail.com, outlook.com, mail.ru, etc)  and focused on commits pushed by authors with a corporate email. While not perfect, it was a good start, and I found some really impactful keys.To understand the impact of specific tokens, I tried to figure out who owns the key and what access it has using open-source tools (e.g. secrets-ninja) and a few custom scripts. During my research, I learned that the Truffle Security team launched an open-source tool to do just that - TruffleHog Analyze. It’s built into TruffleHog; you just have to run . Note: I only did this additional secret enumeration when it was in-scope for specific Bug Bounty or Vulnerability Disclosure programs.Once I found something relevant or interesting, I reported it via a bug-bounty program or directly via email.Vibe Coding for Secret TriageAfter a couple hundred manual checks, I had enough and decided to scale-up my secrets review. I used vercel v0 to vibe-code a whole platform for triaging these “Oops Commit” secrets. The platform was very simple. It was a front-end-only interface (no backend at all) that received a .zip file with JSON files created by the scanner. It then presented them in a very easy-to-use table so I could quickly review them and mark what I had already reviewed. This method proved very efficient, and I used a combination of filters to quickly find the hidden gems!I also added some graphs and pie charts because why not? Looking at these graphs immediately revealed a few insights.First, if you look at the time-series graph below, there’s clearly a direct correlation between the year and amount of  secrets - most likely because older secrets have already been revoked or expired - as they should! Second, MongoDB secrets leaked the most. Based on my review of the data, this is because a lot of junior developers and CS students leaked mostly non-interesting side-project MongoDB credentials. The most interesting leaked secrets were GitHub PAT tokens and AWS credentials. These also generated the highest bounties!Finally, I plotted the frequency of files leaking valid credentials, ahd the results are clear - your file needs extra protection!Besides .env the most leaking filenames are: , , , , , , , , , , , , , , , , , , , , , , , ,, , , , , , , , , , , , , , , I was quite satisfied with my vibe-coded secrets review platform. However, reviewing secrets is still a manual task. Ideally, the process should automatically resolve all secrets to extract basic information about the associated accounts wherever possible. This data could then be passed to a LLAMA-based agent that analyzes and identifies potentially valuable secrets. In essence, the goal is to build an offline agent capable of determining which secrets hold significance from a bug bounty or impact-driven perspective.With the help of my friend Moti Harmats, I started working on it, but there’s still a lot more work to do, so I won’t release it at this time. But here’s a preview of what I started building:Case Study - Preventing a Massive Supply-Chain CompromiseOne of the secrets I found in a deleted commit was a GitHub Personal Access Token (PAT) belonging to a developer. The developer accidentally leaked this secret when they committed their hidden configuration files (dot files). I analyzed this token and found it had admin access to ALL of Istio repositories.Istio is an open-source service mesh that provides a transparent and language-independent way to flexibly and easily automate application network functions. It is designed to manage the communication between microservices in a distributed application, offering features such as traffic management, security, and observability without requiring changes to the application code.The main Istio project has  stars and  forks. Istio is used by a wide range of organizations and teams that run complex, distributed applications, especially those adopting microservices architectures. This includes giant corporations like Google, IBM, Red Hat and many others.And I had ADMIN level access to ALL of Itsio repositories (there are many of them). I could have read environment variables, changed pipelines, pushed code, created new releases, or even deleted the entire project. The potential for a mass supply-chain attack here was scary. Fortunately, Istio has a well-maintained report page, and the team acted quickly to revoke the GitHub PATs as soon as the issue was reported. Thank you!This was a really fun project. I glued together some known discoveries and was able to create a reliable automation that scanned and found thousands of active secrets, even some that were buried for years. I also got the chance to vibe code a secret hunting platform with some nice features that allowed me to find needles in a haystack and earn approximately $25k of bounties and deep-thanks through the process.The common assumption that deleting a commit is secure must change - once a secret is committed it should be considered compromised and must be revoked ASAP. It’s true for git blobs, git commits, and anything else that goes online.]]></content:encoded></item><item><title>Astronomers discover 3I/ATLAS – Third interstellar object to visit Solar System</title><link>https://www.abc.net.au/news/science/2025-07-03/3i-atlas-a11pl3z-interstellar-object-in-our-solar-system/105489180</link><author>gammarator</author><category>hn</category><pubDate>Thu, 3 Jul 2025 03:19:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For only the third time in history, astronomers have discovered a new interstellar object that originated from outside our Solar System.The object, known as 3I/ATLAS, is likely a comet and is much faster than any other interstellar object found before. The object appears to be hurtling towards our Sun at about 60 kilometres a second. "This thing is coming in at such an incredible speed that absolutely nothing in the Solar System could have caused this," Jonti Horner, an astronomer at the University of Southern Queensland, said. "Of the three interstellar objects we have seen, this is by far the fastest."There are only two other interstellar objects that have previously beentracked entering our Solar System — 'Oumuamua and Comet 2I/Borisov."It's so exciting," Professor Horner added.Rumblings of the potential interstellar object started in astronomy groups when the object was first detected early this week."It has been picked up so early — relatively speaking — that we've got at least eight months [during which]we'll be able to observe it," he said.The object was first detected by the ATLAS telescope in Chile on 1 July.Follow-up observations confirmed the orbit was extremely unusual — almost unaffected by the Sun's gravity, Professor Horner said. "Plotting the orbit of this thing [shows] it barely bends as it goes past the Sun."The trajectory of interstellar comet 3I/ATLAS as it passes through the Solar System.()But it wasn't until yesterday that scientists at the Minor Planet Centre in the USconfirmed the object was an interstellar object.They also suggested the object was likely a comet, due to images that showed it had a short tail.More observations will need to be done to confirm this, and get more detail about the object.However, because 3I/ATLAS has been found so early, astronomers will have plenty of time to track it as it moves through the Solar System.When will 3I/ATLAS be closest to the Sun?Currently, estimates suggest it will be closest to the Sun at the end of October, before returning out past Jupiter and into the outer Solar System by March next year.Unfortunately, Earth will be on the other side of the Solar System when 3I/ATLAS is closest to the Sun and at its brightest, making it harder for us to see."If we were on Mars, we'd have a fairly good view of it," Professor Horner said."It's not going to be hugely close to Mars, but it's going to be a lot closer to Mars than it will to the Earth."Because 3I/ATLAS might currently be going through an outburst — a sudden brightening caused by dust and gas being released by the object — it's difficult to track its size.'Oumuamua was in a "cigar" shape, making it much less bright. ()'Oumuamua was quite a small object, and estimates on the size of 2I/Borisov ranged from about 1 kilometre to more than 16km in diameter."I would say this is probably more along the lines of a few hundred metres to a kilometre across, maybe a bit bigger than that," Professor Horner said. "Which is big, but not exceptional."Will we see more interstellar objects? Interstellar objects have been extremely rare so far, but with better telescopes like the Rubin Observatory, we're likely to catch many more of these objects when they arrive. "We've had three [interstellar objects] in less than a decade with our current technology," Professor Horner said. "The Rubin Observatory is probably an order of magnitude better at finding things … so that would suggest we'll find a few of these per year."Within its first 10 hours of operation the observatory detected more than 2,000 previously unknown asteroids in the Solar System. "It's kind of a sneak peek into the future."]]></content:encoded></item><item><title>Trans-Taiga Road (2004)</title><link>https://www.jamesbayroad.com/ttr/index.html</link><author>jason_pomerleau</author><category>hn</category><pubDate>Thu, 3 Jul 2025 01:07:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The Trans-Taiga Road is a gravel road that runs 666 km east 
from near the top end of the  James Bay Road. It 
was built to access the various dams and generating stations that extend upriver 
along the La Grande River.This is an  road, leading 666 km 
east - almost to Labrador - with no settlements or towns aside from Hydro 
Quebec's settlements for workers (not open to the public). At the far end you 
will be 745 km from the nearest town! This is the farthest you can get from a 
town on a road in North America! (Not counting the private Hydro Quebec towns 
that are not open to the public). Along this road is also the
farthest north point you 
can travel on a road in eastern Canada.The road from Brisay (km 582) 
to Caniapiscau is rougher and a 4-wheel 
drive vehicle is recommended by Hydro Quebec. The main reason for this is the 
very coarse gravel used for this road - there's large rocks littering the road 
surface. However, I have heard from people who have driven this road in ordinary 
passenger cars and they say it is fine. You do have to keep your eyes open for 
the larger rocks though.You should definitely travel this road only in a reliable 
vehicle with good tires. It is not an overly rough road; passenger vehicles can 
drive it, but it is gravel. Vehicle breakdowns here can be very costly. Flat 
tires can be a serious (and expensive) incident if your tires are damaged. You 
could be looking at having tires flown in on a non-scheduled flight - there are 
no convenient "tire stores" up here! Please read
Driving the Trans-Taiga Road first.Although there are no towns, there are a couple of outfitters 
along the way that sell fuel and offer meals and lodging. Cell phones do not 
work here.Generally the scenery is fairly level, but this road is 
definitely more interesting than the James Bay Road. For 
most of the length it runs through taiga: spruce and jack pine forest, bogs, 
rocks, and low hills. This is about all you'll see apart from birds and some 
wildlife, the occasional cabin a short distance off the road, and Hydro Quebec 
installations. I once saw a couple of wolves playing in the middle of this road.]]></content:encoded></item><item><title>Whole-genome ancestry of an Old Kingdom Egyptian</title><link>https://www.nature.com/articles/s41586-025-09195-5</link><author>A_D_E_P_T</author><category>hn</category><pubDate>Thu, 3 Jul 2025 00:24:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Garstang, J. in The Burial Customs of Ancient Egypt (ed. Garstang, J.) 26–30 (Archibald Constable, 1907).Skourtanioti, E. et al. Genomic history of Neolithic to Bronze Age Anatolia, Northern Levant, and Southern Caucasus. , 1158–1175.e28 (2020).ArticleCASPubMed
                    Google ScholarHaber, M. et al. Continuity and admixture in the last five millennia of Levantine history from ancient Canaanite and present-day Lebanese genome sequences. , 274–282 (2017).ArticleCASPubMedPubMed Central
                    Google ScholarSalvatori, S. & Usai, D. The neolithic and ‘pastoralism’ along the Nile: a dissenting view. , 251–285 (2019).Wengrow, D. The Archaeology of Early Egypt: Social Transformations in North-East Africa, 10,000 to 2,650 BC (Cambridge Univ. Press, 2006).Bard, K. A. in The Oxford History of Ancient Egypt (ed. Shaw, I.) 61–88 (Oxford Univ. Press, 2000).Stevenson, A. in  (ed. Crawford, H.) 620–636 (Routledge, 2013).Malek, J. in The Oxford History of Ancient Egypt (ed. Shaw, I.) 89–117 (Oxford Univ. Press, 2000).Doherty, S. K. The Origins and Use of the Potter’s Wheel in Ancient Egypt (Archaeopress, 2015).Keita, S. O. Y. Further studies of crania from ancient northern Africa: an analysis of crania from First Dynasty Egyptian tombs. , 245–254 (1992).ArticleCASPubMed
                    Google ScholarProwse, T. L. & Lovell, N. C. Concordance of cranial and dental morphological traits and evidence for endogamy in ancient Egypt. , 237–246 (1996).ArticleCASPubMed
                    Google ScholarIrish, J. D. Who were the ancient Egyptians? Dental affinities among Neolithic through postdynastic peoples. , 529–543 (2006).ArticlePubMed
                    Google ScholarZakrzewski, S. R. in Egyptian Bioarchaeology: Humans, Animals, and the Environment (eds Ikram, S. et al.) 157–167 (Sidestone, 2015).Dicke-Toupin, C. R. Population Continuity or Replacement at Ancient Lachish? (Fairbanks, 2012).Irish, J. D. Diachronic and synchronic dental trait affinities of late and post-Pleistocene peoples from North Africa. , 138–155 (1998).Maaranen, N., Zakrzewski, S. & Schutkowski, H. Who were the Hyksos? , 66–69 (2022).De Meyer, M. et al. in Under the Potter’s Tree: Studies on Ancient Egypt Presented to Janine Bourriau Vol. 204 (eds Aston, D. et al.) 679–702 (Peeters, 2011).Power, R. K. & Tristant, Y. From refuse to rebirth: repositioning the pot burial in the Egyptian archaeological record. , 1474–1488 (2016).Buikstra, J. E. & Ubelaker, D. U. Standards for Data Collection from Human Skeletal Remains (Arkansas Archeological Survey, 1994).Raxter, M. H. et al. Stature estimation in ancient Egyptians: a new technique based on anatomical reconstruction of stature. , 147–155 (2008).ArticlePubMed
                    Google ScholarIşcan, M. Y., Loth, S. R. & Wright, R. K. Age estimation from the rib by phase analysis: white males. , 1094–1104 (1984).ArticlePubMed
                    Google ScholarKennedy, K. A. R. in Reconstruction of Life from the Skeleton (eds Kennedy, K. A. R. & Işcan, M. Y.) 129–160 (Alan R. Liss, 1989).Capasso, L., Kenney, K. A. R. & Wilczak, C. A. Atlas of Occupational Markers on Human Remains (Edigratifal, 1998).Buzon, M. R. & Simonetti, A. Strontium isotope (Sr/Sr) variability in the Nile Valley: identifying residential mobility during ancient Egyptian and Nubian sociopolitical changes in the New Kingdom and Napatan periods. , 1–9 (2013).ArticlePubMed
                    Google ScholarStantis, C., Nowell, G. M., Prell, S. & Schutkowski, H. Animal proxies to characterize the strontium biosphere in the northeastern Nile Delta. Bioarchaeology of the Near East, 1–13 (2019).Touzeau, A. et al. Egyptian mummies record increasing aridity in the Nile Valley from 5500 to 1500 yr before present. , 92–100 (2013).Richards, M. P. in Archaeological Science: An Introduction (eds Richards, M. P. & Britton, K.) 125–144 (Cambridge Univ. Press, 2019).Touzeau, A. et al. Diet of ancient Egyptians inferred from stable isotope systematics. , 114–124 (2014).Macko, S. A. et al. Documenting the diet in ancient human populations through stable isotope analysis of hair. Philos. Trans. R. Soc. London, B: Biol. Sci., 65–76 (1999).ArticleCASPubMed
                    Google ScholarThompson, A. H., Richards, M. P., Shortland, A. & Zakrzewski, S. R. Isotopic palaeodiet studies of ancient Egyptian fauna and humans. , 451–463 (2005).Thompson, A. H., Chaix, L. & Richards, M. P. Stable isotopes and diet at ancient Kerma, Upper Nubia (Sudan). , 376–387 (2008).Poulallion, E. et al. High δN values in Predynastic Egyptian archeological remains: a potential indicator for localised soil fertilisation practices in extreme conditions. Preprint at https://doi.org/10.1101/2024.11.18.624066 (2024).Gansauge, M.-T. & Meyer, M. Single-stranded DNA library preparation for the sequencing of ancient or damaged DNA. , 737–748 (2013).ArticlePubMed
                    Google ScholarZvelebil, M. & Lillie, M. in  (ed Price, T. D.) 57–92 (Cambridge Univ. Press, 2000).Pinhasi, R. & Stock, J. T. Human Bioarchaeology of the Transition to Agriculture (Wiley, 2011).Martin, N. et al. From hunter-gatherers to food producers: new dental insights into the Nile Valley population history (Late Paleolithic-Neolithic). , e24948 (2024).ArticlePubMed
                    Google ScholarStevenson, A. The Egyptian Predynastic and state formation. , 421–468 (2016).Redford, D. B. Egypt, Canaan, and Israel in Ancient Times (Princeton Univ. Press, 1992).Llorente, M. G. et al. Ancient Ethiopian genome reveals extensive Eurasian admixture in Eastern Africa. , 820–822 (2015).Bourriau, J. in The Oxford History of Ancient Egypt (ed. Shaw, I.) 172–206 (Oxford Univ. Press, 2000).Ryholt, K. S. B. & Bülow-Jacobsen, A. The Political Situation in Egypt During the Second Intermediate Period, C. 1800-1550 B.C. (Museum Tusculanum, 1997).Weiss, B. The decline of Late Bronze Age civilization as a possible response to climatic change. , 173–198 (1982).Ward, W. A., Joukowsky, M. S. & Åström, P. The Crisis Years: The 12th Century B.C.: From Beyond the Danube to the Tigris (Kendall Hunt, 1992).Dabney, J. et al. Complete mitochondrial genome sequence of a Middle Pleistocene cave bear reconstructed from ultrashort DNA fragments. Proc. Natl Acad. Sci. USA, 15758–15763 (2013).ArticleCASPubMedPubMed Central
                    Google ScholarKircher, M., Sawyer, S. & Meyer, M. Double indexing overcomes inaccuracies in multiplex sequencing on the Illumina platform. , e3 (2012).ArticleCASPubMed
                    Google ScholarGansauge, M.-T., Aximu-Petri, A., Nagel, S. & Meyer, M. Manual and automated preparation of single-stranded DNA libraries for the sequencing of DNA from ancient biological remains and other sources of highly degraded DNA. , 2279–2300 (2020).ArticleCASPubMed
                    Google ScholarDee, M. C14 data pottery coffin burial excavated by Garstang in Nuwayrat (World Museum, Liverpool, UK, 2016).Vanthuyne, B. Early Old Kingdom Rock Circle Cemeteries in the 15th and 16th Nomes of Upper Egypt. A Socio-archaeological Investigation of the Cemeteries in Dayr al-Barshā, Dayr Abū Ḥinnis, Benī Ḥasan al-Shurūq and Nuwayrāt. PhD thesis, KU Leuven (2017).Reimer, P. J. et al. The IntCal20 Northern Hemisphere radiocarbon age calibration curve (0–55 cal kBP). , 725–757 (2020).Bayliss, A. & Marshall, P. Radiocarbon Dating and Chronological Modelling: Guidelines and Best Practice (Historical Association, 2022).Brown, T. A., Nelson, D. E., Vogel, J. S. & Southon, J. R. Improved collagen extraction by modified Longin method. , 171–177 (1988).Coplen, T. B. Normalization of oxygen and hydrogen isotope data. , 293–297 (1988).Chenery, C. A., Pashley, V., Lamb, A. L., Sloane, H. J. & Evans, J. A. The oxygen isotope relationship between the phosphate and structural carbonate fractions of human bioapatite. Rapid Commun. Mass Spectrom., 309–319 (2012).ArticleCASPubMed
                    Google ScholarFont, L., Nowell, G. M., Graham Pearson, D., Ottley, C. J. & Willis, S. G. Sr isotope analysis of bird feathers by TIMS: a tool to trace bird migration paths and breeding sites. , 513 (2007).Nier, A. O. The isotopic constitution of strontium, barium, bismuth, thallium and mercury. , 275–278 (1938).Avanzinelli, R., Conticelli, S. & Francalanci, L. High precision Sr, Nd, and Pb isotopic analyses using the new generation Thermal Ionisation Mass Spectrometer ThermoFinnigan Triton-Ti®. , 147–166 (2015).Işcan, M. Y., Loth, S. R. & Wright, R. K. Age estimation from the rib by phase analysis: white females. , 853–863 (1985).ArticlePubMed
                    Google ScholarIşcan, M. Y. & Loth, S. R. Determination of age from the sternal rib in white males: a test of the phase method. , 122–132 (1986).ArticlePubMed
                    Google ScholarMeindl, R. S. & Lovejoy, C. O. Ectocranial suture closure: a revised method for the determination of skeletal age at death based on the lateral-anterior sutures. , 57–66 (1985).ArticleCASPubMed
                    Google ScholarLovejoy, C. O., Meindl, R. S., Pryzbeck, T. R. & Mensforth, R. P. Chronological metamorphosis of the auricular surface of the ilium: a new method for the determination of adult skeletal age at death. , 15–28 (1985).ArticleCASPubMed
                    Google ScholarBrooks, S. & Suchey, J. M. Skeletal age determination based on the os pubis: a comparison of the Acsádi-Nemeskéri and Suchey-Brooks methods. , 227–238 (1990).Trotter, M. & Gleser, G. C. Estimation of stature from long bones of American whites and Negroes. , 463–514 (1952).ArticleCASPubMed
                    Google ScholarRobins, G. & Shute, C. C. D. Predynastic Egyptian stature and physical proportions. , 313–324 (1986).Bass, W. M. Human Osteology: A Laboratory and Field Manual (Missouri Archaeological Society, 2006).Richard Scott, G. & Irish, J. D. Human Tooth Crown and Root Morphology (Cambridge Univ. Press, 2017).Howells, W. W. Skull Shapes and the Map: Craniometric Analyses in the Dispersion of Modern Homo, Vol. 79 (Harvard Univ. Press, 1989) .Scott, G. R. et al. rASUDAS: a new web-based application for estimating ancestry from tooth morphology. , 18–31 (2018).Ortner, D. J. & Putschar, W. Identification of Paleopathological Conditions in Human Skeletal Remains (Smithsonian Institution, 1985).Aufderheide, A. C. & Rodríguez-Martín, C. The Cambridge Encyclopedia of Human Paleopathology (Cambridge Univ. Press, 1998).Hawkey, D. E. & Merbs, C. F. Activity‐induced musculoskeletal stress markers (MSM) and subsistence strategy changes among ancient Hudson Bay Eskimos. , 324–338 (1995).Alves-Cardoso, F. & Assis, S. Exploring ‘wear and tear’ of joints and ‘muscle function’ assumptions in skeletons with known occupation at death. , 689–700 (2021).ArticlePubMed
                    Google ScholarWallace, I. J. et al. Experimental evidence that physical activity inhibits osteoarthritis: implications for inferring activity patterns from osteoarthritis in archeological human skeletons. , 223–231 (2022).Wilkinson, C. M. & Mahoney, G. in Craniofacial Identification (eds Wilkinson, C. M. & Rynn, C.) 222–237 (Cambridge Univ. Press, 2012).El-Mehallawi, I. H. & Soliman, E. M. Ultrasonic assessment of facial soft tissue thicknesses in adult Egyptians. , 99–107 (2001).ArticleCASPubMed
                    Google ScholarRynn, C., Balueva, T. & Veselovskaya, E. in Craniofacial Identification (eds Wilkinson, C. M. & Rynn, C.) 193–202 (Cambridge Univ. Press, 2012).Wilkinson, C. M. Cognitive bias and facial depiction from skeletal remains. , 1–14 (2021).Jónsson, H., Ginolhac, A., Schubert, M., Johnson, P. L. F. & Orlando, L. mapDamage2.0: fast approximate Bayesian estimates of ancient DNA damage parameters. , 1682–1684 (2013).ArticlePubMedPubMed Central
                    Google ScholarMeyer, M. et al. Nuclear DNA sequences from the Middle Pleistocene Sima de los Huesos hominins. , 504–507 (2016).ArticleCASPubMed
                    Google ScholarRenaud, G., Slon, V., Duggan, A. T. & Kelso, J. Schmutzi: estimation of contamination and endogenous mitochondrial consensus calling for ancient DNA. , 224 (2015).ArticlePubMedPubMed Central
                    Google ScholarKorneliussen, T. S., Albrechtsen, A. & Nielsen, R. ANGSD: analysis of next generation sequencing data. , 356 (2014).Skoglund, P., Storå, J., Götherström, A. & Jakobsson, M. Accurate sex identification of ancient human remains using DNA shotgun sequencing. , 4477–4482 (2013).Briggs, A. W. et al. Removal of deaminated cytosines and detection of in vivo methylation in ancient DNA. , e87–e87 (2010).ArticlePubMed
                    Google ScholarSchönherr, S., Weissensteiner, H., Kronenberg, F. & Forer, L. Haplogrep3—an interactive haplogroup classification and analysis platform. , W263–W268 (2023).ArticlePubMedPubMed Central
                    Google ScholarFregel, R. et al. Ancient genomes from North Africa evidence prehistoric migrations to the Maghreb from both the Levant and Europe. Proc. Natl Acad. Sci. USA, 6774–6779 (2018).ArticlePubMedPubMed Central
                    Google ScholarOmrak, A. et al. Genomic evidence establishes Anatolia as the source of the European Neolithic gene pool. , 270–275 (2016).ArticleCASPubMed
                    Google ScholarRaghavan, M. et al. Upper Palaeolithic Siberian genome reveals dual ancestry of native Americans. , 87–91 (2014).ArticlePubMed
                    Google ScholarRodríguez-Varela, R. et al. Genomic analyses of pre-European conquest human remains from the Canary Islands reveal close affinity to modern North Africans. , 1677–1679 (2018).ArticlePubMed
                    Google ScholarSchlebusch, C. M. et al. Southern African ancient genomes estimate modern human divergence to 350,000 to 260,000 years ago. , 652–655 (2017).ArticleCASPubMed
                    Google ScholarSeguin-Orlando, A. et al. Genomic structure in Europeans dating back at least 36,200 years. , 1113–1118 (2014).ArticleCASPubMed
                    Google Scholarvan de Loosdrecht, M. et al. Pleistocene North African genomes link Near Eastern and sub-Saharan African human populations. , 548–552 (2018).ArticlePubMed
                    Google ScholarWang, C.-C. et al. Ancient human genome-wide data from a 3000-year interval in the Caucasus corresponds with eco-geographic regions. , 1–13 (2019).Yang, M. A. et al. Ancient DNA indicates human population shifts and admixture in northern and southern China. , 282–288 (2020).ArticleCASPubMed
                    Google ScholarVyas, D. N., Al-Meeri, A. & Mulligan, C. J. Testing support for the northern and southern dispersal routes out of Africa: an analysis of Levantine and southern Arabian populations. , 736–749 (2017).ArticlePubMed
                    Google ScholarMallick, S. et al. The Allen Ancient DNA Resource (AADR) a curated compendium of ancient human genomes. , 1–10 (2024).Rubinacci, S., Ribeiro, D. M., Hofmeister, R. J. & Delaneau, O. Efficient phasing and imputation of low-coverage sequencing data using large reference panels. , 120–126 (2021).ArticleCASPubMed
                    Google ScholarThe 1000 Genomes Project Consortium. A global reference for human genetic variation. , 68–74 (2015).Chaitanya, L. et al. The HIrisPlex-S system for eye, hair and skin colour prediction from DNA: introduction and forensic developmental validation. Forensic Sci. Int. Genet., 123–135 (2018).ArticleCASPubMed
                    Google ScholarWalsh, S. et al. Developmental validation of the HIrisPlex system: DNA-based eye and hair colour prediction for forensic and anthropological usage. Forensic Sci. Int. Genet., 150–161 (2014).ArticleCASPubMed
                    Google Scholar]]></content:encoded></item><item><title>Gmailtail – Command-line tool to monitor Gmail messages and output them as JSON</title><link>https://github.com/c4pt0r/gmailtail</link><author>c4pt0r</author><category>hn</category><pubDate>Thu, 3 Jul 2025 00:06:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What to build instead of AI agents</title><link>https://decodingml.substack.com/p/stop-building-ai-agents</link><author>giuliomagnifico</author><category>hn</category><pubDate>Thu, 3 Jul 2025 00:02:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Websites hosting major US climate reports taken down</title><link>https://apnews.com/article/climate-change-national-assessment-nasa-white-house-057cec699caef90832d8b10f21a6ffe8</link><author>geox</author><category>hn</category><pubDate>Wed, 2 Jul 2025 21:10:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Vitamin C Boosts Epidermal Growth via DNA Demethylation</title><link>https://www.jidonline.org/article/S0022-202X(25)00416-6/fulltext</link><author>gnabgib</author><category>hn</category><pubDate>Wed, 2 Jul 2025 20:28:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A Higgs-Bugson in the Linux Kernel</title><link>https://blog.janestreet.com/a-higgs-bugson-in-the-linux-kernel/</link><author>Ne02ptzero</author><category>hn</category><pubDate>Wed, 2 Jul 2025 18:34:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We recently ran across a strange higgs-bugson that manifested itself in a critical system that stores and distributes the firm’s trading activity data, called Gord. (A higgs-bugson is a bug that is reported in practice but difficult to reproduce, named for the Higgs boson, a particle which was theorized in the 1960s but only found in 2013.) In this post I’ll walk you through the process I took to debug it. I tried to write down relevant details as they came up, so see if you can guess what the bug is while reading along.The NFS (“Network File System”) protocol is designed to access a regular POSIX filesystem over the network. The default security story of NFSv3, which is what we’re using here, is roughly “no security” on an untrusted network: the server only checks whether or not the client is connected from a ”privileged” port number (i.e. less than 1024). If the client says it’s connecting on behalf of a particular user, the server just trusts the client. What could go wrong?The other security option for NFS is Kerberos. When used with NFS, Kerberos cryptographically verifies the identity of the user accessing the file.Gord often does large file copies to ship data around. These copies would very rarely fail with -EACCES (Permission denied) despite the permissions being correctly set on the filesystem. Although retries were possible, it would be sad to lose progress copying these files. Also, strange errors in data storage are scary! It’s possible that spurious errors could indicate a larger issue.There was no obvious pattern in these copies failing. Even identical jobs running simultaneously didn’t necessarily fail together. We did have one clue: if we switched Kerberos off in the dev environment (because the error sounded auth related), the copies never failed.So, maybe something was wrong with the Kerberos credentials?How does the kernel get your Kerberos credentials?In a typical userspace program making use of Kerberos, libkrb5 will parse some environment variables or a config file to find the location of a Kerberos credentials cache. However, applications using NFS don’t need to link libkrb5 or otherwise know anything about Kerberos. They just do normal file I/O syscalls (open, read, write, etc) as if they were accessing a local filesystem. So what’s going on?It turns out the kernel gets credentials via a root userspace daemon called . When your application does its first I/O syscall to a file on NFS, the kernel writes to a file on a special mountpoint to communicate with . (Fun fact: this mountpoint, , is an entirely separate filesystem implementation in the Linux kernel, just like  or  itself.)Making some simplifications, the rpc_gssd program grabs the user’s credentials, constructs the right Kerberos service ticket, and writes to the rpc_pipefs again with the result. This involves an API called GSSAPI (“Generic Security Services API”), which you’ll see mentioned throughout this post.Looking at the rpc_gssd logs around the time of the bug, I noticed that the kernel hadn’t requested credentials for a while. The most recently requested credential should also have been fresh for another few hours. So, this was a dead end.Trying to reproduce the bugI decided to try my luck by running a slow trickle of writes over the weekend. It seemed like the issue would be key-related somehow, so having a long-running process would force key expiry and plausibly reproduce the bug.Checking back in on Monday, none of the dozen boxes I ran this on failed. This wasn’t too surprising, because the issue was pretty rare in production.I then generated some large (~200GB) random files, put them on a test NFS mount, and started copying them to another test NFS mount in a loop on even more boxes. Once again, none of these copies failed.At this point I was surprised I hadn’t seen the issue. To make sure I wasn’t just getting extremely unlucky, I decided to scale up the number of copies running in parallel.I was worried about wasting bandwidth and impacting other users, so I thought a bit about how to make the test a bit more lightweight. One easy win would be to copy from a local disk to NFS instead of from NFS to NFS. However, I had already requested boxes with tiny disks. Fortunately, I had a cool trick in mind.I decided to create a filesystem that contains large random files, but fits entirely in memory on small test machines. Here’s the idea: Instead of storing actual file content, I’d use a noise function (or hash function) to generate consistent random bytes on demand.This turns out to be fairly straightforward. There’s a Rust crate called  that provides a nicely typed implementation of FUSE (“Filesystem in USErspace”). Around 20 minutes later (with some assistance from Claude), I had a fake filesystem that “contained” some large files to copy from.This ultimately did take less time than it would have taken to use larger boxes, but felt slightly yak-shavy before I was sure this would work. I don’t think I would have attempted this trick if I didn’t have an AI assistant to write most of the filesystem for me.Inserting arbitrary code into the Linux kernel at runtimeThe next thing I wanted to do was collect some debug information. If the reproducer  work, I would want to see the kernel stack traces for the syscall that was returning EACCES. I needed to be prepared for this beforehand, because I expected the bug to take a while to show up.There’s a Linux kernel subsystem called “eBPF”, which stands for “extended Berkeley Packet Filter”. As you might imagine, it’s supposed to let you filter network packets. However, it has since eaten the world and now lets you insert ~any code you want at the start or end of basically any function in the Linux kernel at runtime. This is fine. Everything’s going to be ok. Don’t worry about it!There’s a handy tool called  that can quickly print arguments and return values of kernel functions (among other things). I wrote a bpftrace script that instrumented a few interesting-looking functions, something like this:The example above looks at the gss_cred_init function, and prints out the kernel stack trace if it returns an EACCES error. This is a very simple example, but definitely check out the bpftrace manual for other functionality.Back to reproducing the issueThe test setup was as follows:Some jobs that run rsync processes to copy from the FUSE filesystem to a test NFS server.A bpftrace script that watches for  being returned from relevant kernel functions.A way to take a packet capture (PCAP) of just the time surrounding a returned .And… It worked! ! Weirdly,  of my test boxes failed at the same time? That never happened in production. Usually only one or two Gord jobs would fail at a time. One bpftrace message stood out: “gss_validate returned -13 (GSS_S_BAD_SIG)”.Bad signature??? What? Of all the things that would make sense, this made sense the least. Was the server returning a bad signature? Was the client failing to verify it correctly? Was there memory corruption somewhere? Keep in mind all of this software is written in C, so almost anything is possible. Even nasal demons. If this  memory corruption, maybe I found a security vulnerability?I peeked at the packet capture of the bug in Wireshark and did not see any obvious signs of corruption. Other interesting things I noticed were:There were a lot of retransmissions at the NFS level. The test NFS server I was using was small and probably got overloaded.TCP frames were being split up and reassembled.Again: A third of my jobs failed together, which was unexpected given what I saw in production.I didn’t have any good guesses based on the above. Maybe I could try to generate the signature myself to compare it with what’s in the packet? I knew Wireshark could decrypt Kerberos requests in network packets given the user’s Kerberos password, which was enough to grab the signature key (GSS token). All I needed to do was write a program to compute the signature given that token. Seems simple enough in theory, but how exactly do you do that?An NFS request looks something like this. Some interesting things to call out here are:There’s an XID, which matches responses to requests. A client can have multiple requests in flight, and the server can respond to them out of order, so an ID is necessary.The credentials field specifies which GSS context the RPC request is associated with, and includes an incrementing sequence number (“GSS sequence number”). Note that this is a separate sequence number from the XID.In the request, the checksum is the HMAC of roughly all the data in the request header, using the shared GSS key. In the response, the checksum is the HMAC of the GSS sequence number from the request.An HMAC is a “Hash-based Message Authentication Code” – it allows someone with knowledge of the key to verify that someone else with the same key created the checksum.Writing a Wireshark pluginThe next thing I did was write a Wireshark plugin to compute the checksums of replies.While writing the Wireshark plugin I ran into a problem: there were retransmissions in my PCAP, so how do I figure out which of the retransmitted requests corresponds to a response? This was throwaway code for debugging, so I decided to make a big shared-mutable hashmap containing a map from XIDs to GSS sequence numbers. I updated the hashmap whenever Wireshark processed a frame containing an NFS request, assuming it would process them in order.Then, I loaded up my packet capture and browsed to the response with an XID that failed verification.Okay. So the checksum in the packet is correct. Why did the kernel think it wasn’t? I clicked back to the request in the PCAP to take a look. Annoyingly, there were two requests with the same XID, meaning that a retransmission was involved. I then clicked back to the response.Huh. Was my Wireshark plugin buggy?(At this point I think you should have all the information you need to guess what the bug is. It might be fun to think through this. When you’re ready, read on.)Remember how I wasn’t sure which request to use to get the GSS sequence number from? It turns out the kernel has the exact same bug!SunRPC matches responses to requests via their XIDs, so if the server is overloaded and takes a while to respond, the NFS client might retransmit the request. The checksum field in the response is an HMAC over the  GSS sequence number. Note that this is  the XID, and is  included in the response. When the kernel retransmits a request with the same XID, it uses a new sequence number and updates the GSS sequence number it has recorded. If the kernel then receives the response that was associated with the old GSS sequence number, checksum validation fails. If this happens 3x in a row,  is returned to userspace.This is  self-fulfilling because each failure creates another retry. It is not guaranteed, however: you can still get lucky with timing and avoid the bug.Basically, the only reason I was able to reproduce the bug is because I was using a tiny test NFS server, causing latencies in the hundreds of seconds. If I had kept going with low-load testing, I probably would have had to use another method to find the bug.A quick read of some kernel source code confirmed that what I thought was happening  happen, but to be sure, I decided to write a lightweight reproducer that works by delaying packets.There’s a kernel facility called NFQUEUE which allows you to use a userspace process for packet filtering. This is probably intended for security use cases, but what I did was hook it up to a Python script where I can individually look at packets and press  to let them through after enough time has passed to trigger the bug. Basically, I could manually simulate high latency by being a very very slow human firewall.Then it was a matter of writing a little more glue code, and I had a fully automatic reproduction script.At this point I reported my findings to my team, who quickly noticed that the RFC actually does mention this case.“Then when it receives a response with a matching RPC transaction identifier, it can compute the checksum of each sequence number in the cache to try to match the checksum in the reply’s verifier.” - RFC2203 5.3.3.1. (Page 13)The Linux kernel does not actually implement this cache as suggested by the RFC, so I wrote a kernel patch to add this functionality and mailed it off upstream. I also learned that the FreeBSD kernel actually already implements this, so this is new-to-Linux but not new-to-NFS.More importantly, though, all that this cache does is increase the amount of retries needed to hit a bad interleaving. The fundamental problem is that a sequence number mismatch should not cause an immediate retransmission, which makes the problem self-fulfilling. So, I wrote a second kernel patch to not retransmit if a bad checksum is seen.This feels principled, since a checksum mismatch suggests network tampering, so it makes sense to treat it as if we didn’t receive a message at all. The normal timeout logic can take care of retransmission in the unlikely case that one is needed. As final verification, I applied these patches and made sure that the test copy jobs and the Python reproducer no longer failed.Both of these patches are now upstream and will be available in Linux 6.16.]]></content:encoded></item><item><title>Couchers is officially out of beta</title><link>https://couchers.org/blog/2025/07/01/releasing-couchers-v1</link><author>laurentlb</author><category>hn</category><pubDate>Wed, 2 Jul 2025 18:05:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Quick summary: we are out of Beta and into version 1, we're releasing a new strategy around safe & active community instead of bashing our competitors, a fancy redesigned landing page, and a bunch of new features to make core couch surfing functionality better! Share the platform with your friends and let's grow the community together!We are super excited to share that Couchers is today finally out of the Beta phase with our version 1 (v1) launch. After five years of building and developing the platform and community―first through the Alpha stage and a very long Beta stage―we are finally ready for our big launch! This means a brand new strategy, a spiffy new landing page, and a bunch of new features.Today we are launching our new strategy centered around a commitment on being the safest, healthiest, and most active couch surfing community. This is an evolution of our original plan, but adapted for the next stage of the community, where we clearly define what we stand for instead of living the shadow of another platform.In addition, we are launching a brand new landing page to more clearly communicate our values and what sets us apart, and to explain what our community is all about to newbies and veteran couch surfers alike.Finally, we have combined a number of exciting new features into this release. These have been gradually rolled out over the past few months to make sure they are ready for you today (and hopefully mostly free of bugs)! Read on to find out what these are.What does the v1 launch mean?Although the platform has been usable for a long time, until now, even some of the core functionality has been somewhat buggy and not as polished as we would like, in order to call it "complete". The v1 launch signals the completion of the platform in the sense that core functionality has been cleaned up and we believe it is fully ready for all to use! A lot of this cleanup effort was also done in the v0.9.9 release a few months ago.This launch means we can now focus on growing the platform and coming up with new features that set us apart and help you connect with other members better.The features we decided to work on for v1 were picked to best further our goal of being a safe and active community, as well as tighten up the core functionality of the platform to make the experience super smooth and easy.We are really excited about our new landing page, which clearly communicates what Couchers is, what we stand for, and what you can find on the platform once you sign up. We hope this will help us gain more of the right kinds of members!A huge thanks to Nicole who built the majority of the landing page; Pablo and Aapeli who also helped build it; Marta and Charlotte who designed it; Jesse and Chris who helped test it; and countless others who helped write or brainstorm our marketing message, or provided their input in other ways!The new landing page features an anonymized map layer: pins are randomized to be at a random point 2-10 km (1.2-6.2 mi) away from their true location and are not tied to any user info. The map is further restricted to not zoom very far in.We completely overhauled functionality for leaving a reference with two key updates:you can now indicate whether you did not stay with or host someone (and we won't remind you to leave references); andyou can now send our Safety Team private feedback (that's not shown on the public reference) about your interactions through the reference flow. You can use this functionality to let us know if something felt off, even if there's no cause for bigger concern. More information is always better and helps the Safety Team understand situations better and keep the platform safe!Thanks to Aapeli, Nicole, and Jason for these updates. Nicole also added the number of references to the references tab on profiles to draw more attention to it.Find members faster: a new and redesigned mapNicole spent months re-architecting and rebuilding a brand new Map Search page! The old one was creaking with age and very difficult to update and add features to. Nicole rewrote it from scratch! Aapeli helped build some new backend functionality to significantly speed up searching, and Jesse among many others helped test it!We put a lot of thought into making the map as usable as possible, trying to think about what should be shown, what should be hidden, and how to best display all this data. We hope you find it useful!Don't miss out on news: new notification feedWe've been working on notifications for quite a few releases now. In this release we introduced a notification feed, rolled out push notifications for desktop and mobile, as well as added a bunch of new notification types for things like nested replies on discussions, and pending host requests.Don't want so many notifications? No problem. Each notification type can be adjusted for a particular item or for a whole class of notifications. Find this under Notification Settings (accessible from Account Settings). We hope this helps members see what they care about, without having to waste time searching the platform; a core value of not maximizing empty engagement.Speaking your language: a new language selector & translationsWe implemented a language selector and are in the active process of translating the platform to languages other than English! Thanks to Laura, Nicole and Aapeli for their work on the functionality, as well as the many folks helping out on translation!If you'd like to see the platform be translated to your own language, please apply to volunteer for our Translator Position! You can contribute as much or as little as you want. Every bit helps!We have rolled out significant new functionality to make the platform safer:Nicole built the frontend functionality to block (and unblock) users, while Yannic helped fix up some lingering backend functionality for that;Colleen added report flags to event cards; andunder the hood, Aapeli, Jesse and Rafael continue to work on many different moderation and admin features to help combat abuse and make the platform safer for everyone!We also updated our policies and now forbid nudism and shared sleeping surfaces (such as a shared bed). The change was prompted by the significant moderation burden caused by regular issues with this subgroup of the community. This was a decision made by the Board in order to promote safety on the platform and to set clear expectations for everyone. Thanks to the volunteers who spearheaded this difficult change!While we actively work on many features and improvements to the platform, we aren't always great at making those changes easy to see and notice. Sometimes people even criticize us for not making enough progress.To help communicate what we are doing and when, Chris has spearheaded an effort to produce and maintain a public roadmap. This is a great resource to learn about what we're working on and what to look forward to!Aapeli with assistance from Nicole, built out an  system: if you have set your status to hosting but haven't logged in in many months, we will occasionally send you a notification to ask if you are still interested in hosting.This helps us get a better idea of whether you would respond to a prospective surfer coming into town. In the next few weeks we'll be gradually rolling out this new functionality. We hope it helps reduce the number of well-thought out requests sent by surfers to hosts that never read their notification in the first place.In order to execute on our new strategy and marketing plans including this huge launch, we assembled a new team that we call . This new team is in charge of marketing & branding (incl. social media, the blog, landing pages), safety & support (moderation, policies, verification, etc), community building (community creation, engagement, feedback, etc), volunteer recruitment and general operations within Couchers. They did a phenomenal job brainstorming, writing, planning, coordinating, and executing this launch.Our Engineering and Product teamOur engineers, UI/UX designers and other software-focused volunteers continue to work hard on our Engineering and Product team.Our rehauled branding & marketing strategyOur new strategy is centered around a commitment to being the safest, healthiest, and most active couch surfing community.From our founding, we have always been defined by our competition: being the non-profit and open-source alternative to CouchSurfing.com™ that takes our community seriously and is here to fix the structural problems with other platforms. This has been an incredibly effective strategy, and it has helped us seed the community with the right crowd: a lot of veteran couch surfers with the couch surfing spirit.As our community continues to grow and mature, it's time to stop living in the shadow of another platform and start living up to our own identity and values. This is something the volunteer team and Board have been thinking about a lot, and this has steered our priorities since the start of the year. When you look back at our original plan, it's clear that everything we identified as the issues then were different aspects of community health and safety.In the coming months we are going to work on updating our messaging across the board and further building out our public landing pages to help new users learn about the platform and community.Over the next weeks the volunteer teams and Board will be working together to define our concrete priorities and roadmap for the rest of the year. We have brainstormed many ideas and have a rough path forward, but will work on refining it further. We will certainly concentrate a large part of our efforts on building a native mobile app, as well as allocating engineering resources to help our CouchOps team with their marketing and social media efforts.We have plans to build infrastructure to better measure the impact of what we do, taking a more metric-driven approach to features. The community is still at a size where it's hard to make strong statements about the impact of individual changes, but we believe that getting better visibility into what the experience is like for users is key.Couchers.org is only possible due to our amazing and dedicated volunteer team. We're so appreciate of our volunteers who believe in the vision of a non-profit, open-source, safe and active couch surfing community and donate their free time to make it a reality.To everyone mentioned in this post, to all our past volunteers who got us where we are today, and to all our future volunteers to come―thank you!Finally we also want to thank everyone in our community for sticking with us through many years of hard work to get to this point!Want to help Couchers be even more amazing?!Want to hang with other motivated travelers and help our community thrive? We can always use more help!Specifically we could really use help with:Hype us up on social mediaFollow us on the socials, and like+share religiously 🙏We have set a fundraising goal of raising $5000 this year. Help us reach this goal and keep the servers running by donating to our non-profit. You'll also get a fun badge!Written by Aapeli. Published on 2025/07/01]]></content:encoded></item><item><title>AI note takers are flooding Zoom calls as workers opt to skip meetings</title><link>https://www.washingtonpost.com/technology/2025/07/02/ai-note-takers-meetings-bots/</link><author>tysone</author><category>hn</category><pubDate>Wed, 2 Jul 2025 18:05:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Stop Killing Games</title><link>https://www.stopkillinggames.com/</link><author>MYEUHD</author><category>hn</category><pubDate>Wed, 2 Jul 2025 16:45:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Features of D That I Love</title><link>https://bradley.chatha.dev/blog/dlang-propaganda/features-of-d-that-i-love/</link><author>vips7L</author><category>hn</category><pubDate>Wed, 2 Jul 2025 16:45:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is a beginner-friendly post exploring some of my favourite parts of the D programming language, ranging from smaller quality of life stuff, to more major features.I  talk much about D’s metaprogramming in this post as that topic basically requires its own dedicated feature list, but I still want to mention that D’s metaprogramming is world class - allowing a level of flexibility & modelling power that few statically compiled languages are able to rival.I’ll be providing some minimal code snippets to demonstrate each feature, but this is by no means an in depth technical post, but more of an easy to read “huh, that’s neat/absolutely abhorrent!” sort of deal.Feature - Automatic constructorsIf you define a struct (by-value object) without an explicit constructor, the compiler will automatically generate one for you based on the lexical order of the struct’s fields.Very handy for Plain Old Data types, especially with the semi-recent support for named parameters.Feature - Design by contract“in” assertions to confirm that the function’s parameters are valid.“out” assertions to confirm that the function’s return value is in a valid state.Additionally you can attach “invariants” onto structs and classes. Invariants are functions that run at the start and end of every  member function, and can be used to ensure that the type is always in a valid state.Let’s start off with a contrived example of invariants:Now let’s rewrite the above type to use “in” contracts instead, with an extra function to show off “out” contracts:This can allow for an easy self-descriptive validation pattern for consumers/readers of your code, as well as an easy to implement self-checking mechanism for types that have complex internals.Anecdotally I find this to be an underutilised feature of D, and it’s one I like to make use of a lot in my own code.Syntax - The dollar operatorA lot of languages do not provide a shorthand syntax for referencing the length of an array, which can sometimes lead to awkward looking code when e.g. slicing arrays (any Go enjoyers here?).D provides the dollar operator, which is a shorthand syntax for referencing the length of something.Structs and classes can even overload this operator.D compilers provide an interpreter for the language which allows a very large amount of D code to be ran at compile time, as-is, without any special marking or other weirdness to go with it.Generally, anywhere where the language requires a compile-time constant is a place where CTFE will transparently come into play.This feature has a lot of different practical applications, and can allow for much cleaner, robust code than hardcoding precomputed values.Since a lot of use cases relate to metaprogramming I’ll leave the topic here, but CTFE is an extremely instant example of D’s unusual feature set.Feature - Built-in unittestsD has direct support for defining unittests, and even allows you to override the built-in test runner for something more robust (such as with the unit-threaded library).D code usually bundles unittests and normal code within the same file, rather than splitting them out into separate files as with most other languages:This extremely low-friction barrier for writing tests is a godsend for motivating people to write even the most minimal of tests.Of course if you have more complex needs then the option to have a proper testing framework + structure is still available to you, but the vast majority of D code I’ve seen simply uses  blocks, optionally with a library that provides a better test runner.Feature - Exhaustive switch statementsD provides a  statement which has an autogenerated  case that will immediately crash the program if its taken.This allows you to define a switch that will always alert you if a new value needs to be added, or if an invalid value was somehow passed into it.Additionally, if you use a  with an  value, then a compile-time check is triggered to ensure that every value within the  type has been declared, making it impossible to forget to add a new case when the enum is modified.Syntax - Parenthesis omissionD allows you to omit parentheses when calling functions in multiple contexts.When calling a function with no parameters, you can omit them:(Marginally related) When calling a function with 1 parameter, you may use assignment syntax instead:When passing a single template parameter which consists of only 1 lexical token, you may omit the parenthesis:This can do wonders for readability.UFCS allows call chains to be “inverted” by allowing freestanding functions to be used as if they were a member of their first parameter.In other words:  can be rewritten as .The two following snippets are completely equivalent in function, except the second snippet uses UFCS to provide a more clean look.Feature - Scoped & Selective ImportsD supports limiting imports to a specific scope, whether that be a singular if-statement, an entire function, an entire struct/class, etc.D will also allow you to selectively import symbols from other modules, instead of polluting your lookup scope with a ton of unrelated stuff - also helps increase comprehension of the codebase.While it may seem like clutter and extra effort, in the long run this allows for:Making it easy for newcomers to understand where certain functions are coming from.Allows for code to become “portable” between files since the code can carry most of its external dependencies inside of itself, making refactoring a bit easier.Feature - Built-in documentation generatorFinally, D has a built-in documentation generator with a relative standard, easy to read format.There’s also a handful of documentation tools that are detached from the built-in one since the default generated output is a bit lacklustre ( I’m plugging my custom tool here).Here’s a relatively extreme example from one of my personal projects, to get an idea of the basic format:Here’s an example from the standard library, which has minor usage of documentation macros:I tried to focus more on the more simpler day-to-day features, with only a splattering of the bigger more complicated stuff.Hopefully this provides some insight on the wacky-yet-wonderful feature set that D provides.]]></content:encoded></item><item><title>Sony&apos;s Mark Cerny Has Worked on &quot;Big Chunks of RDNA 5&quot; with AMD</title><link>https://overclock3d.net/news/gpu-displays/sonys-mark-cerny-has-worked-on-big-chunks-of-rdna-5-with-amd/</link><author>ZenithExtreme</author><category>hn</category><pubDate>Wed, 2 Jul 2025 16:10:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: CSS generator for a high-def glass effect</title><link>https://glass3d.dev/</link><author>kris-kay</author><category>hn</category><pubDate>Wed, 2 Jul 2025 15:51:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICEBlock climbs to the top of the App Store charts after officials slam it</title><link>https://www.engadget.com/social-media/iceblock-climbs-to-the-top-of-the-app-store-charts-after-officials-slam-it-004319963.html</link><author>doener</author><category>hn</category><pubDate>Wed, 2 Jul 2025 15:47:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[US government officials have condemned ICEBlock and recentcoverage of it, leading to more people hearing about its existence and downloading it from the App Store. Now the application, which allows users to add a pin on a map to show where ICE agents have recently been spotted, has climbed to the to the top of Apple's App Store charts. It's currently the number one free social networking app in the US and the third most downloaded free app overall. piece talked about how the app's developer, Joshua Aaron, launched it in early April after seeing the Trump administration crack down on immigration. When the piece went live, Aaron said the app had 20,000 users, many of whom live in Los Angeles, where ICE has been raiding neighborhoods. In addition to letting users pin ICE agent locations on a map, the app also gives them a way to add notes, such as what the agents are wearing or what car they're driving. Any user within a five mile radius of the sighting will get an alert.But White House press secretary Karoline Leavitt suggested that the  piece was "an incitement of further violence against... ICE officers" when asked to respond to the report on the podium. She said that there's been a 500 percent increase against ICE agents who are just "trying to do their jobs and remove public safety threats from... communities." ICE acting Director Todd M. Lyons also issued a statement, saying that the app paints a target on federal law enforcement officers' backs. " is willfully endangering the lives of officers who put their lives on the line every day and enabling dangerous criminal aliens to evade US law," he continued. "Is this simply reckless 'journalism' or overt activism?"Meanwhile, US Homeland Security Secretary Kristi Noem and US Attorney General Pam Bondi both said the government is going after Aaron. "He's giving a message to criminals where our federal officers are," Bondi said. "...we are looking at it, we are looking at him, and he better watch out, because that's not a protected speech. That is threatening the lives of our law enforcement officers throughout this country."'Aaron told  that ICEBlock doesn't collect personal data, such as device IDs and IP addresses, which  has confirmed in a test. The app is only available on iOS, because it would have to collect information on Android that could put people at risk.]]></content:encoded></item><item><title>Firefox 120 to Firefox 141 Web Browser Benchmarks</title><link>https://www.phoronix.com/review/firefox-benchmarks-120-141</link><author>mikece</author><category>hn</category><pubDate>Wed, 2 Jul 2025 15:11:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For those curious about the direction of Mozilla Firefox web browser performance over the past year and a half, here are web browser benchmarks for every Firefox release from Firefox 120 in November 2023 through the newest Firefox 140 stable and Firefox 140 beta releases from a few days ago. Every major Firefox release was benchmarked on the same Ubuntu Linux system with AMD Ryzen 9 9950X for evaluating the performance and memory usage of this open-source web browser.Following the recent Firefox 141 beta benchmarks looking at the lower RAM usage on Linux, I got carried away and decided to benchmark every Firefox release going back to Firefox 120 that debuted in November of 2023. Firefox 120 was the breaking point since Firefox 119 and prior ended up running into issues with Selenium / Gecko web driver for automating the benchmarks. So due to that breakage, Firefox 120 was the old cut-off but still a useful exercise in seeing the performance of Firefox on Linux over roughly the past nearly two years.The release builds of every major Firefox release from Firefox 120 through Firefox 141 Beta were tested. In the case of Firefox 125, Firefox 125.0.1 was used since Firefox 125.0 binaries were removed due to problems. In addition to looking at the Firefox performance across a variety of web browser benchmarks, the RAM usage was also monitored for reference.The same AMD Ryzen 9 9950X desktop system running Ubuntu 25.04 was used for collecting all of these fresh Mozilla Firefox web browser benchmarks. The testing is very straight-forward so let's get right to it.]]></content:encoded></item><item><title>Gene therapy restored hearing in deaf patients</title><link>https://news.ki.se/gene-therapy-restored-hearing-in-deaf-patients</link><author>justacrow</author><category>hn</category><pubDate>Wed, 2 Jul 2025 15:03:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Exploiting the IKKO Activebuds “AI powered” earbuds (2024)</title><link>https://blog.mgdproductions.com/ikko-activebuds/</link><author>ajdude</author><category>hn</category><pubDate>Wed, 2 Jul 2025 14:06:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[So my journey with these earbuds started after I saw them on this Mrwhosetheboss video about pointless tech. This device seems to be also popular on TikTok. My suspicions were confirmed, this runs android. So of course i went ahead and bought them.245 euros later... and they finally arrived!Before we dive further into this, unlike with rabbit, this issue has been properly reported and patched. This is also my first real blog post/disclosure so feedback is appreciated.I like how they strapped a USB-c cable to the outside of the box while there is also a smaller one inside the box. They ran out of box space it seems...I also wonder if they are legally allowed to use this OpenAI logo (probably not lol)Anyways, we aren't here for fancy boxes, lets get to the main point. The device itself boots up to a screen with the time and ChatGPT front and center.There are some other AI features available too like translations. But this isn't a review of the device, you can watch other YouTube videos about that. The ChatGPT animation looks way too similar to the actual app and OpenAI could probably get them in legal trouble for stealing their brand identity. I will also mention that the audio quality is absolute shit if you use their EQ profiles but can be upped to a usable level by tweaking the EQ curves yourself.There are also some apps available in the IKKO store, the reason that there is no google play store available is because these apps are modified specifically for the screen on the ActiveBuds, at least, that is what the CEO says about them. We will check that out in a bit. These apps include some music apps like Spotify, but also some gaming apps like, oh god, SUBWAY SURFERS BAYBEEEEEOf course all of them unbearable to navigate due to the small screen. However we can now confirm that it most definitely runs android.There is sadly no browser available to directly download other apps. And while you can open the native android settings app, clicking the build number 7 times does not enable developer mode. So i couldn't enable adb it seems. Is it locked that well? heh nope.Let's just plug it into a pc and see what happens....What the fuck, they left ADB enabled. Well, this makes it a lot easier. After sideloading the obligatory DOOM, i began checking out how the ChatGPT integration works on the backend. I first started HTTP inspecting the device, however since i couldn't enable the proper system certificates without rooting the device, i couldn't see exactly to what URL it communicated. Fortunately that wasn't really needed.Holy shit, holy shit, holy shit, it communicates DIRECTLY TO OPENAI. This means that a ChatGPT key must be present on the device!I know that this device can be rooted to get the proper certificates installed because a tool exists on all Spreadtrum/Unisoc devices which can be used to unlock the bootloader as long as companies use the default signing keys. This was indeed the case here too. However, i couldn't get past the confirmation screen as the device does not have a volume up key to confirm the unlock. I think you are able to sign your own partitions to make it flash them without an unlocked bootloader but that's a bit too advanced for my own liking.So, i went back to the drawing board and just dumped all of the apps from it with an APK extractor tool. After popping the launcher app into JADX, things immediately became concerning.The device can communicate to either of these domains.api.openai.com
Obvious, the OpenAI APIchat1.chat.iamjoy.cn
Seems to be the API for the entire device, including features not related to ChatGPT like the app store. Loading it up in a browser gives a login page.chat2.chat.iamjoy.cn
Same thing as chat1, possibly a backup server?openspeech.bytedance.com
No idea, might be a speech recogniser backup instead of whisper, haven't seen communication to this from the device.www.airdimple.cn
Seems like an OpenAI API mirror or proxy?Knowing this i went hunting for api endpoints and keys. I found a file called SecurityStringsAPI which contained encrypted endpoints and authentication keys. You might think, hey that's just base64 idiot, the most basic encoding known to mankind. And well, yeah, it is.However, there is a second stage which is handled by a native library which is obfuscated to hell. I am not going to even try to read that. Fortunately i didn't have to. I just sideloaded the app on a different device which was rooted, and well, just like the rabbit apk, it just works!Yup, that's an OpenAI key.Now, while having this access, we can also expose their (pretty funny) system prompt.The device also has another few modes, which are Angry Dan and In-Love Dan. For the angry one you need to confirm you are 18+ because it actually swears a lot.The system prompts for these are a bit more boring.I also noticed that it logs the chat to another endpoint on the chat1 domain. This is probably just to keep a log of messages since the ChatGPT API does not allow that. Possibly for some Chinese espionage? Well, possibly but not entirely, we will get to that.The headers for this request include the message, model, response and the device IMEI as the device id.I also sideloaded the store app and found out that the apps seem to be mostly ripped straight from apkpure.comAfter discovering this information, i sent an email to the security department of IKKObuds.While waiting for their response i started to investigate their companion app. Wait i forgot to tell you about that? Yeah, these earbuds have a companion app with which you can also directly interface with ChatGPT and see your previous chats from the device. So that's what the logging endpoint is used for! You bind the app by scanning a QR code from the device in the "Membership" menu.So, let's HTTP inspect this app and check out where it gets this information from.Alright so it queries this API with your account token and your device id and returns all the chats you have ever had with the device. However, after removing the account token, the request still worked? So this api has no authentication apart from the device id. I feared the worst.I found a frame in the tutorial video in which the device id wasn't properly blurred and plugged that into the api.YUP, i now had their entire demo device chat history. And as the IMEI has a certain range, you would be able to figure out the chat history of all customers, which may include sensitive details.I also added this new discovery to the email chain.While that email was waiting for a reply i checked if i could fabricate a linking QR code from a known IMEI to bind the device. (The QR code is not the IMEI itself but something encrypted) I found the API endpoint by looking at the same SecurityStringsAPI, which was less secure than i initially thought because the variable names literally expose the encrypted api endpoints (lol)Plugging in the getBindDevQrCode api in postman, i could fabricate a base64 image of the QR code with any IMEI.However, using this QR code to try and bind the device to my app resulted in an error, saying that the device has already been bound to someone else. So that has been the only good security implementation up until now.However, i lied, this is still a security/privacy issue. Why, you may ask? This exposes the username you set when creating the account for the app. However, there is no username field when creating your account. Only first and last name.I created an account with the first name as "Cheese2" and the second name as "Delight2". Turns out that the username is equal to First name + Last name. When trying to bind that device to an app after it has already been bound to another app, the response includes the name "Cheese2Delight2". Great. Doxed.So what we can do now is guess IMEI -> generate QR code -> Bind the device if not bound already, or get your full name when the device is already bound. -> Get all your chat history either way if the device is bound or not.There is an unbind_dev endpoint????Unfortunately that one actually checks account token and does not allow to unbind a random device IMEI. Phew.Hey, do you remember that logging endpoint that actually sent your chats you made with ChatGPT to their servers? This one?Yeah, that also only used the device id as authentication, so we can send arbitrary text to the companion app of anyone....I tried to send some HTML and JS through it to try and exploit the companion app, fortunately they use vue for their app and that has default HTML and JS injection security built in. But we can still send scams or something to any user.Oh hey a reply to my email!First of all, from a gmail address? Come on, actually try to have at least some professionalism. Second, OK they are actually doing something about it. (The YouTube channel mentioned is because i said that i will be making a video about this. I have all the footage for it but i hate my voice with a passion so here we are on this blog post :))Shortly after this email, they locked down the app and put out an announcement stating that the app will be in maintenance for a week.They also wanted to become a sponsor of my empty YouTube channel? What? I don't think that they understood that i would be talking about their horrible security. Anyways.The API was now non functional and displayed a maintenance message. After the service period they put out both an app update and a device update. What changed? The endpoint to get the chat history now needs a "signature" header. Which is composed of your account token, your device id, language and the current time encoded with a public/private key + a password. Anyways, it is now impossible to fetch the chats without having a valid account token. Still doesn't fix the fact that i can generate a QR code with the guessable IMEI and bind the device to an app if it hasn't been bound already. That circumvents this all. The device update broke the ChatGPT functionality from functioning on a device which is not the IkkoBuds itself. The keys remain on device and have not been rotated. So if anyone is able to figure out the broken app on another device or the key encryption system, you can still get your very own free OpenAI API key.However i just gave up at this moment, also because they never replied with anything after my last email criticizing them for leaving the keys on device. This is now a month and a half ago.So, that is it. You can still inject messages into apps of others, link devices that are not already bound to another companion app, thus leaking chat history. And leak first and last names of devices which are bound.I am giving up, but if anyone else wants this company to fix this, be my guest.Also if you liked this deep dive, consider supporting me so i will be able to convince myself that buying more strange android devices is worth it lolhttps://ko-fi.com/mgdproductionsI got this device rooted with help from @haro7zThey are now checking the device's imei before it is able to use the chatgpt integration and are now using a proxy api instead of calling directly to openai. However this proxy api doesn't require any auth and only requires the User-Agent to be set to okhttp/4.9.0 LOLThey have also FINALLY rotated their old chatgpt api key!]]></content:encoded></item><item><title>Azure API vulnerability and roles misconfiguration compromise corporate networks</title><link>https://www.token.security/blog/azures-role-roulette-how-over-privileged-roles-and-api-vulnerabilities-expose-enterprise-networks</link><author>ArielSimon</author><category>hn</category><pubDate>Wed, 2 Jul 2025 13:59:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Token Security researchers have discovered several Azure built-in roles that are misconfigured to be over-privileged - they grant more permissions than intended by Azure.In addition, we discovered another vulnerability in the Azure API that allows attackers to .Combined, these two issues create a new attack chain that lets a weak user gain access to both internal cloud assets and on-premises networks.In this report, we detail the research process that led to the discoveries, their implications, and what organizations can do to stay safe against these threats and other identity-driven attacks.Before jumping in, let’s discuss some basics.Azure’s permissions model, Azure RBAC (Role-Based Access Control), is, as the name states, based on roles.Roles are basically groups of permissions that can be assigned to principals (users, service principals, groups, etc). When granting a role to a principal, you create a Every  contains three main components: - Who is given the permissions? - Which role is assigned? What permissions does it grant? This section states the name of the role and the  and  (allow or deny) that this role is granting. - What resources is the principal given access to? The scope can be vast, such as an entire management group or subscription, or more specific, like a resource group or a single resource (e.g., a specific VM or storage account).In Azure, there are more than 400 built-in roles, which can be divided into 2 categories: - Roles that grant permissions that apply across all resources and all Azure services in the given scope (e.g., , , , etc. - Roles that grant permissions for a specific service or function in the given scope (e.g., Virtual Machine Contributor).If you assign the  role over a subscription scope, it will grant permissions to perform actions over all of the resources in this subscription, regardless if they’re storage accounts, virtual machines, or any other resource. But if you assign the Virtual Machine Contributor, it will grant these permissions only to perform actions over virtual machines in the subscription.So we can see the tradeoff here: using service-specific roles is the more secure approach, since you are granting fewer permissions, but the generic roles are easier to use since you need to manage fewer roles assignments.Let’s take a look at few roles and their permissions. See if you can tell where things go wrong.One of Azure’s built-in roles, called  is a generic role. As you’d expect, it gives read-only permissions over the resources in the chosen scope. Let’s examine its :
{
  "assignableScopes": [
    "/"
  ],
  "description": "View all resources, but does not allow you to make any changes.",
  "id": "/providers/Microsoft.Authorization/roleDefinitions/acdd72a7-3385-48ef-bd42-f606fba81ae7",
  "name": "acdd72a7-3385-48ef-bd42-f606fba81ae7",
  "permissions": [
    {
      "actions": [
        "*/read"
      ],
      "notActions": [],
      "dataActions": [],
      "notDataActions": []
    }
  ],
  "roleName": "Reader",
  "roleType": "BuiltInRole",
  "type": "Microsoft.Authorization/roleDefinitions"
}
As we can see in the  property, the permission given is , which means it lets you perform any read action in the given scope.*Note that this does not include data actions (reading data objects like files in storage accounts, key vault secrets, etc). Those require different, service-specific sensitive permissions.Okay, so a generic role giving generic read permissions, that makes sense.What about the permissions of service-specific roles?
{
  "assignableScopes": [
    "/"
  ],
  "description": "Can read workbooks.",
  "id": "/providers/Microsoft.Authorization/roleDefinitions/b279062a-9be3-42a0-92ae-8b3cf002ec4d",
  "name": "b279062a-9be3-42a0-92ae-8b3cf002ec4d",
  "permissions": [
    {
      "actions": [
        "microsoft.insights/workbooks/read",
        "microsoft.insights/workbooks/revisions/read",
        "microsoft.insights/workbooktemplates/read"
      ],
      "notActions": [],
      "dataActions": [],
      "notDataActions": []
    }
  ],
  "roleName": "Workbook Reader",
  "roleType": "BuiltInRole",
  "type": "Microsoft.Authorization/roleDefinitions"
}
Workbook Reader role definitionIf we analyze the  property, we can see that as the description states, it grants read permissions to a few workbook-related objects. So a service-specific role that grants access to a specific service! So far so good.Managed Applications ReaderNow, let’s check the Managed Applications Reader role, which its description is “Lets you read resources in a managed app and request JIT access.”
{
  "assignableScopes": [
    "/"
  ],
  "description": "Lets you read resources in a managed app and request JIT access.",
  "id": "/providers/Microsoft.Authorization/roleDefinitions/b9331d33-8a36-4f8c-b097-4f54124fdb44",
  "name": "b9331d33-8a36-4f8c-b097-4f54124fdb44",
  "permissions": [
    {
      "actions": [
        "Microsoft.Resources/deployments/*",
        "Microsoft.Solutions/jitRequests/*",
        "*/read"
      ],
      "notActions": [],
      "dataActions": [],
      "notDataActions": []
    }
  ],
  "roleName": "Managed Applications Reader",
  "roleType": "BuiltInRole",
  "type": "Microsoft.Authorization/roleDefinitions"
}
We can see it has access to deployments, jitRequests, and… So this role, which is supposed to grant access to read managed apps and JIT access, actually also allows the user to read every Azure resource?This is clearly not what the description says, and certainly not what someone assigning the Managed Applications Reader role would expect.Essentially, the role’s name and description are misleading the user into thinking the role grants specific permissions, when in fact it grants generic permissions to every resource.I saw this and thought to myself, okay, this is just a read permission... how bad can it be?Well, I was seriously wrong.Let’s dive into what we can actually do with this permission and how can it be useful for an attacker.Since there are so many actions possible here, I divided them into three categories and gave some examples for each.Automation Accounts, Deployment scripts, Web applications - This one really surprised me: this permission actually allows you to read source code and environment variables of scripts and applications. The common thing among the three services I listed here, is that they all interact with your environment, which makes them very likely to contain credentials and secrets!:Storage accounts, container registries, databases - Enumerating all instances and their metadata to find sensitive data spots - Helps identify resources that are considered critical or sensitive - Find DB exports, storage account backups and more - Know about who can access what. Useful for planning privilege escalation pathsDiagnostics settings, alerts, and log analytics workspaces - Know what is being logged, and where, and view security alerts. Useful for OpSec and detection avoidanceNetwork configurations, network security groups, public IPs, virtual network gateways - Further plan attack paths and network advancements - Listing all vaults and their metadataSo you might think to yourself - Okay that’s cool… but I don’t use the Managed Applications Reader role, so I am safe, right?Well, think again.After analyzing all Azure built-in roles, I found that this problem (having an un-needed  permission, basically including the  role) recurs in 
Microsoft.OperationalInsights/workspaces/analytics/query/action 
Microsoft.OperationalInsights/workspaces/search/action
Microsoft.Support/*
Log Analytics Contributor
Microsoft.ClassicCompute/virtualMachines/extensions/*
Microsoft.ClassicStorage/storageAccounts/listKeys/action
Microsoft.Compute/virtualMachines/extensions/*
Microsoft.HybridCompute/machines/extensions/write
Microsoft.Insights/alertRules/*
Microsoft.Insights/diagnosticSettings/*
Microsoft.OperationalInsights/*
Microsoft.OperationsManagement/*
Microsoft.Resources/deployments/*
Microsoft.Resources/subscriptions/resourcegroups/deployments/*
Microsoft.Storage/storageAccounts/listKeys/actionApp Compliance Automation Administrator
Microsoft.AppComplianceAutomation/*
Microsoft.Storage/storageAccounts/blobServices/write
Microsoft.Storage/storageAccounts/fileservices/write
Microsoft.Storage/storageAccounts/listKeys/action
Microsoft.Storage/storageAccounts/write
Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action
Microsoft.Storage/storageAccounts/read
Microsoft.Storage/storageAccounts/blobServices/containers/read
Microsoft.Storage/storageAccounts/blobServices/containers/write
Microsoft.Storage/storageAccounts/blobServices/read
Microsoft.PolicyInsights/policyStates/queryResults/action
Microsoft.PolicyInsights/policyStates/triggerEvaluation/action
Microsoft.Resources/resources/read
Microsoft.Resources/subscriptions/read
Microsoft.Resources/subscriptions/resourceGroups/read
Microsoft.Resources/subscriptions/resourceGroups/resources/read
Microsoft.Resources/subscriptions/resources/read
Microsoft.Resources/subscriptions/resourceGroups/delete
Microsoft.Resources/subscriptions/resourceGroups/write
Microsoft.Resources/tags/read
Microsoft.Resources/deployments/validate/action
Microsoft.Security/automations/read
Microsoft.Resources/deployments/write
Microsoft.Security/automations/delete
Microsoft.Security/automations/write
Microsoft.Security/register/action
Microsoft.Security/unregister/actionApp Compliance Automation ReaderManaged Application Contributor Role
Microsoft.Solutions/applications/*
Microsoft.Solutions/register/action
Microsoft.Resources/subscriptions/resourceGroups/*
Microsoft.Resources/deployments/*Managed Application Operator Role
Microsoft.Solutions/applications/read
Microsoft.Solutions/*/actionManaged Application Operator Role
Microsoft.Solutions/applications/read
Microsoft.Solutions/*/actionManaged Applications Reader
Microsoft.Resources/deployments/*
Microsoft.Solutions/jitRequests/*
Microsoft.AlertsManagement/alerts/*
Microsoft.AlertsManagement/alertsSummary/*
Microsoft.Insights/actiongroups/*
Microsoft.Insights/activityLogAlerts/*
Microsoft.Insights/AlertRules/*
Microsoft.Insights/components/*
Microsoft.Insights/createNotifications/*
Microsoft.Insights/dataCollectionEndpoints/*
Microsoft.Insights/dataCollectionRules/*
Microsoft.Insights/dataCollectionRuleAssociations/*
Microsoft.Insights/DiagnosticSettings/*
Microsoft.Insights/eventtypes/*
Microsoft.Insights/LogDefinitions/*
Microsoft.Insights/metricalerts/*
Microsoft.Insights/MetricDefinitions/*
Microsoft.Insights/Metrics/*
Microsoft.Insights/notificationStatus/*
Microsoft.Insights/Register/Action
Microsoft.Insights/scheduledqueryrules/*
Microsoft.Insights/webtests/*
Microsoft.Insights/workbooks/*
Microsoft.Insights/workbooktemplates/*
Microsoft.Insights/privateLinkScopes/*
Microsoft.Insights/privateLinkScopeOperationStatuses/*
Microsoft.Monitor/accounts/*
Microsoft.OperationalInsights/workspaces/write
Microsoft.OperationalInsights/workspaces/intelligencepacks/*
Microsoft.OperationalInsights/workspaces/savedSearches/*
Microsoft.OperationalInsights/workspaces/search/action
Microsoft.OperationalInsights/workspaces/sharedKeys/action
Microsoft.OperationalInsights/workspaces/storageinsightconfigs/*
Microsoft.AlertsManagement/smartDetectorAlertRules/*
Microsoft.AlertsManagement/actionRules/*
Microsoft.AlertsManagement/smartGroups/*
Microsoft.AlertsManagement/migrateFromSmartDetection/*
Microsoft.AlertsManagement/investigations/*
Microsoft.AlertsManagement/prometheusRuleGroups/*
Microsoft.Monitor/investigations/*
Microsoft.OperationalInsights/workspaces/search/actionResource Policy Contributor
Microsoft.Authorization/policyassignments/*
Microsoft.Authorization/policydefinitions/*
Microsoft.Authorization/policyexemptions/*
Microsoft.Authorization/policysetdefinitions/*
Microsoft.PolicyInsights/*
Microsoft.Resources/deployments/*
Microsoft.Support/*
This risk exists in every user, service principal, managed identity, or group members that are assigned one of these seemingly innocent service-specific roles.As we can see in the table, some of the roles have more specific read permissions in addition to the , like the Microsoft.Solutions/applications/read permission in Managed Application Operator Role, which is already included in the * expression. This shows that one of the permissions is redundant, and may point to it being added by mistake or by laziness.The case of App Compliance Automation Reader is even more absurd, since it only has the * permission, and is essentially identical to the all mighty and powerful generic  role.So this is pretty bad… but can we exploit it even further?I wanted to make this attack scenario even stronger, and find more quirks that are possible using the  permission.So I took a look its documentation:So we understand that identities with read permissions should not be allowed to read secrets (which makes sense because those secrets can then be used to elevate privileges to more than read-only, access more resources, etc.)Using this really nice website, I saw that there are  (!) Azure actions that are included in the  expression (every operation that ends with ‘/read’).If I can find one action, out of those 9,618, that will allow me to leak a secret, I will have a serious vulnerability here!After going through many permissions in the list, I found one that piqued my interest:What is a VPN link? What is a connection? What is a shared key? I don’t know, but it has the word ‘key’ in it, so it must be interesting :)So we have an API call that retrieves some sort of a secret even if I only have read permissions. But why is that happening? What is the mistake that the Azure developer made here?In Azure, API calls are implemented with different HTTP methods. For example, the Virtual Machines - List API call is implemented with , and Virtual Machines - Install Patches is implemented with . That makes sense, because the  API requires data from the user (a body in the request), while the  only retrieves data from the server, and does not require any data from the user.But what I found out through some blackbox research, documentation reading, and a lot of trial and error, is that Azure chose to enforce permissions by varying the HTTP methods used in the API requests. Let me explain:It seems that users with read permissions alone (like *can issue  requests to the API, but will get denied access if they attempt to issue  requests.Regular read operations, like  are implemented with a  as we’ve seen, while operations that read sensitive values, such asStorage Accounts - List Keys or Database Accounts - List Connection Strings, are implemented with  requests - even though the request body is empty. This is to make sure that permissions enforcement is in place, and identities with read permissions alone would not be able to access those sensitive APIs.To prove this, I performed an API call to a URL that doesn’t exist, using a read-only identity. When I issue a GET request, I get  error. But when I issue a POST request to the same non-existent URL, I get   error. This further proves that permission enforcement is determined by the HTTP method, and not by the checking specific API call and whether or not I have access to it.So why is that problematic? Because when you choose to design your software in a way that doesn’t make sense and isn’t intuitive, your developers are bound to make some critical mistakes…My assumption is that at some point, some Azure developer must have accidentally implemented an API call that retrieves a secret with the method that makes the most sense, which is GET, because there is no body needed in the request. And by doing so, they created a vulnerability…If we take a look at the API call that we found earlier, we see that our theory was right! It was accidentally implemented with a GET, allowing read-only users to fetch the key!So now we have a vulnerability. The only thing that is left is to find out what that shared key is…VPN Gateway is an Azure service that acts as a VPN, allowing customers to connect networks over the internet. Organizations mainly use this service to support hybrid environments (connecting cloud and on-premise), as well as connecting between on-premise sites (aka Site-to-Site, or S2S). Individual users can also connect to it, for scenarios such as working remotely (aka Point-to-Site, or P2S).P2S connection types require additional authentication (certificate/radius server/Entra ID login).But a Site-to-Site (S2S) connection type requires only the pre-shared key (PSK), which is a password that is shared between the VPN devices of each site, and the Azure VPN Gateway service. And yes, that’s the key we can fetch using our vulnerability!The attacker compromises a weak identity (identity with read permissions or one that is assigned one of the many over-privileged roles we listed earlier).The attacker fetches the VPN Gateway pre-shared key.Using the key, the attacker connects to the S2S connection and accesses internal networks, including VPCs and on-premise networks that are connected to the same Azure VPN Gateway.The video demonstrates that when a principal with no privileges is assigned the  role, which is only supposed to grant access to read logs but as we know by now is over-privileged, it has permissions to fetch the VPN pre-shared key, which is then used to connect to the VPN.With this access, attackers can create a “rogue site”, essentially granting them access to cloud resources, other sites, and secure networks of the target organization.Depending on the configuration of the VPN Gateway, this may work only when the connection attempt is originating from the IP address configured in the Azure VPN gateway.In that case, an attacker that has some on-premise foothold can use this trick to access sensitive cloud infrastructure and data.There are even more sensitive values you can access using the Reader privileges. Excellent research published by Binary Security shows how you can escalate your Reader privileges in Azure API Management service by fetching subscription keys and SSO tokens, effectively resulting in a full takeover of the service.After reporting this issue to Microsoft, their response was that this is a ‘low severity’ security issue and they decided to not fix it. I later noticed some major documentation changes based on my report: All 10 over-privileged roles’ documentations were added the following sentence:So they chose to fix the documentation instead of fixing the actual issue, which still endangers customers.This was acknowledged by Microsoft as an ‘Important’ severity vulnerability, and the issue was fixed. They also awarded me with a US$7,500 bounty award.I was happy to find out that the fix was not changing the API HTTP method to POST.In this newly created documentation page that explains Azure VPN permissions, it is stated that to fetch/update the PSK, you now need to have the Microsoft.Network/connections/sharedKey/action permission:Mitigations and recommendationsAudit the use of the problematic rolesAs we saw, the over-privileged roles issue is not going to be fixed. Refrain from using those roles in your environment, and use alternatives.Use particular and limited scopesInstead of assigning roles on a wide scope, such as an entire management group or subscription level, limit them only to the specific resource that the principal needs access to, or to a resource group if access to multiple resources is needed.Instead of using built-in roles that are not managed by you, use custom roles and grant only the needed permissions. Replace your current role assignments (at least of the problematic roles mentioned here) with custom roles that have fine-grained permissions.Securing cloud environments and their identities is not easy.The shared responsibility model, which is adopted and presented by all major cloud providers, clearly states which security tasks are the cloud provider’s responsibility, and which are the customer’s. But the issues we discussed here are in the gray area: when the cloud provider is giving you a service that is supposed to help you with identities and permissions management, but in fact misleads you into making dangerous decisions, who is to blame? Is it the cloud provider who caused you to create the security issue, or is it you, who actually created it?There is no easy answer here, but one thing is clear - don’t fully and blindly trust the services that are given to you. Always double check, and be proactive and vigilant about the security of your organization.When you have many identities and big infrastructure, this becomes a real challenge. When you have to question every role, permission, and API call, things can get complicated. But luckily, there is a solution.To learn more about Token Security and how we can help secure your Azure environment (and many more), book a demo here.I’m Ariel Simon, a security researcher from the Token research team, primarily focused on uncovering vulnerabilities and finding new attack techniques in cloud environments.Feel free to contact me on LinkedIn or via email: ariels@token.security.]]></content:encoded></item><item><title>Private sector lost 33k jobs, badly missing expectations of 100k increase</title><link>https://www.cnbc.com/2025/07/02/adp-jobs-report-june-2025.html</link><author>ceejayoz</author><category>hn</category><pubDate>Wed, 2 Jul 2025 13:40:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[People visit booths set up by the City of Sunrise and their police department at the Mega JobNewsUSA South Florida Job Fair at the Amerant Bank Arena on April 30, 2025 in Sunrise, Florida.Joe Raedle | Getty ImagesPrivate sector hiring unexpectedly contracted in June, payrolls processing firm ADP said Wednesday, in a possible sign that the economy may not be as sturdy as investors believe as they bid the  back up to record territory to end the month.Private payrolls lost 33,000 jobs in June, the ADP report showed, the first decrease since March 2023. Economists polled by Dow Jones forecast an increase of 100,000 for the month. The May job growth figure was revised even lower to just 29,000 jobs added from 37,000."Though layoffs continue to be rare, a hesitancy to hire and a reluctance to replace departing workers led to job losses last month," Nela Richardson, ADP's chief economist, said in a press release published Wednesday morning.To be sure, the ADP report has a spotty track record on predicting the subsequent government jobs report, which investors tend to weigh more heavily. May's soft ADP data ended up differing significantly from the monthly jobs report figures that came later in the week.This week, the government's nonfarm payrolls report will be out on Thursday with economists expecting a healthy 110,000 increase for June, per Dow Jones estimates. Economists are expecting the unemployment rate to tick higher to 4.3% from 4.2%. Some economists could revise down their jobs reports estimates following ADP's data.Weekly jobless claims data is also due Thursday, with economists penciling in 240,000. This string of labor stats comes during a shortened trading week, with the market closing early on Thursday and remaining dark on Friday in honor of the July Fourth holiday.Service roles hit hardestThe bulk of job losses came in service roles tied to professional and business services and health and education, according to ADP. Professional/business services notched a decline of 56,000, while health/education saw a net loss of 52,000.Financial activity roles also contributed to the month's decline with a drop of 14,000 on balance.But the contraction was capped by payroll expansions in goods-producing roles across industries such as manufacturing and mining. All together, goods-producing positions grew by 32,000 in the month, while payrolls for service roles overall fell by 66,000.The Midwest and Western U.S. saw the strongest contractions in June, declining by 24,000 and 20,000, respectively. Meanwhile, the Northeast shed 3,000 roles. The Southern U.S. was the sole region tracked by the ADP to see payrolls expand on net in the month, recording an increase of 13,000 positions.The smallest firms tended to see more job losses in the month than their larger counterparts. In fact, businesses with more than 500 employees saw the biggest payroll growth in the month with an increase of 30,000, per ADP. By comparison, businesses with fewer than 20 employees accounted for 29,000 lost roles on net.Annual income growth decreased modestly from May for both job stayers and hoppers. The rate of pay increase for those staying in their jobs ticked down to 4.4% from 4.5%, while those getting new roles slid to 6.8% from 7%.The S&P 500 is up more than 4% for the year, posting a stunning comeback in the second quarter after worries about President Donald Trump's tariff fights nearly sent the benchmark into a bear market.Clarification: The ADP report issued Wednesday referred to June data. That was not clear in an earlier version.]]></content:encoded></item><item><title>What I learned gathering nootropic ratings (2022)</title><link>https://troof.blog/posts/nootropics/</link><author>julianh65</author><category>hn</category><pubDate>Wed, 2 Jul 2025 13:29:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[All data and code can be found here.
How to interpret these ratings
#0 means a substance was totally useless, or had so many side effects you couldn’t continue taking it.1 - 4 means subtle effects, maybe placebo but still useful.5 - 9 means strong effects, definitely not placebo.In order of importance (I think):Lack of random allocationThe people who entered their rating on my recommender system were not randomly assigned to try specific nootropics. Thus we can expect a (usually positive) correlation between “I’m likely to try nootropic A” and “Nootropic A might work on me”.This means that my estimations don’t represent the ratings a random person would get on average (which would usually be lower); they’re instead a prediction of the rating that a person would give to a nootropic if they decided to take it organically. Take this into account when transferring the results to yourself.You take a pill. It makes you feel good. You go on a website which asks you how good the pill is. You say it’s awesome. Little did you know that it was, in fact, merely good.Take this into account when reading these ratings, especially if improving your mood is not your main goal.Lack of control and blindingAll these biases sure seem inconvenient to estimate some “true rating”, but maybe they are not a problem if we just want to compare nootropics? Perhaps in some cases, but not always: lack of random allocation probably creates more bias for medications like SSRIs, which are usually prescribed to people with depression, and self-reported ratings inflation or placebo effect may be especially relevant for substances like Psilocybin, which can produce visual effects, or for hyped-up nootropics.I’m mostly going to ignore these issues in the rest of this article, but keep them in mind and use your best judgment when comparing ratings. This also means that you shouldn’t use this data for anything more serious than reducing your nootropics search space.
Mean rating for each nootropic
#Here are the results (click to see all nootropics):
Probability of positive effect
#Given the scale I used, the  is not so easy to interpret. Another metric I estimated was the probablity that the effect of a nootropic on a user was positive. For my scale, 0 corresponds to a neutral or negative effect, and higher ratings correspond to more-or-less confidence in a positive effect. Here are the results (click to see all nootropics):
Probability of life-changing effect
#Here are the results (click to see all nootropics):
Usefulness for different usages
#With my data, identifying the usefulness of a nootropic for different use cases seems hard. I’ll set this aside for future investigation (if it’s possible at all).The risks of prescribed medications such as Adderall, SSRIs and Modafinil are quite well-documented, but information on the risks of weirder nootropics is scarce.For instance, what would you guess is the probability of becoming tolerant to Phenylpiracetam (apparently between 10 and 20%)? Of becoming addicted to Kratom (apparently between 15 and 25%)?Click to see all nootropics:Click to see all nootropics:Click to see all nootropics:Click to see all nootropics:EDIT: As pointed out by a commenter, the estimated probabilities of long term side effects are somewhat surprising. I’m not completely sure what people had in mind when entering “long term side effects”, and thus I’m not completely sure how to interpret these probabilities.Click to see all nootropics:
Lifestyle is a strong nootropic
#Among the different sport categories, weightlifting is noticeably better rated (+ 0.5 points on adjusted mean) and is actually among the very best nootropics in my database. Furthermore, very few issues are reported. This is very impressive, and maybe you should consider trying it. You may be wondering why you should trust this self-reported data if you don’t trust your gym bros friends who can’t stop talking about how weightlifting changed their life. I’m just saying that there are a lot of people saying the same things in my data. Maybe they’re all the same gym bros, but maybe it means that you should start taking them seriously.If sport and sleep are the nootropic low-hanging fruits, diets are the fruits you can maybe reach on tiptoes. For instance, the Paleo diet, with a mean rating of 5.4, is in the top-20, and Intermittent Fasting, with a mean rating of 5, is in the top-30.Low carbs diets (Keto, Carnivore, Paleo) are rated much higher than Vegetarian or Vegan diets, though the Vegan diet is first for probability of changing your life, with around 5%[3-8% 95%] probability, similar to the Paleo diet.Comparing issues reported, people often stop the Carnivore and Keto diets because of side effects, with a probability between 10 and 20%, much higher than the Paleo diet (between 2 and 10%), and somewhat higher than the Vegetarian and Vegan diets (between 7 and 17%).The Paleo diet seems to be the winner here, though I’m wondering how its particular branding might be skewing results.
Other lifestyle interventions
#Meditation (mean rating = 5.8), bright lights in the morning (5), cold shower (4.7), and masturbation abstinence (4.1) also got impressive to pretty good ratings.
Most famous nootropics aren’t that good
#What are the first things that come to mind when you think of nootropics? Piracetam? Ashwagandha? Ginseng? Theanine? Most of these common nootropics actually got relatively poor ratings. This is compared to potent prescription-only medications like Adderall, but also to sport (see above) and to a lot of lesser-known nootropics.The plots below shows all the common-but-mediocre nootropics (red rectangle) and the uncommon-but-great nootropics (green rectangle):
Selank, Semax, Cerebrolysin, BPC-157 are all peptides, and they are all in the green “uncommon-but-great” rectangle above. Their mean ratings are excellent, but their probabilities of changing your life are especially impressive: between 5 and 20% for Cerebrolysin (which matches anecdotalreports), between 2 and 13% for BPC-157, and between 3 and 7% for Semax.So why are they so unpopular? It may be because they’re really scary. Take Cerebrolysin:Perhaps relatedly, these substances are mostly used in Russia and the former USSR and have an unclear legal status in other countries. This may also explain their unpopularity.But are peptides really that dangerous? The plots above show that the addiction probabilities are tiny and that the tolerance and long-term side effect probabilities are below 5%. Some of these substances, like Cerebrolysin, have small sample sizes, so I quickly checked the literature. A Cochrane review from 2020 found that “Moderate‐quality evidence also indicates a potential increase in non‐fatal serious adverse events with Cerebrolysin use.” with a seemingly dose-dependent effect, while the othermeta-analyses I found reported that “Safety aspects were comparable to placebo.” So it is a bit unclear, but I would be cautious, as I trust the Cochrane review more.
For other peptides, we’re not lucky enough to have a Cochrane review, but the few studies I can find tell me they’re safe: “Semax is well-tolerated with few side-effects.", “BPC-157 is free of side effects.", “BPC-157 is a safe therapeutic agent.", “BPC-157 is a safe anti-ulcer peptidergic agent." We need more data on the safety of peptides but we can already say: “Probably less dangerous than they look.”Bonus: Reddit user OilofOrigano has been collecting data on Cerebrolysin, you can check the results here. Results are quite positive (see below), though I think it was mostly collected on a subreddit dedicated to Cerebrolysin, so I would expect the results to be overly positive.

Zembrin maybe isn’t interesting?
#Of 37 kanna users, 20 used Zembrin and 17 used something else. The subgroup who used Zembrin reported a mean effectiveness of 6.88, which beats out modafinil to make it highest on the list. After ad hoc Bayesian adjustment, it was 6.72, second only to modafinil as the second most effective nootropic on the list. This really excites me - I’ve felt like Zembrin was special for a while, and this is the only case of a newer nootropic on the survey beating the mainstays. And it’s a really unexpected victory. The top eight substances in the list are all either stimulants, addictive, illegal in the US, or all three. Zembrin is none of those, and it beats them all.How can we explain the difference? Maybe the surveyed population is a bit different in my case? But the simplest answer is surely sample noise. SA’s ratings are based on 20 people who tried Zembrin and 17 people who tried non-Zembrin Kanna. My ratings are based on 45 people who tried Zembrin and 49 people who tried non-Zembrin Kanna. Where it gets more complicated is that SA has subsequent data:Based on these preliminary results, I wrote up a short page about Zembrin on my professional website, Lorien Psychiatry, and I asked anyone who planned to try it to preregister with me so I could ask them how it worked later. 29 people preregistered, of whom I was able to follow up with and get data from 22 after a few months. Of those 22, 16 (73%) said it seemed to help, 3 (14%) said it didn’t help, and another 3 (14%) couldn’t tell because they had to stop taking it due to side effects (two headaches, one case of “psychedelic closed-eye visuals”). Only 13 of the 22 people were willing to give it a score from 1-10 (people hate giving 1-10 scores!), and those averaged 5.9 (6.3 if we don’t count people who stopped it immediately due to side effects). That’s a little lower than on the survey, but this was a different population - for example, many of them in their answers specifically compared it to prescription antidepressants they’d taken, whereas the survey-takers were comparing it to nootropics. Although these findings are not very useful without a placebo control, they confirm that most people who take Zembrin at least subjectively find it helpful.Do you trust the (a bit) bigger sample size, or the preregistration? Your choice!What is the best stimulant to take if you have trouble focusing? Adderall and Dexedrine have the best ratings. The latter is perhaps more likely to change your life, and both are far above Ritalin and Modafinil. I was surprised about Dexedrine, which I didn’t know about. Still, it does have higher patient ratings than Adderall on Drugs.com and the like, and there seems to be a debate in the literature on which is more effective for ADHD. This is all from this article by Scott Alexander, which is a great explainer on the different amphetamines used to treat ADHD (and a reminder that most of these substances are quite well-studied in the literature, and that you shouldn’t base any decision on my data!).Guess where Methylphenidate is the only easily available stimulant?In the plot below, I’ve included most of the medications recommended for depression here, and most of the supplements recommended here.
More surprising, using bright light in the morning was ranked second. Interpreting these ratings is hard, as they are not specifically about depression, but this is still impressive.Most supplements users report few issues (except SAM-e and St John Wort, which have a high probability of side effects), while people using prescription medications report  of side effects. For instance, for Bupropion and SSRIs, I estimate a 30 to 40% probability of side-effects making you stop taking them. For SSRIs, there’s a 20 to 30% probability of having long-term side effects. This is quite scary.
Tianeptine users report way fewer side effects, but as a counterpart, they’re more likely to get addicted/tolerant (I wonder how much of this is explained by people getting Tianeptine over-the-counter).All racetams got almost identical pretty low ratings, except Phenyracetam, which was rated much higher (but still way below something like Modafinil).Racetams seem safe: they all got pretty much the same (low) probabilities for all issues, except for Phenylracetam, which has a tolerance probability between 10 and 20%.
How dangerous is Phenibut?
#Only 3% of users got addicted to phenibut. This came as a big surprise to me given the caution most people show about this substance. Both of the two people who reported major addictions were using it daily at doses > 2g. The four people who reported minor addictions were less consistent, and some people gave confusing answers like that they had never used it more than once a month but still considered themselves “addicted”. People were more likely to report tolerance with more frequent use; of those who used it monthly or less, only 6% developed tolerance; of those who used it several times per month, 13%; of those who used it several times per week, 18%; of those who used it daily, 36%.The figures I have are somewhat worse, but still better than I expected: I estimate a 5.5%[3-9%] chance of becoming addicted, a 6%[4-10%] chance of having long-term side effects, and a 20%[15-26%] chance of becoming tolerant.Microdosing psychedelics was quite highly rated, especially for Psilocybin: I estimate a mean rating of 5.6 [5.3-5.8] for LSD and 6[5.8-6.3] for Psilocybin, and a probability of changing your life of 6[4-8]% for LSD and 8[6-11]% for Psilocybin.But be careful: this may be an example of the limit of self-reported, unblinded, data. Indeed, Gwern has been gathering RCTs on the subject (in addition to his N=1 experiment), and most show little to no effect
, except on things like visual intensity or time perception, and sometimes some effect on self-reported mood (how can you be sad when the colors are INTENSE?). People seem to be able to trip on a placebo, so I guess this is an area where we should be careful.While I got a lot of boring results (this is reassuring!), I was really surprised by a few things:Sport (especially Weightlifting) and sleep were really highly rated. More generally all “lifestyle interventions” were rated way higher than most famous nootropics like Piracetam or Rhodiola Rosea.Peptides like Semax or Cerebrolysin were really highly rated, but seem poorly known outside of Russia.Tianeptine was rated much higher than any other antidepressant. What is going on here?Zembrin maybe isn’t better than normal Kanna, as suggested in ACX 2020 Nootropics survey, which would be disappointing.I’m sure there are a lot of interesting things I missed in my data. Feel free to explore them.Subscribe to see new posts:]]></content:encoded></item><item><title>Cloudflare Introduces Default Blocking of A.I. Data Scrapers</title><link>https://www.nytimes.com/2025/07/01/technology/cloudflare-ai-data.html</link><author>stephendause</author><category>hn</category><pubDate>Wed, 2 Jul 2025 13:28:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Microsoft to Cut 9k Workers in Second Wave of Major Layoffs</title><link>https://www.bloomberg.com/news/articles/2025-07-02/microsoft-to-cut-9-000-workers-in-second-wave-of-major-layoffs</link><author>htrp</author><category>hn</category><pubDate>Wed, 2 Jul 2025 13:26:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I&apos;m dialing back my LLM usage</title><link>https://zed.dev/blog/dialing-back-my-llm-usage-with-alberto-fortin</link><author>sagacity</author><category>hn</category><pubDate>Wed, 2 Jul 2025 12:48:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We invited Alberto Fortin, a seasoned software engineer with 15 years of experience, to share his candid journey with AI. Alberto initially embraced LLMs with genuine enthusiasm, hoping they would revolutionize his development workflow. However, after encountering significant challenges while rebuilding his infrastructure with Go and ClickHouse, he wrote a thoughtful blog post reflecting on the gap between AI hype and reality.
For this conversation, Alberto also prepared a detailed follow-up analysis testing newer models like Claude Opus 4, examining whether recent improvements have addressed the core issues he encountered.His experience provides practical lessons for engineers evaluating LLMs in production environments—balancing realistic expectations with an understanding of where these tools genuinely add value versus where they still fall short."I was really shocked at the poor quality of some things, and it was not just about bugs and features not working. I think as a developer who wants to maintain this codebase for the next few years, I also care about it being neat enough.""I feel like I'm a week away from fixing this, but actually a new small error would come up and then that will take another two weeks to fix.""I will give my error output to the LLM and then it will spit out something new that will kind of fix it, but also make things a bit more messed up—and break something else in the process.""I think everyone just got a little bit overexcited about it because the first iteration, the first little feature, the first autocomplete is like, 'Oh my God, this is amazing. This is like reading my mind.' So you kind of get duped into it a little bit.""I think we've gotten to a level where we can do probably 10 times as much coding. So we kind of expect that to happen and we require that from the LLMs, but I think everyone just gets a little bit overexcited about it.""I think this is the biggest difference, like a mental shift... I am the software engineer, the senior software engineer, I am the architect. The LLM is the assistant. The assistant responds to me; I make the plan.""I lost all my trust in LLMs, so I wouldn't give them a big feature again. I'll do very small things like refactoring or a very small-scoped feature.""I started fixing the bugs myself. Because as soon as you understand this—you have a hundred percent understanding of your codebase and what everything is doing—it's so much easier and quicker for you to go in and fix something.""If you are confident enough in your skills—you know, a senior developer—and this is not working for you, there's nothing wrong with you. Just try to do the things that you always did and use AI to leverage your knowledge a little bit.""We've gone up a level, it's great. But also, let's be mindful we're not there yet at the next level... We are offloading some of the programming work, but we still need to do architectural abstractions and make the decisions for the product.""Let's just try to calm down all this hype and find a balanced approach towards AI. Use it, because I think it's such an amazing revolution in technology, but we're not there yet."]]></content:encoded></item><item><title>Don’t use “click here” as link text (2001)</title><link>https://www.w3.org/QA/Tips/noClickHere</link><author>theandrewbailey</author><category>hn</category><pubDate>Wed, 2 Jul 2025 11:39:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Don't use "click here" as link textWhen calling the user to action, use brief but meaningful link text
that:provides some information when read out of contextexplains what the link offersdoesn't talk about mechanicsFor instance, avoid the following sentence on your page: To download W3C's editor/browser Amaya, click
here.
  To download Amaya, go to the Amaya Website and get
  the necessary software.Both of these sentences divulge too much of the mechanics of getting the
Amaya software. If you want to call your reader to action, use something
like:Note that "get" is left out of the hypertext; we do not recommend putting
verb phrases in link text. Thus, rather than:  Tell me more about Amaya: W3C's free
  editor/browser that lets you create HTML, SVG, and MathML
  documents.The W3C QA Tips are short documents explaining useful
bits of knowledge for Web developers or designers, hosted and produced by the Quality Assurance 
Interest Group at W3C.While the tips are carefully reviewed by the participants of the group, they should not be seen
as anything else than informative bits of wisdom, and especially, they are 
normative W3C technical specifications.Learn more about the Tips, how to submit your own pearls of wisdom, and find all the other QA 
tips in the Tips Index.]]></content:encoded></item><item><title>Math.Pow(-1, 2) == -1 in Windows 11 Insider build</title><link>https://github.com/dotnet/runtime/issues/117233</link><author>jai_</author><category>hn</category><pubDate>Wed, 2 Jul 2025 11:04:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>They tried Made in the USA – it was too expensive for their customers</title><link>https://www.reuters.com/business/they-tried-made-usa-it-was-too-expensive-their-customers-2025-07-02/</link><author>petethomas</author><category>hn</category><pubDate>Wed, 2 Jul 2025 10:51:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How large are large language models?</title><link>https://gist.github.com/rain-1/cf0419958250d15893d8873682492c3e</link><author>rain1</author><category>hn</category><pubDate>Wed, 2 Jul 2025 10:39:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jack Welch, the Man Who Broke Capitalism (2022)</title><link>https://www.forbes.com/sites/kylewestaway/2022/05/31/jack-welch-the-man-who-broke-capitalism/</link><author>throw0101b</author><category>hn</category><pubDate>Wed, 2 Jul 2025 10:31:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[David Gelles has been reporting on American CEOs for years at The New York Times. But there’s one CEO who stands heads and shoulders above his peers. He’s been retired for more than two decades, but his impact is still felt. Revered by some and reviled by others, his name is Jack Welch and he served as the CEO for General Electric (GE) from 1981 to 2001.In Gelles’ new book, The Man Who Broke Capitalism: How Jack Welch Gutted the Heartland and Crushed the Soul of Corporate America―and How to Undo His Legacy, he chronicles how Welch’s laser focus on maximizing shareholder value by any means necessary - including layoffs, outsourcing, offshoring, acquisitions, and buybacks - became the new playbook in American business. The book demonstrates how this shareholder maximizing version of capitalism has led to the greatest socioeconomic inequality since the Great Depression and harmed many of the very companies that have embraced it.I recently discussed the book with Mr. Gelles. The interview was condensed and edited for clarity. If you’re interested in viewing a video of the entire discussion, .Why do Americans revere our CEOs so much? Why does Jack Welch stand out among the crowd?To get at the root of why this society seems to put our bosses up on pedestals, look back well over 100 years to the way we celebrated some of the early industrialists who rose to such great heights. We have the same veneration for modern-day technologists and entrepreneurs who are able to create amazing new breakthroughs and products.In a big, diverse country that never had a monarchy or a unified religion, and is increasingly polarized and fractured, we look to our business leaders as some of the most important and perhaps some of the last sort of cultural touchstones who can be relevant to society at large. This helps explain some of the reasons we have some of the problems we do today.I argue in the book that Jack Welch was a celebrity CEO. He was trying to marry this American reverence of CEOs with the modern media ecosystem, and he used it to disastrous effect. It was only through our collective veneration of Welch that he was able to be so influential over such a long period.In the book, you draw a line from Jack Welch to the 737 Max issues at Boeing. Can you walk me through that? Starting in 2019, I was one of the reporters at the  who started digging into Boeing after the second crash of the 737 Max. The plane’s technical problem was very clear early on. There was a bad piece of software that relied on one flimsy sensor on the fuselage of the plane. So, we understood, in theory, what caused the planes to crash.What we started understanding as we dug deeper, though, was that there was a cultural story. Over 25 years, something fundamental had shifted inside Boeing—the company’s priorities and what made it tick. When we started trying to understand that cultural change, it was a story of Jack Welch.Starting in 1997, three successive CEOs who studied at Jack's knee at GE, took over Boeing. They deliberately, explicitly tried to make Boeing more like GE. And in doing so, they transformed one of the great American manufacturers, a company that for nearly 100 years had been focused on aeronautical engineering, into one that was motivated by financial engineering.Records from congressional inquiries revealed messages between mid-level Boeing employees. These records showed that engineers and test pilots were thinking about the stock price when making decisions about safety. The awareness of the company's stock price percolated all the way down to the level of people who should be focused on the quality and safety of the plane, not Wall Street.In 2011, Boeing faced this critical juncture; it was faced with the loss of a major order from American Airlines. The company had been a Boeing customer for decades, so they gave the Boeing CEO, Jim McNerney, a courtesy call to let him know that they were about to place a big order from Airbus instead of Boeing. McNerney asked for a week or two to make a counter-offer. In those few days, Boeing decided to redesign the 737 one more time. Rather than design a whole new plane that was suitable for the 21st century, they tried to re-engineer and tinker with the 737, which had been introduced in the 1960s. And it was that decision that set in motion this cascade of decisions and design changes that required this flawed piece of software to be put in the plane in the first place.In 2009, Welch said that maximizing shareholder value is “the dumbest idea in the world.” Do you believe that he meant that? If not, why did he say it, when it’s against his entire cannon?Jack Welch was a master of reading the room. And I think he understood in that moment, right in the year after the financial crisis, a year during which it did become clear what that kind of management led to, that he was espousing something antithetical to his actions over 20 years as a CEO.There was no great conversion moment.I've spoken with enough CEOs over the years to recognize that many of them are experts at telling themselves a story where they are not the bad guys. So was there a certain amount of rationalizing going on? Or was he saying that in all his decisions that he made to maximize shareholder value at GE, he was actually motivated by something else? I don't know. I can't get inside his head.But you just need to look at his public statements during his time as CEO, and in the aftermath of his retirement, to understand that he was on record as saying the purpose of the business is to increase its profits. When he was asked by the Wall Street Journal about what he believes his greatest legacy was, was making GE the most valuable company on Earth.How did Welch contribute to income inequality?I'm not an academic who has studied inequality in a deep way. But those who have, including most famously Thomas Piketty, draw a direct line between executive compensation and its absolutely relentless upward trajectory over the last decades, and the widening gap between the haves and have-nots.Welch's own enormous executive compensation was immense. He was on the Forbes list of the 400 richest Americans simply for being a people manager. He didn't invent anything. He didn't own the company. He was hired help. And yet, he became something close to a billionaire. By doing so, he set a precedent for hundreds of other managers over the past several years to do the exact same thing. Now we don't even blink when a CEO is rewarded with a $20- or $50 million-a-year pay package.As all that is happening, what's happening to his workers? They're getting laid off en masse. He's outsourcing them to contractors who don't pay nearly as good of wages as GE once did. He's sending jobs overseas in search of low wages and taxes. At the same time, look at what's happened to the American minimum wage: It's stuck at $7.25 an hour. If it had just kept pace with inflation over the last 20 years, it would be closer to $25. But we live in this world that was shaped by Jack Welch's priorities. And we're still trying to dig out of that hole.I wrote the column for the last five years for the  and I got to interview hundreds of CEOs. It was a real privilege. And I got an insight into what makes CEOs tick. After a couple of years, I realized that one name kept coming up: Jack Welch. Some people would bring him up as a cautionary tale, and others look to him as guidance for how they ought to comport themselves. Either way, he was clearly living rent-free in the minds of CEOs today. And that just bugged me. It was just a question mark more than anything else. He hasn’t been a CEO for almost 20 years. Why is he still so influential?When the Boeing story landed, and I realized that it was really a Jack Welch story, it clicked for me. He’s the guy that explains why we are in such a messed-up world today.What’s the antithesis to Welch’s shareholder maximizing capitalism? What are the results? There's a temptation to imagine that something as simple and squishy as stakeholder capitalism represents that antithesis to shareholder capitalism, but I believe that it's really just the very first steps. It's the opening awkward remarks in a conversation about what an equitable economy is actually going to look like.The book covers 80 years—from the moments right after World War II and the way companies were behaving back then. This was the “golden age of capitalism” all the way to the highly unequal society we live in today. So, I recognize that shareholder capitalism has been a generational project. Jack Welch instituted the priorities of Milton Friedman and Friedrich Hayek—a relentless prioritization of shareholder value above everything else.And in the same way, it's going to be a generational project to rebalance things. We’re seeing the start of that as stakeholder capitalism and ESG [environmental, social and governance] are becoming a part of the mainstream conversation. Maybe we’re at that moment in a pendulum's arc where it pauses and starts to begin its trajectory back in the other direction. I hope we're there because we need to reset.I include some practical suggestions at the end of the book. We need to take better care of our workers. We need to give them better wages and better benefits. We need to offer them equity. The distribution of corporate profits over the last 50 years has gotten wildly out of whack. There's no law that says that shareholders and executives are entitled to this enormous slice of the pie. These are choices that people—mostly older white men—make about how wealth in this society is allocated. And we have the opportunity to change that.So let's start talking about what is fair, what is equitable and what actually is healthy for the economy in the long term. We're starting to see that Welch’s philosophy has led us to a moment where cities in the middle of the country are hollowed out, communities across the country are starved for resources, and the tax base is unable to fund things like education and infrastructure.These are choices we've made. We can make different choices that create a different kind of economy.For founders launching companies, what lesson do you believe they should take away from Jack Welch?The first thing that comes to mind is to avoid stack ranking. Stack ranking, also known as rank-and-yank, is a popular talent management system that was popularized by Welch himself at GE in the ‘80s. Managers are forced to sort their people into A, B and C players. The top 20% performers are A players. The middle 70% performers are B players. The bottom 10% performers are C players. Every year, Welch required all C players to be fired. And what was so astonishing is that not only did it take root at other big companies like Microsoft, but it continues to this day to show up in companies like Uber. The employees who experienced stack ranking at both of those companies talk about the absolutely corrosive effect it had on culture. It gets to the point where your job essentially becomes finding a colleague who you could make look bad to your boss in order to gain more job security. It is just the . It's terrible.Additionally, Jack was just a brutally uncompassionate manager. He was crass. He was rude. He was argumentative. He had a serious strain of alpha male machismo, and often made sexist, derogatory remarks. When he wanted to fire someone, he referred to it as “shooting people.” And that’s hard to even talk about during a week like this—after the school shootings. It's that kind of violent rhetoric that caused loyalty inside GE and the culture under Jack Welch to crumble.]]></content:encoded></item><item><title>More assorted notes on Liquid Glass</title><link>https://morrick.me/archives/10068</link><author>freediver</author><category>hn</category><pubDate>Wed, 2 Jul 2025 08:18:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Over the past couple of weeks, I’ve been trying to make sense of Apple’s latest user-interface redesign — Apple calls it  — that will affect all their platforms in the next iteration of their respective OS versions. But it’s hard to make sense of it when, after checking Apple’s own guidance, I’m mostly left with the feeling that at Apple they’re making things up as they go.If you’ve been following me on Mastodon, you’ll be already familiar with a lot of what follows. I just wanted to gather my posts there in a more organic piece here.In the  section, we find this figure:Key navigation elements like tab bars and sidebars float in this Liquid Glass layer to help people focus on the underlying content.Now take a look at the area I’ve highlighted in the image. Why would you want to “focus on the underlying content” here? Tab bars and toolbars still cover the underlying content, and the more transparent/translucent they are, the worse. When something fades to the background, it literally ceases to be in the foreground, so there’s no point in focusing on it. This is like proposing an interface that helps you focus your sight on your peripheral vision.Below the figure, in the paragraph starting with Establish a clear navigation hierarchy, developers are advised to:Ensure that you clearly separate your content from navigation elements, like tab bars and sidebars, to establish a distinct functional layer above the content layer.Which is in direct contrast to what you’ve just shown on the image above. First you propose to blur the lines between controls and content, then you advise to “clearly separate your content from navigation elements”. Which is it? If you stop and think, it’s ironic that Ensure that you clearly separate your content from navigation elements, like tab bars and sidebars, to establish a distinct functional layer above the content layer is the exact description of what’s happening in the ‘Before’ image!Moving on, we get to this figure related to the Extend content beneath sidebars and inspectors paragraph:In other words, create the illusion of an image that extends under a sidebar, and while you won’t actually be able to see the part of the image under the sidebar, on the other hand the transparency effect applied to the sidebar will make the text on it less legible overall. A great lose-lose situation, visually, don’t you think? Also, this might be just a matter of personal perception, but to my eyes, the blank area below the image you can see behind the sidebar looks weird, as if there’s something missing.In  we find this:To give content room to breathe, organizational components like lists, tables, and forms have a larger row height and padding. Sections have an increased corner radius to match the curvature of controls across the system.Which is largely unnecessary. It reduces the amount of information displayed on screen, and you’ll have to scroll more as a consequence. Look at the Before and After layouts: the Before layout doesn’t need solutions to increase its clarity. You’re just injecting white space everywhere. It’s also ironic that where more space and ‘breathing room’ are actually necessary, the header (“Single Table Row” in the figure) is pushed even nearer to the status bar.And don’t get me started on those redesigned, stretched-out switches. They’re the essence of ‘change for change’s sake’.Let’s start with : “Elevate and distinguish the content beneath them”: is this really the role of controls and interface elements? Should content and controls even occupy the same space? Should the lines be blurred between them?In my opinion, the best way both controls and content can shine is by having each their own space: controls are out of content’s way, letting it shine and helping the user focus on it. And in their own space, controls can be clear, neatly organised, ready to be accessed in order to manipulate the content.Align with the concentric design of the hardware and software […]No, seriously, how does one  in a  context? Is that a matter of picking a circle, an arc, a shape? All snark aside, this just sounds poorly worded to me. I get what Apple means here: in your app design, you should pick shapes that resemble the contours of the hardware — the shape of a MacBook’s display and bezel, for example — and the typical shapes that you find in the system’s UI. Pretty obvious stuff that’s wrapped in ‘pretentious designer vocabulary’.Last but not least, :[…] to maintain a consistent design that continuously adapts […]The definition of  is something that is “unchanging in nature, standard, or effect over time”. So, how does a  design continuously ?This paragraph should have read something like: Adopt platform conventions to create a design that remains visually and functionally consistent across window sizes and displays.In the  section of the guidelines for App icons, we find this:Find a concept or element that captures the essence of your app or game, make it the core idea of your icon, and express it in a simple, unique way with a minimal number of shapes. Prefer a simple background, such as a solid color or gradient […]Not only is this the recipe for blandness, it’s also borderline contradictory. Like, Make a unique dish using a minimal number of simple ingredients. While it’s possible to make a few different dishes using just two or three things, you touch the ceiling of uniqueness and variety pretty damn soon.Another thing that irks me about this obsession with icon simplification is that when you abstract things this much, you dilute their meaning instead of distilling it. Take the progressive degradation of the Dictionary icon, for example. In its subsequent iterations (as soon as it loses the ‘book’ shape), it could just be the icon for a font managing app. Because it ends up losing a lot (if not all) of its uniqueness.This image is taken by this post on the history of some of Mac OS icons by Basic Apple Guy. Go take a look at that post and you’ll see a pattern emerge with application icons: they get progressively abstracted to the point that they barely represent what they should represent: the icon for Stickies goes from being an actual depiction of a few yellow sticky notes to being some small vague rounded rectangles inside a clear rounded rectangle. The icon for Notes goes from representing an actual notepad to being a flat square with two lines and a coloured top area. The icon for Calculator, same thing: from depicting a calculator to being what looks more like a security keypad. Game Centre: from an icon representing different types of games, to… a group of colourful bubbles.The most recent iteration of Migration Assistant’s icon is yet another example:Migration Assistant icon in Mac OS 15 Sequoia (left) and how it appears in Mac OS 26 Tahoe Beta 2 (right)Look at it. It’s utterly meaningless. Maybe it can work in an airport to mark an emergency exit or something. The old one is so simple and clear. From an ‘old, now inactive’ system to a ‘fresh new one’. Migration, indeed. Right there. All while preserving the Mac identity. This once again feels like changing things for change’s sake and nothing else.I’m pretty sure that if you were to interview one of the designers at Apple responsible for this icon devolution, they would say something about reducing icons to their . To me, this looks more like squeezing all life out of them. Icons in Mac OS X used to be inventive, well crafted, distinctive, with a touch of fun and personality. Mac OS X’s user interface was sober, utilitarian, intuitive, peppered with descriptive icons that made the user experience fun without signalling ‘this is a kid’s toy’.Same for NeXTSTEP, from which Mac OS X originates. Here, some icons have a more 3D effect, others are flatter; some are logos (like the icon for the Webster’s Dictionary), others are descriptive to a fault (the user’s Home folder is an illustration of a tiny house), but they’re instantly memorable. They do what icons are supposed to do and they take full advantage of the high resolution monitors NeXT sold for their workstations (also remember that some of those monitors were greyscale, so icons had to work even with limited palettes).In recent years, the reverse has happened: Apple has been infantilising and dumbing down Mac OS’s user interface in order to be more similar to simpler mobile devices and to their UIs, while transforming the icons into something bland and ‘corporate’.In the iOS 5 days, the HIG for icons weren’t too restrictive, apart from some basic requirements and guidance. This gave developers plenty of freedom, and the results (if you exclude the usual trash apps) were tasteful and varied; some opted for a rich, skeuomorphic look; others for flatter designs; others for something in between. Apps were instantly recognisable.Now Apple gives you the option of removing colour and depth to all icons. To make everything look samey and nondescript……So that you can “complement your wallpaper.”On my main Mac I’ve left the default Ventura wallpaper because the only time I see it is when I wake the Mac mini from sleep and I’m presented with the Login screen. People who actually  with computers and mobile devices don’t stare at wallpapers and matching icons.But it’s not just that, it’s that these ‘Icon Appearances’ also remove colour, depth, and personality from  too. This further dictates (and interferes with) what kind of design a third-party developer may choose for their apps. All this after recommending employing “a minimal number of shapes” and “prefer[ring] a simple background”.I’ve said this before, but Apple is forcing third party devs to be in service of Apple. The guidelines and rules are meant to sublimate the brands of the third party, and replace it with Apple.Apple has effectively infinite resources and operates on their own timeline, but everyone else does not have this kind of luxury. Springing big changes like this all at once forces so many independent developers, entire companies, and the industry as a whole to freeze their own development schedules to accommodate Apple’s design system.It’s asking a lot. For almost nothing in return. I keep looking at all the changes Liquid Glass brings, and I cannot find one instance where it has markedly improved the experience in any way.Everything that got rounder—except for the things that didn’t — why? Everything that got inset that wasn’t before — why? Everything that is now blurry — why? I don’t think it’s a secret that the content area of some apps decreased. The margins and padding increased — except where it didn’t.In some ways, there’s almost more UI variance than there was before, which doesn’t make any sense. But in other ways, everything feels far more restrictive than it once was. Which I admit, also doesn’t make much sense. App icons weren’t just more expressive on OS X, they could be a much wider-range of materials than merely glass.I know I can still draw anything I want within that square, and that the glass appearance on objects inside of it is purely optional. But the edge of every icon now has a glass appearance I can’t do anything about. If my icon is paper, wood, metal, or—god forbid—leather? It has a glass specular highlight. On macOS, it’s currently locked at a 45° angle. Which is not something I agreed to.Swinging for the fences like this comes with substantial risk. Especially for matured products like macOS. This product is almost 25 years old, and I would hope there would be a little more caution when expecting effort from and forcing changes upon a developer community you’ve largely lost your goodwill with. These kinds of decisions have long-lasting effects and I’m sure many developers would’ve appreciated their time being considered before asking them to incorporate a design they did not sign up for.And in the paragraph just preceding this section I’ve quoted, Mantia writes (emphasis his):In a way, one could say Liquid Glass is like a new version of Aqua. It has reflective properties reminiscent of that. One could also say it’s an evolution of whatever iOS 7 was, leaning into the frosted panels and bright accent colors. But whatever Liquid Glass  it  what many of us were hoping for.Mantia’s piece is so good it’s difficult to extract a few quick quotes. Please take your time and go read it in full.In  there are a few passages that unequivocally convey the message that Apple is in control of  app’s appearance (or part of it). Take for example this, in the  section:Any custom backgrounds and appearances you use in these elements might overlay or interfere with Liquid Glass or other effects that the system provides, such as the scroll edge effect. […] Prefer to remove custom effects and let the system determine the background appearance […]Let the system handle applying masking, blurring, and other visual effects, rather than factoring them into your design.Compare and contrast this with the language used in the 2010 iOS Human Interface Guidelines under :Try to balance eye appeal and clarity of meaning in your icon so that it’s rich and beautiful and clearly conveys the essence of your application’s purpose. Also, it’s a good idea to investigate how your choice of image and color might be interpreted by people from different cultures.After recommending to create different sizes of your application icon for different devices, the guidelines note thatWhen it’s displayed on an iPhone Home screen, iOS adds rounded corners, a drop shadow, and a reflected shine.You can prevent iOS from adding the shine to your application icon. To do this, you need to add the  key to your application’s  file […]Sure, even back then there were visual requirements for icons, but I wouldn’t define this short list as particularly restrictive:Ensure your icon is eligible for the visual enhancements iOS provides. You should produce an image that:Does not have any shine or gloss (unless you’ve chosen to prevent the addition of the reflective shine)Does not use alpha transparencyThe language in these guidelines from 2010 strikes me as , like in this passage:Create a 512×512 pixel version of your application icon for display in the App Store. Although it’s important that this version be instantly recognizable as your application icon, it can be subtly richer and more detailed. There are no visual effects added to this version of your application icon.The language in the  document is overall more prescriptive and impersonal, and as I was reading all the various recommendations, I couldn’t help but feel the underlying message, We created this beautiful look based on glass effects, don’t you dare ruin it with your custom designs, effects, materials, brand identity.The language in the current guidelines for app icons isn’t much different. It also reflects Apple’s current philosophy of ‘keeping it simple’ which, out of context, could be valid design advice — you’re designing icons with small-ish dimensions, not full-page detailed illustrations for a book, so striving for simplicity isn’t a bad thing.And yet — and I might be wrong here — I keep reading between the lines and feel that these guidelines are more concerned with ensuring that developers maintain the same level of blandness and unimaginativeness of Apple’s own redesigned app icons:Embrace simplicity in your icon design. Simple icons tend to be easiest for people to understand and recognize. An icon with fine visual features might look busy when rendered with system-provided shadows and highlights, and details may be hard to discern at smaller sizes. Find a concept or element that captures the essence of your app or game, make it the core idea of your icon, and express it in a simple, unique way with a minimal number of shapes. Prefer a simple background, such as a solid color or gradient, that puts the emphasis on your primary design — you don’t need to fill the entire icon canvas with content.Going back to the Mac OS X Human Interface Guidelines from 2009 is like entering a different dimension. The chapter dedicated to icon design starts off like this:Aqua offers a photo-illustrative icon style — it approaches the realism of photography but uses the features of illustrations to convey a lot of information in a small space. Icons can be represented in 512×512 pixels to allow ample room for detail. Anti-aliasing makes curves and nonrectilinear lines possible. Alpha channels and translucency allow for complex shading and dimensionality. All of these qualities allow you to create lush, vibrant icons that capture the user’s attention. […]Icon genres help communicate what users can do with an application before they open it. Applications are classified by role — user applications, software utilities, and so on — and each category, or genre, has its own icon style. Creating icons that express this differentiation helps users distinguish between types of icons in the Dock.For example, the icons for user applications are colorful and inviting, whereas icons for utilities have a more serious appearance. Figure 11–2 shows user application icons in the top row and utility icons in the bottom row.You may argue that these are simply different icon design guidelines from different eras reflecting different tastes and aesthetic sense, and that it’s not a matter of one being better than the other, or a matter of right versus wrong, and I’ll concede that. But the older guidelines were informed in such a thoughtful way as to give third-party developers a lot of room for creativity and a wide range of choices while remaining within the required system-wide aesthetics of the time. If you look at the Figure 11–2 above, you could have very illustrative icons like the ones for Disk Utility (the hard disk with a stethoscope) or Front Row (the theatre armchair), but also more minimalistic designs such as the icon for the Terminal and AirPort Utility applications.Tangentially, I found this bit ironic given where we are now:Use transparency only when it is convincing and when it helps complete the story the icon is telling. You would never see a transparent sneaker, for example, so don’t use one in your icon.This piece of advice is reiterated in the 2013 edition of Mac OS X’s Human Interface Guidelines:Use transparency when it makes sense. Transparency in an icon can help depict glass or plastic, but it can be tricky to use convincingly. You would never see a transparent tree, for example, so don’t use one in your icon. The Preview and Pages app icons incorporate transparency effectively.Also, since the introduction of retina (high-resolution, high-density) displays in 2012, this part was added in the HIG:Take Advantage of High-Resolution DisplayRetina display allows you to show high-resolution versions of your art and icons. If you merely scale up your existing artwork, you miss out on the opportunity to provide the beautiful, captivating images users expect. Instead, you should rework your existing image resources to create large, higher-quality versions that are:The aesthetics for icon design may have changed dramatically in the intervening years, but I just find it sad that, with the gorgeous displays we have today, Apple recommends simple designs made out of a few boring shapes, and everything is now in service of a ‘liquid glass’ effect the system superimposes on every aspect of the user interface — as if this surface gimmick is more important than the elements it distorts.I’m sorry to sound like a broken record by now, but this is, once again, form before function, looks before workings. And don’t bother deviating from this new norm, because your app will be assimilated.]]></content:encoded></item><item><title>Huawei releases an open weight model trained on Huawei Ascend GPUs</title><link>https://arxiv.org/abs/2505.21411</link><author>buyucu</author><category>hn</category><pubDate>Wed, 2 Jul 2025 07:36:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hilbert&apos;s sixth problem: derivation of fluid equations via Boltzmann&apos;s theory</title><link>https://arxiv.org/abs/2503.01800</link><author>nsoonhui</author><category>hn</category><pubDate>Wed, 2 Jul 2025 00:31:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Australians to face age checks from search engines</title><link>https://ia.acs.org.au/article/2025/australians-to-face-age-checks-from-search-engines.html</link><author>stubish</author><category>hn</category><pubDate>Tue, 1 Jul 2025 23:59:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Australians using search engines while logged in to accounts from the likes of Google and Microsoft will have their age checked by the end of 2025, under a new online safety code co-developed by technology companies and registered by the eSafety Commissioner.Search engines operating in Australia will need to implement age assurance technologies for logged-in users in "no later than six months”, under new rules published on Monday.While only logged-in users will be required to have their age checked, many Australians typically surf the web while logged into accounts from Google, which dominates Australia’s search market and also runs Gmail and YouTube; and Microsoft, which runs the Bing search engine and email platform Outlook.If a search engine’s age assurance systems believe a signed-in user is “likely to be an Australian child” under the age of 18, they will need to set safety tools such as “safe search” functions at their highest setting by default to filter out pornography and high impact violence, including in advertising.Currently, Australians must be at least 13 years of age to manage their own Google or Microsoft account.Age assurance methods can include age verification systems, which use government documents or ID; age estimation systems, which typically use biometrics; and age inference systems, which use data about online activity or accounts to infer age.Search engines will not be required to implement age assurance measures for users who are not logged in to their services, according to the new rules.“Internet search engine services are designed for general public use, with or without an account,” the code states.  However, users who are not logged in should also expect “default blurring of images of online pornography and high-impact violence material detected in search results”.Other compliance measures in the code which search providers must abide by include improving search and age assurance technologies over time, preventing autocomplete predictions “that are sexually explicit or violent”, and responding to searches about eating disorders or self-harm with crisis prevention information.Google and Microsoft were contacted for comment.Earlier this year Google said it would begin using artificial intelligence to estimate users' ages, beginning with tests in the United States, while Microsoft previously stated it had explored age assurance methods while considering potential impacts for user safety and privacy.Changes ‘designed to protect’ Australian kidsThe new rules for search engine operators were “designed to protect" Australian children, according to the code.Drafting of the code was co-led by Digital Industry Group Inc. (DIGI), which was contacted for comment as it counts Google, Microsoft, and Yahoo among its members.eSafety Commissioner Julie Inman Grant said she had registered three new codes submitted by the online industry, which covered harmful content on search engines, enterprise hosting services, and internet carriage services such as telecommunication firms.The codes had been in the works since July 2024 and failure to comply with them could result in civil penalties of up to $49.5 million per breach, her office said.The Commissioner said she had sought extra safety commitments from the industry on six outstanding codes, which covered the likes of app stores, device manufacturers, social media, and messaging services.“It's critical to ensure the layered safety approach which also places responsibility and accountability at critical chokepoints in the tech stack including the app stores and at the device level, the physical gateways to the internet where kids sign-up and first declare their ages,” Inman Grant said.Push to protect children who use AI chatbotsMembers of the technology industry had also been asked to use the remaining six codes to strengthen their protections against generative AI chatbots engaging in harmful behaviours with children, Inman Grant said.“We are already receiving anecdotal reports from school nurses, that kids as young as 10 are spending up to five hours a day with AI chatbots, at times engaging in sexualised conversations and being directed by the chatbots to engage in harmful sexual acts or behaviours,” she said.Inman Grant said she would consider the changes proposed by the industry and would aim to make her final determination on the six outstanding codes by the end of July."If I am not satisfied these industry codes meet appropriate community safeguards, I will move to developing mandatory standards,” she said.]]></content:encoded></item><item><title>Using Sun Ray thin clients in 2025</title><link>https://catstret.ch/202506/sun-ray-shenanigans/</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 1 Jul 2025 23:30:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[i’ve used thin clients at home for quite a while - both for their  use (remotely accessing a desktop of another system); and in the sense of “modern thin clients are x86 boxes that are wildly overpowered for what they run, so they make good mini servers.”recently, i saw a bulk lot of Sun Ray thin clients pop up on Trade Me (NZ’s eBay-like auction site) - and with very little idea of how many clients were actually included in this lot, i jumped on it. after a 9 hour round-trip drive (on some of the worst roads i’ve seen!), i returned home with the back of my car completely packed with Sun Rays. time for some interesting shenanigans!when picking all of these up from the seller, i had guesstimated there was maybe 30 clients in total. turns out i was off by quite a bit.i ended up bringing home:3x Sun Ray 270 - 17” (1280x1024) LCD screens with integrated Sun Ray clients4x Incarta Uvo - 24” 1080p LCD screens with integrated clients
    i can’t find any info about these other than the linked page on the Wayback Machine - if you know more about these, please send me an email!about 40 smart cards, for authentication/hotdeskinga small pile of Sun Type 7 USB keyboards, and some Sun-branded optical miceso that’s  clients all up!a few days prior to picking all this up, i rented a storage unit in a local facility, and put some garage shelving units in there - and boy howdy i’m glad i did!setting up the Sun Ray Server Softwarelooking at the Oracle (eugh.) documentation for the Sun Ray Server Software, it appeared there were two options: run it on ancient Linux, or run it on ancient Solaris. Oracle dropped support for the Sun Rays in 2014, as part of extinguishing everything Sun Microsystems stood for after the 2010 acquisition. i didn’t  want to have a RHEL 6 box kicking around, nor did i want to deal with trying to make Solaris 10 work in a VM on my home Proxmox cluster, so i did some digging.enter  - well, in my case, OpenIndiana. illumos is, essentially, a fork of the pre-Oracle-acquisition OpenSolaris codebase. OpenIndiana is one of many illumos  (in a very similar sense to Linux distributions), and OpenIndiana is more suited for desktop use than most other illumos distributions. the OpenIndiana documentation has a section on setting up the Sun Ray Server Software on OpenIndiana, but even with that in hand there was a lot of pieces to figure out on my own!this is mostly a copy of the docs from the OpenIndiana handbook, with some adjustments to fix things i ran into. i did this on top of a text-only install - OpenIndiana Hipster 2025.04 Text Install DVD (64-bit x86) was the install media i used (from https://www.openindiana.org/downloads/).to get the desktop environment installed:# pkg install mate_install
unlocking the dependencies for SRSS:# pkg change-facet facet.version-lock.gnome/gnome-session=false
# pkg change-facet facet.version-lock.gnome/gnome-settings-daemon=false
# pkg change-facet facet.version-lock.system/display-manager/gdm=false
# pkg change-facet facet.version-lock.library/gnome/libgnomekbd=false
# pkg change-facet facet.version-lock.gnome/window-manager/metacity=false
# pkg change-facet facet.version-lock.library/desktop/gnome-desktop=false
# pkg change-facet facet.version-lock.cde/cde-runtime=false
# pkg change-facet facet.version-lock.library/motif=false
# pkg change-facet facet.version-lock.library/tooltalk=false
# pkg change-facet facet.version-lock.compatibility/packages/SUNWxwplt=false
setting up the package source, and installing the SRSS dependencies:# pkg set-publisher --search-before=openindiana.org -g http://pkg.toc.de/sunray sunray
# pkg set-publisher --non-sticky openindiana.org
# pkg install sunray-essential
after unpacking the Sun Ray Server Software installers (both the Solaris and Linux versions) into , i ran the  script from the OI Handbook, then tried to install SRSS, which bombed out spectacularly with package manager rejections of the This version is excluded by installed incorporation consolidation/userland/userland-incorporation@... sort. so here’s the correct (read: “worked for me!”) steps:# /root/update_dhcp_dependency /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/
# pkg set-publisher -g /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/ sunray
# pkg uninstall entire userland-incorporation
# pkg install SUNWut-srss SUNWut-srwc SUNWuti
to make SRSS happy with isc-dhcp:# rpm2cpio /root/srs_5.4.0.0-Linux.i386/Components/10-SRSS/Content/Sun_Ray_Core_Services_4.5/Linux/Packages/SUNWuto-4.5-44.i386.rpm | bsdtar -C /root -xf - ./opt/SUNWut/lib/dhcp/
# sed 's#$UTDHCPDIR | sort#$UTDHCPDIR | gsort#g' -i.bak /root/opt/SUNWut/lib/dhcp/isc/dhcp_config_linux 
# cp -R /root/opt/SUNWut/lib/dhcp/isc /opt/SUNWut/lib/dhcp/
# cp /opt/SUNWut/lib/dhcp/isc/dhcp_config_linux /opt/SUNWut/lib/dhcp/isc/dhcp_config_solaris
# ln -s /opt/SUNWut/lib/dhcp/isc /etc/opt/SUNWut/dhcp
then apply the needed patch to :now, get the ancient JRE in place:# cd /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Java_Runtime_Environment/Solaris
# ./jre-6u41-solaris-i586.sh
# mv ./jre1.6.0_41 /opt/
# ln -s /opt/jre1.6.0_41 /etc/opt/SUNWut/jre
and, since i wanted the web administration tools to work too:# bsdtar -C /opt -xf /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Apache_Tomcat/apache-tomcat-5.5.36.tar.gz
# ln -s /opt/apache-tomcat /opt/apache-tomcat-5.5.36
i then configured the Sun Ray server:# /opt/SUNWut/sbin/utconfig
# /opt/SUNWut/sbin/utpolicy -a -z both -g -M
# /opt/SUNWut/sbin/utadm -L on
# /opt/SUNWut/sbin/utstart -c
getting the Sun Ray firmware in placesince i was using version 5.4.x of the Sun Ray Server Software, the client firmware wasn’t part of the install - from version 5.3 onwards, you had to have an Oracle support contract to get firmware updates. sigh.thankfully, getting a 5.2.x release (with the firmware included!) wasn’t hard. i grabbed a 5.2.x release for Linux, found the RPM with the firmware in it (), and extracted that with .the Solaris version of SRSS wants to find the firmware in a different place than the Linux version it seems - the Linux versions put it in , but on Solaris/OpenIndiana, it needs to be in /opt/SUNWutdfw/lib/firmware. easy enough.once in place, this was all it took to set up the TFTP server, and make SRSS populate the right places with the firmware:# mkdir /tftpboot
# cd /tftpboot
# ln -f -s . tftpboot
# /opt/SUNWut/sbin/utfwadm -AaV -G force
i wanted to use some of the integrated-into-screens Sun Rays to replace some of the Raspberry Pis (and old iMacs) around the house showing Home Assistant dashboards. i also wanted to set up the Sun Ray server so that when i inserted a particular smart card into a client, it would bring up an RDP session to my existing “desktop” (a Fedora VM running Xrdp).these both turned out to be… interesting to get working.the Sun Ray Server Software has a built-in method for connecting to Microsoft RDP servers - the Sun Ray Windows Connector, also known as .
as you might have guessed, it’s broken as fuck on OpenIndiana, even putting aside the fact that the newest RDP server it knows how to handle would be in the Windows Server 2003 era.so, let’s hack something together with XFreeRDP!i wanted to be able to specify what RDP server each token would connect to. this was a fairly common use case back in the day, and some people wrote helpers to allow things like that - one of which being Daniel Cifuentes’ meta-kiosk, which i borrowed some ideas from.after much trial and error, i got something working!/freerdp


openbox  &
/opt/SUNWut/bin/utscreenresize  all  &

/opt/SUNWut/sbin/utuser  |  | zenity 1
xterm  xfreerdp /cert:tofu /f  /dynamic-resolution /gfx +gfx-thin-client /smartcard /bpp:24 after throwing those in place, install the dependencies and configure the session:# pkg install openbox freerdp
# printf "KIOSK_SESSION=freerdp\n" | /opt/SUNWut/sbin/utkiosk -i FreeRDP
then it’s just a matter of adding the needed data to each token, and assigning the tokens to the FreeRDP session:# /opt/SUNWut/sbin/utkioskoverride -s kiosk -r OpenPlatform.47905167523905788499 -c FreeRDP
upon inserting that token into a client…with much the same setup as the RDP sessions, it’s pretty easy to start a kiosk-mode Firefox, pulling the URL to open from the token data:/kiosk-browser


openbox  &
/opt/SUNWut/bin/utscreenresize  all  &


xset s off
xset s noblank
xset /opt/SUNWut/sbin/utuser  |  | zenity 1
firefox a problem, though. Firefox would show its first-run “Welcome to Firefox” popup… every time. Sun Ray kiosk sessions run as a random user named   (where  is a number), and after the kiosk session ends the home directory of the kiosk user gets fully deleted, so the user can be recycled for other sessions. given i wanted to use this with some always-on Sun Rays, with no input devices attached…thankfully, Firefox policies allow turning that off! throwing this hunk of JSON into /etc/firefox/policies/policies.json fixed that:and with that, i could create a token for an individual client (the tokens for this are , where the MAC is all lower-case), set that token’s “Other Info” field to the URL to show, and assign the kiosk session to that pseudo-token the same way as with smart card tokens.this was a lot of fun to get working. i need to take a break from reading the Sun Ray Administration Guide though, so here’s my thinking for a potential part 2:i want to see how well the multi-head stuff works in SRSS - which joins multiple physical clients together into one desktop session, using the peripherals connected to the “primary” client. unfortunately the Xinerama support is weird (Xinerama and xrandr are mutually exclusive…), but if i can make it play ball it could be a neat thing to use.i want to try and find a newer firmware package too, but that might be a little bit of a lost cause, given i refuse to give Oracle a bunch of money.maybe i’ll set up another OpenIndiana VM and configure the HA failover in SRSS?for now, though… that’s all.]]></content:encoded></item><item><title>Building a Personal AI Factory</title><link>https://www.john-rush.com/posts/ai-20250701.html</link><author>derek</author><category>hn</category><pubDate>Tue, 1 Jul 2025 21:14:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I keep several claude code windows open, each on its own git-worktree. o3 and sonnet 4 create plans, sonnet 3.7 or sonnet 4 execute the plan, and o3 checks the results against the original ask. Any issues found are fed back into the plan template and the code is regenerated. The factory improves itself.Read on to see what might be useful for you.Guiding Principle – Fix Inputs, Not OutputsWhen something goes wrong, I don’t hand-patch the generated code. I don’t argue with claude. Instead, I adjust the plan, the prompts, or the agent mix so the next run is correct by construction.If you know Factorio you know it’s all about building a factory that can produce itself. If not, picture a top-down sandbox where conveyor belts and machines endlessly craft parts because the factory must grow. Do the same thing with AI agents: build a factory of agents that can produce code, verify it, and improve themselves over time.Basic day to day workflow - building the factoryMy main interface is claude code. It’s my computer now. I also have a local mcp which runs Goose and o3. Goose only because I’ve already got it setup to use the models hosted in our Azure OpenAI subscription. Looking to improve this at some point, but it works for now.I’ll give a high level task to claude code, which calls over to o3 to generate a plan. o3 is a good planner and can ask a bunch of good questions to clarify the job to be done. I then have it write out a  file with both my original ask and an implementation plan.First, sonnet 4 reads the plan, verifies it, and turns it into a task list. Next claude code execute the plan, either with sonnet 3.7 or sonnet 4 depending on the complexity of the task. Because most of my day-to-day is in clojure I tend to use sonnet 4 to get the parens right.
One important instruction is to have claude write commits as it goes for each task step. This way either claude or I can revert to a previous state if something goes wrong.Step 3: Verification → Feedback into InputsOnce the code is generated, I have sonnet 4 verify the code against the original plan. Then I have o3 verify the code against the original plan and original ask. o3 is uncompromising. Claude wants to please, so will keep unnecessary backwards compatibility code in place. o3 will call that out and ask for it to be removed. Claude also tends to add “lint ignore flags” to the code which o3 will also call out. Having both models verify the code catches issues and saves me back and forth with claude.Any issue sonnet 4 or o3 finds gets baked back into the plan template, not fixed inline.Git worktrees let me open concurrent claude code instances and build multiple features at once. I still merge manually, but I’m no longer babysitting a single agent.Outputs are disposable; plans and prompts compound.Debugging at the source scales across every future task.It transforms agents from code printers into self-improving colleagues.Example: an agent once wrote code that would load an entire CSV into memory. I made it switch to streaming and had the agent write instructions to the plan to always use streaming for CSVs. Now, my plan checker flags any code that doesn’t use streaming for CSVs, and I don’t have to remember this in every PR review. The factory improves itself.I’ve started to encode more complex workflows, where I have specific agents (behind mcps) for building specific tasks.One MCP will sweep all the clojure code generated and then apply our local style rules. These rules are part of the instructions for the original plan and agent but often the generated code will have style issues. Especially once claude gets in the lint/test/debug cycle. This focused agent means we have tighter behavior and can apply our style rules consistently.I’ve started doing this for internal libraries as well. It’s good at looking at generated code and replacing things like retries and  with our retry library.I’m also building out a collection of these small agents. Each one can take a small specific task, and by composing them together I can build more complex workflows. For example, I can take an api doc, and a set of internally defined business cases and have a composition of agents build integrations, tests, and documentation for the api. This is a powerful way to build out features and integrations without having to do all the work by hand.You don’t get there in one big step. Here’s the secret sauce: It’s essentially free to fire off a dozen attempts at a task - so I do. All agents run in parallel. When one fails, stalls, or lacks context, I feed that lesson into the next iteration. I resist the urge to fix outputs, instead I fix the inputs.That loop is the factory: the code itself is disposable; the instructions and agents are the real asset.I’m working on a few things to improve the factory:Better overall coordination of the agents. I tend to kick things off manually, but I want to have a more automated way to manage the workflow and dependencies between agents.Aligning our business docs with the agents. Changing the information we capture to be at a higher level of abstraction so that the agents can use it more effectively. This means moving away from low level implementation details and focusing on use cases.More complex workflows. I’ve been able to build some pretty complex workflows with the current setup, but I want to push it further. This means more agents, more coordination, and more complex interactions between them.Maximize token usage across providers. I’m pretty limited by bedrock’s token limits especially for sonnet 4. Going to need to be able to switch between the claude max plan and bedrock w/out interruption.That’s where my factory sits today: good enough to ship code while I refill my coffee, not yet good enough to bump me off the payroll. Constraints will shift, but the core principle remains: .]]></content:encoded></item><item><title>Effectiveness of trees in reducing temperature, outdoor heat exposure in Vegas</title><link>https://iopscience.iop.org/article/10.1088/2752-5295/ade17d</link><author>PaulHoule</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:59:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We apologize for the inconvenience...To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.]]></content:encoded></item><item><title>The Roman Roads Research Association</title><link>https://www.romanroads.org/</link><author>bjourne</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:32:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[BookingBookinghttps://youtu.be/DLc8lQvVcvMRoman Roads in Cheshire]]></content:encoded></item><item><title>Fakespot shuts down today after 9 years of detecting fake product reviews</title><link>https://blog.truestar.pro/fakespot-shuts-down/</link><author>doppio19</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:26:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today marks the end of an era. After nearly a decade of helping millions of shoppers navigate the murky waters of online reviews, Fakespot has officially closed its doors. If you tried to check a product listing this morning and found Fakespot not working, you're not alone. The service has permanently shut down. Fakespot, the popular fake review detection tool acquired by Mozilla in 2023, shut down today, July 1, 2025. Founded by Saoud Khalifah in 2016, it helped millions identify unreliable Amazon reviews with 90% accuracy before Mozilla discontinued it due to sustainability challenges.Back in 2016, Saoud Khalifah bought a product on Amazon, trusting the glowing reviews, only to discover he'd been duped by fake feedback. Instead of just leaving his own angry review, Khalifah took a more proactive approach: he built Fakespot.What started as one person's frustration with deceptive sellers became a tool that analyzed millions of reviews across Amazon and other major retailers like eBay and Walmart. The premise was simple but powerful: use AI to spot patterns that human shoppers might miss, like suspiciously similar language or reviewer profiles that didn't quite add up.The magnitude of the deceptionFakespot's technology revealed some eye-opening statistics. About 43% of the best-selling Amazon products had reviews that were unreliable or fabricated, according to a study by app company Circuit. The problem was even worse in certain categories. Clothing and jewelry led the pack with a staggering 88% of reviews deemed unreliable.These numbers painted a sobering picture of the online shopping landscape. Most of us rely on product reviews as a major factor when deciding what to buy, but nearly half of the feedback you read might not be genuine.Three years later, Mozilla acquired Fakespot, bringing the startup's 13-person team into the Firefox family. Mozilla integrated Fakespot's technology directly into Firefox as the "Mozilla Review Checker" feature, making it easier than ever for users to verify product reviews without installing separate extensions.For many users, this felt like a perfect match. Mozilla's reputation for privacy and transparency aligned beautifully with Fakespot's mission to bring honesty to online shopping.But as Mozilla announced in May, not all acquisitions fit into a sustainable long-term model. The company made the difficult decision to discontinue both Pocket and Fakespot as part of a strategic refocus on Firefox's core features and AI-powered innovations.The reasons were practical, if devastating for users. A flood of reviews lamenting the closure have appeared on Fakespot's extension page on the Chrome Web Store:Fakespot's mission resonated strongly with consumers, but Mozilla couldn't find a sustainable model to keep it running. Resources that once supported the service would now flow toward Firefox features like vertical tabs, smart search, and additional AI-powered features.As we say goodbye to Fakespot, it's worth reflecting on what it accomplished. For nine years, it served as a defender against fraud in an increasingly deceptive marketplace. It gave shoppers a fighting chance against promotional reviewers and bot farms that undermine trust in online shopping.For those of us who came to rely on Fakespot's review analysis before making purchases, its absence leaves us less confident in our buying decisions. The need for trustworthy review analysis hasn't gone away. If anything, it's more critical than ever.I know I'm not alone in feeling this gap, which is why I've begun building a tool that aims to be the spiritual successor to Fakespot. TrueStar will use modern AI, streamlined analysis techniques, and sustainable economics to keep costs manageable while maintaining the accuracy shoppers need.
                            Get notified
                        Quick answers about Fakespot's closureWhen did Fakespot shut down?Fakespot officially closed on July 1, 2025, with the Mozilla Review Checker feature in Firefox having ended on June 10, 2025.Why did Fakespot shut down?Mozilla couldn't find a sustainable business model for Fakespot despite its popularity, choosing to redirect resources to core Firefox features and AI-powered browser tools.What happened to Fakespot?Mozilla acquired Fakespot in 2023 but announced in May 2025 that both Fakespot and Pocket would be discontinued as part of a strategic refocus on Firefox development.What are the best Fakespot alternatives?While several options exist including ReviewMeta, The Review Index, and emerging tools like TrueStar, the market is still developing sustainable solutions that balance accuracy with affordability.As Fakespot's servers go dark, let's raise a glass to the tool that made online shopping so much more trustworthy for nearly a decade. Thanks to Saoud Khalifah and his team for showing us what's possible when technology serves truth over profit.Rest in peace, Fakespot. You fought the good fight. 🥂If you found this article helpful, consider sharing it with others who might be wondering why their favorite review checker stopped working today. Let's keep the conversation about online authenticity going.]]></content:encoded></item><item><title>Figma files for proposed IPO</title><link>https://www.figma.com/blog/s1-public/</link><author>kualto</author><category>hn</category><pubDate>Tue, 1 Jul 2025 19:39:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Figma, Inc. (“Figma”) today announced that it has filed a registration statement on Form S-1 with the U.S. Securities and Exchange Commission (“SEC”) relating to a proposed initial public offering of its Class A common stock. Figma has applied to list its Class A common stock on the New York Stock Exchange under the symbol “.”The number of shares to be offered and the price range for the proposed offering have not yet been determined. The offering is subject to market conditions, and there can be no assurance as to whether or when the offering may be completed, or as to the actual size or terms of the offering.Morgan Stanley, Goldman Sachs & Co. LLC, Allen & Company LLC, and J.P. Morgan will act as joint lead book-running managers for the proposed offering. BofA Securities, Wells Fargo Securities, and RBC Capital Markets will act as book-running managers for the proposed offering. William Blair and Wolfe | Nomura Alliance will act as co-managers for the proposed offering.The proposed offering will be made available only by means of a prospectus. Copies of the preliminary prospectus, when available, may be obtained from Morgan Stanley & Co. LLC, Attention: Prospectus Department, 180 Varick Street, 2nd Floor, New York, New York 10014, or by email at prospectus@morganstanley.com; Goldman Sachs & Co. LLC, Attention: Prospectus Department, 200 West Street, New York, New York 10282, by telephone at (866) 471-2526, or by email at prospectus-ny@ny.email.gs.com; Allen & Company LLC, Attention: Prospectus Department, 711 Fifth Avenue, New York, New York 10022, by telephone at (212) 339-2220, or by email at allenprospectus@allenco.com; or J.P. Morgan Securities LLC, c/o Broadridge Financial Solutions, 1155 Long Island Avenue, Edgewood, New York 11717 or by email at prospectus-eq_fi@jpmchase.com and postsalemanualrequests@broadridge.com.A registration statement on Form S-1 relating to these securities has been filed with the SEC but has not yet become effective. These securities may not be sold, nor may offers to buy be accepted, prior to the time the registration statement becomes effective. This press release shall not constitute an offer to sell or the solicitation of an offer to buy these securities, nor shall there be any sale of these securities in any state or jurisdiction in which such offer, solicitation, or sale would be unlawful prior to registration or qualification under the securities laws of any such state or jurisdiction.Figma is where teams come together to turn ideas into the world’s best digital products and experiences. Founded in 2012, Figma has evolved from a design tool to a connected, AI-powered platform that helps teams go from idea to shipped product. Whether you’re ideating, designing, building, or shipping, Figma makes the entire design and product development process more collaborative, efficient, and fun––while keeping everyone on the same page.]]></content:encoded></item><item><title>Sam Altman Slams Meta’s AI Talent Poaching: &apos;Missionaries Will Beat Mercenaries&apos;</title><link>https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/</link><author>spenvo</author><category>hn</category><pubDate>Tue, 1 Jul 2025 18:08:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Altman is hitting back at Meta CEO Mark Zuckerberg’s recent AI talent-poaching spree. In a full-throated response sent to OpenAI researchers Monday evening and obtained by WIRED, Altman made his pitch for why staying at OpenAI is the only answer for those looking to build artificial general intelligence, hinting that the company is evaluating compensation for the entire research organization.He also dismissed Meta’s recruiting efforts, saying what the company is doing could lead to deep cultural problems down the road.“We have gone from some nerds in the corner to the most interesting people in the tech industry (at least),” he wrote on Slack. “AI Twitter is toxic; Meta is acting in a way that feels somewhat distasteful; I assume things will get even crazier in the future. After I got fired and came back I said that was not the craziest thing that would happen in OpenAl history; certainly neither is this.”The news comes on the heels of a major announcement from Zuckerberg. On Monday, the Meta CEO sent a memo to staff introducing the company’s new superintelligence team, which will be helmed by Alexandr Wang, formerly of Scale AI, and Nat Friedman, who previously led GitHub. The list of new hires also included a number of people from OpenAI, including Shengjia Zhao, Shuchao Bi, Jiahui Yu, and Hongyu Ren. OpenAI’s chief research officer, Mark Chen, told staff that it felt like “someone has broken into our home and stolen something.”Altman struck a different tone about the departures in his note on Monday.“Meta has gotten a few great people for sure, but on the whole, it is hard to overstate how much they didn't get their top people and had to go quite far down their list; they have been trying to recruit people for a super long time, and I've lost track of how many people from here they've tried to get to be their Chief Scientist,” he wrote. “I am proud of how mission-oriented our industry is as a whole; of course there will always be some mercenaries.”He added that “Missionaries will beat mercenaries” and noted that OpenAI is assessing compensation for the entire research organization. “I believe there is much, much more upside to OpenAl stock than Meta stock,” he wrote. “But I think it's important that huge upside comes after huge success; what Meta is doing will, in my opinion, lead to very deep cultural problems. We will have more to share about this soon but it's very important to me we do it fairly and not just for people who Meta happened to target.”Altman then made his pitch for people to remain at OpenAI. “I have never been more confident in our research roadmap,” he wrote. “We are making an unprecedented bet on compute, but I love that we are doing it and I'm confident we will make good use of it. Most importantly of all, I think we have the most special team and culture in the world. We have work to do to improve our culture for sure; we have been through insane hypergrowth. But we have the core right in a way that I don't think anyone else quite does, and I'm confident we can fix the problems.”“And maybe more importantly than that, we actually care about building AGI in a good way,” he added. “Other companies care more about this as an instrumental goal to some other mission. But this is our top thing, and always will be. Long after Meta has moved on to their next flavor of the week, or defending their social moat, we will be here, day after day, year after year, figuring out how to do what we do better than anyone else. A lot of other efforts will rise and fall too.”A number of high-ranking employees who’ve worked at Meta followed up in Slack with their own stories about why OpenAI’s culture is superior. “[T]hey constantly rotate their top focus,” wrote one. Another said: “Yes we’re quirky and weird, but that’s what makes this place a magical cradle of innovation,” wrote one. “OpenAI is weird in the most magical way. We contain multitudes.”]]></content:encoded></item><item><title>Show HN: Arch-Router – 1.5B model for LLM routing by preferences, not benchmarks</title><link>https://news.ycombinator.com/item?id=44436031</link><author>adilhafeez</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 17:13:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi HN — we're the team behind Arch (https://github.com/katanemo/archgw), an open-source proxy for LLMs written in Rust. Today we're releasing Arch-Router (https://huggingface.co/katanemo/Arch-Router-1.5B), a 1.5B router model for preference-based routing, now integrated into the proxy. As teams integrate multiple LLMs - each with different strengths, styles, or cost/latency profiles — routing the right prompt to the right model becomes a critical part of the application design. But it's still an open problem. Most routing systems fall into two camps:- Embedding-based routers use intent classifiers — label a prompt as “support,” “SQL,” or “math,” then route to a matching model. This works for simple tasks but breaks down in real conversations. Users shift topics mid-conversation, task boundaries blur, and product changes require retraining classifiers.- Performance-based routers pick models based on benchmarks like MMLU or MT-Bench, or based on latency or cost curves. But benchmarks often miss what matters in production: domain-specific quality or subjective preferences like “Will legal accept this clause?”Arch-Router takes a different approach: route by preferences written in plain language. You write rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini Flash.” The router maps the prompt (and conversation context) to those rules using a lightweight 1.5B autoregressive model. No retraining, no fragile if/else chains. We built this with input from teams at Twilio and Atlassian. It handles intent drift, supports multi-turn conversations, and lets you swap in or out models with a one-line change to the routing policy. Full details are in our paper (https://arxiv.org/abs/2506.16655), but here's a snapshot:- 1.5B params — runs on a single GPU (or CPU for testing)- No retraining needed — point it at any mix of LLMs- Cost and latency aware — route heavy tasks to expensive models, light tasks to faster/cheaper ones- Outperforms larger closed models on our conversational routing benchmarks (details in the paper)]]></content:encoded></item><item><title>Code-GUI bidirectional editing via LSP</title><link>https://jamesbvaughan.com/bidirectional-editing/</link><author>jamesbvaughan</author><category>hn</category><pubDate>Tue, 1 Jul 2025 16:43:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I built a small proof-of-concept for a system that enables real-time
bidirectional editing between any modern code editor and a GUI, enabled by an
LSP server.I like working on small projects at home that benefit from CAD. I’m also a
programmer with a personal development environment that I’ve spent years making
as cozy as possible. Naturally I’ve been interested in finding code-based CAD
system to use for my projects that allows me to use that cozy development
environment.For example: One idea I’m exploring is “bidirectional editing”, so geometry can
be manipulated using either:a purpose-built graphical UI, orthe textual codeCAD languageIf you graphically drag a point around, the coordinates in the source code
should automatically update.
If you edit the source code, the graphical UI should automatically update.A simple way to test this idea is to throw a  in the UI that
displays the corresponding source code.
But to me, that feels terrible because I never want to be coding in some janky,
in-browser  — I want to be working with source code in Emacs, with
all of my familiar key bindings, color schemes, autocomplete, and decades of
cozy practice.That’s the core appeal of a textual programming language.But doing this properly is an absolute boatload of work:How does the system rewrite source code? Is it mediated by files on disk with
reload on save? How do the editor and UI stay in sync and avoid clobbering
each other’s unsaved changes? Maybe we need an LSP server?The language interpreter needs to preserve comments and flow them through,
even when the UI makes edits to the code.What about whitespace / pretty-printing?How much of this needs to be built to evaluate whether bidirectional editing
“fits nicely in the hand”?Maybe we need an LSP server?I’ve been a happy user of LSP servers since they became commonplace in Neovim
setups, but I have almost no experience with language server internals.
I had certainly never considered that they could facilitate bidirectional
editing with a GUI.That line from Kevin’s post was a proper nerd-snipe because a few hours later I
had built this proof-of-concept:What you’re seeing here is a text editor next to a GUI, and data live-updating
both ways between them, made possible by a small server that uses LSP to
communicate with the text editor and WebSockets to communicate with a web app.I’ve shared more technical details and the code for this demo here on
GitHub.Bidirectional editing isn’t new.
What’s new, as far as I’m aware, is real-time bidirectional editing that works
with your favorite text editor.I’ve tried out a handful of code-based CAD systems, but so far I haven’t found
any that achieve more than two out of these three features:Real-time-ish updates in the GUI from changes made in the codeReal-time-ish updates in the code from changes made in the GUIWorks well with my preferred code editorFusion 360 has
decent bidirectional editing for parameters, but it’s not fully code-based and
it certainly doesn’t let me use my own editor.OpenSCAD doesn’t require the use of its own text
editor, and it’s possible to trigger reloads in the GUI via file watching
when you save source files in external editors, but it only goes one way.Zoo has some bidirectional editing, but only
with its built-in editor.Arcol, the tool that I help build at my day job, is
innovating in CAD interface design in some exciting ways, but we’re building for
architects, not programmers.This is just a toy demo, but it’s enough to excite me about the possibility of a
system that achieves  of those points!I don’t plan to develop this demo further, at least not anytime soon, but I hope
it inspires people to find more creative uses (abuses?) of LSP servers.One of the best code-CAD environments I’ve worked in is OpenSCAD + Neovim with
the OpenSCAD LSP server, only using
the OpenSCAD GUI for the viewer, not the built-in text editor.
OpenSCAD is fundamentally not built for GUI editing, but since it’s open source
and has a nice language server already, it could be a good place to develop a
more interesting demo of this concept.Like Kevin’s post said, doing this properly will be a boatload of work.
Handling conflict resolution, incremental edits, and the more complex general
LSP server internals are all serious tasks, let alone creating a whole new
language for CAD.I’m looking forward to seeing what Kevin comes up with for codeCAD!]]></content:encoded></item><item><title>Show HN: Core – open source memory graph for LLMs – shareable, user owned</title><link>https://github.com/RedPlanetHQ/core</link><author>Manik_agg</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 16:24:24 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I keep running in the same problem of each AI app “remembers” me in its own silo. ChatGPT knows my project details, Cursor forgets them, Claude starts from zero… so I end up re-explaining myself dozens of times a day across these apps.1. Not portable – context is vendor-locked; nothing travels across tools.2. Not relational – most memory systems store only the latest fact (“sticky notes”) with no history or provenance.3. Not yours – your AI memory is sensitive first-party data, yet you have no control over where it lives or how it’s queried.- CORE (Context Oriented Relational Engine): An open source, shareable knowledge graph (your memory vault) that lets any LLM (ChatGPT, Cursor, Claude, SOL, etc.) share and query the same persistent context.- Temporal + relational: Every fact gets a full version history (who, when, why), and nothing is wiped out when you change it—just timestamped and retired.- Local-first or hosted: Run it offline in Docker, or use our hosted instance. You choose which memories sync and which stay private.]]></content:encoded></item><item><title>The Fed says this is a cube of $1M. They&apos;re off by half a million</title><link>https://calvin.sh/blog/fed-lie/</link><author>c249709</author><category>hn</category><pubDate>Tue, 1 Jul 2025 16:22:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[At the Federal Reserve Bank of Chicago’s Money Museum, there’s a big transparent cube on display. It’s filled with tightly packed stacks of  bills, claiming to contain .The plaque proudly declares:Have you ever wondered what one million dollars looks like?
You don’t have to wonder anymore because you can see it right in front of you!But I don’t trust signs. I trust counting.I first tried counting the stacks right there in the room. The cube was tall, so I had to step back to see the whole thing, squinting at the stacks, trying to follow each row. I lost track almost immediately.Also, people were starting to look at me funny. Apparently, staring intensely at a pile of cash while muttering numbers isn’t normal museum behavior.Then, I tried with a photo. I zoomed all the way in on my phone, dragging my finger across the screen, mentally tallying as I went.Still couldn’t keep count.All I wanted was a way to click on things in a photo and have the number go up.You’d think this would already exist, a browser based tool for counting things.Turns out it… doesn’t. At least, not as a web app I can find on Google.There are some clunky old Windows programs, niche scientific tools, and image analysis software that assumes you’re trying to count cells under a microscope, not people, penguins, or stacks of $1 bills in a Federal Reserve cube.It’s stupidly simple: upload an image, click to drop a dot, and it tells you how many you’ve placed. That’s it. But somehow, nothing like it existed.I originally made it to investigate this very cube, but I figured other people might need to count stuff in pictures.Count your enemies. Count your blessings. Count your stacks of cash.Because when someone tells you it’s a million dollars, you might want to double check.Assuming each bundle contains  bills*, that’sSo yeah. They’re off by .That’s  in extra cash.“Hey so… we’re $550,400 over budget on the million-dollar cube project.”If you knock  from each dimension (basically pealing away the outermost layer of money bundles), the math actually gets kinda closebut since dollar bills are much wider than they’re tall, it wouldn’t look like a cube anymore.Maybe the Fed is playing the long game.At the Fed’s  inflation target, this cube will be worth  million in today’s dollars in:Can’t wait to come back in 2047 and say: “Nice. Nailed it.”Sure, it does technically contain .And also  of bonus money.Which is kind of like ordering a burger and getting three.I mean, sure, free stuff. But it’s not what you asked for.You can only see the outer stacks. For all we know, the middle is just air and crumpled-up old newspaper.A money shell. A decorative cube. A fiscal illusion. The world’s most expensive piñata (but don’t hit it, security is watching).And get this: just the outermost layer is already worth:You’d only need a 3-layer-thick shell to blow past a million:How  you make a million dollar cube?Turns out U.S. dollars are extremely non-cube-friendly. Each bill is  wide by  tall, a nice and even aspect ratio of:Each 100-bill bundle is  inches thick. stacksWhich gives you a lovely almost-cube: wide deep tallNot perfect. Not terrible. At least it’s honest, unlike that other cube.Maybe it’s  million.Maybe it’s an empty box with a money shell.Most likely it’s  million.All I know is I built a tool, did the math, and triple-checked the stacks.The sign says you don’t have to wonder.
But I did anyway.And now… you don’t have to either.]]></content:encoded></item><item><title>Show HN: HackerNewt – Breadth-first exploring HN client for iOS</title><link>https://apps.apple.com/us/app/hackernewt-for-hacker-news/id6448201970</link><author>hnand</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 15:57:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[App doesn’t work anymore. Nothing loads at all and spinner just spins. Tried clearing cache and uninstalling and reinstalling app. Disappointed I paid for this.Update 6/5/24 - Developer quickly fixed the issue with the app and responded with the feedback. Kudos for listening to feedback and updating. Updating to 5. Thanks!]]></content:encoded></item><item><title>HN Slop: AI startup ideas generated from Hacker News</title><link>https://www.josh.ing/hn-slop</link><author>coloneltcb</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:31:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[© 2025 Just Joshing, LLC. All rights reserved.]]></content:encoded></item><item><title>Ask HN: Who is hiring? (July 2025)</title><link>https://news.ycombinator.com/item?id=44434576</link><author>whoishiring</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:01:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is  an option.Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.Please only post if you are actively filling a position and are committed
to responding to applicants.Commenters: please don't reply to job posts to complain about
something. It's off topic here.Readers: please only email if you are personally interested in the job.Don't miss these other fine threads:]]></content:encoded></item><item><title>Ask HN: Who wants to be hired? (July 2025)</title><link>https://news.ycombinator.com/item?id=44434574</link><author>whoishiring</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:01:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Share your information if you are looking for work. Please use this format:  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:

Please only post if you are personally looking for work. Agencies, recruiters, job boards,
and so on, are off topic here.Readers: please only email these addresses to discuss work opportunities.]]></content:encoded></item><item><title>Conversations with a hit man</title><link>https://magazine.atavist.com/confessions-of-a-hit-man-larry-thompson-jim-leslie-george-dartois-louisiana-shreveport-cold-case/</link><author>gmays</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:29:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[a night for Jim Leslie to savor. On July 8, 1976, the Louisiana State Senate passed what was known as the Right to Work Bill. One of the most fiercely debated pieces of legislation in decades, the law did away with mandatory union membership and allowed businesses to hire nonunion workers.Given the interests involved, this was a staggering achievement. Labor had a muscular presence in Louisiana, largely because it was controlled by organized crime. Developers were ordered to kick back as much as 25 percent of the cost of construction, and those who wouldn’t play along sometimes faced dire consequences. Plants blew up and buildings burned. In January 1976, a gang of more than 75 men commandeered a forklift to smash through the gate of a Jupiter Chemical construction site, where the company was building a new facility using non-AFL-CIO crews. The mob fired hundreds of rounds, killing one man and injuring five others.Hired by the Louisiana Association of Business and Industry, Leslie inserted himself into the debate by creating a savvy ad campaign intended to build support for the Right to Work Bill. He was known for creating some of Louisiana’s first sophisticated TV spots. His reputation was as a brilliant influencer, and he was in high demand. He had run about 60 political campaigns by then, winning more than 80 percent of them. He had already opened three satellite offices—in Dallas, Baton Rouge, and Monroe, Louisiana—and seemed destined for the national stage; people speculated that he would spearhead the Southern arm of a presidential campaign, maybe as soon as 1980.No one in Shreveport was surprised by his success. Leslie was a soft-spoken jokester and spellbinding storyteller with a dimpled chin, a Paul McCartney mop of hair, and a prodigious work ethic. At 20, he’d dropped out of college to become a reporter at the , where he latched on to daring stories. After wrangling an invitation to a Ku Klux Klan meeting, he wrote a piece describing 60 robed members conducting a cross burning. He exposed after-hours liquor sales and served as the prosecution’s star witness when the city shut down two well-known club operators. Once, while covering a shootout between police and a fugitive, Leslie was so excited that he yelled “Shoot! Shoot!” at the staff photographer behind him, only to look back and see his colleague waving a firearm. “No, no! The ” he shouted.Leslie became a public relations executive in 1964, and when his employer announced a move to Houston in 1967, he remained in Shreveport and opened his own agency. In 1972, he scored a chance to lead J. Bennett Johnston’s long-shot campaign for U.S. Senate; after his man won, Leslie’s phone never stopped ringing. In 1975, he elevated Anthony Guarisco from little-known attorney to state senator by filming a commercial featuring an actor dressed in 18th-century garb, quoting poetry. For the Right to Work campaign, he crafted a TV spot that showed labor leaders dangling state legislators on marionette strings. He was a Dixie Don Draper. That July night, after the successful legislative vote, Leslie headed to Baton Rouge’s Camelot Club for an after-party, and then to the Sheraton Inn for drinks with colleagues. But he was not in a celebratory mood, according to Stonecipher. Leslie was married, but he slept around and feared he was facing divorce papers back home. “Jim knew what was waiting on him, family-wise, as soon as he got back to Shreveport,” Stonecipher says. “He was depressed.”Eventually, Leslie said goodnight and made the short drive to the Prince Murat Inn, where he was staying. As he pulled into a parking spot, fireworks detonated close by, vestiges perhaps of the summer’s raucous celebration of the U.S. Bicentennial. It was about 1:50 a.m. when he exited his car. He was still wearing his work clothes: brown checked suit, yellow dress shirt, tie.He’d taken about a dozen steps when a shotgun blast exploded into his upper back, tearing through his heart and lungs. Leslie staggered sideways and fell face down onto the asphalt, dead before he landed.Curtis saw a white 1976 Oldsmobile Cutlass pull onto the property: Jim Leslie’s car. Moments later he heard the blast.John Curtis thought the sound he heard was a cherry bomb. Partiers had been setting off fireworks on the other side of the fence bordering the Prince Murat, in an adjacent shopping center with a grocery store and a club called the Cahoots Lounge. Curtis, the hotel’s assistant manager, had just come outside to see whether the revelry was spreading into the Prince Murat’s parking lot when he saw a white 1976 Oldsmobile Cutlass pull onto the property: Jim Leslie’s car.Moments later he heard the blast. He quickly walked 100 feet to the corner of the hotel, where he could peer into the back lot, and saw a body in a widening pool of blood. Curtis summoned the hotel’s security guard, who checked on the body and told Curtis to dial 911.Baton Rouge Police detective Chris Schroeder happened to be nearby at the time, checking out a report of possible shots fired. He entered the Prince Murat’s lot “simply to drive around the building to check,” he’d later testify. But then he saw a man “waving frantically, pointing around the corner.” It was Curtis. Schroeder accelerated, then had to slam on the brakes to avoid running over the body.Once Leslie was confirmed dead, Schroeder radioed a patrolman dealing with fireworks complaints and told him to look for a man with a shotgun instead. Curtis didn’t see or hear a car leave the hotel, so police thought the shooter might be on foot. They fanned out, searching a large warehouse on the back side of the neighboring shopping center, but came up empty. Meanwhile, officers began processing the scene. Finding a loose board in a wooden fence near where Leslie had parked, they surmised that the killer had pushed a gun barrel between two slats to fire the shot.The murder became national news. covered it, and radio legend Paul Harvey commented on it during his noon broadcast. Rumors churned that Leslie was targeted in retaliation for the Right to Work legislation. But Stonecipher and many others in Shreveport immediately thought someone else was responsible: George D’Artois. Having been present for the call between Leslie and D’Artois, Stonecipher concluded that Leslie had prophesied his own death.As police looked for ties to the disgraced commissioner, there was a startling development: Before they could solve Leslie’s killing, a second murder took place.Rusty Griffith likely felt some anxiety about the meeting on the evening of October 15, 1976. Griffith, 34, was a 400-pound Shreveport man with a mustache and dense curly hair. He had many livelihoods, some of which were legitimate: He ran a seafood business; several nightclubs, including one called Big Daddy’s Lounge; and a limousine service that ferried people from Dallas and Houston to the Louisiana Downs racetrack in Bossier City. His links to organized crime were well-known. He shared an office with Don Gardner, 39, a large, bearded former wrestler from Shreveport. Both consorted with a group of dodgy characters loosely known in law-enforcement circles as the Dixie Mafia. Among them were a trio of men from Baton Rouge: Steve Simoneaux, Clayton Kimble, and Jules Ron Kimbel. The latter two were siblings but spelled their surnames differently.That night, as dusk approached, Griffith drove his brown Cadillac into the Three Rivers Wildlife Management Area, a dense forest nestled between the Mississippi and Red Rivers in Concordia Parish, about 200 miles southeast of Shreveport. Jules Ron Kimbel had asked him to meet there. What Griffith expected remains unclear, but it may have been trouble. About three weeks earlier, he and Kimbel had been indicted in Mississippi for interstate transportation of a stolen bulldozer, part of a Dixie Mafia scheme in which heavy equipment was swiped and taken to Mexico to be fenced.Griffith met Kimbel as planned, on one of Three Rivers’ dirt roads. While they talked, another vehicle pulled up. The driver rolled down his window, and someone fired a shotgun into Griffith’s face. He died at the scene. Griffith’s address book, found in his car, included contact information for numerous organized-crime figures—and for George D’Artois.As police investigated the killing, various Dixie Mafia members claimed that Griffith possessed recordings of conversations about the machinery thefts, illegal gambling activities, and the Leslie murder. These sources said there was concern that Griffith might use the recordings to reduce a potential sentence in the heavy-equipment case. The theory of his murder quickly became that someone had silenced him and sent a warning to others, in case there were copies of the tapes.By November 7, Jules Ron Kimbel had been booked as a material witness in the Griffith murder, and so began what the  would later describe as one of “countless investigations by parish and city law enforcement from Baton Rouge to Shreveport” into the deaths of Griffith and Leslie. Soon, Gardner and six others were arrested for first-degree murder in the Griffith case. As for the Leslie murder, it was difficult to piece together a working theory.  By April 1977, the state had added Clayton Kimble and Steve Simoneaux to its list of witnesses, but their stories were ever changing and influenced by the promise of immunity in exchange for testimony, the possibility of a get-out-of-jail-free card for parole violations, and a $35,000 reward in the Leslie case offered by the Louisiana Association of Business and Industry. The best version of events investigators could piece together was this: Two weeks before the Leslie murder, Griffith and Gardner asked Clayton Kimble if he could find someone to kill Leslie for D’Artois. At a second meeting, Kimble was offered $30,000 to do the job himself but declined; killing wasn’t his kind of work. The three men agreed that the murder should happen in Baton Rouge to insinuate a link to organized labor and deflect attention from D’Artois. Gardner then gave Kimble $5,000 as a deposit while he looked for someone to take the contract.Five days before Leslie was killed, Kimble returned the money, saying that he couldn’t find any takers. Griffith then said he and Gardner would do it themselves. They decided that Griffith would hide behind the hotel fence with a shotgun while Gardner waited in a car parked in the space closest to Leslie’s room. When Leslie arrived, Gardner would leave, prompting Leslie to pull into his spot, and Griffith would do the deed.Kimble told police that several hours after the murder, the conspirators met at a Denny’s, where Griffith and Gardner reported that the plan had worked. Seconds after Griffith killed Leslie, Gardner had retrieved him from behind the fence. The two men then drove to a bridge over the Mississippi River and threw the shotgun Griffith used into the water below.Police obtained an arrest warrant for first-degree murder for Gardner, who denied involvement and said he was in Shreveport the night Leslie was killed. (He also denied having anything to do with Griffith’s death.) Investigators brought the same charge against D’Artois, which only added to his legal headaches. Over the previous year, a grand jury had indicted him for tampering with evidence seized in gambling raids; for the theft of more than $32,000 in city funds; for intimidating grand jury witnesses; and for malfeasance of office. He had resigned from office the previous July.On April 19, 1977, Caddo Parish sheriff’s deputies showed up at D’Artois’s home to arrest him. He quickly locked himself in his attic with a .357 Magnum. Officers persuaded him to come out—only for D’Artois to barricade himself in a bathroom. After an eight-hour siege, three deputies crashed through the bathroom door. One of them told the  that D’Artois “was grabbing for the gun when we pinned him to the floor.” for the Leslie and Griffith cases to begin to fall apart. The police had no physical evidence linking any of the purported participants to either murder; they were relying entirely on Kimble, Kimbel, and Simoneaux and their shifting, conflicting accounts: While the three men sometimes agreed that Griffith had shot Leslie, in other moments Simoneaux claimed Clayton Kimble had pulled the trigger, or that “a man from Dallas” had done it.One by one, charges against the defendants in the Griffith case were dropped for lack of evidence. After the prosecutor in the Leslie case refused to grant Simoneaux, Kimble, and Kimbel immunity, the men took the stand at a hearing and pleaded the Fifth. The murder charges against D’Artois and Gardner were quickly dismissed. But D’Artois didn’t have long to enjoy his emancipation. On June 8, he underwent heart surgery in Houston; three days later, at the age of 51, he died from complications, taking countless secrets with him.D’Artois’s death, along with the collapse of the two cases, left people wondering whether the murders would ever be solved. Then, in 1981, a new federal grand jury was empaneled. The state rounded up the same cast of characters and pegged the late D’Artois as the ringleader in both crimes—directing men to take out Leslie because he was talking to law enforcement, and to murder Griffith because he seemed like he was about to squeal. But after reviewing the evidence, the grand jury issued an indictment that only brought charges in Griffith’s slaying. In 1982, five people were put on trial: Clayton Kimble, Jules Ron Kimbel, Don Gardner, and two peripheral Dixie Mafia players, Kenneth Brouillette and Benny O’Quinn. They were charged with violating the Racketeer Influenced and Corrupt Organizations, or RICO, statute; conspiracy to violate RICO; conspiracy to violate Griffith’s civil rights by murdering him; and obstruction of justice. Steve Simoneaux, who was in prison at the time for a series of robberies of expensive watches in Dallas, took a plea deal and served as star witness; his testimony would reduce his potential life sentence to 20 years. After more than three weeks of proceedings, Kimble and Kimbel were convicted. Brouillete and O’Quinn were acquitted, and a mistrial was declared for Gardner; prosecutors said it would be too expensive to retry him.Leslie’s murder was a point of discussion during the Griffith case, given how intertwined the two men’s killings seemed to be. And U.S. Attorney Don Beckner assured the press that his office, along with the FBI, was still pursuing indictments for Leslie’s death. But Beckner never did bring charges against anyone; he wasn’t able to build the case he’d hoped for.That result was unsatisfying but perhaps predictable, given the lack of physical evidence and the dependence on unreliable narrators. When Kimble and Kimbel appealed the verdict in the Griffith case, a panel of three judges noted: “Simoneaux acknowledged that he was dishonorable and that he had repeatedly lied, including lies under oath, to save his own skin.” (They upheld the brothers’ convictions anyway.)No one found the situation more unpalatable than Elliott Stonecipher. He grew up in Shreveport, in a tiny house in the wrong-side-of-the-tracks Caddo Heights neighborhood. A stellar student, he was also a skilled public speaker, and by the time he returned from college he was touted as the city’s future. “I was supposed to be the mayor of Shreveport,” Stonecipher says.But he was never willing to traffic in the kind of machinations necessary to reach the mayor’s office. Instead, he worked as a pollster, public speaker, and political strategist. In the 2010s, Stonecipher launched an effort to finally untangle Leslie’s unsolved murder, thinking he could lean on his access to Shreveport’s levers of power. He dug through records in the National Archives and at Louisiana State University Shreveport, and attempted to track down people rumored to be involved. But in his telling, the wealthy and powerful didn’t want the matter revisited. Longtime friends suggested that he let it lie and chastised him for reopening old wounds. Potential clients said their boards of directors weren’t comfortable hiring him. “I was simply viewed as a turncoat,” he says. “And those are not tolerated by the wealthy leadership of Shreveport.”Stonecipher has gone as far as he can with his search for answers. He’s eager for help. And in an interview, he makes a provocative claim: He believes that authorities identified the man who shot Leslie, and that it wasn’t Rusty Griffith. It was the guy from Dallas Simoneaux had once mentioned, a shadowy mob-connected assassin named Rick Roberts. Stonecipher has never been able to track him down—but maybe we can.Murdering someone with a shotgun in the middle of a major city without being spotted seemed well beyond the Dixie Mafia’s skill set.Fuller and I find of Rick Roberts is in the 1981 grand jury indictment. It’s odd, however. He was one of nine people named as conspirators in Griffith’s murder, but he was never charged, and he wasn’t included in the indictment’s description of the conspiracy. When it covered the indictment, the  ran a front-page grid with a photo or sketch of everyone named by the grand jury—except Roberts. We also find a story in the  about a taped interview in which Simoneaux told investigators that Roberts took part in planning Leslie’s murder, then served as the getaway driver—a role that hardly seemed to require a seasoned hit man.We find more references to Roberts at the LSU Shreveport library. The archive has a robust section on George D’Artois, and at some point pages of testimony from the grand jury proceedings were deposited there. (By law that testimony is sealed, but someone apparently wanted it in public view.) The documents include testimony that put Roberts in Griffith’s orbit and at the scene of Leslie’s murder: A witness said Roberts worked at one of Griffith’s establishments, the Inside Out Club, but left for Dallas after Griffith unloaded it. Clayton Kimble recalled that the afternoon before the Leslie murder, various conspirators gathered at a Holiday Inn, where Griffith had a “buddy … which we found out later was Mr. Rick Roberts.” Kimble further claimed that he saw Roberts in the back seat of Griffith’s Cadillac, the supposed getaway car, shortly before the murder, and that Simoneaux told him after Leslie’s death that “Rick Roberts killed a man.” But Kimble also said that he never saw Roberts after murder. “I took for granted that he possibly drove the red and white [Cadillac] wherever it had to go,” Kimble testified, referring to the vehicle he’d previously placed Roberts in.Roberts was called to testify before the grand jury too, but his testimony isn’t in the library’s files. There’s only a cover page and a second sheet noting where the proceedings took place and who was present.Was it possible that Griffith, working on behalf of D’Artois, contracted Roberts to shoot Leslie? On the surface, the idea of a professional assassin made sense. Murdering someone with a shotgun in the middle of a major city without being spotted seemed well beyond the Dixie Mafia’s skill set. As Fuller and I zoomed out, questions mounted. How could Griffith have wedged a shotgun into a narrow space in a fence, fired, extracted the weapon, shimmied his 400-pound bulk out of the tight space between the fence and a loading dock, then driven away with Gardner—all within seconds? How could a car have sped away from the scene without the Prince Murat’s assistant manager, who was standing outside the hotel when the murder happened, seeing or hearing it?And also: Where the hell was Rick Roberts?With questions about the Leslie murder rattling around in our heads,Fuller and I return to Wade in March 2022, curious to follow up on the richly embroidered account of Thompson’s life as a hired gun and career criminal in one of his fictionalized memoirs, titled  His character is a consummate professional: no drinking or drugs to impede clear and quick thinking. (“Never trust a drunk” was one of his mantras.) He is unfailingly polite, avoids conversation, and keeps his face and head covered, though he sometimes wears only a bandage on his cheek: If someone is asked to describe him later, that’s the detail they’ll focus on. By then he’ll have long since removed it.Thompson tells us that, as a hit man, he adhered to a set of rigidly compartmentalized behaviors. His killings usually lacked any direct connection to him. He used pseudonyms and avoided anyone involved with a hit—including the person who hired him. He limited contact to an intermediary who shared his adherence to the Mafia’s code of silence. “Everybody that I’ve done business with all my life will tell you that I’m very secretive about what I do,” he says.Fuller and I are eager to learn how this approach informed some of his most notorious hits, including the Maria Marshall murder, which Thompson breaks down for us in detail. But we also want to glean more about the Leslie murder. I tell Thompson that there’s something I want to ask him about from our first meeting. We noticed how readily he recalled the detail about the murder’s location.“You answered so quickly,” I venture, “it sounded as if you were there.”He smiles and says, “I probably was.”By the time we arrive at Wade for our third visit, Fuller and I have become fairly obsessed with the Leslie murder. A known hit man who seemingly admits to involvement in a notorious unsolved crime will do that. We’ve copied or photographed thousands of pages of files kept at the LSU Shreveport archive—old newspaper stories, police reports, transcripts, George D’Artois campaign materials—and pored over them. Fuller has plunged into his early FBI days, exploring possible links to one of his assets in New York City: a former New Orleans–based Marcello-family mobster. He has reconnected with former colleagues from the Shreveport office and cold-called law enforcement who were active in the seventies. We hope that Thompson can provide some additional intel.We greet him by asking about his recent bout with COVID and how his job in the prison kitchen is going; during wild boar season, he tells us, local hunters sometimes drop off cuts of meat. After catching up, I hand him a summary of the East Baton Rouge Parish sheriff’s investigation of Leslie’s murder. I point out something that’s already obvious: Although the probe seemed exhaustive, Thompson’s name didn’t come up, which seems to undercut his claim that he was there that night.It’s a gambit. Fuller and I have now talked to Thompson for a total of 15 or so hours. We have a pretty good idea of how he communicates. If we draw inferences or make intuitive leaps that are accurate, he’ll confirm them in his own way: “Maybe I was” or “I may have.” Our understanding of his code has allowed us to develop a kind of Larry Thompson operating manual. He doesn’t respond well to overly broad questions. “Tell me what happened that night” goes nowhere. We steer him onto a subject and then sit quietly, asking questions but also leaving conversational space to fill.Thompson reads the investigation summary slowly, looking as if he’s just chewed tinfoil. “I don’t know where they get all this shit right here,” he says when he finishes.I ask how he—or whoever was in the parking lot that night—knew when Leslie would return to the hotel. Thompson says that he had a spy, someone who reported back when Leslie had left the party. “I was by a phone,” he says, “waiting for that call.”We nudge him on. Was the plan to shoot through the hotel fence, as police believed the killer did? Thompson shakes his head. He accurately recalls the makeup of the fence, but says, “I didn’t have to go through no fence.”At this point the room ionizes. It’s happening, I think. Thompson is not only telling us that he was there, but he’s confessing, in his own elliptical way, to having pulled the trigger. Not wanting to break the spell, I resist the urge to look over at Fuller. Somewhere outside a door slams. The conversation unfolds with an eerie calm.“You were not behind the fence?” I ask.“So you were just in the parking lot?”He nods. “Behind an automobile.”He knew Leslie’s room number, he says, and “it don’t take a rocket scientist to figure out where he’s gonna park at.” When Leslie pulled in, Thompson tells us, he slipped from behind an adjacent vehicle and fired his gun.He says he knew that the blast would draw attention, so he’d planned a non-getaway getaway: He’d checked into the Prince Murat under a pseudonym the previous day. After killing Leslie, he hustled straight through the back entrance of the hotel and headed to his room, where he stashed the shotgun under his bed. “How many of the police are gonna think the man who shot that man was in the hotel?” he says. At about 6 a.m., Thompson quietly got into a car he’d parked elsewhere in the lot, drove to Shreveport, and disposed of the gun.We ask him: Why did he do it? Did D’Artois order the hit, as was widely believed?Speaking deliberately, pausing for long intervals, Thompson says the hit was  connected to labor and thus to organized crime—no surprise, given the contentious Right to Work vote. Leslie was “talking to the wrong people,” Thompson says. “You didn’t talk to the wrong people back in the day, ’cause there was a lot of money floating around back then, and they didn’t mind spending it.”Some of that money, Thompson says, took the form of bribes—specifically, his buddy Carlos Marcello had tried to influence the state senate’s vote. Leslie became aware of this and was sharing the sensitive information with people in the capital. Why take that risk? Thompson believes Leslie became so successful so fast that he felt untouchable.“We used to call that too big for your britches,” Fuller says.“That’s exactly what we got there,” Thompson replies. “He thought he was somebody he wasn’t.”When Thompson is done telling his story, we sit quietly for a while. “You said quite a bit there,” I say.Thompson gazes at us and nods. “I shouldn’t have,” he replies.Scott almost seems impressed by the planning that went into the crime. “That has a lot of Larry Thompson in it right there,” he says.  when Fuller and I emerge from Wade Correctional. I feel both dazed and wired. We walk silently to the car, letting the enormity of what we’ve heard sink in. We climb into the rental and Fuller looks at me with raised eyebrows. “Well,” he says. I can see how energized he is. He’s not just back in the middle of an investigation; he finally has the chance to get to the bottom of something in Shreveport.Is there a chance Thompson merely told us what we wanted to hear? It’s possible. He could be looking to burnish his legend or simply keep us entertained and coming back. Then again, Thompson has a full visitation list; he has no shortage of company. And his version of events feels utterly plausible: a known hit man pulling off a professional job, a crime no one was ever able to solve.More than that, his story explained what  made sense as we dug into the police investigation—for instance, how, in the short time it took the hotel’s assistant manager to see Leslie’s body, the killer had supposedly maneuvered from behind the fence and vanished in a getaway car without being seen or heard. If Thompson was telling the truth, the answer was now as clear as it was obvious: He didn’t.To check our instincts, we run the story past law enforcement who know Thompson. When we talk to Larry Scott, a former Shreveport cop acquainted with Thompson since the 1980s, he almost seems impressed by the planning that supposedly went into the crime. “That has a lot of Larry Thompson in it right there,” he says. Scott also believes that organized crime was likely responsible for the hit. “The union was 100 percent behind killing Jim Leslie,” he says. “They just let [the police] go after D’Artois.”But how was it that Thompson was never linked to the Leslie murder? “My first thought is that Larry is very good at what he does,” Ford McWilliams, a former Caddo Parish prosecutor, tells us. “He killed all these people and robbed all these banks and night deposits, but he knew to keep his mouth shut. If he had a loose end, he tied it up, and he was just very professional. He didn’t make mistakes.”Robert “Robbo” Davidson is a former chief detective for the DeSoto Parish Sheriff’s Department. He has met with Thompson many times in prison and investigated one of the cold cases Thompson confessed to in an effort to reduce his son’s prison time; arguably no one in law enforcement knows the hit man better. Davidson not only thinks Thompson’s description of the Leslie murder is believable, but he also finds it strange that Thompson’s name  come up during the investigation. “He was so close to D’Artois,” Robertson says. “I think the first person [D’Artois would] go to was Larry Thompson.” But the case was mostly built in Baton Rouge, not Shreveport, and before Thompson had achieved the peak of his notoriety. If his name did come up, it could have slipped through the cracks.What about the motive—did the bribes Thompson mentioned actually occur? I look up people Leslie worked for; most of them are gone, but Anthony Guarisco, whom he helped get elected to the state senate, is still alive. When I search his name online, something astonishing pops up: Guarisco was mentioned at the end of an Associated Press story about Leslie’s murder. Citing a New Orleans newspaper, the piece says that Guarisco told federal authorities he declined an offer of $10,000 to help clear a campaign debt if he opposed the Right to Work Bill.I connect with Guarisco via email, and he sends me a document titled “The Bribe.” It’s only an introduction to the story I want to know—a 326-word passage that begins with a detailed description of Leslie heading to work on the day of the vote. I reply by asking Guarisco to elaborate over the phone, but when I later mention that I’d like to loop in a retired FBI agent when we talk, he decides he has nothing to share about the Leslie case.Fuller reaches out to Mike Barnett, a retired colonel from the East Baton Rouge Sheriff’s Department, which investigated the Leslie killing. Barnett agrees that Leslie was murdered because he inserted himself into an extremely charged political issue. If the mob couldn’t stop the Right to Work Bill with bribes, maybe a reminder of the cost of going against them would help their interests in the future. “I think they believed it was going to intimidate a lot of legislators,” Barnett says.We are building consensus around the idea that Thompson might have pulled the trigger, but if that’s true, a key question remains: What about the elusive Rick Roberts? Was he involved?Fuller, who once worked in the FBI’s Dallas office, canvasses his law-enforcement contacts throughout Louisiana and Texas: longtime members of sheriff’s offices, mob investigators, veteran agents, people who knew criminal heavyweights of all kinds. None had heard of a hit man named Rick Roberts.Elliott Stonecipher says he was equally baffled when he tried to find him. “I tried every way I could to get a lead on Rick Roberts,” he says. “I couldn’t find cops that knew him, couldn’t find anybody.”“Imagine that. Nobody’s ever seen him. Don’t know what he looks like, don’t know what he sounds like. And nobody’s ever met him.”When Warden Goodwin ushers us in for a fourth prison visit, Thompson seems glad to see us. A certain camaraderie has developed. After all those years as rivals, Fuller and Thompson have become, if not exactly friends, at least not  friends.This time we’ve brought pointed questions about the Leslie case, based on details drawn from police and forensic reports. Even after so much time has passed, the hit man’s answers largely match what we know to be true. He estimates that he was 30 feet from Leslie when he fired the shotgun, echoing the distance cited in the initial police report (31 feet). The description of him slipping into his hotel room fits the timeline of how long it took the assistant manager to see Leslie’s body.I have another question for Thompson, one that’s been percolating since our most recent conversation with Stonecipher, about the purported Dallas hit man. I remind Thompson that in his fictional memoirs, his character uses a pseudonym while working as a contract killer. It’s part of that character’s modus operandi to leave no trace. Then I ask: Did he ever use the name Rick Roberts?Without hesitation he chuckles. “I don’t know, I might have,” he says.We’ve learned to pay close attention to Thompson’s reactions. Fuller has occasionally thrown in questions about murders he knows Thompson didn’t commit. When Thompson wasn’t involved, he says so immediately. It’s like introducing a control into an experiment. Now his response to the question of Roberts’s identity feels unpremeditated.“Rick?” Fuller says. “Can we call you Rick?”Thompson laughs deeply. “That don’t mean I’d like to answer to it,” he says. “I would use a lot of names over the years, but nobody’d ever know who it really was.”The admission is such that I have to force myself to stay plugged into the moment. But there’s something in the room that makes it easy. Maybe Thompson is energized by the chance to confess after all these years, and I’m feeling it—Fuller is, too. We are three people who have little in common but are emotionally joined, like the survivors of a plane crash or an infantry unit under fire.We explain that we’ve done extensive research in archives and throughout the law-enforcement community for information about a hit man named Rick Roberts.Thompson nods. “They can’t find him, can they?”“Amazing how someone could just slip by like that.”“Allegedly,” Fuller says, “he might be in Dallas.”“Son of a bitch,” Thompson replies. “Imagine that. Nobody’s ever seen him. Don’t know what he looks like, don’t know what he sounds like. And nobody’s ever met him.”Two things come to me. One is that Thompson was never considered a suspect in the investigation in part because none of the other men charged in connection with the crime ever mentioned his name. If Thompson is now telling the truth about using a pseudonym, that was by design. Keeping his identity a secret from the Dixie Mafia guys who knew Leslie was going to be murdered would have been as much a part of the planning as where to dump the gun afterward.The second thought is a wisp of a memory, something Thompson told us the first time we visited, when Fuller initially asked about the murder. If we’d understood more then, we might have divined its meaning: , he told us then, my name never came up in that deal. with the theory we’re working out in our interview: Investigators and prosecutors did talk to someone named Rick Roberts. We know this because of the bare-bones documents we found at the library, pointing to his grand jury testimony in 1981. Finding other evidence of his existence will require a trip west.In early 2023, Fuller and I drive to the Fort Worth, Texas, branch of the National Archives, which holds transcripts and court filings from the 1982 trial. The staff brings out three boxes and we dig in; there are more than 10,000 pages. They’re divided into binders, each containing a single day’s testimony. Every binder begins with a table of contents naming the day’s witnesses. I churn through them all, scanning the contents. When I open the binder for June 9, 1982, my pulse thumps in my forehead. There he is, on page 58.The transcript takes up a mere ten pages, double-spaced. It reveals little. Roberts knew Rusty Griffith; he’d worked at Griffith’s lounge in Shreveport; they’d talked about a development project in Costa Rica; before that, Roberts sold advertising for Holiday Inn. Roberts said that he only ever spoken to Don Gardner once in his life, and never met the four other defendants charged with conspiring to kill Griffith. When asked about the Leslie murder, Roberts replied, “The only thing I know is that he was killed, and that I have given testimony in front of two grand juries already concerning the fact that I knew nothing of that situation at all.”“Have you ever been a hit man?” he was asked.“Do you know what that term means?”“I believe so. I don’t even own a gun.”Prodded one last time for any knowledge of either killing, Roberts said, “I know nothing about anything. I was just a victim of circumstances, being at the wrong time in the wrong place.” And with that he was dismissed.I show the pages to Fuller, my mind churning. Rick Roberts  exist, and if he played a role in Leslie’s killing, he seems to have artfully avoided any blame. Does that mean Thompson was lying about using a pseudonym? Could Thompson be protecting Roberts? Or, more improbably, could Thompson have been interviewed by the police or appeared in front of a grand jury ?Fortunately, Wade Correctional is our next stop.Fuller and I sit, thunderstruck. This, it occurs to me, is literally how you get away with murder.This visit, we don’t spend much time on preliminaries. Once we’ve settled into the meeting room, I hand over the first page of the Roberts testimony. Thompson is unfazed; he says he sort of knew Roberts. We watch for a reaction, but Thompson reveals nothing. I begin to wonder if he even remembers our previous conversation.We let it slide for the moment and move on to other subjects, but later I return to the topic. Since Roberts turned out to be a real person, maybe there was some kind of miscommunication before, I suggest. Is Thompson certain he used the name as an alias? “Yeah, I’ve used it,” he says.“So it’s just a coincidence that this guy’s name is also Rick Roberts?”Thompson allows one of his trademark long pauses. “I guess,” he finally says. “I used a lot of names in different places. I can walk into a restaurant up there in Arkansas and they’ll call me Mr. Morrow.”But, Fuller says, “You wouldn’t appear before a grand jury and raise your right hand and say your name is Rick Roberts and ‘I live at XYZ address,’ would you?”Perusing the testimony again, Thompson finds a way to answer. “I’ll tell you one thing, I never worked for Rusty Griffith.”As we mull over the riddle, Thompson grins, head tilted back. “When he smiles like that,” Fuller says to me, “there’s more behind the story.”Then it strikes me: What if Thompson used Roberts’s name on the Leslie job because he wanted their identities to blend? What if the  was to be confused with Rick Roberts, to deflect attention in the ensuing investigation?When I put these questions to Thompson, he nods. “That could be what happened.”The genius of this washes over me. If the police were given the name of a nonexistent person, that would raise suspicion. So Thompson could have used an alias that didn’t require him to invent a new identity. He could have borrowed the name of a real person—someone he knew would be quickly dismissed as a suspect, and whose name was commonplace to boot. When I advance this idea, Thompson, arms folded, says: “Might’ve happened just like that.”Fuller and I sit, thunderstruck. This, it occurs to me, is literally how you get away with murder.Fuller and I then try to get a deeper understanding of why the hit was ordered. Thompson claims that Leslie was given information by someone associated with Carlos Marcello about the mob’s attempts to infiltrate the state firefighters’ union. “Organized crime wasn’t controlling that union here [in Louisiana] at the time—not like they did up north—and they were trying to get that established here,” Thompson says.These efforts may or may not have included the bribe attempts he mentioned earlier, like the one that former state senator Anthony Guarisco declined. But it wasn’t the bribes that led to Leslie’s demise; it was his loose talk about them. Thompson tells us that Leslie scheduled a meeting with law enforcement to discuss the payoffs on July 9, only hours after he was murdered. “You can’t do what he was doing and get away with it, not back then,” Thompson says. “Or do what he was doing to the person he tried to do it to.”Leslie’s stature made him a real threat. “He had the power of the media,” Fuller says.“Right, that was his power right there,” Thompson replies. “He had a lot of ears, and he was using some of those ears in the wrong way, and it was getting back to the right people.”We later confirm a key element of this account: According to an investigative series in Baton Rouge’s, Leslie was due in a meeting with U.S. Attorney Douglas Gonzales Sr. the morning after he was killed. The timing of the hit worked in another way, too: By waiting until after the Right to Work vote, Marcello’s people could pin the hit on D’Artois—who was in ever deeper legal trouble.Outside the prison, Fuller and I linger in the humid air for a while, somewhat dazed and in disbelief. It took five trips to Louisiana and nearly 40 hours of conversation. But after three years circling a case that had gone ice cold—47 years after Jim Leslie was shot dead—everything seems to fit together. to Shreveport, in March 2025, a source tells us that Louisiana has created a cold-case task force to try to pin Jim Leslie’s murder on Larry Thompson. The unit’s existence promises something we weren’t sure we’d obtain on our own: closure.It’s unlikely that filing charges will accomplish more than that. Wade’s new warden, Michele Dauzat, tells me that Thompson, now 81, has been moved to another prison. He isn’t responding to our communications like he used to. When Thompson was at Wade, he answered emails within hours; now he doesn’t reply for days, if at all. Extending his sentence would be a symbolic gesture, given the term he’s currently serving, his advanced age, and some kidney-related health problems he mentioned to us. He might agree to a deal to shorten his son’s prison term, but it’s unclear whether he’d be offered one.Still, there are plenty of people that resolution would mean a lot to—most prominently Elliott Stonecipher. “Everything that happened to Jim is insane,” he says. “Everything organized labor got away with, not to mention organized crime, is insane.”Stonecipher has told us countless hours’ worth of stories over the years about the ways Shreveport broke his heart. The racial schisms. The corruption. But one is particularly personal: Jim Leslie and George D’Artois were both buried in the city’s venerable Forest Park Cemetery, less than a year apart. It bothered Stonecipher that the crowd for D’Artois’s funeral rivaled that of Leslie’s. “They couldn’t get in all of the cars that wanted to be in the funeral,” he recalls. “And Forest Park’s road goes on forever.”For Stonecipher, it was a reminder that Shreveport had learned nothing from the senseless tragedy of Leslie’s death. “I never got over that, and frankly, it changed everything that I ever felt about my hometown,” Stonecipher says. “It never came back.”As for Myron Fuller, Shreveport was the one place during his 31 years in the FBI where he struggled to find a gear. Clandestine forces conspired against him, investigations blinked out, people died. He came back looking for a reckoning. He needed to know it wasn’t all on him.He comes away not only with a deeper understanding of his time in Shreveport, but with something more: He may have cracked open one of the most famous unsolved murders in Louisiana history. “When you’re in the middle of the forest, you can’t see the trees,” he says. “When you get out of there and look down from a bird’s-eye view? Holy shit. So that’s what this has done for me.”Our last night in Shreveport, we have no plan. We drive for a bit, get hungry, and spontaneously land at Ernest’s Orleans, a place long known as an organized-crime hangout. It still has an unreconstructed mafioso vibe, as if it were lifted straight from a 1970s Scorsese film. A guy wearing a gigantic cowboy hat sits at a table near ours with an ostentatiously dressed woman. The waiter, a portly, uniformed man who also seems from another era, confirms the mob history: Yes, Carlos Marcello came here.Fuller orders a whiskey on the rocks, looks around. When he talks about Shreveport, the edge is gone. The ghosts have vanished. But only for him. For the city, they remain.]]></content:encoded></item><item><title>Feasibility study of a mission to Sedna – Nuclear propulsion and solar sailing</title><link>https://arxiv.org/abs/2506.17732</link><author>speckx</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:08:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fei-Fei Li: Spatial intelligence is the next frontier in AI [video]</title><link>https://www.youtube.com/watch?v=_PioN-CpOP0</link><author>sandslash</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:00:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Grammarly acquires Superhuman</title><link>https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/</link><author>thm</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:00:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Converting a large mathematical software package written in C++ to C++20 modules</title><link>https://arxiv.org/abs/2506.21654</link><author>vblanco</author><category>hn</category><pubDate>Tue, 1 Jul 2025 13:46:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I built the tool I wished existed for moving Stripe between countries</title><link>https://www.stripemove.com/</link><author>felphos</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 12:52:50 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages</title><link>https://simedw.com/2025/06/23/introducing-spegel/</link><author>simedw</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 12:49:42 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[TL;DR Spegel is a proof-of-concept terminal web browser that feeds HTML through an LLM and renders the result as markdown directly in your terminal.Two weekends ago, after my family had gone to sleep, I found myself unsupervised with a laptop and an itch to build something interesting. A couple of hours later, I had a minimal web browser running in my terminal (no JavaScript, GET requests only) that transformed web content based on my custom prompts.Then, a few days later, Google released Gemini 2.5 Pro Lite, significantly faster inference speed, suddenly my little weekend hack became a tad more practical.Adapting content to suit individual needs isn’t a new idea, think about translating books or summarising lengthy articles. However, this used to be slow and expensive. LLMs have changed this dramatically, making these transformations quick and easy.Spegel ("mirror" in Swedish) lets you explore web content through personalized views using your own prompts. A single page can have multiple views, maybe one simplifying everything down to ELI5 or another highlighting key actions. It's entirely up to you and your prompting skills. Sometimes you don't want to read through someone's life story just to get to a recipe.
A previous version of this screenshot showed an incorrect recipe on the right. That was due to a bug where large websites got truncated. Thanks to everyone who pointed it out!The pipeline is straightforward.Spegel fetches HTML content, processes it through an LLM using prompts stored in a config file (~/.spegel.toml), and outputs markdown rendered via Textual. Prompts and views can be adjusted live during a browsing session.This was my first experience using Textual for a TUI, and it's been delightful, possibly too delightful, as I found myself adding a few unnecessary interface elements just because it was easy.One gotcha was ensuring only completed lines (ending in newline characters) were streamed; otherwise, the markdown renderer would parse incomplete markdown and fail to recover formattingThere are a lot of great terminal browsers out there, Lynx and Links2 are close to my heart. There are also modern attempts like Browsh that can even render graphs using half-block Unicode characters (▄█). Spegel isn’t meant to replace these, it’s more of an exploration or proof-of-concept. It currently doesn't support POST requests (though I have some ideas on handling  elements by creating on-the-fly UIs).But most modern websites aren’t designed with terminal browsing in mind. They rely on CSS and JS, making them cumbersome in small terminal windows, full of clutter and noise. Spegel tries to clear away distractions, providing content tailored more closely to your needs.Spegel is still in the early stages, so expect some rough edges, but it’s usable and kind of fun to play with.Then just run it with a URL:spegelsimedw.comDon't forget to configure your own , (example)Want to check out the source or contribute? It’s all on GitHub:]]></content:encoded></item><item><title>Show HN: Jobs by Referral: Find jobs in your LinkedIn network</title><link>https://jobsbyreferral.com/</link><author>nicksergeant</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 12:47:06 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[JobsByReferral analyzes your professional network to find job openings at companies where you have connections. Get referred by people you already know and dramatically increase your chances of landing interviews.]]></content:encoded></item><item><title>ASCIIMoon: The moon&apos;s phase live in ASCII art</title><link>https://asciimoon.com/</link><author>zayat</author><category>hn</category><pubDate>Tue, 1 Jul 2025 10:53:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
View & cycle through The Moon's phases day to day - rendered in ASCII
        art.
]]></content:encoded></item><item><title>Scientists identify culprit behind biggest-ever U.S. honey bee die-off</title><link>https://www.science.org/content/article/scientists-identify-culprit-behind-biggest-ever-u-s-honeybee-die</link><author>pseudolus</author><category>hn</category><pubDate>Tue, 1 Jul 2025 10:35:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cloudflare to introduce pay-per-crawl for AI bots</title><link>https://blog.cloudflare.com/introducing-pay-per-crawl/</link><author>scotchmi_st</author><category>hn</category><pubDate>Tue, 1 Jul 2025 10:20:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A changing landscape of consumption Many publishers, content creators and website owners currently feel like they have a binary choice — either leave the front door wide open for AI to consume everything they create, or create their own walled garden. But what if there was another way?At Cloudflare, we started from a simple principle: we wanted content creators to have control over who accesses their work. If a creator wants to block all AI crawlers from their content, they should be able to do so. If a creator wants to allow some or all AI crawlers full access to their content for free, they should be able to do that, too. Creators should be in the driver’s seat.After hundreds of conversations with news organizations, publishers, and large-scale social media platforms, we heard a consistent desire for a third path: They’d like to allow AI crawlers to access their content, but they’d like to get compensated. Currently, that requires knowing the right individual and striking a one-off deal, which is an insurmountable challenge if you don’t have scale and leverage. What if I could charge a crawler? We believe your choice need not be binary — there should be a third, more nuanced option: You can charge for access. Instead of a blanket block or uncompensated open access, we want to empower content owners to monetize their content at Internet scale.Introducing pay per crawlPay per crawl, in private beta, is our first experiment in this area. Pay per crawl integrates with existing web infrastructure, leveraging HTTP status codes and established authentication mechanisms to create a framework for paid content access. Each time an AI crawler requests content, they either present payment intent via request headers for successful access (), or receive a  response with pricing. Cloudflare acts as the Merchant of Record for pay per crawl and also provides the underlying technical infrastructure.Publisher controls and pricingPay per crawl grants domain owners full control over their monetization strategy. They can define a flat, per-request price across their entire site. Publishers will then have three distinct options for a crawler: Grant the crawler free access to content. Require payment at the configured, domain-wide price. Deny access entirely, with no option to pay.An important mechanism here is that even if a crawler doesn’t have a billing relationship with Cloudflare, and thus couldn’t be charged for access, a publisher can still choose to ‘charge’ them. This is the functional equivalent of a network level block (an HTTP  response where no content is returned) — but with the added benefit of telling the crawler there could be a relationship in the future. While publishers currently can define a flat price across their entire site, they retain the flexibility to bypass charges for specific crawlers as needed. This is particularly helpful if you want to allow a certain crawler through for free, or if you want to negotiate and execute a content partnership outside the pay per crawl feature. To ensure integration with each publisher’s existing security posture, Cloudflare enforces Allow or Charge decisions via a rules engine that operates only after existing WAF policies and bot management or bot blocking features have been applied.Payment headers and accessAs we were building the system, we knew we had to solve an incredibly important technical challenge: ensuring we could charge a specific crawler, but prevent anyone from spoofing that crawler. Thankfully, there’s a way to do this using  proposals.Generating an Ed25519 key pair, and making the -formatted public key available in a hosted directoryRegistering with Cloudflare to provide the URL of your key directory and user agent information.Once registration is accepted, crawler requests should always include , , and  headers to identify your crawler and discover paid resources.GET /example.html
Signature-Agent: "https://signature-agent.example.com"
Signature-Input: sig2=("@authority" "signature-agent")
 ;created=1735689600
 ;keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U"
 ;alg="ed25519"
 ;expires=1735693200
;nonce="e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg=="
 ;tag="web-bot-auth"
Signature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:Once a crawler is set up, determination of whether content requires payment can happen via two flows:Reactive (discovery-first)Should a crawler request a paid URL, Cloudflare returns an HTTP 402 Payment Required response, accompanied by a  header. This signals that payment is required for the requested resource.HTTP 402 Payment Required
crawler-price: USD XX.XX The crawler can then decide to retry the request, this time including a  header to indicate agreement to pay the configured price.GET /example.html
crawler-exact-price: USD XX.XX Alternatively, a crawler can preemptively include a  header in its initial request.GET /example.html
crawler-max-price: USD XX.XXIf the price configured for a resource is equal to or below this specified limit, the request proceeds, and the content is served with a successful  response, confirming the charge:HTTP 200 OK
crawler-charged: USD XX.XX 
server: cloudflareIf the amount in a  request is greater than the content owner’s configured price, only the configured price is charged. However, if the resource’s configured price exceeds the maximum price offered by the crawler, an  response is returned, indicating the specified cost.  Only a single price declaration header,  or , may be used per request.The  or  headers explicitly declare the crawler's willingness to pay. If all checks pass, the content is served, and the crawl event is logged. If any aspect of the request is invalid, the edge returns an HTTP 402 Payment Required response.Crawler operators and content owners must configure pay per crawl payment details in their Cloudflare account. Billing events are recorded each time a crawler makes an authenticated request with payment intent and receives an HTTP 200-level response with a  header. Cloudflare then aggregates all the events, charges the crawler, and distributes the earnings to the publisher.Content for crawlers today, agents tomorrow At its core, pay per crawl begins a technical shift in how content is controlled online. By providing creators with a robust, programmatic mechanism for valuing and controlling their digital assets, we empower them to continue creating the rich, diverse content that makes the Internet invaluable. We expect pay per crawl to evolve significantly. It’s very early: we believe many different types of interactions and marketplaces can and should develop simultaneously. We are excited to support these various efforts and open standards.For example, a publisher or new organization might want to charge different rates for different paths or content types. How do you introduce dynamic pricing based not only upon demand, but also how many users your AI application has? How do you introduce granular licenses at internet scale, whether for training, inference, search, or something entirely new?The true potential of pay per crawl may emerge in an agentic world. What if an agentic paywall could operate entirely programmatically? Imagine asking your favorite deep research program to help you synthesize the latest cancer research or a legal brief, or just help you find the best restaurant in Soho — and then giving that agent a budget to spend to acquire the best and most relevant content. By anchoring our first solution on , we enable a future where intelligent agents can programmatically negotiate access to digital resources. Pay per crawl is currently in private beta. We’d love to hear from you if you’re either a crawler interested in paying to access content or a content creator interested in charging for access. You can reach out to us at http://www.cloudflare.com/paypercrawl-signup/ or contact your Account Executive if you’re an existing Enterprise customer.]]></content:encoded></item><item><title>Show HN: ToplingDB - A Persistent Key-Value Store for External Storage</title><link>https://github.com/topling/toplingdb</link><author>rockeetterark</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 10:07:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[As the creator of TerarkDB (acquired by ByteDance in 2019), I have developed ToplingDB in recent years.ToplingDB is forked from RocksDB,   where   we have replaced almost all components with more efficient alternatives(db_bench shows ToplingDB is about ~8x faster than RocksDB):* MemTable: SkipList is replaced by CSPP(Crash Safe Parallel Patricia trie), which is 8x faster.* SST: BlockBasedTable is replaced by ToplingZipTable, implemented by searchable compression algo, it is very small and fast, typically less than 1μs per lookup:  * Keys/Indexes are compressed   using NestLoudsTrie(a multi-layer nesting LOUDS succinct trie).

  * Values in a SST are compressed   together with better zip ratio than zstd, and can unzip by a single value at 1GB/sec.

  * BlockCache is no longer needed, double caching(BlockCache & PageCache) is avoided

Other hotspots are also improved:* Flush MemTable to L0 is omited, greatly reducing write amp and is very friendly for large(GB) MemTable  * MemTable   serves as the index of Key to "value position in WAL log"

  * Since WAL file content almost always in page cache, thus value content can be efficiently accessed by mmap

  * When Flush happens, MemTable is dumpped as an SST and WAL is treated as a blob file

    * CSPP MemTable use integer index instead of physical pointers, thus in-memory format is exactly same with in-file format

* Prefix cache for searching candidate SSTs and prefix cache for scanning by iterators  * Caching fixed len key prefix into an array, binary search it as an uint array

* Distributed compaction(superior replacement to rocksdb remote compaction)  * Gracefully support MergeOperator, CompactionFilter, PropertiesCollector...

  * Out of the box, development efforts are significantly reduced

  * Very easy to share compaction service on spot instances for many DB nodes

Useful Bonus Feature:* Config by json/yaml: can config almost all features* Optional embeded WebView: show db structures in web browser, refreshing pages like animation* Online update db configs by httpMySQL integration, ToplingDB has integrated into MySQL by MyTopling, which is forked from MyRocks with great improvements, like improvements of ToplingDB on RocksDB:* WBWI(WriteBatchWithIndex): like MemTable, SkipList is replace with CSPP, 20x faster(speedup is more than MemTable).* LockManager & LockTracker: 10x faster* Encoding & Decoding: 5x fasterMyRocks has many disadvantages compared to InnoDB, while MyTopling outperforms InnoDB at almost all aspect - excluding feature differences.We have create ~100 PRs for RocksDB, in which ~40 were accepted. Our PRs are mostly "small" changes, since big changes are not likely accepted.ToplingDB has been deployed in numerous production environments.]]></content:encoded></item><item><title>Evidence of a 12,800-year-old shallow airburst depression in Louisiana</title><link>https://www.scienceopen.com/hosted-document?doi=10.14293/ACI.2025.0004</link><author>keepamovin</author><category>hn</category><pubDate>Tue, 1 Jul 2025 07:55:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenFLOW – Quickly make beautiful infrastructure diagrams local to your machine</title><link>https://github.com/stan-smith/OpenFLOW</link><author>x0z</author><category>hn</category><pubDate>Tue, 1 Jul 2025 06:29:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>About AI Evals</title><link>https://hamel.dev/blog/posts/evals-faq/</link><author>TheIronYuppie</author><category>hn</category><pubDate>Tue, 1 Jul 2025 02:48:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This post curates the most common questions Shreya and I have recieved from teaching 700+ engineers & PMs in AI Evals. Warning: These are sharp opinions about what works in most cases. They are not universal truths. Use your judgment. (we have to get back to building). Here is a 35% discount code for readers of this post. 👈Question: Should I avoid using RAG for my AI application after reading that “RAG is dead” for coding agents?Many developers are confused about when and how to use RAG after reading articles claiming “RAG is dead.” Understanding what RAG actually means versus the narrow marketing definitions will help you make better architectural decisions for your AI applications.The viral article claiming RAG is dead specifically argues against using naive vector database retrieval for autonomous coding agents, not RAG as a whole. This is a crucial distinction that many developers miss due to misleading marketing.RAG simply means Retrieval-Augmented Generation - using retrieval to provide relevant context that improves your model’s output. The core principle remains essential: your LLM needs the right context to generate accurate answers. The question isn’t whether to use retrieval, but how to retrieve effectively.For coding applications, naive vector similarity search often fails because code relationships are complex and contextual. Instead of abandoning retrieval entirely, modern coding assistants like Claude Code still uses retrieval —they just employ agentic search instead of relying solely on vector databases.similar to how human developers work.You have multiple retrieval strategies available, ranging from simple keyword matching to embedding similarity to LLM-powered relevance filtering. The optimal approach depends on your specific use case, data characteristics, and performance requirements. Many production systems combine multiple strategies or use multi-hop retrieval guided by LLM agents.Unforunately, “RAG” has become a buzzword with no shared definition. Some people use it to mean any retrieval system, others restrict it to vector databases. Focus on the fundamental goal: getting your LLM the context it needs to succeed. Whether that’s through vector search, agentic exploration, or hybrid approaches is a product and engineering decision that requires understanding your users’ failure modes and usage patterns.Rather than following categorical advice to avoid or embrace RAG, experiment with different retrieval approaches and measure what works best for your application.Q: Can I use the same model for both the main task and evaluation?For LLM-as-Judge selection, using the same model is usually fine because the judge is doing a different task than your main LLM pipeline. The judges we recommend building do scoped binary classification tasks. Focus on achieving high True Positive Rate (TPR) and True Negative Rate (TNR) with your judge on a held out labeled test set rather than avoiding the same model family. You can use these metrics on the test set to understand how well your judge is doing.When selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once you’ve established reliable evaluation criteria. We do not recommend using the same model for open ended preferences or response quality (but we don’t recommend building judges this way in the first place!).Q: How much time should I spend on model selection?Many developers fixate on model selection as the primary way to improve their LLM applications. Start with error analysis to understand your failure modes before considering model switching. As Hamel noted in office hours, “I suggest not thinking of switching model as the main axes of how to improve your system off the bat without evidence. Does error analysis suggest that your model is the problem?”Q: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?Engineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.Binary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.Having binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis - you don’t waste time debating whether something is a 3 or 4.For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track “4 out of 5 expected facts included” as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.Start with binary labels to understand what ‘bad’ looks like. Numeric labels are advanced and usually not necessary.Q: How do I debug multi-turn conversation traces?Start simple. Check if the whole conversation met the user’s goal with a pass/fail judgment. Look at the entire trace and focus on the first upstream failure. Read the user-visible parts first to understand if something went wrong. Only then dig into the technical details like tool calls and intermediate steps.When you find a failure, reproduce it with the simplest possible test case. Here’s a example: suppose a shopping bot gives the wrong return policy on turn 4 of a conversation. Before diving into the full multi-turn complexity, simplify it to a single turn: “What is the return window for product X1000?” If it still fails, you’ve proven the error isn’t about conversation context - it’s likely a basic retrieval or knowledge issue you can debug more easily.For generating test cases, you have two main approaches. First, you can simulate users with another LLM to create realistic multi-turn conversations. Second, use “N-1 testing” where you provide the first N-1 turns of a real conversation and test what happens next. The N-1 approach often works better since it uses actual conversation prefixes rather than fully synthetic interactions (but is less flexible and doesn’t test the full conversation). User simulation is getting better as models improve. Keep an eye on this space.The key is balancing thoroughness with efficiency. Not every multi-turn failure requires multi-turn analysis.Q: Should I build automated evaluators for every failure mode I find?Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesn’t meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.Only build expensive evaluators for problems you’ll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that can’t be captured by simple rules.Q: How many people should annotate my LLM outputs?For most small to medium-sized companies, appointing a single domain expert as a “benevolent dictator” is the most effective approach. This person—whether it’s a psychologist for a mental health chatbot, a lawyer for legal document analysis, or a customer service director for support automation—becomes the definitive voice on quality standards.A single expert eliminates annotation conflicts and prevents the paralysis that comes from “too many cooks in the kitchen”. The benevolent dictator can incorporate input and feedback from others, but they drive the process. If you feel like you need five subject matter experts to judge a single interaction, it’s a sign your product scope might be too broad.However, larger organizations or those operating across multiple domains (like a multinational company with different cultural contexts) may need multiple annotators. When you do use multiple people, you’ll need to measure their agreement using metrics like Cohen’s Kappa, which accounts for agreement beyond chance. However, use your judgment. Even in larger companies, a single expert is often enough.Start with a benevolent dictator whenever feasible. Only add complexity when your domain demands it.Q: What gaps in eval tooling should I be prepared to fill myself?Most eval tools handle the basics well: logging complete traces, tracking metrics, prompt playgrounds, and annotation queues. These are table stakes. Here are four areas where you’ll likely need to supplement existing tools.Watch for vendors addressing these gaps—it’s a strong signal they understand practitioner needs.1. Error Analysis and Pattern DiscoveryAfter reviewing traces where your AI fails, can your tooling automatically cluster similar issues? For instance, if multiple traces show the assistant using casual language for luxury clients, you need something that recognizes this broader “persona-tone mismatch” pattern. We recommend building capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.2. AI-Powered Assistance Throughout the WorkflowThe most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping categorize your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like “wrong tone for investor,” “too casual for luxury buyer,” etc. Your tooling should recognize these as the same underlying pattern and suggest a unified “persona-tone mismatch” category.You’ll also want AI assistance in proposing fixes. After identifying 20 cases where your assistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?Additionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like Julius,Hex or SolveIt. These help me discover insights like “location ambiguity errors spike 3x when users mention neighborhood names” or “tone mismatches occur 80% more often in email generation than other modalities.”3. Custom Evaluators Over Generic MetricsBe prepared to build most of your evaluators from scratch. Generic metrics like “hallucination score” or “helpfulness rating” rarely capture what actually matters for your application—like proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.4. APIs That Support Custom Annotation AppsCustom annotation interfaces work best for most teams. This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldn’t have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true bulk export capabilities and, crucially, APIs that let you write annotations back efficiently.Q: What is the best approach for generating synthetic data?A common mistake is prompting an LLM to  without structure, resulting in generic, repetitive outputs. A structured approach using dimensions produces far better synthetic data for testing LLM applications.Start by defining dimensions: categories that describe different aspects of user queries. Each dimension captures one type of variation in user behavior. For example:For a recipe app, dimensions might include Dietary Restriction (, , ), Cuisine Type (, , ), and Query Complexity (, , ).For a customer support bot, dimensions could be Issue Type (, , ), Customer Mood (, , ), and Prior Context (, , ).Choose dimensions that target likely failure modes. If you suspect your recipe app struggles with scaling ingredients for large groups or your support bot mishandles angry customers, make those dimensions. Use your application first—you need hypotheses about where failures occur. Without this, you’ll generate useless test data.Once you have dimensions, create tuples: specific combinations selecting one value from each dimension. A tuple like (, , ) represents a particular use case. Write 20 tuples manually to understand your problem space, then use an LLM to scale up.The two-step generation process is important. First, have the LLM generate structured tuples. Then, in a separate prompt, convert each tuple to a natural language query. This separation prevents repetitive phrasing. For the vegan Italian tuple above, you might get "I need a dairy-free lasagna recipe that I can prep the day before."Don’t generate synthetic data for problems you can fix immediately. If your prompt never mentions handling dietary restrictions, fix the prompt rather than generating hundreds of specialized queries. Save synthetic data for complex issues requiring iteration—like an LLM consistently failing at ingredient scaling math or misinterpreting ambiguous requests.After iterating on your tuples and prompts, run these synthetic queries through your actual system to capture full traces. Sample 100 traces for error analysis. This number provides enough traces to manually review and identify failure patterns without being overwhelming. Rather than generating thousands of similar queries, ensure your 100 traces cover diverse combinations across your dimensions—this variety will reveal more failure modes than sheer volume.Q: How do I approach evaluation when my system handles diverse user queries?Complex applications often support vastly different query patterns—from “What’s the return policy?” to “Compare pricing trends across regions for products matching these criteria.” Each query type exercises different system capabilities, leading to confusion on how to design eval criteria. Your evaluation strategy should emerge from observed failure patterns (e.g. error analysis), not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your system’s actual behavior guide where you invest evaluation effort.During error analysis, you’ll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether they’re simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis should drive your evaluation priorities. It could be that query category is a fine way to group failures, but you don’t know that until you’ve analyzed your data. (we have to get back to building). Here is a 35% discount code for readers of this post. 👈Q: How do I choose the right chunk size for my document processing tasks?Unlike RAG, where chunks are optimized for retrieval, document processing assumes the model will see every chunk. The goal is to split text so the model can reason effectively without being overwhelmed. Even if a document fits within the context window, it might be better to break it up. Long inputs can degrade performance due to attention bottlenecks, especially in the middle of the context. Two task types require different strategies:1. Fixed-Output Tasks → Large ChunksThese are tasks where the output length doesn’t grow with input: extracting a number, answering a specific question, classifying a section. For example:“What’s the penalty clause in this contract?”“What was the CEO’s salary in 2023?”Use the largest chunk (with caveats) that likely contains the answer. This reduces the number of queries and avoids context fragmentation. However, avoid adding irrelevant text. Models are sensitive to distraction, especially with large inputs. The middle parts of a long input might be under-attended. Furthermore, if cost and latency are a bottleneck, you should consider preprocessing or filtering the document (via keyword search or a lightweight retriever) to isolate relevant sections before feeding a huge chunk.2. Expansive-Output Tasks → Smaller ChunksThese include summarization, exhaustive extraction, or any task where output grows with input. For example:“List all customer complaints”In these cases, smaller chunks help preserve reasoning quality and output completeness. The standard approach is to process each chunk independently, then aggregate results (e.g., map-reduce). When sizing your chunks, try to respect content boundaries like paragraphs, sections, or chapters. Chunking also helps mitigate output limits. By breaking the task into pieces, each piece’s output can stay within limits.It’s important to recognize why chunk size affects results. A larger chunk means the model has to reason over more information in one go – essentially, a heavier cognitive load. LLMs have limited capacity to retain and correlate details across a long text. If too much is packed in, the model might prioritize certain parts (commonly the beginning or end) and overlook or “forget” details in the middle. This can lead to overly coarse summaries or missed facts. In contrast, a smaller chunk bounds the problem: the model can pay full attention to that section. You are trading off global context for local focus.No rule of thumb can perfectly determine the best chunk size for your use case – you should validate with experiments. The optimal chunk size can vary by domain and model. I treat chunk size as a hyper parameter to tune.Q: How should I approach evaluating my RAG system?RAG systems have two distinct components that require different evaluation approaches: retrieval and generation.The retrieval component is a search problem. Evaluate it using traditional information retrieval (IR) metrics. Common examples include Recall@k (of all relevant documents, how many did you retrieve in the top k?), Precision@k (of the k documents retrieved, how many were relevant?), or MRR (how high up was the first relevant document?). The specific metrics you choose depend on your use case. These metrics are pure search metrics that measure whether you’re finding the right documents (more on this below).To evaluate retrieval, create a dataset of queries paired with their relevant documents. Generate this synthetically by taking documents from your corpus, extracting key facts, then generating questions those facts would answer. This reverse process gives you query-document pairs for measuring retrieval performance without manual annotation.For the generation component—how well the LLM uses retrieved context, whether it hallucinates, whether it answers the question—use the same evaluation procedures covered throughout this course: error analysis to identify failure modes, collecting human labels, building LLM-as-judge evaluators, and validating those judges against human annotations.Jason Liu’s “There Are Only 6 RAG Evals” provides a framework that maps well to this separation. His Tier 1 covers traditional IR metrics for retrieval. Tiers 2 and 3 evaluate relationships between Question, Context, and Answer—like whether the context is relevant (C|Q), whether the answer is faithful to context (A|C), and whether the answer addresses the question (A|Q).In addition to Jason’s six evals, error analysis on your specific data may reveal domain-specific failure modes that warrant their own metrics. For example, a medical RAG system might consistently fail to distinguish between drug dosages for adults versus children, or a legal RAG might confuse jurisdictional boundaries. These patterns emerge only through systematic review of actual failures. Once identified, you can create targeted evaluators for these specific issues beyond the general framework.Finally, when implementing Jason’s Tier 2 and 3 metrics, don’t just use prompts off the shelf. The standard LLM-as-judge process requires several steps: error analysis, prompt iteration, creating labeled examples, and measuring your judge’s accuracy against human labels. Once you know your judge’s True Positive and True Negative rates, you can correct its estimates to determine the actual failure rate in your system. Skip this validation and your judges may not reflect your actual quality criteria.In summary, debug retrieval first using IR metrics, then tackle generation quality using properly validated LLM judges.Q: What makes a good custom interface for reviewing LLM outputs?Great interfaces make human review fast, clear, and motivating. We recommend building your own annotation tool customized to your domain. The following features are possible enhancements we’ve seen work well, but you don’t need all of them. The screenshots shown are illustrative examples to clarify concepts. In practice, I rarely implement all these features in a single app. It’s ultimately a judgment call based on your specific needs and constraints.1. Render Traces Intelligently, Not Generically: Present the trace in a way that’s intuitive for the domain. If you’re evaluating generated emails, render them to look like emails. If the output is code, use syntax highlighting. Allow the reviewer to see the full trace (user input, tool calls, and LLM reasoning), but keep less important details in collapsed sections that can be expanded. Here is an example of a custom annotation tool for reviewing real estate assistant emails:A custom interface for reviewing emails for a real estate assistant.2. Show Progress and Support Keyboard Navigation: Keep reviewers in a state of flow by minimizing friction and motivating completion. Include progress indicators (e.g., “Trace 45 of 100”) to keep the review session bounded and encourage completion. Enable hotkeys for navigating between traces (e.g., N for next), applying labels, and saving notes quickly. Below is an illustration of these features:An annotation interface with a progress bar and hotkey guide4. Trace navigation through clustering, filtering, and search: Allow reviewers to filter traces by metadata or search by keywords. Semantic search helps find conceptually similar problems. Clustering similar traces (like grouping by user persona) lets reviewers spot recurring issues and explore hypotheses. Below is an illustration of these features:Cluster view showing groups of emails, such as property-focused or client-focused examples. Reviewers can drill into a group to see individual traces.5. Prioritize labeling traces you think might be problematic: Surface traces flagged by guardrails, CI failures, or automated evaluators for review. Provide buttons to take actions like adding to datasets, filing bugs, or re-running pipeline tests. Display relevant context (pipeline version, eval scores, reviewer info) directly in the interface to minimize context switching. Below is an illustration of these ideas:A trace view that allows you to quickly see auto-evaluator verdict, add traces to dataset or open issues. Also shows metadata like pipeline version, reviewer info, and more.General Principle: Keep it minimalKeep your annotation interface minimal. Only incorporate these ideas if they provide a benefit that outweighs the additional complexity and maintenance overhead.Q: How much of my development budget should I allocate to evals?It’s important to recognize that evaluation is part of the development process rather than a distinct line item, similar to how debugging is part of software development.You should always be doing error analysis. When you discover issues through error analysis, many will be straightforward bugs you’ll fix immediately. These fixes don’t require separate evaluation infrastructure as they’re just part of development.The decision to build automated evaluators comes down to cost-benefit analysis. If you can catch an error with a simple assertion or regex check, the cost is minimal and probably worth it. But if you need to align an LLM-as-judge evaluator, consider whether the failure mode warrants that investment.In the projects we’ve worked on, we’ve spent 60-80% of our development time on error analysis and evaluation. Expect most of your effort to go toward understanding failures (i.e. looking at data) rather than building automated checks.Be wary of optimizing for high eval pass rates. If you’re passing 100% of your evals, you’re likely not challenging your system enough. A 70% pass rate might indicate a more meaningful evaluation that’s actually stress-testing your application. Focus on evals that help you catch real issues, not ones that make your metrics look good.Q: What’s the difference between guardrails & evaluators?Guardrails are  that sit directly in the request/response path. They validate inputs or outputs  anything reaches a user, so they typically are: – typically a few milliseconds of latency budget. – regexes, keyword block-lists, schema or type validators, lightweight classifiers.Targeted at clear-cut, high-impact failures – PII leaks, profanity, disallowed instructions, SQL injection, malformed JSON, invalid code syntax, etc.If a guardrail triggers, the system can redact, refuse, or regenerate the response. Because these checks are user-visible when they fire, false positives are treated as production bugs; teams version guardrail rules, log every trigger, and monitor rates to keep them conservative.On the other hand, evaluators typically run  a response is produced. Evaluators measure qualities that simple rules cannot, such as factual correctness, completeness, etc. Their verdicts feed dashboards, regression tests, and model-improvement loops, but they do not block the original answer.Evaluators are usually run asynchronously or in batch to afford heavier computation such as a LLM-as-a-Judge. Inline use of an LLM-as-Judge is possible  when the latency budget and reliability targets allow it. Slow LLM judges might be feasible in a cascade that runs on the minority of borderline cases.Apply guardrails for immediate protection against objective failures requiring intervention. Use evaluators for monitoring and improving subjective or nuanced criteria. Together, they create layered protection.Word of caution: Do not use llm guardrails off the shelf blindly. Always look at the prompt.Q: What’s a minimum viable evaluation setup?Start with error analysis, not infrastructure. Spend 30 minutes manually reviewing 20-50 LLM outputs whenever you make significant changes. Use one domain expert who understands your users as your quality decision maker (a “benevolent dictator”).If possible,  to help you review traces and analyze data. In our opinion, this is the single most effective tool for evals because you can write arbitrary code, visualize data, and iterate quickly. You can even build your own custom annotation interface right inside notebooks, as shown in this video.Q: How do I evaluate agentic workflows?We recommend evaluating agentic workflows in two phases:1. End-to-end task success. Treat the agent as a black box and ask “did we meet the user’s goal?”. Define a precise success rule per task (exact answer, correct side-effect, etc.) and measure with human or aligned LLM judges. Take note of the first upstream failure when conducting error analysis.Once error analysis reveals which workflows fail most often, move to step-level diagnostics to understand why they’re failing.2. Step-level diagnostics. Assuming that you have sufficiently instrumented your system with details of tool calls and responses, you can score individual components such as: - : was the selected tool appropriate? - : were inputs complete and well-formed? - : did the agent recover from empty results or API failures? - : did it preserve earlier constraints? - : how many steps, seconds, and tokens were spent? - : for long workflows verify key milestones.Example: “Find Berkeley homes under $1M and schedule viewings” breaks into: parameters extracted correctly, relevant listings retrieved, availability checked, and calendar invites sent. Each checkpoint can pass or fail independently, making debugging tractable.Use transition failure matrices to understand error patterns. Create a matrix where rows represent the last successful state and columns represent where the first failure occurred. This is a great way to understand where the most failures occur.Transition failure matrix showing hotspots in text-to-SQL agent workflowTransition matrices transform overwhelming agent complexity into actionable insights. Instead of drowning in individual trace reviews, you can immediately see that GenSQL → ExecSQL transitions cause 12 failures while DecideTool → PlanCal causes only 2. This data-driven approach guides where to invest debugging effort. Here is another example from Bryan Bischof, that is also a text-to-SQL agent:Bischof, Bryan “Failure is A Funnel - Data Council, 2025”In this example, Bryan shows variation in transition matrices across experiments. How you organize your transition matrix depends on the specifics of your application. For example, Bryan’s text-to-SQL agent has an inherent sequential workflow which he exploits for further analytical insight. You can watch his full talk for more details.Creating Test Cases for Agent FailuresCreating test cases for agent failures follows the same principles as our previous FAQ on debugging multi-turn conversation traces (i.e. try to reproduce the error in the simplest way possible, only use multi-turn tests when the failure actually requires conversation context, etc.).Q: Seriously Hamel. Stop the bullshit. What’s your favorite eval vendor?Eval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: Langsmith, Arize and Braintrust.When I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - its mainly the human factor that matters, and dare I say, vibes.I have no favorite vendor. At the core, their features are very similar - and I often build custom tools on top of them to fit my needs.My suggestion is to explore the vendors and see which one you like the most.Q: How are evaluations used differently in CI/CD vs. monitoring production?The most important difference between CI vs. production evaluation is the data used for testing.Test datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (that’s why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.For evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.These two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues. (we have to get back to building). Here is a 35% discount code for readers of this post. 👈]]></content:encoded></item><item><title>Writing Code Was Never the Bottleneck</title><link>https://ordep.dev/posts/writing-code-was-never-the-bottleneck</link><author>phire</author><category>hn</category><pubDate>Tue, 1 Jul 2025 01:43:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For years, I’ve felt that writing lines of code  the bottleneck in software engineering.The actual bottlenecks were, and still are, ,  through mentoring and pairing, , , and the human overhead of coordination and communication. All of this wrapped inside the labyrinth of tickets, planning meetings, and agile rituals.These processes, meant to drive quality, often slow us down more than the act of writing code itself because they require thought, shared understanding, and sound judgment.Now, with LLMs making it easy to generate working code faster than ever, a new narrative has emerged: that writing code  the bottleneck, and we’ve finally cracked it.But that’s .The marginal cost of adding new software is approaching , especially with LLMs. But what is the price of , , and  that code? .LLMs shift the workload — they don’t remove itTools like Claude can speed up initial implementation. Still, the result is often more code flowing through systems and more pressure on the people responsible for reviewing, integrating, and maintaining it.This becomes especially clear when:It’s unclear whether the author fully understands what they submitted.The generated code introduces unfamiliar patterns or breaks established conventions.Edge cases and unintended side effects aren’t obvious.We end up in a situation where code is more straightforward to produce but more complex to verify, which doesn’t necessarily make teams move faster overall.It’s not a new challenge. Developers have long joked about , but the velocity and scale that LLMs enable have amplified those copy-paste habits.Understanding code is still the hard part“The biggest cost of code is understanding it — not writing it.”LLMs reduce the time it takes to produce code, but they haven’t changed the amount of effort required to reason about behavior, identify subtle bugs, or ensure long-term maintainability. That work can be even more challenging when reviewers struggle to distinguish between generated and handwritten code or understand why a particular solution was chosen.Teams still rely on trust and shared contextSoftware engineering has always been collaborative. It depends on , , and . However, when code is generated faster than it can be discussed or reviewed, teams risk falling into a mode where quality is assumed rather than ensured. That creates stress on reviewers and mentors, potentially slowing things down in more subtle ways.LLMs are powerful — but they don’t fix the fundamentalsThere’s real value in faster prototyping, scaffolding, and automation. But LLMs don’t remove the need for , , and . If anything, those become even more important as more code gets generated.Yes, the cost of writing code has indeed dropped. But the cost of making sense of it together as a team .That’s still the bottleneck. Let’s not pretend it isn’t.]]></content:encoded></item><item><title>Rust CLI with Clap</title><link>https://tucson-josh.com/posts/rust-clap-cli/</link><author>rajman187</author><category>hn</category><pubDate>Tue, 1 Jul 2025 01:25:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Types are important. In fact, I'd guess that the expressive type system in rust
is the single biggest reason why so many developers love the language. Types
allow us to have a contract between parts of the system about our data and how
to interact with it. All programming languages have the concept of types, but
these exist along several dimensions. Strongly typed vs weakly typed as well as
static vs dynamic typing. Rust stakes out its place as a statically,
strongly typed language.Many languages that are go-to solutions for creating custom command line tools
fall in the opposite quadrant with weak, dynamic typing. Whether looking at
currently popular tooling like python and node.js or more traditional solutions
like awk and perl, they tend to favor a loose approach to types. Perhaps this
is the result of an iterative approach to designing CLI tools that might favor
flexibility. Or it could just be that those languages are already popular,
leading to an abundance of such programs. Regardless of the reasons, I feel that
there is tremendous value for both the developer and user which can arise from
interacting with the command line via the sort of strict contract that rust's
type system enables.I assume that if you're already a rust developer, or at least rust-curious, then
I don't need to convince you of the general value of strong, static typing.
Rather, this is a call to use this same approach for interacting with a command
line user as you would when developing a library or service API.At the very lowest level rust exposes command line arguments through the
 function that returns an  struct, an  for the
 arguments passed to start the program. This is illustrated in the Rust
Book's section on
accepting command line arguments:The naive approach seen above obviously lacks robustness as it relies entirely
on argument positioning and also makes a number of other assumptions about the
results. Perhaps for very simple tools this solution can work but as the number
and types of arguments increases, it seems unlikely that a developer would want
to try and rely on just argument position for the interface to their program.A more flexible approach would be to examine all of the arguments passed in and
parse these for patterns that would allow customary  and  style
options. Doing this by hand for every CLI tool would be error-prone and tedious,
but fortunately some awesome folks have already done that for you with the
excellent clap crate.The Sound of One Hand ClappingThe Command Line Argument Parser for Rust, or clap, is one of the most
widely-used crates in the rust ecosystem. GitHub shows that there are over 445k
repos which depend on clap at the time of writing. Adding clap to your project
will allow you to avoid writing your own parsing logic to interact with the
command line:Out of the box clap offers a builder pattern approach that can be used to
get arguments from the command line without the hassle of parsing an 
of  values:Your users can now invoke the above  program from the command line
and pass in the main argument and optionally enable your x long mode:Clap offers a lot more than just parsing arguments, though. It can also reject
options and arguments that are not specified by the programmer and it provides
built-in help:Okay, so I think we can all agree that clap has some nice features and is far
more robust than trying to roll your own command line argument parser, but this
post started off talking about rust's type system and how that can be used as an
interface with the command line user. And that is where clap's  feature
comes in.Defining Your CLI Interactions with Clap offers a much more ergonomic way to specify your program's arguments than
the builder method shown above, but first you need to include the 
feature in your dependencies:You can now define rust types in your source which will be translated into an
interface contract for your program when called from the command line:The above program behaves identically to the builder version from the previous
section, with a  help option and all the other features that clap offers.
The key difference is that we are now using the type system to define the
interface rather than imperative calls to a builder. Note that the doc
comments for the  struct are used to build the  help subcommand for
the resulting application.Clap isn't limited to simple structs for the definition of the interface either.
As shown above,  works just as you would expect. To build up more
complex command line interactions you can use enums to define subcommand syntax
with configuration options for each different subcommand via associated values
(think  or  subcommands). This offers an elegant solution for managing
the complexity that your tool might need to expose to the user.There are tons of other great features in clap that can be found in the
docs, but rather than get into
the specifics of this crate, I want to discuss how type-driven design
can elevate command line interfaces to be on equal footing with published
libraries and service APIs.What can be gained from specifying your software's command line interactions via
the rust type system?Advantage 1: Code Maintainability and ReadabilityPerhaps the most obvious benefit of using explicit rust types to define your
command line interface is that it provides a clear, concise definition of what
input the program accepts. If you peel away the clap macro calls which annotate
the type, it looks just like any other data structure that you would expect to
pass between portions of the program. Because clap builds help from the doc
comments, the developer documentation for the type also transcends the command
line boundary to help users understand how to properly use your software. There
are no** hidden inputs that will affect your
program. This helps new developers on a project to understand a codebase and
also assists maintainers down the road when they need to add new features, as
there is a single entry point from which they can start designing their changes.Alternative approaches such as using the builder pattern or a custom parsing of
 don't offer this same clarity. At best, these solutions would
be contained in one or more functions that abstract away the interface logic. At
worst these could be scattered across the codebase as each portion of the
program tries to interact directly with the arguments passed in.As software grows in complexity the case grows stronger for type-driven CLI
specification. Imagine that we are creating a tool which will interact with a
key-value store and allow the user to add, remove and list the entries of the
store, all of which also require an access token to validate the user. We could
use the following to model the interface:The  type that we've outlined above allows us to clearly express that a
token is always required for all actions, but the  argument is only needed,
and indeed only allowed, when the user is either adding or removing entries. The
type that we have created is concise and removes the complexity one would have
to deal with if command line arguments were being handled imperatively.Advantage 2: Reduced Test Surface Area and Mock SupportUsing a crate like clap can eliminate huge swathes of imperative code that would
otherwise be necessary to parse, validate and consume arguments from the command
line. Every line of code that you don't write saves time on tests that don't
need to be created as well. Moving your interaction with the command line from
imperative functions to a declarative description of possible states moves the
testing burden upstream to the maintainers of the clap crate, which is widely
used and well supported.Type-driven command line interaction does more for us than just reducing the
surface area, though. It also provides a foundation for more complete unit tests
by providing the simplest possible mock for an actual command line interaction.
Imagine that our key-value client above delegates each of the top-level actions
(add, remove, list) to one function each, where more complex operations are
orchestrated. Something like the following:Some obvious tests of the above method might involve asserting that 
would return an  wrapping a KVStoreError::InvalidRequest if we call the
function with , for instance. We could also verify that the key
returned by the server matches the key we requested to add:The above test is simplistic, but it is representative of the way data must be
structured from an actual user because of the strict typing. This approach gives
us a high fidelity mock of a command line interaction.Advantage 3: Semantic Versioning: Not Just for LibrariesSemantic versioning, or SemVer, is a widely-used framework for determining how
software creators should version their releases so that downstream users of that
code can confidently know what versions are safe to upgrade to from other
versions. It leads to the familiar three-part version number consisting of
 where each component conveys different levels of change and
potential upgrade risk. The rust core team follows SemVer for rust releases and
even have an extensive
section about the topic
in the Cargo Book.Library maintainers generally follow SemVer so that other developers who depend
on their crate can understand when it is safe to upgrade without needing to
delve into the release notes of every single release. Authors of binary tools,
however, have been less likely to strictly follow SemVer, as illustrated by the
rustup 1.28.0 adventure,
wherein a minor release ended up breaking CI for many rust projects.Perhaps the reason why authors of binary CLI tools are less likely to follow
SemVer is because they have an image in their head of the user being a person
who can adapt to changes between versions. The reality, however, is that any
sufficiently useful CLI tool will eventually be integrated into an automated
toolchain that expects input and output to be consistent across versions. Good
CLI tools end up operating very similarly to a library. Unlike libraries,
though, upgrading a binary version doesn't get a chance to throw compiler
errors. Worse yet, CLI tools are often integrated in parts of the stack where
observability is poor and errors are only discovered when catastrophic failure
has already occurred.So, how can a strictly-typed approach to command line arguments help us to
better follow SemVer with CLI applications? The answer to this is through
tooling that already exists,
cargo-semver-checks. This
cargo tool examines your source code and compares it against a prior release
in order to determine if your changes constitute major, minor or merely
patch level changes. Importantly, though, you should begin to think of your
command line program more like a library in order to help cargo-semver-checks
to analyze the importance of changes. Your CLI argument types should be made
 even if this level isn't required for your program to run properly.
They are, after all, truly the most public part of the software. A similar
approach is also reasonable with the types that might represent your program's
output, whether they are used to write back to the shell, to files or some other
form of output. Once you've done this, start versioning your binaries
accordingly. If cargo-semver-checks warns you that a change is major and you
only thought that it was a patch, that's a big warning. Did you really intend to
make a major, breaking change? If you did, then don't hesitate to change the
major version number.Merely knowing about a tool like cargo-semver-checks and having it installed
is nice, but we all know that things like this are best when they become an
automated part of our workflow. It's easy to add a GitHub Action to run a SemVer
check automatically:Now, even if you forget to run your SemVer check manually, you probably won't
push out a binary release that breaks some dependency in a completely hidden
way.Good for the Environment TooThere is a loose end that may have been nagging at some readers going over the
previous sections: What about environment variables? After all, many command
line programs can also look at the shell's environment variables as a source of
input. We see this particularly around secrets or omnipresent settings.
Fortunately clap has us covered here too with the crate feature  that lets
you specify an environment variable which will be queried when a given argument
was not specified as part of the command invocation.Let's use this to flesh out the code from our key-value store client example in
the maintainability section
above. In that example, it would make a lot of sense to make  an argument
which can be stored in an environment variable as well as be overridden from the
command line.All that was required (aside from adding the  feature to our dependencies)
was to add  on line 7. The user can now either pass in the
token via  or by setting the environment variable . The
generated help will automatically pick this up and educate the user about this
option (line 13 below):We are now able to have a fully type-driven specification of our command line
interface that seamlessly incorporates both the arguments passed in as well as
environment variables from the shell. What's not to love?
    If you want to discuss  this post
     or any other, please feel free to drop me a message on
    Instagram
    or over at
    Bluesky.
]]></content:encoded></item><item><title>The Email Startup Graveyard: Why 80%+ of Email Companies Fail</title><link>https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail</link><author>skeptrune</author><category>hn</category><pubDate>Tue, 1 Jul 2025 00:41:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[While many email startups have invested millions in solving perceived problems, we at Forward Email have focused on building reliable email infrastructure from scratch since 2017. This analysis explores the patterns behind email startup outcomes and the fundamental challenges of email infrastructure.: Most email startups don't build actual email infrastructure from scratch. Many build on top of existing solutions like Amazon SES or open-source systems like Postfix. The core protocols work well - the challenge is in the implementation.: For comprehensive details on our approach, architecture, and security implementation, see our Forward Email Technical Whitepaper and About page which documents our complete development timeline since 2017.The Email Startup Failure MatrixHere's every major email startup failure we could find, organized by accelerator, funding, and outcome:The Infrastructure Reality Check: Every single "email startup" is just building UI on top of existing infrastructure. They're not building actual email servers - they're building apps that connect to real email infrastructure.What "Email Startups" Actually BuildKey Pattern for Email Success: The companies that actually succeed in email don't try to reinvent the wheel. Instead, they build infrastructure and tools that enhance existing email workflows. SendGrid, Mailgun, and Postmark became billion-dollar companies by providing reliable SMTP APIs and delivery services - they work  email protocols, not against them. This is the same approach we take at Forward Email.Why Most Email Startups Fail: Email  startups typically fail because they try to replace working protocols, while email  companies can succeed by enhancing existing workflows. The key is understanding what users actually need versus what entrepreneurs think they need.1. Email Protocols Work, Implementation Often Doesn'tThe core email protocols are solid, but implementation quality varies widely:: Better implementation of existing protocols, not protocol replacement.2. Network Effects Are UnbreakableEmail's network effect is absolute:3. They Often Target the Wrong ProblemsMany email startups focus on perceived issues rather than real pain points:Real problems worth solving: Infrastructure reliability, deliverability, spam filtering, and developer tools.4. Technical Debt Is MassiveBuilding real email infrastructure requires:5. The Infrastructure Already ExistsWhy reinvent when you can use:Case Studies: When Email Startups FailCase Study: The Skiff DisasterSkiff perfectly exemplifies everything wrong with email startups.: "Privacy-first email and productivity platform": Better email through privacy and encryptionY Combinator: The Email App FactoryY Combinator has funded dozens of email startups. Here's the pattern:: Mixed results with some notable exits. Several companies achieved successful acquisitions (reMail to Google, Rapportive to LinkedIn), while others pivoted away from email or were acqui-hired for talent.Techstars: The Email Graveyard: Vague value propositions, no real technical innovation, quick failures.: VCs love email startups because they sound simple but are actually impossible. The fundamental assumptions that attract investment are exactly what guarantee failure.VCs love email startups because they sound simple but are actually impossible:: None of these assumptions hold true for email.The Technical Reality: Modern Email StacksWhat Actually Powers "Email Startups"Let's look at what these companies actually run:: Most email apps are Electron-based web apps that consume massive amounts of RAM:Electron Performance Crisis: Modern email clients built with Electron and React Native suffer from severe memory bloat and performance issues. These cross-platform frameworks, while convenient for developers, create resource-heavy applications that consume hundreds of megabytes to gigabytes of RAM for basic email functionality.: Constant syncing and inefficient code:Background processes that never sleepUnnecessary API calls every few secondsPoor connection managementNo third-party dependencies except those absolutely required for core functionalityThe Acquisition Patterns: Success vs. ShutdownClient App Pattern (Usually Fails):Infrastructure Pattern (Often Succeeds)::Industry Evolution and ConsolidationNatural Industry ProgressionThe email industry has naturally evolved toward consolidation, with larger companies acquiring smaller ones to integrate features or eliminate competition. This isn't necessarily negative - it's how most mature industries develop.Post-Acquisition TransitionsWhen email companies are acquired, users often face:: Moving to new platforms: Loss of specialized functionality: Different subscription models: Temporary service disruptionsUser Considerations During TransitionsDuring industry consolidation, users benefit from:: Multiple providers offer similar servicesUnderstanding migration paths: Most services provide export toolsConsidering long-term stability: Established providers often offer more continuityThe Hacker News Reality CheckEvery email startup gets the same comments on Hacker News:. These comments appear on every email startup launch because the fundamental problems are always the same.The Modern AI Email Grift2024 brought a new wave of "AI-powered email" startups, with the first major successful exit already happening:Adding "AI" doesn't solve the fundamental challenges:: AI features require significant infrastructure investment while addressing relatively minor pain points.What Actually Works: The Real Email Success StoriesInfrastructure Companies (The Winners): They build infrastructure, not apps.Email Providers (The Survivors)The JMAP Investment Question: While Fastmail invests resources in JMAP, a protocol that's 10+ years old with limited adoption, they simultaneously refuse to implement PGP encryption that many users request. This represents a strategic choice to prioritize protocol innovation over user-requested features. Whether JMAP will gain broader adoption remains to be seen, but the current email client ecosystem continues to rely primarily on IMAP/SMTP.: Forward Email powers alumni email solutions for top universities, including the University of Cambridge with 30,000 alumni addresses, delivering $87,000 in annual cost savings compared to traditional solutions.: They enhance email, don't replace it.The Exception: Xobni's Success StoryXobni stands out as one of the few email-related startups that actually succeeded by taking the right approach.: Built on top of Outlook instead of replacing it: Contact management and email search: Worked with existing workflows: Targeted business users with real pain pointsWhy Xobni Succeeded Where Others FailedBuilt on proven infrastructure: Used Outlook's existing email handling: Contact management was genuinely broken: Businesses pay for productivity tools: Enhanced rather than replaced existing workflowsThe Founders' Continued Success: Became an active angel investor with investments in Dropbox, Mailbox, and others: Continued building successful companies in the productivity space: Demonstrated that email success comes from enhancement, not replacementCompanies succeed in email when they:Has Anyone Successfully Reinvented Email?This is a crucial question that gets to the heart of email innovation. The short answer is: no one has successfully replaced email, but some have successfully enhanced it.Looking at email innovations over the past 20 years:: All successful innovations  existing email protocols rather than replacing them.New Tools Complement Email (But Don't Replace It): Great for team chat, but still sends email notifications: Excellent for communities, but uses email for account management: Perfect for messaging, but businesses still use email: Essential for video calls, but meeting invites come via email: HEY's founder DHH actually uses our service at Forward Email for his personal domain  and has for several years, demonstrating that even email innovators rely on proven infrastructure.HEY by Basecamp represents the most serious recent attempt to "reinvent" email:: Completely new email paradigm with screening, bundling, and workflows: Mixed - some love it, most stick with existing email: It's still email (SMTP/IMAP) with a different interfaceThe most successful email innovations have been:: Faster servers, better spam filtering, improved deliverability: APIs for sending email, webhooks for tracking: CRM integration, marketing automation, transactional emailNone of these replaced email - they made it better.Building Modern Infrastructure for Existing Email Protocols: Our ApproachBefore diving into the failures, it's important to understand what actually works in email. The challenge isn't that email is broken - it's that most companies try to "fix" something that already works perfectly.The Email Innovation SpectrumEmail innovation falls into three categories:Why We Focus on InfrastructureWe chose to build modern email infrastructure because:The problem is implementation: Most email services use outdated software stacks: Not new features that break existing workflows: Better APIs and management interfacesWhat Actually Works in EmailThe successful pattern is simple: enhance existing email workflows instead of replacing them. This means:Building faster, more reliable SMTP serversCreating better spam filtering without breaking legitimate emailProviding developer-friendly APIs for existing protocolsImproving deliverability through proper infrastructureOur Approach: Why We're DifferentBuild actual infrastructure: Custom SMTP/IMAP servers from scratchEnhance existing workflows: Work with all email clients: APIs and tools that actually workBuild "revolutionary" email clientsTry to replace existing email protocolsAdd unnecessary AI featuresHow We Build Email Infrastructure That Actually WorksOur Anti-Startup ApproachWhile other companies burn millions trying to reinvent email, we focus on building reliable infrastructure:: We've been building email infrastructure for 7+ years: We're building for the long termNo "revolutionary" claims: We just make email work betterGovernment-Grade Compliance: Forward Email is Section 889 compliant and serves organizations like the US Naval Academy, demonstrating our commitment to meeting stringent federal security requirements.OpenPGP and OpenWKD Implementation: Unlike Fastmail, which refuses to implement PGP citing complexity concerns, Forward Email provides full OpenPGP support with OpenWKD (Web Key Directory) compliance, giving users the encryption they actually want without forcing them to use experimental protocols like JMAP.Technical Stack Comparison:= APNIC blog post confirms Proton uses postfix-mta-sts-resolver, indicating they run a Postfix stack: JavaScript across the entire stack vs. 1980s C code: Single language eliminates integration complexity: Built for modern web development from the ground up: Any web developer can understand and contribute: Clean, modern codebase without decades of patches: Our privacy policy ensures we don't store forwarded emails to disk storage or databases, don't store metadata about emails, and don't store logs or IP addresses - operating in-memory only for email forwarding services.: For comprehensive details on our approach, architecture, and security implementation, see our technical whitepaper and extensive technical documentation.Email Service Provider Comparison: Growth Through Proven Protocols: While other providers chase experimental protocols, Forward Email focuses on what users actually want - reliable IMAP, POP3, SMTP, CalDAV, and CardDAV that works across all devices. Our growth demonstrates the value of this approach.in1-smtp.messagingengine.com shows strong growth (+21.1%) with over 500K domains using our MX recordsProven infrastructure wins: Services with reliable IMAP/SMTP show consistent domain adoption: Fastmail's JMAP investment shows slower growth (+14%) compared to providers focusing on standard protocols: The defunct startup lost 55.2% of domains, demonstrating the failure of "revolutionary" email approaches: Domain count growth reflects real user adoption, not marketing metricsWhy We Succeed Where Others FailWe build infrastructure, not apps: Focus on servers and protocolsWe enhance, don't replace: Work with existing email clients: No VC pressure to "grow fast and break things": 7+ years of deep technical experience: APIs and tools that actually solve problemsSecurity Challenges in Email InfrastructureEmail security is a complex challenge that affects all providers in the industry. Rather than highlighting individual incidents, it's more valuable to understand the common security considerations that all email infrastructure providers must address.Common Security ConsiderationsAll email providers face similar security challenges:: Securing user data and communications: Managing authentication and authorization: Protecting servers and databases: Meeting various regulatory requirements like GDPR and CCPA: Our security practices include ChaCha20-Poly1305 encryption for mailboxes, full disk encryption with LUKS v2, and comprehensive protection with encryption-at-rest, encryption-in-memory, and encryption-in-transit.The Value of TransparencyWhen security incidents occur, the most valuable response is transparency and quick action. Companies that:Disclose incidents promptly: Help users make informed decisionsProvide detailed timelines: Show they understand the scope of issues: Demonstrate technical competence: Contribute to industry-wide security improvementsThese responses benefit the entire email ecosystem by promoting best practices and encouraging other providers to maintain high security standards.Ongoing Security ChallengesThe email industry continues to evolve its security practices:These challenges require ongoing investment and expertise from all providers in the space.Conclusion: Focus on Infrastructure, Not AppsAfter analyzing hundreds of email startups:: Most email startups fail completely (this figure is likely WAY higher than 80%; we're being nice): Being acquired usually means death for email clientsInfrastructure can succeed: Companies building SMTP/API services often thriveVC funding creates pressure: Venture capital creates unrealistic growth expectationsTechnical debt accumulates: Building email infrastructure is harder than it looksEmail has been "dying" for 20+ years according to startups:: "Social networks will replace email": "Mobile messaging will kill email": "Slack will replace email": "AI will revolutionize email": "Remote work needs new communication tools": "AI will finally fix email". It's still growing. It's still essential.The lesson isn't that email can't be improved. It's about choosing the right approach:: Reliability and performance beat flashy featuresEnhancement beats replacement: Work with email, don't fight itSustainability beats growth: Profitable businesses outlast VC-funded ones: Tools and APIs create more value than end-user apps: Better implementation of proven protocols, not protocol replacement.Comprehensive Email Service Analysis: For an in-depth comparison of 79 email services in 2025, including detailed reviews, screenshots, and technical analysis, see our comprehensive guide: 79 Best Email Services. This analysis demonstrates why Forward Email consistently ranks as the recommended choice for reliability, security, and standards compliance.If you're thinking about building an email startup, consider building email infrastructure instead. The world needs better email servers, not more email apps.The Extended Email Graveyard: More Failures and ShutdownsGoogle's Email Experiments Gone WrongGoogle, despite owning Gmail, has killed multiple email projects: (2009-2012): "Email killer" that nobody understood (2010-2011): Social email integration disaster email features (2011-2019): Social network email integration: Even Google can't successfully reinvent email.The Serial Failure: Newton Mail's Three Deaths (2013-2016): Email client acquired by Newton (2016-2018): Rebranded, subscription model failed: Email clients can't sustain subscription models.The Apps That Never LaunchedMany email startups died before launching: (2014): Calendar-email integration, shut down pre-launch (2011): Email management tool, acquired before release (2013): Email client, development stoppedThe Acquisition-to-Shutdown PatternEmail Infrastructure ConsolidationThe Open-Source Email Graveyard: When "Free" Isn't SustainableNylas Mail → Mailspring: The Fork That Couldn'tEudora: The 18-Year Death March: Dominant email client for Mac/Windows: Open-sourced as "Eudora OSE": Even successful email clients eventually dieFairEmail: Killed by Google Play PoliticsOpen-source email projects fail because:: Email protocols are complex to implement correctly: Constant security updates required: Must work with all email providers: Volunteer developers burnoutThe AI Email Startup Surge: History Repeating with "Intelligence"The Current AI Email Gold Rush2024's AI email startups:VCs are throwing money at "AI + Email":: "Revolutionary email experience": Building on top of existing infrastructure: Most will fail within 3 yearsWhy They'll All Fail (Again)AI doesn't solve email's non-problems: Email works fine: AI requires reading all your emails: AI processing is expensive, email is commodity: Can't break Gmail/Outlook dominance: Most remaining AI email startups will pivot or shut down: Survivors will be acquired, with mixed outcomes: "Blockchain email" or the next trend will emergeThe Consolidation Catastrophe: When "Survivors" Become DisastersThe Great Email Service ConsolidationThe email industry has consolidated dramatically:Outlook: The "Survivor" That Can't Stop BreakingOur Real-World Experience: We regularly help customers whose Outlook setups break our perfectly compliant IMAP implementation.The Postmark Infrastructure ProblemRecent Email Client Casualties (2024-2025): Users increasingly report poor experience with the email client.: Windows users face licensing issues and subscription confusion.: The Mac/iOS email client, based on the failed Sparrow codebase, continues to receive poor reviews for reliability issues.Email Extension and Service AcquisitionsThe Survivors: Email Companies That Actually WorkNot all email companies fail. Here are the ones that actually work:: Bootstrap success story generating $140K/month as a Gmail extension for email marketing.: These companies succeed because they enhance existing email workflows rather than trying to replace email entirely. They build tools that work  email infrastructure, not against it.]]></content:encoded></item><item><title>Claude Code now supports hooks</title><link>https://docs.anthropic.com/en/docs/claude-code/hooks</link><author>ramoz</author><category>hn</category><pubDate>Tue, 1 Jul 2025 00:01:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Claude Code hooks are user-defined shell commands that execute at various points
in Claude Code’s lifecycle. Hooks provide deterministic control over Claude
Code’s behavior, ensuring certain actions always happen rather than relying on
the LLM to choose to run them.Example use cases include:: Customize how you get notified when Claude Code is awaiting
your input or permission to run something.: Run  on .ts files,  on .go files,
etc. after every file edit.: Track and count all executed commands for compliance or
debugging.: Provide automated feedback when Claude Code produces code that
does not follow your codebase conventions.: Block modifications to production files or sensitive
directories.By encoding these rules as hooks rather than prompting instructions, you turn
suggestions into app-level code that executes every time it is expected to run.In this quickstart, you’ll add a hook that logs the shell commands that Claude
Code runs.Quickstart Prerequisite: Install  for JSON processing in the command line. hooks run before tool calls and can block them while providing
Claude feedback on what to do differently.Select  to run your hook only on Bash tool calls.Select  and enter this command:For storage location, select  since you’re logging to your home
directory. This hook will then apply to all projects, not just your current
project.Then press Esc until you return to the REPL. Your hook is now registered!Run  again or check  to see your configuration: - User settings - Project settings.claude/settings.local.json - Local project settings (not committed)Enterprise managed policy settingsHooks are organized by matchers, where each matcher can have multiple hooks:: Pattern to match tool names (only applicable for  and
)Simple strings match exactly:  matches only the Write toolSupports regex:  or If omitted or empty string, hooks run for all matching events: Array of commands to execute when the pattern matches: Currently only  is supported: The bash command to execute: (Optional) How long a command should run, in seconds, before
canceling all in-progress hooks.Runs after Claude creates tool parameters and before processing the tool call. - File pattern matching,  - File editing,  - Web operationsRuns immediately after a tool completes successfully.Recognizes the same matcher values as PreToolUse.Runs when Claude Code sends notifications.Runs when the main Claude Code agent has finished responding.Runs when a Claude Code subagent (Task tool call) has finished responding.Hooks receive JSON data via stdin containing session information and
event-specific data:The exact schema for  depends on the tool.The exact schema for  and  depends on the tool. is true when Claude Code is already continuing as a result of
a stop hook. Check this value or process the transcript to prevent Claude Code
from running indefinitely.There are two ways for hooks to return output back to Claude Code. The output
communicates whether to block and any feedback that should be shown to Claude
and the user.Hooks communicate status through exit codes, stdout, and stderr:: Success.  is shown to the user in transcript mode
(CTRL-R).: Blocking error.  is fed back to Claude to process
automatically. See per-hook-event behavior below.: Non-blocking error.  is shown to the user and
execution continues.Blocks the tool call, shows error to ClaudeShows error to Claude (tool already ran)N/A, shows stderr to user onlyBlocks stoppage, shows error to ClaudeBlocks stoppage, shows error to Claude subagentHooks can return structured JSON in  for more sophisticated control:All hook types can include these optional fields:If  is false, Claude stops processing after the hooks run.For , this is different from , which only
blocks a specific tool call and provides automatic feedback to Claude.For , this is different from , which
provides automated feedback to Claude.For  and , this takes precedence over any
 output.In all cases,  takes precedence over any
 output. accompanies  with a reason shown to the user, not shown
to Claude. hooks can control whether a tool call proceeds.“approve” bypasses the permission system.  is shown to the user but
not to Claude.“block” prevents the tool call from executing.  is shown to Claude. leads to the existing permission flow.  is ignored. hooks can control whether a tool call proceeds.“block” automatically prompts Claude with . does nothing.  is ignored. and  hooks can control whether Claude must continue.“block” prevents Claude from stopping. You must populate  for Claude
to know how to proceed. allows Claude to stop.  is ignored.Claude Code hooks work seamlessly with
Model Context Protocol (MCP) tools. When MCP servers
provide tools, they appear with a special naming pattern that you can match in
your hooks.MCP tools follow the pattern , for example:mcp__memory__create_entities - Memory server’s create entities toolmcp__filesystem__read_file - Filesystem server’s read file toolmcp__github__search_repositories - GitHub server’s search toolYou can target specific MCP tools or entire MCP servers:Automatically format code after file modifications:Customize the notification that is sent when Claude Code requests permission or
when the prompt input has become idle.: Claude Code hooks execute arbitrary shell commands on
your system automatically. By using hooks, you acknowledge that:You are solely responsible for the commands you configureHooks can modify, delete, or access any files your user account can accessMalicious or poorly written hooks can cause data loss or system damageAnthropic provides no warranty and assumes no liability for any damages
resulting from hook usageYou should thoroughly test hooks in a safe environment before production useAlways review and understand any hook commands before adding them to your
configuration.Here are some key practices for writing more secure hooks:Validate and sanitize inputs - Never trust input data blindlyAlways quote shell variables - Use  not  - Check for  in file paths - Specify full paths for scripts - Avoid , , keys, etc.Direct edits to hooks in settings files don’t take effect immediately. Claude
Code:Captures a snapshot of hooks at startupUses this snapshot throughout the sessionWarns if hooks are modified externallyRequires review in  menu for changes to applyThis prevents malicious hook modifications from affecting your current session.: 60-second execution limit by default, configurable per command.If any individual command times out, all in-progress hooks are cancelled.: All matching hooks run in parallel: Runs in current directory with Claude Code’s environment:PreToolUse/PostToolUse/Stop: Progress shown in transcript (Ctrl-R)Notification: Logged to debug only ()Check if  menu displays your configurationReview stdout and stderr format expectationsEnsure proper quote escapingUse  to debug your hooks. The output of a successful hook
appears like below.Progress messages appear in transcript mode (Ctrl-R) showing:]]></content:encoded></item><item><title>Melbourne man discovers extensive model train network underneath house</title><link>https://www.sbs.com.au/news/article/i-was-shocked-melbourne-mans-unbelievable-find-after-buying-house/m4sksfer8</link><author>cfcfcf</author><category>hn</category><pubDate>Mon, 30 Jun 2025 23:53:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After finalising the purchase of a home in Melbourne's northern suburbs, a Melbourne man found something unexpected.There had been no mention of the expansive model train network beneath the home's floors.Coincidentally, new owner Daniel Xu is a keen train enthusiast and engineer.]]></content:encoded></item><item><title>Show HN: A continuation of IRS Direct File that can be self-hosted</title><link>https://github.com/openfiletax/openfile</link><author>elijahwright_</author><category>dev</category><category>hn</category><pubDate>Mon, 30 Jun 2025 22:08:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[the IRS recently open sourced most of Direct File, a tax tool it has been working on for a few years now. unfortunately, due to recent events, the IRS isn't working on it anymore. I decided to pick up where they left off and I'm trying to get it ready for next tax season]]></content:encoded></item><item><title>The new skill in AI is not prompting, it&apos;s context engineering</title><link>https://www.philschmid.de/context-engineering</link><author>robotswantdata</author><category>hn</category><pubDate>Mon, 30 Jun 2025 20:53:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Context Engineering is new term gaining traction in the AI world. The conversation is shifting from "prompt engineering" to a broader, more powerful concept: . Tobi Lutke describes it as "the art of providing all the context for the task to be plausibly solvable by the LLM.” and he is right.With the rise of Agents it becomes more important what information we load into the “limited working memory”. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.To understand context engineering, we must first expand our definition of "context." It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.Instructions / System Prompt: An initial set of instructions that define the behavior of the model during a conversation, can/should include examples, rules …. Immediate task or question from the user.State / History (short-term Memory): The current conversation, including user and model responses that have led to this moment. Persistent knowledge base, gathered across many prior conversations, containing learned user preferences, summaries of past projects, or facts it has been told to remember for future use.Retrieved Information (RAG): External, up-to-date knowledge, relevant information from documents, databases, or APIs to answer specific questions. Definitions of all the functions or built-in tools it can call (e.g., check_inventory, send_email). Definitions on the format of the model's response, e.g. a JSON object.Why It Matters: From Cheap Demo to Magical ProductThe secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context you provide.Building Agents is less about the code you write or framework you use. The difference between a cheap demo and a “magical” agent is about the quality of the context you provide. Imagine an AI assistant is asked to schedule a meeting based on a simple email:Hey, just checking if you’re around for a quick sync tomorrow. has poor context. It sees only the user's request and nothing else. Its code might be perfectly functional—it calls an LLM and gets a response—but the output is unhelpful and robotic:Thank you for your message. Tomorrow works for me. May I ask what time you had in mind? is powered by rich context. The code's primary job isn't to figure out  to respond, but to  the LLM needs to full fill its goal. Before calling the LLM, you would extend the context to includeYour calendar information (which shows you're fully booked).Your past emails with this person (to determine the appropriate informal tone).Your contact list (to identify them as a key partner).Tools for send_invite or send_email.Then you can generate a response.Hey Jim! Tomorrow’s packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.The magic isn't in a smarter model or a more clever algorithm. It’s in about providing the right context for the right task. This is why context engineering will matter. Agent failures aren't only model failures; they are context failures.From Prompt to Context EngineeringWhat is context engineering? While "prompt engineering" focuses on crafting the perfect set of instructions in a single text string, context engineering is a far broader. Let's put it simply:Context Engineering is the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task. Context isn't just a static prompt template. It’s the output of a  that runs  the main LLM call. Created on the fly, tailored to the immediate task. For one request this could be the calendar data for another the emails or a web search.About the right information, tools at the right time: The core job is to ensure the model isn’t missing crucial details ("Garbage In, Garbage Out"). This means providing both knowledge (information) and capabilities (tools) only when required and helpful.where the format matters: How you present information matters. A concise summary is better than a raw data dump. A clear tool schema is better than a vague instruction.Building powerful and reliable AI Agents is becoming less about finding a magic prompt or model updates. It is about the engineering of context and providing the right information and tools, in the right format, at the right time. It’s a cross-functional challenge that involves understanding your business use case, defining your  outputs, and structuring all the necessary information so that an LLM can “accomplish the task."This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:Thanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn.]]></content:encoded></item><item><title>Next month, saved passwords will no longer be in Microsoft’s Authenticator app</title><link>https://www.cnet.com/tech/microsoft-will-delete-your-passwords-in-one-month-do-this-asap/</link><author>ColinWright</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:31:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Starting this month, you'll no longer be able to use Microsoft Authenticator's autofill password function, a move the company is making to transition from passwords to passkeys. Last month, Microsoft stopped letting you save new passwords in the app.Next month is the biggest change, all your saved passwords will no longer be in the Authenticator app. You'll have to use passkeys instead -- such as a PIN, fingerprint or facial recognition. Attila Tomaschek, CNET's software senior writer and digital security expert, believes passkeys are a safer alternative to the risky password habits that 49% of US adults have, according to a recent CNET survey. "Passwords can be cracked, whereas passkeys need both the public and the locally stored private key to authenticate users, which can help mitigate risks like falling victim to phishing and brute-force or credential-stuffing attacks," said Tomaschek. Using the same password for several accounts or adding personal hints can be a convenient way to remember your login. But it's a big risk for scammers, identity theft and fraud. Here's more on Microsoft's plan for eliminating passwords and how to make the switch to passkeys before August. Microsoft Authenticator will stop supporting passwordsMicrosoft Authenticator houses your passwords and lets you sign into all of your Microsoft accounts using a PIN, facial recognition such as Windows Hello, or other biometric data, like a fingerprint. Authenticator can be used in other ways, such as verifying you're logging in if you forgot your password, or using two-factor authentication as an extra layer of security for your Microsoft accounts.In June, Microsoft stopped letting users add passwords to Authenticator, but here's a timeline of other changes you can expect, according to Microsoft. You won't be able to use the autofill password function. You'll no longer be able to use saved passwords.If you still want to use passwords instead of passkeys, you can store them in Microsoft Edge. However, CNET experts recommend adopting passkeys during this transition. "Passkeys use public key cryptography to authenticate users, rather than relying on users themselves creating their own (often weak or reused) passwords to access their online accounts," said Tomaschek.Why passkeys are a better alternative to passwordsSo what exactly is a passkey? It's a credential created by the Fast Identity Online Alliance that uses biometric data or a PIN to verify your identity and access your account. Think about using your fingerprint or Face ID to log into your account. That's generally safer than using a password that is easy to guess or susceptible to a phishing attack.Passkeys aren't stored on servers like passwords. Instead, they're stored only on your personal device. More conveniently, this takes the guesswork out of remembering your passwords and the need for a .How to set up a passkey in Microsoft AuthenticatorMicrosoft said in a May 1 blog post that it will automatically detect the best passkey to set up and make that your default sign-in option. "If you have a password and 'one-time code' set up on your account, we'll prompt you to sign in with your one-time code instead of your password. After you're signed in, you'll be prompted to enroll a passkey. Then the next time you sign in, you'll be prompted to sign in with your passkey," according to the blog post.To set up a new passkey, open your Authenticator app on your phone. Tap on your account and select "Set up a passkey." You'll be prompted to log in with your existing credentials. After you're logged in, you can set up the passkey.]]></content:encoded></item><item><title>The original LZEXE (A.K.A. Kosinski) compressor source code has been released</title><link>https://clownacy.wordpress.com/2025/05/24/the-original-lzexe-a-k-a-kosinski-compressor-source-code-has-been-released/</link><author>elvis70</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:19:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Last year, I discovered that the Kosinski compression format is actually LZEXE, which was used for compressing DOS executables back in the 90s and the late 80s. Its developer catalogues three versions on his website: v0.90, v0.91, and v0.91e. While only binaries of v0.91 and v0.91e can be found on the website, v0.90 can be found mirrored on various other websites.I got in touch with LZEXE’s developer, Fabrice Bellard, and he was able to release LZEXE’s source code, untouched since 1990! It is released under the terms of the MIT licence, allowing it to be freely used in other projects. To maximise performance, the compression logic was written in x86 assembly, while its frontend was written in Pascal. This particular source code appears to be for v0.91.my own Kosinski compressor which produced identical data to what could be found in the Mega Drive Sonic games. At the time, I noticed that it did not accurately reproduce the Mega CD BIOS’s compressed Sub-CPU payload data. The inaccuracies were so extensive that it appeared that the BIOS’s data was compressed with a different tool to the Sonic games. Notably, the compressor which was used for the Sonic games suffered from a number of bugs and shortcomings, causing the compressed data to less efficient than it should have been. The Mega CD BIOS developers may have used a different version of the compressor, which lacked these bugs, or which had additional bugs.With this in mind, the source code which has been released may not be for the exact compressor which was used by the Sonic games, though it could be modified to function identically to it. Since the compression logic was written in assembly, it should be simple enough to disassemble the compressor executables and compare them to the source code. Devon did the heavy-lifting of extracting and unpacking the core logic, which can be found here.With that, we now have the source code of two of the four ‘KENS’ format compressors – Kosinski and Saxman! Unfortunately, I do not have much hope of ever finding the original compressors for, let alone the source code of, the remaining two formats – Enigma and Nemesis – due to them evidently being custom formats which were designed specifically for the Mega Drive, likely meaning that the compressors and their source code never left the hands of Sega (Enigma encodes plane map data, operating on 16-bit words and specifically acknowledging the separation of bits of the tile’s index from its X/Y flip, palette line, and priority; meanwhile Nemesis encodes tiles, operating on nibbles and bunching data into groups of 32 bytes (8 x 8 4-bit nibbles).]]></content:encoded></item><item><title>End of an Era</title><link>https://www.erasmatazz.com/personal/self/end-of-an-era.html</link><author>marcusestes</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:17:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I recall saying to one of my colleagues at Atari way back in 1982 that I wanted to make a game that would be genuine art. A year later I built a game that was my first experiment in that direction: Gossip. It was a ridiculously simple game in which a player attempted to win favor in a group by calling people and telling them how much you liked or disliked some third party. The underlying concept was that “people like people who like people they like.” For some reason, many players had problems absorbing this simple concept. I later incorporated many of the ideas in Gossip into Excalibur, my first Arthurian game. But Atari collapsed soon after that, and I had to make a living as a freelance game designer. That kept me busy for the rest of the 1980s, but after I published my last game, Balance of the Planet, in 1990, I resolved to come back to the computer-games-as-art goal. I wanted to make another Arthurian game with much greater emphasis on the interpersonal interactions. I knew that my ambitions would take several years to realize, but I felt that the future lay in this direction, so I went ahead. I started building the game, and it went well. I began approaching publishers, and struck out everywhere. They greatly respected my reputation as one of the top game designers in the world, but they had decided that the only games that would sell had to be variations on Doom or Myst. Then the Markle Foundation came to the rescue with an offer that was to bail me out in the short run and send me down the wrong path in the long run. They offered me $350,000 to build a tool for other people to create interactive storyworlds. The idea appealed to me, so I accepted the offer and went to work. Now, most of the money went to various contractors, even though I designed and wrote all the code. The result was the Erasmatron. I don’t have a screenshot of the Erasmatron user interface. My primary goal was to build software that would permit a storyteller to implement many of the dramatic processes that take place in a story. For example, there was a component that permitted one character to spy on two other characters conversing without being seen. There was an extensive system for managing how information traveled through a group of people and how secrets were kept or broken.I won’t dwell on Erasmatron, as it evolved into Storytron, but I will note that Erasmatron had no takers. Other than Laura Mixon, nobody ever built anything with Erasmatron.This was the culmination of my effort to build a software development environment for interactive storytelling. It is best understood as Erasmatron with a much superior user interface and considerably better support features.The central component of Storytron was Deikto, a technology for creating a toy language specific to an interactive storyworld. “Toy language” is my term for a tiny language containing only the words necessary to permit interaction between characters in the storyworld. Toy languages do not need anywhere near as large a vocabulary as a full language. A storyworld for children will have no verbs for sex, higher education, jobs, finance, marriage, alcohol, and many other things. Similarly, a storyworld about corporate politics can dispense with scientific verbs, most economic and financial verbs, verbs for family interactions, and not much about food. A good storyworld designer can, in my estimate, build an adequate toy language for most storyworlds with only a few hundred verbs. The verbs are the core of the system; the player can build sentences out of the verbs and all the other words in the system. There are words for actors, props, and stages.This description is growing too large. Storytron technology had many other wonderful features that I won’t describe here. The important point I want to make is that nobody was interested in Storytron. I spread the word about it as well as I could, but I’m no salesman. I spent about ten years on Storytron and a great deal of my money hiring contractors to do some of the work that I couldn’t do. And it was all for nothing. Storytron was just too complicated for the audience. I don’t think that was because it was intrinsically too complicated for anybody to understand. My impression is that there just weren’t any people willing to make the big commitment required to learn how to use Storytron. It was easier to learn than professional programming systems like Eclipse or the Microsoft suite of software development applications. But it demanded more of its users than they were willing to invest.I made one last effort to make Storytron work using Siboot, a concept that I had developed in 1987. I poured my energy into Siboot, and a number of good people helped me, but after I had expended several years on it, I realized that it was crap. The story felt too mechanical. I realized that it needed a boost in the form of the encounter technology that I had developed for the 1987 version, but at that point I was so discouraged that I just couldn’t go on. I gave up on Siboot.I gave up on Storytron around 2018. It was painful to accept that all the energy, all the creativity, all the sweat I had committed to the project was for naught, but I had no choice. I rested for some months, then in 2020, for my 70th birthday, I realized that I was growing old and would not be able to handle a tough technical challenge for much longer. I therefore decided that the time had come for me to make one last effort, and that effort had to an Arthurian game. I re-read the many Arthurian books I had accumulated during previous efforts, girded my loins, and set to work. I made many changes along the way; the final version of Le Morte d’Arthur was quite unlike the original. But it worked. I knew that, after all these years, I had finally achieved my goal of making genuine interactive art. I was proud, tired, and gratified. Not many people played the storyworld, but I didn’t care. That was the world’s failure, not mine. I continued to fiddle around with interactive storytelling, discussing issues with a small group of people devoted to the problem. I even made a few attempts to make the technology I used for Le Morte d’Arthur available to others, but, once again, nobody was interested.Late in 2024 I happened upon a mention of Narrascope, an annual conference for interactive fiction held once every June. I knew of the conference from previous references, and it occurred to me that I had one last shot at making interactive storytelling technology available to the world. The attendees of Narrascope were not the techie types I had previously dealt with. These were mostly storytellers, weaker on the technology but stronger on the storytelling side. I decided to make my technology available to them, but to do so I would have to strip away all the technical complexity. I set to work building a web page that could edit storyworlds, using HTML, CSS, Javascript, and JSON. My programming powers were fading fast. Time and time again I would send my friend Dave Walker an email declaring that Javascript (or something else) was utterly broken, incapable of executing the simplest program without errors. Dave would ask to see the source code and I would present it to him with detailed notes proving that my code was perfect and Javascript was broken. He’d call me, we’d discuss it, and eventually he’d say something like, “Where did you terminate the loop beginning at line 563?” There would be a long silence, followed by the tiniest  from me. I’d thank him for his help and hang up. A week later, I’d be fuming again about another fundamental flaw in Javascript. Narrascope had accepted my lecture proposal, as well as my request to deliver a workshop on my technology. I spent dozens of hours working on the lecture; my lectures have always been top-notch and I wasn’t about to scrimp on this one. I made scores of nifty-keen images to illustrate my points. When will people learn that text doesn’t belong on a slide???Meanwhile, I struggled with the program. I didn’t quite get it finished, but it was workable and users could readily see that it was close to completion. On the big day I arrived at the airport at 5:00 AM to catch the early flight. We sat on the tarmac for an hour because of a mechanical problem, at which point I realized that I could not possibly make a crucial connection. I had to abort the trip to Narrascope and deliver the lectures via video, which turned out to be disastrous. This was my last-gasp effort to stimulate progress in interactive storytelling. "Once more, into the breach!” I had told myself. Now, more than a week after I delivered my spiel, not one person has answered my call for emails expressing some interest in my technology. Once again, my efforts were in vain.And so it is time for me to admit that, after all those decades of work, I have failed, with the single exception of Le Morte d’Arthur. When I designed for myself, I succeeded. When I designed for others, I failed. It’s time to throw in the towel and leave interactive storytelling to others. I don’t think that the world is ready. I feel like Charles Babbage, who invented the programmable computer in 1850. It used gears, levers, and cams and was brilliant. But the world had no need for programmable computers in 1850, so he never got the funding to build his invention. I’m nowhere near as smart as Charles Babbage, but my life dimly echoed his. I realized that my opus magnus, Le Morte d’Arthur, is a metaphorical autobiography of sorts. At the least, it expresses my experience working on interactive storytelling. Here is Merlin’s final conversation with Arthur:The time has come to close this chapter of my life. Perhaps I shall write a book summarizing my findings. Perhaps I shall not.]]></content:encoded></item><item><title>Xfinity using WiFi signals in your house to detect motion</title><link>https://www.xfinity.com/support/articles/wifi-motion</link><author>bearsyankees</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:03:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The hidden JTAG in a Qualcomm/Snapdragon device’s USB port</title><link>https://www.linaro.org/blog/hidden-jtag-qualcomm-snapdragon-usb/</link><author>denysvitali</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:34:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[EUD stands for Embedded USB Debug: essentially, this is a debug interface built right into almost every Qualcomm SoC since ~2018. Internally it hooks deep into the SoC, providing debug facilities for not just the CPUs but also the myriad of Hexagon co-processor/DSPs; many of the exciting details can be found in this patent from way back in 2014.In practise, for a non-production device (like a dev board, though some production devices seem to work too), EUD can be enabled by writing a few registers and then starting up the USB phy (though the details vary by generation). Instead of whatever typical gadget you might expect, what appears on your PC is a 7-port USB hub, with 1 port populated by the “EUD control interface”.With the right USB commands, a second device will appear, this one exposes an SWD interface! Yes! SWD right over the USB cable, no external tools, no soldering, and no expensive debuggers. Closed case debug that (almost) puts Google’s Suzy-Q to shame!For those unfamiliar: JTAG and SWD are both mechanisms for debugging the CPU cores inside a device, just like you can use GDB to debug programs on your computer (or your IDEs integrated debugger). They let you set breakpoints, halt execution, inspect the registers, single step instructions and all sorts of other useful things.For quite a while there has been a tantalising fork of openOCD published by Qualcomm on CodeLinaro, promising EUD integration. However, it relied on an at-the-time proprietary EUD library, which was only available to Qualcomm employees and their OEM partners.The device-side part of this (enabling the EUD interface so it shows up on your PC) has been somewhat supported in upstream Linux for a while. Back in August last year there was an attempt to extend this support for some newer platforms which have additional requirements. This sparked some discussion over the kernel policy: is it acceptable to have drivers in Linux that are only usable by some internal software, gatekept for Qualcomm and their paying partners? The answer appeared to be no, and this seemed to be enough to push Qualcomm in the right direction as after 8 months of silence, here we are!Let’s be fair, it almost definitely builds just fine on Ubuntu 20.10 with Qualcomm’s GCC 8.x toolchain. But that’s not what most people are using, we have to fix this!It turns out to be not too bad, just some minor stuff. Somehow they have  and  enabled though, and there is no way we’re gonna get that all passing just yet.With everything building, the necessary fixes (and a shiny new ) have been submitted to Qualcomm’s repo here.Now we have EUD building, we can try it with OpenOCD. It looks like they based their changes on the latest OpenOCD release 0.12.0, very nice. But wait, this release came out in 2023, and OpenOCD is still in active development… So there’s 2 years worth of changes, andAlmost 11k commits! It would really be nice to get this upstream eventually, so maybe let’s just rebase it real quick, we need to point it at the cleaned up EUD fork anyways.Among Qualcomm’s changes to support EUD, there are also patches adding Hexagon debugging support (and seemingly some improvements for LLDB as well). These got lost along the way but are almost certainly worth looking into at some point.So here we are, a fun day of fixing up and rebasing some codebases, and a very tasty reward!You can find the rebased OpenOCD patches over on the linux-msm GitHub along with some quickstart instructions in the README. So far this has been tested on the Snapdragon 845, it should work similarly for the 855 and 865 where we can get away with just poking the enable register and then using Linux or U-Boot to start a USB gadget. Newer SoCs however will probably require additional changes like these for SM8450. Let’s hope these old patch series get refreshed now that the tooling side of the story is in better shape!Torvalds himself famously doesn’t support the use of debuggers with the kernel (though that certainly hasn’t stopped the wonderful work on kgdb), he wrote (all the way back in 2000):I don’t like debuggers. Never have, probably never will. I do not condone single-stepping through code to find the bug.So of course, how practically useful JTAG support is really depends on your workflow. In the Qualcomm Landing Team at Linaro, debuggers have never been a staple of our work for all the typical reasons you’d expect (cost and complexity being the main ones), however with more focus being spent on non-kernel things like U-Boot and the secure world this dynamic is shifting.U-Boot is an obvious example for us, since it doesn’t currently provide stack traces when it crashes, diagnosis can sometimes be an arduous process which is made infinitely simpler with a .We are particularly interested in the possibilities that EUD opens up for debugging a vertically integrated BSP, especially when TF-A, OP-TEE and U-Boot are in the mix via the Trusted Substrate layer for OpenEmbedded. If this is something you’d like to explore with us then don’t hesitate to get in touch.In addition to the SWD peripheral, there is also a COM (UART) peripheral, and a trace peripheral. These haven’t yet been explored (and aren’t integrated into OpenOCD) but they should allow for a bidirectional serial port and MMIO tracing respectively. These do open up some more interesting use cases around Closed Cased Debugging in production - this appears to have been intentional on Qualcomm’s behalf with EUD being disabled as part of the production signing process, but with the ability to be re-enabled with a (cryptographically validated) “debug policy”.Some different SoCs use different addresses for the debug base and CTI base registers, as well as the additional changes required to enable EUD. If you’re able to make this work on your board/SoC, please do open an issue on the linux-msm fork and let us know what worked for you.Additionally, there is a strange quirk where the sticky reset bit of the PRSR register is always set, perhaps relating to SMP. For now the sticky reset behaviour of OpenOCD is stubbed out but it would be good to figure out what’s going on.SMP support in general is also currently lacking. The config file has been updated (using rcar as a reference) to define multiple CPU cores, but this doesn’t seem to behave correctly in Linux. For now it’s recommended to boot with  if you want to actually debug your kernel.Whether or not EUD is available on your device seems to depend on a variety of options: there are fuses to configure what debug functionality is allowed, as well as support for an OEM signed “debug policy” which can override this behaviour. On at least one production device (the OnePlus 6) EUD appears to be disabled via fuse, and yet it just works anyway. This device also has “crashdump mode” enabled which is not typical, this suggests that maybe OnePlus shipped the device with a loose debug policy, perhaps by mistake.Lastly, while it is of course extremely useful to have proper JTAG for debugging the kernel (especially when it’s so effortless!). The obvious question is: can this be used to gain control of higher execution levels? And unfortunately the answer appears to be no. If you do manage to halt execution in EL2, all registers will read as 0, and not much seems to be possible, at least on a production device. If your board behaves differently do let us know!EUD gives us a huge new surface to explore, and offers the potential to greatly improve the experience of low level debugging on Qualcomm boards. We are extremely excited that this is now published and freely available to use, and we very much hope it will become a seamless experience as the tooling and drivers are better integrated.It is awesome to see Qualcomm’s commitment to improving the developer experience and making their platforms more open is continuing to be demonstrated in their actions, EUD has the potential to save huge amounts of money on expensive debugging equipment, drastically reduce set-up times and make remote debugging easier too (no doubt it will eventually be integrated into our existing remote debugging tooling). Quite simply this raises the foundations for anyone working on Qualcomm platforms, and we can’t wait to see what’s next.]]></content:encoded></item><item><title>Datadog&apos;s $65M/year customer mystery solved</title><link>https://blog.pragmaticengineer.com/datadog-65m-year-customer-mystery/</link><author>thunderbong</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:31:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The internet has been speculating the past few days on which crypto company spent $65M on Datadog in 2022. I confirmed it was Coinbase, and here are the details of what happened. Originally published on 11 May 2023. with a bonus, free issue of the Pragmatic Engineer Newsletter. We cover one out of six topics in today’s subscriber-only The Scoop issue. To get full newsletters twice a week, Datadog is a leading observability tooling provider which went public in 2019, with a current market cap of $28B. The company made $1.67B revenue in 2022, circa $140M per month. On an earnings call a week ago, on 4 May, the CFO mentioned a “large upfront bill that did not recur,” saying:“Billings were $511 million, up 15% year-over-year. We had a large upfront bill for a client in Q1 2022 that did not recur at the same level or timing in Q1 2023. Pro forma for this client, billings growth was in the low 30s percent year-over-year.”If you’re like me, you’d probably skim over this detail, as it’s 15% here, 30% there. However, analysts attend these calls whose bread and butter is crunching the numbers and figuring out what a company might be trying to hide. A JP Morgan stock analyst did just this, quickly crunching numbers and asking the question:“David, looking at the math on this large upfront bill that did not recur, it seems to be about $65 million, if I'm running that correctly. Can you possibly shed a little more light?“Datadog’s CFO, David Obstler gave more details:“That was a crypto company which continues to be a customer of ours. But that was an early optimizer. We had always talked about some of the industries that were most affected and optimized.“So, who is this mysterious crypto company? Investor Turner Novak speculated that it’s Coinbase:He added he doesn’t know for certain that it is Coinbase, as other crypto companies have also raised silly amounts of money in the past several years.So, did Coinbase spend $65M on Datadog in 2022? Online there’s no shortage of theories, or people pretending to be Coinbase employees, such as this anonymous commenter on Hacker News, claiming that the $65M was for a 3-year upfront payment (which information I could not verify). I wanted to find the truth, so I tracked down software engineers at the company. And I got my answer:Yes. Coinbase spent $65M with Datadog in 2021, and this was their due bill for that year. I can confirm this, having talked with both current and former software engineers at Coinbase who shared details of what happened.Here’s how Datadog’s CEO explained, on the earnings call on what happened:“This is one of those situations where this customer was in an industry that got pretty much decimated over the past year. And their own business was cut in three or four, in terms of the revenue. And when that's the case, we really work with customers to restructure their contracts with us. We want to be part of the solution for them, not part of the problem (...) We restructure their contract, so we keep them as a happy customer for many more years and do a deal that works for everyone with their business profile.”And here’s what actually happened, as I understand from talking with engineers at Coinbase.Coinbase had an incredible 2021 and did not have to care about costs. The company went public in June that year, and was valued at an eye-popping $86B. In comparison, nearly two years later the company is valued around $14B, a 75% decline.During the boom, trading volumes were surging, beating record after record, and Coinbase could barely keep up. Here’s how Coinbase CEO Brian Amstrong summarized it:“So, obviously 2021 was just an incredible year for Coinbase, the kind of thing that you see very rarely in your lifetime, in a business career (...) We hit an all time high in our monthly transacting users of 11.4 million, which is 4x year-over-year, 400% pretty incredible.”Following the IPO in summer 2021, nobody at the company cared about infra costs; the only focus was growth. The company racked up huge bills for the likes of AWS, Snowflake, and also Datadog. And so, the $65M bill was for Datadog, for 2021. Coinbase settled the bill in Q1 2022.In early 2022 Coinbase suddenly needed to cut back infra spending. The crypto industry hit a sudden chill, affecting Coinbase’s business. As revenue dried up, the company turned its attention to reducing its overly high costs.For observability, Coinbase spun up a dedicated team with the goal of moving off of Datadog, and onto a Grafana/Prometheus/Clickhouse stack. A quick summary of these technologies:: a time series database. A very popular open-source solution for systems and services monitoring. Prometheus collects metrics from configured targets (services) at given intervals. It evaluates rules and can trigger alerts. It’s mostly written in Go, with some Java, Python and Ruby parts. Prometheus stores time series in-memory and on storage (HDD or SSD), using an efficient and custom format, and supports sharding and federation.Prometheus is part of the Cloud Native Foundation, membership of which indicates that it’s safe to build on top of Prometheus, as it’s actively maintained and will continue to be.Prometheus can be self-hosted, but several cloud providers also offer managed Prometheus services: both Google Cloud and AWS have this service in production, while Azure has it in preview.: the frontend for visualizing metrics. Grafana is a popular source analytics and monitoring visualization solution. If you need to display or dive into metrics or alerts, it’s the go-to tool, and widely used across tech companies. When I was at Uber, Grafana powered many of our graphs. Here’s an example of Grafana dashboards you can try out:: log management. A fast and open-source column-oriented database management system, which is a popular choice for log management. Clickhouse is written predominantly in C++, and is widely used across the industry. For example, Cloudflare uses Clickhouse to store all its DNS and HTTP logs – which is more than 10M rows per second! – and Uber uses Clickhouse as its central logging platform.Coinbase spun up its in-house approach without the main goal of saving costs, but to have full control and ownership of observability. Observability and reliability is a major differentiator for Coinbase, as it gives a competitive advantage over rivals.However, with the crypto market cooling, costs became a major focus, and it was clear the in-house Grafana/Prometheus solution was much cheaper. The Coinbase team had been double-writing the new stack for months, confirming everything worked well, and ironing out any issues.So Coinbase was ready to pull the plug on Datadog, but Datadog saved its customer relationship at the last minute by making Coinbase a very appealing deal it could not refuse. In future, the bill for Datadog would be nowhere near the $65M of 2021. As Brian Amstrong said of the crypto market during 2021, a $65M bill is the kind of thing you see very rarely in a business career.I asked an engineer at Coinbase who used the in-house stack and Datadog how they felt about the decision to stay on Datadog. They said it was ultimately the right decision, considering the reasonable costs, and the superior Datadog development experience.Coinbase could  have engineered a similar experience in-house. However, to provide a similarly seamless developer experience, would have likely taken tens of engineering years.“Expensive” in observability tooling is relative. Let’s assume that today Coinbase “only” spends, say, $10M per year on Datadog. Is this too much? Looking at the headline number, it’s tempting to think so.However, let’s look a level deeper. A platform like Datadog helps prevent outages, detects them instantly, and mitigates them faster. In 2022, Coinbase had 17 outages, totalling about 12 hours of downtime. The company’s daily average revenue is around $9M/day, based on their 2022 earnings.Assume that Datadog cuts the number of outages by half, by preventing them with early monitoring. That would mean that without Datadog, we’d look at 24 hours’ worth of downtime, not 12. Let’s also assume that using Datadog results in mitigating outages 50% faster than without - thanks to being able to connect health metrics with logs, debug faster, pinpoint the root cause and mitigate faster. In that case, without Datadog, we could be looking at 36 hours worth of total downtime, versus the 12 hours with Datadog. To put it in numbers: the company would make around $9M in revenue it would otherwise lose, Now that $10M/year fee practically pays for itself!What can we learn from Coinbase’s cost reduction exercise? Vendors are tight-lipped about their customers reducing spend, and it is a lucky coincidence that Datadog gave enough hints to find out who their big “early optimizer” customer was, and find out more details. But is the story of Coinbase a one-off?I’m not sure that it is. Three months ago, I covered the trend that Tech companies are aggressively cutting back on vendor spend - and two months later, The Wall Street Journal also reported on the same topic. Coinbase, to me, seems to have been early to the cost optimizing trend. However, look closely at the responses I gathered, and “AWS” and “Datadog” are the two most mentioned vendors as targets for cost savings. This is simply because infra and observability costs tend to be the highest and AWS is the leader for cloud infra, and Datadog the leader for observability.Datadog CEO Olivier Pomel confirmed that this type of optimization is happening across all of their customers, saying:“When we look at our data, when we look at what we hear from the hyperscalers also, we also listen carefully to their commentary on what they foresee in the near future, we don't see anything that gives us confidence that we can call an end to optimization in the next quarter or the quarter after that. So as far as our guidance goes and our plan for the year, we assume that this is going to continue at a similar level for the rest of the year.”I have since confirmed several large companies with thousands of engineers building their own Grafana/Prometheus stack, planning to migrate off of their current observability vendor and operate the observability stack themselves. But why is this?Above $2-5M/year annual spend is where bringing a vendor in-house tends to come up. And this is because it is around this number where the cost of hiring a whole team to do what a vendor is doing can  make sense.As a rule of thumb, you can get infra costs much lower than what vendors charge. This is because both the vendor, and you are probably using the same Cloud infrastructure provider, which is usually AWS, GCP or Azure. However, you would need to hire and staff a dedicated engineering team to build and run that infra.So, from a cost perspective, this is the math problem you need to solve. At what point does is this equation become true:$infra_cost + $platform_team_cost < $current_vendor_costsIn this question, $platform_team_costs will be above $1M, and sometimes above $2M. This is because you need to have a team of 4-5 engineers, plus a manager, and their average total compensation will be somewhere between $150-400K/year, depending on your cost basis.So when you have a bill that is above $2-3M/year, it can start to look tempting to build, rather than buy. The economics of this decision start to get down to how high of a margin is the vendor charging on top of raw infra? The curious question with Coinbase is: did they consider building, when talking about such a huge projected cost that could justify having a team?In the case of Coinbase, building in-house following a $65M bill was a clear no-brainer. They could hire a team of 10 senior and staff-level engineers in the Bay Area, and still have this team cost less than $5M/year. And they then only need to budget for the infra costs, which they can presumably bring down to low double digits per year.Coinbase planned to move off Datadog, but ended up staying. However, it is not the only larger tech company thinking about bringing observability in house. I have another exclusive which even Datadog might not be aware of, yet. This report is about Shopify and its plan to move off Datadog. But could recent layoffs change things? I cover details on this topic in the full The Scoop.This was one out of the five topics covered in this week’s The Scoop. A lot of what I share in The Scoop is exclusive to this publication, meaning it’s not been covered in any other media outlet before and you’re the first to read about it.The full The Scoop edition additionally covers:Will Shopify migrate onto an in-house observability tool? Shopify decided to build its own observability platform and migrate off Datadog. This plan looked certain until Shopify cut the very engineering teams that built its new platform. What happens next?.Microsoft cuts its compensation targets. Almost exactly a year ago, Microsoft employees received a welcome surprise: they could expect higher-than-usual compensation increases. Yesterday, another unexpected email came, but its contents were the opposite of last year’s. I talked with managers and engineers at the tech giant for their reaction to disappointing compensation news. .Shopify letting go most staff in Germany. As part of cutting 20% of staff, most people in Germany were made redundant. These layoffs happened a week before a Works Council election in Germany. Is this unlucky timing, or is there more behind the move?.Senior compensation trending down in Ukraine. Ukraine is one of the few countries for which we have access to nationwide data, through job site Djinni. Data for the first part of this year are in, and they point to something not seen recently: senior engineers are making less. Is this a local trend, or could we see it happening in other countries?.A follow-up to this week’s public tech company compensation article. Why was Netflix lower down the list than many software engineers expected? Plus, new details about Roblox and why Jack Dorsey’s total compensation is $2.75. ]]></content:encoded></item><item><title>Ask HN: What&apos;s the 2025 stack for a self-hosted photo library with local AI?</title><link>https://news.ycombinator.com/item?id=44426233</link><author>jamesxv7</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:10:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[First of all, this is purely a personal learning project for me, aiming to combine three of my passions: photography, software engineering, and my family memories. I have a large collection of family photos and want to build an interactive experience to explore them, ala Google or Apple Photo features.My goal is to create a system with smart search capabilities, and one of the most important requirements is that it must run entirely on my local hardware. Privacy is key, but the main driver is the challenge and joy of building it myself (an obviously learn).The key features I'm aiming for are:Automatic identification and tagging of family members (local face recognition).Generation of descriptive captions for each photo.Natural language search (e.g., "Show me photos of us at the beach in Luquillo from last summer").I've already prompted AI tools for a high-level project plan, and they provided a solid blueprint (eg, Ollama with LLaVA, a vector DB like ChromaDB, you know it). Now, I'm highly interested in the real-world human experience. I'm looking for advice, learning stories, and the little details that only come from building something similar.What tools, models, and best practices would you recommend for a project like this in 2025? Specifically, I'm curious about combining structured metadata (EXIF), face recognition data, and semantic vector search into a single, cohesive application.Any and all advice would be deeply appreciated. Thanks!]]></content:encoded></item><item><title>Sony DTC-700 audio DAT player/recorder</title><link>https://kevinboone.me/dtc-700.html</link><author>naves</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:03:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Don’t let anyone tell you otherwise: DAT players were fantastic. They
offered all the advantages of an audio cassette, but with the sound
quality of a CD. The compact audio cassette was a marvellous invention,
in its own way; but this technology struggled to provide audio fidelity
that would satisfy discerning listeners. Its frequency response was
limited, and the unavoidable background hiss was very obvious in quiet
environments. Still, in the 1970s audio cassettes were  way
most people listened to music, and I still have a stack of them.One thing that made cassettes so popular was that you could record on
them. Setting aside the legal issues, you could record from FM radio, or
from vinyl records, or even from microphones. It was easy to make ‘mix
tapes’ of you favourite tracks, and share them with friends. Cassettes
were everywhere – from portable players like the Walkman, to serious
hardware in hi-fi racks; they were even in cars.
There were shops that sold nothing but cassettes, and they sold by the
million.Serious hi-fi enthusiasts, however, listened to vinyl records or FM
radio. There  good-quality cassette decks, but the
`audiophile’ crowd embraced them with reluctance, if at all. Still, even
the most ardent hi-fi junkie couldn’t deny the usefulness of cassettes.
What we needed was something that could record high-quality sources,
with no loss of fidelity.That’s where DAT, ‘digital audio tape’ comes in. DAT offered digital
recording, in a range of qualities, the highest of which exceeded that
of CD. If you wanted to record from a CD, you could just connect the CD
transport’s digital output to the DAT’s digital input, and away you go.
Well, maybe – more this subject later. Of course, most DAT units could
record from analog sources like radio as well.DAT entered the market at about the same time as CD, but was much
less successful. For all its notional advantages, DAT never really
caught on in the domestic market, although it was somewhat more popular
in professional applications. A companion data storage technology, DDS,
used the same hardware, and was somewhat more successful although,
again, in professional rather than domestic applications.
Sony pulled out of the market in 2005, although I think it was clear
long before then that the format was moribund.The DTC-700, introduced in 1990, was Sony’s ‘budget’ hi-fi DAT
player/recorder. The more expensive DTC-55ES and DTC-60ES models had
fancy (and probably snake oil) features like a copper chassis. Yes,
copper is a better electrical conductor than steel, but a great chunk of
steel like the DTC-700 chassis is a pretty good conductor already. I’ve
not been able to find how much a new DTC-700 cost but, even as the
introductory model in the range, I imagine it was well into
sell-a-kidney territory. In 1995, even a five-year-old, second-hand unit
was eye-wateringly expensive. These days, you can pick up a refurbished
unit for about three hundred quid. It’s well worth the money – if you
can find tapes. There are lots more digital DDS tapes in circulation
that audio tapes; these are not guaranteed to be compatible with audio
players, but early DDS tapes often are.The DTC-700 had a flight-deck of controls, because it offered a stack
of functionality. It had two different digital inputs and an analog
input; there was a headphone amplifier with its own volume control; you
could skip to specific tracks by their number, or to a particular time;
and, of course, you could insert the meta-data that made this possible
when you recorded. And, like all serious hi-fi equipment, it had a
vaccuum-flourescent display, available in different colours. For that
real 70s look, you could buy it with mock-walnut case sides.Compared to cassettes, DAT recordings sounded fantastic. It wasn’t
necessary for the rest of your equipment – amplifier, speakers,
headphones – to be of top quality to realize this: the difference
between DAT and cassette was just that striking. In principle, DAT
offered better-than-CD quality, with its 48kHz sampling rate. In fact,
DAT set the standard here: 48kHz remains a common sampling rate to this
day. Folklore has it that Sony was encouraged to adopt 48kHz to make it
harder to record commercial CDs, which used (and still use) 44.1kHz.
Back in the 90s, technology hardly existed to resample these different
formats on-the-fly; eventually, Sony and others started selling DAT
units that supported 44.1kHz directly. This wasn’t an entirely welcome
move, as I’ll explain later.High cost was one of the reasons – perhaps the main reason – why DAT
didn’t catch on in the domestic market; but it certainly wasn’t the only
one. Another problem was the lack of original material: recording
studios didn’t seem to want to release commercial recordings on DAT.
Their reluctance isn’t hard to understand: DAT tapes could be copied an
unlimited number of times, with no loss of quality. In the the late 80s
it wasn’t easy to copy a CD onto DAT, because of the different sampling
rates. But there would have been no such limitation with a DAT-to-DAT
copy.Representatives of the recording industry were so worried about
illegal copying that, in the USA and elsewhere, they bullied legislators
into placing legal restrictions on the capabilities and sale of DAT
recorders. The USA also introducted taxation on the sales of DAT
devices, which was supposed to offset the loss in tax revenue that
illegal copying would create. This made expensive DAT players even more
expensive. Sony tamed the objections of the recording industry, to some
extent, by the simple expedient of buying CBS Records, one of the main
objectors. Nevertheless, the DTC-700 still suffers from the anti-copying
paranoia of the 80s; it will record a CD, but it will write meta-data
onto the recording to indicate that it’s a copy. The DTC-700, and other
DAT units of the same vintage, won’t record from another DAT unit, if
the meta-data indicates that the source is a copy. There are ways around
this limitation, but they’re fiddly.Whether illegal copying was a genuine risk or not, there never really
was a large selection of original music on DAT. As I recall, there
wasn’t even a “killer album” for DAT, like Dire Straits’  – an album so popular that people bought CD players just to
hear it at its best.DAT units also tended to have problems with reliability;
understanding why requires a basic understanding of how DAT technology
works.From a technological perspective, DAT was implemented in an
interesting way. “Interesting” in this context means, of course, “weird
and unreliable”. The DAT tape itself is only 4mm wide – the same as an
audio cassette. To get sufficient data bandwidth, the tape couldn’t be
scanned lengthwise, as all previous tape formats were. At the speeds
that would have been required, the tape length would have been
unmanageable. Instead, DAT works in a similar way to a VHS video
recorder: the magnetic head is on a rotating drum, aligned at an angle
to the direction of tape movement. This arrangement allows the whole
width of the tape to be used, not just a couple of narrow strips in the
middle.Naturally, the scanning mechanism required close-tolerance alignment
to operate reliably. Even when adjusted perfectly, the high rate of
rotation led to mechanical stresses. This was true of VHS as well, but
VHS players rapidly became throw-away items – eventually nobody really
cared if they only lasted a year or two. But if you’d just paid the
price of a new car for a DAT player, you’d expect a better service life.
And Sony didn’t help itself: the DTC-700 contained a huge number of
low-cost, plastic parts in critical locations. A plastic cog might cost
only pennies to replace, but stripping the machine down to get to it
cost a lot more.In the end, though, I don’t think it was the price, or the lack of
commercial releases, or the questionnable reliability, or the legal
complications that killed off DAT – although all these factors played a
part. Rather, I think it was just that old bugbear of the consumer
electronics industry: market saturation.By about 1992, everybody who was ever likely to want a home DAT
player already had one. The format couldn’t readily be improved, because
it already offered audio fidelity beyond the limits of human hearing. So
there wasn’t a “DAT Mark 2” that manufacturers could have sold to eager
customers. If DAT players could have been made more cheaply, this might
have expanded the customer base a little. But I doubt that DAT units
could ever have become as cheap as cassette players, and certainly not
as portable, because the electromechanical design was so complex and
fussy.It’s not as if any alternative technology has really presented
itself. These days, it’s trivially easy to record from digital or analog
sources, onto hard disk or solid-state storage. Any desktop computer
with a soundcard can do this. A number of manufacturers, including Sony,
did release self-contained hard-disk audio recorders, but they seem to
have enjoyed even less success than DAT. And these days, of course,
there’s even less need for such a device than there was in the 90s. If I
want to listen to a radio broadcast more than once, I can probably just
get it from the broadcaster’s website. Some modern radio tuners even
have built-in digital recording capabilities. No: if there were any
demand for a modern alternative to the DAT recorder, somebody would be
selling one.Many of the audio technologies from my youth have undergone a
revivial recently: vinyl records are the obvious example, but even
cassettes are starting to sell again. Are we likely to see renewed
interest in DAT? On the whole, I think probably not. Plenty of people
look back with fondness on vinyl and cassette, even on CD; I don’t think
DAT gives anybody a warm glow.]]></content:encoded></item><item><title>Proton joins suit against Apple for practices that harm developers and consumers</title><link>https://proton.me/blog/apple-lawsuit</link><author>moose44</author><category>hn</category><pubDate>Mon, 30 Jun 2025 17:58:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Earlier today, Proton filed court papers in the US District Court for the Northern District of California to join an existing class-action lawsuit against Apple. Proton is a plaintiff in the case, but we are representing and suing on behalf of a class of similarly situated developers. Challenging one of the most powerful corporations in the history of capitalism is not a decision we make lightly, but Proton has long championed online freedom, privacy, and security, and we believe this action is necessary to ensure the internet of the future lives up to its potential.Why are we doing this now?We believe that Apple’s conduct, as detailed in the complaint we filed, constitutes further violations of US antitrust law. Without this case, Apple could get away with behavior in the US that is already outlawed in the European Union. If this were to happen, American consumers, and developers focused on the American market, would have to pay higher prices for fewer choices, and be left at a disadvantage.There is also urgency to act now because of a parallel class-action suit by app developers against Apple on May 23, and any settlement there could be binding on all other developers. By joining that lawsuit, we can ensure that this suit will not only be about monetary damages to compensate app developers for the harm caused by Apple’s conduct, but also changes to App Store policies that will improve the state of the internet. We are seeking to permanently end anti-competitive behavior on the App Store, and we are joining this lawsuit to ensure that any future settlement enforces real changes to Apple’s practices and policies to benefit all consumers, developers, and competition, and not just cosmetic changes.While the suit does seek monetary damages on behalf of all developers who have been harmed in order to deter future anti-competitive behavior and provide compensation to class members harmed by Apple’s anti-competitive conduct, Proton will donate any money we receive from the lawsuit to organizations fighting for democracy and human rights so that some portion of Apple’s profits made from countries with authoritarian regimes are redirected to freedom. These donations will be coordinated through the nonprofit Proton Foundation, which oversees Proton and ensures that our work always prioritizes the public good over financial gain.Apple’s monopoly control of software distribution on iOS devices presents a myriad of problems for consumers, businesses, and society as a whole. Anti-monopoly laws exist because the power gifted by monopoly status inevitably leads to abuse. In the case of oligarchic tech giants, these abuses have wide implications for society, and it’s vital to the future of the internet that they be addressed now.The App Store policies hurt privacyApple’s App Store policies disproportionately favor the surveillance capitalism business model employed by companies like Meta and Google and therefore entrench an online business model that routinely violates consumers’ personal privacy. All developers are required to pay Apple an annual fee of $99 to be in the App Store, but Apple also takes a 30% cut from payments made through iOS apps, which are forced to use Apple’s payment system.Companies that monetize user data in exchange for “free” services that abuse your privacy aren’t affected by this, as they don’t process payments through the App Store. However, privacy-first companies that monetize through subscriptions are disproportionately hit by this fee, putting a major barrier toward the adoption of privacy-first business models. Naturally, these are also the very companies Apple is directly competing with through its disingenuous privacy marketing campaigns. This is a significant driver behind the internet’s descent into widespread surveillance capitalism.Apple’s policies undermine freedom and democracyApple’s complete control of the App Store has given it a dangerous level of control over app distribution, giving it the power to decide which apps can and cannot be distributed in different markets. Apple argues this control is necessary for security reasons. But the reality is that this has made Apple the single point of failure for free speech and a tool of dictatorships. There have been numerous incidents where Apple has removed or censored apps at the behest of authoritarian governments, in order to continue profiting from those markets.For example, the advocacy group GreatFire.org publishes important information about the state of censorship in the App Store through its AppleCensorship program, which highlights some striking statistics. Sixty-six of the 100 most popular apps worldwide are unavailable to iOS users in China. Additionally, all 240 VPN apps that the group tested were also unavailable to Chinese users. Overall, 27% of apps are missing from the Chinese App Store, more than double the global average of 13%. Many of those missing apps are news apps (including the likes of The New York Times, BBC News, and Reuters) or social networking or messaging apps, strongly implying that this is a matter of censorship, not security. Apple has also been caught removing apps to help suppress protests, such as the 2019 case of HKmap.Live, which was removed at the height of the pro-democracy protests in Hong Kong.Proton itself has also been victim of Apple’s censorship. In 2020, Apple threatened to take Proton VPN out of the App Store unless we removed language from our App Store description that said the app could be used to “unblock censored websites.” We don’t question Apple’s right to act on behalf of authoritarians for the sake of profit, but Apple’s monopoly over iOS app distribution means it can enforce this perverse policy on all app developers, forcing them to also be complicit. We believe it is critical for the future of the internet to end the monopoly on app distribution, so that developers and companies who are prepared to fight for democracy can do so.App Store policies lead to a worse user experienceApple’s approach to subscriptions management is designed to ensure it maintains complete control over the relationship between users and developers. To guarantee it gets its 30% cut of subscription revenue, it has imposed ironclad rules that dictate what developers can and cannot say to their users, which has a detrimental impact on the user experience. One basic example of this is that developers cannot tell users that other pricing options or discounts may be available if users upgrade via a website instead of inside the app. Not supporting Apple’s payment system is also considered a violation, which can lead to threats to remove your app, as happened to Proton.But this controlling behavior goes even further. Developers are prohibited from linking to their websites at all. Proton cannot even link to FAQ or customer support pages from its apps, as Apple believes it’s possible that users will then navigate from the support page to a pricing page and upgrade their accounts without paying Apple its fee. This has a direct, negative impact on customer experience.It’s also impossible for users to manage their subscriptions from multiple devices, as this would necessitate stepping outside Apple’s walled garden and weakening its control over the user. For example, users who upgraded their accounts on the web and then wish to upgrade or downgrade their subscription are not allowed to do so from their iOS devices. It is similarly impossible for users who purchased a subscription on iOS to change the subscription on the web. In a world where most users are accessing their apps and services over multiple devices, this is an unacceptably poor customer experience.Apple, however, goes even further in a bid to maintain its monopoly and trap users within the Apple ecosystem. Apple intentionally cripples third-party apps that compete with Apple services by making functionality that is available to Apple apps unavailable to other apps. For instance, there is no way to set Proton Calendar as the default calendar app on iOS. Furthermore, in a bid to prevent data portability, competing cloud storage services like Proton Drive are unable to seamlessly do background processing, while no such restrictions are known to exist for iCloud.These examples of coercive behavior illustrate time and time again that Apple is willing to inflict a worse experience and higher prices on consumers out of corporate greed, and it leverages its monopoly control over the App Store to do so.App Store tariffs cause price inflationApple’s 30% fees act as an artificial and arbitrary tax on internet commerce, which, much like a tariff, serves to raise prices, as part or all of this fee is inevitably passed on to the customer. Apple claims this fee is necessary to pay for the maintenance of the App Store, but evidence presented in the  case indicated that Apple makes a 78% profit on App Store fees, raising the question of whether these fees are really necessary or a clear example of the company profiting from its illegal monopoly.The only reason Apple can get away with this behavior is because there’s no competition in iOS app distribution or iOS in-app payments. If you want to provide an app or service to iOS users, you have to go through Apple’s systems, and you have to use Apple’s system for collecting payments. Breaking this monopoly and ending this punitive tax on the internet would allow companies like Proton to collect payments via less expensive methods, enabling the option to pass these savings on to you, and ultimately reducing the prices you pay.The remedies we are seeking would address many of the social ills mentioned above, ensuring that the internet of the future can continue to protect privacy and democracy. Mobile apps are now the dominant platform of the internet and the way the bulk of the world interacts with one another and with the web. Even if app stores started out as niche markets, today they are a critical component of the internet and fundamental to democracy. It is more essential than ever that we fight to create mobile ecosystems that are truly free, competitive, and not beholden to whichever dictator corporate leaders are currently bowing down to.This is also why we enter this fight not just representing ourselves, but as a class representative, to ensure that the outcome of this litigation will benefit all app developers and users of apps in this market. We expect this to be a difficult fight that could take many years, but our mission to build an internet that serves the interest of all of society affords us no other choice. By bringing this case, we hope to set an important precedent that free people, not monopolies, will dictate the future of the internet.Proton is being represented by Quinn Emanuel Urquhart & Sullivan LLP and Cohen Milstein Sellers & Toll PLLC. The full complaint in the case of Proton v. Apple can be found here.]]></content:encoded></item><item><title>That XOR Trick (2020)</title><link>https://florian.github.io//xor-trick/</link><author>hundredwatt</author><category>hn</category><pubDate>Mon, 30 Jun 2025 17:32:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I write type-safe generic data structures in C</title><link>https://danielchasehooper.com/posts/typechecked-generic-c-data-structures/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 30 Jun 2025 16:55:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I write type safe generic data structures in C using a technique that I haven’t seen elsewhere. It uses unions to associate type information with a generic data structure, but we’ll get to that. My approach works for any type of data structure: maps, arrays, binary trees… but for this article I illustrate the ideas by implementing a basic linked list. Since many people aren’t aware you can do C generics , I figured I’d start simple and build up to this:I hesitate to even mention this, because I do not like it, but its worth comparing to the technique at the end of this article. It works like this: you write your data structure in a header, using macros for your types, and then  the header multiple times; once for each type the data structure will be used with.While it  generic and type safe, it has downsides:makes it hard to find where types and functions are defined (because they’re constructed by macros)code completion may not handle them wellbloats your binary size and build times with copies of the same functionsrequires using type-prefixed functions: Foo_list_prepend() and int_list_prepend() vs just Another way to make a data structure generic is to use . It’s not type safe but we’ll get to that.Note:  is used for familiarity, but I highly recommend Arenas instead. You can watch or read about them.Having  and its  as separate allocations isn’t ideal from a memory and performance perspective. It requires 2 allocations per node when one would do, the  pointer uses memory unnecessarily, and you will likely get two cache misses per node when traversing the list: once getting the next node, and once getting its data. We can fix these issues with…Generics level 2: Inline storageInstead of storing a pointer to the node’s data, we can use a Flexible Array Member to store the data inside the node. To do so, we make a single allocation large enough for both the node and the type it stores:Now  and the actual contents of  are beside each other in memory, solving the issues of the  approach. Unfortunately we now have to pass the size, but we’ll fix that in the next sectionIf you wanted to avoid the , and initialize the node’s memory directly, you could do so with a  function:Generics level 3: Type CheckingThe part you’ve all been waiting for: how to get the compiler to error when we try to add the wrong type to a list. The way I found to do this is to use a union with a  member that has a parameterized type:How does that help us? Well, we can use the ternary operator to enforce that the  parameter is the same type as the list’s :The macro also handles passing the item size for us! This is the error Clang produces when adding the wrong type to the list:Macros get a bad rep, but I think this is fairly understandable. Some things to note:  is never used at runtime, it exists just for type information at compile time. Using a union makes  not consume any memory.If you’re writing a generic function that needs to return a pointer to contained data, you can use  to cast the return type from  to the data structure’s  type.  is supported in all three big C compilers (clang, gcc,  msvc since version 19.39).If for some reason you don’t like using the ternary operator to ensure two types are the same, a previous version of this article used a different technique:One annoying thing about C compilers released prior to late 2025 is that they do not consider these two variables to have the same type:Even though the variables have identical type definitions, the compiler still errors because they are . A  avoids the issue:You can use this for any type of data structure, even ones with multiple associated types, like a hash map:For more detail, like how the  macro is implemented, see the code hereThanks to Martin Fouilleul for the encouragement to finish this post, which I’ve been sitting on for months, and the feedback on early drafts.]]></content:encoded></item><item><title>A CarFax for Used PCs; Hewlett Packard wants to give old laptops new life</title><link>https://spectrum.ieee.org/carmax-used-pcs</link><author>rubenbe</author><category>hn</category><pubDate>Mon, 30 Jun 2025 16:38:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ E-waste Monitore-wasteMany enterprises follow a standard three-year replacement cycle, assuming older computers are inefficient. However, many of these devices are still functional and could perform well with minor upgrades or maintenance. The issue is, no one knows what the weak points are for a particular machine, or what the needed maintenance is, and the diagnostics would be too costly and time-consuming. It’s easier to just buy brand new laptops.When buying a used car, dealerships and individual buyers can access each car’s particular CarFax report, detailing the vehicle’s usage and maintenance history. Armed with this information, dealerships can perform the necessary fixes or upgrades before reselling the car. And individuals can decide whether to trust that vehicle’s performance. We at HP realized that, to prevent unnecessary e-waste, we need to collect and make available usage and maintenance data for each laptop, like a CarFax for used PCs.There is a particular challenge to collecting usage data for a PC, however. We need to make sure to protect the user’s privacy and security. So, we set out to design a data-collection protocol for PCs that manages to remain secure.The firmware-level data collector Luckily, the sensors that can collect the necessary data are already installed in each PC. There are thermal sensors that monitor CPU temperature, power-consumption monitors that track energy efficiency, storage health indicators that assess solid state drive (SSD) wear levels, performance counters that measure system utilization, fan-rotation-speed sensors that detect cooling efficiency, and more. The key is to collect and store all that data in a secure yet useful way. We decided that the best way to do this is to integrate the life-cycle records into the firmware layer. By embedding telemetry capabilities directly within the firmware, we ensure that device health and usage data is captured the moment it is collected. This data is stored securely on HP SSD drives, leveraging hardware-based security measures to protect against unauthorized access or manipulation. The secure telemetry protocol we’ve developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we use is the Endpoint Security Controller, a dedicated piece of hardware embedded in business-class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD—where telemetry and sensitive data are stored—by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered.Then, the collected data is recorded in a time-stamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs.The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information.The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP’s factory and supply-chain records, digital-services platforms, customer-support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner databases, third-party component manufacturers like Intel, and other original equipment manufacturers. This multisource approach creates a complete picture of the device’s entire life cycle, from manufacturing through all subsequent ownership and service events.For IT teams within organizations, we hope the PCFax will bring simplicity and give opportunities for optimization. Having access to fine-grained usage and health information for each device in their fleet can help IT managers decide which devices are sent to which users, as well as when maintenance is scheduled. This data can also help device managers decide which specific devices to replace rather than issuing new computers automatically, enhancing sustainability. And this can help with security: With real-time monitoring and firmware-level protection, IT teams can mitigate risks and respond swiftly to emerging threats. All of this can facilitate more efficient use of PC resources, cutting down on unnecessary waste.We also hope that, much as the CarFax gives people confidence in buying used cars, the PCFax can encourage resale of used PCs. For enterprises and consumers purchasing second-life PCs, it provides detailed visibility into the complete service and support history of each system, including any repairs, upgrades, or performance issues encountered during its initial deployment. By making this comprehensive device history readily available, PCFax enables more PCs to find productive second lives rather than being prematurely discarded, directly addressing the e-waste challenge while providing economic benefits to both sellers and buyers in the secondary PC market.While HP’s solutions represent a significant step forward, challenges remain. Standardizing telemetry frameworks across diverse ecosystems is critical for broader adoption. Additionally, educating organizations about the benefits of life-cycle records will be essential to driving uptake. We are also working on integrating AI into our dashboards. We hope to use AI models to analyze historical telemetry data and predict failures before they happen, such as detecting increasing SSD write cycles to forecast impending failure and alert IT teams for proactive replacement, or predicting battery degradation and automatically generating a service ticket to ensure a replacement battery is ready before failure, minimizing downtime.We plan to start rolling out these features at the beginning of 2026.]]></content:encoded></item></channel></rss>