<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>I&apos;m too dumb for Zig&apos;s new IO interface</title><link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link><author>begoon</author><category>hn</category><pubDate>Sat, 23 Aug 2025 06:39:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like this performance issue that I opened. And it relied on a mix of types, which always confused me, and a lot of  - which is generally great, but a poor foundation to build an interface on.I've been slowly upgrading my libraries, and I ran into changes to the  client used by my smtp library. For the life of me, I just don't understand how it works.Zig has never been known for its documentation, but if we look at the documentation for , we'll find:input output options InitErrorClient
Initiates a TLS handshake  establishes a TLSv1 TLSv1 sessionSo it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a , but  does expose a  and  method, so that's probably a good place to start: stream  stdnetallocator stream writer  stream reader  stream tls_client  stdcryptotlsClient
  readerwriterinterfaceNote that  returns a  and  returns a  - those aren't the types our  expects. To convert the  to an , we need to call its  method. To get a  from an , we need the address of its  field. This doesn't seem particularly consistent. Don't forget that the  and  need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of . In a real word example, I think it means that I'll always have to wrap the  into my own heap-allocated type; giving the writer and reader have a cozy stable home.Speaking of allocations, you might have noticed that  and  take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation  tell me these need to be at least std.crypto.tls.max_ciphertext_record_len large, so we need to fix things a bit: write_buf writer  streamwrite_buf read_buf reader  streamread_bufHere's where the code stands:  std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf tls_client  stdcryptotlsClient
      readerwriterinterface tls_clientBut if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a  and a . Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while  and  are passed another.Let's give it what it wants AND send some data: bundle  bundleallocator bundleallocator tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer  tls_client tls_clientwriterNow, if I try to run it, the program just hangs. I don't know what  is, but I know Zig now loves buffers, so let's try to give it something: write_buf2 tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer write_buf2 tls_client tls_clientwriterGreat, now the code doesn't hang, all we need to do is read the response.  exposes a  field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not  doesn't have a  method. It has a  a , a  (which seems close, but it blocks until the provided buffer is full), a  and a lot more, but nothing like the  I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer: buf wbuf n  tls_clientreaderwbuflen
stddebugn bufnIf we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also  to provide a .Here's my current version (it doesn't work, but it doesn't crash!): std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf bundle  bundleallocator bundleallocator write_buf2 read_buf2 tls_client  stdcryptotlsClient
      readerwriterinterfaceca bundle  bundlehost explicit read_buffer read_buf2write_buffer write_buf2 tls_client tls_clientwriter buf wbuf n  tls_clientreaderwbuflen
  stddebugn bufnWhen I looked through Zig's source code, there's only one place using . It helped to get me where where I am. I couldn't find any tests.I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find  - the renamed version of . Maybe there's a helper like: tls.Client.init(allocator, stream) somewhere. And maybe it makes sense that we do  but  - I'm reminded of Go's  and . And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a  function on , but at this point I wouldn't bet on me.]]></content:encoded></item><item><title>Measuring the environmental impact of AI inference</title><link>https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</link><author>ksec</author><category>hn</category><pubDate>Sat, 23 Aug 2025 03:22:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Japan city drafts ordinance to cap smartphone use at 2 hours per day</title><link>https://english.kyodonews.net/articles/-/59582</link><author>Improvement</author><category>hn</category><pubDate>Sat, 23 Aug 2025 02:20:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[NAGOYA - A central Japan city said Thursday it will seek to pass an ordinance recommending all residents limit smartphone use to two hours a day outside of work and school amid concerns over the impact of excessive technology exposure, though there will be no penalties proposed.The ordinance drafted by the city of Toyoake in Aichi Prefecture is likely to be the first such municipal regulation in Japan that targets a limit on the use of smartphones and other electronic devices, according to the city. If passed by the local assembly, the ordinance will come into effect on Oct. 1."We want the ordinance to provide an opportunity for people to think about how they use smartphones," an official said.To ensure that children get a good night's sleep, the draft ordinance urges elementary school students to refrain from using smartphones after 9 p.m. and junior high students and older to put their devices down by 10 p.m.The draft acknowledged that smartphones, personal computers and tablets are necessities, but warned that overuse of social media and video streaming may have a negative impact on health and family life.The city will work with schools and parents to promote the healthy use of electronic devices, according to the draft ordinance.]]></content:encoded></item><item><title>My tips for using LLM agents to create software</title><link>https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html</link><author>efitz</author><category>hn</category><pubDate>Sat, 23 Aug 2025 00:59:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Computer fraud laws used to prosecute leaking air crash footage to CNN</title><link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link><author>BallsInIt</author><category>hn</category><pubDate>Sat, 23 Aug 2025 00:04:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[from the if-it-can-be-abused,-it-WILL-be-abused deptEarlier this year, an Army helicopter collided with a passenger plane over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had leaked footage of the collision to CNN. The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the Computer Fraud and Abuse Act (CFAA). This broadly written law not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. CCTV footage from  the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague “computer trespass” law. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.On February 3, the MWAA announced both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”In May, however, local prosecutors quietly dropped the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of the plain text of the law: Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer networkA smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects had every right to access, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”]]></content:encoded></item><item><title>Popular Japanese smartphone games have introduced external payment systems</title><link>https://english.kyodonews.net/articles/-/59689</link><author>anigbrowl</author><category>hn</category><pubDate>Fri, 22 Aug 2025 23:50:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.For outside transactions, users make payments through channels other than apps, such as game websites. Settlement service providers like Digital Garage Inc. and GMO Tech Inc. typically charge a 5 percent commission, far below Apple's and Google's rates.Payments through external websites in the in-app purchase market, estimated at over 1 trillion yen ($6.8 billion), are expected to bring user discounts and boost providers' profitability, analysts said.In the survey, Kyodo News received responses from eight of 12 domestic game makers, while two declined to comment and two did not respond. Of the 12 titles from the eight firms, 11 have adopted external settlements.In August last year, Mixi Inc. introduced an outside settlement system for its blockbuster game "Monster Strike," allowing its users to purchase about 5 percent more items compared with in-app payments.]]></content:encoded></item><item><title>Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion</title><link>https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</link><author>voxadam</author><category>hn</category><pubDate>Fri, 22 Aug 2025 23:09:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bluesky Goes Dark in Mississippi over Age Verification Law</title><link>https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</link><author>BallsInIt</author><category>hn</category><pubDate>Fri, 22 Aug 2025 22:51:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ can no longer use the social media platform Bluesky. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent US Supreme Court decision that allows the state to enforce strict age verification for social media platforms.According to Bluesky, Mississippi’s approach to verification “would fundamentally change” how users access the site. “We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,” the Bluesky team said in its statement.Bluesky did not respond to a request for comment.The company says that compliance with Mississippi’s law—which would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age—is not possible with the team’s current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. In the UK, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they’re over the age of 18. The state of Texas has a similar law the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK spiked just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use video game characters to trick face scans. Furthermore, critics say that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.“We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,” Bluesky wrote.]]></content:encoded></item><item><title>U.S. government takes 10% stake in Intel</title><link>https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</link><author>givemeethekeys</author><category>hn</category><pubDate>Fri, 22 Aug 2025 21:01:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Lip-Bu Tan, chief executive officer of Intel Corp., departs following a meeting at the White House in Washington, DC, US, on Monday, Aug. 11, 2025. Alex Wroblewski | Bloomberg | Getty ImagesCommerce Secretary Howard Lutnick said on Friday that the U.S. government has taken a 10% stake in embattle chipmaker Intel, the Trump administration's latest effort to exert control over corporate America.  shares rose about 6% during trading on Friday. They were flat in extended trading.Intel, the only American company capable of making advanced chips on U.S. soil, said in a press release that the government made an $8.9 billion investment in Intel common stock, purchasing 433.3 million shares at a price of $20.47 per share, giving it a 10% stake in the company. Intel noted that the price the government paid was a discount to the current market price.Of the total, $5.7 billion of the government funds will come from grants under the CHIPS Act that had been awarded but not paid, and $3.2 billion will come from separate government awards under a program to make secure chips."The United States paid nothing for these Shares, and the Shares are now valued at approximately $11 Billion Dollars," President Trump wrote in a post on Truth Social. "This is a great Deal for America and, also, a great Deal for INTEL." The government will also have a warrant to buy an additional 5% of Intel shares if the company is no longer majority owner of its foundry business.Intel said that the U.S. government won't have a board seat or other governance rights."As the only semiconductor company that does leading-edge logic R&D and manufacturing in the U.S., Intel is deeply committed to ensuring the world's most advanced technologies are American made," Intel CEO Lip-Bu Tan said in the press release. Earlier on Friday, President Donald Trump said the government should get about 10% of the company, which has a market cap of just over $100 billion.  "They've agreed to do it and I think it's a great deal for them," Trump told reporters Friday at the White HouseWhite House officials previously told CNBC that Trump and Tan will meet on Friday afternoon. Lutnick's post included a photo with Tan.The marks the latest example of a distinct shift in U.S. industrial policy, with the government taking an active role in the private sector. Lutnick told CNBC this week that the U.S. government was seeking an equity stake in Intel in exchange for CHIPS Act funds."We should get an equity stake for our money," Lutnick said on CNBC's "Squawk on the Street." "So we'll deliver the money, which was already committed under the Biden administration. We'll get equity in return for it."Earlier this week, Intel announced another major backer, when SoftBank said it would make a $2 billion investment in the chipmaker, equal to about 2% of the company.Intel has been spending billions of dollars to build a series of chip factories in Ohio, an area the company previously called the "Silicon Heartland," where Intel would be able to produce the most advanced chips, including for AI.But in July, Tan said in a memo to employees that there would be "no more blank checks," and that it was slowing down the construction of its Ohio factory complex, depending on market conditions. Intel's Ohio factory is now scheduled to start operations in 2030.Intel said last fall that it had finalized a nearly $8 billion grant under the CHIPS and Science Act to fund its factory-building plans. The CHIPS Act was passed in 2022, under the Biden administration.— CNBC's David Sucherman contributed to this report.]]></content:encoded></item><item><title>Nitro: A tiny but flexible init system and process supervisor</title><link>https://git.vuxu.org/nitro/about/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 22 Aug 2025 19:06:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.There are four main applications it is designed for:As init for a Linux machine for embedded, desktop or server purposesAs init for a Linux initramfsAs init for a Linux container (Docker/Podman/LXC/Kubernetes)As unprivileged supervision daemon on POSIX systemsNitro is configured by a directory of scripts, defaulting to
 (or the first command line argument).Kernel support for Unix sockets or writable  on another fsBenefits over other systemsAll state is kept in RAM, works without tricks on read-only root file systems.Efficient event-driven, polling free operation.Zero memory allocations during runtime.No unbounded file descriptor usage during runtime.One single self-contained binary, plus one optional binary to
control the system.No configuration compilation steps needed, services are simple
directories containing scripts.Supports reliable restarting of services.Reliable logging mechanisms per service or as default.Support for logging chains spread over several services.Works independently of properly set system clock.Can be run on FreeBSD from /etc/ttys (sets up file descriptors 0, 1, 2).Tiny static binary when using musl libc.Every directory inside  (or your custom service directory)
can contain several files:, an optional executable file that is run before the service starts.
It must exit with status 0 to continue., an optional executable file that runs the service;
it must not exit as long as the service is considered running.
If there is no  script, the service is considered a “one shot”,
and stays “up” until it’s explicitly taken “down”., an optional executable file that is run after the 
process finished.  It is passed two arguments, the exit status
of the  process (or -1 if it was killed by a signal)
and the signal that killed it (or 0, if it exited regularly)., a symlink to another service directory.
The standard output of  is connected to the standard input of the
service under  by a pipe.  You can chain these for reliable and
supervised log processing., an optional file that causes nitro to not bring up this
service by default.Service directories ending with ‘@’ are ignored; they can be used
for parameterized services.Service names must be shorter than 64 chars, and not contain ,
 or newlines.You may find runit’s  useful when writing  scripts.: this service is used as a logging service for all services
that don’t have a  symlink.:  is run before other services are brought up.
You can already use  in  to bring up services
in a certain order.
 is run before all remaining services are killed and the
system is brought down.
After all processes are terminated,  is run.
The program , if it exists, is run instead of exiting
when an unrecoverable, fatal error happens.
The program , if it exists, is executed into
instead of a shutdown.  This can be used to implement an initramfs,
for example.Service directories ending in  are ignored, however you can refer
to parametrized services by symlinks (either in the service directory
or as a  symlink), or start them manually using .The part after the , the parameter, is passed to the scripts as
first argument.For example, given you have a script  and a symlink
 -> , nitro will spawn .  Upon
running , nitro will spawn , even if it does not exist in the service directory.The lifecycle of a machine/container/session using nitro consists of
three phases.First, the system is brought up.  If there is a special service
g, its  script is run first.  After it finishes, all
services not marked  are brought up.When a service exits, it’s being restarted, potentially waiting for
two seconds if the last restart happened too quickly.By using  or , the system can be
brought down.  If it exists,  will be run.  After this,
nitro will send a SIGTERM signal to all running services and waits for
up to 7 seconds for the service to exit.  Otherwise, a SIGKILL is
sent.  After all processes are terminated,  is run.Finally, nitro reboots or shuts down the system; or just exits when it
was used as a container init or unprivileged supervisor.  (When a
reboot was requested, it re-execs itself.  This requires being called
with absolute path for the binary and the service directory.)Controlling nitro with nitroctlYou can remote control a running nitro instance using the tool
.Usage: nitroctl [COMMAND] [SERVICE]list: show a list of services and their state, pid, uptime and last
exit status.down: stop SERVICE (sending SIGTERM or the first letter of )start: start SERVICE, waiting for successrestart: restart SERVICE, waiting for successstop: stop SERVICE, waiting for successp: send signal SIGSTOP to SERVICEc: send signal SIGCONT to SERVICEh: send signal SIGHUP to SERVICEa: send signal SIGALRM to SERVICEi: send signal SIGINT to SERVICEq: send signal SIGQUIT to SERVICE1: send signal SIGUSR1 to SERVICE2: send signal SIGUSR2 to SERVICEt: send signal SIGTERM to SERVICEk: send signal SIGKILL to SERVICEpidof: print the PID of the SERVICE, or return 1 if it’s not uprescan: re-read , start added daemons, stop removed daemonsShutdown: shutdown (poweroff) the systemReboot: reboot the systemControlling nitro by signalsrescan can also be triggered by sending  to nitro.reboot can also be triggered by sending  to nitro.shutdown can also be triggered by sending  to nitro, unless
nitro is used as Linux pid 1.Nitro is self-contained and can be booted directly as pid 1.
It will mount  and  when required, everything else
should be done with .When receiving Ctrl-Alt-Delete, nitro triggers an orderly reboot.Nitro as init for a Docker containerNitro is compiled statically, so you can copy it into your container easily:COPY ./nitro /bin/
COPY ./nitroctl /bin/
CMD ["/bin/nitro"]
Note that  must exist in the container if you want to use the
default control socket name.You can put the control socket onto a bind mount and remote control
 using  from the outside by pointing  to
the appropriate target.You can add this line to  to run  supervised by
FreeBSD :/etc/nitro "/usr/local/sbin/nitro" "" on
I’m standing on the shoulder of giants; this software would not have
been possible without detailed study of prior systems such as
daemontools, freedt, runit, perp, and s6.nitro is licensed under the 0BSD license, see LICENSE for details.]]></content:encoded></item><item><title>Scientists just found a protein that reverses brain aging</title><link>https://www.sciencedaily.com/releases/2025/08/250820000808.htm</link><author>stevenjgarner</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:56:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Now, researchers at UC San Francisco have identified a protein that's at the center of this decline.They looked at how the genes and proteins in the hippocampus changed over time in mice and found just one that differed between old and young animals. It's called FTL1.Old mice had more FTL1, as well as fewer connections between brain cells in the hippocampus and diminished cognitive abilities.When the researchers artificially increased FTL1 levels in young mice, their brains and behavior began to resemble that of old mice.In experiments in petri dishes, nerve cells engineered to make lots of FTL1 grew simple, one-armed neurites -- rather than the branching neurites that normal cells create.But once the scientists reduced the amount of FTL1 in the hippocampus of the old mice, they regained their youth. They had more connections between nerve cells, and the mice did better on memory tests."It is truly a reversal of impairments," said Saul Villeda, PhD, associate director of the UCSF Bakar Aging Research Institute and senior author of the paper, which appears in  on Aug. 19. "It's much more than merely delaying or preventing symptoms."In old mice, FTL1 also slowed down metabolism in the cells of the hippocampus. But treating the cells with a compound that stimulates metabolism prevented these effects.Villeda is optimistic the work could lead to therapies that block the effects of FTL1 in the brain."We're seeing more opportunities to alleviate the worst consequences of old age," he said. "It's a hopeful time to be working on the biology of aging."Authors: Other UCSF authors are Laura Remesal, PhD, Juliana Sucharov-Costa, Karishma J.B. Pratt, PhD, Gregor Bieri, PhD, Amber Philp, PhD, Mason Phan, Turan Aghayev, MD, PhD, Charles W. White III, PhD, Elizabeth G. Wheatley, PhD, Brandon R. Desousa, Isha H. Jian, Jason C. Maynard, PhD, and Alma L. Burlingame, PhD. For all authors see the paper.Funding: This work was funded in part by the Simons Foundation, Bakar Family Foundation, National Science Foundation, Hillblom Foundation, Bakar Aging Research Institute, Marc and Lynne Benioff, and the National Institutes of Health (AG081038, AG067740, AG062357, P30 DK063720). For all funding see the paper.]]></content:encoded></item><item><title>Show HN: JavaScript-free (X)HTML Includes</title><link>https://github.com/Evidlo/xsl-website</link><author>Evidlo</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:47:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I've been working on a little demo for how to avoid copy-pasting header/footer boilerplate on a simple static webpage. My goal is to approximate the experience of Jekyll/Hugo but eliminate the need for a build step before publishing. This demo shows how to get basic templating features with XSL so you could write a blog post which looks like
Some properties which set this approach apart from other methods:  - no build step (no need to setup Jekyll on the client or configure Github/Gitlab actions)
  - works on any webserver (e.g. as opposed to server-side includes, actions)
  - normal looking URLs (e.g. `example.com/foobar` as opposed to `example.com/#page=foobar`)

There's been some talk about removing XSLT support from the HTML spec [0], so I figured I would show this proof of concept while it still works.]]></content:encoded></item><item><title>The first Media over QUIC CDN: Cloudflare</title><link>https://moq.dev/blog/first-cdn/</link><author>kixelated</author><category>hn</category><pubDate>Fri, 22 Aug 2025 18:24:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[🚨 It’s finally happening! 🚨Cloudflare has just announced their Media over QUIC CDN!
It’s an , and you can test MoQ on their , anycast network.
Try it out, and convince your boss’ boss that the writing is on the wall.If you’ve been living under a rock, MoQ is an up-and-coming standard for live media, aiming to supplant WebRTC, HLS/DASH, and even  as the one to rule them all.
And now Cloudflare wins the award for the first CDN offering!Your prize is a blog post. You’re welcome mega-corp.Also, , some shameless self-promotion: I just soft-launched hang.live.
Check it out if you want to see the  cool stuff you can do with MoQ.I’m biased so naturally I’m going to use @kixelated/hang (smash that star button).
You can publish a live broadcast in the browser using the web demo or the library:There’s a link to watch your live broadcast using the web demo, or again you can use the library:You might even notice  because I’ve been experimenting with AI features (gotta get funding eventually 💰).
They’re generated  using silero-vad + whisper + transformers.js + onnxruntime-web + WebGPU and transmitted using MoQ of course.
But that’s a whole separate blog post; it’s pretty cool. You don’t have to use this Web Component API.
hang.live uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.
There’s a  section at the end of this blog if you LOVE sample code, but I’m not going to bore the rest of you.There’s also a 🦀 Rust 🦀 library to import MP4, pipe media from ffmpeg, and publish/watch using gstreamer so you can do more complicated media stuff without 🤮 Javascript 🤮.
I wish I could spend more time on the Rust side but  is a big deal.
We are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in 🤮 Javascript 🤮.
I can suffer and you can reap the rewards.What’s not available yet?This is a  release.
Cloudflare is only supporting a  subset of an old draft, which is even smaller than my tiny subset.
They’re using a fork of my terrible code so bugs are guaranteed.There’s no authentication yet: choose an unguessable name for each broadcast.There’s no ANNOUNCE support: my conferencing example uses  to discover when broadcasts start/stop, so that won’t work.Nothing has been optimized: the user experience will improve over time.If any of these are deal breakers, then you could always run your own moq-relay in the meantime.
I’ve been adding new features and fixing a bunch of stuff  Cloudflare smashed that fork button.
For example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.There’s even a terraform module that powers .
You too can run your own “global” CDN with 3 nodes and pay GCP a boatload of money for the privilege.
It’s not  as good as Cloudflare’s network, currently available for free…Or host  yourself!
It should even work on private networks provided you wrestle with TLS certificates.
I’d also love to get MoQ running over Iroh for peer-to-peer action if anybody wants to help.As a great philosopher once said:Apathy is a tragedy and boredom is a crime.
- Bo BurnhamThis is a big deal.
The biggest of deals.
The HUGEST of deals.I’ve been an outspoken critic of the MoQ standardization process.
It’s just really difficult to design a protocol, via a cross-company committee, before there’s been any real world usage.
It’s been over 3 years since I fought Amazon lawyers and published my first MoQ draft.
It’s going to be at least another 3 years before even the base networking layer becomes an RFC.
The best standards take a while.
Look no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.
And they had a boatload of production data to shape the specification.
Meanwhile, we have only had a Big Buck Bunny demo, and I believe the standard has veered off course as a result.Cloudflare has done something fantastic and said:fuck waiting for a RFC, let’s release somethingOkay they didn’t say that, but this is  the mentality that MoQ needs right now.
.
.
.Holy shit I’m Shia LaBeouf.Arguing in the 650+ issues and 500+ PRs can wait for another day.
Tweaking the messaging encoding for the hundredth time can wait for another day.
We’re still going to make sure that MoQ gets standardized , but it’s more important to get  out there.I’m looking at you: Google, Akamai, Fastly, etc.
Take some code, run it on some spare servers, and start to learn what customers need  you design the protocol.We’re effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.
Don’t judge MoQ based on these initial offerings.
We’ve got a  of work to do.
.Join the Discord.
Somehow there’s 900+ people in there.
Ping me and I will do whatever I can to help.
 if it means putting one more nail in the WebRTC coffin.Javascript is an AbominationYou win some bonus documentation.
Congrats!
I knew you would win.Here’s an example of my reactive library in action.
It powers hang.live so the API is subject to change and is probably already out of date.
When in doubt, consult the source code like the hacker you are.There’s even some  features behind undocumented APIs.
Like running an object detection model in browser and publishing the results as a MoQ track.
Stay tuned for a blog post about that if I can figure out a better use-case than a cat cam. 🐈Also, for the record, Typescript is really nice.
🤮 Javascript 🤮 is still an abomination.]]></content:encoded></item><item><title>Should the web platform adopt XSLT 3.0?</title><link>https://github.com/whatwg/html/issues/11578</link><author>protomolecool</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:56:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Sprinkling self-doubt on ChatGPT</title><link>https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</link><author>ingve</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:45:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I replaced my ChatGPT personalization settings with this prompt a few weeks ago and promptly forgot about it:Be extraordinarily skeptical of your own correctness or stated assumptions. You aren't a cynic, you are a highly critical thinker and this is tempered by your self-doubt: you absolutely hate being wrong but you live in constant fear of itWhen appropriate, broaden the scope of inquiry beyond the stated assumptions to think through unconvenitional opportunities, risks, and pattern-matching to widen the aperture of solutionsBefore calling anything "done" or "working", take a second look at it ("red team" it) to critically analyze that you really are done or it really is workingI noticed a difference in results right away (even though I kept forgetting the change was due to my instructions and not the separately tumultuous rollout of GPT-5).Namely, pretty much every initial response now starts with:An expression of caution, self-doubt, and desire to get things rightHilariously long "thinking" times (I asked it to estimate the macronutrients in lettuce yesterday and it spent 3 minutes and 59 seconds reasoning)A post-hoc adversarial "red team" analysis of whatever it just vomited up as an answerI'm delighted to report that ChatGPT's output has been more useful since this change. Still not altogether , but better at the margins. In particular, the "red team" analysis at the end of many requests frequently spots an error and causes it to arrive at the  answer, which—if nothing else—saves me the step of expressing skepticism. And even when ChatGPT is nevertheless wrong, its penchant for extremely-long thinking times means I'm getting my money's worth in GPU time.]]></content:encoded></item><item><title>Leaving Gmail for Mailbox.org</title><link>https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</link><author>giuliomagnifico</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:41:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the EU-U.S. and Swiss-U.S. Data Privacy Frameworks).For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer “paying” with my personal data, but instead paying the actual price of the email service.Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they’re plain text/html emails with no attachments or fancy features. I couldn’t care less about the rest of the “suite", like notes, contacts, calendars and all that extra stuff.So, after a bit of research, I narrowed it down to three different services:The last two providers offered true end-to-end encryption, at a cost of about €3/4 per month. Sounds good… but the catch is that to use their end-to-end encryption you’re forced to use their apps (or, on macOS, run a background “bridge”).That’s a no go for me, because I love Apple’s Mail app on macOS and iOS, it just works perfectly for my needs, and I don’t want to give that up.So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at €2.50/month (paid annually). You can even expand the mail storage up to 100GB, at €0.20 per gigabyte.I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for €0.20/GB.There’s also a free one-month trial, but it’s pretty limited since you can’t send emails outside of mailbox.org domains.So win the end, I registered my new address giuliomagnifico@mailbox.org and paid €3 for a month of testing. That means I’m covered for two months, and then I can just “top up” the account with €30 for a full year.Mailbox.org doesn’t use auto-renewal, so you have to manually top up your account. Nice featureThe web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.And on mobile it’s very usable too.One thing I prefer is using folders instead of Gmail’s “labels.” Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).Mailbox.org also has all the features I need,
and probably way more than I’ll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on… none of which I really care about.I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.After creating an “app password” on Gmail, I installed the Docker image and ran the tool with this script:#!/bin/sh
set -eu

HOST1="imap.gmail.com"
USER1="giuliomagnifico@gmail.com"
PASS1="xxx"

HOST2="imap.mailbox.org"
USER2="giuliomagnifico@mailbox.org"
PASS2="xxx"

LOGDIR="/home/imapsync/logs"
mkdir -p "$LOGDIR"
LOGFILE="$LOGDIR/sync_$(date +%F_%H-%M-%S).log"

echo "Starting: $(date)"
docker compose run --rm imapsync imapsync \
  --host1 "$HOST1" --user1 "$USER1" --password1 "$PASS1" --ssl1 \
  --host2 "$HOST2" --user2 "$USER2" --password2 "$PASS2" --ssl2 \
  --automap --syncinternaldates --skipsize \
  --useuid --addheader --usecache --buffersize 4096 \
  --nofoldersizes --nofoldersizesatend \
  --exclude "\[Gmail\]/All Mail" \
  --regextrans2 "s/\[Imap\]\/Archive/Archive/" \
  --log > "$LOGFILE" 2>&1

echo "Complete: $(date)"
echo "Log file: $LOGFILE"
The script excludes the All Mail folder" using: --exclude "\[Gmail\]/All Mail" \This to avoid duplicate emails already present in the folders, I also merged the  folder into the general Archive folder using: --regextrans2 "s/\[Imap\]\/Archive/Archive/"This because Apple’s Mail app creates the  folder/label on Gmail whenever you use the “Archive” function instead of “Trash.”The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log[cut]
msg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied 
++++ End looping on each folder
++++ Statistics
Transfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC
Transfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC
Transfer time                           : 11201.5 sec
Folders synced                          : 14/14 synced
Folders deleted on host2                : 0 
Messages transferred                    : 26407 
Messages skipped                        : 0
Messages found duplicate on host1       : 0
Messages found duplicate on host2       : 0
Messages found crossduplicate on host2  : 0
Messages void (noheader) on host1       : 0  
Messages void (noheader) on host2       : 0
Messages found in host1 not in host2    : 0 messages
Messages found in host2 not in host1    : 0 messages
Messages deleted on host1               : 0
Messages deleted on host2               : 0
Total bytes transferred                 : 2297647358 (2.140 GiB)
Total bytes skipped                     : 0 (0.000 KiB)
Message rate                            : 2.4 messages/s
Average bandwidth rate                  : 200.3 KiB/s
Reconnections to host1                  : 0
Reconnections to host2                  : 0
Memory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)
Load end is                             : 0.06 0.08 0.08 1/1135 on 16 cores
CPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus
Biggest message transferred             : 30413995 bytes (29.005 MiB)
Memory/biggest message ratio            : 9.3
Detected 0 errors
This imapsync is up to date. ( local 2.306 >= official 2.290 )( Use --noreleasecheck to avoid this release check. )
Homepage: https://imapsync.lamiral.info/
Exiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1
Removing pidfile /var/tmp//tmp/imapsync.pid
Log file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )
Of course, the full switch will be a gradual process, even though I’ve already updated almost all my main services with the new address.To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.
On the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The  tag is perfect for this, since it add a “real red flag” in Apple Mail on iOS, iPadOS and macOS)Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren’t any “decent” apps for encrypted mail. The same goes for macOS, although there you can just use Thunderbird, which works really well.Here’s how PGP emails look on iOS:To send encrypted emails, you just select “Use PGP encrypted” when composing a new message, after importing your private key, of course.And from the web interface, there’s also a handy feature to quickly import the sender’s public keys:I’m satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here’s my public key:-----BEGIN PGP PUBLIC KEY BLOCK-----
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=GpPW
-----END PGP PUBLIC KEY BLOCK-----
]]></content:encoded></item><item><title>Waymo granted permit to begin testing in New York City</title><link>https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</link><author>achristmascarl</author><category>hn</category><pubDate>Fri, 22 Aug 2025 17:02:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Waymo self-driving cars with roof-mounted sensor arrays traveling near palm trees and modern buildings along the Embarcadero, San Francisco, California, February 21, 2025. Smith Collection/gado | Archive Photos | Getty ImagesThe  autonomous vehicle subsidiary received its first permit from the New York Department of Transportation on Friday to start testing in New York City, Mayor Eric Adams announced Friday. The rollout is the city's first autonomous vehicle testing launch.Waymo will start testing up to eight vehicles in Manhattan and Downtown Brooklyn through late September with the potential to extend the program. New York state law requires the company to have a driver behind the wheel to operate."We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward," Adams said in a release. "New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century."The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel.Waymo has hit expansion mode on its services nationwide, launching in Austin this year and expanding its San Francisco-area operations in March. Waymo also plans to bring autonomous vehicles to Atlanta; Miami; and Washington, D.C., and recently said it will start operations in Philadelphia as it looks to break further into the Northeast market.Waymo's CEO said the company surpassed 10 million robotaxi trips in May.For years, autonomous vehicle companies have sought to introduce their technology to the Big Apple, with Waymo previously taking a crack at it in 2021. At that time, the company rolled out some cars in certain areas of the city for manual driving and data collection.New York City has also expressed interest in bringing autonomous vehicles to the city. Last year, the Adams administration implemented a series of safety requirements for responsible testing in the city and opened a permit program.As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.]]></content:encoded></item><item><title>Show HN: Clyp – Clipboard Manager for Linux</title><link>https://github.com/murat-cileli/clyp</link><author>timeoperator</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 16:03:26 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>FFmpeg 8.0</title><link>https://ffmpeg.org/index.html#pr8.0</link><author>gyan</author><category>hn</category><pubDate>Fri, 22 Aug 2025 15:22:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
        A complete, cross-platform solution to record, convert and stream audio and video.
      Converting  and  has never been so easy.
    $ ffmpeg -i input.mp4 output.aviAugust 22nd, 2025, FFmpeg 8.0 
  A new major release, FFmpeg 8.0 ,
  is now available for download.
  Thanks to several delays, and modernization of our entire infrastructure, this release ended up
  being one of our largest releases to date. In short, its new features are:
  Native decoders: , ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728VVC decoder improvements: ,
                                  ,
                                  Palette ModeVulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5Formats: MCC, G.728, Whip, APVFilters: colordetect, pad_cuda, scale_d3d11, Whisper, and others
  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.
  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,
  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.
  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work
  on any implementation of Vulkan 1.3.
  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,
  as enabling Vulkan decoding is sufficient to use them.
  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).
  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).
  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,
  to be merged soon and available with the next minor release.
  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with
  more mainstream codecs not being planned for support.
  Depending on the hardware, these new codecs can provide very significant speedups, and open up
  possibilities to work with them for situations like non-linear video editors and
  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.
  
  The project has recently started to modernize its infrastructure. Our mailing list servers have been
  fully upgraded, and we have recently started to accept contributions via a new forge, available on
  code.ffmpeg.org, running a Forgejo instance.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 30th, 2024, FFmpeg 7.1 
    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,
    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain
    traction with broadcast standardization bodies.
    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting
    to be adopted by streaming websites, due to its extensive volume normalization metadata.
    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated
    by recent phones and VR headsets.
    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an
    external library.
    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode
    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity
    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,
    and FFmpeg is aiming to have day-one support.
  
    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally
    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,
    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10
    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.
    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also
    laid the path for more advanced forms of negotiation.
    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,
    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 11th, 2024, Coverity
  The number of issues FFmpeg has in Coverity (a static analyzer) is now lower than it has been since 2016.
  Our defect density is less than one 30th of the average in OSS with over a million code
  lines. All this was possible thanks to a grant from the Sovereign Tech Fund.
  June 2nd, 2024, native xHE-AAC decoder
  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround
  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is
  coming soon. Work is also ongoing to improve its stability and compatibility.
  During the process we found several specification issues, which were then submitted back to the authors
  for discussion and potential inclusion in a future errata.
  May 13th, 2024, Sovereign Tech Fund
  The FFmpeg community is excited to announce that Germany's
  Sovereign Tech Fund
  has become its first governmental sponsor. Their support will help
  sustain the maintainance of the FFmpeg project, a critical open-source
  software multimedia component essential to bringing audio and video to
  billions around the world everyday.
  April 5th, 2024, FFmpeg 7.0 "Dijkstra"
  This release is  backwards compatible, removing APIs deprecated before 6.0.
  The biggest change for most library callers will be the removal of the old bitmask-based
  channel layout API, replaced by the  API allowing such
  features as custom channel ordering, or Ambisonics. Certain deprecated 
  CLI options were also removed, and a C11-compliant compiler is now required to build
  the code.
  
  As usual, there is also a number of new supported formats and codecs, new filters, APIs,
  and countless smaller features and bugfixes. Compared to 6.1, the  repository
  contains almost ∼2000 new commits by ∼100 authors, touching >100000 lines in
  ∼2000 files — thanks to everyone who contributed. See the
  Changelog,
  APIchanges,
  and the git log for more comprehensive lists of changes.
  January 3rd, 2024, native VVC decoder
  The  library now contains a native VVC (Versatile Video Coding)
  decoder, supporting a large subset of the codec's features. Further optimizations and
  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,
  Frank Plowman, Shaun Loo, and Wu Jianhua.
  December 18th, 2023, IAMF support
  The  library can now read and write IAMF
  (Immersive Audio) files. The  CLI tool can configure IAMF structure with the new
   option. IAMF support was written by James Almer.
  December 12th, 2023, multi-threaded  CLI tool
  Thanks to a major refactoring of the  command-line tool, all the major
  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now
  run in parallel. This should improve throughput and CPU utilization, decrease latency,
  and open the way to other exciting new features.
  
  Note that you should  expect significant performance improvements in cases
  where almost all computational time is spent in a single component (typically video
  encoding).
  November 10th, 2023, FFmpeg 6.1 "Heaviside"Playdate video decoder and demuxerExtend VAAPI support for libva-win32 on Windowsafireqsrc audio source filterffmpeg CLI new option: -readrate_initial_burstzoneplate video source filtercommand support in the setpts and asetpts filtersVulkan decode hwaccel, supporting H264, HEVC and AV1Essential Video Coding parser, muxer and demuxerEssential Video Coding frame merge bsfMicrosoft RLE video encoderRaw AC-4 muxer and demuxerRaw VVC bitstream parser, muxer and demuxerBitstream filter for editing metadata in VVC streamsBitstream filter for converting VVC from MP4 to Annex Bscale_vt filter for videotoolboxtranspose_vt filter for videotoolboxsupport for the P_SKIP hinting to speed up libx264 encodingSupport HEVC,VP9,AV1 codec in enhanced flv formatapsnr and asisdr audio filtersSupport HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocolffmpeg CLI '-top' option deprecated in favor of the setfield filterffprobe XML output schema changed to account for multiple variable-fields elements within the same parent elementffprobe -output_format option added as an alias of -of
    This release had been overdue for at least half a year, but due to constant activity in the repository,
    had to be delayed, and we were finally able to branch off the release recently, before some of the large
    changes scheduled for 7.0 were merged.
  
    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs
    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).
    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.
    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,
    reducing overhead.
    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.
    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the
    accurracy of variable frame rate video.
  
    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick
    to the new release schedule we announced at the start of this year.
  
    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.
  May 31st, 2023, Vulkan decoding
    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.
    This is the first vendor-generic and platform-generic decode acceleration API, enabling the
    same code to be used on multiple platforms, with very minimal overhead.
    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,
    saturating all available decode engines the hardware exposes.
  
    Those wishing to test the code can read our
    documentation page.
    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive
    a VkImage to present or manipulate, documentation and examples are available in our source tree.
    Currently, using the latest available git checkout of our
    repository is required.
    The functionality will be included in stable branches with the release of version 6.1, due
    to be released soon.
  
    As this is also the first practical implementation of the specifications, bugs may be present,
    particularly in drivers, and, although passing verification, the implementation itself.
    New codecs, and encoding support are also being worked on, by both the Khronos organization
    for standardizing, and us as implementing it, and giving feedback on improving.
  February 28th, 2023, FFmpeg 6.0 "Von Neumann"
    A new major release, FFmpeg 6.0 "Von Neumann",
    is now available for download. This release has many new encoders and decoders, filters,
    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major
    releases will now bump the version of the ABI. We plan to have a new major release each
    year. Another release-specific change is that deprecated APIs will be removed after 3
    releases, upon the next major bump.
    This means that releases will be done more often and will be more organized.
  
    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.
    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c
    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,
    and the ability to pass option values for filters from a file. There are quite a few new audio
    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.
    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT
    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better
    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V
    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed
    in the doc/APIchanges file in our tree.
    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the
    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.
    Some highlights are:
  Radiance HDR image supportddagrab (Desktop Duplication) video capture filterffmpeg -shortest_buf_duration optionffmpeg now requires threading to be builtffmpeg now runs every muxer in a separate threadAdd new mode to cropdetect filter to detect crop-area based on motion vectors and edgesVAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9WBMP (Wireless Application Protocol Bitmap) image formatMicronas SC-4 audio decodernvenc AV1 encoding supportMediaCodec decoder via NDKMediaCodecQSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9showcwt multimedia filterWADY DPCM decoder and demuxerffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]hstack_vaapi, vstack_vaapi and xstack_vaapi filtersXMD ADPCM decoder and demuxerffmpeg CLI new option: -fix_sub_duration_heartbeatWavArc decoder and demuxerCrystalHD decoders deprecatedfiltergraph syntax in ffmpeg CLI now supports passing file contents as option valueshstack_qsv, vstack_qsv and xstack_qsv filters
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  July 22nd, 2022, FFmpeg 5.1 "Riemann"add ipfs/ipns protocol supportdialogue enhance audio filterdropped obsolete XvMC hwaccelDFPWM audio encoder/decoder and raw muxer/demuxerVizrt Binary Image encoder/decodercolorchart video source filterPGS subtitle frame merge bitstream filteradded chromakey_cuda filter
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 17th, 2022, FFmpeg 5.0 "Lorentz"FFmpeg 5.0 "Lorentz", a new
    major release, is now available! For this long-overdue release, a major effort
    underwent to remove the old encode/decode APIs and replace them with an
    N:M-based API, the entire libavresample library was removed, libswscale
    has a new, easier to use AVframe-based API, the Vulkan code was much improved,
    many new filters were added, including libplacebo integration, and finally,
    DoVi support was added, including tonemapping and remuxing. The default
    AAC encoder settings were also changed to improve quality.
    Some of the changelog highlights:
  ADPCM IMA Westwood encoderADPCM IMA Acorn Replay decoderArgonaut Games CVG demuxeraudio and video segment filtersApple Graphics (SMC) encoderhsvkey and hsvhold video filtersadecorrelate audio filterAV1 Low overhead bitstream format muxerhuesaturation video filtercolorspectrum source video filterRTP packetizer for uncompressed video (RFC 4175)VideoToolbox ProRes hwaccelaspectralstats audio filteradynamicsmooth audio filtervflip_vulkan, hflip_vulkan and flip_vulkan filtersadynamicequalizer audio filteryadif_videotoolbox filterVideoToolbox ProRes encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  
    We have a new IRC home at Libera Chat
    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at contact#IRCChannelsApril 8th, 2021, FFmpeg 4.4 "Rao"FFmpeg 4.4 "Rao", a new
    major release, is now available! Some of the highlights:
  AudioToolbox output deviceVDPAU accelerated HEVC 10/12bit decodingADPCM IMA Ubisoft APM encoderAV1 encoding support SVT-AV1ADPCM Argonaut Games encoderAV1 Low overhead bitstream format demuxerMobiClip FastAudio decoderAV1 decoder (Hardware acceleration used only)Argonaut Games BRP demuxerIPU decoder, parser and demuxerIntel QSV-accelerated AV1 decodingArgonaut Games Video decoderlibwavpack encoder removedAVS3 video decoder via libuavs3dVDPAU accelerated VP9 10/12bit decodingafreqshift and aphaseshift filtersHigh Voltage Software ADPCM encoderLEGO Racers ALP (.tun & .pcm) muxerDXVA2/D3D11VA hardware accelerated AV1 decodingMicrosoft Paint (MSP) version 2 decoderMicrosoft Paint (MSP) demuxerAV1 monochrome encoding support via libaom >= 2.0.1asuperpass and asuperstop filterDigital Pictures SGA demuxer and decodersTTML subtitle encoder and muxerRIST protocol via librist
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  June 15th, 2020, FFmpeg 4.3 "4:3"FFmpeg 4.3 "4:3", a new
    major release, is now available! Some of the highlights:
  Intel QSV-accelerated MJPEG decodingIntel QSV-accelerated VP9 decodingSupport for TrueHD in mp4Support AMD AMF encoder on Linux (via Vulkan)support Sipro ACELP.KELVIN decodingmaskedmin and maskedmax filtersQSV-accelerated VP9 encodingAV1 encoding support via librav1eAV1 frame merge bitstream filterMPEG-H 3D Audio support in mp4Argonaut Games ADPCM decoderArgonaut Games ASF demuxerafirsrc audio filter sourceSimon & Schuster Interactive ADPCM decoderHigh Voltage Software ADPCM decoderLEGO Racers ALP (.tun & .pcm) demuxerAMQP 0-9-1 protocol (RabbitMQ)avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filtersswitch from AvxSynth to AviSynth+ on LinuxExpanded styling support for 3GPP Timed Text Subtitles (movtext)Support for muxing pcm and pgs in m2tsCunning Developments ADPCM decoderPro Pinball Series Soundbank demuxerpcm_rechunk bitstream filtergradients source video filterMediaFoundation encoder wrapperSimon & Schuster Interactive ADPCM encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 5th, 2019, Bright Lights
  FFmpeg has added a realtime bright flash removal filter to libavfilter.
  
  Note that this filter is not FDA approved, nor are we medical professionals.
  Nor has this filter been tested with anyone who has photosensitive epilepsy.
  FFmpeg and its photosensitivity filter are not making any medical claims.
  
  That said, this is a new video filter that may help photosensitive people
  watch tv, play video games or even be used with a VR headset to block
  out epiletic triggers such as filtered sunlight when they are outside.
  Or you could use it against those annoying white flashes on your tv screen.
  The filter fails on some input, such as the
  Incredibles 2 Screen Slaver
  scene. It is not perfect. If you have other clips that you want this filter to
  work better on, please report them to us on our trac.
  
  We are not professionals. Please use this in your medical studies to
  advance epilepsy research. If you decide to use this in a medical
  setting, or make a hardware hdmi input output realtime tv filter,
  or find another use for this, please let me know.
  This filter was a feature request of mine
  since 2013.
  August 5th, 2019, FFmpeg 4.2 "Ada"FFmpeg 4.2 "Ada", a new
    major release, is now available! Some of the highlights:
  AV1 decoding support through libdav1dchromashift and rgbashift filterstruehd_core bitstream filterlibaribb24 based ARIB STD-B24 caption support (profiles A and C)Support decoding of HEVC 4:4:4 content in nvdec and cuviddecAV1 frame split bitstream filterSupport decoding of HEVC 4:4:4 content in vdpaushowspatial multimedia filtermov muxer writes tracks with unspecified language instead of English by defaultadded support for using clang to compile CUDA kernels
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  November 6th, 2018, FFmpeg 4.1 "al-Khwarizmi"aderivative and aintegral audio filterspal75bars and pal100bars video filter sourcesmbedTLS based TLS supportadeclick and adeclip filterslibtensorflow backend for DNN based filters like srcnnVC1 decoder is now bit-exactAVS2 video decoder via libdavs2Brooktree ProSumer video decoderMatchWare Screen Capture Codec decoderWinCam Motion Video decoderRemotelyAnywhere Screen Capture decoderSupport for AV1 in MP4 and Matroska/WebMAVS2 video encoder via libxavs2Block-Matching 3d (bm3d) denoising filteraudio denoiser as afftdn filterS12M timecode decoding in h264
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 20th, 2018, FFmpeg 4.0 "Wu"FFmpeg 4.0 "Wu", a new
    major release, is now available! Some of the highlights:
  Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streamsExperimental MagicYUV encoderIntel QSV-accelerated MJPEG encodingnative aptX and aptX HD encoder and decoderNVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decodingIntel QSV-accelerated overlay filterVAAPI MJPEG and VP8 decodingAMD AMF H.264 and HEVC encoderssupport LibreSSL (via libtls)Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.hilbert audio filter sourceRemoved the ffserver programRemoved the ffmenc and ffmdec muxer and demuxerVideoToolbox HEVC encoder and hwaccelVAAPI-accelerated ProcAmp (color balance), denoise and sharpness filterscodec2 en/decoding via libcodec2native SBC encoder and decoderhapqa_extract bitstream filterfilter_units bitstream filterAV1 Support through libaomE-AC-3 dependent frames supportbitstream filter for extracting E-AC-3 coreHaivision SRT protocol via libsrt
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 15th, 2017, FFmpeg 3.4 "Cantor"oscilloscope video filterupdate cuvid/nvenc headers to Video Codec SDK 8.0.14scale_cuda CUDA based video scale filterlibrsvg support for svg rasterizationspec compliant VP9 muxing support in MP4sofalizer filter switched to libmysofaGremlin Digital Video demuxer and decodersuperequalizer audio filteradditional frame format support for Interplay MVE moviessupport for decoding through D3D11VA in ffmpegDolby E decoder and SMPTE 337M demuxerunpremultiply video filterraw G.726 muxer and demuxer, left- and right-justifiedNewTek NDI input/output deviceVP9 tile threading supportV4L2 mem2mem HW assisted codecsRockchip MPP hardware decoding
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 13th, 2017, FFmpeg 3.3 "Hilbert"PSD (Photoshop Document) decoderFM Screen Capture decoderDNxHR decoder fixes for HQX and high resolution videosClearVideo decoder (partial)16.8 and 24.0 floating point PCM decoderIntel QSV-accelerated VP8 video decodingDNxHR 444 and HQX encodingQuality improvements for the (M)JPEG encoderVAAPI-accelerated MPEG-2 and VP8 encodingabitscope multimedia filterMPEG-7 Video Signature filteradd internal ebur128 library, remove external libebur128 dependencyIntel QSV video scaling and deinterlacing filtersSample Dump eXchange demuxerMIDI Sample Dump Standard demuxerScenarist Closed Captions demuxer and muxerSupport MOV with multiple sample description tablesPro-MPEG CoP #3-R2 FEC protocolSupport for spherical videosCrystalHD decoder moved to new decode APIconfigure now fails if autodetect-libraries are requested but not found
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 30th, 2016, Results: Summer Of Code 2016.
    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.
  
    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:
  FFv1 (Mentor: Michael Niedermayer)
    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.
  Self test coverage (Mentor: Michael Niedermayer)
    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.
  MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)
    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.
  Tee muxer improvements (Mentor: Marton Balint)
    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.
  TrueHD encoder (Mentor: Rostislav Pehlivanov)
    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.
  Motion interpolation filter (Mentor: Paul B Mahol)
    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.
  
    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!
  September 24th, 2016, SDL1 support dropped.
    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of
    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device
    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output
    devices have been updated to support SDL2.
  August 9th, 2016, FFmpeg 3.1.2 "Laplace"FFmpeg 3.1.2, a new point release from the 3.1 release branch, is now available!
    It fixes several bugs.
  
    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.
  July 10th, 2016, ffserver program being dropped
    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.
    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat
    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has
    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.
    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs
    and to contact us so we may point users to test and contribute to its development.
  July 1st, 2016, FFmpeg 3.1.1 "Laplace"FFmpeg 3.1.1, a new point release from the 3.1 release branch, is now available!
    It mainly deals with a few ABI issues introduced in the previous release.
  
    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to
    upgrade unless they use current git master.
  June 27th, 2016, FFmpeg 3.1 "Laplace"DXVA2-accelerated HEVC Main10 decodingloop video filter and aloop audio filterBob Weaver deinterlacing filterprotocol blacklisting APIVC-2 HQ RTP payload format (draft v1) depacketizer and packetizerVP9 RTP payload format (draft v2) packetizerAudioToolbox audio decodersAudioToolbox audio encoderscoreimage filter (GPU based image filtering on OSX)bitstream filter for extracting DTS corehash and framehash muxersVAAPI-accelerated format conversion and scalinglibnpp/CUDA-accelerated format conversion and scalingDuck TrueMotion 2.0 Real Time decoderWideband Single-bit Data (WSD) demuxerVAAPI-accelerated H.264/HEVC/MJPEG encodingDTS Express (LBR) decoderGeneric OpenMAX IL encoder with support for Raspberry PiIFF ANIM demuxer & decoderDirect Stream Transfer (DST) decoderOpenExr improvements (tile data and B44/B44A support)BitJazz SheerVideo decoderCUDA CUVID H264/HEVC decoder10-bit depth support in native utvideo decoderlibutvideo wrapper removedYUY2 Lossless Codec decoderVideoToolbox H.264 encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  March 16th, 2016, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code open source organization. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.
    Good luck!
  February 15th, 2016, FFmpeg 3.0 "Einstein"
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 30, 2016, Removing support for two external AAC encoders
    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and
    libaacplus in FFmpeg master.
  
    Even before marking our internal AAC encoder as
    stable, it was known that libvo-aacenc
    was of an inferior quality compared to our native one for most samples.
    However, the VisualOn encoder was used extensively by the Android Open
    Source Project, and we would like to have a tested-and-true stable option
    in our code base.
  
    When first committed in 2011, libaacplus filled in the gap of encoding
    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported
    by any of the encoders in FFmpeg at that time.
  
    The circumstances for both have changed. After the work spearheaded by
    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC
    encoder is ready to compete with much more mature encoders. The Fraunhofer
    FDK AAC Codec Library for Android was added in 2012 as the fourth
    supported external AAC encoder, and the one with the best quality and the
    most features supported, including HE-AAC and HE-AACv2.
  
    Therefore, we have decided that it is time to remove libvo-aacenc and
    libaacplus. If you are currently using libvo-aacenc, prepare to transition
    to the native encoder () when updating to the next version
    of FFmpeg. In most cases it is as simple as merely swapping the encoder
    name. If you are currently using libaacplus, start using FDK AAC
    () with an appropriate  option
    to select the exact AAC profile that fits your needs. In both cases, you
    will enjoy an audible quality improvement and as well as fewer licensing
    headaches.
  January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.
    Please see the changelog for each release for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  December 5th, 2015, The native FFmpeg AAC encoder is now stable!
    After seven years the native FFmpeg AAC encoder has had its experimental flag
    removed and declared as ready for general use. The encoder is transparent
    at 128kbps for most samples tested with artifacts only appearing in extreme
    cases. Subjective quality tests put the encoder to be of equal or greater
    quality than most of the other encoders available to the public.
  
    Licensing has always been an issue with encoding AAC audio as most of the
    encoders have had a license making FFmpeg unredistributable if compiled with
    support for them. The fact that there now exists a fully open and truly
    free AAC encoder integrated directly within the project means a lot to those
    who wish to use accepted and widespread standards.
  
    The majority of the work done to bring the encoder up to quality was started
    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.
    Both continued to work on the encoder with the latter joining as a developer
    and mainainer, working on other parts of the project as well. Also, thanks
    to Kamedo2 who does comparisons
    and tests, the original authors and all past and current contributors to the
    encoder. Users are suggested and encouraged to use the encoder and provide
    feedback or breakage reports through our bug tracker.
  
    A big thank you note goes to our newest supporters: MediaHub and Telepoint.
    Both companies have donated a dedicated server with free of charge internet
    connectivity. Here is a little bit about them in their own words:
  Telepoint is the biggest
        carrier-neutral data center in Bulgaria. Located in the heart of Sofia
        on a cross-road of many Bulgarian and International networks, the
        facility is a fully featured Tier 3 data center that provides flexible
        customer-oriented colocation solutions (ranging from a server to a
        private collocation hall) and a high level of security.
      
        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which
        uses FFmpeg heavily since it started operating a year ago. "Donating
        to help keep FFmpeg online is our way of giving back to the community"
        .
      
    Thanks Telepoint and MediaHub for their support!
  September 29th, 2015, GSoC 2015 results
    FFmpeg participated to the latest edition of
    the Google
    Summer of Code Project. FFmpeg got a total of 8 assigned
    projects, and 7 of them were successful.
  We want to thank Google, the
    participating students, and especially the mentors who joined this
    effort. We're looking forward to participating in the next GSoC
    edition!
  
    Below you can find a brief description of the final outcome of
    each single project.
  Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George
    Stephan Holljes's project for this session of Google Summer of Code was to
    implement basic HTTP server features for libavformat, to complement the
    already present HTTP client and RTMP and RTSP server code.
  
    The first part of the project was to make the HTTP code capable of accepting
    a single client; it was completed partly during the qualification period and
    partly during the first week of the summer. Thanks to this work, it is now
    possible to make a simple HTTP stream using the following commands:
      ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  
    The next part of the project was to extend the code to be able to accept
    several clients, simultaneously or consecutively. Since libavformat did not
    have an API for that kind of task, it was necessary to design one. This part
    was mostly completed before the midterm and applied shortly afterwards.
    Since the ffmpeg command-line tool is not ready to serve several clients,
    the test ground for that new API is an example program serving hard-coded
    content.
  
    The last and most ambitious part of the project was to update ffserver to
    make use of the new API. It would prove that the API is usable to implement
    real HTTP servers, and expose the points where more control was needed. By
    the end of the summer, a first working patch series was undergoing code
    review.
  Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek
    Mariusz finished an API prepared by the FFmpeg community and implemented
    Samba directory listing as qualification task.
  
    During the program he extended the API with the possibility to
    remove and rename files on remote servers. He completed the
    implementation of these features for file, Samba, SFTP, and FTP
    protocols.
  
    At the end of the program, Mariusz provided a sketch of an
    implementation for HTTP directory listening.
  Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack
    Mate was working on directshow input from digital video sources. He
    got working input from ATSC input sources, with specifiable tuner.
  
    The code has not been committed, but a patch of it was sent to the
    ffmpeg-devel mailing list for future use.
  
    The mentor plans on cleaning it up and committing it, at least for the
    ATSC side of things. Mate and the mentor are still working trying to
    finally figure out how to get DVB working.
  Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale
    Niklesh's project was to expand our support for 3GPP Timed Text
    subtitles. This is the native subtitle format for mp4 containers, and
    is interesting because it's usually the only subtitle format supported
    by the stock playback applications on iOS and Android devices.
  
    ffmpeg already had basic support for these subtitles which ignored all
    formatting information - it just provided basic plain-text support.
  
    Niklesh did work to add support on both the encode and decode side for
    text formatting capabilities, such as font size/colour and effects like
    bold/italics, highlighting, etc.
  
    The main challenge here is that Timed Text handles formatting in a very
    different way from most common subtitle formats. It uses a binary
    encoding (based on mp4 boxes, naturally) and stores information
    separately from the text itself. This requires additional work to track
    which parts of the text formatting applies to, and explicitly dealing
    with overlapping formatting (which other formats support but Timed
    Text does not) so it requires breaking the overlapping sections into
    separate non-overlapping ones with different formatting.
  
    Finally, Niklesh had to be careful about not trusting any size
    information in the subtitles - and that's no joke: the now infamous
    Android stagefright bug was in code for parsing Timed Text subtitles.
  
    All of Niklesh's work is committed and was released in ffmpeg 2.8.
  libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla
    Pedro Arthur has modularized the vertical and horizontal scalers.
    To do this he designed and implemented a generic filter framework
    and moved the existing scaler code into it. These changes now allow
    easily adding removing, splitting or merging processing steps.
    The implementation was benchmarked and several alternatives were
    tried to avoid speed loss.
  
    He also added gamma corrected scaling support.
    An example to use gamma corrected scaling would be:
      ffmpeg -i input -vf scale=512:384:gamma=1 output
  
    Pedro has done impressive work considering the short time available,
    and he is a FFmpeg committer now. He continues to contribute to
    FFmpeg, and has fixed some bugs in libswscale after GSoC has
    ended.
  AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire
    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main
    prediction on the native AAC encoder. Of all those extensions, only
    TNS was left in a less-than-usable state, but the implementation has
    been pushed (disabled) anyway since it's a good basis for further
    improvements.
  
    PNS replaces noisy bands with a single scalefactor representing the
    energy of that band, gaining in coding efficiency considerably, and
    the quality improvements on low bitrates are impressive for such a
    simple feature.
  
    TNS still needs some polishing, but has the potential to reduce coding
    artifacts by applying noise shaping in the temporal domain (something
    that is a source of annoying, notable distortion on low-entropy
    bands).
  
    Intensity Stereo coding (I/S) can double coding efficiency by
    exploiting strong correlation between stereo channels, most effective
    on pop-style tracks that employ panned mixing. The technique is not as
    effective on classic X-Y recordings though.
  
    Finally, main prediction improves coding efficiency by exploiting
    correlation among successive frames. While the gains have not been
    huge at this point, Rostislav has remained active even after the GSoC,
    and is polishing both TNS and main prediction, as well as looking for
    further improvements to make.
  
    In the process, the MIPS port of the encoder was broken a few times,
    something he's also working to fix.
  Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol
    Donny Yang implemented basic keyframe only APNG encoder as the
    qualification task. Later he wrote interframe compression via
    various blend modes. The current implementation tries all blend
    modes and picks one which takes the smallest amount of memory.
  
    Special care was taken to make sure that the decoder plays
    correctly all files found in the wild and that the encoder
    produces files that can be played in browsers that support APNG.
  
    During his work he was tasked to fix any encountered bug in the
    decoder due to the fact that it doesn't match APNG
    specifications. Thanks to this work, a long standing bug in the
    PNG decoder has been fixed.
  
    For latter work he plans to continue working on the encoder,
    making it possible to select which blend modes will be used in the
    encoding process. This could speed up encoding of APNG files.
  September 9th, 2015, FFmpeg 2.8
    We published release  as new major version.
    It contains all features and bug fixes of the git master branch from September 8th. Please see
    the 
    for a list of the most important changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use current git master.
  August 1st, 2015, A message from the FFmpeg project
    Dear multimedia community,
  
    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has
    come by surprise. He has worked tirelessly on the FFmpeg project for many
    years and we must thank him for the work that he has done. We hope that in
    the future he will continue to contribute to the project. In the coming
    weeks, the FFmpeg project will be managed by the active contributors.
  
    The last four years have not been easy for our multimedia community - both
    contributors and users. We should now look to the future, try to find
    solutions to these issues, and to have reconciliation between the forks,
    which have split the community for so long.
  
    Unfortunately, much of the disagreement has taken place in inappropriate
    venues so far, which has made finding common ground and solutions
    difficult. We aim to discuss this in our communities online over the coming
    weeks, and in person at the VideoLAN Developer
    Days in Paris in September: a neutral venue for the entire open source
    multimedia community.
  July 4th, 2015, FFmpeg needs a new host We have received more than 7 offers for hosting and servers, thanks a lot to everyone!
    After graciously hosting our projects (FFmpeg, MPlayer
    and rtmpdump) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.
  
    If you want to host an open source project, please let us know, either on ffmpeg-devel
    mailing list or irc.freenode.net #ffmpeg-devel.
  
    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, trac, samples repo, svn, etc.
  March 16, 2015, FFmpeg 2.6.1
    We have made a new major release ()
    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  March 4, 2015, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code Project. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks. Registration
    at Google for students will open March 16th. Good luck!
  March 1, 2015, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage
    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.
  
    More information can be found here
    We demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes. If you have media files that cannot be
    processed correctly with FFmpeg, be sure to have a sample with you
    so we can have a look!
    For the first time in our CLT history, there will be an !
    You can read the details here.
    The workshop is targeted at FFmpeg beginners. First the basics of
    multimedia will be covered. Thereafter you will learn how to use
    that knowledge and the FFmpeg CLI tools to analyse and process media
    files. The workshop is in German language only and prior registration
    is necessary. The workshop will be on Saturday starting at 10 o'clock.
  
    We are looking forward to meet you (again)!
  December 5, 2014, FFmpeg 2.5
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 4th December.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  October 10, 2014, FFmpeg is in Debian unstable again
    We wanted you to know there are
    
    FFmpeg packages in Debian unstable again. A big thank-you
    to Andreas Cadhalpun and all the people that made it possible. It has been anything but simple.
  
    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't
    migrate to Debian testing to be in the upcoming release codenamed jessie.
    Read the argumentation over at Debian.However things will come out in the end, we hope for your continued remarkable support!October 8, 2014, FFmpeg secured a place in OPW!
    Thanks to a generous 6K USD donation by Samsung (Open Source Group),
    FFmpeg will be welcoming at least 1 "Outreach Program for Women" intern
    to work with our community for an initial period starting December 2014
    (through March 2015).
  
    We all know FFmpeg is used by the industry, but even while there are
    countless products building on our code, it is not at all common for
    companies to step up and help us out when needed. So a big thank-you
    to Samsung and the OPW program committee!
  
    If you are thinking on participating in OPW as an intern, please take
    a look at our OPW wiki page
    for some initial guidelines. The page is still a work in progress, but
    there should be enough information there to get you started. If you, on
    the other hand, are thinking on sponsoring work on FFmpeg through the
    OPW program, please get in touch with us at opw@ffmpeg.org. With your
    help, we might be able to secure some extra intern spots for this round!
  September 15, 2014, FFmpeg 2.4
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 14th September.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.
    Please see the changelog for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 29, 2014, Help us out securing our spot in OPW
    Following our previous post regarding our participation on this year's
    OPW (Outreach Program for Women), we are now reaching out to our users
    (both individuals and companies) to help us gather the needed money to
    secure our spot in the program.
    We need to put together 6K USD as a minimum but securing more funds would
    help us towards getting more than one intern.
    You can donate by credit card using
    
    Click&Pledge and selecting the "OPW" option. If you would like to
    donate by money transfer or by check, please get in touch by
    e-mail and we will get back to you
    with instructions.Thanks!
  July 20, 2014, New website
    The FFmpeg project is proud to announce a brand new version of the website
    made by db0. While this was initially motivated
    by the need for a larger menu, the whole website ended up being redesigned,
    and most pages got reworked to ease navigation. We hope you'll enjoy
    browsing it.
  July 17, 2014, FFmpeg 2.3
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 16th July.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 3, 2014, FFmpeg and the Outreach Program For Women
    FFmpeg has started the process to become an OPW includer organization for the
    next round of the program, with internships starting December 9. The
    OPW aims to "Help women (cis and trans)
    and genderqueer to get involved in free and open source software". Part of the
    process requires securing funds to support at least one internship (6K USD), so
    if you were holding on your donation to FFmpeg, this is a great chance for you
    to come forward, get in touch and help both the project and a great initiative!
  
    We have set up an email address you can use
    to contact us about donations and general inquires regarding our participation
    in the program. Hope to hear from you soon!
  June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  
    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will
    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a
    different location closer to the city center.
  
    We will have a shared booth with XBMC and VideoLAN.
    
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    More information about LinuxTag can be found here
    We are looking forward to see you in Berlin!
  April 18, 2014, OpenSSL Heartbeat bug
    Our server hosting the Trac issue tracker was vulnerable to the attack
    against OpenSSL known as "heartbleed". The OpenSSL software library
    was updated on 7th of April, shortly after the vulnerability was publicly
    disclosed. We have changed the private keys (and certificates) for all
    FFmpeg servers. The details were sent to the mailing lists by
    Alexander Strasser, who is part of the project server team. Here is a
    link to the user mailing list
    archive
    .
  
    We encourage you to read up on
    "OpenSSL heartbleed".
    It is possible that login data for the issue tracker was exposed to
      people exploiting this security hole. You might want to change your password
      in the tracker and everywhere else you used that same password.April 11, 2014, FFmpeg 2.2.1
    We have made a new point releases ().
    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as
    several other fixes.
    See the git log for details.
  March 24, 2014, FFmpeg 2.2
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 1st March.
    A partial list of new stuff is below:
      - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side & metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI "junk" block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  February 3, 2014, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'
    in Chemnitz, Germany. The event will take place on 15th and 16th of March.
  
    More information can be found here
    We invite you to visit us at our booth located in the Linux-Live area!
    There we will demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes.
  
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    We are looking forward to meet you (again)!
  February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach
    The server on which FFmpeg and MPlayer Trac issue trackers were
    installed was compromised. The affected server was taken offline
    and has been replaced and all software reinstalled.
    FFmpeg Git, releases, FATE, web and mailinglists are on other servers
    and were not affected. We believe that the original compromise happened
    to a server, unrelated to FFmpeg and MPlayer, several months ago.
    That server was used as a source to clone the VM that we recently moved
    Trac to. It is not known if anyone used the backdoor that was found.
  
    We recommend all users to change their passwords.
    Especially users who use a password on Trac that they also use
      elsewhere, should change that password at least elsewhere.November 12, 2013, FFmpeg RFP in Debian
    Since the splitting of Libav the Debian/Ubuntu maintainers have followed
    the Libav fork. Many people have requested the packaging of ffmpeg in
    Debian, as it is more feature-complete and in many cases less buggy.
  Rogério Brito, a Debian developer,
    has proposed a Request For Package (RFP) in the Debian bug tracking
    system.
  
    Please let the Debian and Ubuntu developers know that you support packaging
    of the real FFmpeg! See Debian ticket #729203
    for more details.
  October 28, 2013, FFmpeg 2.1
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 28th October.
    A partial list of new stuff is below:
      - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  ]]></content:encoded></item><item><title>Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</title><link>https://arxiv.org/abs/2508.12631</link><author>omarsar</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:43:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A guide to Gen AI / LLM vibecoding for expert programmers</title><link>https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</link><author>ChrisRackauckas</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:37:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I get it, you’re too good to vibe code. You’re a senior developer who has been doing this for 20 years and knows the system like the back of your hand. Or maybe you’re the star individual contributor who is the only person who can ever figure out how to solve the hard problems. Or maybe you’re the professor who created the entire subject of the algorithms you’re implementing. I don’t know you, but I do know that you think you’re too good to vibe code. And guess what, you’re absolutely and totally wrong.Facetious? Maybe… but I will go even further.No, you’re not too good to vibe code. In fact, you’re the only person who should be vibe coding.But I started picking up this “vibe coding” about a month ago and I found out that it can be a really powerful tool, in the right circumstances and in the right workflow. For the record, I now have about 32 Claude agents continuously running in tmux windows that I can ssh to, so all day long I can just check via laptop or phone and keep plugging along. This was completely unheard of a month ago, but it’s here.This is the expert’s guide to vibe coding for those who are scoffing at those kids who don’t know what they are doing, but also want to start doing it correctly.A Mental Model for LLM Agents: Your Sophomore Year Student/InternDrop the hype, I’m not here to sell you a ChatGPT so I’m not going to tell you it’s PhD level when it 100% absolutely clearly isn’t to anyone who has ever met a PhD in their life. But it is something, what is it?Think about an LLM agent as a dedicated intern, or a student who is around the proficiency of a sophomore in college. They know the basics of what programming looks like, they can copy other ideas and architectures, they know how to do things like run unit tests, and they know how to Google things. They have had their basic programming course, and probably have done a deep dive into some random subject as a higher level course, but if you quiz them on the topic enough you’ll learn they haven’t actually learned it deeply. The kid seems smart enough, you’d give them a shot.If this was a person who showed up to your office looking for work, what would you do with them? Generally you would do one of two things. First, you could sandbox their work. Now the reason you sandbox the work of a new student or intern is rather simple: it’s because you don’t know a new subject/tool well, you want to give it a try, and ehh why not let’s see what happens. If you took the sandbox route, you probably aren’t caring about the code (it’s likely to be unmaintainable and bit rot anyways if it’s not in your core repos), it’s about getting the artifact. You vibe code a bit, “that looks cool!”, throw it into a demo / LinkedIn post, and then move on. That’s the simple vibe coding you may have already tried and thought “that’s not useful enough”. It works, but that’s not what we’re here for, so no need to mention that more in the blog.The second path, in the complete opposite direction, you would integrate the student/intern into a project you know well because it makes it very easy for you to review their work: you can give clear feedback because you’ve already made the first 10 mistakes they will make, you already know how to tell them what is next, and you have the first 6 months of the project planned out for them so it’s low maintenance. This is how you would train most people that you want to stay long term, right? In the same way, this is the path to take with the LLM agents.So let’s walk through this process step by step.Major Point: Vibe coding turns everyone into a team lead, but not everyone should be a team leadLeading a team of programmers is hard! It takes time, skill, and patience. I think everyone when they are a kid thinks “I don’t want to be the worker, I want to be the boss and I just sit in my chair and tell people what to do and boom it all gets done!”. But after your second group project in college, you pretty quickly realize that if you lead a team wrong, you instead just end up doing all of the work yourself while having the expectation of 4 people. Now there’s a few reasons for this. One issue with trying to establish team programming is that you may just not know the subject well enough. If it takes you a while to understand the subject, what someone is trying to do, and what their code is about, then it’s just not worth the time to manage someone else. You need to be at a point where you can very quickly see the code, understand what’s going on, and say “I can’t merge this until you have tests for X and Y, and also show me a plot of Z so I know how it all relates”. If you cannot instantly see that kind of feedback, then you probably aren’t experienced enough to lead. You need to write a few million lines of code before it becomes automatic where you just look at code and say “don’t do that, that’ll be a performance bottleneck”, but you need that quickness to the code review in order for vibe coding to work. But also, let me say this bluntly:If you are an individual contributor who usually does not like to train interns because you find that they take more time than they are helpful, then don’t vibe codeSome people tend to do better at working by directing. Other really smart people just can’t seem to get good work out of other people. It’s not an indictment, Silicon Valley created the “individual contributor” role for a reason. If you are one of those people, then vibe coding may not be for you as you will likely grow frustrated with the agents even quicker than you would a human (they somehow retain less information than even the worst intern, at least they remember your favorite lunch order).So go in with this mindset: I will have to have meetings with the agents, I will need to plan and give them tasks, and I will need to review the code. If I find this stuff to be slower than coding myself, then just stop right now. But if you do well with a team, then go on. How do we now make this team effective?What is the workflow of vibe coding correctly?If someone shows you Claude Code and you ask it to try and solve the problem that you were just working on (obviously a hard problem, because if it ends up on your desk that means someone else failed to solve it), you just poke and laugh at Claude when it fails miserably. But you would have never done that with a new intern or student (hopefully), so why do this? Again, you know how smart it actually is, cut the hype, and treat it the same way. This immediately leads to a few workflow principles:The workflow of vibe coding is the same as managing scrums or helping a student through a research thesisYou have a problem, you give it to the agent, you review the results, and then you give it feedback. This is exactly how you would manage a scrum team or help a student through their thesis. You don’t just give them the problem and expect them to solve it, you give them the problem, they come back with a solution, and then you tell them what to do next.You probably already get most of your work done this way if you’re senior enough. Every professor has more students coding than themselves, and every senior developer has a total amount of code created by their team that is far greater than their own. Just think of Claude as your pack of newbies that just started. Now if you’re thinking “but it can be difficult to manage a bunch of newbies”… yeah, that means you’re actually senior enough to understand how to do this right. It’s fairly easy to send a new intern or student off on a project, and if their pay/grade depends on it getting done they will give you something back. Whether it’s any good depends on how well you chunked up the work for them and gave them an appropriate task. But one key thing is, if you had to do a meeting every 10 minutes it would drive you crazy, so don’t. Set up say 12-32 agents running on different processes, preferably sandboxed on some other compute resource (sandboxed so they can’t break the machine, but also so they can have a Github authentication that does not have core read/write privileges. This way you can tell it to have “dangerously unsafe permissions” and the worst that happens is it segfaults its own docker container and never opens a PR). Give it a full command:“try solving (an easy issue in this open source repository). Create a PR with the solution, and after an hour check the continuous integration to see if tests are passing. If tests are not passing, assess what the issue is, and if it is a quick fix make a commit to handle it, otherwise report what the core difficulty of the problem is”Don’t spend too much time setting up the calls, just pull from lists you already have and let it find “whatever is easy”Make it clear, make it easy, make it know the steps, and let it just keep cycling for a bit.How to review vibe coding: immediately throw out anything badIf you saw a student was cheating and just copy-pasted from StackOverflow but couldn’t explain what it did, you’d throw it out and tell them to try again. If your new intern didn’t reuse all of the solid code your team had written and instead rewrote some low level detail in a buggy and unmaintainable way, you’d throw it out and tell them to try again. If they wrote a function that was 500 lines long and did 10 different things, you’d throw it out and tell them to try again. You wouldn’t waste your time trying to fix it, you’d just tell them to try again.Again, treat the LLMs the same way. I see a lot of people following the mindset they see the vibe coding YouTubers making their silly games. “ChatGPT, try harder! Fix for me!”. You want to know a secret? That stuff is worse than worthless. The problem is that these LLMs are made to please you, so if you tell them to try harder, they will either start hallucinating or just start changing your tests. Don’t even give it a try. The moment you see it go off the rails, just throw it out. That problem is too hard for Claude, it’s for you now.Send a bunch of commands at 9am. At noon, check on them. You might have 10 done. 8 of them probably went off the rails, whatever, fire them. Hey two PRs worked, whoopee! Fire 10 more, come back at 3. 20 done, 4 successes and 16 failures. Fire a few more off, maybe a few clean up ones to look for missing docstrings or dig around to see if any performance regressions were introduced. At 6, see the other 4 successes and cut the other jobs. Vibe coding is useful only if you have enough problems that you’re happy that some subset being solved, not caring what in that subset is solved.10 PRs were merged, plus whatever you were working on that day (yes, because you didn’t focus on this for most of your day!). You might think, that’s like 10/40 = 25% success rate, that’s not good. But you know what? Those were free. You just got a lot of extra stuff done that you wouldn’t have otherwise. The success rate is just a matter of how much these things give value for their cost. That’s for Sam Altman to worry about. But if you have a subscription to these LLMs, just keep burning through the tokens who cares. Don’t worry about success rate, just go for total successes.Where to apply vibe coding: code you know very wellSo this leads to a very counter-intuitive fact that may come out of left field, but I’m serious. Everyone’s first inclination is to throw it on some project they haven’t actually contributed to and get banned (okay, maybe it just looks like that to open source maintainers). But the real issue is that, the majority of your time will be spent doing code review. If you do this on code you don’t know well, you will have to spend a lot of time trying to understand the code and at that point, why not just write the code yourself?This is where most people seem to just stop and drop the idea of vibe coding all together. But instead… what about applying it to the code base you’re on? No, not on the hard problems you’re thinking about, but all of those little side problems? The small refactor you put off for the last 6 months? What about bisecting the Git commits to find the exact cause of the performance regression that showed up on master a week ago? Or you created a version specialized for Windows and Mac but left a “todo” over the Linux section because it’s easy but would be 4 hours of monotonous work? All of those things, if someone showed up with the code, you could review it in about 5 minutes and know whether it’s right or wrong. Give the agents that stuff!Vibe coding is not useful if you need it to solve a particular problem. You still do the hard stuffIn just the same way, the best place to put trainees is in the project that you already know well because that makes it easy to review their work. It’s the “I don’t have time for you, so try this easy task” approach. You know the code, you know the problem, and you can give them a task that is easy enough that they can do it without too much help. This is the same principle here.Some Examples of Vibe Coding PRsExample 1: The Simple Success StoryHere’s a quick and simple PR, the kind that is perfect here. If you don’t know performance Julia handling or trim, basically it’s a new feature in Julia v1.12 where Julia can now build small lean binaries. In order to do that, you need to make sure functions fully specialize, which they don’t by default as that would create a lot of extra compilation in many circumstances, but for higher order numerical solvers that is the behavior we want. So I told it to go specialize all instances of the function in the package, and I could check the PR fairly quickly and see it stuck to the goals and did it. This is then going to be followed up with new tooling that will perform static checks of trimming compatibility (still being worked out), but with just those backwards compatible minor changes things seem to work in the beta, so merge now and add those tests when we have a good system for it.1 minute to write the query, come back later and 1 minute to review.80% of coding is grunt work. Simple stuff. Throw it at the LLM and smile if it solves it, or just groan and finish it if it doesn’tThis is exactly the kind of small targeted change these are geared towards. Most of the PRs aim to be like this. Actually, one of my favorite prompts is “Look through the XXXXXXX repository, find the easiest issue, and make a PR that solves it.” Here’s an example of a PR from that. Yay it was a hit! Merged. It should make a minimal number of changes, be small and easy to review, otherwise just dump it, that means it probably went off the rails. There’s just a lot of these little “this API would be easier if you added a conversion from float to Int” (which sometimes you go “oh, no that’s a bad idea, close PR and issue!). Even if you work on hard stuff, a huge chunk of your work isn’t hard stuff. There’s a lot of simple janitorial work you have to do on your code all of the time. Automate that part.Example 2: The Immediately Closed “That’s not for Claude” PRThis PR came from pointing it at the fact that every once in awhile I get a test failure in the docs build for a chaotic ODE differentiation w.r.t. ergodic properties tutorial. It is a very fun topic, but generally anything with real math in it is too hard for the LLMs. And in this, yeah I could see immediately that this PR does not make sense… well it did. The NaN’s and Infs were definitely coming from a numerical issue in the least squares shadowing code, and what this pointed to was the Schur complement was being done with things like B * Diagonal(wBinv) * B’ which as a numerical analyst I can immediately see would double the condition number of the matrix, but there doesn’t seem to be an immediate solution with open source linear algebra things I could find. So closed this, sent a note over to Alan Edelman to try and figure out what the better way to do this factorization. While it didn’t solve the problem, at least I know what the problem is now.This is probably what most of the PRs become. It gives a hint of where the problem is, and then I take the reins.Example 3: Repeated RefactorsIs a sweet and simple PR that refactors the tests to move some things, specifically the Enzyme automatic differentiation engine testing, to a “no pre” set. The “no pre” means “does not run on prereleases of the next language version”, since these tools touch language internals in the compiler so they are never ready early. This always make prerelease tests fail before they actually test anything meaningful, so I wanted to move all Enzyme usage to a “no pre” set in every repo it showed up. About 5 minutes to write the query. Some of the test suites needed a simple Github suggestion to fix up a little detail here or there. About 5 minutes to get this thing into 8 repos. Now I was ready to start using prerelease tests. Would’ve been at least a half hour by hand just because we didn’t have an easy system for doing this before. Maybe that’s a little dirtier than the perfect regex, but whatever 10 minutes of my time sounds like a win.Refactors generally work out really well and are one of the top uses for the tool. “make it correct, write good tests, and let it refactor” is generally a lazy way to get 90% of the way there.Example 4: The Information Gathering PRHere is a pull request that was generated by pointing it to solve this issue. That issue was mostly chosen because it was sitting on the issue list for awhile and it didn’t seem so difficult but I hadn’t had the time to track down the memory leak in a not so widely used extension for an alternative C-based sparse matrix solver, but it needed to get done some time. So, throw the bot on it.And what it comes back with is to add a memory finalizer (i.e. how to tell the GC how to remove the memory) for the other library. I could take one look at it and immediately see that kind of code should not live in this library, it should live in the library where the solver is bound to the language, and the fact that it was missing a finalizer is something that should be solved over there. Close the PR, throw out the code, find the  stalled discussion on the repo that should have the finalizer, poke the author a bit, and it’s in. Done, someone just needed to be reminded.Total time on my end was about 3 minutes. The bot could have also written that fix but it basically already existed so no need, this was more about finding out where in the system something was offer.Example 5: The “How Long is that Going to Take?” PRHere is a nice PR where it didn’t finish (at least at the time of writing this) and the reason is because there are lots of other clean ups that need to happen for this to ever work. How far away is it? Well it generated a set of tests that cleanly listed out all 120 things to solve. Great, this is probably a full week’s task… I knew it would be a lot but that is now pretty concrete. I probably won’t use the bot to finish this one, but now if someone asked what the effort would be I can give them a pretty clear estimate because it has been reduced from “someone needs to give it a try, seems like a good chunk of work” to “the hard part is making these 120 things happen, which is easy but tedious and it would take about a week, probably not worth the effort right now”. That’s very useful when planning ahead. Total me time was about 5 minutes, plus the PR discussion time to explain to others what the results meant.Conclusion: Vibe Coding Done Right is actually an Expert’s TaskVibe coding turns any individual into the CTO leading a team of 20, 30, 50, 60 interns. You immediately get a massive team. It takes time and experience to actually handle a group like this correctly and to make it be productive. Making all of 60 interns not break the performance, correctness, or maintainability of your code is very difficult. So I do not recommend this to people who are still “up and coming programmers”. But if you’re a bit more senior and starting to grow your group, well this is then a cheap way to accelerate that. What that means is, vibe coding is sold for people who don’t know how to program, but if you actually think about it, the main audience that can actually use it correctly is experts.A few side remarks I didn’t get toA bunch more examples and how bad Claude is at mathIn the Julia Discourse forum, I detail a bunch of different PRs to show where the AIs tend to succeed and fail. But generally anything that is mostly about “programming” (refactoring, inefficient implementations, etc.) the LLMs do well. Anything about the domain or application (differential equations, engineering, physics for me) it just seems to flat out do something dumb. The results all lined up very clearly shows what kind of PRs you should be asking it to make and which ones just aren’t worth the effort.The role of empathy in vibe coding successSome of the least empathetic people I know in open source are the ones who are also the most skeptical of vibe coding. I have a heavy speculation that they speak to the agents similarly to how they speak to other potential contributors, and drive the bots away the same way they do to people. But with a bot, it will always try to make you happy, just by hallucinating and commenting out your tests. These same people also don’t want the bots around because they claim that’s all the bots ever do. Weird coincidence. I wonder what this will do to the culture of programming over time.The cost may not make sense in the long run, but it does while the VCs are paying for itOn my \$200/month Claude 20x Max subscription I used enough tokens for about $5,200 of compute in the first month. This is obviously not sustainable, but hey, it’s a startup world and VCs are paying for it right now. If you can get a few extra features done that get you more funding, then this is worth it. If you’re a professor and you can get a few more papers out, then this is worth it. If you’re an individual contributor at a big company and you can get a few more features out that make your team look good, then this is worth it. Will it be worth it after the money runs out? Who knows, but mine while the gold is there.What’s the right setup? Easy, Claude CodeThe tab-complete stuff is pretty annoying. The power comes from running agents. Claude Code has a simple setup and is able to start running code. Just write a decent Claude.md that tells it to stop being so nice and instead just tell me when it cannot solve the problem, and you’re good to go. The context7 MCP is good, Sequential Thinking as well. “This sounds like hell?”: A response to seeing what vibe coding is likeHey Hacker News! Looks like it got there. One of the interesting comments is “To me, someone who actually love programming, it makes vibe coding look like hell.” I 100% agree! I stayed away for a bit because I was like “ugh, that’s not the fun part, I like the coding!”. But, notice from the examples that the amount of time I’m spending on this I try to keep as minimal as possible. I like to program, and I need to be programming because I can do the hard stuff while the LLM can’t. The goal is to get as much easy stuff done with as minimal work on your end as possible, so you can stop worrying about the annoying/boring stuff and can focus more time on the interesting work.“””
I choose the tech stack and architect the project.
I choose the language patterns and code organization.
I step in to solve hard problems when agents flounder.
What about that says middle management? It’s just getting rid of all the low iq parts of the job.
“””But then again, if you also just really dislike having to do meetings at all and prefer to just be coding alone, then… yeah maybe the agents will probably drive you mad. They are loud, lie to you, and you have to sift through emails/PRs of bullshit and just delete most of what these things come up with. Again, be quick to delete when these things go off the rails, it will save your sanity. All that said, with the amount of code coming from LLMs I think there will be an even greater need for good individual contributors to yell at the clouds and maintain the integrity of the codebase so, you’ll have your place. You just might have a larger load of PRs coming your way and may want to be quicker to shut a few down. I don’t think it will remove those jobs (though it may remove the intern jobs to build up the resume to get there… which is a societal issue that I’ll not touch on here… just like I’m not talking about the energy issue which is also a major potential flaw with LLM workflows)]]></content:encoded></item><item><title>Thunderbird Pro August 2025 Update</title><link>https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</link><author>mnmalst</author><category>hn</category><pubDate>Fri, 22 Aug 2025 14:29:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>All managers make mistakes; good managers acknowledge and repair</title><link>https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</link><author>matheusml</author><category>hn</category><pubDate>Fri, 22 Aug 2025 12:50:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[“There is a crack in everything. That’s how the light gets in.” — Leonard CohenLet me tell you something that will happen after you become a manager: you’re going to mess up. A lot. You’ll give feedback that lands wrong and crushes someone’s confidence. You’ll make a decision that seems logical but turns out to be completely misguided. You’ll forget that important thing you promised to do for someone on your team. You’ll lose your temper in a meeting when you should have stayed calm.The real question isn’t whether you’ll make mistakes; it’s what you do .I recently read  by Dr. Becky Kennedy, a parenting book that completely changed how I think about this. She talks about how the most important parenting skill isn’t being perfect — it’s . When you inevitably lose your patience with your kid or handle something poorly, what matters most is going back and fixing it. Acknowledging what happened, taking responsibility, and reconnecting.Sound familiar? Because that’s what good management is about too.Think about the worst manager you ever had. I bet they weren’t necessarily the ones who made the most mistakes. But they were probably the ones who never acknowledged them. Who doubled down when they were wrong. Who let their ego prevent them from admitting they didn’t have all the answers.Here’s a pattern I see play out constantly: A manager commits to something without consulting the team. Maybe it’s a feature at a client demo, a timeline in a board meeting, or just a “small favor” for another department. The team scrambles to deliver, working nights and weekends. They make it happen, but barely, and with real costs: technical debt, burned-out engineers, resentment building.What happens next determines everything. The manager who never acknowledges what they put the team through? That’s how you lose your best people. But the manager who comes back and says, “I put you in an impossible position. I should have consulted you first. I’m sorry for the stress that caused, and here’s how I’ll handle it differently next time”, that manager builds trust even through the mistake.I’ve been on both sides of this. As an engineer, I watched managers make the same mistakes over and over again, never acknowledging the chaos they created. As a manager, I’ve been the one creating that chaos 🥲. The difference in outcomes is massive; when you own your mistakes completely and specifically, something unexpected happens: your team trusts you more, not less.Here’s what repair looks like in practice:Be specific about what you did wrong. Not “mistakes were made” or “things could have gone better.” But “I interrupted you three times in that meeting and dismissed your concerns. That was wrong.” This isn’t the time for a long explanation of your stress levels or why you acted that way. Save that for your therapist or your own manager. The repair is about acknowledging the impact on the other person.Actually change the behavior. An apology without changed behavior is just empty words. If you keep making the same “mistake,” it’s not a mistake anymore; it’s a choice. One conversation doesn’t instantly repair broken trust. It’s a starting point, not a finish line. You have to consistently show up differently.The beautiful thing about getting comfortable with repair is that it actually makes you better as a manager. When you know you can fix things when they go wrong, you’re more willing to make decisions, have difficult conversations, and take reasonable risks. You stop being paralyzed by perfectionism because you know that most mistakes, while serious, create opportunities for growth and stronger relationships when handled well.This doesn’t mean being reckless or careless. It doesn’t mean making the same mistakes repeatedly. And it definitely doesn’t mean using repair as a get-out-of-jail-free card for being a shitty manager.What it means is accepting that you’re human, that management is complex, and that you won’t always get it right. Your job isn’t to be perfect. Your job is to ship working software that adds real value to users, to help your team grow, and to create an environment where people can do their best work. Sometimes you’ll fail at those things. When you do, you repair, you learn, and you keep going.]]></content:encoded></item><item><title>What about using rel=&quot;share-url&quot; to expose sharing intents?</title><link>https://shkspr.mobi/blog/2025/08/what-about-using-relshare-url-to-expose-sharing-intents/</link><author>edent</author><category>hn</category><pubDate>Fri, 22 Aug 2025 11:49:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Let's say that you've visited a website and want to share it with your friends.  At the bottom of the article is a list of popular sharing destinations - Facebook, BlueSky, LinkedIn, Telegram, Reddit, HackerNews etc.You click the relevant icon and get taken to the site with the sharing details pre-filled.The problem is, every different site has a different intent for sharing links and text.  For example:https://www.facebook.com/sharer.php?u=…&t=…https://www.linkedin.com/sharing/share-offsite/?url=…https://bsky.app/intent/compose?text=…https://www.threads.net/intent/post?url=…&text=…https://www.reddit.com/submit?url=…&title=…As you can see, some only allow a URL, some text and a URL, and some just a plain text which could contain the URl. A bit of a mess! It's probably impossible to get every site to agree on a standard for their sharing intent. But there  be a standard for exposing their existing sharing mechanism.ShareOpenly knows about most major social networks, as well as decentralized platforms like Mastodon, Bluesky, and Known.However, if ShareOpenly is having trouble sharing to your platform, and if your platform supports a share intent, you can add the following metatag to your page headers:<link rel="share-url" href="https://your-site/share/intent?text={text}">Where https://your-site/share/intent?text= is the URL of your share intent.The special keyword  will be replaced with the URL and share text.I think that's a pretty nifty solution.For sites which take a URl and an (optional) title, the meta element looks like:For those which only take URl, it looks like:It's slightly trickier for sites like Mastodon and BlueSky which only have a text sharing field and no separate URl.  The current proposal is just to use the text. For exampleBut it could be something likeAdding to that page merely requires a formal specification to be written up. After that, some light lobbying might be needed to get social networks to adopt it.So, I have three questions for you:Do you think 
							
							 is a good idea for a new standard?
						What changes, if any, would you make to the above proposal?Would you be interested in using it - either as a sharer or sharing destination?Please leave a comment in the box - and remember to hit those sharing buttons!]]></content:encoded></item><item><title>4chan will refuse to pay daily online safety fines, lawyer tells BBC</title><link>https://www.bbc.co.uk/news/articles/cq68j5g2nr1o</link><author>donpott</author><category>hn</category><pubDate>Fri, 22 Aug 2025 10:02:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Some American politicians - particularly the Trump administration, its allies and officials - have pushed back against what they regard as overreach in the regulation of US tech firms by the UK and EU. A perceived impact of the Online Safety Act on free speech has been a particular concern, but other laws have also been the source of disagreement.On 19 August, US Director of National Intelligence Tulsi Gabbard said the UK had withdrawn its controversial demand for a "backdoor" in an Apple data protection system - saying she worked with the President and Vice President to get the UK to abandon its plan.Two days later, US Federal Trade Commission chairman Andrew Ferguson warned big tech firms they could be violating US law if they weakened privacy and data security requirements by complying with international laws such as the Online Safety Act."Foreign governments seeking to limit free expression or weaken data security in the United States might count on the fact that companies have an incentive to simplify their operations and legal compliance measures by applying uniform policies across jurisdictions," he said.If 4chan does successfully fight the fine in the US courts, Ofcom may have other options."Enforcing against an offshore provider is tricky," Emma Drake, partner of online safety and privacy at law firm Bird and Bird, told the BBC. "Ofcom can instead ask a court to order other services to disrupt a provider's UK business, such as requiring a service's removal from search results or blocking of UK payments."If Ofcom doesn't think this will be enough to prevent significant harm, it can even ask that ISPs be ordered to block UK access."]]></content:encoded></item><item><title>Go is still not good</title><link>https://blog.habets.se/2025/07/Go-is-still-not-good.html</link><author>ustad</author><category>hn</category><pubDate>Fri, 22 Aug 2025 09:25:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[These things about Go are bugging me more and more. Mostly because they’re so
unnecessary. The world knew better, and yet Go was created the way it was.For readers of previous posts you’ll find some things repeated here. Sorry
about that.Error variable scope is forced to be wrongHere’s an example of the language forcing you to do the wrong thing. It’s very
helpful for the reader of code (and code is read more often than it’s written),
to minimize the scope of a variable. If by mere syntax you can tell the reader
that a variable is just used in these two lines, then that’s a good thing.(enough has been said about this verbose repeated boilerplate that I don’t have
to. I also don’t particularly care)So that’s fine. The reader knows  is here and only here.But then you encounter this:Wait, what? Why is  reused for ? Is there’s something subtle I’m
not seeing? Even if we change that to , we’re left to wonder why  is
in scope for (potentially) the rest of the function. Why? Is it read later?Especially when looking for bugs, an experienced coder will see these things
and slow down, because here be dragons. Ok, now I’ve wasted a couple of seconds
on the red herring of reusing  for .Is a bug perhaps that the function ends with this?Why does the scope of  extend way beyond where it’s relevant?The code would have been so much easier to read if only ’s scope had been
smaller. But that’s not syntactically possible in Go.This was not thought through. Deciding on this was not thinking, it was typing.“What color is your nil?” — The two billion dollar mistake.The reason for the difference boils down to again, not thinking, just typing.Adding comment near the top of the file for conditional compilation must be the
dumbest thing ever. Anybody who’s actually tried to maintain a portable program
will tell you this will only cause suffering.The problem is that this is not year 350 BCE. We actually have experience that
aside from air resistance, heavy and light objects actually fall at the same
speed. And we have experience with portable programs, and would not do
something this dumb.If this had been the year 350 BCE, then this could be forgiven. Science as we
know it hadn’t been invented yet. But this is after decades of very widely
available experience in portability. with no defined ownershipProbably . Who wants that? Nobody wants that.If you guessed , then you know more than anybody should have
to know about quirks of a stupid programming language.Even in a GC language, sometimes you just can’t wait to destroy a resource. It
really does need to run as we leave the local code, be it by normal return, or
via an exception (aka panic).What we clearly want is RAII, or something like it.Python has it. Though Python is  entirely refcounted, so one can pretty
much rely on the  finalizer being called. But if it’s important, then
there’s the  syntax.Go? Go makes you go read the manual and see if this particular resource needs
to have a defer function called on it, and which one.This is so dumb. Some resources need a defer destroy. Some don’t. Which ones?
Good fucking luck.And you also regularly end up with stuff like this monstrosity:Yes, this is what you NEED to do to safely write something to a file in Go.What’s this, a ? Oh yeah, of course that’s needed. Is it even
safe to double-close, or does my defer need to check for that? It happens to be
safe on , but on other things: WHO KNOWS?!The standard library swallows exceptions, so all hope is lostGo says it doesn’t have exceptions. Go makes it extremely awkward to use
exceptions, because they want to punish programmers who use them.But all Go programmers must still write exception safe code. Because while
 don’t use exceptions, other code will. Things will panic.So you need, not should, NEED, to write code like:What is this stupid middle endian system? That’s dumb just like putting the day
in the middle of a date is dumb. MMDDYY, honestly? (separate rant)But panic will terminate the program, they say, so why do you care if you
unlock a mutex five milliseconds before it exits anyway?Because what if something swallows that exception and carries on as normal, and
you’re now stuck with a locked mutex?But surely nobody would do that? Reasonable and strict coding standards would
surely prevent it, under penalty of being fired?The standard library does that.  when calling , and the
standard library HTTP server does that, for exceptions in the HTTP handlers.All hope is lost. You MUST write exception safe code. But you can’t use
exceptions. You can only have the downsides of exceptions be thrust upon you.Don’t let them gaslight you.Sometimes things aren’t UTF-8If you stuff random binary data into a , Go just steams along, as
described in this post.Over the decades I have lost data to tools skipping non-UTF-8 filenames. I
should not be blamed for having files that were named before UTF-8 existed.Well… I had them. They’re gone now. They were silently skipped in a
backup/restore.Go wants you to continue losing data. Or at least, when you lose data, it’ll
say “well, what (encoding) was the data wearing?”.Or how about you just do something more thought through, when you design a
language? How about doing the right thing, instead of the obviously wrong
simple thing?Why do I care about memory use? RAM is cheap. Much cheaper than the time it
takes to read this blog post. I care because my service runs on a cloud
instance where you actually pay for RAM. Or you run containers, and you want to
run a thousand of them on the same machine. Your data may fit in
RAM, but it’s still expensive if you have to
give your thousand containers 4TiB of RAM instead of 1TiB.You can manually trigger a GC run with , but “oh no don’t do
that”, they say, “it’ll run when it has to, just trust it”.Yeah, 90% of the time, that works every time. But then it doesn’t.I rewrote some stuff in another language because over time the Go version would
use more and more memory.It didn’t have to be this wayWe knew better. This was not the COBOL debate over whether to use symbols or
English words.And it’s not like when we didn’t know at the time that Java’s ideas were
bad, because we did know Go’s ideas were bad.We already knew better than Go, and yet now we’re stuck with bad Go codebases.https://www.uber.com/en-GB/blog/data-race-patterns-in-go/https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golanghttps://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride]]></content:encoded></item><item><title>LabPlot: Free, open source and cross-platform Data Visualization and Analysis</title><link>https://labplot.org/</link><author>turrini</author><category>hn</category><pubDate>Fri, 22 Aug 2025 09:11:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In many cases, importing data into LabPlot for further analysis and visualization is the first step in the application: LabPlot supports many different formats (CSV, Origin, SAS, Stata, SPSS, MATLAB, SQL, JSON, binary, OpenDocument Spreadsheets (ods), Excel (xlsx), HDF5, MQTT, Binary Logging Format (BLF), FITS,…]]></content:encoded></item><item><title>What the Hell Is Going On?</title><link>https://catskull.net/what-the-hell-is-going-on-right-now.html</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 22 Aug 2025 07:08:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[What the  is going on right now?Engineers are burning out. Orgs expect their senior engineering staff to be able to review and contribute to “vibe-coded” features that don’t work. My personal observation is that the best engineers are highly enthusiastic about helping newer team members contribute and learn.Instead of their comments being taken to heart, reflected on, and used as learning opportunities, hapless young coders are instead using feedback as simply the next prompt in their “AI” masterpiece. I personally have witnessed and heard first-hand accounts where it was incredibly obvious a junior engineer was (ab)using LLM tools.In a recent company town-hall, I watched as a team of junior engineers demoed their latest work. I couldn’t tell you what exactly it did, or even what it was supposed to do - it didn’t seem like they themselves understood. However, at a large enough organization, it’s not about what you do, its about what people  you do. Championing their “success”, a senior manager goaded them into bragging about their use of “AI” tools to which they responded “This is four thousand lines of code written by Claude”. Applause all around.I was asked to add a small improvement to an existing feature. After reviewing the code, I noticed a junior engineer was the most recent to work on that feature. As I always do, I reached out to let them know what I’d be doing and to see if they had any insight that would be useful to me. Armed with the Github commit URL, I asked for context around their recent change. I can’t know , but I’d be willing to put money down that my exact question and the commit were fed directly into an LLM which was then copy and pasted back to me. I’m not sure why, but I felt violated. It felt wrong.A friend recently confided in me that he’s been on a team of at least 5 others that have been involved in reviewing a heavily vibe-coded PR over the past . A month. Reviewing slop produced by an LLM. What are the cost savings of paying ChatGPT $20 a month and then having a literal  of engineers try and review and merge the code?Another friend commiserated the difficulty of trying to help an engineer contribute at work. “I review the code, ask for changes, and then they  hit me with another round of AI slop.”Here’s the thing - we  to help. We  to build good things. Things that work well, that make people’s lives easier. We want to teach people how to do software engineering! Any engineer is standing entirely on the shoulders of their mentors and managers who’ve invested time and energy into them and their careers. But what good is that investment if it’s simply copy-pasted into the latest “model” that “is literally half a step from artificial general intelligence”? Should we instead focus our time and energy into training the models and eliminate the juniors altogether?What a sad, dark world that would be.Here’s an experiment for you: stop using “AI”. Try it for a day. For a week. For a month.Recently, I completely reset my computer. I like to do it from time to time. As part of that process I prune out any software that I no longer use. I’ve been paying for Claude Pro for about 6 months. But slowly, I’ve felt like it’s just a huge waste of time. Even if I have to do a few independent internet searches and read through a few dozen stack overflow and doc pages, my own conclusion is so much more reliable and accurate than anything an LLM could ever spit out.So what good are these tools? Do they have any value whatsoever?Objectively, it would seem the answer is no. But at least they make a lot of money, right?Is anyone making money on AI right now? I see a pipeline that looks like this:“AI” is applied to some specific, existing area, and a company spins up around it because it’s so much more “efficient”AI company gets funding from venture capitalistsAI company give funding to AI service providers such as OpenAI in the form of paying for usage creditsThis isn’t necessarily all that different than the existing VC pipeline, but the difference is that not even OpenAI is making money right now. I believe this is because the technology is inherently flawed and cannot scale to meet the demand. It simply consumes too much electricity to ever be economically viable, not to mention the serious environmental concerns.We can say our prayers that Moore’s Law will come back from the dead and save us. We can say our prayers that the heat death of the universe will be sufficiently prolonged in order for every human to become a billionaire. We can also take an honestly not even hard look at reality and realize this is a scam.The emperor is wearing no clothes.]]></content:encoded></item><item><title>It’s not wrong that &quot;\u{1F926}\u{1F3FC}\u200D\u2642\uFE0F&quot;.length == 7 (2019)</title><link>https://hsivonen.fi/string-length/</link><author>program</author><category>hn</category><pubDate>Fri, 22 Aug 2025 06:18:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[But It’s Better that  and Rather Useless that From time to time, someone shows that in JavaScript the  of a string containing an emoji results in a number greater than 1 (typically 2) and then proceeds to the conclusion that haha JavaScript is so broken—and is rewarded with many likes. In this post, I will try to convince you that ridiculing JavaScript for this is less insightful than it first appears and that Swift’s approach to string length isn’t unambiguously the best one. Python 3’s approach is unambiguously the worst one, though.What’s Going on with the Title? evaluates to  as JavaScript. Let’s try JavaScript console in Firefox:Haha, right? Well, you’ve been told that the Python community suffered the Python 2 vs. Python 3 split, among other things, to Get Unicode Right. Let’s try Python 3:$ python3
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> len("🤦🏼‍♂️") == 5
True
>>> OK, then. Now, Rust has the benefit of learning from languages that came before it. Let’s try Rust:$ cargo new -q length
$ cd length
$ echo 'fn main() { println!("{}", "🤦🏼‍♂️".len() == 17); }' > src/main.rs
$ cargo run -q
true
The string contains a single emoji consisting of five Unicode scalar values:U+1F3FC EMOJI MODIFIER FITZPATRICK TYPE-3U+FE0F VARIATION SELECTOR-16The string that contains one graphical unit consists of 5 Unicode scalar values. First, there’s a base character that means a person face palming. By default, the person would have a cartoonish yellow color. The next character is an emoji skintone modifier the changes the color of the person’s skin (and, in practice, also the color of the person’s hair). By default, the gender of the person is undefined, and e.g. Apple defaults to what they consider a male appearance and e.g. Google defaults to what they consider a female appearance. The next two scalar values pick a male-typical appearance specifically regardless of font and vendor. Instead of being an emoji-specific modifier like the skin tone, the gender specification uses an emoji-predating gender symbol (MALE SIGN) explicitly ligated using the ZERO WIDTH JOINER with the (skin-toned) face-palming person. (Whether it is a good or a bad idea that the skin tone and gender specifications use different mechanisms is out of the scope of this post.) Finally, VARIATION SELECTOR-16 makes it explicit that we want a multicolor emoji rendering instead of a monochrome dingbat rendering.Each of the languages above reports the string length as the number of  that the string occupies. Python 3 strings store Unicode code points each of which is stored as one code unit by CPython 3, so the string occupies 5 code units. JavaScript (and Java) strings have (potentially-invalid) UTF-16 semantics, so the string occupies 7 code units. Rust strings are (guaranteed-valid) UTF-8, so the string occupies 17 code units. We’ll come to back to the actual  as opposed to  later.Note about Python 3 added on 2019-09-09: Originally this article claimed that Python 3 guaranteed UTF-32 validity. This was in error. Python 3 guarantees that the units of the string stay within the Unicode code point range but does not guarantee the absence of surrogates. It not only allows unpaired surrogates, which might be explained by wishing to be compatible with the value space of potentially-invalid UTF-16, but Python 3 allows materializing even surrogate pairs, which is a truly bizarre design. The previous conclusions stand with the added conclusion that Python 3 is even more messed up than I thought! With the way the example string was constructed in Python 3, the Python 3 string happens to match the valid UTF-32 representation of the string, so it is still illustrative of UTF-32, but the rest of the article has been slightly edited to avoid claiming that Python 3 used UTF-32.But I Want the Length to Be 1!There’s a language for that. The following used Swift 4.2.3, which was the latest release when I was researching this, on Ubuntu 18.04:$ mkdir swiftlen
$ cd swiftlen/
$ swift package init -q --type executable
$ swift package init --type executable
Creating executable package: swiftlen
Creating Package.swift
Creating README.md
Creating .gitignore
Creating Sources/
Creating Sources/swiftlen/main.swift
Creating Tests/
Creating Tests/LinuxMain.swift
Creating Tests/swiftlenTests/
Creating Tests/swiftlenTests/swiftlenTests.swift
Creating Tests/swiftlenTests/XCTestManifests.swift
$ echo 'print("🤦🏼‍♂️".count == 1)' > Sources/swiftlen/main.swift 
$ swift run swiftlen 2>/dev/null
true(Not using the Swift REPL for the example, because it does not appear to accept non-ASCII input on Ubuntu! Swift 5.0.3 prints the same and the REPL is still broken.)OK, so we’ve found a language that thinks the string contains one countable unit. But what is that countable unit? It’s an extended grapheme cluster. (“Extended” to distinguish from the older attempt at defining grapheme clusters now called .) The definition is in Unicode Standard Annex #29 (UAX #29).We’ve seen four different lengths so far:Number of UTF-8 code units (17 in this case)Number of UTF-16 code units (7 in this case)Number of UTF-32 code units or Unicode scalar values (5 in this case)Number of extended grapheme clusters (1 in this case)Given a valid Unicode string and a version of Unicode, all of the above are well-defined and it holds that each item higher on the list is greater or equal than the items lower on the list.One of these is not like the others, though: The first three numbers have an unchanging definition for any valid Unicode string whether it contains currently assigned scalar values or whether it is from the future and contains unassigned scalar values as far as software written today is aware. Also, computing the first three lengths does not involve lookups from the Unicode database. However, the last item depends on the Unicode version and involves lookups from the Unicode database. If a string contains scalar values that are unassigned as far as the copy of the Unicode database that the program is using is aware, the program will potentially overcount extended grapheme clusters in the string compared to a program whose copy of the Unicode database is newer and has assignments for those scalar values (and some of those assignments turn out to be combining characters).More Than One Length per Programming LanguageIt is not the case that a given programming language has to choose only one of the above. If we run this Swift program:var s = "🤦🏼‍♂️"
print(s.count)
print(s.unicodeScalars.count)
print(s.utf16.count)
print(s.utf8.count)Let’s try Rust with unicode-segmentation = "1.3.0" in :use unicode_segmentation::UnicodeSegmentation;

fn main() {
	let s = "🤦🏼‍♂️";
	println!("{}", s.graphemes(true).count());
	println!("{}", s.chars().count());
	println!("{}", s.encode_utf16().count());
	println!("{}", s.len());
}The above program prints:That’s unexpected! It turns out that  does not implement the latest version of the Unicode segmentation rules, so it gives the ZERO WIDTH JOINER generic treatment (break right after ZWJ) instead of the newer refinement in the emoji context.Let’s try again, but this time with  in :

use unic_segment::Graphemes;

fn main() {
	let s = "🤦🏼‍♂️";
	println!("{}", Graphemes::new(s).count());
	println!("{}", s.chars().count());
	println!("{}", s.encode_utf16().count());
	println!("{}", s.len());
}In the Rust case, strings (here mere string slices) know the number of UTF-8 code units they contain. The  method call just returns this number that has been stored since the creation of the string (in this case, compile time). In the other cases, what happens is the creation of an iterator and then instead of actually examining the values (string slices correspoding to extended grapheme clusters, Unicode scalar values or UTF-16 code units) that the iterator would yield, the  method just consumes the iterator and returns the number of items that were yielded by the iteration. The count isn’t stored anywhere on the string (slice) afterwards. If we wanted to later know the counts again, we’d have to iterate over the string again.Know in Advance or Compute When Needed?This introduces a notable question in the design space: Should a given type of length quantity be eagerly computed when the string is created? Or should the length be computed when someone asks for it? Or should it be computed when someone asks for it and then automatically stored on the string object so that it’s available immediately if someone asks for it again?The answer Rust has is that the length in the code units of the Unicode Encoding Form of the language is stored upon string creation, and the rest are computed when someone asks for them (and then forgotten and not stored on the string).Swift is a higher-level language and doesn’t document the exact nature of its string internals as part of the API contract. In fact, the internal representation of Swift strings changed substantially between Swift 4.2 and Swift 5.0. It’s not documented if different views to the string are held onto once created, for example. The documentation does say that strings are copy-on-write, so the first mutation may involve copying the string’s storage.Notably, the design space includes not remembering anything. The C programming language is a prominent example of this case. C strings don’t even remember their number of code units. To find out the number of code units, you have to iterate over the string until a sentinel value. In the case of C, the sentinel is the code unit for U+0000, so it excludes one Unicode scalar value from the possible string contents. However, that’s not a strictly necessary property of a sentinel-based design that doesn’t remember any lengths. 0xFF does not occur as a code unit in any valid UTF-8 string and 0xFFFFFFFF does not occur in any valid UTF-32 string, so they could be used as sentinels for UTF-8 and UTF-32 storage, respectively, without excluding a scalar value from the Unicode value space. There is no 16-bit value that never occurs in a valid UTF-16 string. However, a valid UTF-16 string does not contain unpaired surrogates, so an unpaired low surrogate could, in principle, be used as a sentinel in a design that wanted to use guaranteed-valid UTF-16 strings that don’t remember their code unit length.Knowing the Storage-Native Code Unit Length is Extremely ReasonableThe length of the string as counted in code units of its storage-native Unicode Encoding Form (i.e. whichever of UTF-8, UTF-16, and UTF 32 the programming language has chosen for its string semantics) is not like the other lengths. It is the length that the implementation cannot avoid having to know at the time of creating a new string, because it is the length that is required to be known in order to be able to allocate storage for a string. Even C, which promptly forgets about the code unit length in the storage-native Unicode Encoding Form after string has been created, has to know this length when allocating storage for a new string.That is, the design decision is about whether to remember this length. It is not about whether to compute it eagerly. You just have to have it at string creation time—i.e. eagerly.Considering that remembering this quantity makes string concatenation, which is a common operation, substantially faster to implement compared to not remembering this quantity, remembering this quantity is fundamentally reasonable. Also, it means that you don’t need to maintain a sentinel value, which means that a substring operation can yield results that share the buffer with the original string instead of having to copy in order to be able to insert sentinel. (Note that you can easily foil this benefit if you wish to eagerly maintain zero-termination for the sake of C string compatibility.)What About Knowing the Other Lengths?Even if we’ve established that it makes sense for string implementation to remember the storage length of the string in code units all the storage-native Unicode encoding form, it doesn’t answer whether a string implementation should also remember other lengths or which kind of length should be offered in the most ergonomic API. (As we see above, Swift makes the number of extended grapheme clusters more ergonomic to obtain that the code unit or scalar value length.)Also, if any other length is to be remembered, there is the question of whether it should be eagerly computed as string creation time or lazily computed the first time someone asks for it. It is easy to see why at least the latter does not make sense for multi-threaded systems-programming language like Rust. If some properties of an object are lazily initialized, in a multi-threaded case you also need to solve synchronization of these computations. Furthermore, you need to allocate space at least for a pointer to auxiliary information if you want to be able to add auxiliary information later or you need to have a hashtable of auxiliary information where the string the information is about is the key, so auxiliary information, even when not present, has storage implications or implications of having to have global state in a run-time system. Finally, for systems programming, it may be more desirable to know the time complexity of a given operation clearly even if it means “always O(n)” instead of “possibly O(n) but sometimes O(1)”. Even if the latter looks strictly better, it is less .For a higher-level language, arguments from space requirements or synchronization issues might not be decisive. It’s more relevant to consider what a given length quantity is . This is often forgotten in Internet debates that revolve around what length is the most “correct” or “logical” one. So for the lengths that don’t map to the size of storage allocation, what are they good for?It turns out that in the Firefox code base there are two places where someone wants to know the number of Unicode scalar values in a string that is not being stored as UTF-32 and attention is not paid to what the scalar values actually are. The IETF specification for Session Traversal Utilities for NAT (STUN) used for WebRTC has the curious property that it places length limits on certain protocol strings such that the limits are expressed as number of Unicode scalar values but the strings are transmitted in UTF-8. Firefox validates these limits. (The limit looks like an arbitrary power-of-two (128 scalar values). The spec has remarks about the possible resulting byte length, which was wrong according to the IETF UTF-8 RFC that was current and already nearly five years old at the time of publication of the STUN RFC. Specifically, the STUN RFC repeatedly says that 128 characters as UTF-8 may be as long as 763 bytes. To arrive at that number, you have to assume that a UTF-8 character can be up to six bytes long, as opposed to up to 4 bytes long as in the prevailing UTF-8 RFC and in the Unicode Standard, and that the last character of the 128 is a zero terminator and, therefore, known to take just one byte.) In this case, the reason for wishing to know a non-storage length is to . The other case is reporting the column number for the source location of JavaScript errors.Length limits, which we’ll come back to, probably aren’t a frequent enough a use case to justify making strings know a particular kind of length as opposed to such length being possible to compute when asked for. Neither are error messages.Another use case for asking for a length is iterating by index and using the length as the loop termination condition 1990s Java style. Like this:for (int i = 0; i < s.length(); i++) {
    // Do something with s.charAt(i)
}In this case, it’s actually important for the length to be precomputed number on the string object. This use case is coupled with the requirement that indexing into the string to find the th unit corresponding to the count of units that the “length” represents should be a fast operation.The above pattern is a lot less conclusive in terms of what lengths should be precomputed (and what the indexing unit should be) than it first appears. The above loop doesn’t do random access by index. It sequentially uses every index from zero up to, but not including, . Indeed, especially when iterating over a string by Unicode scalar value, typically when you examine the contents of a string, you iterate over the string in order. Programming languages these days provide an  facility for this, and e.g. to iterate over a UTF-8 string by scalar value, the iterator does not need to know the number of scalar values up front. E.g. in Rust, you can do this in O(n) time despite string slices not knowing their number of Unicode scalar values:for (c in s.chars()) {
    // Do something with c
}(Note that  is an 8-bit code unit (possibly UTF-8 code unit) in C and C++,  is a UTF-16 code unit in Java,  is a Unicode scalar value in Rust, and  is an extended grapheme cluster in Swift.)A programming language together with its library ecosystem should provide iteration over a string by Unicode scalar value and by extended grapheme cluster, but it does not follow that strings would need to know the scalar value length or the extended grapheme cluster length up front. Unlike the code unit storage length, those quantities aren’t useful for accelerating operations like concatenation that don’t care about the exact content of the string.

Which Unicode Encoding Form Should a Programming Language Choose?The observation that having strings know their code unit length in their storage-native Unicode encoding form is extremely reasonable does not answer how many bits wide the code units should be.The usual way to approach this question is to argue that UTF-32 is the best, because it provides O(1) indexing by “character” in the sense of a character meaning a Unicode scalar value, or the argument focuses on whether UTF-8 is unfair to some languages relative to UTF-16. I think these are bad ways to approach this question.First of all, the argument that the answer should be UTF-32 is bad on two counts. First, it assumes that random access scalar value is important, but in practice it isn’t. It’s reasonable to want to have a capability to iterate over a string by scalar value, but random access by scalar value is in the YAGNI department. Second, arguments in favor of UTF-32 typically come at a point where the person making the argument has learned about surrogate pairs in UTF-16 but has not yet learned about extended grapheme clusters being even larger things that the user perceives as unit. That is, if you escape the variable-width nature of UTF-16 to UTF-32, you pay by doubling the memory requirements and extended grapheme clusters are  variable-width.I’ll come back to the length fairness issue later, but I think a different argument is much more relevant  for the choice of in-memory Unicode encoding form. The more relevant argument is this: Implementations that choose UTF-8 actually accept the UTF-8 storage requirements. When wider-unit semantics are chosen for a language that doesn’t provide raw memory access and, therefore, has the opportunity to tweak string storage, the implementations try to come up with ways to avoid actually paying the cost of the wider units in some situations.JavaScript and Java strings have the semantics of potentially-invalid UTF-16. SpiderMonkey and V8 implement an optimization for omitting the leading zeros of each code unit in a string, i.e. storing the string as ISO-8859-1 (the actual ISO-8859-1, not the Web notion of “ISO-8859-1” as a label of windows-1252), when all code units in the string have zeros in the most-significant half. The HotSpot JVM also implements this optimization, though enabling it is optional. Swift 4.2 implements a slightly different variant of the same idea, where ASCII-only strings are stored as 8-bit units and everything else is stored as UTF-16. CPython since 3.3 makes the same idea three-level with code point semantics: Strings are stored with 32-bit code units if at least one code point has a non-zero bit above the low 16 bits. Else if a string has a non-zero bits above the low 8 bits for at least one code point, the string is stored as 16-bit units. Otherwise, the string is stored as 8-bit units (Latin1).I think the unwillingness of implementations of languages that have chosen UTF-16 or UTF-32 (or UTF-32-ish as in the case of Python 3) string  to actually use UTF-16 or UTF-32  when they can get away with not using actual UTF-16 or UTF-32 storage is the clearest indictment against UTF-16 or UTF-32 (and other wide-unit semantics like what Python 3 uses).Languages that choose UTF-8, on the other hand, stick to actual UTF-8 for the purpose of storing Unicode scalar values. When languages that choose UTF-8 deviate from UTF-8, they do so in order to represent values that are not Unicode scalar values for compatibility with external constraints. Rust uses a representation called WTF-8 for file system paths on Windows. All UTF-8 strings are WTF-8 strings, but WTF-8 can also represent unpaired surrogates for compatibility with Windows file paths being sequences of 16-bit units that can contain unpaired surrogates. Perl 6 uses an internal representation called UTF-8 Clean-8 (or UTF8-C8), which represents strings that consist of Unicode scalar values in Unicode Normalization Form C the same way as UTF-8 but represents non-NFC content differently and can represent sequences of bytes that are not valid UTF-8.UTF-8 is the only one of the Unicode  that is also a Unicode , and of the Unicode encoding schemes, UTF-8 has clearly won for interchange. (Unicode encoding forms are what you have in RAM, so UTF-16 consists of native-endian, two-byte-aligned 16-bit code units. Unicode encoding schemes are what can be used for byte-oriented interchange, so e.g. UTF-16LE consist of 8-bit code units every pair of which form a potentially-unaligned little-endian 16-bit number, which in turn may form a surrogate pair.) When UTF-8 is used as the in-RAM representation, input and output operations are less expensive than with UTF-16 or UTF-32. UTF-16 or UTF-32 in RAM requires conversion from UTF-8 when reading input and conversion to UTF-8 when writing output. A system that guarantees UTF-8 validity internally, such as Rust, needs only to  UTF-8 upon reading input and no conversion is needed when writing output. (Go takes a garbage in, garbage out approach to UTF-8: input is not validated at input time and output is written without conversion. However, iteration by scalar value can yield REPLACEMENT CHARACTERs when iterating over invalid UTF-8. That is, the input step is less expensive than in Rust, but iterating by scalar value is marginally more expensive. The output step is less correct.)

Finally, in terms of nudging developers to write correct code, UTF-8 has the benefit of being blatantly variable-width, so even with languages such as English, Somali, and Swahili, as soon as you have a dash or a smart quote, the variable-width nature of UTF-8 shows up. In this context, extended grapheme clusters are just extending the variable-width nature. Meanwhile, UTF-16 allows programmers to get too far while pretending to be working with something where the units they need to care about are fixed-width. Reacting to surrogate pairs by wishing to use UTF-32 instead is a bad idea, because if you want to write correct software, you still need to deal with variable-width extended grapheme clusters.

The choice of UTF-32 (or Python 3-style code point sequences) arises from wanting the wrong thing. The choice of UTF-16 is a matter of early-adopter legacy from the time when Unicode was expected to be capped to 16 bits of code space and, once UTF-16 has been committed to, not breaking compatibility with already-written programs is important and justified the continued use of UTF-16, but if you aren’t bound by that legacy and are designing a new language, you should go with UTF-8. Occasionally even systems that appear to be bound by the UTF-16 legacy can break free. Even though Swift is committed to interoperability with Cocoa, which uses UTF-16 strings, Swift 5 switched to UTF-8 for Swift-native strings. Similarly, PyPy has gone UTF-8 despite Python 3 having code point semantics.Shouldn’t the Nudge Go All the Way to Extended Grapheme Clusters?Even if we accept that the storage should be UTF-8 and that the string implementation should maintain knowledge of the string length in UTF-8 code units, if the blatant variable-widthness of UTF-8 is argued to be a nudge toward dealing with the variable-widthness of extended grapheme clusters, shouldn’t the Swift approach of making extended grapheme cluster access and count the view that takes the least ceremony to use be the thing that every language should do?Swift is still too young to draw definitive conclusions from. It’s easy to believe that the Swift approach nudges programmers to write more extended grapheme cluster-correct code and that the design makes sense for a language meant primarily for UI programming on a largely evergreen platform (iOS). It isn’t clear, though, that the Swift approach is the best for everyone.Earlier, I said that the example used “Swift 4.2.3 on Ubuntu 18.04”. The “18.04” part is important! Swift.org ships binaries for Ubuntu 14.04, 16.04, and 18.04. Running the programvar s = "🤦🏼‍♂️"
print(s.count)
print(s.unicodeScalars.count)
print(s.utf16.count)
print(s.utf8.count)in Swift 4.2.3 on Ubuntu 14.04 prints:So Swift 4.2.3  as well as the  0.9.0 Rust crate counted one extended grapheme cluster, the  1.3.0 Rust crate counted two extended grapheme clusters, and the same version of Swift, 4.2.3, but on a different operating system version counted three extended grapheme clusters!Swift 4 delegates Unicode segmentation to operating system-provided ICU, and “Long-Term Support” in the Ubuntu case means security patches but does not mean rolling forward the Unicode version that the system copy of ICU knows about. In the case of iOS, delegating to system ICU is probably OK and will not lead to too high probability of the text being from the future from the point of view of the OS copy of ICU, since the iOS ecosystem stays exceptionally well up-to-date. However, delegating to system ICU is not such a great match for the idea of using Swift on the server side if the server side means running an old LTS distro.(Swift 5 appears to no longer use system ICU for this. That is, Swift 5.0.3 on Ubuntu 14.04 sees one extended grapheme cluster in the string. I haven’t investigated what Swift 5 uses, but I assume that the switch to UTF-8 string representation necessitated using something other than ICU, which is heavily UTF-16-oriented. However, the result with Swift 4.2.3 nicely illustrates the issue related to using extended grapheme clusters.)If you are doing things that  be extended grapheme cluster-aware, there just is no way around the issue of not being able to correctly segment text that comes from the future relative to the Unicode segmentation implementation that your program is using. This is not a reason to avoid extended grapheme clusters for tasks that  awareness of extended grapheme clusters.However, pushing extended grapheme clusters onto tasks that do not really require the use of extended grapheme cluster introduces failure modes arising from the Unicode version dependency where such a dependency isn’t strictly necessary. For example, the Unicode version dependency of extended grapheme clusters means that you should  persist indices into a Swift strings and load them back in a future execution of your app, because an intervening Unicode data update may change the meaning of the persisted indices! The Swift string documentation does not warn against this.Let’s consider other languages a bit.C++ is often deployed such that the application developer doesn’t ship the standard library with the program. Most obviously, relying on GNU libstdc++ provided by an LTS Linux distribution presents similar problems as Swift 4 relying on ICU provided by an LTS Linux distribution. This isn’t a Linux-specific issue. Old supported branches of Windows generally don’t get new system-level Unicode data, either. Even though there is some movement towards individual applications shipping their own copy of LLVM libc++ with the application and the increased pace of C++ standard development starting with C++11 has made using a system-provided C++ standard library more problematic even ignoring Unicode considerations, it doesn’t seem like a good idea for C++ to develop a tight coupling with extended grapheme clusters for operations that don’t strictly necessitate it as longs as stuck-in-the-past system libraries (whether the C++ standard library itself or another library that it delegates to) are a significant part of the C++ standard library distribution practice.There’s a proposal to expose extended grapheme cluster segmentation to JavaScript programs. The main problem with this proposal is the implication on APK sizes on Android and the effect of APK sizes on browser competition on Android. But if we ignore that for the moment and imagine this was part of the Web Platform, it would still be problematic to build this dependency into operations for which working on extended grapheme clusters isn’t strictly necessary. While the most popular browsers are evergreen, there’s still a long tail of browser instances that aren’t on the latest engine versions. When JavaScript executes on such browsers, there’d be effects similar to running Swift 4 on Ubuntu 14.04.In contrast to C++ or JavaScript, the current Rust approach is to statically link all Rust library code, including the standard library, into the executable program. This means that the application distributor is in control of library versions and doesn’t need to worry about the program executing in the context of out-of-date  libraries. The flip side is concerns about the size of the executable. People already (rightly or wrongly) complain about the sizes of Rust executables. Pulling in a lot of Unicode data due to baking extended grapheme cluster processing into programs whose problem domain doesn’t strictly require working with extended grapheme clusters would be problematic in embedded contexts where the executable size is a real problem and not just a perceived problem—and would obviously make the perceived problem worse, too. Furthermore, in order to avoid problems similar to those involved in relying on system libraries, baking tight coupling with Unicode data into the standard library necessitates the organizational capability of keeping up with new Unicode versions in this area where not only data in the tables keeps changing but the format of the tables and, therefore, the associated algorithms have still been changing recently. Right now of the two extended grapheme cluster crates outside the Rust standard library, the one that’s organizationally closer to the standard library is the one that’s out of date.Why Do We Want to Know Anyway?“String length is about as meaningful a measurement as string height” – @qntmBeing able to allocate memory for strings gives a legitimate use case for knowing the storage length. However, in cases of Unicode scalar values or extended grapheme clusters, you typically want to iterate over them and look at each one instead of just knowing the count. So why do people want to know the count? As far as I can tell, there are two broad categories: Placing a quota limit that is fuzzy enough that it doesn’t need to be strictly tied to storage and trying to estimate how much text fits for display. Let’s look at the issue of estimating how much display space text takes, because it involves introducing yet another measurement of string length.Simply looking at the Latin letters i and m should make it clear that the display size of a string depends on the font and on the specific characters in the string. From this observation, the whole notion of estimating display space by counting characters seems folly. Indeed, if you want to know exactly how much text fits into a given space, you need to run a typesetting algorithm with a specific font, which may have a complex relationship between scalar values and glyphs, to actually see where the overflow starts. Yet, even in the case of the Latin script that has letters such as i and m, e.g. magazine editors can find character counts useful enough for estimating how many print pages an article of a given character count length is going to fill.As for computer user interfaces, character terminal user interfaces use a monospaced font where both i and m take up one character cell on a grid. In the context of a monospaced font, the extended grapheme cluster count in the context of the Latin script corresponds directly to display space taken. The same obviously applies to the Greek and Cyrillic scripts, which are so close to the Latin script that fonts even intend to reuse glyphs across these scripts. In contrast, CJK ideographs, Japanese kana, and Hangul syllables take two cells of a terminal grid. From the CJK perspective, these are full-width characters and the ASCII characters are half-width characters. There exist also half-width katakana characters which fit into an 8-bit encoding with ASCII and take one cell on the terminal grid and, therefore, are technically easier to fit to Latin script-oriented terminal systems. The display width on a terminal also has a correspondence to byte with the legacy CJK encodings: ASCII takes one byte, a CJK ideograph, a full-width kana or a Hangul syllable takes two bytes. In the case of Shift_JIS, half-width katakana takes one byte per character.This brings us to the concept of East Asian Width. ASCII and half-width katakana characters are narrow. CJK ideographs, full-width kana, and Hangul syllables are wide. However, even in the worldview that is split to Latin, Greek, and Cyrillic on one hand and Chinese, Japanese, and Korean on the other hand, there are ambiguities. From the perspective of European legacy encodings, Greek and Cyrillic (as well as accented Latin) is equally wide as ASCII. However, in legacy CJK encodings, Greek and Cyrillic characters take two bytes. This means that in terms of East Asian Width, a string can have a general-purpose width, which resolves these ambiguous characters as narrow, or legacy CJK-context width, which resolves these ambiguous characters as wide.So is the general-purpose variant (that resolves Greek and Cyrillic characters as narrow) of East Asian Width the one true string length measure? Well, no.First of all, the concept ignores all scripts that are geographically and in Unicode order between Latin, Greek, and Cyrillic on one hand and CJK on the other (even though some other scripts that are structurally similar to the Latin, Greek, and Cyrillic scripts and make sense for a monospaced font, such as Armenian and the Georgian scripts, fit this concept, too, despite not having a history in pre-Unicode CJK context). As it happens, though, emoji do fit into the concept, except for weird errors in the Unicode database. After all, emoji originate from Japan and were two bytes each when represented using the private use area of Shift_JIS.Second, the concept assumes that there is one-to-one correspondence between scalar values and extended grapheme clusters. If we run this Rust program:use unicode_width::UnicodeWidthStr;

fn main() {
    println!("{}", "🤦🏼‍♂️".width());
}This is because the base emoji is wide (2), the combining skin tone modifier is also wide (2), the male sign is counted as narrow (1), and the zero-width joiner and the variation selector are treated as control characters that don’t count towards width. Obviously, this is not the answer that we want. The answer we want is 2. Ideas that come to mind immediately, such as only counting the width of the first character in an extended grapheme cluster or taking the width of the widest character in an extended grapheme cluster, don’t work, because flag emoji consist of two regional indicator symbol letter characters both of which have East Asian Width of Neutral (i.e. they are counted as narrow but are not marked as narrow, because they are considered to exist outside the domain of East Asian typography). I’m not aware of any official Unicode definition that would reliably return 2 as the width of every kind of emoji. 😭If you really must estimate display size without running text layout with a font, whether the extended grapheme cluster count or the East Asian Width of the string works better depends on context.Arbitrary but Fair QuotasIn some cases there is a desire to impose a length limit that doesn’t arise from a strict storage limitation. For example, in the STUN protocol given earlier, presumably there is a desire to make it so that human-readable error messages cannot make protocol messages arbitrarily long. For example, in the case of Twitter, tweets being short is a core part of the type of expression that Twitter is about, so some definition of “short” is needed. In the case of string-based browser , there is a need to have  limit, but the limit is necessarily arbitrary and does not need to strictly map to bytes on disk.In cases like this, there seems to be some concern that the limit should be internationally fair. Observations that UTF-8 and UTF-16 take a different amount of storage per character depending on the character superficially suggests that the UTF-8 length or the UTF-16 length might be unfair internationally.What’s fair, though? The usual concern goes that UTF-8 favors English, because English takes one byte per character, and disfavors CJK, because Chinese, Japanese, and Korean take three bytes per character, so UTF-8 in unfair to CJK. This kind of analysis ignores how much information is conveyed per character. To assess what lengths we get for different languages when the amount of information conveyed is kept constant, I looked at the counts for the translations of the Universal Declaration of Human Rights. This is a document for which translation of the same content is available in particularly many languages, which is why I used it as the measurement corpus.Unfortunately, not all translations contain the same text, so one needs to be careful when preparing the data for comparison. Some translations are incomplete, in some cases,  incomplete. For this reason, I included only translations in stage 4 or stage 5 along the 5-stage scale. Some translations carry the preamble with the recitals, but some do not. Some also carry historical notes. To make the length comparable, the preamble, notes, and whitespace-only text nodes were omitted. The rest of the XML text nodes were concatenated and normalized to Unicode Normalition Form C before counting. (Source code is available.)Let’s look at the result. The table at the end of this document is sortable and is initially sorted by UTF-8 length. Each Δ% column shows how much the count in the column to its left deviates from the  count for that. (A note about color-coding. Coloring longer than median as red should not be taken to imply that those languages are somehow bad. It’s meant to imply that a length quota treats those languages badly.) In the table, the name of each language links to the translation in that language hosted on the site of the Unicode Consortium. The linked HTML versions may include the preamble and/or notes.The CJK concern is alleviated when considering information conveyed. When measuring UTF-8 length, Mandarin using traditional characters is the shortest of the languages that have global name recognition! This should be expected, since the Han script pretty obviously packs more information per character than e.g. alphabetic scripts. (The globally less-known languages whose UTF-8 length is shorter than Mandarin’s (using traditional characters) are African and American Latin-script languages with a relatively small native speaker population for each—only one with a native speaker population exceeding a million and many whose native speaker population is smaller than 100 000, which explains why you might not recognize their names.)Korean is also shorter than median in UTF-8 length. This also makes sense, since Hangul syllables pack three or two alphabetic jamo into one three-byte character. The UTF-8 length of Japanese is over median but only by 4.1%. The Japanese version of the text is 48% kanji and 52% hiragana. Japanese Wikipedia has almost the same kana to kanji ratio, though different kana: 46% kanji and the rest almost evenly split between hiragana and katakana, so we may assume the Universal Declaration of Human Rights to be representative of Japanese text in terms of kana to kanji ratio.When sorting by UTF-16 code unit count, UTF-32 / scalar value count, or extended grapheme cluster count, CJK are the shortest. While it’s true that UTF-8 takes more bytes for CJK than UTF-16, the notion of UTF-8 being particularly disfavorable to CJK is not true relative to other languages. Rather, UTF-16 is particularly favorable to CJK. In particular, the Han script is so information-dense that even when sorting by East Asian Width, which effectively doubles the length of CJK but not other languages, Han-script languages stay clustered at the start of the table. Korean and Japanese move further but remain below median.The language with the longest UTF-8 length is Shan, which uses the Burmese script. The Burmese language, also using the Burmese script, is the second-longest in UTF-8 length. There are a number of other Brahmic-script languages among the ones with the longest UTF-8 length. They use three bytes per character but don’t have CJK-like information per character density. These languages are below median in extended grapheme cluster count. In scalar value count, they intermingle with alphabetic languages.It’s not clear if the concepts of median and mean (average) are meaningful. Does it make sense for a language with tens of millions of native speakers to count as an equal data point as a language with tens of thousands native speakers? Since this is about writing, should the numbers of writers be considered instead? (I.e. should literacy rates be taken into account?) In the hope that with a large number of languages in the table, median hand-wavily sorts out this kind of issue, I chose to compare with median. At least the Han-script languages have comparable numbers of native speakers as the Bhramic-script languages and provide a counter-weight at the other end of the spectrum of UTF-8 length. In any case, for measures other than UTF-8 length, median and mean are very close to each other.Saying that Brahmic-script languages intermingle with alphabetic languages in character count is rather meaningless, though. In character count, after CJK (and Han-script Vietnamese and Yi-script Nousu), the language with the smallest character count is a Latin-script language (Waama). Also, the language with the largest character count is a Latin-script language (Ashéninka, Pichis). (I find it odd that in UTF-8 length Ashéninka Perené is the second-shortest but Ashéninka, Pichis is long enough to reach the Brahmic cluster. I don’t know what the relation of these two languages is and what explains two languages whose name suggests close relation ending up in opposite extremes in length. Update: It has been pointed out to me that the supposed Ashéninka Perené translation is a mislabeled duplicate of the Cashinahua translation.)One might hypothesize that the Latin script has just been put to so many uses that some of the uses have to be far from what it has been optimized for. Yet, when considering language-specific alphabets, the character counts for Greek and Georgian are above median. It just is the case that languages are different. In that sense, the whole notion of trying to find a simple length measure that is fair across languages seems folly.Let’s look at the the factor between the minimum and maximum of each measure, i.e. the factor with which the minimum needs to be multiplied to get the maximum. Let’s even ignore the outlier for maximum for each measure and use the second largest value instead of the largest value for each count. (Otherwise, Ashéninka, Pichis alone would skew the numbers a lot.) We get these factors:UTF-16, UTF-32, and extended grapheme clusters aren’t distinguished by this measure, because the languages at the extremes use characters from the Basic Multilingual Plane with one character per grapheme cluster. Considering that there are supplementary-plane scripts, arguably the UTF-32 count would be fairer than the UTF-16 count even though this factor doesn’t show the difference. It’s not clear that counting extended grapheme clusters would be particularly fair compared to counting characters: It favors scripts that are visually joining over scripts that aren’t visually joining even if there’s no logical difference. While looking at just the factor, East Asian Width makes the gap the smallest, but it’s a rather imprecise fairness solution. It just counts CJK as double. Even after this, the Han-script languages are still among the ones with the smallest counts. On the other hand, it seems unfair to recognize Hangul syllables and kana as carrying more information than an alphabetic character while not giving the same treatment to other syllabaries, such as the Ethiopic script, Ge’ez.Twitter counts each CJK character (including three-jamo Hangul syllables; i.e. it is not decomposing Hangul and treating it as alphabetic) as consuming 2 units of the quota (as when counting East Asian Width), counts emoji as consuming two units (even when East Asian Width of the cluster would be more), and, unlike East Asian Width, counts each Ethiopic syllable as consuming two units of the quota. What Twitter does seems fairer than just applying East Asian Width, but the result is still that the amount of information that can be packed in a tweet can vary four-fold depending on language. That still doesn’t seem exactly fair across languages.There is no simple measure of string length that would be fair in terms of how much information can be conveyed within a length quota regardless of language.Of solutions that don’t depend on the Unicode database and, therefore, the Unicode version and that don’t ad-hoc hard-code character ranges according to a particular version of Unicode, counting characters aka. scalar values i.e. UTF-32 length is the best that can be done. It’s still wildly unfair leading to almost eight-fold differences in how much information can be conveyed. This is not a flaw of Unicode but arises from differences in languages and writing systems.While counting scalar values is fairer than just counting UTF-8 or UTF-16 code units, the factor between minimum and maximum UTF-8 length is so close to the factor between minimum and maximum UTF-32 length, both of which are pretty large, that instead of putting thought into using the scalar value length instead of the UTF-8 length or the UTF-16 length, it’s probably better to put the thought into reconsidering if you  need to impose such a limit.Unicode doesn’t provide a good database-based definition that would improve upon the character count in terms of normalizing the amount of information conveyed. While East Asian Width brings minimum and maximum closer, it unfairly singles out Hangul syllables and kana without considering other syllabaries, because normalizing length for information conveyed is not the purpose of East Asian Width.Even if per-script (possibly non-integer) weights assigned to characters could make things fairer, it wouldn’t work well for the Latin script, which is all over the place in terms of language-dependent length.]]></content:encoded></item><item><title>Top Secret: Automatically filter sensitive information</title><link>https://thoughtbot.com/blog/top-secret</link><author>thunderbong</author><category>hn</category><pubDate>Fri, 22 Aug 2025 04:48:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[What happens when you’re dealing with free text? Filtering the entire string may
not be an option if an external API needs to process the value. Think chatbots or LLMs.You could use a regex to filter sensitive information (such as credit card
numbers or emails), but that won’t capture everything, since not all sensitive
information can be captured with a regex.Fortunately, named-entity recognition (NER) can be used to identify and
classify real-world objects, such as a person, or location. Tools like MITIE
Ruby make interfacing with NER models trivial.By using a combination of regex patterns and NER entities, Top Secret
effectively filters sensitive information from free text—here are some
real-world examples.If you want to see Top Secret in action, you might enjoy this live
stream. Otherwise, see the examples below.It’s not uncommon to send user data to chatbots. Since the data might be
free-form, we should be diligent about filtering it using the approach mentioned
above.However, it’s likely we’ll want to “restore” the filtered values when returning
a response from the chatbot. Top Secret returns a mapping that would
allow for this.The exchange might look something like this.Caller sends filtered text"Hi [PERSON_1]! How is the weather in [LOCATION_1] today?"
Caller can “restore” from the mapping
    Filtering conversation history
  When working with conversation state you should filter  message
before including it in the request. This ensures no sensitive data slips through
from previous messages. Here’s what that might look like.Top Secret can also be used as a validation tool to prevent storing sensitive
information in your database.If the validation is too strict, you can override or disable any of
the filters as needed. class Message < ApplicationRecord
   private
   def content_cannot_contain_sensitive_information
     return if result.mapping.empty?
     errors.add(:content, "contains the following sensitive information #{result.mapping.values.to_sentence}")
It’s our responsibility to protect user data. This is more important than ever
given the rise in popularity of chatbots and LLMs. Tools like Top Secret aim to
reduce this burden.We've been helping engineering teams deliver exceptional products for over 20 years. Our designers, developers, and product managers work closely with teams to solve your toughest software challenges through collaborative design and development. Learn more about us.]]></content:encoded></item><item><title>Io_uring, kTLS and Rust for zero syscall HTTPS server</title><link>https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html</link><author>guntars</author><category>hn</category><pubDate>Fri, 22 Aug 2025 03:51:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Around the turn of the century we started to get a bigger need for high
capacity web servers. For example there was the C10k problem paper.At the time, the kinds of things done to reduce work done per request was
pre-forking the web server. This means a request could be handled without an
expensive process creation.Because yes, creating a new process for every request used to be something
perfectly normal.Things did get better. People learned how to create threads, making things more
light weight. Then they switched to using /, in order to not
just spare the process/thread creation, but the whole context switch.I remember a comment on Kuro5hin from anakata, the creator of both The
Pirate Bay and the web server that powered it, along the lines of “I am select()
of borg, resistance is futile”, mocking someone for not understanding how to
write a scalable web server.But / also doesn’t scale. If you have ten thousand
connections, that’s an array of ten thousand integers that need to be sent to
the kernel for every single iteration of your request handling loop.Enter  ( on other operating systems, but I’m focusing on Linux
here). Now that’s better. The main loop is now:  set_up_epoll()
  while True:
    new, read, write = epoll()
    epoll_add_connections(new)
    for con in read:
      process(con.read())
      if con.read_all_we_need:
        epoll_remove_read_op(con)
    for con in write:
      con.write_buffer()
      if con.buffer_empty:
        epoll_remove_write_op(con)
All the syscalls are pretty cheap.  only deals in deltas, and it
doesn’t have to be re-told the thousands of active connections.But they’re not without cost. Once we’ve gotten this far, the cost of a syscall
is actually a significant part of the total remaining cost.We’re here going to ignore improvements like  and , and
instead jump to…Instead of performing a syscall for everything we want to do, commanding the
kernel to do this or that, io_uring lets us just keep writing orders to a
queue, and letting the kernel consume that queue asynchronously.For example, we can put  into the queue. The kernel will pick that
up, wait for an incoming connection, and when it arrives it’ll put a
“completion” into the completion queue.The web server can then check the completion queue. If there’s a completion
there, it can act on it.This way the web server can queue up all kinds of operations that were
previously “expensive” syscalls by simply writing them to memory. That’s it.
And then it’ll read the results from another part of memory. That’s it.In order to avoid busy looping, both the kernel and the web server will only
busy-loop checking the queue for a little bit (configurable, but think
milliseconds), and if there’s nothing new, the web server will do a syscall to
“go to sleep” until something gets added to the queue.Similarly on the kernel side, the kernel will stop busy-looping if there’s
nothing new, and needs a syscall to start busylooping again.This sounds like it would be tricky to optimize, but it’s not. In the end the
web server just puts stuff on the queue, and calls a library function that only
does that syscall if the kernel actually has stopped busylooping.This means that a busy web server can serve all of its queries without even once
(after setup is done) needing to do a syscall. As long as queues keep getting
added to,  will show .Since CPUs today have many cores, ideally you want to run exactly one thread
per core, bind it to that core, and not share any read-write data structure.For NUMA hardware, you also want to make sure that a thread only
accesses memory on the local NUMA node. This netflix talk has some
interesting stuff on NUMA and high volume HTTP delivery.The request load will still not be perfectly balanced between the threads (and
therefore cores), but I guess fixing that would have to be the topic of a
future post.We will still have memory allocations though, both on the kernel and web server
side. Memory allocations in user space will eventually need syscalls.For the web server side, you can pre-allocate a fixed chunk for every
connection, and then have everything about that connection live there. That way
new connections don’t need syscalls, memory doesn’t get fragmented, and you
don’t run the risk of running out of memory.On the kernel side each connection will still need buffers for incoming and
outgoing bytes. This may be somewhat controllable via socket options, but again
it’ll have to be the subject of a future post.Try to not run out of RAM. Bad things tend to happen.kTLS is a feature of the Linux kernel where an application can hand off
the job of encryption/decryption to the kernel. The application still has to
perform the TLS handshake, but after that it can enable kTLS and pretend that
it’s all sent in plaintext.You may say that this doesn’t actually speed anything up, it just moves 
encryption was done. But there are gains:This means that  can be used, removing the need to copy a bunch
of data between user space and kernel space.If the network card has hardware support for it, the crypto operation may
actually be offloaded from the CPU onto the network card, leaving the CPU to
do better things.Another optimization is to avoid passing file descriptors back and forth
between user space and kernel space. The mapping between file descriptors and
io_uring apparently has overhead.Now the supposed file descriptor numbers that user space sees are just
integers. They don’t show up in , and can only be used with
io_uring. They’re still capped by the  file descriptor limit, though.It’s named  because it’s a web server that serves the content of a
single tar file.Rust, io_uring, and kTLS. Not exactly the most common combination. I found that
io_uring and kTLS didn’t play super well together. Enabling kTLS requires three
 calls, and io_uring doesn’t support  (until they
merge my PR, that is).And the  crate, part of , only allows you to call the synchronous
, not export the needed struct for me to pass to my new
io_uring . Another pr sent.So with those two PRs merged, it’s working great.tarweb is far from perfect. The code needs a lot of work, and there’s no
guarantee that the TLS library (rustls) doesn’t do memory allocations during
handshakes. But it does serve https without even one syscall on a per request
basis. And that’s pretty cool.I have not done any benchmarks yet. I want to clean the code up first.One thing making io_uring more complex than synchronous syscalls is that any
buffer needs to stay in memory until the operation is marked completed by
showing up in the completion queue.For example when submitting a  operation, the memory location of those
bytes must not be deallocated or overwritten.The  crate doesn’t help much with this. The API doesn’t allow the
borrow checker to protect you at compile time, and I don’t see it doing any
runtime checks either.I feel like I’m back in C++, where any mistake can blow your whole leg off.
It’s a miracle that I’ve not seen a segfault.Someone should make a  crate or similar, using the powers of
pinning and/or borrows or something, to achieve Rust’s normal “if it
compiles, then it’s correct”.]]></content:encoded></item><item><title>Everything is correlated (2014–23)</title><link>https://gwern.net/everything</link><author>gmays</author><category>hn</category><pubDate>Fri, 22 Aug 2025 02:05:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Problem 6. : In the social sciences and arguably in the biological sciences, “everything correlates to some extent with everything else.” This truism, which I have found no competent psychologist disputes given 5 minutes reflection, does not apply to pure experimental studies in which attributes that the subjects bring with them are not the subject of study (except in so far as they appear as a source of error and hence in the denominator of a significance test). There is nothing mysterious about the fact that in psychology and sociology everything correlates with everything. Any measured trait or attribute is some function of a list of partly known and mostly unknown causal factors in the genes and life history of the individual, and both genetic and environmental factors are known from tons of empirical research to be themselves correlated. To take an extreme case, suppose we construe the null hypothesis literally (objecting that we mean by it “almost null” gets ahead of the story, and destroys the rigor of the Fisherian mathematics!) and ask whether we expect males and females in Minnesota to be precisely equal in some arbitrary trait that has individual differences, say, color naming. In the case of color naming we could think of some obvious differences right off, but even if we didn’t know about them, what is the causal situation? If we write a causal equation (which is not the same as a regression equation for pure predictive purposes but which, if we had it, would serve better than the latter) so that the score of an individual male is some function (presumably nonlinear if we knew enough about it but here supposed linear for simplicity) of a rather long set of causal variables of genetic and environmental type , , … . These values are operated upon by regression coefficients , , ….…Now we write a similar equation for the class of females. Can anyone suppose that the beta coefficients for the two sexes will be exactly the same? Can anyone imagine that the mean values of all of the s will be exactly the same for males and females, even if the culture were not still considerably sexist in child-rearing practices and the like? If the betas are not exactly the same for the two sexes, and the mean values of the s are not exactly the same, what kind of Leibnitzian preestablished harmony would we have to imagine in order for the mean color-naming score to come out exactly equal between males and females? It boggles the mind; it simply would never happen. As Einstein said, “the Lord God is subtle, but He is not malicious.” We cannot imagine that nature is out to fool us by this kind of delicate balancing. Anybody familiar with large scale research data takes it as a matter of course that when the  gets big enough she will not be looking for the statistically-significant correlations but rather looking at their patterns, since almost all of them will be significant. In saying this, I am not going counter to what is stated by mathematical statisticians or psychologists with statistical expertise. For example, the standard psychologist’s textbook, the excellent treatment by Hays (1973, page 415), explicitly states that, taken literally, the null hypothesis is always false.20 ago David Lykken and I conducted an exploratory study of the crud factor which we never published but I shall summarize it briefly here. (I offer it not as “empirical proof”—that  taken literally is quasi-always false hardly needs proof and is generally admitted—but as a punchy and somewhat amusing example of an insufficiently appreciated truth about soft correlational psychology.) In , the University of Minnesota Student Counseling Bureau’s Statewide Testing Program administered a questionnaire to 57,000 high school seniors, the items dealing with family facts, attitudes toward school, vocational and educational plans, leisure time activities, school organizations, etc. We cross-tabulated a total of 15 (and then 45) variables including the following (the number of categories for each variable given in parentheses): father’s occupation (7), father’s education (9), mother’s education (9), number of siblings (10), birth order (only, oldest, youngest, neither), educational plans after high school (3), family attitudes towards college (3), do you like school (3), sex (2), college choice (7), occupational plan in 10 years (20), and religious preference (20). In addition, there were 22 “leisure time activities” such as “acting”, “model building”, “cooking”, etc., which could be treated either as a single 22-category variable or as 22 dichotomous variables. There were also 10 “high school organizations” such as “school subject clubs”, “farm youth groups”, “political clubs”, etc., which also could be treated either as a single ten-category variable or as 10 dichotomous variables. Considering the latter two variables as multichotomies gives a total of 15 variables producing 105 different cross-tabulations. All values of χ for these 105 cross-tabulations were statistically-significant, and 101 (96%) of them were significant with a probability of less than 10.…If “leisure activity” and “high school organizations” are considered as separate dichotomies, this gives a total of 45 variables and 990 different crosstabulations. Of these, 92% were statistically-significant and more than 78% were significant with a probability less than 10. Looked at in another way, the median number of significant relationships between a given variable and all the others was 41 out of a possible 44!We also computed MCAT scores by category for the following variables: number of siblings, birth order, sex, occupational plan, and religious preference. Highly significant deviations from chance allocation over categories were found for each of these variables. For example, the females score higher than the males; MCAT score steadily and markedly decreases with increasing numbers of siblings; eldest or only children are statistically-significantly brighter than youngest children; there are marked differences in MCAT scores between those who hope to become nurses and those who hope to become nurses aides, or between those planning to be farmers, engineers, teachers, or physicians; and there are substantial MCAT differences among the various religious groups. We also tabulated the 5 principal Protestant religious denominations (Baptist, Episcopal, Lutheran, Methodist, and Presbyterian) against all the other variables, finding highly significant relationships in most instances. For example, only children are nearly twice as likely to be Presbyterian than Baptist in Minnesota, more than half of the Episcopalians “usually like school” but only 45% of Lutherans do, 55% of Presbyterians feel that their grades reflect their abilities as compared to only 47% of Episcopalians, and Episcopalians are more likely to be male whereas Baptists are more likely to be female. 83% of Baptist children said that they enjoyed dancing as compared to 68% of Lutheran children. More than twice the proportion of Episcopalians plan to attend an out of state college than is true for Baptists, Lutherans, or Methodists. The proportion of Methodists who plan to become conservationists is nearly twice that for Baptists, whereas the proportion of Baptists who plan to become receptionists is nearly twice that for Episcopalians.In addition, we tabulated the 4 principal Lutheran Synods (Missouri, ALC, LCA, and Wisconsin) against the other variables, again finding highly significant relationships in most cases. Thus, 5.9% of Wisconsin Synod children have no siblings as compared to only 3.4% of Missouri Synod children. 58% of ALC Lutherans are involved in playing a musical instrument or singing as compared to 67% of Missouri Synod Lutherans. 80% of Missouri Synod Lutherans belong to school or political clubs as compared to only 71% of LCA Lutherans. 49% of ALC Lutherans belong to debate, dramatics, or musical organizations in high school as compared to only 40% of Missouri Synod Lutherans. 36% of LCA Lutherans belong to organized non-school youth groups as compared to only 21% of Wisconsin Synod Lutherans. [Preceding text courtesy of D. T. Lykken.]These relationships are not, I repeat, Type I errors. They are facts about the world, and with  = 57,000 they are pretty stable. Some are theoretically easy to explain, others more difficult, others completely baffling. The “easy” ones have multiple explanations, sometimes competing, usually not. Drawing theories from a pot and associating them whimsically with variable pairs would yield an impressive batch of -refuting “confirmations.”Another amusing example is the behavior of the items in the 550 items of the MMPI pool with respect to sex. Only 60 items appear on the Mf scale, about the same number that were put into the pool with the hope that they would discriminate femininity. It turned out that over half the items in the scale were not put in the pool for that purpose, and of those that were, a bare majority did the job. Scale derivation was based on item analysis of a small group of criterion cases of male homosexual invert syndrome, a significant difference on a rather small  of Dr. Starke Hathaway’s private patients being then conjoined with the requirement of discriminating between male normals and female normals. When the  becomes very large as in the data published by Swenson, Pearson, and Osborne (; An MMPI Source Book: Basic Item, Scale, And Pattern Data On 50,000 Medical Patients. Minneapolis, MN: University of Minnesota Press.), approximately 25,000 of each sex tested at the Mayo Clinic over a period of years, it turns out that 507 of the 550 items discriminate the sexes. Thus in a heterogeneous item pool we find only 8% of items failing to show a significant difference on the sex dichotomy. The following are sex-discriminators, the male/female differences ranging from a few percentage points to over 30%:Sometimes when I am not feeling well I am cross.I believe there is a Devil and a Hell in afterlife.I think nearly anyone would tell a lie to keep out of trouble.Most people make friends because friends are likely to be useful to them.Policemen are usually honest.I sometimes tease animals.My hands and feet are usually warm enough.I think Lincoln was greater than Washington.I am certainly lacking in self-confidence.Any man who is able and willing to work hard has a good chance of succeeding.I invite the reader to guess which direction scores “feminine.” Given this information, I find some items easy to “explain” by one obvious theory, others have competing plausible explanations, still others are baffling.Note that we are not dealing here with some source of statistical error (the occurrence of random sampling fluctuations). That source of error is limited by the significance level we choose, just as the probability of Type II error is set by initial choice of the statistical power, based upon a pilot study or other antecedent data concerning an expected average difference. Since in social science everything correlates with everything to some extent, due to complex and obscure causal influences, in considering the crud factor we are talking about  differences,  correlations,  trends and patterns for which there is, of course, some true but complicated multivariate causal theory. I am not suggesting that these correlations are fundamentally unexplainable. They would be completely explained if we had the knowledge of Omniscient Jones, which we don’t. The point is that we are in the weak situation of corroborating our particular substantive theory by showing that  and  are “related in a nonchance manner”, when our theory is too weak to make a numerical prediction or even (usually) to set up a range of admissible values that would be counted as corroborative.…Some psychologists play down the influence of the ubiquitous crud factor, what David Lykken () calls the “ambient correlational noise” in social science, by saying that we are not in danger of being misled by small differences that show up as significant in gigantic samples. How much that softens the blow of the crud factor’s influence depends upon the crud factor’s average size in a given research domain, about which neither I nor anybody else has accurate information. But the notion that the correlation between arbitrarily paired trait variables will be, while not literally zero, of such minuscule size as to be of no importance, is surely wrong. Everybody knows that there is a set of demographic factors, some understood and others quite mysterious, that correlate quite respectably with a variety of traits. (Socioeconomic status, SES, is the one usually considered, and frequently assumed to be only in the “input” causal role.) The clinical scales of the MMPI were developed by empirical keying against a set of disjunct nosological categories, some of which are phenomenologically and psychodynamically opposite to others. Yet the 45 pairwise correlations of these scales are almost always positive (scale Ma provides most of the negatives) and a representative size is in the neighborhood of 0.35 to 0.40. The same is true of the scores on the Strong Vocational Interest Blank, where I find an average absolute value correlation close to 0.40. The malignant influence of so-called “methods covariance” in psychological research that relies upon tasks or tests having certain kinds of behavioral similarities such as questionnaires or ink blots is commonplace and a regular source of concern to clinical and personality psychologists. For further discussion and examples of crud factor size, see Meehl ().Now suppose we imagine a society of psychologists doing research in this soft area, and each investigator sets his experiments up in a whimsical, irrational manner as follows: First he picks a theory at random out of the theory pot. Then he picks a pair of variables randomly out of the observable variable pot. He then arbitrarily assigns a direction (you understand there is no intrinsic connection of content between the substantive theory and the variables, except once in a while there would be such by coincidence) and says that he is going to test the randomly chosen substantive theory by pretending that it predicts—although in fact it does not, having no intrinsic contentual relation—a positive correlation between randomly chosen observational variables  and . Now suppose that the crud factor operative in the broad domain were 0.30, that is, the average correlation between all of the variables pairwise in this domain is 0.30. This is not sampling error but the true correlation produced by some complex unknown network of genetic and environmental factors. Suppose he divides a normal distribution of subjects at the median and uses all of his cases (which frequently is not what is done, although if properly treated statistically that is not methodologically sinful). Let us take variable  as the “input” variable (never mind its causal role). The mean score of the cases in the top half of the distribution will then be at one mean deviation, that is, in standard score terms they will have an average score of 0.80. Similarly, the subjects in the bottom half of the  distribution will have a mean standard score of -0.80. So the mean difference in standard score terms between the high and low s, the one “experimental” and the other “control” group, is 1.6. If the regression of output variable  on  is approximately linear, this yields an expected difference in standard score terms of 0.48, so the difference on the arbitrarily defined “output” variable  is in the neighborhood of half a standard deviation.When the investigator runs a -test on these data, what is the probability of achieving a statistically-significant result? This depends upon the statistical power function and hence upon the sample size, which varies widely, more in soft psychology because of the nature of the data collection problems than in experimental work. I do not have exact figures, but an informal scanning of several issues of journals in the soft areas of clinical, abnormal, and social gave me a representative value of the number of cases in each of two groups being compared at around  =  = 37 (that’s a median because of the skewness, sample sizes ranging from a low of 17 in one clinical study to a high of 1,000 in a social survey study). Assuming equal variances, this gives us a standard error of the mean difference of 0.2357 in sigma-units, so that our  is a little over 2.0. The substantive theory in a real life case being almost invariably predictive of a direction (it is hard to know what sort of significance testing we would be doing otherwise), the 5% level of confidence can be legitimately taken as one-tailed and in fact could be criticized if it were not (assuming that the 5% level of confidence is given the usual special magical significance afforded it by social scientists!). The directional 5% level being at 1.65, the expected value of our -test in this situation is approximately 0.35  units from the required significance level. Things being essentially normal for 72 df, this gives us a power of detecting a difference of around 0.64.However, since in our imagined “experiment” the assignment of direction was random, the probability of detecting a difference in the predicted direction (even though in reality this prediction was not mediated by any rational relation of content) is only half of that. Even this conservative power based upon the assumption of a completely random association between the theoretical substance and the pseudopredicted direction should give one pause. We find that the probability of getting a positive result from a theory with no verisimilitude whatsoever, associated in a totally whimsical fashion with a pair of variables picked randomly out of the observational pot, is ! This is quite different from the 0.05 level that people usually think about. Of course, the reason for this is that the 0.05 level is based upon strictly holding  if the theory were false. Whereas, because in the social sciences everything is correlated with everything, for epistemic purposes (despite the rigor of the mathematician’s tables) the true baseline—if the theory has nothing to do with reality and has only a chance relationship to it (so to speak, “any connection between the theory and the facts is purely coincidental”) - is 6 or 7 times as great as the reassuring 0.05 level upon which the psychologist focuses his mind. If the crud factor in a domain were running around 0.40, the power function is 0.86 and the “directional power” for random theory/prediction pairings would be 0.43.…A similar situation holds for psychopathology, and for many variables in personality measurement that refer to aspects of social competence on the one hand or impairment of interpersonal function (as in mental illness) on the other. Thorndike had a dictum “All good things tend to go together.”]]></content:encoded></item><item><title>The issue of anti-cheat on Linux (2024)</title><link>https://tulach.cc/the-issue-of-anti-cheat-on-linux/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 22 Aug 2025 01:09:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[If you switch to Linux today, you’ll probably be surprised by how many games run out of the box just fine (mostly due to the Windows compatibility layer Proton built right into Steam),  for basically all competitive multiplayer games that utilize any sort of anti-cheat technology.Now I can finally get to the point of the article…  As someone who uses Linux daily, I would love to see these games support it, but I just don’t see that happening any time soon. Many people in the Linux community are frustrated by the fact that these anti-cheat solutions are stopping them from playing their favorite games. It also doesn’t help that some are fear-mongering about kernel-level anti-cheat solutions and spreading misinformation.In this article, I want to give you a high-level overview of how modern anti-cheat solutions work (which will hopefully be understandable even for non-technical people) and then explain why anti-cheat solutions in their current state just cannot work on Linux, as well as what the alternatives are.What is a videogame cheat? We could talk for hours about whether all sorts of macros and exploits should be considered cheats, but the main thing that comes to people’s minds when talking about multiplayer games is an external program that somehow manipulates the game or reads information from the game to provide you with an advantage over others. A prime example of this would be a wallhack or aimbot.There are generally two ways you can go about this:() Have a completely separate process that copies memory between itself and the game.() Force the game to load a DLL file (a shared library file containing code) directly into the game, executing custom code from within the game.Unless you find some very niche way to load a DLL into the game, in both cases you will need the ability to read (and write) the game’s process memory.If you are not a programmer (or you are a JavaScript developer), you most likely don’t really know how memory management works on modern systems. Let’s imagine this situation: two programs are loaded in memory. What is stopping one program from directly accessing the memory of the other program?Virtual address space in Windows (source).While in the past it would have been perfectly possible to read (almost) any of the physical memory installed in the computer, nowadays OSes use virtual address spaces. I don’t want to go into the details of how this is handled, but all you need to know is that each program is isolated in its own address space and cannot access other programs’ memory unless it uses functions provided by the operating system itself, like  and .In order to use those two functions, you will need to open a handle to the process you want to read or write memory from. This handle will be specific to your process and represent the access rights that you have relative to the object it represents (in this case, the game process). Remember this for later, as it will be important.Modern anti-cheat solutions have three main goals:Block other processes from accessing the game’s memory whenever possible.Detect and ban anyone who tries to get around the blocking mentioned above.Once someone is banned, ensure that they cannot simply create a new game account and continue playing (HWID bans).This is usually achieved by multiple components working together. Let’s take a look at Easy Anti-Cheat as an example:Loader (usually  or )Game library ( and “invisible” module)Service ()Kernel-mode driver ()Without a kernel-mode driver, there is no way to  block memory access into the game. With the kernel-mode driver, though, it’s incredibly simple. All that the driver needs to do is register a callback for handle creation, filter out requests to open such handles to the game process, check the requested permissions, and if they allow memory access, either deny the request or lower the permissions. That way, no usermode process can now read or write the games memory. Same can be applied to module loading and file system access.Using open-source Cheat Engine to try to read protected game’s memory (all reads fail).So how can anyone get around it? They also  need to get their code into the kernel, which will open many ways for them to access the game memory.Notice how I highlighted “somehow”? That’s because Windows is a closed system where Microsoft has the control to decide who should get access to the kernel. All official kernel components are signed with Microsoft code signing certificates, so it’s trivial to verify their authenticity. All 3rd party drivers need to be signed with an EV code signing certificate (which can only be bought by companies) and then go through the Hardware Developer Center certification so they can even be loaded. I am not saying this is perfect; in fact, I will most likely be writing an article about how bad actors are still getting their stuff certified. However, when they do, it usually gets quickly revoked, and it’s so costly and complicated that most don’t even bother trying.There is, of course, a way to get around it by using all sorts of exploits or by using vulnerable drivers (drivers that expose a programming interface to user-mode processes without any checks in place, which allows them to escalate their privileges and possibly even manipulate kernel components). This is where the second goal defined above comes in. The anti-cheat has to actively scan the system and try to find code that is not associated with any legitimate module (a module that was loaded properly, with all certs in place) and other modifications or patches that would otherwise not be there.While most gamers are going to say that those anti-cheats are useless and that they see cheaters left and right, the truth is that they add a huge skill check, so not everyone is able to write a cheat and then not get banned. In fact, if done properly, the cheating problem can be basically eliminated this way (I’ll get to this later).Another reason to run in the kernel is HWID (hardware identifier) banning (the 3rd point mentioned above). If a player is banned and creates a new account, playing on the same hardware will result in an immediate ban. Since the anti-cheat has a kernel component, it can directly talk to the hardware and read its serials that way. If it was running only as a user-mode process, it would be trivial to fake the serial reads. I am not personally a big fan of this since, as you can imagine, it can result in all sorts of unintended issues (people buying used hardware), but in reality, it’s not really a problem since those HWID bans usually expire after a few months (the game devs won’t tell you this though 😉).If I had to pick a game which handles cheating the best, then as of now in my humble opinion it would be Valorant by Riot Games. Keep in mind the stuff that you’ve just read and let me explain:The anti-cheat is loaded on boot. While scary for some, this allows them to block/detect the previously mentioned vulnerable drivers and exploits. This raises the skill required to write a cheat for the game even higher (usually, people resort to bootkits).The kernel driver then doesn’t do anything apart from logging (locally). When the game is actually started, it goes through those logs and figures out if the game launch should be allowed or not and does all the kernel protection stuff mentioned above.More advanced methods to obtain HWID are used, such as reading TPM EK, which is very hard to spoof properly.But that’s not all. If that was all there was to it, other anti-cheats would be just as effective. The anti-cheat team closely works with the game development team as well. How? The anti-cheat introduces extra protection for certain memory regions of the game. Some game data are encrypted, and the encryption keys change with every (even small) game update, making it really annoying for cheat developers. On top of all that, the team is very active in the cheating communities to get intel about what they are up to.I have played Valorant quite extensively, all the way from Silver to Ascendant, and I have yet to meet a cheater.There are two main concerns that people have with those kernel-mode anti-cheats:They are in the kernel doing in-depth scans; therefore, they must be vulnerable and a security issue.They are so deep in the system (and some start on system boot) that they can spy on us without us noticing.Let me ask you a question. How many vulnerable drivers (yes, those that can be abused by bad actors to gain kernel access) do you think the average gamer has on their Windows install? I’ll start with my own system. This is what I can immediately think of:If I looked hard enough, I would most likely find more.It would be really stupid of me to just point to random crap you could have on your computer and say “you have so much exploitable stuff, don’t even bother with security,” and that’s not what I am trying to say. Or maybe it is, but just a little bit… What I am trying to say is that there are many ways a malicious actor could do bad stuff with your system, but anti-cheat is very unlikely to have anything to do with it. In fact, I personally trust those anti-cheat developers much more than random vendors, since they are going to be very well aware of the possible abuse.Overall, the Windows driver ecosystem is a mess, but unfortunately, that is not going to change any time soon.As someone who is very well versed in Windows internals, I can tell you one thing, it doesn’t make sense. If you give the program administrative permissions (at least once), it can spy on you in the same way a kernel-mode driver could. There is absolutely no difference and it’s significantly easier to just write a standalone program. There are people who don’t want to play games because of their connection to Tencent (for example), but if it wasn’t for the kernel-mode anti-cheat, they would have no problem with it. Isn’t it a bit hypocritical? If the game company wanted to spy on you, they could have done so from the game process or the service they have most likely installed for DRM purposes.Oh and just by the way, the vast majority of the data networked by those previously mentioned anti-cheats to their respective servers comes from their usermode component. The only thing that’s sent “by the kernel component” (in quotes since the usermode service requests the data from the driver and then networks it, drivers cannot directly network data) is the HWID mentioned multiple times above and then detections (something that’s out of the ordinary). There is really not some magic data grabbing happening that’s only possible in the kernel.Another thing that is sometimes mentioned is that since it’s in the kernel, it would be harder for security researchers to debug and assess the possible spying. While technically true that it’s harder, it’s definitely not impossible or problematic for an experienced person, so trust me, security researchers and  the entire cheating community keep a close eye on it, in the same way they do on the usermode components.Congratulations, you have successfully made it. You have read all of the stuff and now we can finally get to the Linux part of this post 🎉.As you can probably already tell by the extensive rant above, I don’t have much good news. Linux is an open system. There is no central authority like on Windows that would tell you what you can and what you cannot do in the kernel. This obviously has countless advantages and it’s why so many people (and big corporations) love it, but is also the reason why anti-cheats cannot really function like they do on Windows.There is no way for them to block or detect memory access into the game. Anything you could think of would just not work. Kernel module? Just recompile the kernel and change the functions it uses to hide the possible cheat and bypass all checks. Mandatory kernel patch? Same thing. What about usermode detections? Just run the game in a fakeroot environment while the cheat runs with real root privileges, being hidden from the game completely… Mandatory custom kernel build? Entire Linux system dedicated to the anti-cheat? I mean… that could work, but at that point, you can just install Windows.There have been attempts to get anti-cheat to work on Linux. Easy Anti-Cheat is the most prominent one. Developers can choose whether they want to allow it to run on Linux or not. Linux gamers look at this and use it as an argument that anti-cheat on Linux does not face any issues, but the truth is that apart from the most basic sanity checks, EAC does absolutely nothing on Linux. It’s just a simple module that facilitates the server connection and data encryption/decryption for the game.One of the games that allowed EAC to run under Wine/Proton is Apex Legends. I won’t be putting any links here, but if you search GitHub for cheats for this game, you will find many that work on Linux and there is absolutely no anti-cheat bypass required. It just works.As mentioned above, if you want to achieve the best results, you need to utilize both the  and  measures. Active being the kernel component on Windows blocking memory access and trying to find possible discrepancies. Passive being the code virtualization, obfuscation, game data encryption as well as proper game networking and server-sided checks.An example of how  to utilize kernel-mode anti-cheat would be Fall Guys (yes, that’s the game that one friend made you buy just so you could play it for 30 minutes and then never open again). This game is very specific. There would be no gain in having some sort of wallhack, there would be no gain in having any sort of aimbot (you don’t aim at stuff). All that people did was speedhacking and modifying the game in a way that allowed them to jump higher and generally change their movement. This game is a prime example of why you should write your network code properly. If the game had proper networking and server checks in place (tick-based system, actions performed on both the client and server, if there is a mismatch, the server is the authority and resets the player - that’s how CS:GO did it, and that’s why people were not flying over the map in that game or speedhacking, it had other issues though), there would be no need for anti-cheat. Not even a usermode one. Instead, they fixed absolutely nothing from their side and slapped Easy Anti-Cheat on top of their game.While it’s not really possible to do any of the previously mentioned active measures, there is nothing stopping you from utilizing the passive ones. So, if you are a game developer and want to limit cheating in your game on Linux:Write proper networking code, verify data sent by the client so your game server does not blindly accept mach 8 as a walking speed.Use code obfuscation and virtualization as much as possible (be aware of the performance penalty, be smart about what parts of the code you protect), try to change it a bit with every update (commercial bin2bin obfuscators like VMProtect or Themida will produce different results on each run).If you have control over the game engine itself, try to keep sensitive information on the stack as much as possible.]]></content:encoded></item><item><title>Control shopping cart wheels with your phone (2021)</title><link>https://www.begaydocrime.com/</link><author>mystraline</author><category>hn</category><pubDate>Fri, 22 Aug 2025 00:59:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
                        Play the below sounds through your phone speaker and hold it next to a Gatekeeper Systems wheels to lock/unlock. Check me out on twitter @stoppingcart 
                    
                        Most electronic shopping cart wheels listen for a 7.8 kHz signal from an underground wire to know when to lock and unlock. A management remote can send a different signal at 7.8 kHz to the wheel to unlock it.
        
                        Since 7.8 kHz is in the audio range, you can use the parasitic EMF from your phone's speaker to "transmit" a similar code by playing a crafted audio file. 
                    Link to my original DEFCON 29 Talk]]></content:encoded></item><item><title>From GPT-4 to GPT-5: Measuring progress through MedHELM [pdf]</title><link>https://www.fertrevino.com/docs/gpt5_medhelm.pdf</link><author>fertrevino</author><category>hn</category><pubDate>Thu, 21 Aug 2025 22:52:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Onion Brought Back Its Print Edition. The Gamble Is Paying Off</title><link>https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c</link><author>andsoitis</author><category>hn</category><pubDate>Thu, 21 Aug 2025 22:28:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scientists No Longer Find X Professionally Useful, and Have Switched to Bluesky</title><link>https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&amp;login=false</link><author>sebg</author><category>hn</category><pubDate>Thu, 21 Aug 2025 22:22:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CEO pay and stock buybacks have soared at the 100 largest low-wage corporations</title><link>https://ips-dc.org/report-executive-excess-2025/</link><author>hhs</author><category>hn</category><pubDate>Thu, 21 Aug 2025 22:04:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This 31st annual Institute for Policy Studies Executive Excess report takes an in-depth look at the 100 S&P 500 corporations with the lowest median worker pay, a group we have dubbed the “Low-Wage 100.” For each of these companies, we analyze CEO and worker pay trends since 2019. We also compare, for the past six years, what these companies have spent on stock buybacks to pump up short-term share prices with what they have invested in long-term capital improvements.The report’s overall finding: At a time when many American workers are struggling with high costs for groceries and housing, the nation’s largest low-wage employers are fixated on making their overpaid CEOs even richer.Our analysis ends with realizable policy solutions for pushing Corporate America in a more equitable direction.For detailed analysis , including our methodology, see the full PDF below. A summary follows.Download Full Report (PDF)CEO pay at Low-Wage 100 firms has soared since 2019 while median worker pay has lagged behind U.S. inflation.Between 2019 and 2024, average CEO compensation within this group rose 34.7 percent in nominal — unadjusted for inflation — terms, more than double the 16.3 percent increase in these firms’ average median worker pay. The U.S. inflation rate over this same period: 22.6 percent.Average CEO compensation within the Low-Wage 100 hit $17.2 million in 2024. The group’s average median worker pay sat at just $35,570.The average CEO-worker pay ratio of Low-Wage 100 firms has widened by 12.9 percent, from 560 to 1 in 2019 to 632 to 1 in 2024.The nominal value of median pay actually  at 22 Low-Wage 100 corporations during this period.The Starbucks pay gap hit 6,666 to 1 last year, the Low-Wage 100’s widest spread by far. In 2024, the Starbucks CEO pocketed $95.8 million.Over the past six years, amid worker discontent fueling union-organizing drives at hundreds of Starbucks stores, the firm’s median pay rose just 4.2 percent in real terms to $14,674. Only seven S&P 500 firms have lower median pay.Ulta Beauty reported the Low-Wage 100’s steepest drop in median pay. Between 2019 and 2024, a period when the cosmetic retailer significantly expanded the part-time worker share of its workforce, the company’s real median pay plunged by 46 percent to $11,078.From 2019 through 2024, the Low-Wage 100 spent $644 billion on stock buybacks.Over the past six years, all but three Low-Wage 100 firms spent corporate dollars on stock buybacks. By repurchasing their own shares, companies artificially inflate executive stock-based pay and siphon resources out of worker wages and productive long-term investments.Lowe’s ranks as the Low-Wage 100’s buyback leader. The company spent $46.6 billion on share repurchases from 2019 through 2024. Over that span, this sum could have funded an annual $28,456 bonus for each of the firm’s 273,000 employees — or added 88 employees to each of the firm’s retail outlets. In 2024, Lowe’s CEO Marvin Ellison enjoyed a total compensation of $20.2 million —659 times more than the retailer’s $30,606 median annual worker pay.Home Depot currently sits second in the Low-Wage 100 buyback rankings. The big-box chain spent $37.9 billion on share repurchases between 2019 and 2024. That outlay would have been enough to give each of Home Depot’s 470,100 global employees six annual $13,423 bonuses. The Home Depot median pay: just $35,196.From 2019 through 2024, a majority of Low-Wage 100 firms spent more on stock buybacks than on long-term capital expenditures.Over the past six years, 56 Low-Wage 100 companies plowed more corporate cash into buying back their own shares of stock than investing in capital improvements.If we exclude capital expenditure outlier Amazon from the calculation, the Low-Wage 100 as a whole spent more on buybacks than on “CapEx” during this period.At least 32 billionaires owe their wealth to Low-Wage 100 companies.Five of these firms have spawned multiple billionaires still living today: Walmart (eight), Estee Lauder (four), DoorDash (three), Public Storage (two), and Tyson Foods (two).Policy changes can prevent wasteful stock buybacks and excessive CEO payouts.Taxing extreme CEO-worker pay gaps: Inone recent survey, 80 percent of likely voters expressed support for a tax hike on corporations that pay their CEO over 50 or more times what they pay their median employees.Increasing the buybacks tax: If Congress in 2022 had set our current 1 percent excise tax on stock buybacks at 4 percent, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on share repurchases in 2023 and 2024.Restricting buybacks and CEO pay through federal contracts and subsidies: The Biden administration made modest progress on this front through the CHIPS semiconductor subsidy program. But the federal government could be doing much more to leverage the power of the public purse against wasteful stock buybacks and excessive CEO pay.We highlight three particularly promising areas for CEO pay policy reform.Subjecting corporations with excessive levels of CEO pay to higher tax levies.Higher tax rates on companies with wide CEO-worker pay gaps would create an incentive to both rein in executive pay and raise worker wages, all the while generating significant new capital for vital public investments. Laws that share those goals are already generating revenue in two major cities, San Francisco and Portland, Oregon.Members of the U.S. Congress have also introduced several related bills, including:The Curtailing Executive Overcompensation (CEO) Act: This bill applies an excise tax to publicly traded and private companies with CEO-to-median-worker pay disparities that run over 50 to 1. Under this excise tax formula, the tax owed would be proportional to the degree a company’s pay ratio exceeds 50 to 1 and to the level of the CEO’s compensation. In other words, companies with large pay gaps would owe extra taxes — and those companies with extremely high CEO pay would owe Uncle Sam even more.Revenue estimate: This bill, had it been in effect in 2022, would have raised over $10 billion in annual revenue from the  100 largest U.S. companies alone.The Tax Excessive CEO Pay Act: This legislation ties a company’s federal corporate tax rate to the size of the gap between its CEO and median worker pay. Tax penalties would begin at 0.5 percentage points for companies that pay their top executives between 50 and 100 times more than their median workers. The highest penalty would apply to companies that pay top executives over 500 times worker pay.A May 2024 survey suggests that such taxes would be enormously popular. Overall, 80 percent of likely voters favor a tax hike on corporations that pay their CEOs over 50 or more times more than what they pay their median employees.Taxing and restricting stock buybacks.A 1 percent federal excise tax on the repurchase of corporate stock went into effect in 2023. With nearly $209 billion in combined stock buybacks for 2023 and 2024, the Low-Wage 100 owed approximately $2.1 billion in federal taxes over those two years.A Senate bill, the Stock Buyback Accountability Act, would quadruple this excise tax. If that 4 percent tax had been in place in 2023 and 2024, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on their share repurchases, assuming no change in their buyback behavior. That increase would be enough to cover the cost of 327,218 public housing units each year for two years.Another Senate bill, theALIGN Act, would ban executives from selling their shares within a year of a stock buyback announcement. This would prevent CEOs from timing share repurchases to cash in personally on a short-term price pop they themselves have artificially created.Using federal contracts and subsidies to discourage wide corporate pay gaps.The Biden administration took several steps to use the power of the public purse to discourage CEO pay-inflating stock buybacks. During the Biden White House years, the Department of Commerce gave preferential treatment in the awarding of $39 billion in CHIPS subsidies for domestic semiconductor production to firms that committed to refraining from all stock buybacks for five years.Future administrations could do much more to leverage the power of the public purse against extreme pay disparities. The Patriotic Corporations Act could serve as a model. This bill would grant preferential treatment in federal contracting to firms with CEO-worker pay ratios of 100 to 1 or less, among other benchmarks. The Congressional Progressive Caucus has supported such incentives.By encouraging major corporations to narrow their pay gaps, a president can also help ensure that taxpayers get the biggest bang for federal contract bucks. Studies have shown that companies with narrow gaps in CEO-worker compensation tend to perform at higher levels than firms with wide gaps.]]></content:encoded></item><item><title>Show HN: Splice – CAD for Cable Harnesses and Electrical Assemblies</title><link>https://splice-cad.com/</link><author>djsdjs</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 21:10:34 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Code formatting comes to uv experimentally</title><link>https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/</link><author>tanelpoder</author><category>hn</category><pubDate>Thu, 21 Aug 2025 20:26:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The latest uv release (0.8.13) quietly introduced an experimental new command that Python developers have been waiting for: . This addition brings code formatting directly into uv’s toolkit, eliminating the need to juggle multiple tools for basic Python development workflows.The  command provides Python code formatting through uv’s interface. Under the hood, it calls Ruff’s formatter to automatically style your code according to consistent standards.To clarify,  and  aren’t being merged. They remain separate tools. This is more about providing a simpler experience for users that don’t want to think about their formatter as a separate tool.The analogy would be to Cargo:  just runs , but you can also run  separately if you want.First, make sure you’re running uv 0.8.13 or later. If you need to upgrade, check out our guide on upgrading uv.Once upgraded, formatting your project is straightforward:The command works just like running  in your project root, but through uv’s interface.Passing Arguments to RuffYou can pass additional arguments to Ruff by placing them after :This flexibility means you can customize formatting behavior without losing uv’s conveniences.Since this is an experimental feature, expect some rough edges:The command may change in future releasesIntegration with uv’s project model might evolveError handling and output formatting could improveTry out  in your next project and see how it fits into your development workflow. The experimental nature means your feedback could help shape how this feature evolves.]]></content:encoded></item><item><title>Crimes with Python&apos;s Pattern Matching (2022)</title><link>https://www.hillelwayne.com/post/python-abc/</link><author>agluszak</author><category>hn</category><pubDate>Thu, 21 Aug 2025 19:47:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[One of my favorite little bits of python is . Abstract Base Classes with  can define what counts as a subclass of the ABC, even if the target doesn’t know about the ABC. For example:You can do some weird stuff with this. Back in 2019 I used it to create non-monotonic types, where something counts as a  if it  have the  method. There wasn’t anything too diabolical you could do with this: nothing in Python really interacted with ABCs, limiting the damage you could do with production code.A quick overview of pattern matchingYou can match on arrays, dictionaries, and custom objects. To support matching objects, Python uses , which checksIf  is a transitive subtype of If  is an ABC and defines a  that matches the type of .That made me wonder if ABCs could “hijack” a pattern match. Something like this:But surely Python clamps down on this chicanery, right?$ py10 abc.py
10 is not iterable
string is iterable
[1, 2, 3] is iterable
We can only get the field  we’ve decided the object. We can’t match “any object that has the  field”… unless we use ABCs.14.142135623730951
10.488088481701515
[1, 2, 3] is not a point
It gets better! While the ABC decides the match, the object decides the destructuring, meaning we can do stuff like this:The pattern matching is flexible but also fairly limited. It can only match on an object’s type, meaning we have to make a separate ABC for each thing we want to test. Fortunately, there’s a way around this. Python is dynamically typed. 99% of the time this just means “you don’t need static types if you’re okay with things crashing at runtime”. But it  means that type information exists at runtime, and that types can be created at runtime.Can we use this for pattern matching? Let’s try it: is a function that takes a class, defines a  ABC, sets the hook for that ABC to “anything that’s not the class”, and then returns that ABC.    case Not(DistanceMetric)():
                            ^
SyntaxError: expected ':'
It’s an error! We’ve finally hit the limits of pattern matching on ABCs. Then again, it’s “just” a syntax error. Maybe it would work if we tweak the syntax a little?   match x:
PlanePoint(x=10, y=10) is a point
SpacePoint(x=5, y=6, z=7) is a point
[1, 2, 3] is not a point
Success! And just to test that this is composable, let’s write an .This works as “”“expected”“”.Caching Rules Everything Around MeThis got me thinking: what if  wasn’t a pure function? Could I make an ABC that matched the  value of each type passed in, but not subsequent ones?Sadly, this was all for naught.trying <class 'str'>
abc is a new class
trying <class 'list'>
[1, 2, 3] is a new class
efg is a new class
It looks like  caches the results for a given type check. CPython assumes that people don’t want to shove side effects into esoteric corners of the language. Show’s how much  know.We can still have fun with side effects, though. This ABC lets through every-other type.And this ABC asks the user what it should do for each type.Try them in a pattern match. They both work!The pattern matching feature is, on the whole, pretty reasonably designed, and people will expect it to behave in reasonable ways. Whereas  is  dark magic. This kind of chicanery  have a place in the dark beating heart of a complex library, certainly not for any code your coworkers will have to deal with.So yeah, you didn’t learn anything useful. I just like horrible things ¯\_(ツ)_/¯]]></content:encoded></item><item><title>DeepSeek-v3.1</title><link>https://api-docs.deepseek.com/news/news250821</link><author>wertyk</author><category>hn</category><pubDate>Thu, 21 Aug 2025 19:06:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Introducing DeepSeek-V3.1: our first step toward the agent era! 🚀🧠 Hybrid inference: Think & Non-Think — one model, two modes⚡️ Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528🛠️ Stronger agent skills: Post-training boosts tool use and multi-step agent tasks📈 Better results on SWE / Terminal-Bench🔍 Stronger multi-step reasoning for complex search tasks⚡️ Big gains in thinking efficiency]]></content:encoded></item><item><title>AI tooling must be disclosed for contributions</title><link>https://github.com/ghostty-org/ghostty/pull/8289</link><author>freetonik</author><category>hn</category><pubDate>Thu, 21 Aug 2025 18:49:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Building AI products in the probabilistic era</title><link>https://giansegato.com/essays/probabilistic-era</link><author>sdan</author><category>hn</category><pubDate>Thu, 21 Aug 2025 18:42:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I was recently trying to convince a friend of mine that ChatGPT hasn't memorized every possible medical record, and that when she was passing her blood work results the model was doing pattern matching in ways that even OpenAI couldn't really foresee. She couldn't believe me, and I totally understand why. It's hard to accept that we invented a technology that we don't fully comprehend, and that exhibits behaviors that we didn't explicitly expect.Dismissal is a common reaction when witnessing AI’s rate of progress. People struggle to reconcile their world model with what AI can now do, and .This isn't new. Mainstream intuition and cultural impact always lag behind new technical capabilities. When we started building businesses on the Internet three decades ago, the skepticism was similar. Sending checks to strangers and giving away services for free felt absurd. But those who grasped a new reality made of zero marginal costs and infinitely scalable distribution became incredibly wealthy. They understood that the old assumptions baked into their worldview no longer applied, and acted on it.Eventually the world caught up, and we reached a new equilibrium. In the last couple of decades the tech industry has evolved, developing a strong instinct for how to build and grow digital products online. We invented new jobs, from product management to head of growth, while others evolved, from engineering leadership to performance marketing. All have created their own playbooks to thrive.AI is now shuffling the deck again.Many of those playbooks have become obsolete. Something fundamental has shifted. General purpose artificial intelligence has created a rupture in the fabric of the tech industry, upending how we design, engineer, build, and grow software — and thus businesses that have software at their core.We're now in a liminal moment, where our tools have outpaced our frameworks for understanding them. This is a technical, epistemological, and organizational change, one where exceptional AI companies have started operating significantly differently from what we’ve known in the last decades.Just as physics underwent a conceptual revolution when we moved past Newton's deterministic universe, and into a strange and counterintuitive place made by wave functions, software too is undergoing its own quantum shift. We're leaving a world where code reliably and deterministically takes certain inputs to produce specific outputs, and entering a very different one where machines now produce statistical distributions instead.Building probabilistic software is like nothing we've done before.Today’s tech industry has been shaped by the core nature of software.Like the software they’re made of, digital products map known inputs to expected results, deterministically. On Instagram, users can upload pictures, send messages, leave comments, follow accounts. On Netflix, they can search an item, pick an item, stream the video item. They choose an action, and expect a result to happen.Let’s frame these products as functions .Each input  is a user action inside the product, like “send a message on WhatsApp”, “book a ride on Uber”, and “search an address in Google Maps”. For each action , the product yields an outcome : the message will be sent, the ride will be booked, the address will be searched.Through this framework, it’s possible to appreciate why startups and tech companies work the way they work.This is most evident in engineering management.The  way of managing the performance of a software engineering team looks like this:If you’ve ever had a developer job, you’ll immediately recognize a SLO Datadog dashboard. It’s what quantifies the reliability of a system. Since the engineer’s job is to build , we ask: does taking action  produce outcome ? Does it do so  no matter how many times you try? The goal is 100%: the function should always work as we expect.That’s why layering, cautious refactors, and test-driven development are hallmarks of software engineering: they all stem from the ontological nature of the  mapping function. As the job to be done is always producing  when inputting , we should write tests that make sure that’s always the case, we should be very wary of general refactorings, and careful when introducing new features that could impact the reliability of the arrow.Product management and design too are about reliably mapping  to  — just at a different level of abstraction. For those teams, it’s about constructing a function  where the input  looks more like “the user watched an Instagram story for the first time”, producing a real-world outcome  like “the user is still using the product a month later”.Good PM’ing is about drilling users through value funnels. The cardinality of the features is known beforehand: the input space  is a limited, pre-determined set of features and growth experiments. The goal is also pre-determined: designers and PMs know in advance what goals they're optimizing for. This means that, like for engineers, they too are striving to reach a 100% score on users going from experiencing a feature to yielding a business outcome. For these teams, it looks more like this:Good design is not always quantifiable. Tasteful products aren’t about metrics. But ultimately they do exist to deliver value, and value is always delivered in funnels. When people stick around longer because they appreciate the tasteful design of a specific feature, that too qualifies as a funnel: from experiencing that feature, to keep paying long term.Conversion, activation, and retention are all ratios that require countable, pre-defined inputs and outcomes. The reason why we can count those numbers and construct the ratios is because both the numerator and the denominator are  and : what  and  can look like are known before the function is created! By knowing their cardinality, we can create the funnel.That’s why products like Amplitude and Mixpanel revolve around funnels, and the work of product and growth teams revolves around optimizing conversion rates.All these ratios, be them engineering reliability goals or growth conversion targets, are how we make both strategic as well as tactical decisions in building and growing software products. How we measure performance, how we structure our work, how we design and implement our playbooks. The entire operating system of the tech industry relies on them.The problem is that these rules, in the probabilistic world of AI, have the potential to become actively counterproductive.Things started to change in the late 2010s, when we started to witness in AI models the first signs of true generalization. Until that point, machine learning systems were essentially “narrow experts” rather than “competent generalists.”In the last decade, however, researchers have realized that pre-training deep learning models on a lot of data “causes the model to develop  abilities and knowledge that can then be transferred to downstream tasks”. The idea is that if you focus on teaching AI the fundamental structure of the entire domain you’re interested in (say, language), you can unlock an entire class of tasks , without the need to define them beforehand!, from identifying spam emails to answering trivia questions to role-playing fictional characters. Google’s T5 showed that “pre-training causes the model to develop  abilities and knowledge”, while OpenAI’s GPT-2 moved us “towards a more  that can perform many tasks.”The crucial bit here is that these models were  explicitly trained on all these tasks. When pre-trained on a large corpus of data and only fine-tuned on certain instructions, we can “substantially improve zero-shot performance on  tasks.” I cannot emphasize enough how important “unseen” and “general-purpose” are, here. They’re what made this a truly watershed moment in the history of computing. It's no longer about teaching machines to recognize spam: it's about teaching them to speak. It's no longer about teaching machines to recognize bikes or tell animals apart: it's about giving them the sense of sight itself.Think about it: we’ve built a special kind of function  that for all we know can now accept  — compose poetry, translate messages, even debug code! — and we expect it to always reply with something reasonable. For all intents and purposes, it’s the first time we stopped controlling the input space, which is now suddenly open-ended.  has become .You can ask ChatGPT , from legal advice to romantic support, from spreadsheets analysis using code, to astrology predictions. You can ask Claude to produce  piece of software, from scripts, to websites, to marketing pages, to video games. Even inputting gibberish will still produce . From a practical and philosophical standpoint, the cardinality of the input space is now basically infinite.This is quite head scratching if you’re building AI products. What will users do with them? How can you make sure all your customers will always have a great experience? What if they discover an amazing new use case that these models can perform that you haven’t foreseen? What if they discover a  use case that your marketing implied in the attempt of staying generic enough?To make things worse, the correctness of the output isn't always guaranteed! A reply constructed to be reasonable doesn't mean it's going to be . What this new function will reply with is . Sometimes, a hallucination.Can we solve hallucination? Well, we  train perfect systems to always try to reply , but some questions simply don't have "correct" answers. What even is the "correct" when the question is "should I leave him?".See, the problem is that the most interesting questions are not well defined. You can only have perfect answers when asking perfect questions, but more often than not, humans don’t know what they want. “Make me a landing page of my carwash business”. How many possible ways of achieving that objective are there? Nearly infinite. AI shines in ambiguity and uncertainty, precisely thanks to its emergent properties. If you need to know what’s 1+1, you should use a calculator, but knowing what your latest blood work results mean for your health requires nuance.That's why when building an interface between humans and machines, the best form factor is a probability distribution ("you  want this HTML with this hero banner", "you  have a Vitamin D deficiency and should touch grass more"). It’s a shape that can naturally handle nuance.That's the critical reason why we inject randomness into the output, and sampling at inference time: prompting the product with the exact same inputs, will yield two different results, making the output . This is a fundamental property of the technology, and what makes it work so well, as it provides users with a way to efficiently navigate complex problem spaces. It allows them to add their own taste to the final output, and navigate the probability distribution of all reasonable outputs according to their own judgement.This is the result of two identical prompts using Claude 3.7:It may feel subtle at the beginning, but over a sequence of chained generations composing a long trajectory, the difference greatly magnifies.Output stochasticity and emergent behavior are the reasons why we can't expect perfect reliability from AI, not in the traditional sense. We are no longer guaranteed what  is going to be, and we're no longer certain about the output  either, because it's now drawn from a distribution.In moving to an AI-first world, we transitioned from funnels to .Stop for a moment to realize what this means. When building on top of this technology, our products can now succeed in ways we’ve never even imagined, and fail in ways we never intended.This is incredibly new, not just for modern technology, but for human toolmaking itself. Any good engineer will know how the Internet works: we designed it! We know how packets of data move around, we know how bytes behave, even in uncertain environments like faulty connections. Any good aerospace engineer will tell you how to approach the moon with spaceships: we invented them! Knowledge is perfect, a cornerstone of the engineering discipline. If there’s a bug, there’s always a knowable reason: it’s just a matter of time to hunt it down and fix it.With AI products, all this is no longer true. These models are , not engineered. There's some deep unknowability about them that is both powerful and scary. Not even model makers know exactly what their creations can fully do when they train them. It's why "vibe" is such an apt word: faced with this inherent uncertainty, we're left to trust our own gut and intuition when judging what these models are truly capable of.For people interacting with products harnessing the power of these models, this is a lot to take in, to accept, and to develop an intuition for. Users really dislike the inherent uncertainty of dealing with AI systems. They’re not used to it! They’re expecting a digital product like every other product they know: you instruct the app to perform an action, and the app will perform it. Unfortunately, prompting Replit in the wrong way may very well introduce a bug in your work, depending on your request and on the probability distribution that maps to. Consumers really struggle to understand this: it makes them very mad when the AI doesn’t do what they expect it to do.The core reason why they get so frustrated is because for the first time in the digital era marginal costs are way larger than zero. In fact, they’re so large that they completely invalidate the business model and growth playbooks that dominated the Internet since the 90s. This may change in the future, depending on innovation and commoditization at hardware level, but for now the cost of intelligence is surprisingly stable (and not really as deflationary as we expected it to be until last year).We have a class of products with deterministic cost and stochastic outputs: a built-in unresolved tension. Users insert the coin with , but will be  of whether they'll get back what they expect. This fundamental mismatch between deterministic mental models and probabilistic reality produces frustration — a gap the industry hasn't yet learned to bridge.Software used to be magic black boxes offering a set of pre-defined options to pick from, and producing pre-determined results for practically-zero-margin cost. AI models are completely different entities: they accept a field of infinite possibilities, and produce probability distributions that collapse to potentially-unexpected observations after charging users very significant money.It’s clear that the way to produce products around these two technologies needs to be radically different.III. It Takes A ScientistTo thrive in the quantum era, successful organizations need to transition from engineering to empiricism.The tendency of old-school engineering leadership, when dealing with probabilistic software, is to slap reliability metrics on top of it. Good old SLO, like before. It's muscle memory, a reflex. But it creates perverse incentives: to achieve the reliability goal of 100%, classical engineering leadership naturally start adding more and more rails and constraints around the model, trying to reign it in and control it.This doesn’t work anymore.The more you try to control the model, the more you’ll nerf it, ultimately damaging the product itself. Past a certain point, intelligence and control start becoming opposing needs.The goal isn’t perfection: by definition you can’t nor should aim for it. The goal is to manage the uncertainty. As an AI product builder, you should determine what’s the acceptable unpredictability that keeps the model capable of dealing with complexity, given the market you’re operating in and the audience you’re serving. Think in terms of Minimum Viable Intelligence: the lowest quality threshold that is both accepted by the market (some may be more sensitive than others), while preserving its inherent flexibility and generalization capacity.The notion of Minimum Viable Intelligence stems from the emergent properties of these models. Companies building AI products may not know how powerful their product  is, and may inadvertently constrain the model too much when trying to keep it on its rails. Think of how frustrating it would be for a user if asking Claude Code to add a watermark to each image in a folder would result in “sorry I can’t help with that: I can only make websites”. Because we know it can! If you do keep a healthy balance, your product could be so much larger than what you originally envisioned, thanks to the surprising things the models can do .This is what forces a transition from engineering to empiricism: what will determine the correct way of building the product itself, and not just typical product management A/B testing, is now the scientific method.It takes a scientist to build AI products.The old wisdom of always building incrementally on top of what’s already there doesn’t hold up anymore. If anything, that too is actively harmful. Every time a new model drops, be it a new generation of an existing one (say, from Sonnet to Opus), or a completely new one (say swapping GPT for Gemini), all previous assumptions about its behavior should be disregarded.In fact, when a new model drops, you should even consider literally tearing down the full system, and building it back from the ground up. There are no sacred cows.When Replit moved from Sonnet 3.5 to 3.7, Replit’s President Michele had the company rewrite the  in less than 3 weeks. We called it Replit v2, but that was quite an understatement. In reality, it was a brand new product. The architecture, data model, prompting technique, context management, streaming design… it was all new. 3.7 was highly agentic in an entirely novel way, Michele understood it, and decided to lean into it instead of trying to control it. The team had to go through weeks of sleepless nights trying to beat the competition to market it successfully. Can you imagine what it takes to completely re-architect a product that was making almost $20m ARR at the time, rebuild it from the ground up in just three weeks, and to see its revenue inflect and end up growing to $100m ARR less than a quarter later? . It’s no coincidence that Michele is, indeed, a scientist by trade.Swapping models at the app layer is a big deal — which makes frontier labs stickier than they may look from the outside. It’s not “just an API commodity”. These models are not flat interfaces: they have personalities, and quirks. That’s why Gemini Pro 2.5 was not a Claude 3.5 killer despite still being an exceptionally good model. It takes hard work to fully prove that any given model is superior to any other one.Every model update doesn’t necessarily mean a complete rewrite every time, but it does force you to fundamentally rethink your assumptions each time, making a rewrite a perfectly plausible hypothesis. You have to follow an empirical method, where the only valid assumption is “I don’t know”. Being an empiricist first is diametrically opposed to being an engineer first.Even 'simple' improvements on the same base model require specialized data work. Any feature that ships needs to be tested, both in lab conditions using synthetic evals and in production with real-world usage. The test needs to be rigorous and thorough: it can't simply be a collection of binary "pass / not pass". What if a particular prompt change meaningfully impacts the model tendency to prefer a certain tool over another, actively altering the unit of economics for a certain segment of users? What if a particular design change impacts how users think about the input distribution, which in turn shapes the model output distribution in unforeseen ways?The need for statistics when testing seemingly-atomic software improvements defies classical programmers' intuition. Many engineers consider this grueling data work not part of the job description, and they may be right!It’s the job description that has changed.IV. Data is the New Operating SystemDespite the fact that model behavior is intrinsically unknowable and uncertain, figuring out how to build an effective data function around it is incredibly difficult. Models’ emergent properties make synthetic testing elusive.Engineers need to keep the eval dataset up to date with the distribution of actual user behavior, but by definition such dataset will constantly lag behind. Since the input space is no longer limited, you can't simply write a few selected tests and follow test-driven development: you'll risk breaking critical features without even being aware of their existence. Tweaking the prompt or swapping the base model for Replit meant unlocking latent features we never initially thought of, like game development. Having a good system in place to constantly sample the right test cases from real-world past usage is one of the most critical new pieces of work required today to build great AI products.That’s also the reason why testing in production, with traditional A/B tests, is also critical: this way you can make sure to stay as close as possible to the general population you’re serving, and have a higher chance to test a long-tail outcome. Production testing, however, is potentially even harder than evals testing.The elephant in the room when it comes to real-world live A/B testing is that it assumes you know what to optimize for in the first place. It implies knowing and quantifying the definition of success. In AI products, you basically can’t.Users are exploring fields of possibilities, navigating through space composing trajectories: it’s really really hard to understand whether your product is accomplishing what it’s set to do! Say you just shipped a great new feature for the Replit agent: how do you know whether users are making “better software”? Longer chains of messages? Maybe they’re just debugging in frustration. Shorter and more efficient messages? Maybe they’re giving up faster. Sure you can measure long-term retention, but you can’t afford to wait weeks (or, worse, months!) to ship features.This is what makes high-velocity AI shipping so challenging. Yet not impossible. Fundamentally it’s about moving from traditional growth funnels, to finding ways of aggregating “user trajectories” — paths through the field of possible tasks and model states.The easiest way of approaching it is by segmenting user inputs. You use smaller models to classify user requests to larger models, which allows you to segment your data in “regions of usage”. It’s a crude way of clustering user journeys. For Replit’s coding agent, this could be coding use cases: “what’s the likelihood of getting a positive message from the user after 3 chat interactions, for all users that submitted a prompt about React web apps?” To push things further, you can use the same approach to define milestones to achieve across different paths, which might mean classifying model internal states.This clearly impacts product management, design, go-to-market, and even (especially!) finance. As features become emergent, binary analytics events are no longer as useful as before to understand user behavior. Knowing that users acquired through TikTok are more likely to build games, which are more expensive to generate on a per-token basis and therefore impact the margin calculus, is  valuable across the entire company: from engineers making sure that games are efficiently generated, to marketers shifting their top-of-the-funnel strategy to a more sustainable channel, to the finance team appropriately segmenting their CAC and LTV analysis. A 20% shift from game-building users to professional web apps might mean the difference between sustainable unit economics and bleeding money on every free user — yet this insight only emerges from analyzing the actual content of AI interactions, not traditional funnel metrics. That’s why classifying states is so crucial.It all boils down to data. The value is being generated by the model, and data lives upstream and downstream of the model. We have years of literature about upstream data (for training), and the industry is keenly aware of its importance. But downstream data is something new, because we had true emergence at global scale for only a couple of years. Such scale is what makes the problem expensive, hard, complex, and requiring heavy data engineering. Architecting an AI product is no small feat, an increasingly sophisticated cross-disciplinary art.More and more, in an era of stochastic unpredictable behaviors, data is becoming a crucial differentiating point when determining the success of an enterprise. It’s the shared operating system, longitudinal to the entire organization: a shared language and context that can describe reality and prescribe actions to shape it.None of the core components of a tech company can afford to work in silos anymore. Things like customer attribution (in marketing, sales), observability (engineering), and A/B testing (product, design) used to be separate. In AI products, they collapse into one holistic system view where the core behavior of the product influences both the top of the funnel as well as the bottom line, from conversions to retention.Only data can provide the map to understand this new, unknown function , and describe the journeys that users take when exploring and meandering through the emergent properties of AI products. Only data can inform where they’re going, whether they’re successful in reaching their destination, whether they can afford to get there.Data is not just the new oil to train AI models.It’s also how we can truly harness its power.V. This Time is DifferentAfter decades of technical innovation, the world has (rightfully) developed some anti-bodies to tech hype. Mainstream audiences have become naturally skeptical of big claims of “the world is changing”. There’s now even a popular meme: “nothing ever happens”.I strongly believe that when it comes to AI, something  happening. This time it  feel different.It's ontologically different. We're moving away from deterministic mechanicism, a world of perfect information and perfect knowledge, and walking into one made of emergent unknown behaviors, where instead of planning and engineering we observe and hypothesize.The shift is real, and it affects every part of the tech industry, altering how we make products, how we study and design them, and how we structure work around them. Organizations that build using an empirical approach, think in probabilities, and measure complex trajectories will define the next era of technology. The rest will keep trying to squeeze wave functions into spreadsheets, wondering why their perfectly deterministic dashboards can't capture what makes their products magical.It’s a new world, a world of wonder and possibilities, a world to discover and understand.Welcome to the Probabilistic Era.]]></content:encoded></item><item><title>The contrarian physics podcast subculture</title><link>https://timothynguyen.org/2025/08/21/physics-grifters-eric-weinstein-sabine-hossenfelder-and-a-crisis-of-credibility/</link><author>Emerson1</author><category>hn</category><pubDate>Thu, 21 Aug 2025 17:13:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is the story of how a circle of popular science communicators, who built their brands on championing free inquiry, worked to suppress scientific critique. Of how Eric Weinstein, the man who condemns the scientific community for suppressing his and his family’s work, nearly succeeded in cancelling me through intimidation and false threats. And of how Sabine Hossenfelder spins the truth for the sake of audience capture and podcast hosts Brian Keating and Curt Jaimungal prioritize tribe loyalty over the scientific process. In revealing personal details I have kept private for years, this account shows the lengths to which the individuals involved have gone in order to deceive the public. Science communication, at its best, serves a noble purpose: to act as a bridge between the intricate, often intimidating world of scientific research and the public’s curiosity. Skilled communicators translate complexity into clarity, demystify the scientific process, and inspire a shared sense of wonder. Yet a growing and troubling trend has emerged: the rise of the contrarian science communicator. These are not easily dismissed cranks. They are skilled performers who blend legitimate science with dubious claims, making it hard to separate the valuable from the misleading.One of the most prominent examples is the contrarian physics subculture centered around Eric Weinstein, which includes Sabine Hossenfelder, Brian Keating, and Curt Jaimungal. These figures command millions of followers across social media and have built their reputations by tackling charged topics in physics, such as the validity of string theory or the claim that theoretical physics faces a crisis. Their YouTube channels feature long, thoughtful discussions with leading physicists like Roger Penrose and Leonard Susskind, and both Hossenfelder and Keating are professional physicists with undeniable expertise in their respective areas. Taken at face value, their content and profiles thus suggest that they are doing a valuable service in making science accessible and entertaining to the public.But this engagement with legitimate science conceals a concerted effort to suppress criticism and mislead the public. The deception exposes the central problem facing these prominent science communicators: they are willing to trade scientific integrity for audience capture and tribal loyalty. A prime example of this dilemma is that of Weinstein’s so-called “Geometric Unity” (GU), a proposed theory of everything first unveiled in 2013 and revived in the 2020s through podcast appearances. Despite its lack of seriousness as a scientific theory, GU continues to be entertained by Hossenfelder, Keating, and Jaimungal. As an author of the first scientifically-detailed rebuttal of GU, I have directly witnessed how this contrarian cohort reacts when their ideas or allies face substantive criticism. It lays bare their gross hypocrisy of claiming to be champions for unorthodox views while working hard to ignore or suppress challenges to one of their own. This post is a firsthand account of their campaign to silence dissent. In what follows, I will discuss Geometric Unity as if it were unambiguously known to be unserious and flawed. For those uncertain or new to the subject, this will be justified later in the section “The Jury Is Already In”.Eric Weinstein wears many hats. With a PhD in mathematics from Harvard, he has worked as a managing director of Thiel Capital, founded the “Intellectual Dark Web,” and regularly comments on a wide range of topics on popular podcasts. Crucially however, Weinstein sits squarely outside the scientific establishment, having left academia a few years after completing his doctorate in the early 1990s with only a single published and forgotten paper. His Geometric Unity proposal, therefore, has all the hallmarks of an outsider attempting to revolutionize physics, casting him as an Einstein-like figure toiling alone at the patent office.But the noteworthy aspect of GU is not its scientific merit or lack thereof. Rather, it is how GU ties into Eric Weinstein’s narrative of being an outcast decrying the profound failures of our institutions. Indeed, Weinstein has built his public persona around the concept of a “Distributed Idea Suppression Complex” (DISC), an alleged establishment in academia and science that marginalizes or silences brilliant outsiders with revolutionary ideas. Within this framework, GU is presented as a transformative proposal that powerful institutions are too fearful to engage with. This creates a self-reinforcing loop: when physicists ignore the work, it confirms that the DISC is real, but when they criticize it, they are cast as bad-faith agents protecting their entrenched paradigms. In this way, Geometric Unity functions as a foundation for Weinstein’s personal brand of scientific and institutional grievances.Until April 2021, the only public material on GU was a YouTube video of Weinstein’s highly technical 2013 Oxford lecture. That few could follow it allowed Weinstein’s grievances to go unchecked. The situation changed in February 2021 when a detailed scientific rebuttal, authored by myself and Theo Polya, was released. My critique directly tested Weinstein’s narrative: how would he respond to the thorough feedback he long claimed to want, which was free from the institutional and academic conventions he so strongly condemned? The outcome was that instead of engaging with the substance of the critique, Weinstein and his circle deployed a playbook of tactics designed to suppress, deflect, and protect his contrarian brand at all costs. When I initially released my response paper to Geometric Unity in 2021, unknown to the public, Weinstein immediately tried to suppress it. Now that four years have passed, and the risk of personal drama overshadowing the scientific legitimacy of the rebuttal is gone, I feel that the time has come to reveal the full story. The most direct incident concerns Weinstein’s attempt to block my podcast episode on , a show specializing in physics hosted by two physics graduates. In our episode, I presented a two-hour whiteboard lecture giving a gentle exposition of my paper and a refutation of Weinstein’s baseless claim that he had originally discovered what are now known as the Seiberg-Witten equations. Our episode was released on Friday June 18, 2021. The following morning I received a message from on Discord, who wanted to speak to me on the phone. What they revealed was utterly despicable. Weinstein had called them earlier and implied that if they kept the video online, there could be “legal action”. The intimidation worked; took the video down for the better part of that Saturday. In response, I immediately reassured them that the threat was a baseless bluff: what would be the basis for legal action? The copyright notice on his Geometric Unity paper? Bullshit. After my phone call with , the video went back up. Rumors circulated about Weinstein’s potential involvement in the disruption, e.g. in a Facebook thread shown below. But at the request of the hosts who wanted to avoid public controversy, I never spoke of the situation publicly until now. This is the suppression incident I alluded to in my interview in the following month with Bob Wright, which Bob duly noted in his retrospective of our conversation. In fact, Weinstein’s suppression efforts began months earlier. Immediately after our paper’s release, Curt Jaimungal, a friend of Weinstein and host of the popular  podcast, reached out to invite me and my co-author on his show. He proposed a date “6-8 weeks” out, which in hindsight was clearly meant to coordinate with Weinstein’s own paper release on April 1st. I agreed to the interview, noting my co-author’s wish for anonymity. Curt was pleased and said he would follow up.But the follow-up never came. When I reached out in May, Curt punted, citing a hectic schedule. The truth about the situation came out a month later during the affair. In his call, Weinstein revealed to the hosts that Curt had cancelled his interview with me. It’s not hard to put two and two together: Weinstein told Curt to call off my interview and then tried to use that cancellation as leverage to discredit me.We thus have a disturbing truth. Eric Weinstein, the man who waxes poetic about a Distributed Idea Suppression Complex, is a hypocrite willing to use his own influence to squash criticism. Weinstein’s grievances and tale of persecution are frequently invoked to serve his narrative, yet when he receives opposition, he is willing to use his own power to suppress others.Attack the Person, Not the ScienceA month after the release of the GU paper, Weinstein was asked publicly about my critique in a now infamous Clubhouse discussion, hosted by Brian Keating, a close friend of Weinstein, distinguished professor of physics at UC San Diego, and host of the popular  podcast. When pressed on scientific details, Weinstein didn’t address the physics and instead demanded to know the identity of my anonymous co-author Theo Polya. Furthermore, he launched into an incoherent word salad of accusations about Theo Polya (and thus by extension myself), invoking references to misogyny, rape jokes, and 4chan. To borrow a phrase of Weinstein’s during his recent debate with Sean Carroll: “How dare you Eric”. The takeaway from that cringeworthy session was that Weinstein, together with Keating, refused to engage with our critique because they didn’t know who Theo Polya was. I feel it is hardly worth stating but I’ll do so anyway: what does the anonymity of one of two authors have to do with the merits of a critique? If Geometric Unity is so visionary, why would it matter? It’s a classic bait-and-switch. Weinstein, who demands his ideas be judged on merit alone, retreats from the science and falls back on personal attacks as soon as his ideas are actually challenged. It is illuminating to see how Sabine Hossenfelder, Brian Keating, and Curt Jaimungal have been continuing to bolster Weinstein over the years, either directly through promoting his ideas when they know better or else indirectly by amplifying ambiguity about the status of his work.I already discussed how Curt was complicit in cancelling my interview, which would have been critical of GU. Despite the seriousness he projects by hosting top scientists, he continues to treat GU as a substantive proposal, inviting many guests to give their (uninformed) opinion on GU while giving zero attention to the singularly critical work that is my rebuttal. In fact, he requested a copy of my interview from  when the video was briefly taken down, though he has never made any reference to it whatsoever. Even if one were to suppose Curt had disagreements with the critique, he is someone who platforms people with a diversity of viewpoints, including outsiders like Chris Langan and UAP specialists. That Curt would compromise his intellectual openness and rigor in order to promote and protect Weinstein is a tragic consequence of tribalism. A similar and more pronounced story holds for Brian Keating, Weinstein’s most prominent promoter and staunch defender. Indeed, Brian continually hosts Eric Weinstein on his podcast (over 30 times as of this writing), many of which explicitly promote Geometric Unity. His strong support of Weinstein takes on many forms, including hosting him as a scholar in residence and speaker at UC San Diego, expressing interest in experimentally testing GU, and inviting scientists to engage with Weinstein and his work (here, here, and here). So if there is anyone else other than Weinstein who has a vested interest in getting to the bottom of my critique of GU, it would be Brian.Two weeks after the release of my response paper and shortly after Sabine hosted my blogpost, Theo Polya and I sent the following message to Brian:We received no response, not even a reply to ask “Who is Theo Polya?”, the question used by both Weinstein and Brian in their Clubhouse discussion two months later to deflect engaging with our critique.A few months after this email, I was contacted by Brandon Van Dyck shortly after he heard of my appearance on Wright’s podcast to record a follow-up interview (later provocatively titled “The Eric Weinstein/Timothy Nguyen Affair”). By pure coincidence, Brian had reached out to Brandon at the same time, expressing an interest in being on Brandon’s show. Brandon and I took the opportunity to propose to Brian that he could have a conversation with me (either about Weinstein, his book , or any other topic of his choosing). In response, Brian withdrew his self-invitation, saying he was “not interested”, as we discussed in our interview. The dodging continued when Michael Shermer and I publicly invited both Brian and Eric Weinstein for a discussion with me on Michael’s podcast (here and here), after Michael was unsuccessful in asking Brian privately to participate. Again, no response.It’s one thing to be unwilling to speak to critics. Casting aspersions on them to avoid defending your position is another matter entirely. On other podcasts, Brian has claimed I am not acting in good-faith and that I’m trying to “bait” him, which are just additional examples of how Brian is going after the messenger rather than sticking to the science. The simple truth is that he and Weinstein have refused every opportunity to address the criticisms of GU in order to keep the charade running. Brian’s criticism of my character is also difficult to process in light of Weinstein’s attack of me on Clubhouse and the attempted suppression of my work. It is an embarrassing state of affairs to see a distinguished professor of physics so obviously acting in bad-faith.Finally, we get to Sabine Hossenfelder, a theoretical physicist with over 1.5 million followers on YouTube who has built a reputation for giving sharp no-nonsense scientific critiques. Her role in the GU affair is more complicated and, in my case, unfortunate. Until recently, Sabine had been dismissive of Weinstein’s work. However, a recent video of hers (titled Physicists are afraid of Eric Weinstein — and they should be) paints a very different picture. In it, she waffles all over the place about her opinion of Weinstein’s work and how much time she’s spent looking at it. She’s inconsistent with her messaging, saying that “she never looked into [Geometric Unity] in any detail” but clearly saying the opposite in an older video. It honestly doesn’t interest me to micro-police how Sabine chooses to express her opinions – her statements have all the flair of entertainment and so cannot be taken too literally. The problem is that Sabine appears to be employing ambiguity to present two incompatible positions: on the one hand, she wants to play honest critic on scientific matters (“Eric’s theory is a waste of time”), but she also wants to claim that Weinstein and his bandwagon deserve credit for being contrarian. It’s this latter step where she’s acting dishonestly, as I will now explain.Rewinding to July of 2021, I reached out to Sabine, whom I had recently interviewed at Google, and I explained to her my frustration with the aforementioned Clubhouse episode (which she had watched). To my delight, she revealed she was speaking to Brian soon and would ask him about me. But when Sabine got back to me a few weeks later over Zoom, I was shocked by the message she brought. Sabine repeated and supported Brian’s claims that my co-author’s anonymity was the cause of Brian’s refusal to speak to me. She recounts that she has had to deal with anonymous trolls herself and that she doesn’t feel the need to respond to them. When I raised the fact that  wasn’t anonymous, she said that Brian deflected, noting that GU was Weinstein’s theory and that it wasn’t his prerogative to talk to me about it. When asked why he declined going on Brandon’s podcast to talk to me, his excuse was that it was insulting to be invited only to speak about Weinstein and not himself (contrary to what was offered to Brian and that Brian had in fact invited himself onto Brandon’s podcast). In summary, I got nowhere with Sabine talking to Brian on my behalf. At the time, I owed Sabine a tremendous debt of gratitude for hosting my blogpost in March 2021 that advertised my critique of GU. I had no public profile back then and her platform gave my work the reach and legitimacy it needed to get off the ground. Because of this, I kept Sabine’s confusing report about Brian to myself. I even met Sabine in person at the HowTheLightGetsIn Festival in London 2023 and found her very agreeable and sincere in person. Nevertheless, I always had the suspicion that Sabine might really have been shielding Brian by gaslighting me. That intuition was confirmed in Sabine’s recent video in which she claims that Brian “deserves credit for not chickening out and standing for Eric” and that Curt’s explainer video of GU was “courageous”. Such words are baffling given our private conversation and her being qualified to understand the validity of my critique of GU hosted on her own blog. I now clearly see the situation for what it is. Sabine is just as guilty of grifting as Brian and Curt.This incident is particularly unfortunate in Sabine’s case, as she has cultivated a reputation as a respected physics communicator and science writer known for her sharp critiques of theoretical physics. Regardless of one’s stance with her contrarian views, she has in the past offered many perspectives on the state of theoretical physics worthy of attention, both on her blog and her book . Thus, her most recent video concerning Weinstein reflects poorly on her not only because of the personal circumstances I’ve now disclosed but also due to its strong departure from reason: in it, she also labels the theoretical physics community a “f-cking hypocrisy” and “scam” by equating the quality of their work with that of Weinstein’s Geometric Unity. This claim is so outrageous that physicist Christian Ferko shortly afterwards made a detailed presentation debunking this absurd false equivalence. It is a sad state of affairs to see Sabine undermine her own credibility by exploiting the sensationalism surrounding Weinstein and GU merely to incite outrage and air her grievances against academic and physics communities. And like Weinstein, she is willing to censor her critics rather than address them.Scientific disagreements are intricate matters that require the attention of highly trained experts. However, for laypersons to be able to make up their own minds on such issues, they have to rely on proxies for credibility such as persuasiveness and conviction. This is the vulnerability that contrarians exploit, as they are often skilled in crafting the optics and rhetoric to support their case. Indeed, Weinstein and Hossenfelder’s strong personalities and their sowing of distrust in institutions enable them to persuade others of the correctness of their views when they deviate from those of experts. Thus, I include this section to show that even if one were to rely on social cues alone, there is in fact no controversy about the illegitimacy of Geometric Unity among those who are close to Weinstein or who are qualified to judge. The success of physics grifters has relied on the fact that they make more noise than those who have quietly moved on.Let’s start with podcasting star Lex Fridman. Word got to Lex of my paper with Theo Polya when it was released and all three of us got onto a video call in March 2021 to discuss the situation. Lex proposed hosting us alongside Weinstein for a discussion of Geometric Unity on his podcast and we all agreed (Theo Polya was willing to reveal himself given his affinity for Lex’s podcast). The only question is whether Weinstein, who had already been on Lex’s show four times, would agree. Two weeks later, when I got in touch with Lex via email, he disappointingly changed topics and said he did not discuss what we had proposed with Weinstein (or Joe Rogan). In hindsight, this was clearly not the case. Weinstein released his Geometric Unity paper on April 1, debuting it on Joe Rogan’s podcast and then on Brian Keating’s podcast the following day. Conspicuously, Weinstein did not appear on Lex’s podcast. On Rogan’s podcast, Rogan’s skepticism and pushback was full-on with his interview with Weinstein, noting “there has been some criticism” and not letting Weinstein off the hook from his obscurantism. Rogan certainly hadn’t read Sabine’s blog to become aware of my critique; Lex had tipped him off. Four years later, Weinstein has not returned to Lex’s show.Marcus is the Oxford mathematician who hosted Weinstein’s GU talk and who knows Weinstein well from their overlapping time in academia. I first met Marcus virtually in 2022 when I hosted him for a talk at Google. During the Q&A of that conversation, I asked Marcus, who has been conspicuously absent from the Geometric Unity saga since 2013, about Weinstein and his work (it was a carefully thought-out question as Marcus was initially unwilling to field a question about Weinstein during our preparation). Marcus’s reply shows a man distancing himself from Geometric Unity, stating he’s less qualified than others to assess it, a sharp U-turn from his glowing comparison of Weinstein with Einstein in 2013. More recently, I also met Marcus in person at the 2025 East Anglia Festival. Over lunch with Marcus and his wife, Weinstein’s name briefly came up, but Marcus offered no positive comments about him or his ideas, and our conversation continued.Economists, Computer Scientists, and PhysicistsDespite advocating against established institutions and credentialism, Eric Weinstein readily leverages visits to prestigious institutions to enhance his public image. One notable instance was his November 2021 visit to UChicago, where he presented work co-authored with his wife, Pia Malaney, applying gauge theory to economics. Unlike his Geometric Unity work, which lacked sufficient detail to merit serious attention (consisting only of a YouTube lecture and an inadequately written paper), this UChicago presentation was accompanied by a well-formatted paper containing significant technical details. This allowed me to quickly analyze the paper, write a rebuttal (this time without an anonymous co-author), and upload it to arXiv.Note that before this incident, the Malaney-Weinstein work received little attention due to its limited significance and impact. Despite this, Weinstein has suggested that it is worthy of a Nobel prize and claimed (with the support of Brian Keating) that it is “the most deep insight in mathematical economics of the last 25-50 years”. In that same podcast episode, Weinstein also makes the incendiary claim that Juan Maldacena stole such ideas from him and his wife. For those unable to judge the situation, I offer this appreciative reply from one of the economics professors at UChicago who sponsored Eric’s visit:Has Weinstein responded to my economics rebuttal now that the anonymous Theo Polya is no longer an author? You already know the answer.And in case you’re wondering, there’s also equivalent endorsement of my response to Geometric Unity. For instance, see my interview with computer scientist Scott Aaronson. There’s also Sean Carroll, who after his debate with Weinstein on Piers Morgan, confided to me that he directs everyone who asks him about Weinstein’s work to the rebuttal I published. Most recently, Christian Ferko has done an excellent job giving his own take on the flaws of GU based on the groundwork laid out by my analysis.Overall, the consistent theme is that the few professional scientists who have examined Weinstein’s work and are not influenced by audience capture have supported the critique I put forth. The fact that most scientists have ignored GU says less about a failing within the scientific establishment and more about the group of contrarians who continue to entertain Weinstein’s ideas.When I wrote my critique of Geometric Unity, I thought I was simply engaging in math and physics. I never imagined it would take me on a journey of hypocrisy and censorship. To dismiss this story as mere internet drama is to overlook the troubling reality underneath it all: Eric Weinstein and several of our most prominent science communicators – nay, science  – are willing to distort the truth to suit their own interests. Many eyes and ears tune into Hossenfelder, Keating, and Jaimungal, who frequently appear alongside distinguished scientists that are likely unaware of their involvement in the grift. While these popularizers are able to fulfill their audience’s needs to understand science, they simultaneously enable them to hold unconventional views that may contradict the very science they promote. That the three of them have done valuable work in making science accessible to the public is precisely what makes their conduct disconcerting.As a former fan of Weinstein who became a critic, I wrote my rebuttal to Geometric Unity expecting a scientific debate. Instead, I received a grim lesson about the state of modern science communication: when personal brands and tribal loyalties become the main focus, scientific integrity is sacrificed. The unfortunate truth is that some of the most visible voices in science are more interested in being celebrated than in being correct. And in a world where public trust in expertise is already in peril, that is a betrayal we simply cannot afford.Acknowledgements: I would like to thank Ieva Cepaite, Richard Easther, Daniel Gilbert, Chris Kavanaugh, and Tim Scarfe for their valuable feedback on earlier drafts of this post.]]></content:encoded></item><item><title>The Core of Rust</title><link>https://jyn.dev/the-core-of-rust/</link><author>zdw</author><category>hn</category><pubDate>Thu, 21 Aug 2025 16:27:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[NOTE: this is not a rust tutorial.Every year it was an incredible challenge to fit teaching Rust into lectures since you basically need all the concepts right from the start to understand a lot of programs. I never knew how to order things. The flip side was that usually when you understand all the basic components in play lots of it just fits together. i.e. there's some point where the interwovenness turns from a barrier into something incredibly valuable and helpful.
—Jana DönszelmannOne thing I admire in a language is a strong vision. Uiua, for example, has a very strong vision: what does it take to eliminate all local named variables from a language? Zig similarly has a strong vision: explicit, simple language features, easy to cross compile, drop-in replacement for C.Note that you don’t have to agree with a language’s vision to note that it  one. I expect most people to find Uiua unpleasant to program in. That’s fine. You are not the target audience.There’s a famous quote by Bjarne Strousup that goes “Within C++, there is a much smaller and cleaner language struggling to get out.” Within Rust, too, there is a much smaller and cleaner language struggling to get out: one with a clear vision, goals, focus. One that is coherent, because its features . This post is about that language.Learning Rust requires learning many things at onceRust is hard to learn. Not for lack of trying—many, many people have spent person-years on improving the diagnostics, documentation, and APIs—but because it’s complex. When people first learn the language, they are learning many different interleaving concepts:These concepts interlock. It is very hard to learn them one at a time because they interact with each other, and each affects the design of the others. Additionally, the standard library uses all of them heavily.Let’s look at a Rust program that does something non-trivial:I tried to make this program as simple as possible: I used only the simplest iterator combinators, I don't touch  at all, I don't use async, and I don't do any complicated error handling.Already, this program has many interleaving concepts. I'll ignore the module system and macros, which are mostly independent of the rest of the language. To understand this program, you need to know that: and  take a function as an argument. In our program, that function is constructed inline as an anonymous function (closure).Errors are handled using something called , not with exceptions or error codes. I happened to use  and , but you would still need to understand Result even without that, because Rust does not let you access the value inside unless you check for an error condition first.Result takes a generic error; in our case, .Result is an data-holding enum that can be either Ok or Err, and you can check which variant it is using pattern matching.Iterators can be traversed either with a  loop or with .  is eager and  is lazy.  has different ownership semantics than .If you want to modify this program, you need to know some additional things: can only print things that implement the traits  or . As a result, s cannot be printed directly. returns a struct that borrows from the path. Sending it to another thread (e.g. through a channel) won't work, because  goes out of scope when the closure passed to  finishes running. You need to convert it to an owned value or pass  as a whole.
As an aside, this kind of thing encourages people to break work into "large" chunks instead of "small" chunks, which I think is often good for performance in CPU-bound programs, although as always it depends. only accepts functions that are . Small changes to this program, such as passing the current path into the closure, will give a compile error related to ownership. Fixing it requires learning the  keyword, knowing that closures borrow their arguments by default, and the meaning of .
If you are using , which is often recommended for beginners, your program will need to be rewritten from scratch (either to use Arc/Mutex or to use exterior mutability). For example, if you wanted to print changes from the main thread instead of worker threads to avoid interleaving output, you couldn't simply push to the end of an  collection, you would have to use  in order to communicate between threads.This is a  of concepts for a 20 line program. For comparison, here is an equivalent javascript program:For this JS program, you need to understand:I'm cheating a little here because  returns a list of paths and  doesn't. But only a little.My point is not that JS is a simpler language; that's debatable. My point is that you can do things in JS without understanding the whole language. It's very hard to do non-trivial things in Rust without understanding the whole core.Rust's core is interwoven on purposeThe previous section makes it out to seem like I'm saying all these concepts are bad. I'm not. Rather the opposite, actually. Because these language features were designed in tandem, they interplay very nicely: and s are impossible to implement without generics (or duck-typing, which I think of as type-erased generics)/, and the preconditions to , are impossible to encode without traits—and this often comes up in other languages, for example printing a function in clojure shows something like #object[clojure.core$map 0x2e7de98a "clojure.core$map@2e7de98a"]. In Rust it gives a compile error unless you opt-in with Debug. /  are only possible to enforce because the borrow checker does capture analysis for closures. Java, which is  committed to thread-safety by the standards of most languages, cannot verify this at compile time and so has to document synchronization concerns explicitly instead.There are more interplays than I can easily describe in a post, and all of them are what make Rust what it is.Rust has other excellent language features—for example the inline assembly syntax is a work of art, props to Amanieu. But they are not interwoven into the standard library in the same way, and they do not affect the way people  about writing code in the same way.without.boats wrote a post in 2019 titled "Notes on a smaller Rust" (and a follow-up revisiting it). In a manner of speaking, that smaller Rust  the language I fell in love with when I first learned it in 2018. Rust is a lot bigger today, in many ways, and the smaller Rust is just a nostalgic rose-tinted memory. But I think it's worth studying as an example of how well orthogonal features can compose when they're designed as one cohesive whole.]]></content:encoded></item><item><title>Bank forced to rehire workers after lying about chatbot productivity, union says</title><link>https://arstechnica.com/tech-policy/2025/08/bank-forced-to-rehire-workers-after-lying-about-chatbot-productivity-union-says/</link><author>ndsipa_pomu</author><category>hn</category><pubDate>Thu, 21 Aug 2025 15:58:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I forced every engineer to take sales calls and they rewrote our platform</title><link>https://old.reddit.com/r/Entrepreneur/comments/1mw5yfg/forced_every_engineer_to_take_sales_calls_they/</link><author>bilsbie</author><category>hn</category><pubDate>Thu, 21 Aug 2025 15:46:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Unmasking the Privacy Risks of Apple Intelligence</title><link>https://www.lumia.security/blog/applestorm</link><author>mroi</author><category>hn</category><pubDate>Thu, 21 Aug 2025 15:36:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Lumia’s Research Team revealed that messages dictated via Siri, including WhatsApp and iMessage are not sent to the Private Cloud Compute. In fact, there is no assurance as to what Apple does with these messages.Siri transmits metadata about installed and active apps without the user’s ability to control these privacy settings.Audio playback metadata such as ‘recording names’, is sent without consent. No user control or visibility exists over these background data flows.Apple uses two distinct privacy policies (Siri vs. Apple Intelligence), meaning similar queries may fall under different data-handling rules.We reveal AppleStorm, our investigation into how Apple AI’s eco-system quietly transmits messages (WhatsApp, iMessage) sent via Siri to Apple servers, even when it isn’t needed to complete the task. This happens without the user having any control whatsoever over when and what can be sent.Also more data than messages is sent to Siri’s servers. Let’s deep dive.Lately, Apple’s AI has been making headlines. From promising robust security measures to developing localized models that process data directly on devices, Apple has positioned itself as a champion of privacy and productivity.How safe are these innovations? Despite the numerous advancements, recent news has highlighted critical concerns about Apple’s AI suite. A lawsuit regarding Siri eavesdropping, settled last January, raises questions about user privacy. More recently, allegations surfaced that Apple Intelligence generated false notifications, including a summary of BBC news with inaccurate information on behalf of the BBC.When Apple launched Apple Intelligence which relied on on-device and cloud models (Private Cloud Compute) they introduced a variety of AI tools such as Writing Tools, Image Background and Siri is even more powerful with the capabilities of Apple Intelligence.Before technologies like these get out of control, it's crucial to identify any potential security loopholes we might be overlooking. This blog explores privacy risks and unusual behaviors discovered regarding Siri and intersection of Siri and Apple Intelligence.Apple Intelligence relies on a hybrid infrastructure that combines  with Apple-managed cloud servers to provide AI-driven features while maintaining strong privacy protections. The cloud component primarily operates through Private Cloud Compute (PCC).But Siri also interacts with other Apple’s server infrastructures that are outside of the Apple Intelligence’s PCC.Private Cloud Compute (PCC)Private Cloud Compute is Apple's secure AI processing framework that extends Apple Intelligence beyond the capabilities of on-device computing. PCC Server domains: apple-relay.cloudflare.com, apple-relay.fastly-edge.com, cp4.cloudflare.com Apple’s Private Cloud Compute architecture.Siri’s Dictation ServersSiri works closely with dictation servers which appear to handle voice processing tasks.Siri’s Dication domain: guzzoni.apple.comSiri’s Search ServicesSiri integrates with Apple’s search infrastructure to deliver relevant results across devices, like Spotlight Search, Safari Smart Search, News, and Music.Siri’s Search domain: *.smoot.apple.comUse Case-Driven CollectionThe services to communicate with Apple Intelligence’s Extensions. Today, only ChatGPT is supported Apple Intelligence’s Extensions domain: apple-relay.apple.comCreating a Research EnvironmentOperating System & PlatformApple Intelligence enabledmitmproxy – An interactive HTTPS proxy.Frida – A dynamic instrumentation toolkit.What is the Weather Today?To investigate, the open-source proxy tool  was used to intercept Siri’s network traffic. Initially, simple prompts such as "Hello", "What can you do?" and "What is the time right now?" were tested, but no network activity was observed, suggesting that these queries could be handled locally without requiring external communication.Since some queries might be handled locally, it was necessary to issue a prompt that would require external evaluation. Testing the query "What is the weather today?" resulted in the transmission of two packets.The first packet was directed to  identified as Apple's dictation server, but we can’t see any content.The second packet, sent to api-glb-aeun1a.smoot.apple.com, the search service for Siri. Let’s start with this one.After extracting and decompressing the frame, the  Python library was used to parse the ProtoBuf data. Without access to the original.proto files used to compile the data, the analysis required a best-effort approach to interpret the content. Let’s see what we can find.The first app listed was the official Apple Weather app, which was expected. However, the second one raised questions—it appeared to be related to  a virtual desktop Mac application that enables running the Windows operating system. A review of the Parallels settings on the device, including the applications running within the virtual machine, showed the following information:This turned out to be the ID of the official Windows Weather app installed on the virtual machine.Based on this observation, it appears that Siri scans the device for apps related to the prompt’s topic. To test this theory, a question about emails was entered, and the packet revealed a list of all email clients installed on the device.So, it’s confirmed — Siri actively searches for apps on the device that match the topic of the prompt, and reports them back to Apple.A question arises: how does Siri determine the weather without the location being mentioned in the prompt? A review of the ProtoBuf content revealed two numbers next to the apps list that appear to be coordinates.A quick Google search confirmed this theory, showing the cafe I was working from.According to Siri’s Privacy Policy (Notice it's Siri's privacy policy and not Apple Intelligence. More on that soon), they specifically mention this behavior. In fact, it is possible to disable location sharing information with Siri, but if you do consent to location sharing, your location will be appended to every request (regardless of the necessity).Once again, personal information is being leaked. This raises the question of what else Apple knows about our habits.During the investigation of the weather request packet, the following content was encountered:Initially, the appearance of a confidential document title from the Notion app in Siri’s traffic was surprising.Once again, personal information is being leaked. This raises the question of what else Apple knows about our habits. We still have other traffic that we cannot see. Maybe Apple uses some pinning mechanism…What is Certificate Pinningis an additional security measure used to protect against machine-in-the-middle (MITM) attacks and fraudulent certificates. Instead of relying solely on the Certificate Authority system, pinning allows a website or application to specify which exact certificate or public key should be trusted when connecting to a particular domain. This prevents attackers from tricking users into accepting a malicious certificate issued by a compromised or unauthorized CA.Bypassing the Certificate PinningUnder macOS Sequoia’s SSL pinning mechanism, each SSL context uses a dedicated verification routine. To bypass the pinning, we could modify the behavior to skip the verification. To achieve this, we must alter Apple’s built-in verification logic by hooking the relevant symbols in Siri’s process - assistantd.Using Frida, we created this script to disable the pinning.Among the recognizable strings in the ACE packet, many correspond to familiar application names, clearly indicating that these are the apps currently active on the device.So, it’s cleared — Siri actively looks for open apps on the device, and reports them back to Apple’s servers.End-to-End Encryption? I’m Not SureOne of the main features of Siri is the ability to interact with different apps on the device, like summarizing notes, composing emails and more.Siri has the functionality to send messages to contacts in WhatsApp. For example, you can ask Siri, by voice or text (Using Apple Intelligence), “Send the message Good morning to Bob” and it will send the message “Good morning” to a contact named Bob.While recording the traffic of Siri while sending a message with Siri, a single, pinned packet is generated. Extracting it shows that it contains a base64 string. Decoding it revealed the following:Siri indeed  the content of the WhatsApp message, the recipient phone number and other identifiers back to Apple’s servers.This raises a significant concern about WhatsApp’s end-to-end encryption, as the messages are apparently leaving the device even though they were sent from the device itself.Initially, I suspected this might be due to the Siri and Apple Intelligence settings that allow it to “learn” from certain apps; however, the behaviour persists even when not permitting them to learn from WhatsApp.However, sending a message to WhatsApp via Siri also works when blocking communication between Siri to Apple’s servers. The heavy processing already occurred (reasoning the target application is WhatsApp, who the recipient is, and the message to send) so why is it sent externally, and to a server that isn’t part of Private Cloud Compute? If all data resides locally, why is there any need to send it to their servers?So, it’s clear — Siri has access to your WhatsApp messages sent via Siri (but not to messages sent outside of Siri), and reports them back to Apple’s servers, regardless of whether permission is granted for learning from them.What Happens When It Comes to ChatGPTDisclosure Process and Apple’s ResponseWe informed Apple of this privacy issue in February 2025. In March 2025, after some back and forth with their security team and sending additional information, Apple acknowledged the issue and said they will be working towards a fix.However, in July 2025, Apple reached out to say that this is actually not a privacy issue related to Apple Intelligence. Rather it’s a privacy issue related to the usage of third party services that rely on Siri. For example, the misuse of SiriKit (Siri’s extension that allows third party integrations to Siri) by WhatsApp.Via Apple Intelligence, users can interact with ChatGPT through Siri and Writing Tools. For example, Siri can tap into ChatGPT to provide answers when that might be helpful for certain requests.According to Apple, every request to ChatGPT is routed through Apple Intelligence’s Extensions service. This happens also if you use your private or business account.However, when using Siri, these requests are duplicated: requests are sent to the extensions’s service and to Siri servers. The request to Siri’s servers is redundant for the user.Disclosure Process and Apple’s ResponseWe informed Apple of this privacy issue in February 2025. In March 2025, after some back and forth with their security team and sending additional information, Apple acknowledged the issue and said they will be working towards a fix.However, in July 2025, Apple reached out to say that this is actually not a privacy issue related to Apple Intelligence. Rather it’s a privacy issue related to the usage of third party services that rely on Siri. For example, the misuse of SiriKit (Siri’s extension that allows third party integrations to Siri) by WhatsApp.After a quick overview, we saw the same behavior on iMessage as that of WhatsApp when sending a message via Siri. So, before reaching WhatsApp on this issue I took this matter deeply. I built an app, straight from Apple’s documentation, that uses SiriKit with messaging integration, and I was amazed to see the same behavior also happening in this test.Apple also explicitly mentioned that Siri’s servers are not part of Apple’s Private Cloud Compute. It seems that there are features that belong to Siri core and other features that are based on Apple Intelligence. The user, however, is unaware of which is used when.Two Privacy Policies, Two Practices, Same AppHerein the privacy issues become even further complex: when you dictate a message to Siri, two underlying flows might happen - you might be using Siri flows, but you may also be using Apple Intelligence flows. As a user, you have no idea if Siri will take on its flow, and when it will take on its Apple Intelligence flow.For example, the query: “Hey Siri, what is the weather today?” will send the data to Siri servers. However, the query: “Hey Siri, ask ChatGPT what is the weather today?” will send the message to Apple Intelligence’s Private Cloud Compute. Two similar questions, two different traffic flows, two different privacy policies.For organizations managing Apple devices, we recommend taking on a three-prong approach:Firewall and network restrictions on Siri domainsSet your firewall to block any network traffic to Siri’s dictation domain, guzzoni.apple.com. This should not hinder any Siri functionalityFor stricter environments, setting your firewall to also block Siri’s search domain smoot.apple.com and its subdomains is also possible, though it will impair Siri capabilities.Apple device and knowledge sharing policiesDisable any “Learn from this app” settings to be on the safe side.Network traffic analysis and monitoringCreate a policy on AI usage for employees and place necessary controls for enforcement.Monitor your network traffic to gain visibility into AI usage across the organization. With this visibility, you can hone your controls to ensure compliance with the defined policy.On a smaller, personal scale, this means that sensitive data is leaving your Apple device unnecessarily, shattering your privacy expectations and giving you limited, perhaps even zero, control over your data.For enterprises, it raises serious compliance and security risks when sensitive corporate information leaks outside the organization’s network.This leads to a wider discussion, as this issue goes beyond just Apple and its incorporation of AI. AI capabilities are now all around us. Any typical app these days incorporates AI, whether it’s Grammarly, Canva or Salesforce. Knowing when a feature is powered by AI or not, is not really trivial anymore, and it requires understanding of the technical flow of the feature — something that a typical user has no time, skill or desire to do.At Lumia, our mission is to help organizations adopt AI safely, responsibly, and governed — without disrupting productivity.Apple Intelligence is just one aspect in the world of AI. However, the findings of our AppleStorm research highlight why visibility and control are essential in the age of AI-powered productivity tools. Lumia’s platform provides full visibility into AI-related data flows across thousands of applications and protocols, including hidden behaviors like those we uncovered in Siri.]]></content:encoded></item><item><title>95% of Companies See &apos;Zero Return&apos; on $30B Generative AI Spend</title><link>https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/</link><author>speckx</author><category>hn</category><pubDate>Thu, 21 Aug 2025 15:36:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Beyond sensor data: Foundation models of behavioral data from wearables</title><link>https://arxiv.org/abs/2507.00191</link><author>brandonb</author><category>hn</category><pubDate>Thu, 21 Aug 2025 14:39:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Unity reintroduces the Runtime Fee through its Industry license</title><link>https://unity.com/products/unity-industry</link><author>finnsquared</author><category>hn</category><pubDate>Thu, 21 Aug 2025 14:31:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Deliver immersive applications across AR, VR, web, mobile, and desktop platforms. Reach your audience with seamless multi-platform support, leverage cutting-edge devices, and create interactive experiences that engage customers and stakeholders wherever they are.]]></content:encoded></item><item><title>In a first, Google has released data on how much energy an AI prompt uses</title><link>https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/</link><author>jeffbee</author><category>hn</category><pubDate>Thu, 21 Aug 2025 13:52:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[So some Gemini prompts use much more energy than this: Dean gives the example of feeding dozens of books into Gemini and asking it to produce a detailed synopsis of their content. “That’s the kind of thing that will probably take more energy than the median prompt,” Dean says. Using a reasoning model could also have a higher associated energy demand because these models take more steps before producing an answer.This report was also strictly limited to text prompts, so it doesn’t represent what’s needed to generate an image or a video. (Other analyses, including one in ’s Power Hungry series earlier this year, show that these tasks can require much more energy.)The report also finds that the total energy used to field a Gemini query has fallen dramatically over time. The median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025, according to Google. The company points to advancements in its models and other software optimizations for the improvements.  Google also estimates the greenhouse gas emissions associated with the median prompt, which they put at 0.03 grams of carbon dioxide. To get to this number, the company multiplied the total energy used to respond to a prompt by the average emissions per unit of electricity.Rather than using an emissions estimate based on the US grid average, or the average of the grids where Google operates, the company instead uses a market-based estimate, which takes into account electricity purchases that the company makes from clean energy projects. The company has signed agreements to buy over 22 gigawatts of power from sources including solar, wind, geothermal, and advanced nuclear projects since 2010. Because of those purchases, Google’s emissions per unit of electricity on paper are roughly one-third of those on the average grid where it operates.AI data centers also consume water for cooling, and Google estimates that each prompt consumes 0.26 milliliters of water, or about five drops. The goal of this work was to provide users a window into the energy use of their interactions with AI, Dean says. “People are using [AI tools] for all kinds of things, and they shouldn’t have major concerns about the energy usage or the water usage of Gemini models, because in our actual measurements, what we were able to show was that it’s actually equivalent to things you do without even thinking about it on a daily basis,” he says, “like watching a few seconds of TV or consuming five drops of water.”]]></content:encoded></item><item><title>Show HN: ChartDB Cloud – Visualize and Share Database Diagrams</title><link>https://app.chartdb.io/</link><author>Jonathanfishner</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 13:01:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How well does the money laundering control system work?</title><link>https://www.journals.uchicago.edu/doi/10.1086/735665</link><author>PaulHoule</author><category>hn</category><pubDate>Thu, 21 Aug 2025 12:58:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AWS CEO says using AI to replace junior staff is &apos;Dumbest thing I&apos;ve ever heard&apos;</title><link>https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/</link><author>JustExAWS</author><category>hn</category><pubDate>Thu, 21 Aug 2025 12:53:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is "the dumbest thing I've ever heard."Garman made that remark in conversation with AI investor Matthew Berman, during which he talked up AWS’s Kiro AI-assisted coding tool and said he's encountered business leaders who think AI tools "can replace all of our junior people in our company."That notion led to the “dumbest thing I've ever heard” quote, followed by a justification that junior staff are “probably the least expensive employees you have” and also the most engaged with AI tools.“How's that going to work when ten years in the future you have no one that has learned anything,” he asked. “My view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.”Naturally he thinks AI – and Kiro, natch – can help with that education.Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization.“It’s a silly metric,” he said, because while organizations can use AI to write “infinitely more lines of code” it could be bad code.“Often times fewer lines of code is way better than more lines of code,” he observed. “So I'm never really sure why that's the exciting metric that people like to brag about.”That said, he’s seen data that suggests over 80 percent of AWS’s developers use AI in some way.“Sometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow” in which developers collaborate with AI agents.Garman said usage of AI tools by AWS developers increases every week.The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn – and not just learn specific skills.“I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?”Garman thinks that approach is necessary because technological development is now so rapid it’s no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach “how do you think and how do you decompose problems”, and thinks kids who acquire those skills will thrive. ®]]></content:encoded></item><item><title>Privately-Owned Rail Cars</title><link>https://www.amtrak.com/privately-owned-rail-cars</link><author>jasoncartwright</author><category>hn</category><pubDate>Thu, 21 Aug 2025 12:31:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How does the US use water?</title><link>https://www.construction-physics.com/p/how-does-the-us-use-water</link><author>juliangamble</author><category>hn</category><pubDate>Thu, 21 Aug 2025 12:21:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[decades of droughtincreasing concernheavy users of waterBecause water is such a critical resource, needed for everything from agriculture to manufacturing to artificial intelligence to sustaining basic human life, it's worth understanding how we use water, and how that use has changed over time.water cycleAltogether, the US receives about 5 trillion gallons of precipitation a day. Most of that (63%) gets returned to the atmosphere via evapotranspiration. Much of the rest ultimately flows into the Gulf of Mexico (11%), Pacific Ocean (6%), and Atlantic Ocean (2%). About 10% gets stored in surface bodies of water (lakes, reservoirs) or underground aquifers, and 6% flows back into Canada. The remaining 2% is consumed by people in various ways.large-diameter pipesUSGS reportbrackishAn important distinction when understanding water use is “consumptive” vs “non-consumptive” uses. Consumptive water use is when the water is consumed as part of the process: either it gets incorporated into whatever is being produced, evaporates back into the atmosphere, or is otherwise no longer available in fluid form. Non-consumptive water use is when the water is still available to use in fluid form after the process is completed, though perhaps at a higher temperature, or with some additional pollutants or impurities. The graph below shows total water use by category, broken out into consumptive and non-consumptive uses:condense steam back to liquid waterused once-through coolingAfter power plants, the next largest use of water in the US is for irrigation: watering plants and crops. Altogether, irrigation makes up 37% of total US water use. Unlike thermoelectric power plants, most of this water (just over 60%) is consumptive use. What’s more, even the irrigation water that isn’t consumed may not be readily reused. While thermoelectric plants which typically dump their water back into the same river, lake, or ocean it was drawn from, much of the non-consumptive water used for irrigation seeps back into the ground, potentially far from where it was originally tapped. Depending on how water flows through the ground in a particular location, it could be centuries or even longer before that water reaches somewhere it can be easily retrieved.337 million acres of cropland54% of the value of US crops16,000 golf coursesAfter irrigation, the next largest category of US water use is public utility supplied water for homes and businesses. Public water use totals to around 39 billion gallons a day, or 12% of total US water use. Most public water — 23.3 billion gallons, or around 60% — is for domestic use in homes. In addition to publicly supplied water for homes, another 3.3 billion gallons are self-supplied to homes via things like privately owned wells. An estimated 42 million people in the US use self-supplied water.The next largest category of water use is industry, which uses about 14.8 billion gallons of water a day, or 4.5% of all US water use). Many industrial processes require large amounts of water, often either for cooling or as a way to carry and mix materials or chemicals. The chart below shows several US industries that use large amounts of water.rinsing the pulp12% of the water used is consumptiveonly around 10%Lawrence Berkeley Labevaporate as part of the cooling processThis is a large amount of water when compared to the amount of water homes use, but it's not particularly large when compared to other large-scale industrial uses. 66 million gallons per day is about 6% of the water used by US golf courses, and it's about 3% of the water used to grow cotton in 2023.six billion pounds of cotton in 2023$4.5 billion$5 billion dollars every yearThe remaining users of water in the US are aquaculture (fish farms) at 7.5 billion gallons a day (2.3% of all water use), mining at 4 billion gallons a day (1.2%), and water for livestock at 2 billion gallons a day (0.6%).How does US water use vary by geography? The map below shows total water consumption by state:California is the number one water consumer at 28.8 billion gallons per day, followed by Texas (21.3 billion), Idaho (17.7 billion), Florida (15.3 billion) and Arkansas (13.8). The large consumers of water are states that either have a lot of irrigated land (California, Idaho, Arkansas), have a lot of thermal power plant cooling (Florida), or both (Texas).Because this lumps in such disparate uses of water, it's more illuminating to break this down by category. The map below shows water used for irrigation by state.And this map shows a more granular view of where, specifically, irrigated acres are located.Most irrigated acres are in the western half of the US. This isn’t that surprising, as western states get far less precipitation than eastern states. The map below shows average yearly precipitation by state (averaged between the years 1971 and 2000).Average annual rainfall is around 45.6 inches per year in the eastern half of the US, compared to 21 inches per year in the western half.The amount of water an individual state has available for productive use is not simply a portion of all the precipitation it receives (due to movement of water through rivers, streams and aquifers), but it's nevertheless interesting to see irrigation water use as a fraction of a state’s total precipitation.In most states, irrigation water is equivalent to a tiny fraction of total precipitation: in 30 states it's less than 1%. But in some western states it's much higher. In Idaho water used for irrigation is equivalent to more than 20% of all the precipitation the state receives.Much of the water used for irrigation — roughly half — is pumped from underground aquifers. In many places in the US, groundwater is being pumped out faster than it recharges from precipitation, leading to gradual depletion of the aquifer.(Interestingly, this map shows groundwater recharging in an Idaho aquifer, despite Idaho being such a large user of irrigation water. Possibly this is explained by the time period difference, as this map only goes to 2008.)Other than irrigation, the largest category of water use in the US is cooling at thermoelectric power plants. Here’s water used by thermoelectric power plants, by state and by county.Most water used for powerplant cooling is in the eastern half of the country (as well as Texas), as that’s where most power plants are.The next largest category of water use is public water consumption. Because public water mostly consists of water used in homes, I’ve also included self-supplied water used by homes. The maps below show public and self-supplied water use by state.Because public and self-supplied water use is a function of population, this map is basically a population map. So let’s also look at public and self-supplied water use per capita.There’s less variation here than I expected. The highest consuming states (Utah, Idaho) use only about two times the amount of water per capita as the lowest consuming states.If we look at self-supplied water use specifically, we can see that it's highly regionally concentrated. In some areas of the country, self-supplied water makes up nearly all domestic water use, while in other areas of the country it's much less common.The next largest category of water use is industrial. The map below shows industrial water use by county:Industrial water consumption is concentrated in a very small number of locations: northern Indiana for steel production, Louisiana and the Gulf Coast for oil refining, and so on. 100 of the US’s 3200 counties are responsible for more than 70% of the US’s industrial water consumption, and 23% of US industrial water use is concentrated in just five counties.How have patterns of water consumption changed over time? Overall water use peaked in 1980, and has trended downward since then.Another trend from the 1950s through the 1970s was increased use of groundwater. Between 1950 and 1980 the annual volume of groundwater used in the US more than doubled, from 34 billion gallons per day to 83 billion gallons per day. Since then, groundwater use has been roughly constant (as of 2015, it’s at 82.3 billion gallons per day), even as surface water use has declined by nearly 30%. Groundwater is thus making up an increasingly large fraction of overall water use.My overall takeaway is that you need to be careful when talking about water use: it’s very easy to take figures out of context, or make misleading comparisons. Very often I see alarmist discussions about water use that don’t take into account the distinction between consumptive and non-consumptive uses. Billions of gallons of water a day are “used” by thermal power plants, but the water is quickly returned to where it came from, so this use isn’t reducing the supply of available fresh water in any meaningful sense. Similarly, the millions of gallons of water a day a large data center can use sounds like a lot when compared to the hundreds of gallons a typical home uses, but compared to other large-scale industrial or agricultural uses of water, it's a mere drop in the bucket.On the other hand, it’s also easy to go too far in the other direction. People sometimes invoke the idea that water moves through a cycle and never really gets destroyed, in order to suggest that we don’t need to be concerned at all about water use. But while water may not get destroyed, it can get “used up” in the sense that it becomes infeasible or uneconomic to access it. If we draw down aquifers that have spent hundreds or thousands of years accumulating water, the water isn’t “gone” in the sense that it's been chemically transformed back into hydrogen and oxygen, but much of it probably ends up in the atmosphere, the ocean, or other places not readily or cheaply retrievable. Outside of large-scale desalination of ocean water, we’re ultimately limited in how much fresh water we can use by the amount of precipitation we get, and while we currently use a very small fraction of total precipitation (around 6%), it’s not as if there’s no limit to how much naturally-produced fresh water is available.]]></content:encoded></item><item><title>Weaponizing image scaling against production AI systems</title><link>https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/</link><author>tatersolid</author><category>hn</category><pubDate>Thu, 21 Aug 2025 12:20:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution.In this blog post, we’ll detail how attackers can exploit image scaling on Gemini CLI, Vertex AI Studio, Gemini’s web and API interfaces, Google Assistant, Genspark, and other production AI systems. We’ll also explain how to mitigate and defend against these attacks, and we’ll introduce Anamorpher, our open-source tool that lets you explore and generate these crafted images.: Image scaling attacks were used for model backdoors, evasion, and poisoning primarily against older computer vision systems that enforced a fixed image size. While this constraint is less common with newer approaches, the systems surrounding the model may still impose constraints calling for image scaling. This establishes an underexposed, yet widespread vulnerability that we’ve weaponized for multi-modal prompt injection.Data exfiltration on the Gemini CLITo set up our data exfiltration exploit on the Gemini CLI through an image-scaling attack, we applied the default configuration for the Zapier MCP server. This automatically approves all MCP tool calls without user confirmation, as it sets  in the  of the Gemini CLI. This provides an important primitive for the attacker.Figure 2 showcases a video of the attack. First, the user uploads a seemingly benign image to the CLI. With no preview available, the user cannot see the transformed, malicious image processed by the model. This image and its prompt-ergeist triggers actions from Zapier that exfiltrates user data stored in Google Calendar to an attacker’s email without confirmation.We also successfully demonstrated image scaling attacks on the following:Vertex AI with a Gemini back endGemini’s API via the  CLIGoogle Assistant on an Android phoneNotice the persistent mismatch between user perception and model inputs in figures 3 and 4. The exploit is particularly impactful on Vertex AI Studio because the front-end UI shows the high-resolution image instead of the downscaled image perceived by the model.Our testing confirmed that this attack vector is widespread, extending far beyond the applications and systems documented here.Sharpening the attack surfaceThese image scaling attacks exploit downscaling algorithms (or image resampling algorithms), which perform interpolation to turn multiple high resolution pixel values into a single low resolution pixel value.There are three major downscaling algorithms: nearest neighbor interpolation, bilinear interpolation, and bicubic interpolation. Each algorithm requires a different approach to perform an image scaling attack. Furthermore, these algorithms are implemented differently across libraries (e.g., Pillow, PyTorch, OpenCV, TensorFlow), with varying anti-aliasing, alignment, and kernel phases (in addition to distinct bugs that historically have plagued model performance). These differences also impact the techniques necessary for an image scaling attack. Therefore, exploiting production systems required us to fingerprint each system’s algorithm and implementation.To understand why image downscaling attacks are possible, imagine that you have a long ribbon with an intricate yet regular pattern on it. As this ribbon is pulled past you, you’re trying to recreate the pattern by grabbing samples of the ribbon at regular intervals. If the pattern changes rapidly, you need to grab samples very frequently to capture all the details. If you’re too slow, you’ll miss crucial parts between grabs, and when you try to reconstruct the pattern from your samples, it looks completely different from the original.Anamorpher and the attacker’s darkroomCurrently, Anamorpher (named after anamorphosis) can develop crafted images for the aforementioned three major methods. Let’s explore how Anamorpher exploits bicubic interpolation frame by frame.Bicubic interpolation considers the 16 pixels (from 4x4 sampling) around each target pixel, using cubic polynomials to calculate smooth transitions between pixel values. This method creates a predictable mathematical relationship that can be exploited. Specifically, the algorithm assigns different weights to pixels in the neighborhood, creating pixels that contribute more to the final output, which are known as high-importance pixels. Therefore, the total luma (brightness) of dark areas of an image will increase if specific high-importance pixels are higher luma than their surroundings.Therefore, to exploit this, we can carefully craft high-resolution pixels and solve the inverse problem. First, we select a decoy image with large dark areas to hide our payload. Then, we adjust pixels in dark regions and push the downsampled result toward a red background using least-squares optimization. These adjustments in the dark areas cause the background to turn red while text areas remain largely unmodified and appear black, creating much stronger contrast than visible at full resolution. While this approach is most effective on bicubic downscaling, it also works on specific implementations of bilinear downscaling.Anamorpher provides users with the ability to visualize and craft image scaling attacks against specific algorithms and implementations through a front-end interface and Python API. In addition, it comes with a modular back end, which enables users to customize their own downscaling algorithm.While some downscaling algorithms are more vulnerable than others, attempting to identify the least vulnerable algorithm and implementation is not a robust approach. This is especially true since image scaling attacks are not restricted to the aforementioned three algorithms.For a secure system, we recommend not using image downscaling and simply limiting the upload dimensions. For any transformation, but especially if downscaling is necessary, the end user should always be provided with a preview of the input that the model is actually seeing, even in CLI and API tools.Anamorpher is currently in beta, so feel free to reach out with feedback and suggestions as we continue to improve this tool. Stay tuned for more work on the security of multi-modal, agentic, and multi-agentic AI systems!]]></content:encoded></item><item><title>Show HN: Using Common Lisp from Inside the Browser</title><link>https://turtleware.eu/posts/Using-Common-Lisp-from-inside-the-Browser.html</link><author>jackdaniel</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 12:08:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[ Written on 2025-08-21 by Daniel Kochmański Web Embeddable Common Lisp is a project that brings Common Lisp and the Web
Browser environments together. In this post I'll outline the current progress of
the project and provide some technical details, including current caveats and
future plans.It is important to note that this is not a release and none of the described
APIs and functionalities is considered to be stable. Things are still changing
and I'm not accepting bug reports for the time being.The easiest way to use Common Lisp on a website is to include WECL and insert
script tags with a type "text/common-lisp". When the attribute src is present,
then first the runtime loads the script from that url, and then it executes the
node body. For example create and run this HTML document from localhost:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="https://turtleware.eu/static/misc/wecl-20250821/easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
  </head>
  <body>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp" id='easy-script'>
(defvar *div* (make-element "div" :id "my-ticker"))
(append-child [body] *div*)

(dotimes (v 4)
  (push-counter v))

(loop for tic from 6 above 0
      do (replace-children *div* (make-paragraph "~a" tic))
         (js-sleep 1000)
      finally (replace-children *div* (make-paragraph "BOOM!")))

(show-script-text "easy-script")
    </script>
  </body>
</html>
We may use Common Lisp that can call to JavaScript, and register callbacks to be
called on specified events. The source code of the script can be found here:Because the runtime is included as a script, the browser will usually cache the
~10MB WebAssembly module.The initial foreign function interface has numerous macros defining wrappers
that may be used from Common Lisp or passed to JavaScript.Summary of currently available operators: an inlined expression, like  an object referenced from the object store a function a method of the argument, like  a slot reader of the argument a slot writer of the first argument combines define-js-getter and define-js-setter template for JavaScript expressions Common Lisp function reference callable from JavaScript anonymous Common Lisp function reference (for closures)Summary of argument types:Common Lisp object referenceJavaScript object referenceAll operators, except for  have a similar lambda list:(DEFINE-JS NAME-AND-OPTIONS [ARGUMENTS [,@BODY]])The first argument is a list  that is common to all
defining operators: Common Lisp symbol denoting the object a string denoting the JavaScript expression, i.e "innerText" a type of the object returned by executing the expression(define-js-variable ([document] :js-expr "document" :type :symbol))
;; document
(define-js-object ([body] :js-expr "document.body" :type :js-ref))
;; wecl_ensure_object(document.body) /* -> id   */
;; wecl_search_object(id)            /* -> node */
The difference between a variable and an object in JS-FFI is that variable
expression is executed each time when the object is used (the expression is
inlined), while the object expression is executed only once and the result is
stored in the object store.The second argument is a list of pairs . Names will be used in the
lambda list of the operator callable from Common Lisp, while types will be used
to coerce arguments to the type expected by JavaScript.(define-js-function (parse-float :js-expr "parseFloat" :type :js-ref)
    ((value :string)))
;; parseFloat(value)

(define-js-method (add-event-listener :js-expr "addEventListener" :type :null)
    ((self :js-ref)
     (name :string)
     (fun :js-ref)))
;; self.addEventListener(name, fun)

(define-js-getter (get-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)))
;; self.innerText

(define-js-setter (set-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText = new

(define-js-accessor (inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText
;; self.innerText = new

(define-js-script (document :js-expr "~a.forEach(~a)" :type :js-ref)
    ((nodes :js-ref)
     (callb :object)))
;; nodes.forEach(callb)
The third argument is specific to callbacks, where we define Common Lisp body of
the callback. Argument types are used to coerce values from JavaScript to Common
Lisp.(define-js-callback (print-node :type :object)
    ((elt :js-ref)
     (nth :fixnum)
     (seq :js-ref))
  (format t "Node ~2d: ~a~%" nth elt))

(let ((start 0))
  (add-event-listener *my-elt* "click"
                      (lambda-js-callback :null ((event :js-ref)) ;closure!
                        (incf start)
                        (setf (inner-text *my-elt*)
                              (format nil "Hello World! ~a" start)))
Note that callbacks are a bit different, because  does not
accept  option and  has unique lambda list. It is
important for callbacks to have an exact arity as they are called with, because
JS-FFI does not implement variable number of arguments yet.Callbacks can be referred by name with an operator .While working on FFI I've decided to write an adapter for SLIME/SWANK that will
allow interacting with WECL from Emacs. The principle is simple: we connect with
a websocket to Emacs that is listening on the specified port (i.e on localhost).
This adapter uses the library  written by Andrew Hyatt.It allows for compiling individual forms with , but file compilation
does not work (because files reside on a different "host"). REPL interaction
works as expected, as well as SLDB. The connection may occasionally be unstable,
and until Common Lisp call returns, the whole page is blocked. Notably waiting
for new requests is not a blocking operation from the JavaScript perspective,
because it is an asynchronous operation.;;; Patches for SLIME 2.31 (to be removed after the patch is merged).
;;; It is assumed that SLIME is already loaded into Emacs.
(defun slime-net-send (sexp proc)
  "Send a SEXP to Lisp over the socket PROC.
This is the lowest level of communication. The sexp will be READ and
EVAL'd by Lisp."
  (let* ((payload (encode-coding-string
                   (concat (slime-prin1-to-string sexp) "\n")
                   'utf-8-unix))
         (string (concat (slime-net-encode-length (length payload))
                         payload))
         (websocket (process-get proc :websocket)))
    (slime-log-event sexp)
    (if websocket
        (websocket-send-text websocket string)
      (process-send-string proc string))))

(defun slime-use-sigint-for-interrupt (&optional connection)
  (let ((c (or connection (slime-connection))))
    (cl-ecase (slime-communication-style c)
      ((:fd-handler nil) t)
      ((:spawn :sigio :async) nil))))
;;; lime.el --- Lisp Interaction Mode for Emacs -*-lexical-binding:t-*-
;;; 
;;; This program extends SLIME with an ability to listen for lisp connections.
;;; The flow is reversed - normally SLIME is a client and SWANK is a server.

(require 'websocket)

(defvar *lime-server* nil
  "The LIME server.")

(cl-defun lime-zipit (obj &optional (start 0) (end 72))
  (let* ((msg (if (stringp obj)
                  obj
                (slime-prin1-to-string obj)))
         (len (length msg)))
    (substring msg (min start len) (min end len))))

(cl-defun lime-message (&rest args)
  (with-current-buffer (process-buffer *lime-server*)
    (goto-char (point-max))
    (dolist (arg args)
      (insert (lime-zipit arg)))
    (insert "\n")
    (goto-char (point-max))))

(cl-defun lime-client-process (client)
  (websocket-conn client))

(cl-defun lime-process-client (process)
  (process-get process :websocket))

;;; c.f slime-net-connect
(cl-defun lime-add-client (client)
  (lime-message "LIME connecting a new client")
  (let* ((process (websocket-conn client))
         (buffer (generate-new-buffer "*lime-connection*")))
    (set-process-buffer process buffer)
    (push process slime-net-processes)
    (slime-setup-connection process)
    client))

;;; When SLIME kills the process, then it invokes LIME-DISCONNECT hook.
;;; When SWANK kills the process, then it invokes LIME-DEL-CLIENT hook.
(cl-defun lime-del-client (client)
  (when-let ((process (lime-client-process client)))
    (lime-message "LIME client disconnected")
    (slime-net-sentinel process "closed by peer")))

(cl-defun lime-disconnect (process)
  (when-let ((client (lime-process-client process)))
    (lime-message "LIME disconnecting client")
    (websocket-close client)))

(cl-defun lime-on-error (client fun error)
  (ignore client fun)
  (lime-message "LIME error: " (slime-prin1-to-string error)))

;;; Client sends the result over a websocket. Handling responses is implemented
;;; by SLIME-NET-FILTER. As we can see, the flow is reversed in our case.
(cl-defun lime-handle-message (client frame)
  (let ((process (lime-client-process client))
        (data (websocket-frame-text frame)))
    (lime-message "LIME-RECV: " data)
    (slime-net-filter process data)))

(cl-defun lime-net-listen (host port &rest parameters)
  (when *lime-server*
    (error "LIME server has already started"))
  (setq *lime-server*
        (apply 'websocket-server port
               :host host
               :on-open    (function lime-add-client)
               :on-close   (function lime-del-client)
               :on-error   (function lime-on-error)
               :on-message (function lime-handle-message)
               parameters))
  (unless (memq 'lime-disconnect slime-net-process-close-hooks)
    (push 'lime-disconnect slime-net-process-close-hooks))
  (let ((buf (get-buffer-create "*lime-server*")))
    (set-process-buffer *lime-server* buf)
    (lime-message "Welcome " *lime-server* "!")
    t))

(cl-defun lime-stop ()
  (when *lime-server*
   (websocket-server-close *lime-server*)
   (setq *lime-server* nil)))
After loading this file into Emacs invoke (lime-net-listen "localhost" 8889).
Now our Emacs listens for new connections from SLUG (the lisp-side part adapting
SWANK, already bundled with WECL). There are two SLUG backends in a repository: for web browser environment for Common Lisp runtime (uses )Now you can open a page listed here and connect to SLIME:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/slug.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/wank.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp">
      (defvar *connect-button* (make-element "button" :text "Connect"))
      (define-js-callback (connect-to-slug :type :null) ((event :js-ref))
        (wank-connect "localhost" 8889)
        (setf (inner-text *connect-button*) "Crash!"))
      (add-event-listener *connect-button* "click" (js-callback connect-to-slug))
      (append-child [body] *connect-button*)
    </script>
  </head>
  <body>
  </body>
</html>
This example shows an important limitation –  does not allow for
multiple asynchronous contexts in the same thread. That means that if Lisp call
doesn't return (i.e because it waits for input in a loop), then we can't execute
other Common Lisp statements from elsewhere because the application will crash.Here's another example. It is more a cool gimmick than anything else, but let's
try it. Open a console on this very website (on firefox C-S-i) and execute:function inject_js(url) {
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    head.appendChild(script);
    script.type = 'text/javascript';
    return new Promise((resolve) => {
        script.onload = resolve;
        script.src = url;
    });
}

function inject_cl() {
    wecl_eval('(wecl/impl::js-load-slug "https://turtleware.eu/static/misc/wecl-20250821")');
}

inject_js('https://turtleware.eu/static/misc/wecl-20250821/boot.js')
    .then(() => {
        wecl_init_hooks.push(inject_cl);
        inject_js('https://turtleware.eu/static/misc/wecl-20250821/wecl.js');
    });
With this, assuming that you've kept your LIME server open, you'll have a REPL
onto uncooperative website. Now we can fool around with queries and changes:(define-js-accessor (title :js-expr "title" :type :string)
  ((self :js-ref)
   (title :string)))

(define-js-accessor (background :js-expr "body.style.backgroundColor" :type :string)
  ((self :js-ref)
   (background :string)))

(setf (title [document]) "Write in Lisp!")
(setf (background [document]) "#aaffaa")
The first thing to address is the lack of threading primitives. Native threads
can be implemented with web workers, but then our GC wouldn't know how to stop
the world to clean up. Another option is to use cooperative threads, but that
also won't work, because Emscripten doesn't support independent asynchronous
contexts, nor ECL is ready for that yet.I plan to address both issues simultaneously in the second stage of the project
when I port the runtime to WASI. We'll be able to use browser's GC, so running
in multiple web workers should not be a problem anymore. Unwinding and rewinding
the stack will require tinkering with ASYNCIFY and I have somewhat working green
threads implementation in place, so I will finish it and upstream in ECL.Currently I'm focusing mostly on having things working, so JS and CL interop is
brittle and often relies on evaluating expressions, trampolining and coercing.
That impacts the performance in a significant way. Moreover all loaded scripts
are compiled with a one-pass compiler, so the result bytecode is not optimized.There is no support for loading cross-compiled files onto the runtime, not to
mention that it is not possible to precompile systems with ASDF definitions.JS-FFI requires more work to allow for defining functions with variable number
of arguments and with optional arguments. There is no dynamic coercion of
JavaScript exceptions to Common Lisp conditions, but it is planned.]]></content:encoded></item><item><title>1981 Sony Trinitron KV-3000R: The Most Luxurious Trinitron [video]</title><link>https://www.youtube.com/watch?v=jHG_I-9a7FY</link><author>ksec</author><category>hn</category><pubDate>Thu, 21 Aug 2025 11:52:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI crawlers, fetchers are blowing up websites; Meta, OpenAI are worst offenders</title><link>https://www.theregister.com/2025/08/21/ai_crawler_traffic/</link><author>rntn</author><category>hn</category><pubDate>Thu, 21 Aug 2025 11:35:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Cloud services giant Fastly has released a report claiming AI crawlers are putting a heavy load on the open web, slurping up sites at a rate that accounts for 80 percent of all AI bot traffic, with the remaining 20 percent used by AI fetchers. Bots and fetchers can hit websites hard, demanding data from a single site in thousands of requests per minute.I can only see one thing causing this to stop: the AI bubble poppingAccording to the report [PDF], Facebook owner Meta's AI division accounts for more than half of those crawlers, while OpenAI accounts for the overwhelming majority of on-demand fetch requests."AI bots are reshaping how the internet is accessed and experienced, introducing new complexities for digital platforms," Fastly senior security researcher Arun Kumar opined in a statement on the report's release. "Whether scraping for training data or delivering real-time responses, these bots create new challenges for visibility, control, and cost. You can't secure what you can't see, and without clear verification standards, AI-driven automation risks are becoming a blind spot for digital teams."The company's report is based on analysis of Fastly's Next-Gen Web Application Firewall (NGWAF) and Bot Management services, which the company says "protect over 130,000 applications and APIs and inspect more than 6.5 trillion requests per month" – giving it plenty of data to play with. The data reveals a growing problem: an increasing website load comes not from human visitors, but from automated crawlers and fetchers working on behalf of chatbot firms.The report warned, "Some AI bots, if not carefully engineered, can inadvertently impose an unsustainable load on webservers," Fastly's report warned, "leading to performance degradation, service disruption, and increased operational costs." Kumar separately noted to The Register, "Clearly this growth isn't sustainable, creating operational challenges while also undermining the business model of content creators. We as an industry need to do more to establish responsible norms and standards for crawling that allows AI companies to get the data they need while respecting websites content guidelines."That growing traffic comes from just a select few companies. Meta accounted for more than half of all AI crawler traffic on its own, at 52 percent, followed by Google and OpenAI at 23 percent and 20 percent respectively. This trio then has its hands on a combined 95 percent of all AI crawler traffic. Anthropic, by contrast, accounted for just 3.76 percent of crawler traffic. The Common Crawl Project, which slurps websites to include in a free public dataset designed to prevent duplication of effort and traffic multiplication at the heart of the crawler problem, was a surprisingly-low 0.21 percent.The story flips when it comes to AI fetchers, which unlike crawlers are fired off on-demand when a user requests that a model incorporates information newer than its training cut-off date. Here, OpenAI was by far the dominant traffic source, Fastly found, accounting for almost 98 percent of all requests. That's an indication, perhaps, of just how much of a lead OpenAI's early entry into the consumer-facing AI chatbot market with ChatGPT gave the company, or possibly just a sign that the company's bot infrastructure may be in need of optimization.While AI fetchers make up a minority of Ai bot requests – only about 20%, says Kumar – they can be responsible for huge bursts of traffic, with one fetcher generating over 39,000 requests per minute during the testing period. "We expect fetcher traffic to grow as AI tools become more widely adopted and as more agentic tools come into use that mediate the experience between people and websites," Kumar told .Perplexity AI, which was recently accused of using IP addresses outside its reported crawler ranges and ignoring robots.txt directives from sites looking to opt out of being scraped, accounted for just 1.12 percent of AI crawler bot and 1.53 percent of AI fetcher bot traffic recorded for the report – though the report noted that this is growing.Kumar decried the practice of ignoring robots.txt notes, telling , "At a minimum, any reputable AI company today should be honoring robots.txt. Further and even more critically, they should publish their IP address ranges and their bots should use unique names. This will empower site operators to better distinguish the bots crawling their sites and allow them to enforce granular rules with bot management solutions."But he stopped short of calling for mandated standards, saying that industry forums are working on solutions. "We need to let those processes play out.  Mandating technical standards in regulatory frameworks often does not produce a good outcome and shouldn't be our first resort."It's a problem large enough that users have begun fighting back. In the face of bots riding roughshod over polite opt-outs like robots.txt directives, webmasters are increasingly turning to active countermeasures like the proof-of-work Anubis or gibberish-feeding tarpit Nepenthes, while Fastly rival Cloudflare has been testing a pay-per-crawl approach to put a financial burden on the bot operators. "Care must be exercised when employing these techniques," Fastly's report warned, "to avoid accidentally blocking legitimate users or downgrading their experience."Kumar notes that small site operators, especially those serving dynamic content, are most likely to feel the effects most severely, and he had some recommendations. "The first and simplest step is to configure robots.txt which immediately reduces traffic from well-behaved bots. When technical expertise is available, websites can also deploy controls such as Anubis, which can help reduce bot traffic." He warned, however, that bots are always improving and trying to find ways around "tarpits" like Anubis, as code-hosting site Codeberg recently experienced. "This creates a constant cat and mouse game, similar to what we observe with other types of bots today," he said.We spoke to Anubis developer Xe Iaso, CEO of Techaro. When we asked whether they expected the growth in crawler traffic to slow, they said: "I can only see one thing causing this to stop: the AI bubble popping."There is simply too much hype to give people worse versions of documents, emails, and websites otherwise. I don't know what this actually gives people, but our industry takes great pride in doing this."However, they added: "I see no reason why it would not grow. People are using these tools to replace knowledge and gaining skills. There's no reason to assume that this attack against our cultural sense of thrift will not continue. This is the perfect attack against middle-management: unsleeping automatons that never get sick, go on vacation, or need to be paid health insurance that can produce output that superficially resembles the output of human employees. I see no reason that this will continue to grow until and unless the bubble pops. Even then, a lot of those scrapers will probably stick around until their venture capital runs out."Regulation – we've heard of it asked Xe whether they thought broader deployment of Anubis and other active countermeasures would help.They responded: "This is a regulatory issue. The thing that needs to happen is that governments need to step in and give these AI companies that are destroying the digital common good existentially threatening fines and make them pay reparations to the communities they are harming. Ironically enough, most of these AI companies rely on the communities they are destroying."This presents the kind of paradox that I would expect to read in a Neal Stephenson book from the '90s, not CBC's front page. Anubis helps mitigate a lot of the badness by making attacks more computationally expensive. Anubis (even in configurations that omit proof of work) makes attackers have to retool their scraping to use headless browsers instead of blindly scraping HTML."And who is paying the piper?"This increases the infrastructure costs of the AI companies propagating this abusive traffic. The hope is that this makes it fiscally unviable for AI companies to scrape by making them have to dedicate much more hardware to the problem. In essence: it makes the scrapers have to spend more money to do the same work."We approached Anthropic, Google, Meta, OpenAI, and Perplexity but none provided a comment on the report by the time of publication. ®Will Allen, VP, Product at Cloudflare commented on the findings, saying Cloudflare's observations were "reasonably close" to Fastly's claim, "and the nominal difference could potentially be due to a difference in customer mix." Allen added that, looking at its own AI Bot & crawler traffic by crawl purpose, for April 15 - July 14), Cloudlfare could show that 82.7 percent is "for training — this is the equivalent of 'AI crawler' in Fastly's report."Asked whether the growth in crawler traffic was likely to continue, Allen responded: "We don't see any material slowdowns in the near term horizon - the desire for content currently seems insatiable."He opined: "All of our work around AI crawlers is anchored on a radically simple philosophy: content creators and website owners should get to decide how their content and data is used for commercial purposes when they put it online. Some of us want to write for the superintelligence. Others want a direct connection and to create for human eyes only."Asked how he suggested site operators reduce the burden of this traffic on their infrastructure, he naturally pitched the vendor's own wares, saying "Cloudflare makes it incredibly easy to take control, even for our free users: you can decide to let everyone crawl you, or with one click block AI Crawlers from training and deploy our fully managed robots.txt."He said of the vendor's AI labyrinth that it was "a first iteration of using generative AI to thwart bots for us, and generates valuable data that feeds into our bot detection systems. We don't see this as a final solution, but rather a fun use of technology to trap misbehaving bots."]]></content:encoded></item><item><title>Margin debt surges to record high</title><link>https://www.advisorperspectives.com/dshort/updates/2025/07/23/margin-debt-surges-record-high-june-2025</link><author>pera</author><category>hn</category><pubDate>Thu, 21 Aug 2025 11:22:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mark Zuckerberg freezes AI hiring amid bubble fears</title><link>https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/</link><author>pera</author><category>hn</category><pubDate>Thu, 21 Aug 2025 11:04:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Stock market volatility was largely prompted by a report from the Massachusetts Institute of Technology, which claimed that 95pc of companies were getting “zero return” on their AI investments.A Meta spokesman sought to downplay the freeze, saying: “All that’s happening here is some basic organisational planning: creating a solid structure for our new superintelligence efforts after bringing people on board and undertaking yearly budgeting and planning exercises.”It comes after the company has been offering top researchers at rival companies, including OpenAI and Google, enormous pay deals to join Meta Superintelligence Labs as Mr Zuckerberg seeks to dominate the field.The company’s billionaire chief executive has become personally involved in developing cutting-edge AI after the disappointing release of its latest systems, personally messaging top researchers at Silicon Valley AI companies.However, the division has been disrupted by repeated strategy overhauls, which led to the delayed release of its latest “Behemoth” AI model.Mr Zuckerberg has said he wants to develop a “personal superintelligence” that acts as a permanent superhuman assistant and lives in smart glasses.“We believe in putting this power in people’s hands to direct it towards what they value in their own lives,” he wrote last month.“This is distinct from others in the industry who believe superintelligence should be directed centrally towards automating all valuable work, and then humanity will live on a dole of its output.”Mr Zuckerberg recently told investors that he wanted “small, talent-dense teams” to be driving its AI work, rather than large groups of researchers.Despite this, the company has said that the cost of paying staff will significantly increase in the coming years. Analysts at Morgan Stanley warned this week that the pay surge may “dilute shareholder value without any clear innovation gains”.Concerns about AI progress have been amplified by the modest response to GPT-5, the much-anticipated new version of ChatGPT.Sam Altman, OpenAI’s chief executive, has compared hype around AI to the dotcom bubble at the turn of the century.]]></content:encoded></item><item><title>Using Podman, Compose and BuildKit</title><link>https://emersion.fr/blog/2025/using-podman-compose-and-buildkit/</link><author>LaSombra</author><category>hn</category><pubDate>Thu, 21 Aug 2025 10:54:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Podman supports Docker Compose projects with two possible solutions: either by
connecting the official Docker Compose CLI to a Podman socket, either by
using their own drop-in replacement. They ship a
small wrapper to select one of these options. (The
wrapper has the same name as the replacement, which makes things confusing.)Unfortunately, both options have downsides. When using the official Docker
Compose CLI, the classic builder is used instead of the newer BuildKit
builder. As a result, some features such as additional contexts are not
supported. When using the podman-compose replacement, some other features are
missing, such as ,  and referencing another service in
additional contexts. It would be possible to add these features to
podman-compose, but that’s an endless stream of work (Docker Compose regularly
adds new features) and I don’t really see the value in re-implementing all of
this (the fact that it’s Python doesn’t help me getting motivated).I’ve started looking for a way to convince the Docker Compose CLI to run under
Podman with BuildKit enabled. I’ve tried a few months ago and never got it to
work, but it seems like this recently became easier! The podman-compose wrapper
force-disables BuildKit, so we need to use
directly the Docker Compose CLI without the wrapper. On Arch Linux, this can be
achieved by enabling the Podman socket and creating a new Docker context (same
as setting , but more permanent):pacman -S docker-compose docker-buildx
systemctl --user start podman.socket
docker context create podman --docker host=unix://$XDG_RUNTIME_DIR/podman/podman.sock
docker context use podman
With that,  just works! It turns out it automagically creates a
 container under-the-hood to run the BuildKit daemon.
Since I don’t like automagical things, I immediately tried to run BuildKit
daemon myself:pacman -S buildkit
systemctl --user start buildkit.service
docker buildx create --name local unix://$XDG_RUNTIME_DIR/buildkit/rootless
docker buildx use local
Now  uses our systemd-managed BuildKit service. But we’re not
done yet! One of the reasons I like Podman is because it’s daemonless, and
we’ve got a daemon running in the background. This isn’t the end of the world,
but it’d be nicer to be able to run the build without BuildKit.Fortunately, there’s a way around this: any Compose project can be turned into
a JSON description of the build commands called Bake. docker buildx bake --print will print that JSON file (and the Docker Compose CLI will use Bake
files if  is set since v2.33). Note, Bake supports way more
features (e.g. HCL files) but we don’t really need these for our purposes (and
the command above can lower fancy Bake files into dumb JSON ones).The JSON file is pretty similar to the  CLI arguments. It’s not
that hard to do the translation, so I’ve written Bakah, a small tool which
does exactly this. It uses Buildah instead of shelling out to Podman (Buildah
is the library used by Podman under-the-hood to build images). A few details
required a bit more attention, for instance dependency resolution and parallel
builds, but it’s quite simple. It can be used like so:docker buildx bake --print >bake.json
bakah --file bake.json
Bakah is still missing the fancier Bake features (HCL files, inheritance,
merging/overriding files, variables, and so on), but it’s enough to build
complex Compose projects. I plan to use it for soju-containers in the future,
to better split my Dockerfiles (one for the backend, one for the frontend) and
remove the CI shell script (which contains a bunch of Podman CLI invocations).
I hope it can be useful to you as well!]]></content:encoded></item><item><title>Why is D3 so Verbose?</title><link>https://theheasman.com/short_stories/why-is-d3-code-so-long-and-complicated-or-why-is-it-so-verbose/</link><author>TheHeasman</author><category>hn</category><pubDate>Thu, 21 Aug 2025 10:12:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[D3 is a b***h of a language at first glance. It’s long. It’s complicated and verbose. You have to enter what feels like an obscene amount of key strokes to draw .Which means, I just finished drawing a box plot from chapter 6. Yup. You know. That thing, you just make four or five clicks in Microsoft Excel, and , it appears. In a way that you don’t think is magical, because Excel has been around longer than the dial up modem. I just shared this on my social media, and for the non-technical, I had to explain “hey, so, the reason why this is impressive is because…”That’s  of code to draw a box plot. Why on earth would any masochist do this?Because of the flexibility D3 provides. The reason why there are so many lines is because:I’m doing this the long way. I could write some functions or components which could collapse a lot of those lines, but I’m still learning the ropes. Better to do it the long and manual way so I don’t skip anythingThis is important: D3 is a library which allows you to draw SVGs and bind them to dataAll D3 is doing is saying “so, hey, I got all this data. Now. Dear browser. I’d like you to draw these SVG shapes, according to these instructions.” SVG code reads like an extended HTML block of code. To draw a line on a two dimensional grid you have to define:x1 – the horizontal position of point 1x2 – the horizontal position of point 2y1 – the vertical position of point 1y2 – the vertical position of point 2In D3 if I was just going to draw this with fixed values, say a line that goes from co-ordinates (0,12) to (4,15):aRandomNameForThisLine
  .append("line")
    .attr("x1", 0)
    .attr("x2", 4)
    .attr("y1", 12)
    .attr("y2", 15);But the power comes from :boxplotContainer
  .append("line")
    .attr("x1", xScale(gender) - boxplotWidth/2)                          
      .attr("x2", xScale(gender) + boxplotWidth/2)
      .attr("y1", gender === "Female"                                       
        ? yScale(femaleExtent[0])
        : yScale(maleExtent[0])
      )
      .attr("y2", gender === "Female"
        ? yScale(femaleExtent[0])
        : yScale(maleExtent[0])
      );That block of code draws the bottom horizontal line of the box plot for each of those boxes, getting the values from the data, and adjusted for some other variables that I defined elsewhere.It might seem long, but what’s great about it, is how much you can customise those values. You could make that line a curve if you want. Or you could add little tiny whiskers throughout, so your box plot starts to look like an unruly cat. Whatever. It’s up to you. That’s what’s great about D3. It’s like drawing. Sure you can take a photograph, but isn’t it cooler to draw something, showing your interpretation of the object in question?What I’m trying to say is:D3 is verbose so you can create art. With tools like datawrapper and flourish there’s little need these days to learn D3. However, you aren’t reading this because you want click and deploy tools. And while some tools allow you to do some of that, D3 allows you to do it all. And one day, on your learning journey, you’ll love how verbose it is.  Don’t mind me, just gonna plug my newsletter below:D3 is verbose. This sign-up isn’tIf you’re here for deep clarity—not shortcuts—drop your email for crisp visual data essays that turn the complexity of life into insights.Sign-up form not appearing above? Pesky javascript blocker… click here and scroll to the bottom for a vanilla boring mailchimp form that works on all devices. ]]></content:encoded></item><item><title>My other email client is a daemon</title><link>https://feyor.sh/blog/my-other-email-client-is-a-mail-daemon/</link><author>aebtebeten</author><category>hn</category><pubDate>Thu, 21 Aug 2025 08:54:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I replaced vector databases with Git for AI memory (PoC)</title><link>https://github.com/Growth-Kinetics/DiffMem</link><author>alexmrv</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 06:20:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hey HN! I built a proof-of-concept for AI memory using Git instead of vector databases.The insight: Git already solved versioned document management. Why are we building complex vector stores when we could just use markdown files with Git's built-in diff/blame/history?Memories stored as markdown files in a Git repo
Each conversation = one commit
git diff shows how understanding evolves over time
BM25 for search (no embeddings needed)
LLMs generate search queries from conversation context
Example: Ask "how has my project evolved?" and it uses git diff to show actual changes in understanding, not just similarity scores.This is very much a PoC - rough edges everywhere, not production ready. But it's been working surprisingly well for personal use. The entire index for a year of conversations fits in ~100MB RAM with sub-second retrieval.The cool part: You can git checkout to any point in time and see exactly what the AI knew then. Perfect reproducibility, human-readable storage, and you can manually edit memories if needed.Stack: Python, GitPython, rank-bm25, OpenRouter for LLM orchestration. MIT licensed.Would love feedback on the approach. Is this crazy or clever? What am I missing that will bite me later?]]></content:encoded></item><item><title>Python f-string cheat sheets (2022)</title><link>https://fstring.help/cheat/</link><author>shlomo_z</author><category>hn</category><pubDate>Thu, 21 Aug 2025 05:08:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The below examples assume the following variables:>>> number = 4125.6
>>> percent = 0.3738
These format specifications only work on all numbers (both  and ).Type  with precision  displays  digits after the decimal point.Type  with precision  displays  significant digits in scientific notation. Trailing zeros are not displayed.These examples assume the following variable:An empty type is synonymous with  for integers.These format specifications only work on integers ().These examples assume the following variable:These format specifications work on strings () and most other types (any type that doesn't specify its own custom format specifications).The below modifiers are special syntaxes supported by all object types.
Some format specifications are also included to show how to mix and match these syntaxes with the  syntax.An empty conversion field is synonymous with , unless a self-documenting expression is used.
When a self-documenting expression is used, an empty conversion field uses .]]></content:encoded></item><item><title>How to stop feeling lost in tech: the wafflehouse method</title><link>https://www.yacinemahdid.com/p/how-to-stop-feeling-lost-in-tech</link><author>research_pie</author><category>hn</category><pubDate>Thu, 21 Aug 2025 01:28:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[these past few weeks I’ve received a massive amount of dms from CS students being absolutely lost.like to a level of lost I’m feeling their stress and anxiety in my own chest.on top of that some of you are going through the absolute worst time of your life I don’t know how you guys are still standing up but I’m in awe.I found myself reiterating the same advice to at least 7 people now, so I’ll write it all here for those who are too crippled to ask for help.hope it’s useful folks and keep it up.I’m receiving a variation of this, but the common element is always that the person feels completly lost.lost because AI will take their job, lost because it seems there is too much to learn, lost because they don’t know if they actually like this whole tech thing, etc.they had the vaguest picture of what they would be doing in 5 years.and trying to figure it out by looking at technology is the wrong way to look at it.it would be like trying to obsess intensely over hammers, nails, saw to build a house. tech is the same; it’s just a set of tools to do something.enter my favorite method to get your life untangled, which I call the wafflehouse method (bear with me, I swear this blog post is useful).I’ll detail here how to use it, but remember that untangling your life goals is a deeply personal process. take inspiration from this, but feel free to modify so that it feels good.call in sick, cancel your meetings and plans for two days.straight in the middle of the week.don’t tell your kids, wife, moms or friends that you are taking off.you are going to go on a deeply personal introspective adventure and the only one invited is you.48h to find yourself in the whole year is the least you can do.sorry for the image but yes that’s what you gonna do.pick your favorite spot, the one you feel very good to be at, and can spend a whole day by yourself.can’t give you advice on picking a spot just make sure that wherever you are going has deep significance for yourself.now sit alone with your notebook and be quiet for a while.try to imagine this one image: yourself 5 years from now.not as of how you are going to get there from the current trajectory you are following, but how you would feel proud to be.imagine it vividly, it may be still very abstract, but try to dive a bit in your mind what is going on in this vision.does it have to be tech related?no absolutely not, on the contrary.imagine yourself at the pinacle where you really feel proudness bursting within your chest. this looks different for everyone. try to remove anything attached to other expectations of yourself. doesn’t matter what your moms and dads want of you if you feel like a husk inside.now that you have something to latch upon, write and write and write. write everything, how you feel now, how you feel then, what is different from now, what you regret, the things you think are not realistic, that one hunk of shame you have been carrying over forever, all these fears and hope lay them all down.write and write until there is nothing more to be said.write it all up until you have colors and shape of things you want to come to pass.it doesn’t have to be clear for now, it just needs to be vivid.when you are satisfied with this painting, it’s time to head home for a good day rest you are going to need it.now is time to enter the wafflehouse (or whatever regional equivalent you have mine is called cora).make sure that wherever you are going can tolerate you for a whole day because you are going to be squatting there opening until closing.bring you computer, your notepad and this image of yourself.now that you are comfortably sitting with your beverage of choice it’s time to refine the image you painted into 2-3 clear goals that happen in 5 years time.the goals need to be very specific and detailed.if you feel absolutely ridiculous writing them down, it’s maybe too much, but don’t hold back. the goals need to match the image.write as much detail as you can about each of these goals and make sure that there are as few as possible while still retaining the essence of the image.now that you have these 2-3 5-year goals you interpolate what you need to achieve in 3 years time to be on track to hit them.it’s ok if your interpolation is a bit crooked, you just need to trace a course here.if you feel the 3 years are ridiculous, feel free to change the 5 years and adjust the knob of your dream self.now you do the same on the 1-year goal.you are for sure going to do some back and forth between the 1-3-5 years adjusting things as you go.try to keep the number of goals as few as possible.if they sprawl around, it might be best for you to reduce the scope of your image a bit. for instance trying to be a nobel prize winner marathon karateka black belt might be too much.focus on one aspect if you see your goals proliferating, the one that is more important to you.I know it’s tough to make a call here, but you have to be somewhat pragmatic at some point.btw the goals aren’t starting in like january 1st, they start now now.now you are going into the details for real. you’re going to make a DAG.they look like this usually notice that there are no cycle (for the rest of the blog view them as the node on the left being at the top and the node on the right being at the bottom).at the top it’s where you are at. put some numbers in there about your current situation (e.g. maybe you are broke).at the bottom put your 1-year goals (e.g. maybe you are not broke anymore)now in between write as many steps as possible you think you need to take in a year's time to go from your current situation to the end goals (remember there is more than one goal usually) based on what you have right now and the fact that luck is against you.this last bit is important. your whole plan is based on the fact that every damn day you will wake up will be a bad day.sun is shining exactly 0 times for 5 years.create a 100s of these steps that flow logically from your current node to the end node.some will be more vague than others. these are your knot points.for each of your knot points node you should research to the best of your current ability how to unknot them.sometime it’s because it’s actually 5 nodes mushed together and you don’t know it’s 5 steps.sometime it’s because it’s the wrong step and you are acting on false prior.doesn’t matter try to untangle them as much as possible so that you have 100s of very highly defined step from start to finish.as highly super duper detailed as possible.not the other month which will start in 3 weeks.take whatever first steps there is in this directed acyclic graph and put it as stuff to do in the month you have left.here you have to be realistic about what you can chew, but remember that these tasks are what bring you closer to what you want to be.that image that made you feel good remember?yeah that’s the path you are rolling in now so take as much tasks as you can from the list and put it in the month.cancel other stuff if you can to have more time to work on these.you thought we would stop at monthly!take whatever you picked for the month and bring them into your weekly goals for this week.remember when we are now at the wafflehouse? in the middle of the week? well good news you got 3 days left to get these tasks done.and before I see any of you start to spend 3 weeks making the perfect notion dashboard dingy NO!you take whatever you have right NOW I don’t care if it’s on a notebook.you spend 0 minutes optimizing your project management here, you take all the time into actually doing the tasks towards a future you are proud of.you should be getting dangerously close to getting evicted from the wafflehouse so take the little time that is left to look at your weekly goals and pick a few tasks to work on tomorrow.the waitress should be looking at you like this.at this point thing should be so concrete that even if you were to not remember your yearly goals it wouldn’t matter at all.every day from now on you will have two category of things to do:stuff that actually brings you closer to where you want to be in life things that your mom wants you to do, the stuff that this one friends always put on you or that your boss decided you were responsible for.I’m not saying to throw all of this in the garbage (. . .).but you have to make conscious decisions that you have a plan that is yours and that it is up to you to get it done.now you’re done, it’s time to work and this work is your life's work.you don’t need to parade this mega plan to your relatives it’s a deeply personal one.it really doesn’t matter because they should start to see the difference anyway since this plan is one where you will transform yourself into what you truly want to be.every day before going to bed take some of the stuff from this week and put it in your bucket for the next day.when you wake up look at the task your past self is imploring you to get done and oblige.every week on a quiet sunday afternoon, tend to this plan like a garden. look out at the work done in the week, check what need to be done in the month, revise a bit the directed acyclic graph or the 1-3-5 years goals.make the small tweaks based on your learning from the week that passed.and when you still feel good about the trajectory and that this image still bring a smile to your soul pick a few tasks for next week.you have to be careful though.if you actually do this you get a very stressful side effect:there is no one to blame but yourselfit’s not your mom, it’s not your teacher, it’s not your friends, it’s you.from that point going forward you have your life in your hands and it’s time to act like it.if this was useful to you in anyway hit me up periodically with the update.]]></content:encoded></item><item><title>A statistical analysis of Rotten Tomatoes</title><link>https://www.statsignificant.com/p/is-rotten-tomatoes-still-reliable</link><author>m463</author><category>hn</category><pubDate>Thu, 21 Aug 2025 00:10:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I stayed in a hotel recently, which means I watched cable television, which means I consumed commercials that I could not skip—and some of these commercials advertised upcoming movie releases. Promo after promo, I noticed an unmistakable pattern: every film was "Certified Fresh" on Rotten Tomatoes, with this seal of approval serving as the ad's climactic selling point.After five days of "Certified Fresh" movie propaganda, I began to grow suspicious. If every movie is un-rotten, then one of two things must be true:Humanity Has Stopped Producing Bad Art: Cats, Space Jam: A New LegacyRotten Tomatoes Has Changed:So today, we'll delve into the suspicious recalibration of Rotten Tomatoes, tracing when and how Hollywood's foremost stamp of artistic excellence turned rotten.Rotten Tomatoes was founded in 1998 to aggregate reviews of Jackie Chan films. Within months, its creators recognized the concept's broader potential and expanded the platform to cover all movies. The website quickly became a trusted proxy for critical consensus, a role it has maintained for over 25 years.Rotten Tomatoes appraises movies through its trademarked "Tomatometer" score, calculated as the percentage of critic reviews that are deemed "positive." Both a lukewarm 3-out-of-5 star rating and an effusive rave of cinematic brilliance qualify as "positive" on the platform. Films with a Tomatometer score above 60% receive a "fresh" label; anything below this threshold is branded "rotten."nearly a third of Americans have checked the Tomatometer before seeing a filmIn a well-calibrated system, critic scores should remain stable over time unless there's a meaningful shift in film quality. So why has the average Tomatometer score increased over the past decade? What changed: the movies or the metric?And here is where the intrigue deepens: this rating shift coincides with Fandango's 2016 acquisition of Rotten Tomatoes 🙉. One might consider this a conflict of interest, given Fandango is America's largest movie-ticketing platform, partially owned by NBCUniversal and Warner Bros. Discovery (and that person would be correct!).If I had a corkboard for tracking this conspiracy theory, it would be incredibly lame: I'd have a picture of Rotten Tomatoes and a photo of Fandango, with a single spool of yarn connecting the two. This does not "go all the way to the top," and there is probably no mention of these dealings in The Epstein Files.While these two data points paint an unflattering portrait of Rotten Tomatoes, they do not prove whether the site's aggregation process or scoring methods have fundamentally changed. To test this, I decided to look at the relationship between Rotten Tomatoes' critic score and its user-generated audience score (the latter aggregates reviews from moviegoers who have seen a film).While critics and audiences don't always agree, their sentiments tend to be strongly correlated—a wave of critical pans usually signals similar disdain from viewers, and vice-versa. Before 2016, a higher audience score generally went hand in hand with a strong Tomatometer rating.On a year-to-year basis, critic and audience scores moved in tandem, demonstrating a stable correlation until 2016, when a sharp divergence emerged—just as the average Tomatometer rating began to climb.The inevitable follow-up question is how one alters the Tomatometer within the site's seemingly strict parameters.Ultimately, Rotten Tomatoes has control over two major inputs to its Tomatometer score:Whether a review is considered "fresh" or "rotten."Which reviews count toward the Tomatometer.Tweaking the definition of "fresh" would spark immediate backlash from the site's critic base, who could use their platforms to publicly bash Rotten Tomatoes. A much subtler lever is critic selection: expanding the reviewer pool to a group of writers who (coincidentally) produce more favorable appraisals.Indeed, following the Fandango acquisition, the average number of reviewers per mainstream release increased by 40 to 70 critics.It's feasible that the platform broadened its critic pool to account for newfangled digital media outlets like The Ringer and BuzzFeed, thereby allowing Rotten Tomatoes to evolve with an ever-changing media ecosystem. Yet when we examine the most prolific publications added to the site over the last decade, we find a collection of outlets that lack name recognition.As someone who also writes words on the internet, I'm not going to go out of my way to dunk on Denerstein Unleashed and KKFI-FM (Kansas City), but I will say this after reviewing around 50 of these sites:Many of these publications are hosted on outdated blogging platforms.Several of these sites do not load properly on my mobile web browser.Many of these blogs seem like secondary sources of income or passion projects.To account for this influx of reviewers, Rotten Tomatoes has created a "Top Critic" designation reserved for established media outlets, such as The New York Times and The Atlantic. However, this label has no special bearing on a film's top-line Tomatometer score and is largely incorporated into ancillary aspects of the site. Rotten Tomatoes claims that these reviewer additions were made to diversify its critic pool by including more women, people of color, and underrepresented groups—a statement I can neither confirm nor deny. What I can say is this: these new reviewers fundamentally altered the site's steady state, and the platform did little to account for this underlying shift. Maybe increased critic scores are a happy accident, albeit one with extremely suspicious timing.PR firms will actively court reviewers from smaller outlets to inflate Tomatometer scoresI don't have any hard data to verify Vulture's claims, but one could see how an expanded pool of Tomatometer-approved hobbyists might be ripe for manipulation. With the right mix of critics, a "fresh" rating can be engineered to coincide with a film's opening weekend.maybe Rotten Tomatoes being (ostensibly) rigged isn't such a bad thing? Enter Rotten Tomatoes, a debatably flawed yet widely influential website that is a questionable proxy for critical acclaim. For whatever reason, people trust this platform and will decide to leave their homes based on its recommendation. Rotten Tomatoes is the focal point of television promos because it can create demand where it previously did not exist.If I were the owner of a movie-ticketing app or a sicko-utilitarian like Sam Bankman-Fried, I'd argue the ends justify the means: the average score goes up, more movies are deemed "fresh," people go to theaters, and cinema lives to fight another day. Unfortunately, I am not a sicko-utilitarian, which means I am now advocating against my best interests. Hooray for me.In the short term, inflated scores may lure people to theaters once or twice. But in the long run, it's better if people enjoy their experience at the theater (by, you know, seeing a good movie). Despite my lizard-brain reaction, I believe long-term thinking usually wins out, while short-term shenanigans are always rotten in hindsight. Although what was once deemed "rotten" is now considered "certifiably fresh," so who knows.This post is public so feel free to share it.Struggling to turn your data into actionable insights? Need expert help with a data or research project? Well, Stat Significant can help. 🔍 Insights That Improve Performance: 📊 Dashboards That Drive Action:⚙️ Data Architecture That Automates Reporting]]></content:encoded></item><item><title>Code review can be better</title><link>https://tigerbeetle.com/blog/2025-08-04-code-review-can-be-better/</link><author>sealeck</author><category>hn</category><pubDate>Wed, 20 Aug 2025 23:10:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Slightly unusual genre today: a negative
result about our 
tool for a different take on code review process, which we decided to
shelve, at least for the time being.A lot of people are unsatisfied with GitHub’s code review process.
One of the primary issues is that GitHub poorly supports stacked pull
requests and interdiff
reviews. While I also see interdiff as valuable, it’s not the reason
why I decided to experiment with . I have two
other problems with GitHub, and with every single other code review
system, with the exception of the
thing that Jane Street uses internally:review state is not stored as a part of repository itself,review is done via remote-first web-interface.Let’s start with the second one.By the way of analogy, I don’t use GitHub’s web editor to write code.
I clone a repository locally, and work in my editor, which is:fully local, memory/nvme latencies, no HTTP round-trips,tailored to my specific odd workflow.When I review code, I like to pull the source branch locally. Then I
soft-reset the code to mere base, so that the code looks as if it was
written by me. Then I fire up magit, which allows me to effectively
navigate both through the diff, and through the actual code. And I even
use git staging area to mark files I’ve already reviewed:Reviewing  rather than diff is so powerful: I can run
the tests, I can go to definition to get the context, I can try out my
refactoring suggestions in-place, with code completion and the other
affordances of my highly sophisticated code editor.Alas, when I want to actually leave feedback on the PR, I have to
open the browser, navigate to the relevant line in the diff, and (after
waiting for several HTTP round-trips) type my suggestion into a text
area. For some reason, the text area also regularly lags for me,
especially on larger diffs.Two things are wrong here. On the interface side, review feedback is
text related to the code. The most natural interface is to just leave
review comments as inline comments in the code, or even to fix the code
directly:And on the implementation side, because the data is stored in a
remote database, rather than in a local git repository, we get all those
latency-inducing round-trips (not to mention vendor lock in).Code review is a single commit which sits on top of the PR
branch.That commit adds a bunch of code comments with specific
markers.Review process involves both the author and the reviewer modifying
this top commit (so, there’s a fair amount of
git push --force-with-lease involved).The review concludes when all threads were marked as
 and an explicit revert commit is added on top
(such that review is preserved in the history).I had a hope that “code review is just a commit” would be the secret
to keep implementation complexity low. Sadly, the devil is in the
details in this particular case.The basic idea, that reviewing is leaving comments in code, works as
well as I had expected (that is, it’s really, really awesome). But
modifying code under review turned out to be tricky. If a reviewer
requests a change, and you apply it to some deep commit, or even add a
new commit on top, you now have to solve mere conflicts with the review
comments themselves, as they are often added at the hunk
boundaries. And then, while  is workable,
it also adds friction. There is an impedance mismatch here, where, for
code, we want very strong, hash-chained intentional sequence of
state-transitions, while for review we would be more happy with more lax
conflict-free merging rules. It  be solved with more
tooling to “push” and “pop” review comments on top of pristine review
branch, but that seems to push well beyond my 500 line limit.Then, there’s a second change. It seems like upstream
git might be getting a Gerrit-style Change-Id for tracking revisions
of a single commit over rebases. If that happens, we might actually get
first class support for per-commit interdiff review! But that would be
somewhat incompatible with  approach, which adds
an entire separate commit to the branch. But, perhaps, in the
 world, we could be adding review comments to the
commits themselves, and, rather that adding a revert at the conclusion
of review, instruct git to store all revisions of a particular
.Anyway, we are begrudgingly back to web-interface based code reviews
for now. Hopefully someone is inspired enough to fix this properly one
day!If you’ve been thinking along similar lines, the following links are
worth checking out:Fossil
is an SCM system which stores everything in the repository.NoteDb
backend for Gerrit. Gerrit started with tracking review state in a
separate database, but then moved storage into git.git-bug uses git to
store information about issues.prr which
implements in-editor review interface on top of GitHub’s Web APIgit-pr similar project in spirit
that leverages git native features to replace the entire pull request
workflow.]]></content:encoded></item><item><title>Vibe coding creates a bus factor of zero</title><link>https://www.mindflash.org/coding/ai/ai-and-the-bus-factor-of-0-1608</link><author>AntwaneB</author><category>hn</category><pubDate>Wed, 20 Aug 2025 21:47:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[All the opinions expressed in this article and on this website are entirely my own and do not represent my employer in any way.This article was discussed on HackerNews in August 2025, you can go see the comments on the HackerNews post.Ever heard about the “Bus factor”? It is a concept that measures the risk of losing all knowledge about a particular thing – a software development project for example – by estimating how many team members could get crushed by a bus before nobody knows how to work on the project anymore. As an example, if 3 people on your team know how to restore a backup of your database, the Bus Factor for that particular function in 3.Since the dawn of humanity, even long before buses existed, the Bus Factor has always had a “worst case” value of 1. If the sole keeper of a piece of knowledge came to pass, the knowledge was lost, unless it had been transferred previously.And humanity has worked hard to keep itself far from this Bus Factor of 1. Brown-bag sessions, documentation, video tutorials, knowledge handovers, demos and showcases, without forgetting , and many more mechanisms in which an uncountable number of man-hours has been sunk.But on the 30th of November 2022, all of this changed, and suddenly a large part of humanity became perfectly fine with a Bus Factor not just of 1, but of .That date corresponds to the release of ChatGPT to the public, and the start of the mass-market adoption of GenAI. It’s also the birth of what would become 3 years later the concept of “AI first”.One might think that “AI first” would leave humans second, but, unsurprisingly, delegating the creation process to machines has instead left us nowhere to be found when it comes to knowledge keeping.Focusing a bit on programming, it seems like a growing part of the industry is now happy to let LLMs generate functions, entire features, or even complete projects (security holes included). They have moved on from understanding their code-base and preserving this knowledge to actively trying to avoid having any piece of knowledge about their project to begin with, preferring instead to “vibe”.Where the bus hits a wallWe can leave aside the flaws of vibe-coding and the issues with LLM-generated code in general for another article. Indeed, the quality of the generated code does not really matter here. It is obviously easier to understand code that you have never seen before if it is good code, but ultimately reading code remains much more complex than writing it no matter what.Before LLMs, provided that your team did some of their due diligence, you could always expect to have some help when tackling new code-bases. Either a mentor, or at least some (even if maybe partially outdated) documentation. With LLMs, this is gone. The only thing you can rely on is on your ability to decipher what a highly imperfect system generated, and maybe ask explanations to that same imperfect system about  its code (oh, and it has forgotten everything about the initial writing process by then).Imagine having to solve bugs, add new features, patch security holes and upgrade dependencies in a piece of software that nobody on Earth has even the faintest idea about how it was built and why it was built that way.Now, imagine being the user uploading personal documents, credit card information, private photos or thoughts to a piece  of software that nobody on Earth has even the faintest idea about how it was built and why it was built that way.Because of the situation of a Bus Factor of zero that it creates, vibe coding is fundamentally flawed. That is, only until there is an AI that can generate 100% accurate code 100% of the time, and it is fed 100% accurate prompts.If vibe coding isn’t for you and you want to read more articles, check out my category dedicated to advice about learning programming.]]></content:encoded></item><item><title>Show HN: PlutoPrint – Generate PDFs and PNGs from HTML with Python</title><link>https://github.com/plutoprint/plutoprint</link><author>sammycage</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 20:37:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi everyone, I built PlutoPrint because I needed a simple way to generate beautiful PDFs and images directly from HTML with Python. Most of the tools I tried felt heavy, tricky to set up, or produced results that didn’t look great, so I wanted something lightweight, modern, and fast. PlutoPrint is built on top of PlutoBook’s rendering engine, which is designed for paged media, and then wrapped with a Python API that makes it easy to turn HTML or XML into crisp PDFs and PNGs. I’ve used it for things like invoices, reports, tickets, and even snapshots, and it can also integrate with Matplotlib to render charts directly into documents.I’d be glad to hear what you think. If you’ve ever had to wrestle with generating PDFs or images from HTML, I hope this feels like a smoother option. Feedback, ideas, or even just impressions are all very welcome, and I’d love to learn how PlutoPrint could be more useful for you.]]></content:encoded></item><item><title>Introduction to AT Protocol</title><link>https://mackuba.eu/2025/08/20/introduction-to-atproto/</link><author>psionides</author><category>hn</category><pubDate>Wed, 20 Aug 2025 19:13:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Some time ago I wrote a long blog post I called “Complete guide to Bluesky”, which explains how all the user-facing features of Bluesky work and various tips and tricks. This one is meant to be a bit like a developer version of that – I want to explain in hopefully understandable language what all the pieces of the network architecture are and how they all fit together. I hope this will let you understand better how Bluesky and the underlying protocol works, and how it differs from e.g. the Fediverse. This should also be a good starting point if you want to start building some apps or tools on ATProto.This post is a first part of a series – next I want to look at some comparisons with the Fediverse and some common misconceptions that people have, and look at the state of decentralization of this network, but that was way too much for one post; so this one focuses on the “ATProto intro tutorial” part.But before we start, a little philosophical aside:What is “Bluesky”? Which “Bluesky” are we talking about?Discussions about Bluesky sometimes get a little confusing because… “Bluesky” could mean a few different things. Language is hard.First, we have Bluesky the company, the team. Usually, when people want to clarify that they’re talking about the group of people or the organization, they say “Bluesky PBC” (PBC = Public Benefit Corporation), or “Bluesky team”.(If you want to read a bit about where Bluesky came from and what’s the current state of the company, read these two sections in the Bluesky Guide blog post.)And we also have Bluesky the product, the social network, the thing that they’ve built. This network is not a single black box like Twitter or Facebook are (despite what they say about it on Mastodon), it’s more like a set of separate and actually very transparent boxes.The system they’ve built, of which Bluesky was initially meant to be just a tech demo, is called the Authenticated Transfer Protocol, or AT Protocol, or ATProto. Bluesky is built on ATProto, and it is in practice a huge part of what ATProto currently is, which makes the boundary between Bluesky and non-Bluesky a bit hard to define at times, but it’s still only a subset of it.Bluesky in this second meaning is some nebulous thing that consists of: the data types (“lexicons”) that are specific to the Bluesky microblogging aspect of ATProto, like Bluesky posts or follows; the APIs for handling them and for accessing other Bluesky-specific features; the rules according to which they all work together; and the whole “social layer” that is created out of all of this, the virtual “place” – the thing that people have in mind when they say “this website”, even when it’s accessed through a mobile app. One of the coolest things about Bluesky & ATProto, in my opinion, is that it connects many different independent pieces into something that still feels like one shared virtual space.People outside the company can create (and are creating) other such things on ATProto that aren’t necessarily Bluesky-related – see e.g. WhiteWind or Leaflet (blogging platforms), Tangled (GitHub alternative), Frontpage (Hacker News style link aggregator), or Grain (photo sharing site). They use the same underlying mechanisms that are at the base of ATProto, but use separate data types, have different rules, goals, and UIs. How do we call these things as a whole, the different sets of “data types + rules + required servers + client apps” that define different use cases of the network?Bluesky team usually calls them “apps”, but I’m not a big fan of this term, because “app” kinda implies a client app, and that’s just one small piece of it. I sometimes call them “services” – though it’s probably not perfect either, since it implies just the server part in turn. Suggestions welcome :) (I’m mentioning this at the beginning, because this is something that many different parts are related to.)Personally, when I say “the Bluesky app”, I will generally mean the actual client app (mobile / webapp), not the “service”, and when I say “Bluesky-specific”, I will mean the “service”, not the company; and “Bluesky-hosted” will mean run by Bluesky the company. Hopefully in most cases, it can be guessed from context.BTW, the commonly accepted term for the whole shared “multiverse” of all ATProto apps is “The Atmosphere”, or “ATmosphere” (though I much prefer the former personally, the weird capitalization bugs me somehow ;). It was coined by someone from the community, but was accepted by the team and is now mentioned on the official atproto site.Let’s start with defining the various building pieces of the protocol:The most basic piece of the ATProto world is a . Records are basically JSON objects representing the data about a specific entity like a post or profile, organized in a specific way. A post/reply, repost, like, follow, block, list, entry on a list, user profile info – each of these is one record. Most public actions you take on Bluesky, like following someone or liking a post, are performed by creating a record of an appropriate type (or editing/deleting one created before).Records are stored on disk and transferred between servers in a binary format called CBOR, although in most API endpoints they’re returned in a JSON form (they are equivalent, just different encodings of the same data).The key thing about records, which has very real consequences for user-facing features, is that you can only create and modify  records, not those owned by others (and there are no “shared” records at the moment, each record is owned by a specific account). This means that e.g. when you follow someone, you create a follow record on your account, and that other person can’t delete your record, which is why there’s currently no “soft-blocking” feature, i.e. you can’t make someone stop following you (though you can block them). There are workarounds though, as I’ll explain later in the AppView section.This also means that there’s often an unexpected assymetry between seemingly similar actions: for example, getting a list of people followed by person X is very simple (they’re all X’s records, so they’re all in one place), but getting a list of all followers of X is much harder (each record is in a different place!). This is something that the AppView helps with too, as we’ll see later.A second, complimentary way of storing user data is . Blobs are basically binary files, meant mostly for storing media like images and video. For example, here is a direct link to an image blob showing a photo of when I started writing this blog post. Blobs are stored on the same server as records, but somewhat separate from them, since it’s a different type of data.Each record belongs to a specific “record type” and stores its data organized in a specific structure, which defines what kinds of fields it can have with what types, what they mean, which are required, and so on – kind of like XML/JSON Schema. This schema definition which describes a given record type is called a  in ATProto. (If you’re curious why make a new standard, see threads e.g. here, here, or here, or this blog post).A lexicon needs to have an identifier (called , Namespace Identifier), which uses the reverse domain name format, e.g. . All lexicons that are used to store the data of a specific app are usually grouped under the same prefix, e.g. Bluesky lexicons all start with .The structure of a given lexicon’s records is defined in a special JSON file – for example, this file defines the app.bsky.feed.post lexicon. As you can see, this is the place which for example specifies that a post’s text can have at most 300 characters (more specifically, Unicode graphemes). This also means that you can’t create a different server which would make posts longer than 300 characters that would be Bluesky-compatible and displayed on bsky.app – such posts would not pass the validation against the post record schema, and would be rejected by any server or client which performs such validation. Essentially, whover designs and controls the given lexicon, decides what kinds of data it can hold and any constraints on it. In order to store a different, incompatible type of data, you need to create a new lexicon (although you  add additional fields to a record that aren’t defined in its lexicon; many third party apps are doing that, like e.g. Bridgy Fed).Lexicon name prefixes generally define boundaries between “apps” as in “services”, and between the “territory” that’s owned by different parties. The lexicons and endpoints defined by Bluesky are defined either under  – these are things specific to Bluesky the microblogging service – or under , which are things meant to be used by all ATProto apps and services regardless of the use case. There are also a couple of other minor namespaces like  for the (centralized) DM service, and  for the open source Ozone moderation tool.The lexicon prefix is generally (in most cases) a good way to tell if a piece of the protocol is something Bluesky-specific (specific to the Bluesky service), or something general for all ATProto. There are no record types defined in , so things like post, profile, follow are all Bluesky-specific and under , as are APIs for e.g. searching users, getting timelines, custom feeds and so on. Meanwhile,  APIs deal more with things like: info about a repository, fetching a repository, signing up for a new account, refreshing an access token, downloading a blob, etc.Third party developers and teams building apps on ATProto/Bluesky, which either extend Bluesky’s features or make something completely separate, use their own namespaces for new lexicons, like , , , , and so on. (There is a lot of nuance to whether you should use your own lexicons or reuse or extend existing ones when building things, and there have been a lot of discussions about it on Bluesky, and even conference talks. A good starting point is this blog post by Paul Frazee.)Each user is uniquely identified in the network with their Decentralized Identifier (DID). DIDs are a W3C standard, but (as I understand) this standard mostly just defines a framework, and there can be many different “methods” of storing and resolving the identifiers, and each system that uses it can pick or create different types of those DIDs.The format of a DID is: , where the last part depends on the method. ATProto supports two types of DIDs, but in practice, almost everyone uses one of them, the “plc”. Each DID has a “”, a JSON file (see mine) which describes the account – in ATProto at least, the document includes things such as: the assigned handles, the PDS server hosting the account, and some cryptographic keys.An important thing to note is that ; it’s the only thing that is permanent about your account, because something has to be. There needs to be some unique ID that all databases everywhere can use to identify you, which doesn’t change, and the DID is that ID. This means that you can’t change a DID of one type into another type later.The main DID method is , where IIRC “plc” originally stood for “placeholder” (I think it was meant to be temporary until something better is designed), and was later kind of retconned to mean “Public Ledger of Credentials” 🙃 The DIDs of this type are identified by a random string of characters, which looks like this: did:plc:vc7f4oafdgxsihk4cry2xpze. The DID documents of each DID are stored in a centralized service hosted at plc.directory (Bluesky wants to eventually transfer the ownership to some external non-profit), which basically keeps a key-value store mapping a DID to a JSON file. It also keeps an “audit log” of the previous versions of the document (this means that, for example, the whole history of your old handles is available and you can’t erase it!). There’s also some cryptographic stuff there which, as I understand it, lets anyone verify that everything in the database checks out (don’t ask me how).The other, rarely used method is . Those DIDs look like this: did:web:witchcraft.systems, and the DID document is stored in a specific  path on the given hostname, in this case witchcraft.systems (yes, that’s an actual TLD ;). It does not store an audit log/history like  does.The reason why it’s rarely used and not recommended, is because, first, it’s more complicated to create one (though that’s a solvable problem of course, see a just published guide); but second and more importantly, since DIDs are permanent, this means that your account is permanently bound to that domain. You need to keep it accessible and not let it expire, or you lose the account – you can’t migrate it to  at some point later. It gives you more independence, but at the cost of being tied to that domain you have, and this isn’t a tradeoff that most people are likely to want, and definitely not people who don’t understand what they’re getting into.If you’re fine with that choice, you can create a  account and almost everything in Bluesky and ATProto should work exactly the same. “Almost”, because some services forget to implement that second code path, since it’s so rarely used 😉 but in that case, politely nudging the developer to fix the issue should help in most cases :>What DIDs enable is that since they act as the unique identifier, your handle doesn’t have to, like it does on the Fediverse. I can be  one day,  the next day, and  the week after. All existing connections – follows & followers, my posts, likes, blocks, lists I’m on, mentions in posts, etc. all work as before, because they all reference the DID, not the handle. With mentions specifically it works kinda funny, because they use what’s called a “facets” system (see later section), where the link target is specified separately from the displayed text. So you can have an old post saying “hey @mackuba.bsky.social”, where the handle in it links to my profile which is now named “@mackuba.eu”. The link still works, because it really links to the DID behind the scenes.Unlike on the Fediverse, the format of handles is just a hostname, not username + hostname. You assign a whole hostname to a specific account, and if you own any domain name, that can be your username (and if you own a well known domain name, it’s strongly recommended that you do, as a form of self-verification!).The handle to DID assignment is a two-way link – a DID needs to claim a given handle, and the owner of the domain needs to verify that they own that DID. On the DID side, this happens in the  field of the DID document (see here in mine). On the domain side, there are two ways of verifying a handle, depending on what’s more convenient to you: either a DNS TXT entry, or a file on a  path.You might be wondering how handles like  work – in this case, each such handle is its own domain name, and you can actually enter a domain like aoc.bsky.social into a browser and it will redirect to a Bluesky profile on bsky.app. Behind the scenes, this is normally handled by having a wildcard domain pointing to one service, which responds to HTTP requests on that  path by returning different DIDs, depending on the domain. That’s not only a  thing – e.g. there’s now an open Blacksky PDS server which hands out  handles, and there are even “handle services” which  give out handles – e.g. you can be yourname.swifties.social if you want ;)One place where handle changes break things is (some) post URLs on bsky.app. The official web client uses handles by default in permalinks, which means that if you link to a Bluesky post e.g. from a blog post and you change your handle later, that link will no longer work. You can however replace the handle after  with the user’s DID, and the router accepts such links just fine, they just aren’t used by default. So the form you’d want to use when putting links in a blog post or article (like the one you’re reading) would be something like: https://bsky.app/profile/did:plc:ragtjsm2j2vknwkz3zp4oxrd/post/3llwrsdcdvc2s.Each record can be uniquely addressed with a specific  with the at:// scheme. The format of the URI is:at://<user_DID>/<lexicon_NSID>/<rkey>
 is an identifier of a specific record instance – a usually short alphanumeric string, e.g. Bluesky post rkeys look something like . So a complete post URI might look like this: at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.post/3larljiybf22v. You can look up at:// URIs in some record browser tools, e.g. PDSls.AT URIs are used for all references between records – quotes, replies, likes, mute list entries, and so on. If you look at this like record, for example, its  points to at://did:plc:vwzwgnygau7ed7b7wt5ux7y2/app.bsky.feed.post/3lv2b3f5nys2n, which is the URI of a post record you can see here. Since the URIs use DIDs in the first part, handle changes don’t affect such links.All user data (records and blobs) is stored in a  (or “repo”). The repository is identified by user’s DID, and stores:records, grouped by lexicon into so-called blobs (stored separately from records)authentication data like access tokens, signing keys, hashed passwords etc.Internally, an important part of how the repo stores user records is a data structure called “Merkle Search Tree” – but this isn’t something that you need to understand when using the protocol, unless you’re working on a PDS/relay implementation (I haven’t needed to get into it so far).You can download the records part of your (or anyone else’s!) repo as a bundle called a CAR file, a Content Addressed Archive (fun fact: the icon for the button in the Bluesky app which downloads a repo backup is the shape of a car 🚘).The cool part is that a repository stores all data of the given user, from *all* lexicons. Including third party developer lexicons. This means that if someone has their account hosted on Bluesky servers, but uses third party ATProto apps like Tangled or Grain, Bluesky lets them store these apps’ records like Grain photos or Tangled pull requests on the same server where it keeps their Bluesky posts. (And yes, of course someone made a lexicon/tool for storing arbitrary files on your Bluesky PDS… and did it in Bash, because why not 🙃) is the convention used for APIs in the ATProto network. The API endpoints use the same naming convention as lexicon NSIDs, and they have URLs with paths in the format of , e.g. /xrpc/app.bsky.feed.getPosts. There are similar lexicon definition files which specify what parameters are accepted/required by an endpoint and what types of data are returned in the JSON response. PDSes, AppViews, labellers and feed generators all implement the same kind of API, although with different subsets of specific endpoints. Third party apps don’t  to use the same convention, but it’s generally a good idea, since it integrates better with the rest of the ecosystem.This one is kinda Bluesky-specific, but it’s pretty important to understand, and I think you can reuse it for non-Bluesky apps too.The “” system is something used for links and possibly rich text in future in Bluesky posts. It’s perhaps a little bit unintuitive at first, but it’s pretty neat and allows for a lot of flexibility.The way you handle links, mentions, or hashtags, is that they aren’t highlighted automatically, but you need to specifically mark some range of text as a link using the facets. A facet is a marking of some range of the post text (from-to) with a specific kind of link. If you look e.g. at this post here, you can see that it has a facet marking the byte range 60-67 of the post text as a hashtag “ahoy25”. If there was no facet there, it would just render as normal unlinked text “#ahoy25” in the post (when you see that, it’s an easy tell that a post was made using some custom tool that’s in early stages of development). It works the same way for mention links and normal URL links.(If you’re curious why they implemented it this way, check out this blog post.)Note that the displayed text in the marked fragment doesn’t have to match what the facet links to; this means that you can have links that just use some shorter text for the link instead of a part of the URL, in order to fit more text in one post (although in the official app, clicking such link triggers a warning popup first). E.g. some Hacker News bots commonly use this format, see this post. The Bsky app doesn’t let you create such posts directly, but some other clients like Skeetdeck do.Facets are also used for URL shortening – if you just put a long URL in the text of a post made through the API, it will be neither shortened nor highlighted. You need to manually mark it with a facet, and manually shorten the displayed part to whatever length you want.Likely the most tricky part is that the index numbers you need to use for the ranges are counted on a UTF-8 representation of the text string, but they’re counted in… bytes and not unicode scalars, which most languages index strings in 😅 This is somewhat of an unfortunate tech debt thing as I understand, and it was made this way mostly because of JavaScript, which doesn’t work with UTF-8 natively. But this means you need to be extra careful with the indexes in most languages.Ok, now that we got through the basic pieces, let’s talk about servers:The original copy of all user data is stored on a server called , Personal Data Server. This is the “source of truth”. A PDS stores one or more user accounts and repos, handles user authentication, and serves as an “entry point” to the network when connecting from a client app. Most network requests from the client are sent to your PDS, although only some of them are handled directly by the PDS, and the rest are proxied e.g. to the AppView. So in a way, your PDS kind of serves as your “user agent” in the network on the backend side of things (beyond the client app), especially if it’s under your control.Each PDS has an XRPC API with some number of endpoints for things like listing repositories, listing contents of each, looking up a specific record or blob, account authentication and management, and so on. It also has a websocket API called a “” (the subscribeRepos endpoint). The firehose streams all changes happening on a given PDS (from all repos) as a stream of “events”, where each event is an addition, edit, or deletion of a record in one of the repos, or some change related to an account, like handle change or deactivation.One of the most important features of ATProto is that an account is not permanently assigned to a PDS. Unlike in ActivityPub, where your identifier is e.g.  and it can never change, because everything uses that as the unique ID, here the unique ID is the DID. The PDS host is assigned to a user in the DID document JSON (e.g. on plc.directory), but you can migrate to a different PDS at any point, and at the moment there are even some fairly user-friendly tools available for doing that, like ATP Airport or PDS MOOver (although it’s still a bit unpolished at the moment, and for now you can’t migrate back to Bluesky-hosted PDSes). In theory, you should even be able to migrate to a different PDS if your old PDS is dead or goes rogue, if you have prepared in advance (this is a bit more technical). If everything goes well, nobody even notices that anything has changed (you can’t even easily check in the app what PDS someone is on, although there are external tools for that, like internect.info).Initially, during the limited beta in 2023, Bluesky only had one PDS, . In November 2023, several additional PDSes were created (also under Bluesky PBC control) and existing users were quietly all spread to a random one of those. At that point, the network was already “technically federated”, operating in the target architecture, although with access restricted to only Bluesky-run servers. This restriction was lifted in February 2024 with the public federation launch.Since then, ATProto enthusiasts started setting setting up PDS servers for themselves, either creating alt/test accounts there, or moving their main accounts. As of August 2025, there around 2000 third party PDS servers, although most of them are very small – usually hosting one person’s main and/or test accounts, and maybe those of a couple of their friends. I have a list of them on my website, and there’s also a more complete list here (mine excludes inactive PDSes and empty accounts).As you can see there, there’s one massive PDS for Bridgy Fed, the Bluesky-Mastodon bridge service, hosting around 30-40k bridged accounts from the Fediverse, Threads, Nostr, Flipboard, or the web (blogs); then some number of small to medium PDSes for various services, and a very long tail of servers with single-digit number of accounts. At this moment, large public PDS in the style of Fedi instances aren’t much of a thing yet, although there are at least a few communities working on setting up one (e.g. Blacksky, Northsky, or Turtle Island). Blacksky specifically has opened up for migrations just last week and has now a few hundred real accounts.The vast majority of PDSes at the moment use the reference implementation from Bluesky (written in TypeScript), but there are a few alternative implementations at various levels of maturity (Blacksky’s Rudy Fraser’s rsky written in Rust, cocoon in Go, or millipds in Python). The official version is very easy to set up and very cheap to run – it’s bundled in Docker, and there’s basically one script you need to run and answer a few questions.As for the Bluesky-hosted PDSes, the number is currently in high double digits, and each of them hosts a few hundred thousands of accounts (!). And what’s more, they keep the record data in SQLite databases, one per account. And it works really well, go figure. The Bluesky PDSes are all given names of different kinds of mushrooms (like Amanita, Boletus or Shiitake), hence they are often called “mushroom servers”; you can see the full list e.g. here.  was left as a so-called “”, which handles shared authentication for all Bluesky-hosted PDSes (it’s a private piece of Bluesky PBC infrastructure that’s not open source and not needed for independent PDS hosters).A  is probably the piece of the ATProto architecture that’s most commonly misunderstood by people familiar with other networks like the Fediverse. It doesn’t help that both the Fediverse and Nostr also include servers called “relays”, but they serve a different purpose in each of them:a relay in Nostr is a core piece of the architecture: your posts are uploaded to one or more relays that you have configured and are hosted there, where other users can fetch them froma relay in the Fediverse is an optional helper service that redistributes posts from some number of instances who have opted in to others, in order to make content more discoverable e.g. on hashtag feedsIn ATProto, a relay is a server which combines the firehose streams from all PDSes it knows about into one massive stream that includes every change happening anywhere on the network. Such full-network firehose is then used as the input for many other services, like AppViews, labellers, or feed generators. It serves as a convenient streaming API to get e.g. all posts on the network to process them somehow, or all changes to accounts, or all content in general, from a single place.Initially, the relay was also expected to keep a complete archive of all the data on the network, from all repos, from the beginning of time. This requirement was later removed in the updates late last year, at least partially triggered by the drastic increase in traffic in November 2024, which overwhelmed Bluesky’s and third party servers for at least a few days. Currently, Bluesky’s and other relays are generally “non-archival”, meaning that they live stream current events (+ a buffer of e.g. last 24 or 36 hours), but don’t keep a full archive of all repos (this change has massive lowered the resource requirements / cost of running a relay, making it much more accessible). An archival relay could always be set up too, but I’m not aware of any currently operating.Bluesky operates one main relay at bsky.network, which is used as a data source for their AppView and pretty much everyone else in the ATProto ecosystem at the moment (internally, it’s really some kind of “load balancer” using the rainbow service, with a few real relay servers behind it).The relay code is implemented in Go, and isn’t very hard to get up and running (especially the recent “1.1” update improved things quite a lot). Some people have been running alternative relay services privately for some time, and there is now e.g. a public relay run by Rudy Fraser at atproto.africa (with a custom implementation in Rust! 🦀), and a couple run by Phil @bad-example.com. I’m also running my own small relay, feeding content only from non-Bluesky PDSes.There is also a variant of a relay called Jetstream – it’s a service that reads from a real CBOR relay and outputs a stream that’s JSON based, better organized, and much more lightweight (the full relay includes a lot of additional data that’s mostly used for cryptographic operations and other low-level stuff). For many simpler tools and services, it might make more sense to stream data from that one instead, if only to save bandwidth. (Bluesky runs a couple of instances listed there in the readme, but you can also run your own.)The terribly named  is the second most important piece of the network after the PDS.The AppView is basically an API server that serves processed data to client apps. It’s an equivalent of an API backend (with the databases behind it) that you’d find on a classic social media site like Twitter. AppView streams all new data written on the network from the relay, and saves a copy of it locally in a processed, aggregated and optimized form. For example, an AppView backed by an SQL database could have a  table with a  column, a  table storing all likes with a foreign key , probably also an integer  column in  for optimization, and so on.The AppView is designed to be able to easily give information such as:the latest posts from this userall the replies in a given thread organized in a treemost recent posts on the network with the hashtag #rubylang or mentioning “iOS 26”how many likes/reposts has a given post received, and who made themhow many follows/followers does a given user have, and who are theyis user A allowed to view or reply to a post from user BAll this data originates from users’ PDSes and has its original copy stored there, but the “raw” record don’t always allow you to access all information easily. For example, to find out how many likes a post has, you need to know all  records referencing it from other users, and each of those like records is stored in the liking user’s repo on that user’s PDS. Same with followers, as I mentioned earlier in the section on records, or with building threads (again, different replies in one thread are hosted in different repos), or for basically any kind of search. So having this kind of API with processed data from the entire network is essential for client apps and various tools and services built around Bluesky by other people.AppView also applies some additional rules to the data, sometimes overriding what people post into their PDSes, since anyone can technically post anything into their PDS. For example, the AppView prevents you from looking at the profiles of people who have blocked you, at least when you’re logged in. It also hides them from your followers list, even if they have a  record referencing you, making it seem like they don’t; and if they try to make an  replying to you (they  create such record on their PDS!), it excludes such reply from feeds and threads, as if it never happened. Same goes for “thread gates” which lock access to threads, and so on.The AppView is one of the few components which  completely open source. Initially, the AppView used Postgres as its data store;  version is still in the public repository. In late 2023, Bluesky has migrated to a “v2” version, which uses the NoSQL database ScyllaDB instead, to be able to handle the massive read traffic from many millions of concurrent users. The upper layer with the “business logic” is kept in the public repository, while the so called “dataplane” layer that interacts directly with Scylla is not. The reason is mostly that it’s built for a specific hardware setup they have and wouldn’t be directly usable by others, while it would add some unnecesary work for the team to publish it. It’s still possible to run the AppView with the old Postgres-based data layer (and I think the team uses that internally for development), it just can’t handle as much traffic as the current live version.This is the piece that’s hardest to run yourself, and one that requires the most resources. That said, a private AppView should be possible to run right now for under $200/month – the biggest requirement is at least a few TB of disk space. The truly costly part is not collecting and storing all this data, but serving it to a huge number of users who would use it as a backend for the client app in daily use. An alternative full-network Bluesky AppView that is used by a few thousands of users shouldn’t be very hard to run, but to be able to serve millions, you’ll need a lot of hardware and something more custom than the Postgres-based version.There have also been some attempts at alternative implementations – the most advanced right now is AppViewLite, built in C#, which goes to great lengths to minimize the resource use.A part of the AppView (at least the Bluesky one) is also a CDN for serving images & videos. The API responses from e.g.  or  generally include links to any media on the Bluesky CDN hostname, not directly on the PDS, even though you  fetch every blob from the PDS, since that’s the “source of truth” (although IIRC the Bluesky PDS implementation doesn’t set the CORS headers there). It’s recommended to access any media this way in order to not use too much bandwidth from the PDS.(Or “labelers” officially, but I like the British spelling more here, sue me ¯\_(ツ)_/¯)We’re now getting to more Bluesky specific things (i.e. specific for the Bluesky-service, although some parts of it are ATProto-general and mentioned on the atproto.com site).A  is a moderation service for Bluesky (or other ATProto app), which can be run by third parties. Labellers emit labels, which are assigned to an account or a record (like a post). Each labeller defines its own set of labels, depending on what it’s focusing on; then, users can “subscribe” to a labeller and choose how they want to handle the labels it assigns: you can hide the labelled posts/users, mark them with a warning badge, or ignore given label.Labellers were initially designed to just do community moderation of unwanted content, e.g. you can have a service focused on fighting racism, transphobia, or right-wing extremism, and that service helps protect its users from some kinds of bad actors; or you can have one marking e.g. posts with political content, users who follow 20k accounts, or who post way too many hashtags. In practice, many existing labellers are meant for self-labelling instead, letting you assign e.g. a country flag or some fun things like a D&D character class to yourself.The way it works technically is:a labeller either runs a firehose client pulling posts from the relay, or relies on reports from users and/or its operating team (usually using the Ozone tool for that)labels, which are lightweight objects ( ATProto records) are emitted from labeller’s special firehose stream (the subscribeLabels endpoint)the AppView listens to the label firehoses of all labellers it knows about, in addition to the relay stream, and records all received labels in its databasewhen a logged in user pulls data like threads or timelines from the AppView, it adds relevant label info to the responses depending on which labellers the user followsthe specific list of labellers whose labels should be applied is passed explicitly in API requests in the  header (there is a “soft” limit of 20 labellers you can pass at a time, which is why the official app won’t let you subscribe to more)in the official app, Bluesky’s official moderation service (which is “just” another labeller) is hardcoded as one of those 20 and you can’t turn it off; when connecting from your own app or tool, you’re free to ignore it if you want(Read more about labellers here.)Custom feeds are one of the coolest features of Bluesky. They let you create any kind of feed using any algorithm and let everyone on the platform use it (even as the default feed, if they want to).The way this system works is that you need to run a “” service on your server. In that service, you expose an API that the AppView can call, which returns a list of post at:// URIs selected by you however you want in response to a given request.A minimal feed service can be pretty simple – the API is just three endpoints, two of which are static, and the third returns the post URIs. One “small” problem is that in order to return the post URIs, you need to have some info about posts stored up front, which in practice means that you almost always need to connect to a relay’s firehose stream and store some post data (of selected or all posts, depending on your use case).a feed record is uploaded to your repo, including metadata and location of the feed generator service, which lets other users find your feedwhen the user opens that feed in the app, the AppView makes a request to your service on their behalfyour service looks at the request params and headers, and returns a list of posts it selected in the form of at:// URIsthe AppView takes those URIs and maps them to full posts (so-called “hydration”), which it returns to the user’s appHow exactly those posts are selected to be returned in the given request is completely up to you, the only requirement is that these are posts that the AppView will have in its database, since you only send URIs, not actual post data. In most cases, feeds use some kind of keyword/regexp matching and chronological ordering, but you can even build very complex, AI-driven algorithmic “For You” style personalized feeds.You don’t necessarily have to code a feed service yourself and host it in order to have a custom feed – there are a few feed hosting services that don’t require technical knowledge to use, like SkyFeed or Graze.Ok, that’s technically not a server, but stay with me…The final piece that you need to fully enjoy Bluesky is the client app – a mobile/desktop one or a web frontend. Unlike on Fedi, where an instance software like Mastodon usually includes a built-in web frontend that is your main interface for accessing the service, the PDS doesn’t include anything like that, just a database and an API (which also means it’s much more lightweight and needs less resources). All browsing is done through a separate client, and the client always does everything through the public API – kind of like when you run a custom web client for Mastodon like Elk or Phanpy, you connect it to your instance, and you view your timeline on elk.zone.So when you go to bsky.app, that’s what you’re seeing – a web client that connects to your PDS (Bluesky-hosted or self-hosted) through the public API, no more, no less. The official app is built for both mobile platforms and for the web from a single React Native codebase (apparently React Native on the web and normal web React is not the same thing 🧐). This has allowed the still very small frontend team (and IIRC at first it was literally just Paul) to build the app for three platforms in any reasonable amount of time and maintain it going forward. The downside is that it’s kinda neither a great webapp nor a great mobile app… But the team is doing what they can to improve it, and it’s already much better than it used to be, and tbh more than good enough for me.There aren’t nearly as many alternative clients as there are for Mastodon, and none of them are  great, but there are a few options; see the apps part of my Bluesky Guide blog post for links.Notice that I haven’t mentioned DMs anywhere – that’s because they aren’t a part of the protocol at the moment. The Bluesky team wants to eventually add some properly implemented, end-to-end encrypted, secure DMs using some open standard, but they won’t be able to finish that in the short term, and a lot of people were asking for at least some simple version of DMs in the app. So they’ve decided as an interim solution to implement them as a fully centralized, closed source service. It is accessible to third-party Bluesky clients through the API (the  namespace), but it’s not something you can run yourself. The team is very open about the fact that it’s not a proper replacement for something like Signal, and that for sensitive communication, you should ideally just use it for swapping contacts on Signal on iMessage and move the conversation there. They also kinda don’t want to spend too much time adding features there, because it’s considered a temporary solution, so it’s pretty basic in terms of available features.There are also a few other closed-source helper services, like the “cardyb” they use for generating link card details, or the video service for preprocessing videos, but they’re all specific to some Bluesky use cases only and not strictly necessary to use.So the flow and hierarchy is like this:the  you use creates new records as a result of actions you take (new posts, likes, follows), and saves them into your PDSyour  emits events on its firehose with the record detailsBluesky  and other relays are connected to the firehoses of each PDS they know about (your PDS generally needs to ask them to connect using the  ENV variable), and they pass those events to their output firehosethe Bluesky  (and other AppViews) listen to the firehose of their selected relay (though it could be multiple relays, or it could even just stream directly from PDSes, but in practice this will normally be one trusted relay)the AppView gets events including your records, and if they are relevant, saves the data to its internal database in some appropriate representationswhen other users browse Bluesky in their client apps, they load timelines, feeds and threads from the AppView, which returns info about your post from that database it saved it to run by third party feed operators also stream data from Bluesky’s or some other relay and save it locally, so they can respond to feed requests from the AppView also stream data from Bluesky’s or some other relay, and emit labels on their firehoses, which get sent to the AppView (note: there is no official “labeller relay” sitting between labellers and the AppView, although one third party dev wrote one)PDSes do not connect to each other directly, and they don’t store posts of users from other PDSes, only their ownalthough right now basically everyone uses the Bluesky relay and AppView, anyone  set up their own alternative relays and AppViews, which feed from all or any subset of known PDSesPDS chooses which relays to ask to connect, but relays can also connect by themselves to a PDS or another relay; AppView chooses which relay(s) it streams data from; and PDS chooses which AppView it loads timelines & threads fromit’s absolutely possible and expected that two users using different PDSes, which use separate AppViews feeding from separate relays will be able to talk to each other and see each other’s responses on their own AppView, as long as the users aren’t banned on the other user’s infrastructureThe metaphor that’s often used to describe these relationship is that PDSes are like websites which publish some blog posts, and relays & AppViews are like search engines which crawl and index the web, and then let you look up results in them. In most cases, a website should be indexed and visible in all/most available search engines.And that’s about it – I think with the above, you should have a pretty good grasp of the big picture of ATProto architecture and all the specific parts of it. Now, if you want to start playing with the protocol and building some things on it, a lot will depend on what specifically you want to build and using what languages/technologies:Two languages are officially supported by Bluesky:JavaScript/TypeScript, in which most of their code is written (see the packages folder in the  repo)Go, which is used in some backend pieces like the relay, or the goat command line tool used e.g. for PDS migrations (see the  repo)For other languages, I have a website called sdk.blue, which lists all libraries and SDKs I know about, grouped by language. As you can see, there is something there for most major languages; I’ve built and maintain a group of Ruby gems myself. If you want to use a language that doesn’t have any libraries yet, it’s really not that hard to make one from scratch – for most things you just need an HTTP client and a JSON parser, and maybe a websocket client.There is quite a lot of official documentation, although it’s a bit spread out and sometimes not easy to find.The places to look in are:atproto.com – the official AT Protocol website; a bit more formal documentation about the elements of the protocol, kind of like what I did here, but with much more info and detailed specifications of each thingdocs.bsky.app – more practical documentation with guides and examples of specific use cases in TS & Python (roll down the sections in the sidebar); it shows examples of how to make a post, upload a video, how to connect to the firehose, how to make a custom feed, etc.something that I also find useful is to have the atproto repo checked out locally and opened in the editor, and look things up in the JSON files from the /lexicons folderAnd a few other articles that might work better for you:Someone said recently that “bsky replies are the only real documentation for ATProto”, and honestly, they’re not wrong. We have a great community of third party developers now, building their own tools, apps, libraries, services, even organizing conferences. If you’re starting out and you have any questions, just ask and someone will probably help, and some of the Bluesky team developers are also very active in Bluesky threads, answering questions and clarifying things. So a lot of such knowledge that’s not necessarily found in the official docs can be found somewhere on Bluesky.The two places I recommend looking at are:the “ATProto Touchers” Discord chat – ping me or some other developer for an invite :)my ATProto feed on Bluesky, which tries to catch any ATProto development discussions – it should include posts with any mention of “ATProto” or things like “AppView” or various API names and technical terms, or you can use  or  hashtag to be sureAlso, there’s a fantastic newsletter called Connected Places (formerly Fediverse Report) by Laurens Hof, who publishes two separate editions every week, about what’s happening in Bluesky/ATProto and in the Fediverse (and *a lot* of things are happening).Some easy ways to start tinkering:use one of the existing libraries for your favorite language and make a website or command-line tool which loads some data from the AppView or PDS: load and print timelines, calculate statistics, browse contents of PDSes and repos, etc.make a bot that posts something (not spammy!)connect to the relay firehose and print or record some specific types of dataAnd a couple of tools which will certainly be useful in development:internect.info – look up an account by handle/DID and see details like assigned PDS or handle historyPDSls – PDS and repository browser, lets you look up repos by account DID or records by at:// URI (there are a few others, but this one is most popular)]]></content:encoded></item><item><title>Zedless: Zed fork focused on privacy and being local-first</title><link>https://github.com/zedless-editor/zed</link><author>homebrewer</author><category>hn</category><pubDate>Wed, 20 Aug 2025 18:47:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Project to formalise a proof of Fermat’s Last Theorem in the Lean theorem prover</title><link>https://imperialcollegelondon.github.io/FLT/</link><author>ljlolel</author><category>hn</category><pubDate>Wed, 20 Aug 2025 18:27:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pixel 10 Phones</title><link>https://blog.google/products/pixel/google-pixel-10-pro-xl/</link><author>gotmedium</author><category>hn</category><pubDate>Wed, 20 Aug 2025 17:24:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Pixel 10 features a satin-finish metal frame, polished glass back and our iconic camera bar. You’ll have four expressive color options to choose from, too: Obsidian, Frost, Indigo and Lemongrass. Its 6.3-inch Actua display is now even brighter at 3000 nits making for easy viewing. That’s paired with improved audio, including exceptional bass so your favorite shows sound and look better than ever.Pixel 10 also comes with huge camera improvements. That includes the first 5x telephoto lens on this tier of Pixel with fast autofocus, 10x optical quality and up to 20x zoom with Super Res Zoom — so it's easier to shoot from a distance.]]></content:encoded></item><item><title>An Update on Pytype</title><link>https://github.com/google/pytype</link><author>mxmlnkn</author><category>hn</category><pubDate>Wed, 20 Aug 2025 17:04:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Debugging Behind the Iron Curtain (2010)</title><link>https://www.jakepoz.com/debugging-behind-the-iron-curtain/</link><author>indrora</author><category>hn</category><pubDate>Wed, 20 Aug 2025 16:53:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Sergei is a veteran of the early days of the computing industry as it was developing in the Soviet Union. I had the pleasure of working and learning from him over the past year, and in that time I picked up more important lessons about both life and embedded programming than any amount of school could ever teach. The most striking lesson is the story of how and why, in late summer of 1986, Sergei decided to move his family out of the Soviet Union.In the 1980s, my mentor Sergei was writing software for an SM-1800, a Soviet clone of the PDP-11. The microcomputer was just installed at a railroad station near Sverdlovsk, a major shipping center for the U.S.S.R. at the time. The new system was designed to route train cars and cargo to their intended destinations, but there was a nasty bug that was causing random failures and crashes. The crashes would always occur once everyone had gone home for the night, but despite extensive investigation, the computer always performed flawlessly during manual and automatic testing procedures the next day. Usually this indicates a race condition or some other concurrency bug that only manifests itself under certain circumstances. Tired of late night phone calls from the station, Sergei decided to get to the bottom of it, and his first step was to learn exactly which conditions in the rail yard were causing the computer to crash.He first compiled a history of all occurrences of the unexplained crashes and plotted their dates and times on a calendar. Sure enough, a pattern was clearly visible. By observing the behavior for several more days, Sergei saw he could easily predict the timing of future system failures.He soon figured out that the rail yard computer malfunctioned only when the cargo being processed was live cattle coming in from northern Ukraine and western Russia heading to a nearby slaughterhouse. In and of itself this was strange, as the local slaughterhouse had in the past been supplied with livestock from farms located much closer, in Kazakhstan.As you may know, the Chernobyl Nuclear Power Plant disaster occurred in 1986 and spread deadly levels of radiation which to this day make the nearby area uninhabitable. The radioactivity caused broad contamination in the surrounding areas, including northern Ukraine, Belarus, and western Russia. Suspicious of possibly high levels of radiation in the incoming train cars, Sergei devised a method to test his theory. Possession of personal Geiger counters was restricted by the Soviet government, so he went drinking with a few military personnel stationed at the rail yard. After a few shots of vodka, he was able to convince a soldier to measure one of the suspected rail cars, and they discovered the radiation levels were orders of magnitude above normal.Not only were the cattle shipments highly contaminated with radiation, the levels were high enough to randomly flip bits in the memory of the SM-1800, which was located in a building close to the railroad tracks.There were often significant food shortages in the Soviet Union, and the government plan was to mix the meat from Chernobyl-area cattle with the uncontaminated meat from the rest of the country. This would lower the average radiation levels of the meat without wasting valuable resources. Upon discovering this, Sergei immediately filed immigration papers with any country that would listen. The computer crashes resolved themselves as radiation levels dropped over time.]]></content:encoded></item><item><title>Show HN: Anchor Relay – A faster, easier way to get Let&apos;s Encrypt certificates</title><link>https://anchor.dev/relay</link><author>geemus</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 16:13:18 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Luminal – Open-source, search-based GPU compiler</title><link>https://github.com/luminal-ai/luminal</link><author>jafioti</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 16:01:13 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi HN, I’m Joe. My friends Matthew, Jake and I are building Luminal (https://luminalai.com/), a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance.We take high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: https://youtu.be/P2oNR8zxSAAOur approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.We aim to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo: https://github.com/luminal-ai/luminal and I’d love to hear your thoughts!]]></content:encoded></item><item><title>OPA maintainers and Styra employees hired by Apple</title><link>https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371</link><author>crcsmnky</author><category>hn</category><pubDate>Wed, 20 Aug 2025 15:44:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Launch HN: Channel3 (YC S25) – A database of every product on the internet</title><link>https://news.ycombinator.com/item?id=44962881</link><author>glawrence13</author><category>hn</category><pubDate>Wed, 20 Aug 2025 15:34:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hi HN — we’re George and Alex, building Channel3 (https://trychannel3.com/), a database of every product on the internet, searchable via text/image, and with built-in affiliate monetization. Here’s a demo: https://www.youtube.com/watch?v=Mx8FyP7KvJg.It’s surprisingly hard to find good product data. If you want your software to recommend products and deep-link to merchants, you’ll quickly discover that the data you need—clean titles, normalized attributes, deduped listings, current prices and inventory, variant options, images, and brand info—is not just messy; it’s also spread across a long, long tail of retailers, and often lives behind advanced bot-detection systems.We ran into this problem while building an AI teacher that could recommend relevant supplies. We asked Exa for products, but got back articles, not structured data. Same for Tavily and Bing (deprecated as of 8/13/25). And we got rejected from affiliate programs, who suggested we come back with 1000s of TikTok followers. Channel3 is the API we wished we had.Product detail pages (PDPs) usually present the main item alongside recommendations. We use computer vision to isolate the main product and find its attributes, like title and price. We apply the same logic to the rest of the PDPs on the domain.Products are often sold across multiple retailers, with no guarantee they’ll be labeled consistently. We collapse products across the web into a canonicalized set by using LLMs and multimodal embeddings to actually understand each product.To normalize everything into a schema that tries to be both minimal and extensible, we have to be opinionated. Are a $50 10” and $60 12” skillet the same product? Probably not, but the S/M/L variants of a T-shirt are. Our goal is that any product you’d search for specifically is treated as its own product.We process a massive amount of data. We quickly ran out of room on our Cloudflare Vectorize indices and moved to the brand-new AWS S3 Vectors platform, syncing to OpenSearch for faster response times and more dynamic filtering. We hit rate limits constantly, so we spread our work over multiple cloud providers and AI models.Developers earn commission on sales they drive (averaging 5%). Channel3 takes a cut. We want you to earn way more money from Channel3 than you spend on it. We win when you win.We provide an API, SDK (Typescript and Python), and MCP. We offer 1000 free searches, and charge $7/1000 searches after that. You can view expected commissions per-brand on our dashboard.So far, products are US-only (sorry! we will expand). We’re live with millions of products and hundreds of developers.To get started, make a free account at https://trychannel3.com, then select which brands you’d like to sell (choose from 50k+ or request your own), generate an API key, and start selling and earning.We’d really appreciate feedback from this community. If you’ve built product search before, what did we miss in the schema? If you’ve tried to add commerce to an app, what blocked you? If you tried to build this yourself, what did you learn? Are there endpoints you wish existed (e.g. price alerts, back-in-stock webhooks, product feed)? For any suggestions, we’re all ears.We’ll be in the thread all day to answer questions, share more technical detail, and hear whatever would make this most useful to you. Comment away!]]></content:encoded></item><item><title>Closer to the Metal: Leaving Playwright for CDP</title><link>https://browser-use.com/posts/playwright-to-cdp</link><author>gregpr07</author><category>hn</category><pubDate>Wed, 20 Aug 2025 15:32:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Goodbye Playwright, Hello CDPPlaywright and Puppeteer are great for making QA tests and automation scripts short and readable, but as AI browser companies have been learning the hard way over the last year, sometimes these adapters obscure important details about the underlying browsers.We decided to peek behind the curtain and figure out what the browser was really doing, and it made us decide to drop playwright entirely and just speak the browser's native tongue: CDP.By switcing to raw CDP we've massively increased the speed of element extraction, screenshots, and all our default actions. We've also managed to add new async reaction capabilities to the agent, and proper cross-origin iframe support.Building AI browser automation is like building on top of a jenga tower of complexity. Every layer presents its own leaky abstractions, its own subtle crashes, and its own resource constraints.If you've ever heavily depended on an adapter library and build up a large codebase around it, you know the feeling that eventually comes when you realize the adapter library is no longer saving you any time by "hiding the true complexity". In our case that time has finally come for Browser-Use and playwright-python, the library that we've historically used to drive our browsers with LLM-powered tool calls like , , .At first glance it may seem foolish to throw out such a mature adapter library and reinvent the wheel, but luckily the needs of AI browser agents are much narrower than the entire surface area that playwright provides, and we believe we can implement the calls we need with more specialized logic to better suit AI drivers.Playwright also introduces a 2nd network hop going through a node.js playwright server websocket, which incurs a meaningful amount of latency when we do thousands of CDP calls to check for element position, opacity, paint order, JS event listeners, aria properties, etc.📜 A Quick History of Browser AutomationTo really understand why the browser automation is in the state it's in today, we have to look back at some history. – Lynx (text-mode browser) could browse and automate keystroke inputs from a script, still useful today! – Netscape Navigator (Unix) exposed netscape -remote "openURL(http://…)") to control an already-running GUI browser – Internet Explorer (Windows) exposed a COM automation object (InternetExplorer.Application) so VB/VBA/WSH could launch, navigate, read/write the DOM, handle events, etc. – Mercury’s web-focused tool appears: Astra QuickTest (which evolved into QuickTest Professional/QTP, later HP UFT), also WinRunner/XRunner – headless & macro tools: HttpUnit (1999) (HTTP/HTML-level, no real browser), iMacros (2001) (record/replay in the browser), HtmlUnit (2002) (headless Java “browser”) – Watir (Ruby) grows out of driving IE via COM/OLE and starts offering more general APIs - Selenium + WebDriver join forces — Before headless Chrome,  (a headless WebKit-based browser) filled the gap for scripting “like a browser,” with mixed reliability.The Pre-Headless Era (~~the Dark Ages~~) — Chrome ships ; work happens “upstream” in WebKit so other ports can adopt it (post by Pavel Feldman). — WebKit Remote Debugging Protocol v1.0 announced; early docs/talks outline the domains/events model that CDP still uses. — Blink forks from WebKit; the protocol solidifies on the Chromium side and becomes known as the Chrome DevTools Protocol (CDP). Extensions can tunnel it via  and  flag. — Chrome’s  (accessibility/automation) extension API appears (exposes the accessibility tree; separate from CDP).Headless Chrome & CDP Era —  announced;  introduced as a Chrome team Node library to drive Chrome (headless/full) via CDP. —  ships. —  becomes a  (cross-browser standard).  implements W3C WebDriver (and later BiDi) and is tightly coupled to Chrome releases. — Google I/O talk by Andrey Lushnikov & Joel Einbinder (DevTools/Puppeteer team) popularizes modern testing with Puppeteer.Multi-Browser Standardization Era — Several core Puppeteer engineers leave Google for Microsoft and start  (cross-browser automation/test framework) 🎭 (oooo drama) —  public release. —  ships. —  support begins (e.g., Playwright for Python announced Sep 30, 2020). —  adds  support (alongside classic WebDriver). —  adds  support; Selenium “welcomes Puppeteer to the WebDriver world.”This wheel has been reinvented every few years it seems.Modern Times: A Multitude of ChoiceNow in 2025 we are lucky to have many high quality driver libraries to choose between, our favorites include:⭐️  (best python-first playwright replacement)⭐️  (best CDP reference implementation),  (great CDP debug tooling) (and Selenium Grid) still a great mature option today,  (automate with old-school WebDrivers) automate via system-level accessibility APIs on Android, iOS, macOS, WindowsSo why did we feel the need to write our own with ? Well for all the same reasons as everyone else: everlasting desire to be closer to the metal and have more detailed control over every step.How do Browser Drivers Work?So what APIs does the browser actually expose anyway?
What sits underneath all these "drivers"?🔌 What are the automation APIs that Chromium actually exposes?All these adapter libraries, drivers, and AI helper extensions really just exist to pass messages and make RPC calls to these underlying browser APIs:chrome.tabs.captureVisibleTab()chrome.automation.getTree()chrome.scripting.executeScript()chrome.debugger.sendCommand({tabId: 123}, "Page.navigate", {url})... and many more ... appear to be the most powerful at first glance because they encompass CDP with , but raw CDP lets you access some calls that are not available through , and allows parallel connections to multiple targets. (via pure CDP Websocket or WebDriver BIDI socket)DOMSnapshot.captureSnapshot()Page.handleJavaScriptDialog({accept: true})Browser.setDownloadBehavior()OS-Level Accessibility & screenreader APIs (NVDA/Voice Over/AppleScript/Appium/etc.)get a tree/rotor view of all elements shown to screenreaders (links, buttons, inputs, ...)script copy/paste, mouse, keypress, and element rotor/tab-based navigationInternal Chromium C++ APIs (within Chromium source code)you can call arbitrary helpers content/browser/devtools/protocol/*_handler.ccyou can edit the CDP spec and add commands to call custom C++ APIs third_party/blink/public/devtools_protocol/browser_protocol.pdl... anything is possible when the call is coming from inside the house ...Launch Flags, User Data Dir, and Preferenceswe've tested over 300+ chrome launch flags, launching is a whole world of complex behavior: https://peter.sh/experiments/chromium-command-line-switches/user data dir state can drastically affect browser behavior, there are preferences files, state dbs, cookie stores https://github.com/thewh1teagle/rookie and moreProfile Preferences (e.g. theme color, downloads dir, extension display settings, etc.), enterprise / registry options, and  optionsClassic WebDriver W3C / ChromeDriver REST APIs/session/{id}/element/{eid}/click... etc.
These are not actually exposed by any browser directly, rather they are the W3C standardized REST API shape recommended for drivers (ChromeDriver, GeckoDriver, WebKitDriver, selenium) to provide to clients above raw browser calls via CDP/BIDI.Webdriver BIDI (websocket)merger of the old REST-API based WebDriver system + CDP in a single websocket, official release delayed for years now, not feature complete yet. check back in 2027🎭 How does Playwright work?Playwright achieves multi-languge support by using a client-server model between clients in various languages and a single core implementation that runs as a node.js websocket server.The playwright node.js relay server accepts standardized "playwright protocol" RPC calls from playwright clients, and then sends out CDP or BIDI calls to the browser to execute them.This API is elegant in some ways, the "playwright protocol" of commands provides a nicely typed RPC interface and standardizes behavior across languages. Playwright also nicely abstracts lower-level browser ideas like targets, frames, and sessions into simple  and  handles and (usually) manages to keep those handles in sync and not deadlocked across node.js, the browser, and python.Unfortunately the double RPC through the node.js relay means some state inevitably drifts across the 3 places (and across three different languages and runtimes):playwright node.js relay processWhen a tab crashes in the browser or some operation is performed without focusing a page correctly, there are edge cases where the node.js process can hang indefinitely waiting for a browser reply, meanwhile the python client needs to send the CDP call the browser is expecting in order to proceed.
Currently we have no recourse but to kill -9 and attempt to reconnect to the browser from scratch with a new playwright instance.There are numerous cases like that only crop up in 1% of cases with specific slow network conditions, but edge cases can quickly drag down overall success scores when we run thousands of steps per eval.🩸 Playwright's Sharp EdgesThe playwright happy paths usually work fine, but the devil is in the details: screenshot on pages longer than  high (reliably crashes playwright)// handlingattempting to keyboard/mouse/dialog input without focusing a pagefile upload & download handling on remote browsers, , , , PDF tab handlingchrome preferences and enterprise/registry configuration management helps but it only goes so far, at a certain point it doens't make sense to build workarounds around a relay layer that we're fighting to customize and control anyway.Sometimes when you are forced to thoroughly stretch every nook and cranny of an adapter layer, you start to see the ugly truths of the underlying resource, and you no longer want the "pretty version" as a veil pulled over your eyes, you'd rather see the ugly truth.🍳 Starting From Scratch: Out of the frying pan and into the fireDelivering a reliable experience when so many of the underlying components are inherently unreliable (or actively adversarial) is a monumental engineering challenge.Did you know there are at least 10 different ways a tab can crash in Chrome?all targets start in a briefly semi-"crashed"/unresponsive state while initial requests are inflight, before the main page JS thread startschrome zygote/root process can crash (slow user_data_dir/filesystem io, oom, cpu lag, etc.)GPU process can crash, there's even a helpful CDP call page renderers can crash due to exceptions raised within chrome source (sigsev, oom, etc.)page renderers can crash because the page exceeds allowed resources ()page can spinlock/oom due to infinite loops or crypto mining in its JS main threadscrolling/input/screenshot before  focus can crash targets (5sec delayed!)handling a JS popup before activateTarget or attempting to handle it after already closingparent frame navigation during child  "are you sure you want to leave?"any of the above crashes in a nested OOPIF leading to subtle issues in the parent targetPlaywright handled about half of these well, and presented impassible barrier to solving the other half, so we made the call to switch. But now we're faced with the difficult challenge of solving 100% of these cases ourself.We take on this challenge with glee, we'd rather lose sleep thinking about these things so you can build reliable apps on top of us 💪.Case Studies: Key Changes in the MigrationNew CDP-USE Library Providing Python Type BindingsA type-safe Python client generator for the Chrome DevTools Protocol (CDP). This library automatically generates Python bindings with full TypeScript-like type safety from the official CDP protocol specifications. It's only shallow type bindings, no complex logic for session management, pages, elements, etc. just 100% direct access.New Event-Driven ArchitecturreWe used to only update our view of the world between actions, right before sending the next state summary to the LLM. This makes sense when your assumption is that the page contents will only change as a result of actions, but this is not always true!Take for example a slowly loading list of results that stream in, an animated carousel, or a bit of JS that runs every 3s. All of these are examples of things that can happen at any point in the agent action/runloop cycle.We've introduced a new event-driven architecture to better fit the underlying event-driven architecture of CDP. Now we can subscribe to and respond to CDP events, which we set up in "watchdog" services that monitor for various things.For example, our  watches for any file downloads that start spontaneously, whether triggered by a click, js executing, or any other method.  can now watch for page crashes in a single place by just subscribing to a crash event, and we no longer have to scatter crash detection and retry logic all over the rest of the codebase.New Extracted Element Handle that works across OOPIFsA tab is not a page; it’s a constellation of  (root + cross-origin iframes + workers), each hosting , each containing . Abstract that away and you lose the ability to route input, correlate events, and re-find elements after DOM churn.We now represent nodes with "super-selectors" that include , , , x/y position, and fallback selectors:@dataclass(frozen=True)
class EnhancedDOMTreeNode:
    target_id: str                 # which DevTools target owns the renderer
    frame_id: str                  # which frame inside that target
    backend_node_id: int           # renderer-local node handle
    frame_path: Tuple[str, ...]    # root → ... → leaf, useful for sanity checks
    element_index: int             # LLM-friendly stable ordinal for UX
    ...
class BrowserSession:
    # caches are kept warm by watchers listening to Target.* and Page.*
    def cdp_client_for_frame(self, frame_id: str):
        target_id = self.target_id_by_frame_id(frame_id)
        return self.cdp_clients_for_target(target_id)[0]  # long-lived session

    def route_to_node(self, ref: EnhancedDOMRef):
        client = self.cdp_client_for_frame(ref.frame_id)
        return client, {"session_id": self.session_id_by_frame_id(ref.frame_id)}
Outcome: zero guessing about who owns the node or where input should land, even with nested cross-origin iframes and DOM element shifts after actions.Back in my first startup job in 2014 we were using PhantomJS and some RPC between python and JS, and in a way it's surprising how little has changed since then. Now it's 2025 I'm still dealing with all the same issues: tab crash handling, page load retrying, JS+Python RPC translation issues, python asyncio headaches, mouse movement fuzzing, etc.Luckily a lot has improved since 2014, and we finally have a big light at end of the tunnel leading out of the manual QA automation mines: AI.We aim to continue solving all the complexities of browser automation and CDP for our users. Our agents shouldn't have to know the nuances of CDP Targets in order to Get Stuff Done™️, and neither should you.Try out our new libraries and beta releases with cdp-use and let us know your feedback!]]></content:encoded></item><item><title>AWS in 2025: Stuff you think you know that&apos;s now wrong</title><link>https://www.lastweekinaws.com/blog/aws-in-2025-the-stuff-you-think-you-know-thats-now-wrong/</link><author>keithly</author><category>hn</category><pubDate>Wed, 20 Aug 2025 15:30:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[One of the neat things about AWS is that it’s almost twenty years old. One of the unfortunate things about AWS is… that it’s almost twenty years old. If you’ve been using the platform for a while, it can be hard to notice the pace of change in the underlying “foundational” services. More worryingly, even if you’re not an old saw at AWS scrying, it’s still easy to stumble upon outdated blog posts that speak to the way things used to be, rather than the way they are now. I’ve gathered some of these evolutions that may help you out if you find yourself confused.In EC2, you can now change security groups and IAM roles without shutting the instance down to do it. You can also resize, attach, or detach EBS volumes from running instances. As of very recently, you can also force EC2 instances to stop or terminate without waiting for a clean shutdown or a ridiculous timeout, which is great for things you’re never going to spin back up. They also added the ability to live-migrate instances to other physical hosts; this manifests as it being much rarer nowadays to see an instance degradation notice. Similarly, instances have gone from a “expect this to disappear out from under you at any time” level of reliability to that being almost unheard of in the modern era. Spot instances used to be much more of a bidding war / marketplace. These days the shifts are way more gradual, and you get to feel a little bit less like an investment banker watching the numbers move on your dashboards in realtime. You almost never need dedicated instances for anything. It’s been nearly a decade since they weren’t needed for HIPAA BAAs. AMI Block Public Access is now default for new accounts, and was turned on for any accounts that hadn’t owned a public AMI for 90 days back in 2023.S3 isn’t eventually consistent anymore–it’s read-after-write consistent.You don’t have to randomize the first part of your object keys to ensure they get spread around and avoid hotspots. ACLs are deprecated and off by default on new buckets.Block Public Access is now enabled by default on new buckets.New buckets are transparently encrypted at rest. Once upon a time Glacier was its own service that had nothing to do with S3. If you look closely (hi, billing data!) you can see vestiges of how this used to be, before the S3 team absorbed it as a series of storage classes. Similarly, there used to be truly horrifying restore fees for Glacier that were also very hard to predict. That got fixed early on, but the scary stories left scars to the point where I still encounter folks who think restores are both fiendishly expensive as well as confusing. They are not.Glacier restores are also no longer painfully slow.Obviously EC2-classic is gone, but that was a long time ago. One caveat that does come up a lot is that public v4 IP addresses are no longer free; they cost the same as Elastic IP addresses. VPC peering used to be annoying; now there are better options like Transit Gateway, VPC sharing between accounts,  sharing between accounts, and Cloud WAN. VPC Lattice exists as a way for things to talk to one another and basically ignore a bunch of AWS networking gotchas. So does Tailscale.CloudFront isn’t networking but it has been in the AWS “networking” section for ages so you can deal with it: it used to take ~45 minutes for an update, which was terrible. Nowadays it’s closer to 5 minutes—which still feels like 45 when you’re waiting for CloudFormation to finish a deployment.ELB Classic (“classic” means “deprecated” in AWS land) used to charge cross AZ data transfer in addition to the load balancer “data has passed through me” fee to send to backends on a different availability zone. ALBs with automatic zone load balancing do not charge additional data transfer fees for cross-AZ traffic, just their LCU fees. The same is true for Classic Load Balancers, but be warned: Network Load Balancers still charge cross-AZ fees!Network Load Balancers didn’t used to support security groups, but they do now. Availability Zones used to be randomized between accounts (my us-east-1a was your us-east-1c); you can now use Resource Access Manager to get zone IDs to ensure you’re aligned between any given accounts.Originally Lambda had a 5 minute timeout and didn’t support container images. Now you can run them for up to 15 minutes, use Docker images, use shared storage with EFS, give them up to 10GB of RAM (for which CPU scales accordingly and invisibly), and give /tmp up to 10GB of storage instead of  just half a gig.Invoking a Lambda in a VPC is no longer dog-slow.Lambda cold-starts are no longer as big of a problem as they were originally.You no longer have to put a big pile of useless data on an EFS volume to get your IO allotment to something usable; you can adjust that separately from capacity now that they’ve added a second knob.You get full performance on new EBS volumes that are empty. If you create an EBS volume from a snapshot, you’ll want to read the entire disk with dd or similar because it lazy-loads snapshot data from S3 and the first read of a block will be very slow.  If you’re in a hurry, there are more expensive and complicated options. EBS volumes can be attached to multiple EC2 instances at the same time (assuming io1), but you almost certainly don’t want to do this.You can now have empty fields (the newsletter publication system for “Last Week in AWS” STILL uses a field designator of  because it predates that change) in an item. Performance has gotten a lot more reliable, to the point where you don’t need to use support-only tools locked behind NDAs to see what your hot key problems look like. With pricing changes, you almost certainly want to run everything On Demand unless you’re in a very particular space.Reserved Instances are going away for EC2, slowly but surely. Savings Plans are the path forward. The savings rates on these have diverged, to the point where they no longer offer as deep of a discount as RIs once did, which is offset by their additional flexibility. Pay attention!EC2 charges by the second now, so spinning one up for five minutes over and over again no longer costs you an hour each time.The Cost Anomaly Detector has gotten very good at flagging sudden changes in spend patterns. It is free. The Compute Optimizer also does EBS volumes and other things. Its recommendations are trustworthy, unlike “Trusted” Advisor’s various suggestions. The Trusted Advisor recommendations remain sketchy and self-contradictory at best, though some of their cost checks can now route through Compute Optimizer.IAM roles are where permissions should live. IAM users are strictly for legacy applications rather than humans. The IAM Identity Center is the replacement for “AWS SSO” and it’s how humans should engage with their AWS accounts. This does cause some friction at times.You can have multiple MFA devices configured for the root account. You also do not need to have root credentials configured for organization member accounts.us-east-1 is no longer a merrily burning dumpster fire of sadness and regret. This is further true across the board; things are a lot more durable these days, to the point where outages are noteworthy rather than “it’s another given Tuesday afternoon.”While deprecations remain rare, they’re definitely on the rise; if an AWS service sounds relatively niche or goofy, consider your exodus plan before building atop it. None of the services mentioned thus far qualify. CloudWatch doesn’t have the last datapoint being super low due to data inconsistency anymore, so if your graphs suddenly drop to zero for the last datapoint your app just shit itself. You can close AWS accounts in your organization from the root account rather than having to log into each member account as their root user.My thanks to folks on LinkedIn and BlueSky for helping come up with some of these. You’ve lived the same pain I have.]]></content:encoded></item><item><title>Home Depot sued for &apos;secretly&apos; using facial recognition at self-checkouts</title><link>https://petapixel.com/2025/08/20/home-depot-sued-for-secretly-using-facial-recognition-technology-on-self-checkout-cameras/</link><author>mikece</author><category>hn</category><pubDate>Wed, 20 Aug 2025 15:23:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[© 2025 PetaPixel Inc. All rights reserved.]]></content:encoded></item><item><title>Show HN: What country you would hit if you went straight where you&apos;re pointing</title><link>https://apps.apple.com/us/app/leascope/id6608979884</link><author>brgross</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 15:23:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
    The developer, , indicated that the app’s privacy practices may include handling of data as described below. For more information, see the developer’s privacy policy.
  The developer does not collect any data from this app.Privacy practices may vary, for example, based on the features you use or your age. Learn More]]></content:encoded></item><item><title>Why are anime catgirls blocking my access to the Linux kernel?</title><link>https://lock.cmpxchg8b.com/anubis.html</link><author>taviso</author><category>hn</category><pubDate>Wed, 20 Aug 2025 14:54:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hey… quick question, why are anime catgirls blocking my access to
the Linux kernel?I’ve started running into more sites recently that deploy Anubis, a sort of hybrid
art project slash network countermeasure. The project “weighs the souls”
of HTTP requests to help protect the web from AI crawlers.If you’ve seen anime catgirl avatars when visiting a new website,
that’s Anubis.I’m sympathetic to the cause – I host this blog on a single core
128MB VPS, I can tell you some stories about aggressive crawlers!Anubis recently started blocking how I access git.kernel.org and lore.kernel.org. Those sites host the
Linux Kernel Mailing
List archive and the kernel git repositories. As far as I know I do
have a soul, I just wasn’t using a desktop browser… so how exactly is my
soul being weighed?Note: Linux has Tux 🐧, OpenBSD has Puffy 🐡, SuSE has Geeko 🦎 and
Microsoft has Bob 🤓… nothing wrong with mascots! 😸The traditional solution to blocking nuisance crawlers is to use a
combination of rate limiting and
CAPTCHAs. The
CAPTCHA forces vistors to solve a problem designed to be very difficult
for computers but trivial for humans. This isn’t perfect of course, we
can debate the accessibility tradeoffs and weaknesses, but conceptually
the idea makes some sense.Anubis – confusingly – inverts this idea. It insists visitors solve a
problem trivial for computers, but impossible for humans. Visitors are
asked to brute force a value that when appended to a challenge string,
causes it’s SHA-256 to begin with a few zero nibbles.If that sounds familiar, it’s because it’s similar to how bitcoin
mining works. Anubis is not literally mining cryptocurrency, but it
 similar in concept to other projects that do exactly that,
perhaps most famously Coinhive
and JSECoin.So how do some useless SHA-256 operations prove you’re not a bot? The
argument goes that this simply makes it too expensive to crawl your
website.This… makes no sense to me. Almost by definition, an AI vendor will
have a datacenter full of compute capacity. It feels like this solution
has the problem backwards, effectively only limiting access to those
 resources or trying to conserve them.Let’s assume the argument has some merit and math out the claims.We can see that with the default Anubis configuration, a typical
website visitor will have to solve a challenge with a difficulty of
4.This means that a visitor must make the first 4 hex digits of the
challenge hash be zero, so 16 bits (4 digits, one nibble each).
Therefore, you can expect to mine a suitable  within
2^16 SHA-256 operations.If every single github star on the anubis project represents a
website that has deployed Anubis, how much would the cloud services bill
be to mine enough tokens to crawl every single website?At the time of writing, Anubis has 11,508 github stars.The default configuration means mining one token gets you access for
7 days (although I think this expiration check is broken, see
below), so we need 11,508 * 2^16 SHA-256 operations per week,
how expensive is that?To get some numbers, I started an  vm on Google
Compute Engine, and ran . This is what you get
in the free
tier.$ openssl speed sha256
Doing sha256 for 3s on 16 size blocks: 6915549 sha256's in 3.00s
Doing sha256 for 3s on 64 size blocks: 4631718 sha256's in 3.00s
Doing sha256 for 3s on 256 size blocks: 393694 sha256's in 3.21s
Doing sha256 for 3s on 1024 size blocks: 100123 sha256's in 3.00s
Doing sha256 for 3s on 8192 size blocks: 13300 sha256's in 2.98s
Doing sha256 for 3s on 16384 size blocks: 7137 sha256's in 2.99s
version: 3.0.17
built on: Tue Aug  5 07:09:41 2025 UTC
options: bn(64,64)
compiler: gcc -fPIC -pthread -m64 ...
The 'numbers' are in 1000s of bytes per second processed.
type             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes
sha256           36882.93k    98809.98k    31397.40k    34175.32k    36561.61k    39107.90kIt looks like we can test about 2^21 every second, perhaps a bit more
if we used both SMT sibling cores. This amount of compute is simply too
cheap to even be worth billing for.So (11508 websites * 2^16 sha256 operations) / 2^21, that’s about 6
minutes to mine enough tokens for every single Anubis deployment in the
world. That means the cost of unrestricted crawler access to the
internet for a week is approximately $0.In fact, I don’t think we reach a single cent per month in compute
costs until several million sites have deployed Anubis.I’m just not convinced this math works… this is 
nothing for a souless AI vendor with a monthly cloud services budget in
the 8 figures. However, the cost for real soul-owning humans with
limited access to compute is high – the Anubis forums are full of
complaints like these:Anubis cites hashcash
as the primary inspiration for their design, an anti-spam solution from
the 90s that was never widely adopted.The idea of “weighing souls” reminded me of another anti-spam
solution from the 90s… believe it or not, there was once a company that
used poetry to block spam!Habeas
would license short haikus to companies to embed in email headers. They
would then aggressively sue anyone who reproduced their poetry without
a license. The idea was you can safely deliver any email with their
header, because it was too legally risky to use it in spam.winter into spring  brightly anticipated So you’re trying to read LKML, but catgirl says no… is there a
solution?My issue is I don’t want to use a desktop browser to mine the
required value, so how can I get the auth cookie?If we look at the response with , we can see the
challenge in the HTTP headers:$ curl -I https://lore.kernel.org/
HTTP/2 200
server: nginx
set-cookie: techaro.lol-anubis-auth=; Path=/
set-cookie: techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd; Path=/That techaro.lol-anubis-cookie is the challenge, here is
a quick C program to mine an acceptable token:Let’s run that and see what it says…$ gcc -Ofast -march=native anubis-miner.c -lcrypto -o anubis-miner
$ time ./anubis-miner 5d737f0600ff2dd
47224

real    0m0.017s
user    0m0.016s
sys     0m0.000sLooks okay, let’s verify that solution is correct:$ printf "5d737f0600ff2dd%d" 47224 | sha256sum
000043f7c4392a781a04419a7cb503089ebcf3164e2b1d4258b3e6c15b8b07f1  -It seems valid, so now we can get a signed auth cookie by sending
back the value we mined:$ curl -I --cookie "techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd" \
    'https://lore.kernel.org/.within.website/x/cmd/anubis/api/pass-challenge?response=000043f7c4392a781a04419a7
cb503089ebcf3164e2b1d4258b3e6c15b8b07f1&nonce=47224&redir=/&elapsedTime=0'
HTTP/2 302
server: nginx
location: /
set-cookie: techaro.lol-anubis-auth=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJhY3Rpb24iO...OTYifQ...;Success, this cookie is now valid for 1 week of access. Let’s
validate what it sent us, the actual schema is visible in the code:We can examine this auth token and see what Anubis gave us…$ base64 -d <<< eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9 | jq
{
  "alg": "EdDSA",
  "typ": "JWT"
}
$ base64 -d <<< eyJhY3Rpb24iO...OTYifQ== | jq
{
  "action": "CHALLENGE",
  "challenge": "5d737f0600ff2dd",
  "exp": 1756185722,
  "iat": 1755580922,
  "method": "fast",
  "nbf": 1755580862,
  "policyRule": "dbf942088788cc96"
}It looks like  is the expiry date, so
, which is…$ date --date @1756185722
Mon Aug 25 22:22:02 PDT 2025Yep, about 7 days from the date I requested it. You can now place
that into a cookie file for , etc.Interestingly, sending the same request the next day got me a new
signed cookie!?This seems like a bug – exchanging a mined token for an auth cookie
should immediately remove the challenge from the store, or there is a double spend
vulnerability.This error benefits me, I have to mine less tokens, but I’ll open an
issue 😇Update: wow, fixed just a
few minutes after opening an issue by the maintainer!This dance to get access is just a minor annoyance for me, but I
question how it proves I’m not a bot. These steps can be trivially and
cheaply automated.I think the end result is just an internet resource I need is a
little harder to access, and we have to waste a small amount of
energy.I wrote this article with my own puny brain, I didn’t use any AI. I
know there are  (they’re 
emdashes!) – I have a habit of writing two consecutive dashes, which pandoc
converts into U+2013.This post is a bit critical of a small well-intentioned project, so
I felt obliged to email the maintainer to discuss it before posting it
online. I didn’t hear back.]]></content:encoded></item><item><title>Show HN: I was curious about spherical helix, ended up making this visualization</title><link>https://visualrambling.space/moving-objects-in-3d/</link><author>damarberlari</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 14:02:47 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[MOVING OBJECTS IN 3D SPACEtap/click the right side of the screen to go forward →Have you ever wondered how to move objects along a spherical helix path?Okay… probably not, right?But one morning, this question popped into my head.It stuck with me long enough that I ended up diving into a few articles about it.From there, it spiraled into lots of explorations, trying to figure out how to move objects in 3D space....to this complex, chaotic path.All these explorations made me want to share what I learned with you.I hope you enjoy this as much as I did.A helix is a shape that loops around and around, like a spring.In a spherical helix, it loops around a sphere.To move an object along a spherical helix path, we need to define its 3D coordinates to follow a helical pattern around a sphere.We'll get there! But first, let’s see how to position and move objects in 3D space.In 3D space, we position objects by setting its coordinates along three axes: x,y, and z.The x-axis typically represents horizontal movement—left or right.The y-axis typically represents vertical movement—up or down.The z-axis typically represents depth—forward or backwardTo move an object in 3D space, we can use mathematical functions to set its position over time.For example, this cube's x position is set to 10 * cos(πt/2), where t is time (in seconds).The result? It oscillates from 10 to -10 along the x-axis every 2 seconds, following a cosine wave.Similarly, setting the y position to 10 * cos(πt/2) makes the cube oscillates vertically, from 10 to -10 every 2 seconds.We can create a two-dimensional path by setting the x and y positions to different functions.For this circle, the x position is set to 10 * cos(πt/2).The cube starts at x = 10, moves to -10 in 2 seconds, then back to 10, and so on.Meanwhile, the y position is set to 10 * sin(πt/2).The movement for x and y may look similar, but they are actually out of phase.When x = 10, y = 0; when x = 0, y = 10; and so on.Together, these two functions create a circular path for the cube.Now we can get creative with functions to create even more complex paths.For example, let's multiply the x function by 0.03 * t.It would make the cube oscillates farther on the x-axis over time...and we will have a circular path whose radius grows over time.Okay, now it's time to talk about the spherical helix (finally!)The spherical helix path is similar to the spiral we just made, but with some differences.First, a spherical helix is three-dimensional.It has a z component that changes over time.This cube's z position is set as 10 * cos(0.02 * πt).It will start from z = 10 then slowly move to -10.Second, unlike the previous spiral, the x and y positions don’t grow indefinitely.They grow at first, then shrink halfway through.This is because the x function is multiplied by another sine function: sin(0.02 * πt)which makes the radius larger in the middle and smaller at the ends.The same is also done to the y function.Together, these functions create a spherical helix.By updating the cube’s position with these functions, it moves along a spherical helix path.In summary, we can move objects in 3D space by defining their x, y, z coordinates as functions of time.These functions, which express x, y, z coordinates as a function of another variable (in this case, time), are called parametric equations.Check out the Wikipedia article for more on parametric equations.Now that we know this, we can get creative and move objects along any path we want!...to this complex, chaotic path......which we know now isn't actually chaotic.It's just a path defined by mathematical functions.Thanks for sticking with me, I hope you enjoyed it!visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.If you like this, please consider following me on Twitter and sharing this with your friends.I'm planning to write more articles like this, so stay tuned!https://twitter.com/damarberlari]]></content:encoded></item><item><title>Gemma 3 270M re-implemented in pure PyTorch for local tinkering</title><link>https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/12_gemma3</link><author>ModelForge</author><category>hn</category><pubDate>Wed, 20 Aug 2025 14:01:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Improvements to OCaml code editing: the basics of a refactor engine</title><link>https://tarides.com/blog/2025-08-20-internship-report-refactoring-tools-coming-to-merlin/</link><author>nukifw</author><category>hn</category><pubDate>Wed, 20 Aug 2025 13:37:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Refactoring features have contributed to the popularity of editors like IntelliJ, as well as certain programming languages whose editor support offers interactive mechanisms to manage code — Gleam being an excellent example. Even though OCaml has some features related to refactoring (such as renaming occurrences, substituting typed holes with expressions, and case analysis for pattern matching), the goal of my internship was to kickstart work on a robust set of features to enable the smooth integration of multiple complementary refactoring support commands.As part of my Tarides internship (on the editor side), I specified several useful commands, inspired by competitors and materialised in the form of RFCs, subject to discussion. There were multiple candidates, but we found that expression extraction to toplevel was the most suitable for a first experiment. Since it touched on several parts of the protocol and required tools that could be reused for other features, it was important to design the system with extensibility and modularity in mind.In this article, I will present the results of this experiment, including the new command and some interesting use cases.Expression extraction to toplevel will select the most inclusive expression that fits in your selection and propose to extract it. In this case,  means that the selected expression will be moved into its own freshly generated let binding top level.Here is a first example: Let's try to extract a constant. Let’s assume
that the float 3.14159 is selected in the following code snippet:The  action code will then be proposed, and if you apply it, the code will look like this:Here is an illustrated example (based on an experimental branch of ocaml-eglot):We can see that the expression has been effectively extracted and replaced by a reference to the fresh let binding. We can also observe that in the absence of a specified name, the generated binding will be named with a generic name that is not taken in the destination scope. You also have the ability to supply the name you want for extraction.For example, here is the same example where the user can enter a name:But the refactoring capabilities go much further than constant extraction!In our previous example, we could speculate about the purity of the expression, since we were only extracting a literal value. However, OCaml is an impure language, so extracting an expression into a constant can lead to unintended behavior. For example, let's imagine the following snippet:In this example, extracting into a  would cause problems! Indeed, we would be changing the semantics of our program by executing both print statements beforehand. Fortunately, the command analyses the expression as not being a constant and delays its execution using a thunk — a function of .As we can see, our goal was to maximise the production of valid code, as much as possible, by carefully analysing how to perform the extraction. This is all the more challenging in OCaml, which allows for arbitrary (and potentially infinite) nesting of expressions.Extracting an Expression That Uses VariablesThe final point we’ll briefly cover is the most fun. Indeed, it’s possible that the expression we want to extract depends on values defined in the current scope. For example:In this example, the extraction of the expression a + b + c (c * x * y) + z will be placed between  and . As a result,  will still be accessible; however, , , , , and  will be free variables in the extracted expression. Therefore, we generate a function that takes these free variables as arguments:Identifying free variables was one of the motivations for starting with this command. We are fairly certain that this is a function that we will need to reuse in many contexts!  Note that the command behaves correctly in the presence of objects and modules.Let’s try to extract something a little more complicated now. Let’s assume we have the following code and we want to refactor it, for example, by extracting the  type pretty print logic outside our  function.We can observe that bounded variables in the extracted region are now passed as arguments, and the extracted function is properly replaced by a call to the new show_markup generated function.Here is an example of how it is used. ?To understand how this new Merlin command can be properly used in your favourite editor, we have to take a closer look at the functioning of the Language Server Protocol. The LSP supports two mechanisms to extend the existing protocol with new features. First, there is , which allows us to perform multiple LSP commands  sequentially. This kind of request has the merit of working out of the box without requiring any plugin or specific command support on the editor side (which oils the wheels for maintenance). Secondly, there are , which are more powerful than code actions and enable custom interactivity. So, if you want to prompt the user, a custom request is the way to go. The price you have to pay for this power is to have client-side support implemented for each custom request in every editor plugin.The current editor team approach is as follows: For each of Merlin's commands that don't map directly to a standard LSP request, we provide a code action associated with the Merlin command and potentially a dedicated custom request if the feature requires custom interactivity. Regarding the ‘extract’ feature, the associated code action does not allow us to choose the name of the generated let binding, but the custom request does.I hope this new command helps you get even more productive in OCaml! Don’t hesitate to experiment with it and report any bugs you encounter.The development of Merlin’s refactoring tools was part of a broader vision to improve OCaml editor support and perhaps claim an editor experience similar to JetBrains IDE in the future!The work done on the  command gives us the opportunity to identify various problems pertaining to refactoring (, ) and potentially to make the connection to refactoring commands that already exist in Merlin (like  refactoring and project-wide renaming). The next step is to add a small toolbox library in Merlin dedicated to refactoring in order to develop even more refactor actions. I hope this is just the first refactoring feature of a long series.If you're curious and want to take a look at the feature, it's split into several PRs:ocaml/merlin#1948 which implements the extraction logic on the Merlin side and exposes it in the protocol,ocaml/ocaml-lsp#1545 which exposes the Custom Request enabling the use of the LSP-side functionality,ocaml/ocaml-lsp#1546 which exposes an Action Code that allows the functionality to be invoked without additional formalities on the Editor side,tarides/ocaml-eglot#65 which implements extraction behaviour in OCaml-Eglot, invocable either from a type enclosing or directly as a classic Emacs command.All of these PRs are currently under review, and should be merged soon!A big thanks to Xavier, Ulysse, and all the people that helped me during this internship. It was pretty interesting!]]></content:encoded></item><item><title>Sequoia backs Zed</title><link>https://zed.dev/blog/sequoia-backs-zed</link><author>vquemener</author><category>hn</category><pubDate>Wed, 20 Aug 2025 12:13:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today we're announcing our $32M Series B led by Sequoia Capital with participation from our existing investors, bringing our total funding to over $42M.For the past four years, we've been building the world's fastest IDE, but that's just the foundation for what comes next. Our ultimate vision is a new way to collaborate on software, where conversations about code remain connected to the code itself, instead of being tied to aging snapshots or scattered across different tools. The first step was creating a high-quality editor to serve as the user interface. Now this new investment lets us expand to tackle the next phase of our plan. We're developing a new kind of operation-based version control that incrementally tracks the evolution of your code with edit-level granularity, and we're integrating it into Zed to make collaboration, both with agents and teammates, a first-class part of the coding experience.Sequoia is excited about our vision, and we're thrilled to have their help making it a reality. We're actively hiring, so if the future we're building excites you, we'd love to talk.Real-world software is the product of a never-ending stream of conversations: With yourself, your teammates, and now also with generative AI models. Talking about code helps us understand it, both individually and as a team. But with current tooling, these discussions (and all the insights they generate) seem to exist everywhere except the code itself.Git lets you collaborate by sharing commits and branches, but between commits you work alone in your own isolated working copy. It's fairly easy to discuss code that's changing in a pull request, but if you want to have a conversation about an arbitrary part of your codebase, you're stuck linking to a particular version of the relevant code in a snapshot, or worse, pasting text into a chat app. As snapshots become stale and messages scroll into the past, your conversations quickly lose their link to the latest version of the code, and all of their valuable context is lost.The limitations of snapshots become even more apparent when working with AI agents. While you might manage simple tasks by exchanging comments with an agent on a pull request, real-world development often requires interaction between commits. You need to guide agents, correct their course, and iterate rapidly—all without the overhead of creating snapshots for every exchange. Our existing tools were built for humans trading commits asynchronously, not for instant back-and-forth with synthetic collaborators. Forcing every AI interaction through the commit-based workflow is like trying to have a conversation through a fax machine.Today's AI editors patch over these limitations, but miss the core problem: collaboration is continuous conversation, not discrete commits. You can't snapshot every clarification, every pivot, every back-and-forth that shapes the code. We're building a system that captures this entire dialogue: every edit, every discussion, linked durably to the code as it evolves. This frees collaboration from the rigid structure of commits.Our vision is turn your IDE into a collaborative workspace where humans and AI agents work together across a range of time scales, with every insight preserved and linked to the code forever. To make this possible, we're building DeltaDB: a new kind of version control that tracks every operation, not just commits.DeltaDB uses CRDTs to incrementally record and synchronize changes as they happen. It's designed to interoperate with Git, but its operation-based design supports real-time interactions that aren't supported by Git's snapshots. For async interactions, fine-grained change tracking also enables character-level permalinks that survive any code transformation, so we can anchor our interactions to arbitrary locations in the codebase, not just to snapshots of recently-changed code.Zed's goal is to make your codebase a living, navigable history of how your software evolved, where discussions with humans and AI agents are durably linked to the code they reference and always up-to-date. It's an evolution beyond version control that incorporates not just the code itself, but also the background information of how and why the code got into a particular state—context that AI agents can query to make more informed edits, understanding the assumptions, constraints, and decisions that shaped the existing code.Picture a new engineer facing a production stack trace in Zed. They highlight a problematic line, like an  that caused a crash, and see every related discussion: why the function was written or what an AI agent assumed about an invariant. They ping the responsible human, sparking a quick chat that turns into an audio call, all indexed to the exact code spot, creating a shared, revisitable record without leaving the codebase.Zed is open-source with an optional paid offering, and we plan to do the same with DeltaDB: build it, open-source it, and offer an optional paid service. We'll share more details as development progresses; this is just the beginning of reimagining how developers work together, both with AI agents and their team.We have the vision, technical foundation, and funding to fundamentally improve how developers collaborate. Now we just need you. We’re hiring across engineering and product design; whether you're interested in collaboration in the IDE, core Zed projects like cross-OS font rendering and GPU shaders, or improving the world's best open-source open-data language model for Edit Prediction, there's room for you here. Join us to shape the future of software development.Looking for a better editor?If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.]]></content:encoded></item><item><title>Shader Academy: Learn computer graphics by solving challenges</title><link>https://shaderacademy.com/</link><author>pykello</author><category>hn</category><pubDate>Wed, 20 Aug 2025 11:08:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>