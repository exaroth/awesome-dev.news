<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Huawei releases an open weight model trained on Huawei Ascend GPUs</title><link>https://arxiv.org/abs/2505.21411</link><author>buyucu</author><category>hn</category><pubDate>Wed, 2 Jul 2025 07:36:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hilbert&apos;s sixth problem: derivation of fluid equations via Boltzmann&apos;s theory</title><link>https://arxiv.org/abs/2503.01800</link><author>nsoonhui</author><category>hn</category><pubDate>Wed, 2 Jul 2025 00:31:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Australians to face age checks from search engines</title><link>https://ia.acs.org.au/article/2025/australians-to-face-age-checks-from-search-engines.html</link><author>stubish</author><category>hn</category><pubDate>Tue, 1 Jul 2025 23:59:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Australians using search engines while logged in to accounts from the likes of Google and Microsoft will have their age checked by the end of 2025, under a new online safety code co-developed by technology companies and registered by the eSafety Commissioner.Search engines operating in Australia will need to implement age assurance technologies for logged-in users in "no later than six months”, under new rules published on Monday.While only logged-in users will be required to have their age checked, many Australians typically surf the web while logged into accounts from Google, which dominates Australia’s search market and also runs Gmail and YouTube; and Microsoft, which runs the Bing search engine and email platform Outlook.If a search engine’s age assurance systems believe a signed-in user is “likely to be an Australian child” under the age of 18, they will need to set safety tools such as “safe search” functions at their highest setting by default to filter out pornography and high impact violence, including in advertising.Currently, Australians must be at least 13 years of age to manage their own Google or Microsoft account.Age assurance methods can include age verification systems, which use government documents or ID; age estimation systems, which typically use biometrics; and age inference systems, which use data about online activity or accounts to infer age.Search engines will not be required to implement age assurance measures for users who are not logged in to their services, according to the new rules.“Internet search engine services are designed for general public use, with or without an account,” the code states.  However, users who are not logged in should also expect “default blurring of images of online pornography and high-impact violence material detected in search results”.Other compliance measures in the code which search providers must abide by include improving search and age assurance technologies over time, preventing autocomplete predictions “that are sexually explicit or violent”, and responding to searches about eating disorders or self-harm with crisis prevention information.Google and Microsoft were contacted for comment.Earlier this year Google said it would begin using artificial intelligence to estimate users' ages, beginning with tests in the United States, while Microsoft previously stated it had explored age assurance methods while considering potential impacts for user safety and privacy.Changes ‘designed to protect’ Australian kidsThe new rules for search engine operators were “designed to protect" Australian children, according to the code.Drafting of the code was co-led by Digital Industry Group Inc. (DIGI), which was contacted for comment as it counts Google, Microsoft, and Yahoo among its members.eSafety Commissioner Julie Inman Grant said she had registered three new codes submitted by the online industry, which covered harmful content on search engines, enterprise hosting services, and internet carriage services such as telecommunication firms.The codes had been in the works since July 2024 and failure to comply with them could result in civil penalties of up to $49.5 million per breach, her office said.The Commissioner said she had sought extra safety commitments from the industry on six outstanding codes, which covered the likes of app stores, device manufacturers, social media, and messaging services.“It's critical to ensure the layered safety approach which also places responsibility and accountability at critical chokepoints in the tech stack including the app stores and at the device level, the physical gateways to the internet where kids sign-up and first declare their ages,” Inman Grant said.Push to protect children who use AI chatbotsMembers of the technology industry had also been asked to use the remaining six codes to strengthen their protections against generative AI chatbots engaging in harmful behaviours with children, Inman Grant said.“We are already receiving anecdotal reports from school nurses, that kids as young as 10 are spending up to five hours a day with AI chatbots, at times engaging in sexualised conversations and being directed by the chatbots to engage in harmful sexual acts or behaviours,” she said.Inman Grant said she would consider the changes proposed by the industry and would aim to make her final determination on the six outstanding codes by the end of July."If I am not satisfied these industry codes meet appropriate community safeguards, I will move to developing mandatory standards,” she said.]]></content:encoded></item><item><title>Using Sun Ray thin clients in 2025</title><link>https://catstret.ch/202506/sun-ray-shenanigans/</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 1 Jul 2025 23:30:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[i’ve used thin clients at home for quite a while - both for their  use (remotely accessing a desktop of another system); and in the sense of “modern thin clients are x86 boxes that are wildly overpowered for what they run, so they make good mini servers.”recently, i saw a bulk lot of Sun Ray thin clients pop up on Trade Me (NZ’s eBay-like auction site) - and with very little idea of how many clients were actually included in this lot, i jumped on it. after a 9 hour round-trip drive (on some of the worst roads i’ve seen!), i returned home with the back of my car completely packed with Sun Rays. time for some interesting shenanigans!when picking all of these up from the seller, i had guesstimated there was maybe 30 clients in total. turns out i was off by quite a bit.i ended up bringing home:3x Sun Ray 270 - 17” (1280x1024) LCD screens with integrated Sun Ray clients4x Incarta Uvo - 24” 1080p LCD screens with integrated clients
    i can’t find any info about these other than the linked page on the Wayback Machine - if you know more about these, please send me an email!about 40 smart cards, for authentication/hotdeskinga small pile of Sun Type 7 USB keyboards, and some Sun-branded optical miceso that’s  clients all up!a few days prior to picking all this up, i rented a storage unit in a local facility, and put some garage shelving units in there - and boy howdy i’m glad i did!setting up the Sun Ray Server Softwarelooking at the Oracle (eugh.) documentation for the Sun Ray Server Software, it appeared there were two options: run it on ancient Linux, or run it on ancient Solaris. Oracle dropped support for the Sun Rays in 2014, as part of extinguishing everything Sun Microsystems stood for after the 2010 acquisition. i didn’t  want to have a RHEL 6 box kicking around, nor did i want to deal with trying to make Solaris 10 work in a VM on my home Proxmox cluster, so i did some digging.enter  - well, in my case, OpenIndiana. illumos is, essentially, a fork of the pre-Oracle-acquisition OpenSolaris codebase. OpenIndiana is one of many illumos  (in a very similar sense to Linux distributions), and OpenIndiana is more suited for desktop use than most other illumos distributions. the OpenIndiana documentation has a section on setting up the Sun Ray Server Software on OpenIndiana, but even with that in hand there was a lot of pieces to figure out on my own!this is mostly a copy of the docs from the OpenIndiana handbook, with some adjustments to fix things i ran into. i did this on top of a text-only install - OpenIndiana Hipster 2025.04 Text Install DVD (64-bit x86) was the install media i used (from https://www.openindiana.org/downloads/).to get the desktop environment installed:# pkg install mate_install
unlocking the dependencies for SRSS:# pkg change-facet facet.version-lock.gnome/gnome-session=false
# pkg change-facet facet.version-lock.gnome/gnome-settings-daemon=false
# pkg change-facet facet.version-lock.system/display-manager/gdm=false
# pkg change-facet facet.version-lock.library/gnome/libgnomekbd=false
# pkg change-facet facet.version-lock.gnome/window-manager/metacity=false
# pkg change-facet facet.version-lock.library/desktop/gnome-desktop=false
# pkg change-facet facet.version-lock.cde/cde-runtime=false
# pkg change-facet facet.version-lock.library/motif=false
# pkg change-facet facet.version-lock.library/tooltalk=false
# pkg change-facet facet.version-lock.compatibility/packages/SUNWxwplt=false
setting up the package source, and installing the SRSS dependencies:# pkg set-publisher --search-before=openindiana.org -g http://pkg.toc.de/sunray sunray
# pkg set-publisher --non-sticky openindiana.org
# pkg install sunray-essential
after unpacking the Sun Ray Server Software installers (both the Solaris and Linux versions) into , i ran the  script from the OI Handbook, then tried to install SRSS, which bombed out spectacularly with package manager rejections of the This version is excluded by installed incorporation consolidation/userland/userland-incorporation@... sort. so here’s the correct (read: “worked for me!”) steps:# /root/update_dhcp_dependency /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/
# pkg set-publisher -g /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/ sunray
# pkg uninstall entire userland-incorporation
# pkg install SUNWut-srss SUNWut-srwc SUNWuti
to make SRSS happy with isc-dhcp:# rpm2cpio /root/srs_5.4.0.0-Linux.i386/Components/10-SRSS/Content/Sun_Ray_Core_Services_4.5/Linux/Packages/SUNWuto-4.5-44.i386.rpm | bsdtar -C /root -xf - ./opt/SUNWut/lib/dhcp/
# sed 's#$UTDHCPDIR | sort#$UTDHCPDIR | gsort#g' -i.bak /root/opt/SUNWut/lib/dhcp/isc/dhcp_config_linux 
# cp -R /root/opt/SUNWut/lib/dhcp/isc /opt/SUNWut/lib/dhcp/
# cp /opt/SUNWut/lib/dhcp/isc/dhcp_config_linux /opt/SUNWut/lib/dhcp/isc/dhcp_config_solaris
# ln -s /opt/SUNWut/lib/dhcp/isc /etc/opt/SUNWut/dhcp
then apply the needed patch to :now, get the ancient JRE in place:# cd /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Java_Runtime_Environment/Solaris
# ./jre-6u41-solaris-i586.sh
# mv ./jre1.6.0_41 /opt/
# ln -s /opt/jre1.6.0_41 /etc/opt/SUNWut/jre
and, since i wanted the web administration tools to work too:# bsdtar -C /opt -xf /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Apache_Tomcat/apache-tomcat-5.5.36.tar.gz
# ln -s /opt/apache-tomcat /opt/apache-tomcat-5.5.36
i then configured the Sun Ray server:# /opt/SUNWut/sbin/utconfig
# /opt/SUNWut/sbin/utpolicy -a -z both -g -M
# /opt/SUNWut/sbin/utadm -L on
# /opt/SUNWut/sbin/utstart -c
getting the Sun Ray firmware in placesince i was using version 5.4.x of the Sun Ray Server Software, the client firmware wasn’t part of the install - from version 5.3 onwards, you had to have an Oracle support contract to get firmware updates. sigh.thankfully, getting a 5.2.x release (with the firmware included!) wasn’t hard. i grabbed a 5.2.x release for Linux, found the RPM with the firmware in it (), and extracted that with .the Solaris version of SRSS wants to find the firmware in a different place than the Linux version it seems - the Linux versions put it in , but on Solaris/OpenIndiana, it needs to be in /opt/SUNWutdfw/lib/firmware. easy enough.once in place, this was all it took to set up the TFTP server, and make SRSS populate the right places with the firmware:# mkdir /tftpboot
# cd /tftpboot
# ln -f -s . tftpboot
# /opt/SUNWut/sbin/utfwadm -AaV -G force
i wanted to use some of the integrated-into-screens Sun Rays to replace some of the Raspberry Pis (and old iMacs) around the house showing Home Assistant dashboards. i also wanted to set up the Sun Ray server so that when i inserted a particular smart card into a client, it would bring up an RDP session to my existing “desktop” (a Fedora VM running Xrdp).these both turned out to be… interesting to get working.the Sun Ray Server Software has a built-in method for connecting to Microsoft RDP servers - the Sun Ray Windows Connector, also known as .
as you might have guessed, it’s broken as fuck on OpenIndiana, even putting aside the fact that the newest RDP server it knows how to handle would be in the Windows Server 2003 era.so, let’s hack something together with XFreeRDP!i wanted to be able to specify what RDP server each token would connect to. this was a fairly common use case back in the day, and some people wrote helpers to allow things like that - one of which being Daniel Cifuentes’ meta-kiosk, which i borrowed some ideas from.after much trial and error, i got something working!/freerdp


openbox  &
/opt/SUNWut/bin/utscreenresize  all  &

/opt/SUNWut/sbin/utuser  |  | zenity 1
xterm  xfreerdp /cert:tofu /f  /dynamic-resolution /gfx +gfx-thin-client /smartcard /bpp:24 after throwing those in place, install the dependencies and configure the session:# pkg install openbox freerdp
# printf "KIOSK_SESSION=freerdp\n" | /opt/SUNWut/sbin/utkiosk -i FreeRDP
then it’s just a matter of adding the needed data to each token, and assigning the tokens to the FreeRDP session:# /opt/SUNWut/sbin/utkioskoverride -s kiosk -r OpenPlatform.47905167523905788499 -c FreeRDP
upon inserting that token into a client…with much the same setup as the RDP sessions, it’s pretty easy to start a kiosk-mode Firefox, pulling the URL to open from the token data:/kiosk-browser


openbox  &
/opt/SUNWut/bin/utscreenresize  all  &


xset s off
xset s noblank
xset /opt/SUNWut/sbin/utuser  |  | zenity 1
firefox a problem, though. Firefox would show its first-run “Welcome to Firefox” popup… every time. Sun Ray kiosk sessions run as a random user named   (where  is a number), and after the kiosk session ends the home directory of the kiosk user gets fully deleted, so the user can be recycled for other sessions. given i wanted to use this with some always-on Sun Rays, with no input devices attached…thankfully, Firefox policies allow turning that off! throwing this hunk of JSON into /etc/firefox/policies/policies.json fixed that:and with that, i could create a token for an individual client (the tokens for this are , where the MAC is all lower-case), set that token’s “Other Info” field to the URL to show, and assign the kiosk session to that pseudo-token the same way as with smart card tokens.this was a lot of fun to get working. i need to take a break from reading the Sun Ray Administration Guide though, so here’s my thinking for a potential part 2:i want to see how well the multi-head stuff works in SRSS - which joins multiple physical clients together into one desktop session, using the peripherals connected to the “primary” client. unfortunately the Xinerama support is weird (Xinerama and xrandr are mutually exclusive…), but if i can make it play ball it could be a neat thing to use.i want to try and find a newer firmware package too, but that might be a little bit of a lost cause, given i refuse to give Oracle a bunch of money.maybe i’ll set up another OpenIndiana VM and configure the HA failover in SRSS?for now, though… that’s all.]]></content:encoded></item><item><title>Building a Personal AI Factory</title><link>https://www.john-rush.com/posts/ai-20250701.html</link><author>derek</author><category>hn</category><pubDate>Tue, 1 Jul 2025 21:14:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I keep several claude code windows open, each on its own git-worktree. o3 and sonnet 4 create plans, sonnet 3.7 or sonnet 4 execute the plan, and o3 checks the results against the original ask. Any issues found are fed back into the plan template and the code is regenerated. The factory improves itself.Read on to see what might be useful for you.Guiding Principle – Fix Inputs, Not OutputsWhen something goes wrong, I don’t hand-patch the generated code. I don’t argue with claude. Instead, I adjust the plan, the prompts, or the agent mix so the next run is correct by construction.If you know Factorio you know it’s all about building a factory that can produce itself. If not, picture a top-down sandbox where conveyor belts and machines endlessly craft parts because the factory must grow. Do the same thing with AI agents: build a factory of agents that can produce code, verify it, and improve themselves over time.Basic day to day workflow - building the factoryMy main interface is claude code. It’s my computer now. I also have a local mcp which runs Goose and o3. Goose only because I’ve already got it setup to use the models hosted in our Azure OpenAI subscription. Looking to improve this at some point, but it works for now.I’ll give a high level task to claude code, which calls over to o3 to generate a plan. o3 is a good planner and can ask a bunch of good questions to clarify the job to be done. I then have it write out a  file with both my original ask and an implementation plan.First, sonnet 4 reads the plan, verifies it, and turns it into a task list. Next claude code execute the plan, either with sonnet 3.7 or sonnet 4 depending on the complexity of the task. Because most of my day-to-day is in clojure I tend to use sonnet 4 to get the parens right.
One important instruction is to have claude write commits as it goes for each task step. This way either claude or I can revert to a previous state if something goes wrong.Step 3: Verification → Feedback into InputsOnce the code is generated, I have sonnet 4 verify the code against the original plan. Then I have o3 verify the code against the original plan and original ask. o3 is uncompromising. Claude wants to please, so will keep unnecessary backwards compatibility code in place. o3 will call that out and ask for it to be removed. Claude also tends to add “lint ignore flags” to the code which o3 will also call out. Having both models verify the code catches issues and saves me back and forth with claude.Any issue sonnet 4 or o3 finds gets baked back into the plan template, not fixed inline.Git worktrees let me open concurrent claude code instances and build multiple features at once. I still merge manually, but I’m no longer babysitting a single agent.Outputs are disposable; plans and prompts compound.Debugging at the source scales across every future task.It transforms agents from code printers into self-improving colleagues.Example: an agent once wrote code that would load an entire CSV into memory. I made it switch to streaming and had the agent write instructions to the plan to always use streaming for CSVs. Now, my plan checker flags any code that doesn’t use streaming for CSVs, and I don’t have to remember this in every PR review. The factory improves itself.I’ve started to encode more complex workflows, where I have specific agents (behind mcps) for building specific tasks.One MCP will sweep all the clojure code generated and then apply our local style rules. These rules are part of the instructions for the original plan and agent but often the generated code will have style issues. Especially once claude gets in the lint/test/debug cycle. This focused agent means we have tighter behavior and can apply our style rules consistently.I’ve started doing this for internal libraries as well. It’s good at looking at generated code and replacing things like retries and  with our retry library.I’m also building out a collection of these small agents. Each one can take a small specific task, and by composing them together I can build more complex workflows. For example, I can take an api doc, and a set of internally defined business cases and have a composition of agents build integrations, tests, and documentation for the api. This is a powerful way to build out features and integrations without having to do all the work by hand.You don’t get there in one big step. Here’s the secret sauce: It’s essentially free to fire off a dozen attempts at a task - so I do. All agents run in parallel. When one fails, stalls, or lacks context, I feed that lesson into the next iteration. I resist the urge to fix outputs, instead I fix the inputs.That loop is the factory: the code itself is disposable; the instructions and agents are the real asset.I’m working on a few things to improve the factory:Better overall coordination of the agents. I tend to kick things off manually, but I want to have a more automated way to manage the workflow and dependencies between agents.Aligning our business docs with the agents. Changing the information we capture to be at a higher level of abstraction so that the agents can use it more effectively. This means moving away from low level implementation details and focusing on use cases.More complex workflows. I’ve been able to build some pretty complex workflows with the current setup, but I want to push it further. This means more agents, more coordination, and more complex interactions between them.Maximize token usage across providers. I’m pretty limited by bedrock’s token limits especially for sonnet 4. Going to need to be able to switch between the claude max plan and bedrock w/out interruption.That’s where my factory sits today: good enough to ship code while I refill my coffee, not yet good enough to bump me off the payroll. Constraints will shift, but the core principle remains: .]]></content:encoded></item><item><title>Effectiveness of trees in reducing temperature, outdoor heat exposure in Vegas</title><link>https://iopscience.iop.org/article/10.1088/2752-5295/ade17d</link><author>PaulHoule</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:59:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We apologize for the inconvenience...To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.]]></content:encoded></item><item><title>The Roman Roads Research Association</title><link>https://www.romanroads.org/</link><author>bjourne</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:32:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[BookingBookinghttps://youtu.be/DLc8lQvVcvMRoman Roads in Cheshire]]></content:encoded></item><item><title>Fakespot shuts down today after 9 years of detecting fake product reviews</title><link>https://blog.truestar.pro/fakespot-shuts-down/</link><author>doppio19</author><category>hn</category><pubDate>Tue, 1 Jul 2025 20:26:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today marks the end of an era. After nearly a decade of helping millions of shoppers navigate the murky waters of online reviews, Fakespot has officially closed its doors. If you tried to check a product listing this morning and found Fakespot not working, you're not alone. The service has permanently shut down. Fakespot, the popular fake review detection tool acquired by Mozilla in 2023, shut down today, July 1, 2025. Founded by Saoud Khalifah in 2016, it helped millions identify unreliable Amazon reviews with 90% accuracy before Mozilla discontinued it due to sustainability challenges.Back in 2016, Saoud Khalifah bought a product on Amazon, trusting the glowing reviews, only to discover he'd been duped by fake feedback. Instead of just leaving his own angry review, Khalifah took a more proactive approach: he built Fakespot.What started as one person's frustration with deceptive sellers became a tool that analyzed millions of reviews across Amazon and other major retailers like eBay and Walmart. The premise was simple but powerful: use AI to spot patterns that human shoppers might miss, like suspiciously similar language or reviewer profiles that didn't quite add up.The magnitude of the deceptionFakespot's technology revealed some eye-opening statistics. About 43% of the best-selling Amazon products had reviews that were unreliable or fabricated, according to a study by app company Circuit. The problem was even worse in certain categories. Clothing and jewelry led the pack with a staggering 88% of reviews deemed unreliable.These numbers painted a sobering picture of the online shopping landscape. Most of us rely on product reviews as a major factor when deciding what to buy, but nearly half of the feedback you read might not be genuine.Three years later, Mozilla acquired Fakespot, bringing the startup's 13-person team into the Firefox family. Mozilla integrated Fakespot's technology directly into Firefox as the "Mozilla Review Checker" feature, making it easier than ever for users to verify product reviews without installing separate extensions.For many users, this felt like a perfect match. Mozilla's reputation for privacy and transparency aligned beautifully with Fakespot's mission to bring honesty to online shopping.But as Mozilla announced in May, not all acquisitions fit into a sustainable long-term model. The company made the difficult decision to discontinue both Pocket and Fakespot as part of a strategic refocus on Firefox's core features and AI-powered innovations.The reasons were practical, if devastating for users. A flood of reviews lamenting the closure have appeared on Fakespot's extension page on the Chrome Web Store:Fakespot's mission resonated strongly with consumers, but Mozilla couldn't find a sustainable model to keep it running. Resources that once supported the service would now flow toward Firefox features like vertical tabs, smart search, and additional AI-powered features.As we say goodbye to Fakespot, it's worth reflecting on what it accomplished. For nine years, it served as a defender against fraud in an increasingly deceptive marketplace. It gave shoppers a fighting chance against promotional reviewers and bot farms that undermine trust in online shopping.For those of us who came to rely on Fakespot's review analysis before making purchases, its absence leaves us less confident in our buying decisions. The need for trustworthy review analysis hasn't gone away. If anything, it's more critical than ever.I know I'm not alone in feeling this gap, which is why I've begun building a tool that aims to be the spiritual successor to Fakespot. TrueStar will use modern AI, streamlined analysis techniques, and sustainable economics to keep costs manageable while maintaining the accuracy shoppers need.
                            Get notified
                        Quick answers about Fakespot's closureWhen did Fakespot shut down?Fakespot officially closed on July 1, 2025, with the Mozilla Review Checker feature in Firefox having ended on June 10, 2025.Why did Fakespot shut down?Mozilla couldn't find a sustainable business model for Fakespot despite its popularity, choosing to redirect resources to core Firefox features and AI-powered browser tools.What happened to Fakespot?Mozilla acquired Fakespot in 2023 but announced in May 2025 that both Fakespot and Pocket would be discontinued as part of a strategic refocus on Firefox development.What are the best Fakespot alternatives?While several options exist including ReviewMeta, The Review Index, and emerging tools like TrueStar, the market is still developing sustainable solutions that balance accuracy with affordability.As Fakespot's servers go dark, let's raise a glass to the tool that made online shopping so much more trustworthy for nearly a decade. Thanks to Saoud Khalifah and his team for showing us what's possible when technology serves truth over profit.Rest in peace, Fakespot. You fought the good fight. 🥂If you found this article helpful, consider sharing it with others who might be wondering why their favorite review checker stopped working today. Let's keep the conversation about online authenticity going.]]></content:encoded></item><item><title>Figma files for proposed IPO</title><link>https://www.figma.com/blog/s1-public/</link><author>kualto</author><category>hn</category><pubDate>Tue, 1 Jul 2025 19:39:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Figma, Inc. (“Figma”) today announced that it has filed a registration statement on Form S-1 with the U.S. Securities and Exchange Commission (“SEC”) relating to a proposed initial public offering of its Class A common stock. Figma has applied to list its Class A common stock on the New York Stock Exchange under the symbol “.”The number of shares to be offered and the price range for the proposed offering have not yet been determined. The offering is subject to market conditions, and there can be no assurance as to whether or when the offering may be completed, or as to the actual size or terms of the offering.Morgan Stanley, Goldman Sachs & Co. LLC, Allen & Company LLC, and J.P. Morgan will act as joint lead book-running managers for the proposed offering. BofA Securities, Wells Fargo Securities, and RBC Capital Markets will act as book-running managers for the proposed offering. William Blair and Wolfe | Nomura Alliance will act as co-managers for the proposed offering.The proposed offering will be made available only by means of a prospectus. Copies of the preliminary prospectus, when available, may be obtained from Morgan Stanley & Co. LLC, Attention: Prospectus Department, 180 Varick Street, 2nd Floor, New York, New York 10014, or by email at prospectus@morganstanley.com; Goldman Sachs & Co. LLC, Attention: Prospectus Department, 200 West Street, New York, New York 10282, by telephone at (866) 471-2526, or by email at prospectus-ny@ny.email.gs.com; Allen & Company LLC, Attention: Prospectus Department, 711 Fifth Avenue, New York, New York 10022, by telephone at (212) 339-2220, or by email at allenprospectus@allenco.com; or J.P. Morgan Securities LLC, c/o Broadridge Financial Solutions, 1155 Long Island Avenue, Edgewood, New York 11717 or by email at prospectus-eq_fi@jpmchase.com and postsalemanualrequests@broadridge.com.A registration statement on Form S-1 relating to these securities has been filed with the SEC but has not yet become effective. These securities may not be sold, nor may offers to buy be accepted, prior to the time the registration statement becomes effective. This press release shall not constitute an offer to sell or the solicitation of an offer to buy these securities, nor shall there be any sale of these securities in any state or jurisdiction in which such offer, solicitation, or sale would be unlawful prior to registration or qualification under the securities laws of any such state or jurisdiction.Figma is where teams come together to turn ideas into the world’s best digital products and experiences. Founded in 2012, Figma has evolved from a design tool to a connected, AI-powered platform that helps teams go from idea to shipped product. Whether you’re ideating, designing, building, or shipping, Figma makes the entire design and product development process more collaborative, efficient, and fun––while keeping everyone on the same page.]]></content:encoded></item><item><title>Sam Altman Slams Meta&apos;s AI Talent Poaching: &apos;Missionaries Will Beat Mercenaries&apos;</title><link>https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/</link><author>spenvo</author><category>hn</category><pubDate>Tue, 1 Jul 2025 18:08:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Altman is hitting back at Meta CEO Mark Zuckerberg’s recent AI talent-poaching spree. In a full-throated response sent to OpenAI researchers Monday evening and obtained by WIRED, Altman made his pitch for why staying at OpenAI is the only answer for those looking to build artificial general intelligence, hinting that the company is evaluating compensation for the entire research organization.He also dismissed Meta’s recruiting efforts, saying what the company is doing could lead to deep cultural problems down the road.“We have gone from some nerds in the corner to the most interesting people in the tech industry (at least),” he wrote on Slack. “AI Twitter is toxic; Meta is acting in a way that feels somewhat distasteful; I assume things will get even crazier in the future. After I got fired and came back I said that was not the craziest thing that would happen in OpenAl history; certainly neither is this.”The news comes on the heels of a major announcement from Zuckerberg. On Monday, the Meta CEO sent a memo to staff introducing the company’s new superintelligence team, which will be helmed by Alexandr Wang, formerly of Scale AI, and Nat Friedman, who previously led GitHub. The list of new hires also included a number of people from OpenAI, including Shengjia Zhao, Shuchao Bi, Jiahui Yu, and Hongyu Ren. OpenAI’s chief research officer, Mark Chen, told staff that it felt like “someone has broken into our home and stolen something.”Altman struck a different tone about the departures in his note on Monday.“Meta has gotten a few great people for sure, but on the whole, it is hard to overstate how much they didn't get their top people and had to go quite far down their list; they have been trying to recruit people for a super long time, and I've lost track of how many people from here they've tried to get to be their Chief Scientist,” he wrote. “I am proud of how mission-oriented our industry is as a whole; of course there will always be some mercenaries.”He added that “Missionaries will beat mercenaries” and noted that OpenAI is assessing compensation for the entire research organization. “I believe there is much, much more upside to OpenAl stock than Meta stock,” he wrote. “But I think it's important that huge upside comes after huge success; what Meta is doing will, in my opinion, lead to very deep cultural problems. We will have more to share about this soon but it's very important to me we do it fairly and not just for people who Meta happened to target.”Altman then made his pitch for people to remain at OpenAI. “I have never been more confident in our research roadmap,” he wrote. “We are making an unprecedented bet on compute, but I love that we are doing it and I'm confident we will make good use of it. Most importantly of all, I think we have the most special team and culture in the world. We have work to do to improve our culture for sure; we have been through insane hypergrowth. But we have the core right in a way that I don't think anyone else quite does, and I'm confident we can fix the problems.”“And maybe more importantly than that, we actually care about building AGI in a good way,” he added. “Other companies care more about this as an instrumental goal to some other mission. But this is our top thing, and always will be. Long after Meta has moved on to their next flavor of the week, or defending their social moat, we will be here, day after day, year after year, figuring out how to do what we do better than anyone else. A lot of other efforts will rise and fall too.”A number of high-ranking employees who’ve worked at Meta followed up in Slack with their own stories about why OpenAI’s culture is superior. “[T]hey constantly rotate their top focus,” wrote one. Another said: “Yes we’re quirky and weird, but that’s what makes this place a magical cradle of innovation,” wrote one. “OpenAI is weird in the most magical way. We contain multitudes.”]]></content:encoded></item><item><title>Code-GUI bidirectional editing via LSP</title><link>https://jamesbvaughan.com/bidirectional-editing/</link><author>jamesbvaughan</author><category>hn</category><pubDate>Tue, 1 Jul 2025 16:43:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I built a small proof-of-concept for a system that enables real-time
bidirectional editing between any modern code editor and a GUI, enabled by an
LSP server.I like working on small projects at home that benefit from CAD. I’m also a
programmer with a personal development environment that I’ve spent years making
as cozy as possible. Naturally I’ve been interested in finding code-based CAD
system to use for my projects that allows me to use that cozy development
environment.For example: One idea I’m exploring is “bidirectional editing”, so geometry can
be manipulated using either:a purpose-built graphical UI, orthe textual codeCAD languageIf you graphically drag a point around, the coordinates in the source code
should automatically update.
If you edit the source code, the graphical UI should automatically update.A simple way to test this idea is to throw a  in the UI that
displays the corresponding source code.
But to me, that feels terrible because I never want to be coding in some janky,
in-browser  — I want to be working with source code in Emacs, with
all of my familiar key bindings, color schemes, autocomplete, and decades of
cozy practice.That’s the core appeal of a textual programming language.But doing this properly is an absolute boatload of work:How does the system rewrite source code? Is it mediated by files on disk with
reload on save? How do the editor and UI stay in sync and avoid clobbering
each other’s unsaved changes? Maybe we need an LSP server?The language interpreter needs to preserve comments and flow them through,
even when the UI makes edits to the code.What about whitespace / pretty-printing?How much of this needs to be built to evaluate whether bidirectional editing
“fits nicely in the hand”?Maybe we need an LSP server?I’ve been a happy user of LSP servers since they became commonplace in Neovim
setups, but I have almost no experience with language server internals.
I had certainly never considered that they could facilitate bidirectional
editing with a GUI.That line from Kevin’s post was a proper nerd-snipe because a few hours later I
had built this proof-of-concept:What you’re seeing here is a text editor next to a GUI, and data live-updating
both ways between them, made possible by a small server that uses LSP to
communicate with the text editor and WebSockets to communicate with a web app.I’ve shared more technical details and the code for this demo here on
GitHub.Bidirectional editing isn’t new.
What’s new, as far as I’m aware, is real-time bidirectional editing that works
with your favorite text editor.I’ve tried out a handful of code-based CAD systems, but so far I haven’t found
any that achieve more than two out of these three features:Real-time-ish updates in the GUI from changes made in the codeReal-time-ish updates in the code from changes made in the GUIWorks well with my preferred code editorFusion 360 has
decent bidirectional editing for parameters, but it’s not fully code-based and
it certainly doesn’t let me use my own editor.OpenSCAD doesn’t require the use of its own text
editor, and it’s possible to trigger reloads in the GUI via file watching
when you save source files in external editors, but it only goes one way.Zoo has some bidirectional editing, but only
with its built-in editor.Arcol, the tool that I help build at my day job, is
innovating in CAD interface design in some exciting ways, but we’re building for
architects, not programmers.This is just a toy demo, but it’s enough to excite me about the possibility of a
system that achieves  of those points!I don’t plan to develop this demo further, at least not anytime soon, but I hope
it inspires people to find more creative uses (abuses?) of LSP servers.One of the best code-CAD environments I’ve worked in is OpenSCAD + Neovim with
the OpenSCAD LSP server, only using
the OpenSCAD GUI for the viewer, not the built-in text editor.
OpenSCAD is fundamentally not built for GUI editing, but since it’s open source
and has a nice language server already, it could be a good place to develop a
more interesting demo of this concept.Like Kevin’s post said, doing this properly will be a boatload of work.
Handling conflict resolution, incremental edits, and the more complex general
LSP server internals are all serious tasks, let alone creating a whole new
language for CAD.I’m looking forward to seeing what Kevin comes up with for codeCAD!]]></content:encoded></item><item><title>Show HN: Core – open source memory graph for LLMs – shareable, user owned</title><link>https://github.com/RedPlanetHQ/core</link><author>Manik_agg</author><category>hn</category><pubDate>Tue, 1 Jul 2025 16:24:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I keep running in the same problem of each AI app “remembers” me in its own silo. ChatGPT knows my project details, Cursor forgets them, Claude starts from zero… so I end up re-explaining myself dozens of times a day across these apps.1. Not portable – context is vendor-locked; nothing travels across tools.2. Not relational – most memory systems store only the latest fact (“sticky notes”) with no history or provenance.3. Not yours – your AI memory is sensitive first-party data, yet you have no control over where it lives or how it’s queried.- CORE (Context Oriented Relational Engine): An open source, shareable knowledge graph (your memory vault) that lets any LLM (ChatGPT, Cursor, Claude, SOL, etc.) share and query the same persistent context.- Temporal + relational: Every fact gets a full version history (who, when, why), and nothing is wiped out when you change it—just timestamped and retired.- Local-first or hosted: Run it offline in Docker, or use our hosted instance. You choose which memories sync and which stay private.]]></content:encoded></item><item><title>The Fed says this is a cube of $1M. They&apos;re off by half a million</title><link>https://calvin.sh/blog/fed-lie/</link><author>c249709</author><category>hn</category><pubDate>Tue, 1 Jul 2025 16:22:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[At the Federal Reserve Bank of Chicago’s Money Museum, there’s a big transparent cube on display. It’s filled with tightly packed stacks of  bills, claiming to contain .The plaque proudly declares:Have you ever wondered what one million dollars looks like?
You don’t have to wonder anymore because you can see it right in front of you!But I don’t trust signs. I trust counting.I first tried counting the stacks right there in the room. The cube was tall, so I had to step back to see the whole thing, squinting at the stacks, trying to follow each row. I lost track almost immediately.Also, people were starting to look at me funny. Apparently, staring intensely at a pile of cash while muttering numbers isn’t normal museum behavior.Then, I tried with a photo. I zoomed all the way in on my phone, dragging my finger across the screen, mentally tallying as I went.Still couldn’t keep count.All I wanted was a way to click on things in a photo and have the number go up.You’d think this would already exist, a browser based tool for counting things.Turns out it… doesn’t. At least, not as a web app I can find on Google.There are some clunky old Windows programs, niche scientific tools, and image analysis software that assumes you’re trying to count cells under a microscope, not people, penguins, or stacks of $1 bills in a Federal Reserve cube.It’s stupidly simple: upload an image, click to drop a dot, and it tells you how many you’ve placed. That’s it. But somehow, nothing like it existed.I originally made it to investigate this very cube, but I figured other people might need to count stuff in pictures.Count your enemies. Count your blessings. Count your stacks of cash.Because when someone tells you it’s a million dollars, you might want to double check.Assuming each bundle contains  bills*, that’sSo yeah. They’re off by .That’s  in extra cash.“Hey so… we’re $550,400 over budget on the million-dollar cube project.”If you knock  from each dimension (basically pealing away the outermost layer of money bundles), the math actually gets kinda closebut since dollar bills are much wider than they’re tall, it wouldn’t look like a cube anymore.Maybe the Fed is playing the long game.At the Fed’s  inflation target, this cube will be worth  million in today’s dollars in:Can’t wait to come back in 2047 and say: “Nice. Nailed it.”Sure, it does technically contain .And also  of bonus money.Which is kind of like ordering a burger and getting three.I mean, sure, free stuff. But it’s not what you asked for.You can only see the outer stacks. For all we know, the middle is just air and crumpled-up old newspaper.A money shell. A decorative cube. A fiscal illusion. The world’s most expensive piñata (but don’t hit it, security is watching).And get this: just the outermost layer is already worth:You’d only need a 3-layer-thick shell to blow past a million:How  you make a million dollar cube?Turns out U.S. dollars are extremely non-cube-friendly. Each bill is  wide by  tall, a nice and even aspect ratio of:Each 100-bill bundle is  inches thick. stacksWhich gives you a lovely almost-cube: wide deep tallNot perfect. Not terrible. At least it’s honest, unlike that other cube.Maybe it’s  million.Maybe it’s an empty box with a money shell.Most likely it’s  million.All I know is I built a tool, did the math, and triple-checked the stacks.The sign says you don’t have to wonder.
But I did anyway.And now… you don’t have to either.]]></content:encoded></item><item><title>HN Slop: AI startup ideas generated from Hacker News</title><link>https://www.josh.ing/hn-slop</link><author>coloneltcb</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:31:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[© 2025 Just Joshing, LLC. All rights reserved.]]></content:encoded></item><item><title>Ask HN: Who is hiring? (July 2025)</title><link>https://news.ycombinator.com/item?id=44434576</link><author>whoishiring</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:01:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is  an option.Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.Please only post if you are actively filling a position and are committed
to responding to applicants.Commenters: please don't reply to job posts to complain about
something. It's off topic here.Readers: please only email if you are personally interested in the job.Don't miss these other fine threads:]]></content:encoded></item><item><title>Ask HN: Who wants to be hired? (July 2025)</title><link>https://news.ycombinator.com/item?id=44434574</link><author>whoishiring</author><category>hn</category><pubDate>Tue, 1 Jul 2025 15:01:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Share your information if you are looking for work. Please use this format:  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:

Please only post if you are personally looking for work. Agencies, recruiters, job boards,
and so on, are off topic here.Readers: please only email these addresses to discuss work opportunities.]]></content:encoded></item><item><title>Feasibility study of a mission to Sedna - Nuclear propulsion and solar sailing</title><link>https://arxiv.org/abs/2506.17732</link><author>speckx</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:08:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Grammarly acquires Superhuman</title><link>https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/</link><author>thm</author><category>hn</category><pubDate>Tue, 1 Jul 2025 14:00:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Converting a large mathematical software package written in C++ to C++20 modules</title><link>https://arxiv.org/abs/2506.21654</link><author>vblanco</author><category>hn</category><pubDate>Tue, 1 Jul 2025 13:46:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I built the tool I wished existed for moving Stripe between countries</title><link>https://www.stripemove.com/</link><author>felphos</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 12:52:50 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages</title><link>https://simedw.com/2025/06/23/introducing-spegel/</link><author>simedw</author><category>hn</category><pubDate>Tue, 1 Jul 2025 12:49:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[TL;DR Spegel is a proof-of-concept terminal web browser that feeds HTML through an LLM and renders the result as markdown directly in your terminal.Two weekends ago, after my family had gone to sleep, I found myself unsupervised with a laptop and an itch to build something interesting. A couple of hours later, I had a minimal web browser running in my terminal (no JavaScript, GET requests only) that transformed web content based on my custom prompts.Then, a few days later, Google released Gemini 2.5 Pro Lite, significantly faster inference speed, suddenly my little weekend hack became a tad more practical.Adapting content to suit individual needs isn’t a new idea, think about translating books or summarising lengthy articles. However, this used to be slow and expensive. LLMs have changed this dramatically, making these transformations quick and easy.Spegel ("mirror" in Swedish) lets you explore web content through personalized views using your own prompts. A single page can have multiple views, maybe one simplifying everything down to ELI5 or another highlighting key actions. It's entirely up to you and your prompting skills. Sometimes you don't want to read through someone's life story just to get to a recipe.
A previous version of this screenshot showed an incorrect recipe on the right. That was due to a bug where large websites got truncated. Thanks to everyone who pointed it out!The pipeline is straightforward.Spegel fetches HTML content, processes it through an LLM using prompts stored in a config file (~/.spegel.toml), and outputs markdown rendered via Textual. Prompts and views can be adjusted live during a browsing session.This was my first experience using Textual for a TUI, and it's been delightful, possibly too delightful, as I found myself adding a few unnecessary interface elements just because it was easy.One gotcha was ensuring only completed lines (ending in newline characters) were streamed; otherwise, the markdown renderer would parse incomplete markdown and fail to recover formattingThere are a lot of great terminal browsers out there, Lynx and Links2 are close to my heart. There are also modern attempts like Browsh that can even render graphs using half-block Unicode characters (▄█). Spegel isn’t meant to replace these, it’s more of an exploration or proof-of-concept. It currently doesn't support POST requests (though I have some ideas on handling  elements by creating on-the-fly UIs).But most modern websites aren’t designed with terminal browsing in mind. They rely on CSS and JS, making them cumbersome in small terminal windows, full of clutter and noise. Spegel tries to clear away distractions, providing content tailored more closely to your needs.Spegel is still in the early stages, so expect some rough edges, but it’s usable and kind of fun to play with.Then just run it with a URL:spegelsimedw.comDon't forget to configure your own , (example)Want to check out the source or contribute? It’s all on GitHub:]]></content:encoded></item><item><title>Show HN: Jobs by Referral: Find jobs in your LinkedIn network</title><link>https://jobsbyreferral.com/</link><author>nicksergeant</author><category>hn</category><pubDate>Tue, 1 Jul 2025 12:47:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[JobsByReferral analyzes your professional network to find job openings at companies where you have connections. Get referred by people you already know and dramatically increase your chances of landing interviews.]]></content:encoded></item><item><title>Scientists identify culprit behind biggest-ever U.S. honey bee die-off</title><link>https://www.science.org/content/article/scientists-identify-culprit-behind-biggest-ever-u-s-honeybee-die</link><author>pseudolus</author><category>hn</category><pubDate>Tue, 1 Jul 2025 10:35:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cloudflare to introduce pay-per-crawl for AI bots</title><link>https://blog.cloudflare.com/introducing-pay-per-crawl/</link><author>scotchmi_st</author><category>hn</category><pubDate>Tue, 1 Jul 2025 10:20:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A changing landscape of consumption Many publishers, content creators and website owners currently feel like they have a binary choice — either leave the front door wide open for AI to consume everything they create, or create their own walled garden. But what if there was another way?At Cloudflare, we started from a simple principle: we wanted content creators to have control over who accesses their work. If a creator wants to block all AI crawlers from their content, they should be able to do so. If a creator wants to allow some or all AI crawlers full access to their content for free, they should be able to do that, too. Creators should be in the driver’s seat.After hundreds of conversations with news organizations, publishers, and large-scale social media platforms, we heard a consistent desire for a third path: They’d like to allow AI crawlers to access their content, but they’d like to get compensated. Currently, that requires knowing the right individual and striking a one-off deal, which is an insurmountable challenge if you don’t have scale and leverage. What if I could charge a crawler? We believe your choice need not be binary — there should be a third, more nuanced option: You can charge for access. Instead of a blanket block or uncompensated open access, we want to empower content owners to monetize their content at Internet scale.Introducing pay per crawlPay per crawl, in private beta, is our first experiment in this area. Pay per crawl integrates with existing web infrastructure, leveraging HTTP status codes and established authentication mechanisms to create a framework for paid content access. Each time an AI crawler requests content, they either present payment intent via request headers for successful access (), or receive a  response with pricing. Cloudflare acts as the Merchant of Record for pay per crawl and also provides the underlying technical infrastructure.Publisher controls and pricingPay per crawl grants domain owners full control over their monetization strategy. They can define a flat, per-request price across their entire site. Publishers will then have three distinct options for a crawler: Grant the crawler free access to content. Require payment at the configured, domain-wide price. Deny access entirely, with no option to pay.An important mechanism here is that even if a crawler doesn’t have a billing relationship with Cloudflare, and thus couldn’t be charged for access, a publisher can still choose to ‘charge’ them. This is the functional equivalent of a network level block (an HTTP  response where no content is returned) — but with the added benefit of telling the crawler there could be a relationship in the future. While publishers currently can define a flat price across their entire site, they retain the flexibility to bypass charges for specific crawlers as needed. This is particularly helpful if you want to allow a certain crawler through for free, or if you want to negotiate and execute a content partnership outside the pay per crawl feature. To ensure integration with each publisher’s existing security posture, Cloudflare enforces Allow or Charge decisions via a rules engine that operates only after existing WAF policies and bot management or bot blocking features have been applied.Payment headers and accessAs we were building the system, we knew we had to solve an incredibly important technical challenge: ensuring we could charge a specific crawler, but prevent anyone from spoofing that crawler. Thankfully, there’s a way to do this using  proposals.Generating an Ed25519 key pair, and making the -formatted public key available in a hosted directoryRegistering with Cloudflare to provide the URL of your key directory and user agent information.Once registration is accepted, crawler requests should always include , , and  headers to identify your crawler and discover paid resources.GET /example.html
Signature-Agent: "https://signature-agent.example.com"
Signature-Input: sig2=("@authority" "signature-agent")
 ;created=1735689600
 ;keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U"
 ;alg="ed25519"
 ;expires=1735693200
;nonce="e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg=="
 ;tag="web-bot-auth"
Signature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:Once a crawler is set up, determination of whether content requires payment can happen via two flows:Reactive (discovery-first)Should a crawler request a paid URL, Cloudflare returns an HTTP 402 Payment Required response, accompanied by a  header. This signals that payment is required for the requested resource.HTTP 402 Payment Required
crawler-price: USD XX.XX The crawler can then decide to retry the request, this time including a  header to indicate agreement to pay the configured price.GET /example.html
crawler-exact-price: USD XX.XX Alternatively, a crawler can preemptively include a  header in its initial request.GET /example.html
crawler-max-price: USD XX.XXIf the price configured for a resource is equal to or below this specified limit, the request proceeds, and the content is served with a successful  response, confirming the charge:HTTP 200 OK
crawler-charged: USD XX.XX 
server: cloudflareIf the amount in a  request is greater than the content owner’s configured price, only the configured price is charged. However, if the resource’s configured price exceeds the maximum price offered by the crawler, an  response is returned, indicating the specified cost.  Only a single price declaration header,  or , may be used per request.The  or  headers explicitly declare the crawler's willingness to pay. If all checks pass, the content is served, and the crawl event is logged. If any aspect of the request is invalid, the edge returns an HTTP 402 Payment Required response.Crawler operators and content owners must configure pay per crawl payment details in their Cloudflare account. Billing events are recorded each time a crawler makes an authenticated request with payment intent and receives an HTTP 200-level response with a  header. Cloudflare then aggregates all the events, charges the crawler, and distributes the earnings to the publisher.Content for crawlers today, agents tomorrow At its core, pay per crawl begins a technical shift in how content is controlled online. By providing creators with a robust, programmatic mechanism for valuing and controlling their digital assets, we empower them to continue creating the rich, diverse content that makes the Internet invaluable. We expect pay per crawl to evolve significantly. It’s very early: we believe many different types of interactions and marketplaces can and should develop simultaneously. We are excited to support these various efforts and open standards.For example, a publisher or new organization might want to charge different rates for different paths or content types. How do you introduce dynamic pricing based not only upon demand, but also how many users your AI application has? How do you introduce granular licenses at internet scale, whether for training, inference, search, or something entirely new?The true potential of pay per crawl may emerge in an agentic world. What if an agentic paywall could operate entirely programmatically? Imagine asking your favorite deep research program to help you synthesize the latest cancer research or a legal brief, or just help you find the best restaurant in Soho — and then giving that agent a budget to spend to acquire the best and most relevant content. By anchoring our first solution on , we enable a future where intelligent agents can programmatically negotiate access to digital resources. Pay per crawl is currently in private beta. We’d love to hear from you if you’re either a crawler interested in paying to access content or a content creator interested in charging for access. You can reach out to us at http://www.cloudflare.com/paypercrawl-signup/ or contact your Account Executive if you’re an existing Enterprise customer.]]></content:encoded></item><item><title>Show HN: ToplingDB - A Persistent Key-Value Store for External Storage</title><link>https://github.com/topling/toplingdb</link><author>rockeetterark</author><category>dev</category><category>hn</category><pubDate>Tue, 1 Jul 2025 10:07:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[As the creator of TerarkDB (acquired by ByteDance in 2019), I have developed ToplingDB in recent years.ToplingDB is forked from RocksDB,   where   we have replaced almost all components with more efficient alternatives(db_bench shows ToplingDB is about ~8x faster than RocksDB):* MemTable: SkipList is replaced by CSPP(Crash Safe Parallel Patricia trie), which is 8x faster.* SST: BlockBasedTable is replaced by ToplingZipTable, implemented by searchable compression algo, it is very small and fast, typically less than 1μs per lookup:  * Keys/Indexes are compressed   using NestLoudsTrie(a multi-layer nesting LOUDS succinct trie).

  * Values in a SST are compressed   together with better zip ratio than zstd, and can unzip by a single value at 1GB/sec.

  * BlockCache is no longer needed, double caching(BlockCache & PageCache) is avoided

Other hotspots are also improved:* Flush MemTable to L0 is omited, greatly reducing write amp and is very friendly for large(GB) MemTable  * MemTable   serves as the index of Key to "value position in WAL log"

  * Since WAL file content almost always in page cache, thus value content can be efficiently accessed by mmap

  * When Flush happens, MemTable is dumpped as an SST and WAL is treated as a blob file

    * CSPP MemTable use integer index instead of physical pointers, thus in-memory format is exactly same with in-file format

* Prefix cache for searching candidate SSTs and prefix cache for scanning by iterators  * Caching fixed len key prefix into an array, binary search it as an uint array

* Distributed compaction(superior replacement to rocksdb remote compaction)  * Gracefully support MergeOperator, CompactionFilter, PropertiesCollector...

  * Out of the box, development efforts are significantly reduced

  * Very easy to share compaction service on spot instances for many DB nodes

Useful Bonus Feature:* Config by json/yaml: can config almost all features* Optional embeded WebView: show db structures in web browser, refreshing pages like animation* Online update db configs by httpMySQL integration, ToplingDB has integrated into MySQL by MyTopling, which is forked from MyRocks with great improvements, like improvements of ToplingDB on RocksDB:* WBWI(WriteBatchWithIndex): like MemTable, SkipList is replace with CSPP, 20x faster(speedup is more than MemTable).* LockManager & LockTracker: 10x faster* Encoding & Decoding: 5x fasterMyRocks has many disadvantages compared to InnoDB, while MyTopling outperforms InnoDB at almost all aspect - excluding feature differences.We have create ~100 PRs for RocksDB, in which ~40 were accepted. Our PRs are mostly "small" changes, since big changes are not likely accepted.ToplingDB has been deployed in numerous production environments.]]></content:encoded></item><item><title>OpenFLOW – Quickly make beautiful infrastructure diagrams local to your machine</title><link>https://github.com/stan-smith/OpenFLOW</link><author>x0z</author><category>hn</category><pubDate>Tue, 1 Jul 2025 06:29:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust CLI with Clap</title><link>https://tucson-josh.com/posts/rust-clap-cli/</link><author>rajman187</author><category>hn</category><pubDate>Tue, 1 Jul 2025 01:25:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Types are important. In fact, I'd guess that the expressive type system in rust
is the single biggest reason why so many developers love the language. Types
allow us to have a contract between parts of the system about our data and how
to interact with it. All programming languages have the concept of types, but
these exist along several dimensions. Strongly typed vs weakly typed as well as
static vs dynamic typing. Rust stakes out its place as a statically,
strongly typed language.Many languages that are go-to solutions for creating custom command line tools
fall in the opposite quadrant with weak, dynamic typing. Whether looking at
currently popular tooling like python and node.js or more traditional solutions
like awk and perl, they tend to favor a loose approach to types. Perhaps this
is the result of an iterative approach to designing CLI tools that might favor
flexibility. Or it could just be that those languages are already popular,
leading to an abundance of such programs. Regardless of the reasons, I feel that
there is tremendous value for both the developer and user which can arise from
interacting with the command line via the sort of strict contract that rust's
type system enables.I assume that if you're already a rust developer, or at least rust-curious, then
I don't need to convince you of the general value of strong, static typing.
Rather, this is a call to use this same approach for interacting with a command
line user as you would when developing a library or service API.At the very lowest level rust exposes command line arguments through the
 function that returns an  struct, an  for the
 arguments passed to start the program. This is illustrated in the Rust
Book's section on
accepting command line arguments:The naive approach seen above obviously lacks robustness as it relies entirely
on argument positioning and also makes a number of other assumptions about the
results. Perhaps for very simple tools this solution can work but as the number
and types of arguments increases, it seems unlikely that a developer would want
to try and rely on just argument position for the interface to their program.A more flexible approach would be to examine all of the arguments passed in and
parse these for patterns that would allow customary  and  style
options. Doing this by hand for every CLI tool would be error-prone and tedious,
but fortunately some awesome folks have already done that for you with the
excellent clap crate.The Sound of One Hand ClappingThe Command Line Argument Parser for Rust, or clap, is one of the most
widely-used crates in the rust ecosystem. GitHub shows that there are over 445k
repos which depend on clap at the time of writing. Adding clap to your project
will allow you to avoid writing your own parsing logic to interact with the
command line:Out of the box clap offers a builder pattern approach that can be used to
get arguments from the command line without the hassle of parsing an 
of  values:Your users can now invoke the above  program from the command line
and pass in the main argument and optionally enable your x long mode:Clap offers a lot more than just parsing arguments, though. It can also reject
options and arguments that are not specified by the programmer and it provides
built-in help:Okay, so I think we can all agree that clap has some nice features and is far
more robust than trying to roll your own command line argument parser, but this
post started off talking about rust's type system and how that can be used as an
interface with the command line user. And that is where clap's  feature
comes in.Defining Your CLI Interactions with Clap offers a much more ergonomic way to specify your program's arguments than
the builder method shown above, but first you need to include the 
feature in your dependencies:You can now define rust types in your source which will be translated into an
interface contract for your program when called from the command line:The above program behaves identically to the builder version from the previous
section, with a  help option and all the other features that clap offers.
The key difference is that we are now using the type system to define the
interface rather than imperative calls to a builder. Note that the doc
comments for the  struct are used to build the  help subcommand for
the resulting application.Clap isn't limited to simple structs for the definition of the interface either.
As shown above,  works just as you would expect. To build up more
complex command line interactions you can use enums to define subcommand syntax
with configuration options for each different subcommand via associated values
(think  or  subcommands). This offers an elegant solution for managing
the complexity that your tool might need to expose to the user.There are tons of other great features in clap that can be found in the
docs, but rather than get into
the specifics of this crate, I want to discuss how type-driven design
can elevate command line interfaces to be on equal footing with published
libraries and service APIs.What can be gained from specifying your software's command line interactions via
the rust type system?Advantage 1: Code Maintainability and ReadabilityPerhaps the most obvious benefit of using explicit rust types to define your
command line interface is that it provides a clear, concise definition of what
input the program accepts. If you peel away the clap macro calls which annotate
the type, it looks just like any other data structure that you would expect to
pass between portions of the program. Because clap builds help from the doc
comments, the developer documentation for the type also transcends the command
line boundary to help users understand how to properly use your software. There
are no** hidden inputs that will affect your
program. This helps new developers on a project to understand a codebase and
also assists maintainers down the road when they need to add new features, as
there is a single entry point from which they can start designing their changes.Alternative approaches such as using the builder pattern or a custom parsing of
 don't offer this same clarity. At best, these solutions would
be contained in one or more functions that abstract away the interface logic. At
worst these could be scattered across the codebase as each portion of the
program tries to interact directly with the arguments passed in.As software grows in complexity the case grows stronger for type-driven CLI
specification. Imagine that we are creating a tool which will interact with a
key-value store and allow the user to add, remove and list the entries of the
store, all of which also require an access token to validate the user. We could
use the following to model the interface:The  type that we've outlined above allows us to clearly express that a
token is always required for all actions, but the  argument is only needed,
and indeed only allowed, when the user is either adding or removing entries. The
type that we have created is concise and removes the complexity one would have
to deal with if command line arguments were being handled imperatively.Advantage 2: Reduced Test Surface Area and Mock SupportUsing a crate like clap can eliminate huge swathes of imperative code that would
otherwise be necessary to parse, validate and consume arguments from the command
line. Every line of code that you don't write saves time on tests that don't
need to be created as well. Moving your interaction with the command line from
imperative functions to a declarative description of possible states moves the
testing burden upstream to the maintainers of the clap crate, which is widely
used and well supported.Type-driven command line interaction does more for us than just reducing the
surface area, though. It also provides a foundation for more complete unit tests
by providing the simplest possible mock for an actual command line interaction.
Imagine that our key-value client above delegates each of the top-level actions
(add, remove, list) to one function each, where more complex operations are
orchestrated. Something like the following:Some obvious tests of the above method might involve asserting that 
would return an  wrapping a KVStoreError::InvalidRequest if we call the
function with , for instance. We could also verify that the key
returned by the server matches the key we requested to add:The above test is simplistic, but it is representative of the way data must be
structured from an actual user because of the strict typing. This approach gives
us a high fidelity mock of a command line interaction.Advantage 3: Semantic Versioning: Not Just for LibrariesSemantic versioning, or SemVer, is a widely-used framework for determining how
software creators should version their releases so that downstream users of that
code can confidently know what versions are safe to upgrade to from other
versions. It leads to the familiar three-part version number consisting of
 where each component conveys different levels of change and
potential upgrade risk. The rust core team follows SemVer for rust releases and
even have an extensive
section about the topic
in the Cargo Book.Library maintainers generally follow SemVer so that other developers who depend
on their crate can understand when it is safe to upgrade without needing to
delve into the release notes of every single release. Authors of binary tools,
however, have been less likely to strictly follow SemVer, as illustrated by the
rustup 1.28.0 adventure,
wherein a minor release ended up breaking CI for many rust projects.Perhaps the reason why authors of binary CLI tools are less likely to follow
SemVer is because they have an image in their head of the user being a person
who can adapt to changes between versions. The reality, however, is that any
sufficiently useful CLI tool will eventually be integrated into an automated
toolchain that expects input and output to be consistent across versions. Good
CLI tools end up operating very similarly to a library. Unlike libraries,
though, upgrading a binary version doesn't get a chance to throw compiler
errors. Worse yet, CLI tools are often integrated in parts of the stack where
observability is poor and errors are only discovered when catastrophic failure
has already occurred.So, how can a strictly-typed approach to command line arguments help us to
better follow SemVer with CLI applications? The answer to this is through
tooling that already exists,
cargo-semver-checks. This
cargo tool examines your source code and compares it against a prior release
in order to determine if your changes constitute major, minor or merely
patch level changes. Importantly, though, you should begin to think of your
command line program more like a library in order to help cargo-semver-checks
to analyze the importance of changes. Your CLI argument types should be made
 even if this level isn't required for your program to run properly.
They are, after all, truly the most public part of the software. A similar
approach is also reasonable with the types that might represent your program's
output, whether they are used to write back to the shell, to files or some other
form of output. Once you've done this, start versioning your binaries
accordingly. If cargo-semver-checks warns you that a change is major and you
only thought that it was a patch, that's a big warning. Did you really intend to
make a major, breaking change? If you did, then don't hesitate to change the
major version number.Merely knowing about a tool like cargo-semver-checks and having it installed
is nice, but we all know that things like this are best when they become an
automated part of our workflow. It's easy to add a GitHub Action to run a SemVer
check automatically:Now, even if you forget to run your SemVer check manually, you probably won't
push out a binary release that breaks some dependency in a completely hidden
way.Good for the Environment TooThere is a loose end that may have been nagging at some readers going over the
previous sections: What about environment variables? After all, many command
line programs can also look at the shell's environment variables as a source of
input. We see this particularly around secrets or omnipresent settings.
Fortunately clap has us covered here too with the crate feature  that lets
you specify an environment variable which will be queried when a given argument
was not specified as part of the command invocation.Let's use this to flesh out the code from our key-value store client example in
the maintainability section
above. In that example, it would make a lot of sense to make  an argument
which can be stored in an environment variable as well as be overridden from the
command line.All that was required (aside from adding the  feature to our dependencies)
was to add  on line 7. The user can now either pass in the
token via  or by setting the environment variable . The
generated help will automatically pick this up and educate the user about this
option (line 13 below):We are now able to have a fully type-driven specification of our command line
interface that seamlessly incorporates both the arguments passed in as well as
environment variables from the shell. What's not to love?
    If you want to discuss  this post
     or any other, please feel free to drop me a message on
    Instagram
    or over at
    Bluesky.
]]></content:encoded></item><item><title>The Email Startup Graveyard: Why 80%+ of Email Companies Fail</title><link>https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail</link><author>skeptrune</author><category>hn</category><pubDate>Tue, 1 Jul 2025 00:41:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[While many email startups have invested millions in solving perceived problems, we at Forward Email have focused on building reliable email infrastructure from scratch since 2017. This analysis explores the patterns behind email startup outcomes and the fundamental challenges of email infrastructure.: Most email startups don't build actual email infrastructure from scratch. Many build on top of existing solutions like Amazon SES or open-source systems like Postfix. The core protocols work well - the challenge is in the implementation.: For comprehensive details on our approach, architecture, and security implementation, see our Forward Email Technical Whitepaper and About page which documents our complete development timeline since 2017.The Email Startup Failure MatrixHere's every major email startup failure we could find, organized by accelerator, funding, and outcome:The Infrastructure Reality Check: Every single "email startup" is just building UI on top of existing infrastructure. They're not building actual email servers - they're building apps that connect to real email infrastructure.What "Email Startups" Actually BuildKey Pattern for Email Success: The companies that actually succeed in email don't try to reinvent the wheel. Instead, they build infrastructure and tools that enhance existing email workflows. SendGrid, Mailgun, and Postmark became billion-dollar companies by providing reliable SMTP APIs and delivery services - they work  email protocols, not against them. This is the same approach we take at Forward Email.Why Most Email Startups Fail: Email  startups typically fail because they try to replace working protocols, while email  companies can succeed by enhancing existing workflows. The key is understanding what users actually need versus what entrepreneurs think they need.1. Email Protocols Work, Implementation Often Doesn'tThe core email protocols are solid, but implementation quality varies widely:: Better implementation of existing protocols, not protocol replacement.2. Network Effects Are UnbreakableEmail's network effect is absolute:3. They Often Target the Wrong ProblemsMany email startups focus on perceived issues rather than real pain points:Real problems worth solving: Infrastructure reliability, deliverability, spam filtering, and developer tools.4. Technical Debt Is MassiveBuilding real email infrastructure requires:5. The Infrastructure Already ExistsWhy reinvent when you can use:Case Studies: When Email Startups FailCase Study: The Skiff DisasterSkiff perfectly exemplifies everything wrong with email startups.: "Privacy-first email and productivity platform": Better email through privacy and encryptionY Combinator: The Email App FactoryY Combinator has funded dozens of email startups. Here's the pattern:: Mixed results with some notable exits. Several companies achieved successful acquisitions (reMail to Google, Rapportive to LinkedIn), while others pivoted away from email or were acqui-hired for talent.Techstars: The Email Graveyard: Vague value propositions, no real technical innovation, quick failures.: VCs love email startups because they sound simple but are actually impossible. The fundamental assumptions that attract investment are exactly what guarantee failure.VCs love email startups because they sound simple but are actually impossible:: None of these assumptions hold true for email.The Technical Reality: Modern Email StacksWhat Actually Powers "Email Startups"Let's look at what these companies actually run:: Most email apps are Electron-based web apps that consume massive amounts of RAM:Electron Performance Crisis: Modern email clients built with Electron and React Native suffer from severe memory bloat and performance issues. These cross-platform frameworks, while convenient for developers, create resource-heavy applications that consume hundreds of megabytes to gigabytes of RAM for basic email functionality.: Constant syncing and inefficient code:Background processes that never sleepUnnecessary API calls every few secondsPoor connection managementNo third-party dependencies except those absolutely required for core functionalityThe Acquisition Patterns: Success vs. ShutdownClient App Pattern (Usually Fails):Infrastructure Pattern (Often Succeeds)::Industry Evolution and ConsolidationNatural Industry ProgressionThe email industry has naturally evolved toward consolidation, with larger companies acquiring smaller ones to integrate features or eliminate competition. This isn't necessarily negative - it's how most mature industries develop.Post-Acquisition TransitionsWhen email companies are acquired, users often face:: Moving to new platforms: Loss of specialized functionality: Different subscription models: Temporary service disruptionsUser Considerations During TransitionsDuring industry consolidation, users benefit from:: Multiple providers offer similar servicesUnderstanding migration paths: Most services provide export toolsConsidering long-term stability: Established providers often offer more continuityThe Hacker News Reality CheckEvery email startup gets the same comments on Hacker News:. These comments appear on every email startup launch because the fundamental problems are always the same.The Modern AI Email Grift2024 brought a new wave of "AI-powered email" startups, with the first major successful exit already happening:Adding "AI" doesn't solve the fundamental challenges:: AI features require significant infrastructure investment while addressing relatively minor pain points.What Actually Works: The Real Email Success StoriesInfrastructure Companies (The Winners): They build infrastructure, not apps.Email Providers (The Survivors)The JMAP Investment Question: While Fastmail invests resources in JMAP, a protocol that's 10+ years old with limited adoption, they simultaneously refuse to implement PGP encryption that many users request. This represents a strategic choice to prioritize protocol innovation over user-requested features. Whether JMAP will gain broader adoption remains to be seen, but the current email client ecosystem continues to rely primarily on IMAP/SMTP.: Forward Email powers alumni email solutions for top universities, including the University of Cambridge with 30,000 alumni addresses, delivering $87,000 in annual cost savings compared to traditional solutions.: They enhance email, don't replace it.The Exception: Xobni's Success StoryXobni stands out as one of the few email-related startups that actually succeeded by taking the right approach.: Built on top of Outlook instead of replacing it: Contact management and email search: Worked with existing workflows: Targeted business users with real pain pointsWhy Xobni Succeeded Where Others FailedBuilt on proven infrastructure: Used Outlook's existing email handling: Contact management was genuinely broken: Businesses pay for productivity tools: Enhanced rather than replaced existing workflowsThe Founders' Continued Success: Became an active angel investor with investments in Dropbox, Mailbox, and others: Continued building successful companies in the productivity space: Demonstrated that email success comes from enhancement, not replacementCompanies succeed in email when they:Has Anyone Successfully Reinvented Email?This is a crucial question that gets to the heart of email innovation. The short answer is: no one has successfully replaced email, but some have successfully enhanced it.Looking at email innovations over the past 20 years:: All successful innovations  existing email protocols rather than replacing them.New Tools Complement Email (But Don't Replace It): Great for team chat, but still sends email notifications: Excellent for communities, but uses email for account management: Perfect for messaging, but businesses still use email: Essential for video calls, but meeting invites come via email: HEY's founder DHH actually uses our service at Forward Email for his personal domain  and has for several years, demonstrating that even email innovators rely on proven infrastructure.HEY by Basecamp represents the most serious recent attempt to "reinvent" email:: Completely new email paradigm with screening, bundling, and workflows: Mixed - some love it, most stick with existing email: It's still email (SMTP/IMAP) with a different interfaceThe most successful email innovations have been:: Faster servers, better spam filtering, improved deliverability: APIs for sending email, webhooks for tracking: CRM integration, marketing automation, transactional emailNone of these replaced email - they made it better.Building Modern Infrastructure for Existing Email Protocols: Our ApproachBefore diving into the failures, it's important to understand what actually works in email. The challenge isn't that email is broken - it's that most companies try to "fix" something that already works perfectly.The Email Innovation SpectrumEmail innovation falls into three categories:Why We Focus on InfrastructureWe chose to build modern email infrastructure because:The problem is implementation: Most email services use outdated software stacks: Not new features that break existing workflows: Better APIs and management interfacesWhat Actually Works in EmailThe successful pattern is simple: enhance existing email workflows instead of replacing them. This means:Building faster, more reliable SMTP serversCreating better spam filtering without breaking legitimate emailProviding developer-friendly APIs for existing protocolsImproving deliverability through proper infrastructureOur Approach: Why We're DifferentBuild actual infrastructure: Custom SMTP/IMAP servers from scratchEnhance existing workflows: Work with all email clients: APIs and tools that actually workBuild "revolutionary" email clientsTry to replace existing email protocolsAdd unnecessary AI featuresHow We Build Email Infrastructure That Actually WorksOur Anti-Startup ApproachWhile other companies burn millions trying to reinvent email, we focus on building reliable infrastructure:: We've been building email infrastructure for 7+ years: We're building for the long termNo "revolutionary" claims: We just make email work betterGovernment-Grade Compliance: Forward Email is Section 889 compliant and serves organizations like the US Naval Academy, demonstrating our commitment to meeting stringent federal security requirements.OpenPGP and OpenWKD Implementation: Unlike Fastmail, which refuses to implement PGP citing complexity concerns, Forward Email provides full OpenPGP support with OpenWKD (Web Key Directory) compliance, giving users the encryption they actually want without forcing them to use experimental protocols like JMAP.Technical Stack Comparison:= APNIC blog post confirms Proton uses postfix-mta-sts-resolver, indicating they run a Postfix stack: JavaScript across the entire stack vs. 1980s C code: Single language eliminates integration complexity: Built for modern web development from the ground up: Any web developer can understand and contribute: Clean, modern codebase without decades of patches: Our privacy policy ensures we don't store forwarded emails to disk storage or databases, don't store metadata about emails, and don't store logs or IP addresses - operating in-memory only for email forwarding services.: For comprehensive details on our approach, architecture, and security implementation, see our technical whitepaper and extensive technical documentation.Email Service Provider Comparison: Growth Through Proven Protocols: While other providers chase experimental protocols, Forward Email focuses on what users actually want - reliable IMAP, POP3, SMTP, CalDAV, and CardDAV that works across all devices. Our growth demonstrates the value of this approach.in1-smtp.messagingengine.com shows strong growth (+21.1%) with over 500K domains using our MX recordsProven infrastructure wins: Services with reliable IMAP/SMTP show consistent domain adoption: Fastmail's JMAP investment shows slower growth (+14%) compared to providers focusing on standard protocols: The defunct startup lost 55.2% of domains, demonstrating the failure of "revolutionary" email approaches: Domain count growth reflects real user adoption, not marketing metricsWhy We Succeed Where Others FailWe build infrastructure, not apps: Focus on servers and protocolsWe enhance, don't replace: Work with existing email clients: No VC pressure to "grow fast and break things": 7+ years of deep technical experience: APIs and tools that actually solve problemsSecurity Challenges in Email InfrastructureEmail security is a complex challenge that affects all providers in the industry. Rather than highlighting individual incidents, it's more valuable to understand the common security considerations that all email infrastructure providers must address.Common Security ConsiderationsAll email providers face similar security challenges:: Securing user data and communications: Managing authentication and authorization: Protecting servers and databases: Meeting various regulatory requirements like GDPR and CCPA: Our security practices include ChaCha20-Poly1305 encryption for mailboxes, full disk encryption with LUKS v2, and comprehensive protection with encryption-at-rest, encryption-in-memory, and encryption-in-transit.The Value of TransparencyWhen security incidents occur, the most valuable response is transparency and quick action. Companies that:Disclose incidents promptly: Help users make informed decisionsProvide detailed timelines: Show they understand the scope of issues: Demonstrate technical competence: Contribute to industry-wide security improvementsThese responses benefit the entire email ecosystem by promoting best practices and encouraging other providers to maintain high security standards.Ongoing Security ChallengesThe email industry continues to evolve its security practices:These challenges require ongoing investment and expertise from all providers in the space.Conclusion: Focus on Infrastructure, Not AppsAfter analyzing hundreds of email startups:: Most email startups fail completely (this figure is likely WAY higher than 80%; we're being nice): Being acquired usually means death for email clientsInfrastructure can succeed: Companies building SMTP/API services often thriveVC funding creates pressure: Venture capital creates unrealistic growth expectationsTechnical debt accumulates: Building email infrastructure is harder than it looksEmail has been "dying" for 20+ years according to startups:: "Social networks will replace email": "Mobile messaging will kill email": "Slack will replace email": "AI will revolutionize email": "Remote work needs new communication tools": "AI will finally fix email". It's still growing. It's still essential.The lesson isn't that email can't be improved. It's about choosing the right approach:: Reliability and performance beat flashy featuresEnhancement beats replacement: Work with email, don't fight itSustainability beats growth: Profitable businesses outlast VC-funded ones: Tools and APIs create more value than end-user apps: Better implementation of proven protocols, not protocol replacement.Comprehensive Email Service Analysis: For an in-depth comparison of 79 email services in 2025, including detailed reviews, screenshots, and technical analysis, see our comprehensive guide: 79 Best Email Services. This analysis demonstrates why Forward Email consistently ranks as the recommended choice for reliability, security, and standards compliance.If you're thinking about building an email startup, consider building email infrastructure instead. The world needs better email servers, not more email apps.The Extended Email Graveyard: More Failures and ShutdownsGoogle's Email Experiments Gone WrongGoogle, despite owning Gmail, has killed multiple email projects: (2009-2012): "Email killer" that nobody understood (2010-2011): Social email integration disaster email features (2011-2019): Social network email integration: Even Google can't successfully reinvent email.The Serial Failure: Newton Mail's Three Deaths (2013-2016): Email client acquired by Newton (2016-2018): Rebranded, subscription model failed: Email clients can't sustain subscription models.The Apps That Never LaunchedMany email startups died before launching: (2014): Calendar-email integration, shut down pre-launch (2011): Email management tool, acquired before release (2013): Email client, development stoppedThe Acquisition-to-Shutdown PatternEmail Infrastructure ConsolidationThe Open-Source Email Graveyard: When "Free" Isn't SustainableNylas Mail → Mailspring: The Fork That Couldn'tEudora: The 18-Year Death March: Dominant email client for Mac/Windows: Open-sourced as "Eudora OSE": Even successful email clients eventually dieFairEmail: Killed by Google Play PoliticsOpen-source email projects fail because:: Email protocols are complex to implement correctly: Constant security updates required: Must work with all email providers: Volunteer developers burnoutThe AI Email Startup Surge: History Repeating with "Intelligence"The Current AI Email Gold Rush2024's AI email startups:VCs are throwing money at "AI + Email":: "Revolutionary email experience": Building on top of existing infrastructure: Most will fail within 3 yearsWhy They'll All Fail (Again)AI doesn't solve email's non-problems: Email works fine: AI requires reading all your emails: AI processing is expensive, email is commodity: Can't break Gmail/Outlook dominance: Most remaining AI email startups will pivot or shut down: Survivors will be acquired, with mixed outcomes: "Blockchain email" or the next trend will emergeThe Consolidation Catastrophe: When "Survivors" Become DisastersThe Great Email Service ConsolidationThe email industry has consolidated dramatically:Outlook: The "Survivor" That Can't Stop BreakingOur Real-World Experience: We regularly help customers whose Outlook setups break our perfectly compliant IMAP implementation.The Postmark Infrastructure ProblemRecent Email Client Casualties (2024-2025): Users increasingly report poor experience with the email client.: Windows users face licensing issues and subscription confusion.: The Mac/iOS email client, based on the failed Sparrow codebase, continues to receive poor reviews for reliability issues.Email Extension and Service AcquisitionsThe Survivors: Email Companies That Actually WorkNot all email companies fail. Here are the ones that actually work:: Bootstrap success story generating $140K/month as a Gmail extension for email marketing.: These companies succeed because they enhance existing email workflows rather than trying to replace email entirely. They build tools that work  email infrastructure, not against it.]]></content:encoded></item><item><title>Claude Code now supports hooks</title><link>https://docs.anthropic.com/en/docs/claude-code/hooks</link><author>ramoz</author><category>hn</category><pubDate>Tue, 1 Jul 2025 00:01:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Claude Code hooks are user-defined shell commands that execute at various points
in Claude Code’s lifecycle. Hooks provide deterministic control over Claude
Code’s behavior, ensuring certain actions always happen rather than relying on
the LLM to choose to run them.Example use cases include:: Customize how you get notified when Claude Code is awaiting
your input or permission to run something.: Run  on .ts files,  on .go files,
etc. after every file edit.: Track and count all executed commands for compliance or
debugging.: Provide automated feedback when Claude Code produces code that
does not follow your codebase conventions.: Block modifications to production files or sensitive
directories.By encoding these rules as hooks rather than prompting instructions, you turn
suggestions into app-level code that executes every time it is expected to run.In this quickstart, you’ll add a hook that logs the shell commands that Claude
Code runs.Quickstart Prerequisite: Install  for JSON processing in the command line. hooks run before tool calls and can block them while providing
Claude feedback on what to do differently.Select  to run your hook only on Bash tool calls.Select  and enter this command:For storage location, select  since you’re logging to your home
directory. This hook will then apply to all projects, not just your current
project.Then press Esc until you return to the REPL. Your hook is now registered!Run  again or check  to see your configuration: - User settings - Project settings.claude/settings.local.json - Local project settings (not committed)Enterprise managed policy settingsHooks are organized by matchers, where each matcher can have multiple hooks:: Pattern to match tool names (only applicable for  and
)Simple strings match exactly:  matches only the Write toolSupports regex:  or If omitted or empty string, hooks run for all matching events: Array of commands to execute when the pattern matches: Currently only  is supported: The bash command to executeRuns after Claude creates tool parameters and before processing the tool call. - File pattern matching,  - File editing,  - Web operationsRuns immediately after a tool completes successfully.Recognizes the same matcher values as PreToolUse.Runs when Claude Code sends notifications.Runs when Claude Code has finished responding.Hooks receive JSON data via stdin containing session information and
event-specific data:The exact schema for  depends on the tool.The exact schema for  and  depends on the tool. is true when Claude Code is already continuing as a result of
a stop hook. Check this value or process the transcript to prevent Claude Code
from running indefinitely.There are two ways for hooks to return output back to Claude Code. The output
communicates whether to block and any feedback that should be shown to Claude
and the user.Hooks communicate status through exit codes, stdout, and stderr:: Success.  is shown to the user in transcript mode
(CTRL-R).: Blocking error.  is fed back to Claude to process
automatically. See per-hook-event behavior below.: Non-blocking error.  is shown to the user and
execution continues.Blocks the tool call, shows error to ClaudeShows error to Claude (tool already ran)N/A, shows stderr to user onlyBlocks stoppage, shows error to ClaudeHooks can return structured JSON in  for more sophisticated control:All hook types can include these optional fields:If  is false, Claude stops processing after the hooks run.For , this is different from , which only
blocks a specific tool call and provides automatic feedback to Claude.For , this is different from , which
provides automated feedback to Claude.For , this takes precedence over any  output.In all cases,  takes precedence over any
 output. accompanies  with a reason shown to the user, not shown
to Claude. hooks can control whether a tool call proceeds.“approve” bypasses the permission system.  is shown to the user but
not to Claude.“block” prevents the tool call from executing.  is shown to Claude. leads to the existing permission flow.  is ignored. hooks can control whether a tool call proceeds.“block” automatically prompts Claude with . does nothing.  is ignored. hooks can control whether Claude must continue.“block” prevents Claude from stopping. You must populate  for Claude
to know how to proceed. allows Claude to stop.  is ignored. hooks can control tool execution:Claude Code hooks work seamlessly with
Model Context Protocol (MCP) tools. When MCP servers
provide tools, they appear with a special naming pattern that you can match in
your hooks.MCP tools follow the pattern , for example:mcp__memory__create_entities - Memory server’s create entities toolmcp__filesystem__read_file - Filesystem server’s read file toolmcp__github__search_repositories - GitHub server’s search toolYou can target specific MCP tools or entire MCP servers:Automatically format code after file modifications:Customize the notification that is sent when Claude Code requests permission or
when the prompt input has become idle.: Claude Code hooks execute arbitrary shell commands on
your system automatically. By using hooks, you acknowledge that:You are solely responsible for the commands you configureHooks can modify, delete, or access any files your user account can accessMalicious or poorly written hooks can cause data loss or system damageAnthropic provides no warranty and assumes no liability for any damages
resulting from hook usageYou should thoroughly test hooks in a safe environment before production useAlways review and understand any hook commands before adding them to your
configuration.Here are some key practices for writing more secure hooks:Validate and sanitize inputs - Never trust input data blindlyAlways quote shell variables - Use  not  - Check for  in file paths - Specify full paths for scripts - Avoid , , keys, etc.Direct edits to hooks in settings files don’t take effect immediately. Claude
Code:Captures a snapshot of hooks at startupUses this snapshot throughout the sessionWarns if hooks are modified externallyRequires review in  menu for changes to applyThis prevents malicious hook modifications from affecting your current session.: 60-second execution limit: All matching hooks run in parallel: Runs in current directory with Claude Code’s environment:PreToolUse/PostToolUse/Stop: Progress shown in transcript (Ctrl-R)Notification: Logged to debug only ()Check if  menu displays your configurationReview stdout and stderr format expectationsEnsure proper quote escapingProgress messages appear in transcript mode (Ctrl-R) showing:]]></content:encoded></item><item><title>Melbourne man discovers extensive model train network underneath house</title><link>https://www.sbs.com.au/news/article/i-was-shocked-melbourne-mans-unbelievable-find-after-buying-house/m4sksfer8</link><author>cfcfcf</author><category>hn</category><pubDate>Mon, 30 Jun 2025 23:53:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After finalising the purchase of a home in Melbourne's northern suburbs, a Melbourne man found something unexpected.There had been no mention of the expansive model train network beneath the home's floors.Coincidentally, new owner Daniel Xu is a keen train enthusiast and engineer.]]></content:encoded></item><item><title>Show HN: A continuation of IRS Direct File that can be self-hosted</title><link>https://github.com/openfiletax/openfile</link><author>elijahwright_</author><category>dev</category><category>hn</category><pubDate>Mon, 30 Jun 2025 22:08:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[the IRS recently open sourced most of Direct File, a tax tool it has been working on for a few years now. unfortunately, due to recent events, the IRS isn't working on it anymore. I decided to pick up where they left off and I'm trying to get it ready for next tax season]]></content:encoded></item><item><title>The new skill in AI is not prompting, it&apos;s context engineering</title><link>https://www.philschmid.de/context-engineering</link><author>robotswantdata</author><category>hn</category><pubDate>Mon, 30 Jun 2025 20:53:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Context Engineering is new term gaining traction in the AI world. The conversation is shifting from "prompt engineering" to a broader, more powerful concept: . Tobi Lutke describes it as "the art of providing all the context for the task to be plausibly solvable by the LLM.” and he is right.With the rise of Agents it becomes more important what information we load into the “limited working memory”. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.To understand context engineering, we must first expand our definition of "context." It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.Instructions / System Prompt: An initial set of instructions that define the behavior of the model during a conversation, can/should include examples, rules …. Immediate task or question from the user.State / History (short-term Memory): The current conversation, including user and model responses that have led to this moment. Persistent knowledge base, gathered across many prior conversations, containing learned user preferences, summaries of past projects, or facts it has been told to remember for future use.Retrieved Information (RAG): External, up-to-date knowledge, relevant information from documents, databases, or APIs to answer specific questions. Definitions of all the functions or built-in tools it can call (e.g., check_inventory, send_email). Definitions on the format of the model's response, e.g. a JSON object.Why It Matters: From Cheap Demo to Magical ProductThe secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context you provide.Building Agents is less about the code you write or framework you use. The difference between a cheap demo and a “magical” agent is about the quality of the context you provide. Imagine an AI assistant is asked to schedule a meeting based on a simple email:Hey, just checking if you’re around for a quick sync tomorrow. has poor context. It sees only the user's request and nothing else. Its code might be perfectly functional—it calls an LLM and gets a response—but the output is unhelpful and robotic:Thank you for your message. Tomorrow works for me. May I ask what time you had in mind? is powered by rich context. The code's primary job isn't to figure out  to respond, but to  the LLM needs to full fill its goal. Before calling the LLM, you would extend the context to includeYour calendar information (which shows you're fully booked).Your past emails with this person (to determine the appropriate informal tone).Your contact list (to identify them as a key partner).Tools for send_invite or send_email.Then you can generate a response.Hey Jim! Tomorrow’s packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.The magic isn't in a smarter model or a more clever algorithm. It’s in about providing the right context for the right task. This is why context engineering will matter. Agent failures aren't only model failures; they are context failures.From Prompt to Context EngineeringWhat is context engineering? While "prompt engineering" focuses on crafting the perfect set of instructions in a single text string, context engineering is a far broader. Let's put it simply:Context Engineering is the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task. Context isn't just a static prompt template. It’s the output of a  that runs  the main LLM call. Created on the fly, tailored to the immediate task. For one request this could be the calendar data for another the emails or a web search.About the right information, tools at the right time: The core job is to ensure the model isn’t missing crucial details ("Garbage In, Garbage Out"). This means providing both knowledge (information) and capabilities (tools) only when required and helpful.where the format matters: How you present information matters. A concise summary is better than a raw data dump. A clear tool schema is better than a vague instruction.Building powerful and reliable AI Agents is becoming less about finding a magic prompt or model updates. It is about the engineering of context and providing the right information and tools, in the right format, at the right time. It’s a cross-functional challenge that involves understanding your business use case, defining your  outputs, and structuring all the necessary information so that an LLM can “accomplish the task."This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:]]></content:encoded></item><item><title>The original LZEXE (A.K.A. Kosinski) compressor source code has been released</title><link>https://clownacy.wordpress.com/2025/05/24/the-original-lzexe-a-k-a-kosinski-compressor-source-code-has-been-released/</link><author>elvis70</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:19:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Last year, I discovered that the Kosinski compression format is actually LZEXE, which was used for compressing DOS executables back in the 90s and the late 80s. Its developer catalogues three versions on his website: v0.90, v0.91, and v0.91e. While only binaries of v0.91 and v0.91e can be found on the website, v0.90 can be found mirrored on various other websites.I got in touch with LZEXE’s developer, Fabrice Bellard, and he was able to release LZEXE’s source code, untouched since 1990! It is released under the terms of the MIT licence, allowing it to be freely used in other projects. To maximise performance, the compression logic was written in x86 assembly, while its frontend was written in Pascal. This particular source code appears to be for v0.91.my own Kosinski compressor which produced identical data to what could be found in the Mega Drive Sonic games. At the time, I noticed that it did not accurately reproduce the Mega CD BIOS’s compressed Sub-CPU payload data. The inaccuracies were so extensive that it appeared that the BIOS’s data was compressed with a different tool to the Sonic games. Notably, the compressor which was used for the Sonic games suffered from a number of bugs and shortcomings, causing the compressed data to less efficient than it should have been. The Mega CD BIOS developers may have used a different version of the compressor, which lacked these bugs, or which had additional bugs.With this in mind, the source code which has been released may not be for the exact compressor which was used by the Sonic games, though it could be modified to function identically to it. Since the compression logic was written in assembly, it should be simple enough to disassemble the compressor executables and compare them to the source code. Devon did the heavy-lifting of extracting and unpacking the core logic, which can be found here.With that, we now have the source code of two of the four ‘KENS’ format compressors – Kosinski and Saxman! Unfortunately, I do not have much hope of ever finding the original compressors for, let alone the source code of, the remaining two formats – Enigma and Nemesis – due to them evidently being custom formats which were designed specifically for the Mega Drive, likely meaning that the compressors and their source code never left the hands of Sega (Enigma encodes plane map data, operating on 16-bit words and specifically acknowledging the separation of bits of the tile’s index from its X/Y flip, palette line, and priority; meanwhile Nemesis encodes tiles, operating on nibbles and bunching data into groups of 32 bytes (8 x 8 4-bit nibbles).]]></content:encoded></item><item><title>End of an Era</title><link>https://www.erasmatazz.com/personal/self/end-of-an-era.html</link><author>marcusestes</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:17:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I recall saying to one of my colleagues at Atari way back in 1982 that I wanted to make a game that would be genuine art. A year later I built a game that was my first experiment in that direction: Gossip. It was a ridiculously simple game in which a player attempted to win favor in a group by calling people and telling them how much you liked or disliked some third party. The underlying concept was that “people like people who like people they like.” For some reason, many players had problems absorbing this simple concept. I later incorporated many of the ideas in Gossip into Excalibur, my first Arthurian game. But Atari collapsed soon after that, and I had to make a living as a freelance game designer. That kept me busy for the rest of the 1980s, but after I published my last game, Balance of the Planet, in 1990, I resolved to come back to the computer-games-as-art goal. I wanted to make another Arthurian game with much greater emphasis on the interpersonal interactions. I knew that my ambitions would take several years to realize, but I felt that the future lay in this direction, so I went ahead. I started building the game, and it went well. I began approaching publishers, and struck out everywhere. They greatly respected my reputation as one of the top game designers in the world, but they had decided that the only games that would sell had to be variations on Doom or Myst. Then the Markle Foundation came to the rescue with an offer that was to bail me out in the short run and send me down the wrong path in the long run. They offered me $350,000 to build a tool for other people to create interactive storyworlds. The idea appealed to me, so I accepted the offer and went to work. Now, most of the money went to various contractors, even though I designed and wrote all the code. The result was the Erasmatron. I don’t have a screenshot of the Erasmatron user interface. My primary goal was to build software that would permit a storyteller to implement many of the dramatic processes that take place in a story. For example, there was a component that permitted one character to spy on two other characters conversing without being seen. There was an extensive system for managing how information traveled through a group of people and how secrets were kept or broken.I won’t dwell on Erasmatron, as it evolved into Storytron, but I will note that Erasmatron had no takers. Other than Laura Mixon, nobody ever built anything with Erasmatron.This was the culmination of my effort to build a software development environment for interactive storytelling. It is best understood as Erasmatron with a much superior user interface and considerably better support features.The central component of Storytron was Deikto, a technology for creating a toy language specific to an interactive storyworld. “Toy language” is my term for a tiny language containing only the words necessary to permit interaction between characters in the storyworld. Toy languages do not need anywhere near as large a vocabulary as a full language. A storyworld for children will have no verbs for sex, higher education, jobs, finance, marriage, alcohol, and many other things. Similarly, a storyworld about corporate politics can dispense with scientific verbs, most economic and financial verbs, verbs for family interactions, and not much about food. A good storyworld designer can, in my estimate, build an adequate toy language for most storyworlds with only a few hundred verbs. The verbs are the core of the system; the player can build sentences out of the verbs and all the other words in the system. There are words for actors, props, and stages.This description is growing too large. Storytron technology had many other wonderful features that I won’t describe here. The important point I want to make is that nobody was interested in Storytron. I spread the word about it as well as I could, but I’m no salesman. I spent about ten years on Storytron and a great deal of my money hiring contractors to do some of the work that I couldn’t do. And it was all for nothing. Storytron was just too complicated for the audience. I don’t think that was because it was intrinsically too complicated for anybody to understand. My impression is that there just weren’t any people willing to make the big commitment required to learn how to use Storytron. It was easier to learn than professional programming systems like Eclipse or the Microsoft suite of software development applications. But it demanded more of its users than they were willing to invest.I made one last effort to make Storytron work using Siboot, a concept that I had developed in 1987. I poured my energy into Siboot, and a number of good people helped me, but after I had expended several years on it, I realized that it was crap. The story felt too mechanical. I realized that it needed a boost in the form of the encounter technology that I had developed for the 1987 version, but at that point I was so discouraged that I just couldn’t go on. I gave up on Siboot.I gave up on Storytron around 2018. It was painful to accept that all the energy, all the creativity, all the sweat I had committed to the project was for naught, but I had no choice. I rested for some months, then in 2020, for my 70th birthday, I realized that I was growing old and would not be able to handle a tough technical challenge for much longer. I therefore decided that the time had come for me to make one last effort, and that effort had to an Arthurian game. I re-read the many Arthurian books I had accumulated during previous efforts, girded my loins, and set to work. I made many changes along the way; the final version of Le Morte d’Arthur was quite unlike the original. But it worked. I knew that, after all these years, I had finally achieved my goal of making genuine interactive art. I was proud, tired, and gratified. Not many people played the storyworld, but I didn’t care. That was the world’s failure, not mine. I continued to fiddle around with interactive storytelling, discussing issues with a small group of people devoted to the problem. I even made a few attempts to make the technology I used for Le Morte d’Arthur available to others, but, once again, nobody was interested.Late in 2024 I happened upon a mention of Narrascope, an annual conference for interactive fiction held once every June. I knew of the conference from previous references, and it occurred to me that I had one last shot at making interactive storytelling technology available to the world. The attendees of Narrascope were not the techie types I had previously dealt with. These were mostly storytellers, weaker on the technology but stronger on the storytelling side. I decided to make my technology available to them, but to do so I would have to strip away all the technical complexity. I set to work building a web page that could edit storyworlds, using HTML, CSS, Javascript, and JSON. My programming powers were fading fast. Time and time again I would send my friend Dave Walker an email declaring that Javascript (or something else) was utterly broken, incapable of executing the simplest program without errors. Dave would ask to see the source code and I would present it to him with detailed notes proving that my code was perfect and Javascript was broken. He’d call me, we’d discuss it, and eventually he’d say something like, “Where did you terminate the loop beginning at line 563?” There would be a long silence, followed by the tiniest  from me. I’d thank him for his help and hang up. A week later, I’d be fuming again about another fundamental flaw in Javascript. Narrascope had accepted my lecture proposal, as well as my request to deliver a workshop on my technology. I spent dozens of hours working on the lecture; my lectures have always been top-notch and I wasn’t about to scrimp on this one. I made scores of nifty-keen images to illustrate my points. When will people learn that text doesn’t belong on a slide???Meanwhile, I struggled with the program. I didn’t quite get it finished, but it was workable and users could readily see that it was close to completion. On the big day I arrived at the airport at 5:00 AM to catch the early flight. We sat on the tarmac for an hour because of a mechanical problem, at which point I realized that I could not possibly make a crucial connection. I had to abort the trip to Narrascope and deliver the lectures via video, which turned out to be disastrous. This was my last-gasp effort to stimulate progress in interactive storytelling. "Once more, into the breach!” I had told myself. Now, more than a week after I delivered my spiel, not one person has answered my call for emails expressing some interest in my technology. Once again, my efforts were in vain.And so it is time for me to admit that, after all those decades of work, I have failed, with the single exception of Le Morte d’Arthur. When I designed for myself, I succeeded. When I designed for others, I failed. It’s time to throw in the towel and leave interactive storytelling to others. I don’t think that the world is ready. I feel like Charles Babbage, who invented the programmable computer in 1850. It used gears, levers, and cams and was brilliant. But the world had no need for programmable computers in 1850, so he never got the funding to build his invention. I’m nowhere near as smart as Charles Babbage, but my life dimly echoed his. I realized that my opus magnus, Le Morte d’Arthur, is a metaphorical autobiography of sorts. At the least, it expresses my experience working on interactive storytelling. Here is Merlin’s final conversation with Arthur:The time has come to close this chapter of my life. Perhaps I shall write a book summarizing my findings. Perhaps I shall not.]]></content:encoded></item><item><title>Xfinity using WiFi signals in your house to detect motion</title><link>https://www.xfinity.com/support/articles/wifi-motion</link><author>bearsyankees</author><category>hn</category><pubDate>Mon, 30 Jun 2025 19:03:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The hidden JTAG in a Qualcomm/Snapdragon device’s USB port</title><link>https://www.linaro.org/blog/hidden-jtag-qualcomm-snapdragon-usb/</link><author>denysvitali</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:34:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[EUD stands for Embedded USB Debug: essentially, this is a debug interface built right into almost every Qualcomm SoC since ~2018. Internally it hooks deep into the SoC, providing debug facilities for not just the CPUs but also the myriad of Hexagon co-processor/DSPs; many of the exciting details can be found in this patent from way back in 2014.In practise, for a non-production device (like a dev board, though some production devices seem to work too), EUD can be enabled by writing a few registers and then starting up the USB phy (though the details vary by generation). Instead of whatever typical gadget you might expect, what appears on your PC is a 7-port USB hub, with 1 port populated by the “EUD control interface”.With the right USB commands, a second device will appear, this one exposes an SWD interface! Yes! SWD right over the USB cable, no external tools, no soldering, and no expensive debuggers. Closed case debug that (almost) puts Google’s Suzy-Q to shame!For those unfamiliar: JTAG and SWD are both mechanisms for debugging the CPU cores inside a device, just like you can use GDB to debug programs on your computer (or your IDEs integrated debugger). They let you set breakpoints, halt execution, inspect the registers, single step instructions and all sorts of other useful things.For quite a while there has been a tantalising fork of openOCD published by Qualcomm on CodeLinaro, promising EUD integration. However, it relied on an at-the-time proprietary EUD library, which was only available to Qualcomm employees and their OEM partners.The device-side part of this (enabling the EUD interface so it shows up on your PC) has been somewhat supported in upstream Linux for a while. Back in August last year there was an attempt to extend this support for some newer platforms which have additional requirements. This sparked some discussion over the kernel policy: is it acceptable to have drivers in Linux that are only usable by some internal software, gatekept for Qualcomm and their paying partners? The answer appeared to be no, and this seemed to be enough to push Qualcomm in the right direction as after 8 months of silence, here we are!Let’s be fair, it almost definitely builds just fine on Ubuntu 20.10 with Qualcomm’s GCC 8.x toolchain. But that’s not what most people are using, we have to fix this!It turns out to be not too bad, just some minor stuff. Somehow they have  and  enabled though, and there is no way we’re gonna get that all passing just yet.With everything building, the necessary fixes (and a shiny new ) have been submitted to Qualcomm’s repo here.Now we have EUD building, we can try it with OpenOCD. It looks like they based their changes on the latest OpenOCD release 0.12.0, very nice. But wait, this release came out in 2023, and OpenOCD is still in active development… So there’s 2 years worth of changes, andAlmost 11k commits! It would really be nice to get this upstream eventually, so maybe let’s just rebase it real quick, we need to point it at the cleaned up EUD fork anyways.Among Qualcomm’s changes to support EUD, there are also patches adding Hexagon debugging support (and seemingly some improvements for LLDB as well). These got lost along the way but are almost certainly worth looking into at some point.So here we are, a fun day of fixing up and rebasing some codebases, and a very tasty reward!You can find the rebased OpenOCD patches over on the linux-msm GitHub along with some quickstart instructions in the README. So far this has been tested on the Snapdragon 845, it should work similarly for the 855 and 865 where we can get away with just poking the enable register and then using Linux or U-Boot to start a USB gadget. Newer SoCs however will probably require additional changes like these for SM8450. Let’s hope these old patch series get refreshed now that the tooling side of the story is in better shape!Torvalds himself famously doesn’t support the use of debuggers with the kernel (though that certainly hasn’t stopped the wonderful work on kgdb), he wrote (all the way back in 2000):I don’t like debuggers. Never have, probably never will. I do not condone single-stepping through code to find the bug.So of course, how practically useful JTAG support is really depends on your workflow. In the Qualcomm Landing Team at Linaro, debuggers have never been a staple of our work for all the typical reasons you’d expect (cost and complexity being the main ones), however with more focus being spent on non-kernel things like U-Boot and the secure world this dynamic is shifting.U-Boot is an obvious example for us, since it doesn’t currently provide stack traces when it crashes, diagnosis can sometimes be an arduous process which is made infinitely simpler with a .We are particularly interested in the possibilities that EUD opens up for debugging a vertically integrated BSP, especially when TF-A, OP-TEE and U-Boot are in the mix via the Trusted Substrate layer for OpenEmbedded. If this is something you’d like to explore with us then don’t hesitate to get in touch.In addition to the SWD peripheral, there is also a COM (UART) peripheral, and a trace peripheral. These haven’t yet been explored (and aren’t integrated into OpenOCD) but they should allow for a bidirectional serial port and MMIO tracing respectively. These do open up some more interesting use cases around Closed Cased Debugging in production - this appears to have been intentional on Qualcomm’s behalf with EUD being disabled as part of the production signing process, but with the ability to be re-enabled with a (cryptographically validated) “debug policy”.Some different SoCs use different addresses for the debug base and CTI base registers, as well as the additional changes required to enable EUD. If you’re able to make this work on your board/SoC, please do open an issue on the linux-msm fork and let us know what worked for you.Additionally, there is a strange quirk where the sticky reset bit of the PRSR register is always set, perhaps relating to SMP. For now the sticky reset behaviour of OpenOCD is stubbed out but it would be good to figure out what’s going on.SMP support in general is also currently lacking. The config file has been updated (using rcar as a reference) to define multiple CPU cores, but this doesn’t seem to behave correctly in Linux. For now it’s recommended to boot with  if you want to actually debug your kernel.Whether or not EUD is available on your device seems to depend on a variety of options: there are fuses to configure what debug functionality is allowed, as well as support for an OEM signed “debug policy” which can override this behaviour. On at least one production device (the OnePlus 6) EUD appears to be disabled via fuse, and yet it just works anyway. This device also has “crashdump mode” enabled which is not typical, this suggests that maybe OnePlus shipped the device with a loose debug policy, perhaps by mistake.Lastly, while it is of course extremely useful to have proper JTAG for debugging the kernel (especially when it’s so effortless!). The obvious question is: can this be used to gain control of higher execution levels? And unfortunately the answer appears to be no. If you do manage to halt execution in EL2, all registers will read as 0, and not much seems to be possible, at least on a production device. If your board behaves differently do let us know!EUD gives us a huge new surface to explore, and offers the potential to greatly improve the experience of low level debugging on Qualcomm boards. We are extremely excited that this is now published and freely available to use, and we very much hope it will become a seamless experience as the tooling and drivers are better integrated.It is awesome to see Qualcomm’s commitment to improving the developer experience and making their platforms more open is continuing to be demonstrated in their actions, EUD has the potential to save huge amounts of money on expensive debugging equipment, drastically reduce set-up times and make remote debugging easier too (no doubt it will eventually be integrated into our existing remote debugging tooling). Quite simply this raises the foundations for anyone working on Qualcomm platforms, and we can’t wait to see what’s next.]]></content:encoded></item><item><title>Datadog&apos;s $65M/year customer mystery solved</title><link>https://blog.pragmaticengineer.com/datadog-65m-year-customer-mystery/</link><author>thunderbong</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:31:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The internet has been speculating the past few days on which crypto company spent $65M on Datadog in 2022. I confirmed it was Coinbase, and here are the details of what happened. Originally published on 11 May 2023. with a bonus, free issue of the Pragmatic Engineer Newsletter. We cover one out of six topics in today’s subscriber-only The Scoop issue. To get full newsletters twice a week, Datadog is a leading observability tooling provider which went public in 2019, with a current market cap of $28B. The company made $1.67B revenue in 2022, circa $140M per month. On an earnings call a week ago, on 4 May, the CFO mentioned a “large upfront bill that did not recur,” saying:“Billings were $511 million, up 15% year-over-year. We had a large upfront bill for a client in Q1 2022 that did not recur at the same level or timing in Q1 2023. Pro forma for this client, billings growth was in the low 30s percent year-over-year.”If you’re like me, you’d probably skim over this detail, as it’s 15% here, 30% there. However, analysts attend these calls whose bread and butter is crunching the numbers and figuring out what a company might be trying to hide. A JP Morgan stock analyst did just this, quickly crunching numbers and asking the question:“David, looking at the math on this large upfront bill that did not recur, it seems to be about $65 million, if I'm running that correctly. Can you possibly shed a little more light?“Datadog’s CFO, David Obstler gave more details:“That was a crypto company which continues to be a customer of ours. But that was an early optimizer. We had always talked about some of the industries that were most affected and optimized.“So, who is this mysterious crypto company? Investor Turner Novak speculated that it’s Coinbase:He added he doesn’t know for certain that it is Coinbase, as other crypto companies have also raised silly amounts of money in the past several years.So, did Coinbase spend $65M on Datadog in 2022? Online there’s no shortage of theories, or people pretending to be Coinbase employees, such as this anonymous commenter on Hacker News, claiming that the $65M was for a 3-year upfront payment (which information I could not verify). I wanted to find the truth, so I tracked down software engineers at the company. And I got my answer:Yes. Coinbase spent $65M with Datadog in 2021, and this was their due bill for that year. I can confirm this, having talked with both current and former software engineers at Coinbase who shared details of what happened.Here’s how Datadog’s CEO explained, on the earnings call on what happened:“This is one of those situations where this customer was in an industry that got pretty much decimated over the past year. And their own business was cut in three or four, in terms of the revenue. And when that's the case, we really work with customers to restructure their contracts with us. We want to be part of the solution for them, not part of the problem (...) We restructure their contract, so we keep them as a happy customer for many more years and do a deal that works for everyone with their business profile.”And here’s what actually happened, as I understand from talking with engineers at Coinbase.Coinbase had an incredible 2021 and did not have to care about costs. The company went public in June that year, and was valued at an eye-popping $86B. In comparison, nearly two years later the company is valued around $14B, a 75% decline.During the boom, trading volumes were surging, beating record after record, and Coinbase could barely keep up. Here’s how Coinbase CEO Brian Amstrong summarized it:“So, obviously 2021 was just an incredible year for Coinbase, the kind of thing that you see very rarely in your lifetime, in a business career (...) We hit an all time high in our monthly transacting users of 11.4 million, which is 4x year-over-year, 400% pretty incredible.”Following the IPO in summer 2021, nobody at the company cared about infra costs; the only focus was growth. The company racked up huge bills for the likes of AWS, Snowflake, and also Datadog. And so, the $65M bill was for Datadog, for 2021. Coinbase settled the bill in Q1 2022.In early 2022 Coinbase suddenly needed to cut back infra spending. The crypto industry hit a sudden chill, affecting Coinbase’s business. As revenue dried up, the company turned its attention to reducing its overly high costs.For observability, Coinbase spun up a dedicated team with the goal of moving off of Datadog, and onto a Grafana/Prometheus/Clickhouse stack. A quick summary of these technologies:: a time series database. A very popular open-source solution for systems and services monitoring. Prometheus collects metrics from configured targets (services) at given intervals. It evaluates rules and can trigger alerts. It’s mostly written in Go, with some Java, Python and Ruby parts. Prometheus stores time series in-memory and on storage (HDD or SSD), using an efficient and custom format, and supports sharding and federation.Prometheus is part of the Cloud Native Foundation, membership of which indicates that it’s safe to build on top of Prometheus, as it’s actively maintained and will continue to be.Prometheus can be self-hosted, but several cloud providers also offer managed Prometheus services: both Google Cloud and AWS have this service in production, while Azure has it in preview.: the frontend for visualizing metrics. Grafana is a popular source analytics and monitoring visualization solution. If you need to display or dive into metrics or alerts, it’s the go-to tool, and widely used across tech companies. When I was at Uber, Grafana powered many of our graphs. Here’s an example of Grafana dashboards you can try out:: log management. A fast and open-source column-oriented database management system, which is a popular choice for log management. Clickhouse is written predominantly in C++, and is widely used across the industry. For example, Cloudflare uses Clickhouse to store all its DNS and HTTP logs – which is more than 10M rows per second! – and Uber uses Clickhouse as its central logging platform.Coinbase spun up its in-house approach without the main goal of saving costs, but to have full control and ownership of observability. Observability and reliability is a major differentiator for Coinbase, as it gives a competitive advantage over rivals.However, with the crypto market cooling, costs became a major focus, and it was clear the in-house Grafana/Prometheus solution was much cheaper. The Coinbase team had been double-writing the new stack for months, confirming everything worked well, and ironing out any issues.So Coinbase was ready to pull the plug on Datadog, but Datadog saved its customer relationship at the last minute by making Coinbase a very appealing deal it could not refuse. In future, the bill for Datadog would be nowhere near the $65M of 2021. As Brian Amstrong said of the crypto market during 2021, a $65M bill is the kind of thing you see very rarely in a business career.I asked an engineer at Coinbase who used the in-house stack and Datadog how they felt about the decision to stay on Datadog. They said it was ultimately the right decision, considering the reasonable costs, and the superior Datadog development experience.Coinbase could  have engineered a similar experience in-house. However, to provide a similarly seamless developer experience, would have likely taken tens of engineering years.“Expensive” in observability tooling is relative. Let’s assume that today Coinbase “only” spends, say, $10M per year on Datadog. Is this too much? Looking at the headline number, it’s tempting to think so.However, let’s look a level deeper. A platform like Datadog helps prevent outages, detects them instantly, and mitigates them faster. In 2022, Coinbase had 17 outages, totalling about 12 hours of downtime. The company’s daily average revenue is around $9M/day, based on their 2022 earnings.Assume that Datadog cuts the number of outages by half, by preventing them with early monitoring. That would mean that without Datadog, we’d look at 24 hours’ worth of downtime, not 12. Let’s also assume that using Datadog results in mitigating outages 50% faster than without - thanks to being able to connect health metrics with logs, debug faster, pinpoint the root cause and mitigate faster. In that case, without Datadog, we could be looking at 36 hours worth of total downtime, versus the 12 hours with Datadog. To put it in numbers: the company would make around $9M in revenue it would otherwise lose, Now that $10M/year fee practically pays for itself!What can we learn from Coinbase’s cost reduction exercise? Vendors are tight-lipped about their customers reducing spend, and it is a lucky coincidence that Datadog gave enough hints to find out who their big “early optimizer” customer was, and find out more details. But is the story of Coinbase a one-off?I’m not sure that it is. Three months ago, I covered the trend that Tech companies are aggressively cutting back on vendor spend - and two months later, The Wall Street Journal also reported on the same topic. Coinbase, to me, seems to have been early to the cost optimizing trend. However, look closely at the responses I gathered, and “AWS” and “Datadog” are the two most mentioned vendors as targets for cost savings. This is simply because infra and observability costs tend to be the highest and AWS is the leader for cloud infra, and Datadog the leader for observability.Datadog CEO Olivier Pomel confirmed that this type of optimization is happening across all of their customers, saying:“When we look at our data, when we look at what we hear from the hyperscalers also, we also listen carefully to their commentary on what they foresee in the near future, we don't see anything that gives us confidence that we can call an end to optimization in the next quarter or the quarter after that. So as far as our guidance goes and our plan for the year, we assume that this is going to continue at a similar level for the rest of the year.”I have since confirmed several large companies with thousands of engineers building their own Grafana/Prometheus stack, planning to migrate off of their current observability vendor and operate the observability stack themselves. But why is this?Above $2-5M/year annual spend is where bringing a vendor in-house tends to come up. And this is because it is around this number where the cost of hiring a whole team to do what a vendor is doing can  make sense.As a rule of thumb, you can get infra costs much lower than what vendors charge. This is because both the vendor, and you are probably using the same Cloud infrastructure provider, which is usually AWS, GCP or Azure. However, you would need to hire and staff a dedicated engineering team to build and run that infra.So, from a cost perspective, this is the math problem you need to solve. At what point does is this equation become true:$infra_cost + $platform_team_cost < $current_vendor_costsIn this question, $platform_team_costs will be above $1M, and sometimes above $2M. This is because you need to have a team of 4-5 engineers, plus a manager, and their average total compensation will be somewhere between $150-400K/year, depending on your cost basis.So when you have a bill that is above $2-3M/year, it can start to look tempting to build, rather than buy. The economics of this decision start to get down to how high of a margin is the vendor charging on top of raw infra? The curious question with Coinbase is: did they consider building, when talking about such a huge projected cost that could justify having a team?In the case of Coinbase, building in-house following a $65M bill was a clear no-brainer. They could hire a team of 10 senior and staff-level engineers in the Bay Area, and still have this team cost less than $5M/year. And they then only need to budget for the infra costs, which they can presumably bring down to low double digits per year.Coinbase planned to move off Datadog, but ended up staying. However, it is not the only larger tech company thinking about bringing observability in house. I have another exclusive which even Datadog might not be aware of, yet. This report is about Shopify and its plan to move off Datadog. But could recent layoffs change things? I cover details on this topic in the full The Scoop.This was one out of the five topics covered in this week’s The Scoop. A lot of what I share in The Scoop is exclusive to this publication, meaning it’s not been covered in any other media outlet before and you’re the first to read about it.The full The Scoop edition additionally covers:Will Shopify migrate onto an in-house observability tool? Shopify decided to build its own observability platform and migrate off Datadog. This plan looked certain until Shopify cut the very engineering teams that built its new platform. What happens next?.Microsoft cuts its compensation targets. Almost exactly a year ago, Microsoft employees received a welcome surprise: they could expect higher-than-usual compensation increases. Yesterday, another unexpected email came, but its contents were the opposite of last year’s. I talked with managers and engineers at the tech giant for their reaction to disappointing compensation news. .Shopify letting go most staff in Germany. As part of cutting 20% of staff, most people in Germany were made redundant. These layoffs happened a week before a Works Council election in Germany. Is this unlucky timing, or is there more behind the move?.Senior compensation trending down in Ukraine. Ukraine is one of the few countries for which we have access to nationwide data, through job site Djinni. Data for the first part of this year are in, and they point to something not seen recently: senior engineers are making less. Is this a local trend, or could we see it happening in other countries?.A follow-up to this week’s public tech company compensation article. Why was Netflix lower down the list than many software engineers expected? Plus, new details about Roblox and why Jack Dorsey’s total compensation is $2.75. ]]></content:encoded></item><item><title>Ask HN: What&apos;s the 2025 stack for a self-hosted photo library with local AI?</title><link>https://news.ycombinator.com/item?id=44426233</link><author>jamesxv7</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:10:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[First of all, this is purely a personal learning project for me, aiming to combine three of my passions: photography, software engineering, and my family memories. I have a large collection of family photos and want to build an interactive experience to explore them, ala Google or Apple Photo features.My goal is to create a system with smart search capabilities, and one of the most important requirements is that it must run entirely on my local hardware. Privacy is key, but the main driver is the challenge and joy of building it myself (an obviously learn).The key features I'm aiming for are:Automatic identification and tagging of family members (local face recognition).Generation of descriptive captions for each photo.Natural language search (e.g., "Show me photos of us at the beach in Luquillo from last summer").I've already prompted AI tools for a high-level project plan, and they provided a solid blueprint (eg, Ollama with LLaVA, a vector DB like ChromaDB, you know it). Now, I'm highly interested in the real-world human experience. I'm looking for advice, learning stories, and the little details that only come from building something similar.What tools, models, and best practices would you recommend for a project like this in 2025? Specifically, I'm curious about combining structured metadata (EXIF), face recognition data, and semantic vector search into a single, cohesive application.Any and all advice would be deeply appreciated. Thanks!]]></content:encoded></item><item><title>Sony DTC-700 audio DAT player/recorder</title><link>https://kevinboone.me/dtc-700.html</link><author>naves</author><category>hn</category><pubDate>Mon, 30 Jun 2025 18:03:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Don’t let anyone tell you otherwise: DAT players were fantastic. They
offered all the advantages of an audio cassette, but with the sound
quality of a CD. The compact audio cassette was a marvellous invention,
in its own way; but this technology struggled to provide audio fidelity
that would satisfy discerning listeners. Its frequency response was
limited, and the unavoidable background hiss was very obvious in quiet
environments. Still, in the 1970s audio cassettes were  way
most people listened to music, and I still have a stack of them.One thing that made cassettes so popular was that you could record on
them. Setting aside the legal issues, you could record from FM radio, or
from vinyl records, or even from microphones. It was easy to make ‘mix
tapes’ of you favourite tracks, and share them with friends. Cassettes
were everywhere – from portable players like the Walkman, to serious
hardware in hi-fi racks; they were even in cars.
There were shops that sold nothing but cassettes, and they sold by the
million.Serious hi-fi enthusiasts, however, listened to vinyl records or FM
radio. There  good-quality cassette decks, but the
`audiophile’ crowd embraced them with reluctance, if at all. Still, even
the most ardent hi-fi junkie couldn’t deny the usefulness of cassettes.
What we needed was something that could record high-quality sources,
with no loss of fidelity.That’s where DAT, ‘digital audio tape’ comes in. DAT offered digital
recording, in a range of qualities, the highest of which exceeded that
of CD. If you wanted to record from a CD, you could just connect the CD
transport’s digital output to the DAT’s digital input, and away you go.
Well, maybe – more this subject later. Of course, most DAT units could
record from analog sources like radio as well.DAT entered the market at about the same time as CD, but was much
less successful. For all its notional advantages, DAT never really
caught on in the domestic market, although it was somewhat more popular
in professional applications. A companion data storage technology, DDS,
used the same hardware, and was somewhat more successful although,
again, in professional rather than domestic applications.
Sony pulled out of the market in 2005, although I think it was clear
long before then that the format was moribund.The DTC-700, introduced in 1990, was Sony’s ‘budget’ hi-fi DAT
player/recorder. The more expensive DTC-55ES and DTC-60ES models had
fancy (and probably snake oil) features like a copper chassis. Yes,
copper is a better electrical conductor than steel, but a great chunk of
steel like the DTC-700 chassis is a pretty good conductor already. I’ve
not been able to find how much a new DTC-700 cost but, even as the
introductory model in the range, I imagine it was well into
sell-a-kidney territory. In 1995, even a five-year-old, second-hand unit
was eye-wateringly expensive. These days, you can pick up a refurbished
unit for about three hundred quid. It’s well worth the money – if you
can find tapes. There are lots more digital DDS tapes in circulation
that audio tapes; these are not guaranteed to be compatible with audio
players, but early DDS tapes often are.The DTC-700 had a flight-deck of controls, because it offered a stack
of functionality. It had two different digital inputs and an analog
input; there was a headphone amplifier with its own volume control; you
could skip to specific tracks by their number, or to a particular time;
and, of course, you could insert the meta-data that made this possible
when you recorded. And, like all serious hi-fi equipment, it had a
vaccuum-flourescent display, available in different colours. For that
real 70s look, you could buy it with mock-walnut case sides.Compared to cassettes, DAT recordings sounded fantastic. It wasn’t
necessary for the rest of your equipment – amplifier, speakers,
headphones – to be of top quality to realize this: the difference
between DAT and cassette was just that striking. In principle, DAT
offered better-than-CD quality, with its 48kHz sampling rate. In fact,
DAT set the standard here: 48kHz remains a common sampling rate to this
day. Folklore has it that Sony was encouraged to adopt 48kHz to make it
harder to record commercial CDs, which used (and still use) 44.1kHz.
Back in the 90s, technology hardly existed to resample these different
formats on-the-fly; eventually, Sony and others started selling DAT
units that supported 44.1kHz directly. This wasn’t an entirely welcome
move, as I’ll explain later.High cost was one of the reasons – perhaps the main reason – why DAT
didn’t catch on in the domestic market; but it certainly wasn’t the only
one. Another problem was the lack of original material: recording
studios didn’t seem to want to release commercial recordings on DAT.
Their reluctance isn’t hard to understand: DAT tapes could be copied an
unlimited number of times, with no loss of quality. In the the late 80s
it wasn’t easy to copy a CD onto DAT, because of the different sampling
rates. But there would have been no such limitation with a DAT-to-DAT
copy.Representatives of the recording industry were so worried about
illegal copying that, in the USA and elsewhere, they bullied legislators
into placing legal restrictions on the capabilities and sale of DAT
recorders. The USA also introducted taxation on the sales of DAT
devices, which was supposed to offset the loss in tax revenue that
illegal copying would create. This made expensive DAT players even more
expensive. Sony tamed the objections of the recording industry, to some
extent, by the simple expedient of buying CBS Records, one of the main
objectors. Nevertheless, the DTC-700 still suffers from the anti-copying
paranoia of the 80s; it will record a CD, but it will write meta-data
onto the recording to indicate that it’s a copy. The DTC-700, and other
DAT units of the same vintage, won’t record from another DAT unit, if
the meta-data indicates that the source is a copy. There are ways around
this limitation, but they’re fiddly.Whether illegal copying was a genuine risk or not, there never really
was a large selection of original music on DAT. As I recall, there
wasn’t even a “killer album” for DAT, like Dire Straits’  – an album so popular that people bought CD players just to
hear it at its best.DAT units also tended to have problems with reliability;
understanding why requires a basic understanding of how DAT technology
works.From a technological perspective, DAT was implemented in an
interesting way. “Interesting” in this context means, of course, “weird
and unreliable”. The DAT tape itself is only 4mm wide – the same as an
audio cassette. To get sufficient data bandwidth, the tape couldn’t be
scanned lengthwise, as all previous tape formats were. At the speeds
that would have been required, the tape length would have been
unmanageable. Instead, DAT works in a similar way to a VHS video
recorder: the magnetic head is on a rotating drum, aligned at an angle
to the direction of tape movement. This arrangement allows the whole
width of the tape to be used, not just a couple of narrow strips in the
middle.Naturally, the scanning mechanism required close-tolerance alignment
to operate reliably. Even when adjusted perfectly, the high rate of
rotation led to mechanical stresses. This was true of VHS as well, but
VHS players rapidly became throw-away items – eventually nobody really
cared if they only lasted a year or two. But if you’d just paid the
price of a new car for a DAT player, you’d expect a better service life.
And Sony didn’t help itself: the DTC-700 contained a huge number of
low-cost, plastic parts in critical locations. A plastic cog might cost
only pennies to replace, but stripping the machine down to get to it
cost a lot more.In the end, though, I don’t think it was the price, or the lack of
commercial releases, or the questionnable reliability, or the legal
complications that killed off DAT – although all these factors played a
part. Rather, I think it was just that old bugbear of the consumer
electronics industry: market saturation.By about 1992, everybody who was ever likely to want a home DAT
player already had one. The format couldn’t readily be improved, because
it already offered audio fidelity beyond the limits of human hearing. So
there wasn’t a “DAT Mark 2” that manufacturers could have sold to eager
customers. If DAT players could have been made more cheaply, this might
have expanded the customer base a little. But I doubt that DAT units
could ever have become as cheap as cassette players, and certainly not
as portable, because the electromechanical design was so complex and
fussy.It’s not as if any alternative technology has really presented
itself. These days, it’s trivially easy to record from digital or analog
sources, onto hard disk or solid-state storage. Any desktop computer
with a soundcard can do this. A number of manufacturers, including Sony,
did release self-contained hard-disk audio recorders, but they seem to
have enjoyed even less success than DAT. And these days, of course,
there’s even less need for such a device than there was in the 90s. If I
want to listen to a radio broadcast more than once, I can probably just
get it from the broadcaster’s website. Some modern radio tuners even
have built-in digital recording capabilities. No: if there were any
demand for a modern alternative to the DAT recorder, somebody would be
selling one.Many of the audio technologies from my youth have undergone a
revivial recently: vinyl records are the obvious example, but even
cassettes are starting to sell again. Are we likely to see renewed
interest in DAT? On the whole, I think probably not. Plenty of people
look back with fondness on vinyl and cassette, even on CD; I don’t think
DAT gives anybody a warm glow.]]></content:encoded></item><item><title>Proton joins suit against Apple for practices that harm developers and consumers</title><link>https://proton.me/blog/apple-lawsuit</link><author>moose44</author><category>hn</category><pubDate>Mon, 30 Jun 2025 17:58:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Earlier today, Proton filed court papers in the US District Court for the Northern District of California to join an existing class-action lawsuit against Apple. Proton is a plaintiff in the case, but we are representing and suing on behalf of a class of similarly situated developers. Challenging one of the most powerful corporations in the history of capitalism is not a decision we make lightly, but Proton has long championed online freedom, privacy, and security, and we believe this action is necessary to ensure the internet of the future lives up to its potential.Why are we doing this now?We believe that Apple’s conduct, as detailed in the complaint we filed, constitutes further violations of US antitrust law. Without this case, Apple could get away with behavior in the US that is already outlawed in the European Union. If this were to happen, American consumers, and developers focused on the American market, would have to pay higher prices for fewer choices, and be left at a disadvantage.There is also urgency to act now because of a parallel class-action suit by app developers against Apple on May 23, and any settlement there could be binding on all other developers. By joining that lawsuit, we can ensure that this suit will not only be about monetary damages to compensate app developers for the harm caused by Apple’s conduct, but also changes to App Store policies that will improve the state of the internet. We are seeking to permanently end anti-competitive behavior on the App Store, and we are joining this lawsuit to ensure that any future settlement enforces real changes to Apple’s practices and policies to benefit all consumers, developers, and competition, and not just cosmetic changes.While the suit does seek monetary damages on behalf of all developers who have been harmed in order to deter future anti-competitive behavior and provide compensation to class members harmed by Apple’s anti-competitive conduct, Proton will donate any money we receive from the lawsuit to organizations fighting for democracy and human rights so that some portion of Apple’s profits made from countries with authoritarian regimes are redirected to freedom. These donations will be coordinated through the nonprofit Proton Foundation, which oversees Proton and ensures that our work always prioritizes the public good over financial gain.Apple’s monopoly control of software distribution on iOS devices presents a myriad of problems for consumers, businesses, and society as a whole. Anti-monopoly laws exist because the power gifted by monopoly status inevitably leads to abuse. In the case of oligarchic tech giants, these abuses have wide implications for society, and it’s vital to the future of the internet that they be addressed now.The App Store policies hurt privacyApple’s App Store policies disproportionately favor the surveillance capitalism business model employed by companies like Meta and Google and therefore entrench an online business model that routinely violates consumers’ personal privacy. All developers are required to pay Apple an annual fee of $99 to be in the App Store, but Apple also takes a 30% cut from payments made through iOS apps, which are forced to use Apple’s payment system.Companies that monetize user data in exchange for “free” services that abuse your privacy aren’t affected by this, as they don’t process payments through the App Store. However, privacy-first companies that monetize through subscriptions are disproportionately hit by this fee, putting a major barrier toward the adoption of privacy-first business models. Naturally, these are also the very companies Apple is directly competing with through its disingenuous privacy marketing campaigns. This is a significant driver behind the internet’s descent into widespread surveillance capitalism.Apple’s policies undermine freedom and democracyApple’s complete control of the App Store has given it a dangerous level of control over app distribution, giving it the power to decide which apps can and cannot be distributed in different markets. Apple argues this control is necessary for security reasons. But the reality is that this has made Apple the single point of failure for free speech and a tool of dictatorships. There have been numerous incidents where Apple has removed or censored apps at the behest of authoritarian governments, in order to continue profiting from those markets.For example, the advocacy group GreatFire.org publishes important information about the state of censorship in the App Store through its AppleCensorship program, which highlights some striking statistics. Sixty-six of the 100 most popular apps worldwide are unavailable to iOS users in China. Additionally, all 240 VPN apps that the group tested were also unavailable to Chinese users. Overall, 27% of apps are missing from the Chinese App Store, more than double the global average of 13%. Many of those missing apps are news apps (including the likes of The New York Times, BBC News, and Reuters) or social networking or messaging apps, strongly implying that this is a matter of censorship, not security. Apple has also been caught removing apps to help suppress protests, such as the 2019 case of HKmap.Live, which was removed at the height of the pro-democracy protests in Hong Kong.Proton itself has also been victim of Apple’s censorship. In 2020, Apple threatened to take Proton VPN out of the App Store unless we removed language from our App Store description that said the app could be used to “unblock censored websites.” We don’t question Apple’s right to act on behalf of authoritarians for the sake of profit, but Apple’s monopoly over iOS app distribution means it can enforce this perverse policy on all app developers, forcing them to also be complicit. We believe it is critical for the future of the internet to end the monopoly on app distribution, so that developers and companies who are prepared to fight for democracy can do so.App Store policies lead to a worse user experienceApple’s approach to subscriptions management is designed to ensure it maintains complete control over the relationship between users and developers. To guarantee it gets its 30% cut of subscription revenue, it has imposed ironclad rules that dictate what developers can and cannot say to their users, which has a detrimental impact on the user experience. One basic example of this is that developers cannot tell users that other pricing options or discounts may be available if users upgrade via a website instead of inside the app. Not supporting Apple’s payment system is also considered a violation, which can lead to threats to remove your app, as happened to Proton.But this controlling behavior goes even further. Developers are prohibited from linking to their websites at all. Proton cannot even link to FAQ or customer support pages from its apps, as Apple believes it’s possible that users will then navigate from the support page to a pricing page and upgrade their accounts without paying Apple its fee. This has a direct, negative impact on customer experience.It’s also impossible for users to manage their subscriptions from multiple devices, as this would necessitate stepping outside Apple’s walled garden and weakening its control over the user. For example, users who upgraded their accounts on the web and then wish to upgrade or downgrade their subscription are not allowed to do so from their iOS devices. It is similarly impossible for users who purchased a subscription on iOS to change the subscription on the web. In a world where most users are accessing their apps and services over multiple devices, this is an unacceptably poor customer experience.Apple, however, goes even further in a bid to maintain its monopoly and trap users within the Apple ecosystem. Apple intentionally cripples third-party apps that compete with Apple services by making functionality that is available to Apple apps unavailable to other apps. For instance, there is no way to set Proton Calendar as the default calendar app on iOS. Furthermore, in a bid to prevent data portability, competing cloud storage services like Proton Drive are unable to seamlessly do background processing, while no such restrictions are known to exist for iCloud.These examples of coercive behavior illustrate time and time again that Apple is willing to inflict a worse experience and higher prices on consumers out of corporate greed, and it leverages its monopoly control over the App Store to do so.App Store tariffs cause price inflationApple’s 30% fees act as an artificial and arbitrary tax on internet commerce, which, much like a tariff, serves to raise prices, as part or all of this fee is inevitably passed on to the customer. Apple claims this fee is necessary to pay for the maintenance of the App Store, but evidence presented in the  case indicated that Apple makes a 78% profit on App Store fees, raising the question of whether these fees are really necessary or a clear example of the company profiting from its illegal monopoly.The only reason Apple can get away with this behavior is because there’s no competition in iOS app distribution or iOS in-app payments. If you want to provide an app or service to iOS users, you have to go through Apple’s systems, and you have to use Apple’s system for collecting payments. Breaking this monopoly and ending this punitive tax on the internet would allow companies like Proton to collect payments via less expensive methods, enabling the option to pass these savings on to you, and ultimately reducing the prices you pay.The remedies we are seeking would address many of the social ills mentioned above, ensuring that the internet of the future can continue to protect privacy and democracy. Mobile apps are now the dominant platform of the internet and the way the bulk of the world interacts with one another and with the web. Even if app stores started out as niche markets, today they are a critical component of the internet and fundamental to democracy. It is more essential than ever that we fight to create mobile ecosystems that are truly free, competitive, and not beholden to whichever dictator corporate leaders are currently bowing down to.This is also why we enter this fight not just representing ourselves, but as a class representative, to ensure that the outcome of this litigation will benefit all app developers and users of apps in this market. We expect this to be a difficult fight that could take many years, but our mission to build an internet that serves the interest of all of society affords us no other choice. By bringing this case, we hope to set an important precedent that free people, not monopolies, will dictate the future of the internet.Proton is being represented by Quinn Emanuel Urquhart & Sullivan LLP and Cohen Milstein Sellers & Toll PLLC. The full complaint in the case of Proton v. Apple can be found here.]]></content:encoded></item><item><title>I write type-safe generic data structures in C</title><link>https://danielchasehooper.com/posts/typechecked-generic-c-data-structures/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 30 Jun 2025 16:55:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I write type safe generic data structures in C using a technique that I haven’t seen elsewhere. It uses unions to associate type information with a generic data structure, but we’ll get to that. My approach works for any type of data structure: maps, arrays, binary trees… but for this article I illustrate the ideas by implementing a basic linked list. Since many people aren’t aware you can do C generics , I figured I’d start simple and build up to this:I hesitate to even mention this, because I do not like it, but its worth comparing to the technique at the end of this article. It works like this: you write your data structure in a header, using macros for your types, and then  the header multiple times; once for each type the data structure will be used with.While it  generic and type safe, it has downsides:makes it hard to find where types and functions are defined (because they’re constructed by macros)code completion may not handle them wellbloats your binary size and build times with copies of the same functionsrequires using type-prefixed functions: Foo_list_prepend() and int_list_prepend() vs just Another way to make a data structure generic is to use . It’s not type safe but we’ll get to that.Note:  is used for familiarity, but I highly recommend Arenas instead. You can watch or read about them.Having  and its  as separate allocations isn’t ideal from a memory and performance perspective. It requires 2 allocations per node when one would do, the  pointer uses memory unnecessarily, and you will likely get two cache misses per node when traversing the list: once getting the next node, and once getting its data. We can fix these issues with…Generics level 2: Inline storageInstead of storing a pointer to the node’s data, we can use a Flexible Array Member to store the data inside the node. To do so, we make a single allocation large enough for both the node and the type it stores:Now  and the actual contents of  are beside each other in memory, solving the issues of the  approach. Unfortunately we now have to pass the size, but we’ll fix that in the next sectionIf you wanted to avoid the , and initialize the node’s memory directly, you could do so with a  function:Generics level 3: Type CheckingThe part you’ve all been waiting for: how to get the compiler to error when we try to add the wrong type to a list. The way I found to do this is to use a union with a  member that has a parameterized type:How does that help us? Well, we can use the ternary operator to enforce that the  parameter is the same type as the list’s :The macro also handles passing the item size for us! This is the error Clang produces when adding the wrong type to the list:Macros get a bad rep, but I think this is fairly understandable. Some things to note:  is never used at runtime, it exists just for type information at compile time. Using a union makes  not consume any memory.If you’re writing a generic function that needs to return a pointer to contained data, you can use  to cast the return type from  to the data structure’s  type.  is supported in all three big C compilers (clang, gcc,  msvc since version 19.39).If for some reason you don’t like using the ternary operator to ensure two types are the same, a previous version of this article used a different technique:One annoying thing about C compilers released prior to late 2025 is that they do not consider these two variables to have the same type:Even though the variables have identical type definitions, the compiler still errors because they are . A  avoids the issue:You can use this for any type of data structure, even ones with multiple associated types, like a hash map:For more detail, like how the  macro is implemented, see the code hereThanks to Martin Fouilleul for the encouragement to finish this post, which I’ve been sitting on for months, and the feedback on early drafts.]]></content:encoded></item><item><title>A CarFax for Used PCs; Hewlett Packard wants to give old laptops new life</title><link>https://spectrum.ieee.org/carmax-used-pcs</link><author>rubenbe</author><category>hn</category><pubDate>Mon, 30 Jun 2025 16:38:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ E-waste Monitore-wasteMany enterprises follow a standard three-year replacement cycle, assuming older computers are inefficient. However, many of these devices are still functional and could perform well with minor upgrades or maintenance. The issue is, no one knows what the weak points are for a particular machine, or what the needed maintenance is, and the diagnostics would be too costly and time-consuming. It’s easier to just buy brand new laptops.When buying a used car, dealerships and individual buyers can access each car’s particular CarFax report, detailing the vehicle’s usage and maintenance history. Armed with this information, dealerships can perform the necessary fixes or upgrades before reselling the car. And individuals can decide whether to trust that vehicle’s performance. We at HP realized that, to prevent unnecessary e-waste, we need to collect and make available usage and maintenance data for each laptop, like a CarFax for used PCs.There is a particular challenge to collecting usage data for a PC, however. We need to make sure to protect the user’s privacy and security. So, we set out to design a data-collection protocol for PCs that manages to remain secure.The firmware-level data collector Luckily, the sensors that can collect the necessary data are already installed in each PC. There are thermal sensors that monitor CPU temperature, power-consumption monitors that track energy efficiency, storage health indicators that assess solid state drive (SSD) wear levels, performance counters that measure system utilization, fan-rotation-speed sensors that detect cooling efficiency, and more. The key is to collect and store all that data in a secure yet useful way. We decided that the best way to do this is to integrate the life-cycle records into the firmware layer. By embedding telemetry capabilities directly within the firmware, we ensure that device health and usage data is captured the moment it is collected. This data is stored securely on HP SSD drives, leveraging hardware-based security measures to protect against unauthorized access or manipulation. The secure telemetry protocol we’ve developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we use is the Endpoint Security Controller, a dedicated piece of hardware embedded in business-class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD—where telemetry and sensitive data are stored—by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered.Then, the collected data is recorded in a time-stamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs.The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information.The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP’s factory and supply-chain records, digital-services platforms, customer-support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner databases, third-party component manufacturers like Intel, and other original equipment manufacturers. This multisource approach creates a complete picture of the device’s entire life cycle, from manufacturing through all subsequent ownership and service events.For IT teams within organizations, we hope the PCFax will bring simplicity and give opportunities for optimization. Having access to fine-grained usage and health information for each device in their fleet can help IT managers decide which devices are sent to which users, as well as when maintenance is scheduled. This data can also help device managers decide which specific devices to replace rather than issuing new computers automatically, enhancing sustainability. And this can help with security: With real-time monitoring and firmware-level protection, IT teams can mitigate risks and respond swiftly to emerging threats. All of this can facilitate more efficient use of PC resources, cutting down on unnecessary waste.We also hope that, much as the CarFax gives people confidence in buying used cars, the PCFax can encourage resale of used PCs. For enterprises and consumers purchasing second-life PCs, it provides detailed visibility into the complete service and support history of each system, including any repairs, upgrades, or performance issues encountered during its initial deployment. By making this comprehensive device history readily available, PCFax enables more PCs to find productive second lives rather than being prematurely discarded, directly addressing the e-waste challenge while providing economic benefits to both sellers and buyers in the secondary PC market.While HP’s solutions represent a significant step forward, challenges remain. Standardizing telemetry frameworks across diverse ecosystems is critical for broader adoption. Additionally, educating organizations about the benefits of life-cycle records will be essential to driving uptake. We are also working on integrating AI into our dashboards. We hope to use AI models to analyze historical telemetry data and predict failures before they happen, such as detecting increasing SSD write cycles to forecast impending failure and alert IT teams for proactive replacement, or predicting battery degradation and automatically generating a service ticket to ensure a replacement battery is ready before failure, minimizing downtime.We plan to start rolling out these features at the beginning of 2026.]]></content:encoded></item><item><title>Donkey Kong Country 2 and Open Bus</title><link>https://jsgroth.dev/blog/posts/dkc2-open-bus/</link><author>colejohnson66</author><category>hn</category><pubDate>Mon, 30 Jun 2025 15:01:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Donkey Kong Country 2 has a pretty well-known bug in the old SNES emulator ZSNES where some stages have spinning barrels that don’t work properly. One of the earliest pictured here, in the first stage of Krem Quay (third world):After you jump into the barrel, you’re supposed to be able to completely control its rotation by pressing left and right on the d-pad, with the barrel only rotating while you’re holding left or right. In ZSNES, this is horribly bugged. Tapping left or right makes the barrel spin forever in that direction, until you press the opposite direction…which simply makes it spin forever in the opposite direction.This is more than just annoying - it makes these stages significantly more difficult than the developers intended, since later on the spinning barrels show up over spikes and other hazards:This used to be somewhat documented in threads on the ZSNES forums, but those unfortunately seem to have gone offline since last I looked at them, and I can’t find the relevant threads indexed in the Wayback Machine.This bug is caused by ZSNES not emulating open bus behavior. I believe this was originally discovered by Anomie roughly two decades ago, who subsequently fixed the same bug in Snes9x. This original fix hardcoded the specific addresses to return the values that the game depends on rather than properly emulating open bus, but it fixed DKC2 and probably didn’t break anything else. The bug was never fixed in ZSNES, which is now a long abandoned project (last release in 2007).Purely out of curiosity, I wanted to dig into this a little more to figure out what exactly in the game code causes these barrels to spin forever in an emulator that doesn’t emulate open bus behavior.On older platforms like the SNES, reading from an invalid memory address  does not crash the program. There are cases where accessing specific invalid addresses can cause the hardware to lock up, but I don’t believe this can happen on SNES.Instead, reading from an invalid address usually triggers open bus behavior, where the CPU re-reads the last value that was put on the data bus. SNES specifically has several different internal buses that can retain different open bus values, but this doesn’t affect DKC2.The main SNES CPU is a 65C816 (aka 65816). There’s some other hardware around it as part of the Ricoh 5A22 S-CPU package, such as a multiplication/division unit and a DMA unit, but the core CPU is a 65816.65816 is a 16-bit extension of the 6502, a very popular 8-bit CPU used in many systems including the NES (with slight modifications). The 65816 is mostly backwards compatible with 6502 software, which was not important for the SNES (which has no NES backwards compatibility) but was very important for the Apple IIGS that this CPU was originally designed for.I personally think the 65816 ISA is pretty awkward. 8-bit vs. 16-bit operation is based on new processor status flags M (accumulator / memory size) and X (index register size) rather than being encoded into opcode bits, so software needs to frequently execute the new instructions SEP (set processor flags) and REP (reset processor flags) to manually adjust register and memory access sizes as needed. This also makes 65816 disassembly extraordinarily painful without tracing execution in an emulator, since some instructions vary in length depending on the current processor flags - e.g. an immediate operand can be either 1 byte (8-bit) or 2 bytes (16-bit).Beyond that (and slightly more relevant to this post), addressing more than 64 KB of memory requires dealing with memory banking which is not fun. The 65816 has a 24-bit address bus, but most addresses are created by combining an 8-bit bank with a 16-bit offset. This is sort of similar to how the earliest x86 CPUs segment memory into 64 KB segments, except 65816 has no address overlap between different 64 KB memory banks.Many instructions still operate using 16-bit addresses internally, like on 6502, plus the program counter is still 16-bit. There’s a new 8-bit program bank register (PBR / K) used for instruction fetches, and a new 8-bit data bank register (DBR / B) used for instructions and addressing modes that produce a 16-bit memory address rather than 24-bit. Software needs to manually track and update these bank registers as needed. There are long jump instructions that simultaneously update PBR and PC, but regular jump instructions and conditional branch instructions cannot jump between different program banks.The hardware stack and the direct page (65816’s replacement for the zero page) are not banked - they are always located within memory bank $00.The SNES memory map is  designed around the 65816’s memory banking. It’s much more useful to think of SNES memory addresses as a separate 8-bit bank and 16-bit offset rather than a single 24-bit address.When you’re inside one of these spinning barrels, Donkey Kong Country 2 reads from addresses $2000 and $2001 in bank $B3. In some other banks these addresses would map to either the cartridge or RAM, but in bank $B3 they are not mapped to anything, so reading from them is open bus behavior. Why does the game do this?Here’s a disassembly of the part of the game code that performs the open bus read, generated from an execution trace and then edited a bit for clarity (e.g. replacing the relative branch offset with a label). This is part of a routine that’s executed once per frame, beginning when you release left/right on the gamepad while you’re in a spinning barrel:This routine accesses a few memory addresses in the $0000-$2001 range. Some of them are through absolute addressing modes that use the current data bank of $B3, while others use direct page addressing modes that always access bank $00. The direct page itself is located at $0000 here, same as the 6502 zero page.Banks $B3 and $00 happen to have the same memory map for this $0000-$2001 address range. In banks $00-$3F and $80-$BF, $0000-$1FFF always maps to the first 8 KB of the console’s 128 KB of working RAM (WRAM). $2000-$20FF is entirely unmapped, so the  instruction is an open bus read.It’s using a few WRAM addresses here that seem to contain the following, based on what values the game writes to them and what it uses them for:$0EE6 ($48 + X=$0E9E): The current barrel orientation$0E0A ($0028 + Y=$0DE2): Per-frame rotation amount, as a change to barrel orientation$0032: Seems to be used as just a temporary variableI imagine the exact orientation/rotation locations in WRAM are different for different barrels.The barrel orientation appears to be on a scale where 0x0000 is pointing straight down, 0x4000 is pointing straight left, etc.The rotation amount determines the barrel rotation speed. For the barrel I looked at, the game sets the rotation amount to 0x0300 when rotating clockwise and 0xFD00 (-0x0300) when rotating counterclockwise. This makes a full 360 degree rotation take just over 85 frames, a little less than 1.5 seconds at 60 frames per second.Okay, starting to step through this routine:This part is straightforward: It loads the current orientation, adds the rotation amount, and stores the result in a temporary variable. It executes CLC (clear carry flag) before ADC (add with carry) because the 65816, like the 6502, does not have an add without carry instruction.Next is the interesting part:It XORs the updated orientation with the previous orientation, bitwise ANDs the result with an open bus read from $2000, and then branches based on whether the bitwise AND produced zero. The spinning forever bug triggers when the branch is  taken because the bitwise AND result is  zero.On actual hardware, the 16-bit open bus read from $2000 always returns 0x2020. This is because the last byte read from the bus is always the high byte of the $2000 absolute address encoded in the instruction bytes, little-endian:Since the 65816 only has an 8-bit data bus, it implements 16-bit reads by performing two consecutive 8-bit reads, which in this case will both return 0x20. Hence the 16-bit value 0x2020.So, in practice, that part of the routine behaves equivalently to this:Moving on, when the AND result is zero and the  branch is taken, it does this:It loads the pre-XOR rotated orientation from the temporary variable, writes it to the permanent orientation location in WRAM, then returns. The rotation will continue next frame.When the AND is non-zero and the branch is not taken, it does this:First, it zeroes out the rotation amount. 65816 has a dedicated instruction STZ (store zero) for zeroing out a memory location, but STZ doesn’t support any Y-indexed addressing modes like what the game uses here (absolute indexed Y).Next, it loads the pre-XOR rotated orientation, adds 0x1000, and masks out all but the highest 3 bits. This is a crude but fast way of approximately rounding to the nearest multiple of 0x2000.Finally, it writes the rounded orientation to the permanent location in WRAM and then returns.All together, in a higher level language, this routine is doing this:With the open bus read returning 0x2020, the XOR-then-AND result will be non-zero when adding the rotation amount to orientation changes either bit 5 (0x0020) or bit 13 (0x2000). Given a rotation amount of 0x0300 or 0xFD00, bit 5 is always 0, so only bit 13 can ever change.For convenience, here’s the orientation values diagram again:
Looking at this, an orientation change of 0x2000 corresponds to a single-step change in cardinal or ordinal direction. This means that bit 13 will change when the barrel either reaches or passes over one of these 8 directions. Whether it changes upon reaching or upon passing over depends on the rotation direction, but it’s not really significant from a player perspective since it’s only a 1-frame difference and only in specific cases.Rounding to the nearest multiple of 0x2000 ensures that the stopped barrel points exactly in a cardinal or ordinal direction, since it may have passed over the direction on the final rotation frame.So, if you replace the open bus read with a constant 0x2000, I think this logic makes sense! When you release the d-pad, the barrel continues to rotate in the same direction until it reaches the next cardinal or ordinal direction, and then the rotation stops with the barrel pointing exactly in that direction.At this point I am pretty sure the open bus read was simply caused by a typo.I think that  instruction (absolute addressing) was supposed to be  (immediate addressing).  just happens to work because the 16-bit open bus read from $2000 returns a value that is functionally equivalent to 0x2000 in this logic as long as the per-frame rotation amount always has its lowest 6 bits set to 0.The incorrect opcode is executed at bank $B3 offset $EDAC, which maps to $33EDAC in the game’s 4 MB of ROM. Changing this byte from 0x2D (AND with absolute addressing) to 0x29 (AND with immediate addressing) makes the spinning barrels work correctly even if open bus reads always return 0. The exact location in ROM probably varies between different revisions of the game; I only looked at one revision.This was purely an academic exercise since the game works perfectly fine in just about every SNES emulator other than the long-obsolete ZSNES, but my curiosity is satisfied.]]></content:encoded></item><item><title>There are no new ideas in AI, only new datasets</title><link>https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only</link><author>bilsbie</author><category>hn</category><pubDate>Mon, 30 Jun 2025 14:43:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[“Moore’s Law for AI”Although I don’t really agree with this specific framing for a number of reasons, I can’t deny the trend of progress. Every year, our AIs get a little bit smarter, a little bit faster, and a little bit cheaper, with no end in sight.Most people think that this continuous improvement comes from a steady supply of ideas from the research community across academia – mostly MIT, Stanford, CMU – and industry – mostly Meta, Google, and a handful of Chinese labs, with lots of research done at other places that we’ll never get to learn about.And we certainly have made a lot of progress due to research, especially on the systems side of things. This is how we’ve made models cheaper in particular. Let me cherry-pick a few notable examples from the last couple years:FlashAttentionspeculative decodingDeepMindMuonDeepSeek-R1ArXivAlexNet model4. Reasoning: in 2024 OpenAI released O1, which led to DeepSeek R1If you squint just a little, these four things (DNNs → Transformer LMs → RLHF → Reasoning) summarize everything that’s happened in AI. We had DNNs (mostly image recognition systems), then we had text classifiers, then we had chatbots, now we have reasoning models (whatever those are).Say we want to make a fifth such breakthrough; it could help to study the four cases we have here. What new research ideas led to these groundbreaking events?all the underlying mechanisms of these breakthroughs existed in the 1990s,Supervised learning via cross-entropy, the main way we pre-train language models, emerged from Claude Shannon’s work in the 1940s.introduction of policy-gradient methods in 1992enabled us to learn from a new data source:ImageNet3. RLHF allowed us to learn from human labels indicating what “good text” is (mostly a vibes thing)“verifiers”Remind yourself that each of these milestones marks the first time the respective data source (ImageNet, The Web, Humans, Verifiers) was used at scale. Each milestone was followed by a frenzy of activity: researchers compete to (a) siphon up the remaining useful data from any and all available sources and (b) make better use of the data we have through new tricks to make our systems more efficient and less data-hungry. (I expect we’ll see this trend in reasoning models throughout 2025 and 2026 as researchers compete to find, categorize, and verify everything that might be verified.)There’s something to be said for the fact that our actual technical innovations may not make a huge difference in these cases. Examine the counterfactual. If we hadn’t invented AlexNet, maybe another architecture would have come along that could handle ImageNet. If we never discovered Transformers, perhaps we would’ve settled with LSTMs or SSMs or found something else entirely to learn from the mass of useful training data we have available on the Web.This jibes with the theory some people have that nothing matters but data. Some researchers have observed that for all the training techniques, modeling tricks, and hyperparameter tweaks we make, the thing that makes the biggest difference by-and-large is changing the data.developing a new BERT-like model using an architecture other than transformers*there is an upper bound to what we might learn from a given dataset*The Bitter LessonThe obvious takeaway is that our next paradigm shift isn’t going to come from an improvement to RL or a fancy new type of neural net. It’s going to come when we unlock a source of data that we haven’t accessed before, or haven’t properly harnessed yet.a random site on the WebIt’s safe to say that as soon as our models get efficient enough, or our computers grow beefy enough, Google is going to start training models on YouTube. They own the thing, after all; it would be silly not to use the data to their advantage.It’s hard to say whether YouTube or robots or something else will be the Next Big Thing for AI. We seem pretty deeply entrenched in the camp of language models right now, but we also seem to be running out of language data pretty quickly. But if we want to make progress in AI, maybe we should stop looking for new ideas, and start looking for new data.]]></content:encoded></item><item><title>Show HN: TokenDagger – A tokenizer faster than OpenAI&apos;s Tiktoken</title><link>https://github.com/M4THYOU/TokenDagger</link><author>matthewolfe</author><category>dev</category><category>hn</category><pubDate>Mon, 30 Jun 2025 12:33:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[TokenDagger is a drop-in replacement for OpenAI’s Tiktoken (the tokenizer behind Llama 3, Mistral, GPT-3.*, etc.). It’s written in C++ 17 with thin Python bindings, keeps the exact same BPE vocab/special-token rules, and focuses on raw speed.I’m teaching myself LLM internals by re-implementing the stack from first principles. Profiling TikToken’s Python/Rust implementation showed a lot of time was spent doing regex matching. Most of my perf gains come from a) using a faster jit-compiled regex engine; and b) simplifying the algorithm to forego regex matching special tokens at all.Benchmarking code is included. Notable results show:
- 4x faster code sample tokenization on a single thread.
- 2-3x higher throughput when tested on a 1GB natural language text file.]]></content:encoded></item><item><title>Reverse Engineering Vercel&apos;s BotID</title><link>https://www.nullpt.rs/reversing-botid</link><author>hazebooth</author><category>hn</category><pubDate>Mon, 30 Jun 2025 12:19:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: New Ensō – first public beta</title><link>https://untested.sonnet.io/notes/new-enso-first-public-beta/</link><author>rpastuszak</author><category>dev</category><category>hn</category><pubDate>Mon, 30 Jun 2025 11:02:55 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[The new version of Ensō (codename: Occult Vampire Keanu) is available for public testing!
This is a temporary icon I used for testing. I am considering creating a simplified version of it. PS. here's the original image (on potato.horse, of course)Following MISS, my focus is on removing distractions over adding new features. This can be surprisingly challenging (e.g. how do I tell users about feature X or Y without breaking their flow?) but also gives me time to focus on polishing the app.(we will discuss these in more detail in future posts)Short version (as explained by Hermes Trismegistus) Simplified, more accessible UIAn even more simple, streamlined UI, following the MISS philosophy.Most of the UI has been moved to the application menu bar for easier discoverability and shortcut access. So far no one has missed the old inline UI, but you can read more about it towards the end of this note.5½ Accessibility-friendly themes to choose fromWe have  5½ predefined themes focussed on accessibility and specific use patterns based on feedback I've collected over the years.writing during the day in regular light conditionswriting in low light for devices with OLED screens
writing in extremely low light conditions, with reduced light exposure  (See Midnight, Obsidian for Vampires)
designed for OLED screensthe main use case here is writing at night, to put myself to sleep.5½ and not 6 because one theme still needs some work. Is there a specific use case or theme you'd like to see in Ensō? Let me know!This is one of the few truly new features in Ensō. Coffeeshop mode allows you to stop worrying that someone standing behind you might see what you're typing. The text itself is concealed but you still know what you're writing.  Use  to toggle on and off at any time.I've been using it for a couple of months and found it super helpful, especially for journaling in public places, but not only (read more here: Sketch - Ensō Coffeeshop Mode).A few smaller accessibility improvements
Note: if you remove the  menu and call it , MacOs won't add its AI crap to your settings. toggle autocorrect, autocapitalise, spelling control text size (previously not possible in the native version)A new, polished text rendering engineThe new text rendering engine allows for better control over typography settings, supports alternative display modes like Coffeeshop, and uses a custom caret.I don't know how to describe it objectively (and I obviously lack the distance to) but writing in the new UI feels different, more fluid. The text is easy to read, but also somewhat softer (though not blurry).Less is more, so why do I care about it?  less is more. I want Ensō to feel familiar and high-quality, like a good Moleskine notebook. I want people to feel comfortable paying $10 for a typing app without text selection. I want them to enjoy it as much as I do. Fewer features allow me to focus more on what  there.Ensō will be published via the AppStore by default. We will keep the old version on Gumroad, but there's no reason to maintain it, since the new version is better in every possible way and functionally the same by default.The reasons I decided to skip the AppStore and use Gumroad, plus what I learned from that are beyond the scope of this note (you can click the link to request that particular write-up).several users complained that Gumroad payment looked, for the lack of a better word, shady, especially at the step with a PayPal payment screen. The ones who messaged me still bought the app, but I imagine there were many who turned back.AppStore with all its flaws makes delivering apps... slow and annoying, but also relatively easy without much code.I can add OTA updates and re-publish Ensō via Gumroad later, which makes sense as an iterative improvement.The Gumroad version of Ensō will stay as a backup, but will not be maintained.I've been using Ensō daily for 6 years. I've also received a ton of high-quality feedback, not via analytics but from users who were kind enough to reach out to me. I like to think that I have a fairly good idea of how and why people use Ensō.The previous version of Ensō would pass an anonymous impression event on load. Now, by design, no network traffic is made at all. Here's our new Privacy Page.
Current version of our Privacy page (source)It will come, but the new version is already so much better than the previous, that I feel like waiting for more features would be a wasted opportunity.I'm working on a UX that balances discoverability with staying focussed. Each option, each new choice is a chance for you to get distracted, so the key is to do this thoughtfully and with respect towards my users' time.RTL (or non-LTR) language supportThis one will be included in the next test build. Many Ensō users speak languages written in non-Latin alphabets (to my knowledge, mainly Persian, Arabic and Hebrew).It makes me both grateful and somewhat sad that one (non-techie) user went as far as even sharing a code sample with me when asking for fixing the issue. Adding rudimentary RTL support can be as simple as a one-line change in your code. Even if it's not perfect - it's still a huge improvement that your non-Latin script users will notice, believe me.The previous version of Ensō displayed the UI in the same space as the text. That's not the case any more.I'm still considering adding a hamburger menu in the main app canvas, however only two (less frequent) users of Ensō have brought it up so far.ease of use, reducing distractions
There's tension between 1. and 2. as every new feature implies more choices on the user's part; every new choice is an opportunity for distraction. This might seem pedantic, but small, seemingly insignificant changes do add up.Removing things is harder than adding them (see 3.). Perhaps that's why commits with negative LoC count feel so good.
Where to go from here?Collect the test feedback and respond to itPrepare basic marketing materials
I might put an ad on social media, trying to get people off it (Sit.) but what I call marketing is mostly talking about Ensō and related subjects here, plus engaging with communities I already know, such as forums Windows and Linux support — I'll revisit it in the next few months. I'm moving towards supporting myself from my own projects and I need to be selective how I use my time. If you're interested in testing a Windows or Linux build, let me know. Quick Save - hitting  would automatically save a snapshot of your notes to a predefined directory with a time-stamped file name, e.g.  Toybox - an optional menu feature with experimental tools released episodically, such as: visual experiments (e.g. different typography styles or letters and words turning into vines that grow as you type)If Toybox becomes a reality, it'll be buried in the menus to avoid distractions and will act mainly as my platform for experimentation and play with users. If there's a chance it might introduce more distractions - it'll become a separate app. (Kind software)Every day in small chunks and some days in longer stretches.I'm approaching this just like My Recent Art Exhibition - working on different things simultaneously, focussing on their interplay rather than looking at each feature in isolation.While I believe you should Share your unfinished, scrappy work, I know Ensō well enough that I can allow myself more flexibility. This style of work gives me a lot of joy and the end results have so far been better than expected.The new Ensō is not the type of project I can share in small unfinished bits, feature by feature. I will repeat this ad nauseam: I want to give you something that will get out of your way but also feel beautiful, polished, yours.This is akin to good typography or UX - when it's there, you don't notice it, but at a subconscious level, you feel more comfortable with the tool and want to spend more time using it. That has been my experience so far.Tauri is much more mature than when I released the first macOS version of Ensō. I spent weeks getting the previous version to build properly on Mac with notarisation, provisioning profiles and undocumented AppStore Connect APIs. Now, most of the things just work (sometimes with a bit of scripting, which is where Claude Code turned out to be indispensable).I'm not an "IndieHacker", I'm not in a rush, I'm a wannabe-carpenter (Brief History of Galician Carpentry) and Ensō happens to be made of stuff that can be worked in a carpentry-like manner. The small feature set means I can afford to take time to work on this with enough care, which I hope shows in the final product.Building a theme switcher can be a weirdly complex problem (if you complicate it well enough). The difficult part was letting users set themes for dark/light/sync with OS mode, with previews, making it obvious when changes are saved, all in a single piece of UI, with max 2-3 clicks.Most of my attempts at this resulted in something that looks more than the Dwarf Fortress GUI than a simple theme picker. I understand now why almost no one is doing this and why the few who do split the UI in several steps.I'm still happy with using a browser as the text rendering engine. Especially with Safari, the amount of control over typography is just excellent (e.g.  ).I wish there was an easy way of getting the native accent colour from the OS, but that's not possible at the moment.  can be customised, but not read.I'm not planning to remove the free web version of Ensō. I want to get paid for my work, but people reach out to me and buy it with virtually no marketing. I'm hopeful, even optimistic that the trust I've earned so far, as well as the quality of the final product, will be enough for it to grow slowly but steadily.That's all for today. Thanks for reading!]]></content:encoded></item><item><title>The provenance memory model for C</title><link>https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/</link><author>HexDecOctBin</author><category>hn</category><pubDate>Mon, 30 Jun 2025 09:25:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[[The wordpress markdown inclusion does a very bad job, it seems, there have been some idiotic formatting errors. I hope that these are fixed, now.]A years-long effort led by Kayvan
Memarian and Peter
Sewell from Cambridge University,
UK, Martin
Uecker from Graz
University of Technology, Austria, and
myself
(from ICube/Inria, France) has guided the C community to accept a
common vision of so-called , defining how to trace
the origins of pointer values through program execution.  Our
provenance-aware memory object model for
C
provides a precise mathematical specification, in place of the
ambiguity of these aspects of the current C
standard. It has also
stimulated and informed discussion of provenance in the broader C,
C++, Rust, and compiler communities.This work has finally resulted in the publication of an international
standard, Technical Specification ISO/IEC TS
6010 (edited by Henry
Kleynhans, Bloomberg,
UK).  With the goal of making modern information systems safer and
more secure, this official technical specification provides direction
to all stakeholders in the industry such that they can converge their
platforms and tools.Pointer aliasing and program optimizationWe say that two pointer values  and  during the execution of a
program  if they point to the same object in
memory. To see that the question if two pointers alias
has an influence on the optimization of code, let’s consider the
following simple example of an iterative function.It implements an approximation algorithm for the reciprocal  of the value  which doesn’t use division.// Reciprocal approximation
//
// This interface is horrific, don't use it.
// This just serves as an artificial example
// for this article.

constexpr double ε  = 0x1P-24;
constexpr double Π⁻ = 1.0 - ε;
constexpr double Π⁺ = 1.0 + ε;

void recip(double* aₚ, double* řₚ) {
    for (;;) {
        register double Π = (*aₚ)*(*řₚ);
        if ((Π⁻ < Π) && (Π < Π⁺)) {
            break;
        } else {
            (*řₚ) *= (2.0 - Π);
        }
    }
}

The function receives a pointer to a second value  with a
rough approximation for . It then iteratively approaches  within
a chosen precision : the current values  and  are multiplied
into a value  and if that value is sufficiently close to  the
iteration stops. If it is not,  is corrected and the loop
continues.What is interesting for our context of aliasing is that this function
has two pointer arguments that both point to a value of the same type
. One of these pointer targets  is loaded from memory,
modified and stored at each iteration. In total, the non-optimized
function as specified above in each iteration has3 load and 1 store operations,So loads and stores from memory make up 4 of about 10 operations in
total.But wait, can’t this be done better? Yes, obviously, a much better
version of this could look as follows.void recip⁺(double* aₚ, double* řₚ) {
    register double a = *aₚ;
    register double ř = *řₚ;
    for (;;) {
        register double Π = a*ř;
        if ((Π⁻ < Π) && (Π < Π⁺)) {
            break;
        } else {
            ř *= (2.0 - Π);
        }
    }
    *řₚ = ř;
}
That is, we load  and  once, at the beginning, and then only
update  when we have finished our computation. The hope here is
that these new local variables use “hardware registers” that can be
used directly by the processor, without going through loads and stores
from and to the program memory.So roughly the optimized function saves us 40% of the operations.
This means that the optimized function is in general much faster and
achieves its goal by wasting less energy.Unfortunately no C compiler can do this optimization automatically:The functions  and  and not equivalent.We didn’t see this, yet, because we failed to discuss an important
feature of the original program; the pointers  and  may either
pointIndeed, our optimized function  only covers the first of these
possibilities and not the second: the second case provides a
completely different algorithm for which I wouldn’t know any use or
properties. But since the compiler can’t know which of these cases we
have in mind (maybe both?) it cannot do an optimization that excludes
the second.In general, loads and stores from memory are expensive operations, so
a lot of effort in modern compiler frameworks goes into so-called
alias analysis, that is in an detailed analysis of which pointers
may alias each other (or not). Such alias analysis then can be used to
mechanically prove whether a specific optimization is feasible or not.Good alias analysis of pointer targets is crucial for
optimization in modern compilers.In the case of  we see that our specification is not precise
enough to know whether or not the second case (where the pointers
alias), can be excluded. We also see that misguided assumptions about
pointer aliasing can result in implementing a completely different
algorithm. Or, in other words, they may result in a severe bug.Mislead aliasing assumptions result in bugs.In one of our
papers,
we provide a long list of examples where pointer aliasing can lead to
different interpretations by users and compilers, see also this paper
here.Note also that a previous attempt by the C committee to introduce
stricter aliasing rules had turned into years of flame wars between
alienated parts of the user community, compiler builders and committee
members.  A communication disaster that was very harmful for the
industry in general and to the trust of the community in the C
committee in particular.Modern compilers have several mechanisms that help to deduce more
information about the intended use cases for pointers and from there
to provide correct optimizations. These mechanisms areType-based alias analysis.Flow-based alias analysis.For type-based alias analysis, C takes a relatively simple approach,
at least on the surface level.Two pointers that point to objects of different non-character type
are supposed not to alias.So, for our function  above, it would have been sufficient to
have arguments pointing to differently typed objects. Then, an
optimization similar to  would have been valid.… notice the “supposed” in the phrase above. In C, it is entirely
the programmers responsibility to ensure that two differently typed
objects are in fact different. If you are casting spells to convert
pointers from one target type to another, you have to prove for
yourself that you may pass such pointers as arguments to functions
without causing problems, there. You are completely on your own.Flow-based analysis in C is supported by several toolsThe  storage class.The  pointer qualifier.The  qualifier on the target type.The first ensures that the address of a so-declared object is never
taken. And in C, when there are no pointers, there is no
aliasing. Although this is easy enough to use and
provides feedback if one tries to obtain the address of such an
object, it seems that this is much less used than it should.The second is much more subtle. If a pointer parameter of a function
is declared with , every user of the function has to ensure
that the pointer value is the unique view on the object from within
for each call to the function. The definition in the standard of that
feature is obscure and, again, it has the same drawback as type-based
alias analysis in C: you are completely on your own.The third is a feature that imposes that each access to a 
qualified object reloads the object from memory, regardless what the
compiler might know about its previous value. Using this feature
excessively is like stepping on the break: basically all optimization
opportunities concerning that object are lost.As extensions, some compilers also have command line flags that help
the user to steer through. For example gcc has the options
 and  to switch aliasing
analysis as defined by the C standard on or off.And then, there is provenance …In the current C standard provenance is only a hidden concept, the
word “provenance” appears in it exactly 0 times. Nevertheless, it
marks an important assumption for existing C, namely that somehow
compilers are allowed to assume that two pointers don’t alias when
they are known to come from different “sources” in the program.For programmers, the current state is a disaster. In many cases there
is no way to know which assumptions a compiler makes:There is no way (but  and types, see above) to claim that
two pointers never alias.It is difficult (but for ) to specify that they might
alias such as to force optimization to be more restricted,
either. This even in situations where, for example, the base types
are in fact different, but where the user knows that the underlying
object might still be the same.The concept of provenance created many difficulties because it was left undefined exactly what it meant, so each compiler implementer had their own ideas how to assert that two pointers do (or don’t) alias, and how to use that in optimization.When digging deeper we observed several problem spots, and it took us
in fact a long time to understand the different assumptions.The first question that comes up is to figure out at which granularity
we can do or want to do aliasing analysis. At the level of bytes,
words, basic type objects, structure objects, memory allocations, or,
the whole address space?The main problem here is that in C we can move from one pointer
address to the next via arithmetic. The easiest examples are
arrays. If we have access to an element in the middle of an array, we
can move back and forth by simply adding or subtracting an integer
value to the corresponding pointer value. How would we (the
programmers) or the compiler know if two pointers point into the some
array but at different elements?  What happens if we start to step
backwards from such pointers?This picture becomes even more confusing, if we allow pointer
arithmetic on the byte level. In C, the  macro allows us to
access arbitrary members of a structure by using such pointer
arithmetic.Pointer equality and object life timeWhen you are programming with pointers in C, you are hopefully aware
of the “dangling pointer” problem. This refers to the fact that for
example a pointer to a local variable becomes invalid once you leave
the scope (e.g. function) where it is declared.For aliasing analysis, this becomes even more complicated: not only
does a pointer become invalid, the address of the dead object may be
reused in a different context. So two pointers may even be equal, one
pointing to a dead object, the other to a new one.There is another situation where two pointer values are the same, but
where they talk about two different objects. This occurs when two
arrays,  and , say, happen to live in adjacent memory
locations. If  has  elements, the pointer value  (the
end of ) might be the same as  (the start of ).  So then
we have two pointer values that are the same but that have quite a
different meaning for the programmer.Information flow for addressesAnother concept influences aliasing analysis a lot, namely the
possibility that a pointer value escapes from a limited scope of the
program (e.g the address of a local variable) and becomes known in
other parts of the program.For example a variable that is declared  in a block is usually
only visible there. In general the compiler masters very well which
pointers may alias with a pointer value that corresponds to the
address of that  variable. If the address of that variable
“escapes” from the local scope, for example by passing it as an
argument to a function, the address could come back into the same
scope via unknown paths that are difficult to control.There are surprisingly many ways that pointer values as a whole or
parts of them may escape from one context and reappear in
another. Among them aremanipulations of the byte-representations of pointers for example
manipulation of integers that are the result of pointer-to-integer
conversions.Perhaps surprisingly the latter is an important point that had to be
handled carefully in our proposal. The reason is that pointer
bit-manipulations are used in contexts where the available memory
relatively constrained, such as systems programming for
example. Bit-manipulation tricks are then used to save on the size of
data structures. This has not only the advantage that storage space
for the data can be reduced, but also that a reduced size also
improves the performance for data that are used intensively due to
improved use of processor caches.A famous example is the XOR trick for doubly linked lists, where a
data structure stores the XOR of the bit patterns of two pointer
values. So for that particular use we have an integer (here of type
) that contains information that comes from two different
pointers, that then is used to sometimes reconstruct one or the other
of these pointer values.typedef struct elem elem;
struct elem {
    uintptr_t both;
    double data;
};

void elem_store(elem* e, elem* prev, elem* next, double val) {
    (*e) = (elem){
        both = (uintptr_t)prev ^ (uintptr_t)next,
        data = val,
    };
}
Such a data structure can then be used for elements of a list that can
be traversed consistently forward and backward, but the memory
footprint is only that of a simply-linked list:elem* elem_next(elem const* e, elem* prev) {
    return (elem*)((uintptr_t)prev ^ e->both);
}

elem* elem_prev(elem const* e, elem* next) {
    return (elem*)((uintptr_t)next ^ e->both);
}
As you can see, besides their names these two functions are completely
identical, and I can’t imagine any compiler being able to track the
origin of a pointer that one of these two functions returns.Tracking provenance through integers?The fact that in C pointer values are closely related to integers
creates a lot of confusion. In the case of aliasing analysis, the lack
of separation between those terms has it that we have to integrate a
model of information flow of pointer values (or just some bits or
bytes of them) through integers and back to pointer values.When Peter and Kayvan started their investigations, they had to
consider different possibilities, one of them being to track
information about pointers through such a chain of conversions. It
turned out, that such a model would be possible (they provided a sound
specification for it) but that it would come at an important cost. For
code as for the XOR trick used above a pointer value (the result of a
call to  for example) would have two origins ( and
 in a call to ) and not only one. Since such
different origins could then accumulate if we do more operations,
basically an integer and a pointer derived from it, could have an
arbitrary number of origins.Such a model with multiple origins of pointers seemed complicated and
impractical. Complicated for users, because they would have to be
aware that information about pointers could be used in surprising ways
by a sophisticated compiler. Impractical for compilers because keeping
track of all possible in-flow of information would result in a
combinatorial explosion of the state of the abstract machine.Presented with such a complication, one possibility would have been to
just “forbid” using pointers in that way. We could have stated
something along the lines of “if a pointer has several origins, the
behavior is undefined” and thus leaving everything (the “undefined”
part) to compiler implementations. But because these situation appear
in real life code, this would have left these important parts
non-portable between different compilers and architectures.So the overall conclusion was not to ban such usage of pointers
through integers, but to formalize it and label such pointers as
, that is that no user has to fear that compilers will
present them with optimization that uses knowledge of such information
flow, and no compiler has the pressure to optimize such code, either.The provenance model that we came up with, and which is at the base of
TS 6010, tries to take all of these aspects into account with the
goal to provide something that at the same time can easily be referred
to by users and compiler implementers. It provides some compromise
between the expectations of the two communities in the sense that it
does not leave all the liberty they might dream of to compiler
providers for optimization, and it still has some sort of complexity
and difficulty for users.In the following we present the most important parts of that model.As said above, we have to agree upon the granularity of memory
accesses for which aliasing of pointers will be considered. When we
combed through the existing standard (C11 an C17 at the time) we
quickly noticed that there was not even agreement within that
standard. When it talks about what is found at the other end of a
pointer, it talks about “object”, “space”, “memory” or
“storage” and even some combinations of these.It seemed important to us to emphasize that pointers and addresses are
already an abstraction that does not necessarily denote a physical
device: most modern platforms nowadays form so-called virtual address
spaces. Such “virtual addresses” then are in general transformed by
low-level tools to “physical addresses” that represent concrete memory
hardware. To make that distinction clearer we decided to use the term
“storage” in most places where one of the terms noted above appear.Another important observation to have is that we even have to talk
about things that do not have an address. For example if we declare a
variable width the  storage class, we cannot receive a
pointer to that object and the whole point is that it is not necessary
for the compiler to realize this variable in the main memory.Then, the aspect of temporality also comes into play. A chunk of
memory that is obtained for example by a call to  can be
returned to the system by calling  and then might again be
served by another call to . It is important that the two
entities to which the pointer refers are seen as completely different
and that the fact that they reside in the same memory location is a
simple coincidence.For the granularity, we decided to go on the level of maximal region
in which “legally” a C program could operate. Since inside any
allocation or declared object all bytes are reachable by
character-pointer arithmetic, we decided to take this as the level of
granularity. ThereforeA  is the maximal region of storage that is
provided byan allocation ( and similar),an object with temporary lifetime.Note here that the second point talks about the definition of a
variable, not its declaration. For local variables these two coincide,
but for file scope variables (outside any function) there can be
declarations (with ) that are not definitions. The definition
of a variable is always unique and specifies  it is located,
namely in our terminology here, where the storage instance that
represents it comes to be.If a storage instance is addressable, the conversion of a pointer to
its start to the type  points into an array, called
its , where each element is one byte of the storage
instance.By that definition, conversions of pointers to character types and to
 are defined and it is uniquely prescribed how arithmetic on a
byte level works.A storage instance has a lifetime that expandsfrom the allocation (typically ) to the deallocation
(typically )from the definition of the variable to the point where the block of the
definition left (for a VLA)from the point of entering the block of the definition until it is
left (for other variables with automatic storage duration)from the point of entering the innermost block that contains the
expression until that scope is left (for compound literals with
automatic storage duration)from the start of the program execution until its end (for 
objects)from the start of the thread execution until its end (for
 objects)during the evaluation of the full expression that contains it (for
objects of temporary lifetime).So in addition to the “where” above, this definition describes 
the storage instance that represents an object comes to be and ceases
to exist.If you are not familiar with all the concepts in that item list, just
ignore these. The importance here really is to make it clear for the
features that you use in your program and know about, that in general
their lifetime is limited and that any such allocation or definition
gives rise to one single storage instance per context in which the
construct is met.For example in the following code we see three storage instances in
action{
    size_t n  = 32;
    double (*A)[n][n] = malloc(sizeof(*A));
    ...
    free(A);
    A = nullptr;
    ...
}
The one for , a variable of integer type, and for which the
lifetime starts when we enter the block at the  and that ends
when we leave it at .The storage instance that is allocated by the call to  and
that is deallocated by the call to .But then there is also a storage instance for the pointer  itself
that, similar to , lives during the execution of the block. In
particular, after we freed the contents of  we may still access it
to set it to a null pointer value.For a case where the notion of storage instance is perhaps a bit less
intuitive we note that calls to  are a bit peculiar with
respect to that definition. In a callvoid* p = realloc(q, 77);
we first have the storage instance to which  points. Then, if the
call is successful, that old storage instance is released and a
pointer to a new storage instance is stored in . Even if these two
pointers are identical (possibly the storage instances start a the
same address) they are nevertheless considered as two different
entities.With the term storage instance immediately comes the notion of
provenance.The  of a valid pointer value is the storage instance
into which (or one beyond which) the pointer value points.With the exception of one particular border case (see
below) the provenance of a valid pointer
value is unique.To be useful in an aliasing model, the concept of an address space is
not provided with enough precision in the current C standard. We need
to talk consistently about addresses, how pointers convert, compare or
relate.The model we came up with, has the following properties:To each object pointer value corresponds an abstract address that is
a positive integer value.Bytes within an addressable storage instance have abstract addresses
that increase from start to end.If the distinct storage instances  and  are alive at a common
point in time, the abstract addresses of all bytes of  are either
all smaller or all greater than the abstract addresses of all bytes
of .Two pointer values are equal if they correspond to the same abstract
address.One pointer value is smaller than another pointer value, if both
point into the same storage instance and if the address of the first
is smaller than the one of the second.If the platform is able to represent all addresses in some integer
type, the type  is provided and a conversion from a
pointer to that type provides the abstract address.Conversion from pointers to any integer type are consistent with
that mapping to abstract addresses.This model falls short from defining a “flat” address space:Arithmetic on pointers and arithmetic on abstract addresses need not
to be consistent.Even within the same storage instance, the increase from one byte to
next may not be one, and may not even be uniform.The type  may not exist.Conversion to integer types that are too narrow has undefined
behavior.The reasons for only having such a lax definition are simple, for each
of the weird properties in the list there are examples that make it
necessary.  In particular, there are platforms with segmented memories
that have “bumps” in the address space, and platforms that pack
additional bits into pointer values that are not related to the
corresponding abstract address.Exposure and synthesis of pointer valuesAnother observation is crucial for our model: most aliasing analysis
isn’t perfect. That is, compilers as well as programmers have limits
of which tracks of the pointer information they can follow. For
example the XOR trick shows that a pointer value can have several
origins. In all we have to foresee a mechanism that describes the
boundaries of the assumption that a compiler may make on one hand, and
the guarantees that a programmer has to give on the other.The mechanism with which we came up has two sidesA pointer value is  if information about its abstract
address or its in-memory representation leaks to the outside or to
distant parts of the program.A pointer value is  if it is assembled from outside
information, from byte information or from integer values.The goal is to describe that mechanism in a way such that (in
principle) some auxiliary information could be added to each pointer
value that would either allowcompilers and users to establish aliasing properties of a set of
pointer values, oreasily come to a consensus for situations for which such analysis is
abandoned an may not be assumed.Let’s now have a look at a possible normative text as it should be
integrated into the C standard at some pointA storage instance becomes  when a pointer value  of effective
type  with this provenance is used in the following contexts:Any byte of the object representation of  is used in an
expression.The byte array pointed-to by the first argument of a call to the
 library function intersects with an object representation of
. is converted to an integer. is used as an argument to a  conversion specifier of the
 family of library functions.The idea of the first bullet point is that if we read bytes of the
object representation of a pointer, cascaded  control flow
could be used to reconstruct pointers and thus jeopardize any aliasing
analysis.  But what we also didn’t wanted, is that “normal” operations
that programmers do on pointer representations as a whole would have
similar effect as access.Therefore notice that  or similar functions do not appear in
the list above; as long as we use it to copy pointer representations
as a whole, provenance can simply be transferred.  For example, using
 on structure objects that have pointer members is fine: such
an operation copies the whole pointer without exposing individual
bytes. So we simply assume that the provenance information is
transferred at the same time to the target structure.We also don’t want that small changes in the way that we look at a
pointer representation has an influence on aliasing
analysis. Therefore we add the following paragraphNevertheless, if the object representation of  is read through an
lvalue of a pointer type  that has the same representation and
alignment requirements as , that lvalue has the same provenance
as  and the provenance does not thereby become exposed.Here the term “same representation and alignment” covers for example
the possibility to look ata pointer that has different qualifiers than the original,where one type would be a signed type and the other unsigned, orone would be a structure and the other would be another structure
that sits at the beginning of the first.Also exposure is meant to be a one-way street, once exposed we will
never know where the information about the pointer could creep into
our program execution.Exposure of a storage instance is irreversible and constitutes a
side effect in the abstract state machine.The inverse operation that uses other information to produce a pointer
looks as follows:A pointer value  is synthesized if it is constructed by one of the
following:Any byte of the object representation of  is changed
by an explicit byte operation,by type punning with a non-pointer object or with a pointer object
that only partially overlaps,or by a call to  or similar function that does not write
the entire pointer representation or where the source object does
not have an effective pointer type.The object representation of  intersects with a byte array
pointed-to by the first argument of a call to the  library
function. is converted from an integer value. is used as an argument to a  conversion specifier of the
 family of library functions.Additionally, we always require that the storage instance that is
synthesized has been exposed before.With all of that the situation about adjacent storage
instances still remains. That
is, a situation where two arrays  and  are adjacent in
memory. Let’s suppose the two arrays have two elements, each, and that
the base type has four bytes:When we are taking addresses the following are valid expressions:
 which points to the element just after the array  and
 which points to the beginning of . So if  and  are
adjacent objects, the pointer value of the first expression is exactly
the same as for the second. So both represent different semantics but
with an abstract address that is the same, the byte address of byte
.So far, so good. Using these pointers with clearly indicated semantics
doesn’t pose a problem for aliasing analysis. In particular in our
model a pointer value as given above always has a unique provenance. A
problem arises only ifThe storage instance of  and  have both been exposed, by a
conversion to , say.A pointer  is synthesized that corresponds to the byte address of
 by converting some  value back.Then we are in the dilemma that  could have both provenances, that
of  or that of .Already from the length of the text that is needed to describe the
situation you might guess that this is a very rare situation, the
easiest remedy against its difficulties is just not to have it in the
first place. But in the case that it arises we have to foresee a
mechanism that is consistent with the model. Since there is generally
no additional information available that could guide the compiler to
see which semantics the programmer meant, the semantics are deduced
from the usage of the pointer value:A synthesized pointer value  with two possible provenances  or
 is assumed to have the one provenance among the two that is
consistent with its use in expressions.That is for example, if we use  in,  or  it is assumed that the provenance is the one
of  or  it is assumed that the provenance is the one of
.A use as in  that has the same value  gives no indication of a
direction in which we want to step through the array. So it does not
fix a choice of one provenance or the other. But  which
resolves to  has the provenance of  and similarly
 has the one of .So if and when you have to use that marginal constructs, make sure
that you give clear indications to the compiler into which of the two
storage instance you want to walk.If you were brave enough to follow this article up to here, you
probably deserve some reassuring words such that you are not left
alone with a complicated web of choices and interrelationships between
parts of your code that are impossible to master. In the contrary, our
model provides a very simple method to guarantee sound aliasing
analysis by compilers of your every day code:Do not expose pointer values …… if you can avoid it. And in most cases you can. In particular avoidtaking the address of variables (maybe use  to be sure),casts of pointer values to and from integers,accessing individual bytes of pointer targets (AKA coversion to
character pointers),conversions of pointer values to any other unrelated pointer type,accessing individual bytes of byte-representations of pointer values,printing pointer values with  or by using ,using the end address of an array to walk backwards into the array,Obviously, these features are important for C and contribute for a lot of
its power. But you should only use them when (and where) you master
them in all of their consequences. For example, if in the context of
system’s programming you need the XOR trick for your doubly-linked
list, that is fine as long as you are aware, that this might cost you
some other optimization opportunities. Or if you are debugging your
code, printing pointer values with  can be crucial, but you should
make sure that you disable all traces of such printing when you
compile for production.Generally, using more modern features of C will help your compiler to
provide you with more efficient and safer executables. For example, in
addition to the abovenot using pointer parameters on functions when you are only
interested in the value (our  function is a bad example),using  qualification wherever you may,not using integer zero as a null pointer,using signed and unsigned integer types consistently,all contribute to help your C compiler to do at what it’s best:
nagging you about things that you might have overlooked.]]></content:encoded></item><item><title>Want to meet people, try charging them for it?</title><link>https://notes.eatonphil.com/2025-06-28-want-to-meet-people-charge-them.html</link><author>ArneVogel</author><category>hn</category><pubDate>Mon, 30 Jun 2025 06:13:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I have been blogging consistently since 2017. And one of my goals in
speaking publicly was always to connect with like-minded people. I
always left my email and hoped people would get in touch. Even while
my blog and twitter became popular, passing 1M views and 20k
followers, I basically never had people get in touch to chat or meet up.So it felt kind of ridiculous when last November I started charging
people $100 to chat. I mean, who am
I? But people started showing up fairly immediately. Now granted the
money did not go to me. It went to an education non-profit and I
merely received the receipt.And at this point I've met a number of interesting people, from VCs to
business professors to undergraduate students to founders and everyone
in between. People wanting to talk about trends in databases, about
how to succeed as a programmer, about marketing for developers, and so
on. Women and men thoughout North America, Europe, Africa, New
Zealand, India, Nepal, and so on. And I've raised nearly $6000 for
educational non-profits.How is it that you go from giving away your time for free and getting
no hits to charging and almost immediately getting results? For one,
every person responded very positively to it being a fundraiser. It
also helps me be entirely shameless about sharing on social media
every single time someone donates; because it's such a positive thing.But also I think that in "charging" for my time it helps people feel
more comfortable about actually taking my time, especially when we
have never met. It gives you a reasonable excuse to take time from
an internet rando.On the other hand, a lot of people come for advice and I think giving
advice is pretty dangerous, especially since my background is not
super conventional. I try to always frame things as just sharing my
opinion and my perspective and that they should talk with many others
and not take my suggestions without consideration.And there's also the problem that by charging everyone for my time
now, I'm no longer available to people who could maybe use it the
most. I do mention on my page that I will still take calls from people
who don't donate, as my schedule allows. But to be honest I feel less
incentivized to spend time when people do not donate. So I guess this
is an issue with the program.But I mitigated even this slightly, and significantly jump-started the
program, during my 30th birthday when
I took calls with any person who donated at least $30.Anyway, I picked this path because I have wanted to get involved with
helping students figure out their lives and careers. But without a
degree I am literally unqualified for many volunteering programs. And
I always found the time commitments for non-profits painful.So until starting this I figured it wouldn't be until I retire that I
find some way to make a difference. But ultimately I kept meeting
people who were starting their own non-profits now or donated
significantly to help students. Peer pressure. I wanted to do my part
now. And 30 minutes of my time in return for a donation receipt has
been an easy trade.While only raising a humble $6,000 to date, the Chat for
Education program has been more
successful than I imagined. I've met many amazing people through
it. And it's something that should be easy to keep up indefinitely.I hope to meet you through it too!]]></content:encoded></item><item><title>LetsEncrypt – Expiration Notification Service Has Ended</title><link>https://letsencrypt.org/2025/06/26/expiration-notification-service-has-ended/</link><author>zdw</author><category>hn</category><pubDate>Mon, 30 Jun 2025 04:49:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Since its inception, Let’s Encrypt has been sending expiration notification emails to subscribers that have provided an email address to us via the ACME API. This service ended on June 4, 2025. The decision to end the service is the result of the following factors:Over the past 10 years more and more of our subscribers have been able to put reliable automation into place for certificate renewal.Providing expiration notification emails means that we have to retain millions of email addresses connected to issuance records. As an organization that values privacy, removing this requirement is important to us.Providing expiration notifications costs Let’s Encrypt tens of thousands of dollars per year, money that we believe can be better spent on other aspects of our infrastructure.Providing expiration notifications adds complexity to our infrastructure, which takes time and attention to manage and increases the likelihood of mistakes being made. Over the long term, particularly as we add support for new service components, we need to manage overall complexity by phasing out system components that can no longer be justified.For those who would like to continue receiving expiration notifications, we recommend using a third party service such as Red Sift Certificates Lite (formerly Hardenize). Red Sift’s monitoring service providing expiration emails is free of charge for up to 250 certificates. More monitoring options can be found here.We have deleted the email addresses provided to Let’s Encrypt via the ACME API that were stored in our CA database in association with issuance data. This doesn’t affect addresses signed up to mailing lists and other systems. They are managed in a separate ISRG system unassociated with issuance data.Going forward, if an email address is provided to Let’s Encrypt via the ACME API, Let’s Encrypt will not store the address but will instead forward it to the general ISRG mailing list system unassociated with any account data. If the email address has not been seen before, that system may send an onboarding email with information about how to subscribe to various sources of updates.If you’d like to stay informed about technical updates and other news about Let’s Encrypt and our parent nonprofit, ISRG, based on the preferences you choose, you can sign up for our email lists below:]]></content:encoded></item><item><title>Bought myself an Ampere Altra system</title><link>https://marcin.juszkiewicz.com.pl/2025/06/27/bought-myself-an-ampere-altra-system/</link><author>pabs3</author><category>hn</category><pubDate>Mon, 30 Jun 2025 04:38:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In the hunt for a development machine, I got to the next phase. I did some
shopping, and there it is: my own Ampere Altra-based system.As you may have read in my previous post,
I used several AArch64 systems for local development. And the latest one, an
Apple MacBook Pro, is nice and fast but has some limits — does not support 64k
page size. Which I need for my work.So I have decided to buy myself an Ampere Altra system. As cheap as possible.The only part I needed to buy brand new was a motherboard. And the only
“affordable” one was AsrockRack -,
which was a product for data centres (so I was told).Next, a used processor. At first, the idea was to buy a Q64-22 (64 cores,
2.2 GHz clock), but when the seller on eBay was not responding, one of my friends
decided to upgrade his Altra systems and offered me a Q80-30 (80 cores, 3.0 GHz clock).The  requires cooling. There are not many options for the  4926 socket. I
found an Arctic Freezer 4U-M in one of the online stores here in Poland and
bought the only one they had.Working fine (after re-seating three sticks):DRAM populated DIMMs:
  SK0 MC0 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC1 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC2 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC3 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC4 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC5 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC6 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
  SK0 MC7 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN
The rest of the parts are ordinary ones available in any random store.The case was a challenge. The - is a “deep MicroATX” case which
means I needed a case that can take an  motherboard (they are a bit deeper
than ).I looked through pictures in online stores and selected about 10 candidates. Then
I started watching reviews and crossed several of them out. Usually because the
holes for handling cables were too close to the edge of a board.Finally, bought an Endorfy 700 Air case. It came with five 120mm fans (3 at the
front, one at the rear, and one on the top). There was a lot of space behind
the motherboard’s plate for cabling, and a fan splitter so I could connect all
five case fans as one to the motherboard.And the  compartment has an extra hole for PCIe power cables!The - motherboard is expected to be powered by a 12V  only.
There is no connection port for the  24-pin plug. Instead, there is an
adapter that takes only power-on and power-good signals from it and connects to
the small 4-pin port on the motherboard.There are three  connectors available. I used two of them. The  power supply I bought comes with two such cables and has an option for
connecting the third one.What else is needed? Some  for storage (Lexar ) and a random
graphics card (Radeon Pro , remembering the old times).And I was ready to build the system.The motherboard feels small once the  cooler is mounted. And a low-profile
graphics card is almost invisible:The back side of the case shows where I hid the  24-pin adapter and most of
the cables.Of course, if I went cheap, then how cheap did it end up being?In total: 7 732  (around 1 800 ). About 500  more than I anticipated at first.The  upgrade to 80 cores was extra 100 . Memory came from another seller,
as the first one ended their sale before I was ready to buy, resulting in an
extra 40  to the cost.At first, I wanted to have  per core, but with the  upgrade, I would have
needed to go to 256 , and that would have been another 250-300  extra.The case had a 30% discount due to a promotion, and the  came with 7% cashback.My current plans for this system are:create some  instances:put a Radeon  inside and check whether it would worktry to use it as a desktopdo some crazy experimentsAll Linux systems will have both 4k and 64k page size kernels.]]></content:encoded></item><item><title>Gridfinity: The modular, open-source grid storage system</title><link>https://gridfinity.xyz/</link><author>nateb2022</author><category>hn</category><pubDate>Mon, 30 Jun 2025 03:37:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Get started with online generators!The best things in life are integer multiples of 42x42x7mm.Gridfinity could be your workshop's ultimate modular storage system to keep you productive, organized, and safe. It is free, open source, and almost 100% 3D printable.Now Gridfinity is in the hands of a thriving community that continually adapts it to their needs. We invite you to join this community by using and adapting the system!This site is a work in progress! Come join us on #gridfinity in Zack's discord to help!
]]></content:encoded></item><item><title>Use keyword-only arguments in Python dataclasses</title><link>https://chipx86.blog/2025/06/29/tip-use-keyword-only-arguments-in-python-dataclasses/</link><author>Bogdanp</author><category>hn</category><pubDate>Mon, 30 Jun 2025 00:45:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Python dataclasses are a really nice feature for constructing classes that primarily hold or work with data. They can be a good alternative to using dictionaries, since they allow you to add methods, dynamic properties, and subclasses. They can also be a good alternative to building your own class by hand, since they don’t need a custom  that reassigns attributes and provide methods like  out of the box.One small tip to keeping dataclasses maintainable is to always construct them with , like so:from dataclasses import dataclass


@dataclass(kw_only=True)
class MyDataClass:
    x: int
    y: str
    z: bool = True
This will construct an  that looks like this:class MyDataClass:
    def __init__(
        self,
        *,
        x: int,
        y: str,
        z: bool = True,
    ) -> None:
        self.x = x
        self.y = y
        self.z = z
class MyDataClass:
    def __init__(
        self,
        x: int,
        y: str,
        z: bool = True,
    ) -> None:
        self.x = x
        self.y = y
        self.z = z
That  in the argument list means everything that follows must be passed as a keyword argument, instead of a positional argument.There are two reasons you probably want to do this:It allows you to reorder the fields on the dataclass without breaking callers. Positional arguments means a caller can use MyDataClass(1, 'foo', False), and if you remove/reorder any of these arguments, you’ll break those callers unexpectedly. By forcing callers to use MyDataClass(x=1, y='foo', z=False), you remove this risk.It allows subclasses to add required fields. Normally, any field with a default value (like  above) will force any fields following it to also have a default. And that includes  fields defined by subclasses. Using  gives subclasses the flexibility to decide for themselves which fields must be provided by the caller and which have a default.These reasons are more important for library authors than anything. We spend a lot of time trying to ensure backwards-compatibility and forwards-extensibility in Review Board, so this is an important topic for us. And if you’re developing something reusable with dataclasses, it might be for you, too. One important point I left out is Python compatibility. This flag was introduced in Python 3.10, so if you’re supporting older versions, you won’t be able to use this just yet. If you want to optimistically enable this just on 3.10+, one approach would be:import sys
from dataclasses import dataclass


if sys.version_info[:2] >= (3, 10):
    dataclass_kwargs = {
        'kw_only': True,
    }
else:
    dataclass_kwargs = {}

...

@dataclass(**dataclass_kwargs)
class MyDataClass:
    ...
...
But this won’t solve the subclassing issue, so you’d still need to ensure any subclasses use default arguments if you want to support versions prior to 3.10.]]></content:encoded></item><item><title>Nearly 20% of cancer drugs defective in four African nations</title><link>https://www.dw.com/en/nearly-20-of-cancer-drugs-defective-in-4-african-nations/a-73062221</link><author>woldemariam</author><category>hn</category><pubDate>Sun, 29 Jun 2025 23:23:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[An alarming number of people across Africa may be taking cancer drugs that don't contain the vital ingredients needed to contain or reduce their disease.It's a concerning finding with roots in a complex problem: how to regulate a range of therapeutics across the continent.A US and pan-African research group published the findings this week in . The researchers had collected dosage information, sometimes covertly, from a dozen hospitals and 25 pharmacies across Ethiopia, Kenya, Malawi and Cameroon.They tested nearly 200 unique products across several brands. Around 17% — roughly one in six — were found to have incorrect active ingredient levels, including products used in major hospitals.Patients who receive insufficient dosages of these ingredients could see their tumors keep growing, and possibly even spread.Similar numbers of substandard antibiotics, antimalarial and tuberculosis drugs have been reported in the past, but this is the first time that such a study has found high levels of falsified or defective anticancer drugs in circulation."I was not surprised by these results," said Lutz Heide, a pharmacist at the University of Tübingen in Germany who has previously worked for the Somali Health Ministry and has spent the past decade researching substandard and falsified medicines.Heide was not part of the investigative group, but said the report shed light on a problem not previously measured."I was delighted that, finally, someone published such a systemic report," he said. "That is a first, really significant systematic study of this area."Causes need addressing, but it's not straightforward"There are many possible causes for bad-quality products," Marya Lieberman of the University of Notre Dame in the US, the investigation's senior researcher, told DW.Those causes can include faults in the manufacturing process or product decay due to poor storage conditions. But some drugs are also counterfeit, and that increases the risk of discrepancies between what's on the product label and the actual medicine within.Fake drug pandemic in AfricaSpotting substandard and falsified products can be difficult. Usually, a medical professional or patient is only able to perform a visual inspection — literally checking a label for discrepancies or pills and syringes for color differences — to spot falsified products.But that's not a reliable method. In the study, barely a quarter of the substandard products were identified through visual inspection. Laboratory testing identified the rest.Fixing the problem, Lieberman said, will require improving regulation and providing screening technologies and training where they're needed."If you can't test it, you can't regulate it," she said. "The cancer medications are difficult to handle and analyze because they're very toxic, and so many labs don't want to do that. And that's a core problem for the sub-Saharan countries where we worked. Even though several of those countries have quite good labs, they don't have the facilities that are needed for safe handling of the chemo drugs established."Not only cancer treatments are affectedNearly a decade ago, the World Health Organization found around one in 10 medicines used in low and middle-income countries were substandard or falsified. Independent research conducted since has backed those figures up, sometimes finding rates that are potentially twice as high."This could lead to treatment failure, adverse reactions, disease progression," health economist Sachiko Ozawa told DW. Ozawa contributed to the investigation on anticancer drugs and has separately researched other cases of defective medicines.   "For the community, there's also economic losses in terms of wasted resources,” she said. “So countries may be spending a lot of money on medications that are not going to be effective."While high-income countries can monitor supply chains and have stringent regulatory systems in place to identify and withdraw suspect products, the infrastructure to do that is far from common in other regions.In those places, poor access to affordable medication often drives patients to less-regulated marketplaces. Inadequate governance and regulation, as well as a scarcity of surveillance and diagnostic equipment to test pharmaceuticals, are all contributing to the problem in Africa."In high-income countries, I think there's a much more secure supply chain where you know the manufacturers are vetted, it has to go through very stringent regulatory processes to get approval...it gets tested more frequently," said Ozawa.The WHO told DW that following the report's findings, it was working with the four affected countries to address the problem."We are concerned with the findings the article has highlighted. WHO is in contact with national authorities of four impacted countries and obtaining relevant data," it said in a statement. "We expect to assess full information to evaluate the situation, which often takes time and capacity. But we're committed to address these issues working with the relevant countries and partners."The WHO also reiterated its ongoing call for countries to improve their regulatory frameworks to "prevent incidents of substandard and falsified medicines, including in settings of cancer programs."South African men don heels to spotlight male breast cancerPrevention, detection and responseIn 2017, the WHO's review of substandard and falsified medicines offered three solutions based around prevention, detection and response.Stopping the manufacture and sale of those medicines is the primary preventative measure, but where defective products make it to market, surveillance and response programs can prevent poor quality medicines from reaching patients.But regulatory reform sought by experts and authorities takes time. More immediate solutions are being developed in the form of better screening technologies.Lieberman is working on a "paper lab" — a type of test that can be used by trained professionals to chemically test the quality of a product before it's administered to a patient. Other laboratory technologies are also under development.One comforting point is that while a significant proportion of the medication circulating in medical facilities in the four African countries was defective, the majority of the products tested met required standards."[With] two-thirds of the suppliers, all the products [were] good quality, so there are good quality suppliers," said Heide. "But a few of them really have a suspiciously high number of failing samples."Edited by: Derrick Williams]]></content:encoded></item><item><title>Finding a former Australian prime minister’s passport number on Instagram (2020)</title><link>https://mango.pdf.zone/finding-former-australian-prime-minister-tony-abbotts-passport-number-on-instagram/</link><author>guiambros</author><category>hn</category><pubDate>Sun, 29 Jun 2025 22:22:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[So you know when you’re flopping about at home, minding your own business, drinking from your water bottle in a way that does not possess  intent to subvert the Commonwealth of Australia?It’s a feeling I know all too well, and in which I was vigorously partaking when I got this message in “the group chat”.The man in question is Tony Abbott, one of Australia’s  former Prime Ministers.For security reasons, we try to change our Prime Minister every six months, and to never use the same Prime Minister on multiple websites.This particular former PM had just posted a picture of his boarding pass on Instagram (Instagram, in case you don’t know it, is an app you can open up on your phone any time to look at ads).My friend (who we will refer to by their group chat name, 𝖍𝖔𝖌𝖌𝖊 𝖒𝖔𝖆𝖉𝖊) is asking whether I can “hack this man” not because I am the kind of person who regularly commits 𝒄𝒚𝒃𝒆𝒓 𝒕𝒓𝒆𝒂𝒔𝒐𝒏 on a whim, but because we’d recently been talking about boarding passes.I’d said that people post pictures of their boarding passes all the time, not knowing that it can sometimes be used to get their passport number and stuff. They just post it being like “omg going on holidayyyy 😍😍😍”, unaware that they’re posting cringe.Meanwhile, some hacker is rubbing their hands together, being all “yumyum identity fraud 👀” in their dark web Discord, because this happens a .So there I was, making intense and meaningful eye contact with this chat bubble, asking me if I could “hack this man”.Of course, my friend wasn’t  asking me to hack the former Prime Minister.I mean… what are you gonna do,  click it? Are you gonna let a  that’s like 50% advertising tracking ID tell you what to do? Wouldn’t you be ?The former Prime Minister had just posted his boarding pass. Was that ? Was someone in danger? I didn’t know.What I did know was: the  I could do for my country would be to have a casual browse 👀Investigating the boarding pass photoSo I had a bit of a casual browse, and got the picture of the boarding pass, and then…. I didn’t know what was supposed to happen after that.Well, I’d heard that it’s bad to post your boarding pass online, because if you do, a bored 17 year-old Russian boy called “Katie-senpai” might somehow use it to commit identity fraud. But I don’t know anyone like that, so I just clumsily googled some stuff.Googling how 2 hakc boarding passEventually I found a blog post explaining that yes, pictures of boarding passes can indeed be used for Crimes. The part you wanna be looking at for all your criming needs is the barcode, because it’s got the “Booking Reference” (e.g. ) in it.Why do you want the booking reference? It’s one of the two things you need to log in to the airline website to manage your flight.The second one is your… last name. I was really hoping the second one would be like a password or something. But, no, it’s the booking reference the airline emails you and prints on your boarding pass. And it also lets you log in to the airline website?That sounds suspiciously like a password to me, but like I’m still fine to pretend it’s not if you are.I’ve been practicing every morning at sunrise, but still can’t scan barcodes with my eyes. I had to settle for a barcode scanner app on my phone, but when I tried to scan the picture in the Instagram post, it didn’t work :((Step 2: Scan the barcode, but moreWell, maybe it wasn’t scanning because the picture was too blurry.I spent around 15 minutes in an “enhance, ENHANCE” montage, fiddling around with the image, increasing the contrast, and so on. Despite the montage taking up way too much of the 22 minute episode, I couldn’t even get the barcode to scan.Step 2: Notice that the Booking Reference is printed right there on the paperAfter staring at this image for 15 minutes, I noticed the Booking Reference is just… printed on the baggage receipt.But it did not prepare me for this.Step 3: Visit the airline’s websiteAfter recovering from  emotional rollercoaster, I went to qantas.com.au, and clicked “Manage Booking”. In case you don’t know it because you live in a country with fast internet, Qantas is the main airline here in Australia.(I also very conveniently started recording my screen, which is gonna pay off  in just a moment.)Step 4: Type in the Booking ReferenceWell, the login form was just… , and it was asking for a Booking Reference and a last name. I had just flawlessly read the Booking Reference from the boarding pass picture, and, well… I knew the last name.I did hesitate for a split-second, but… no, I had to know.Can I get a YIKES in the chatLeave a comment if you really felt that.I guess I was now logged the heck in as Tony Abbott? And for all I know, everyone else who saw his Instagram post was right there with me. It’s kinda wholesome, to imagine us all there together. But also probably suboptimal in a governmental sense.Was there anything secret in here?I then just incredibly browsed the page, browsed it so hard.I saw Tony Abbott’s name, flight times, and Frequent Flyer number, but not really anything  secret-looking. Not gonna be committing any cyber treason with a Frequent Flyer number. The flight was in the past, so I couldn’t change anything, either.The page said the flight had been booked by a travel agent, so I guessed some information would be missing because of that.I clicked around and scrolled a considerable length, but still didn’t find any government secrets.Some people might give up here. But I, the Icarus of computers, was simply too dumb to know when to stop.We’re not done just because a  says we’re doneI wanted to see if there were juicy things hidden  the page. To do it, I had to use the  hacker tool I know.Listen. This is the only part of the story that might be confused for highly elite computer skill. It’s not, though. Maybe later someone will show you this same thing to try and flex, acting like only  know how to do it. You will not go gently into that good night. You will refuse to acknowledge their flex, killing them instantly.How does “Inspect Element” work?“Inspect Element”, as it’s called, is a feature of Google Chrome that lets you see the computer’s internal representation (HTML) of the page you’re looking at. Kinda like opening up a clock and looking at the cool cog party inside.Everything you see when you use “Inspect Element” was already downloaded to your computer, you just hadn’t asked Chrome to show it to you yet. Just like how the cogs were already in the watch, you just hadn’t opened it up to look.But let us dispense with frivolous cog talk. Cheap tricks such as “Inspect Element” are used by programmers to try and understand how the website works. This is ultimately futile: Nobody can understand how websites work. Unfortunately, it kinda  like hacking the first time you see it.If you’d like to know more about it, I’ve prepared a short video.Browsing the “Manage Booking” page’s HTMLI scrolled around the page’s HTML, not really knowing what it meant, furiously trying to find anything that looked out of place or secret.I eventually realised that manually reading HTML with my eyes was not an efficient way of defending my country, and Ctrl + F’d the HTML for “passport”.At this point I was fairly sure I was looking at the  secret government-issued ID of the 28th Prime Minister of the Commonwealth of Australia, servant to her Majesty Queen Elizabeth II and I was  worried that I was somehow doing something wrong, but like, not enough to stop.….anything  in this page?Well damn, if Tony Abbott’s passport number is in this treasure trove of computer spaghetti, maybe there’s wayyyyy more. Perhaps this HTML contains the lost launch codes to the Sydney Opera House, or Harold Holt.Maybe there’s a phone number?Searching for  and  didn’t get anywhere, so I searched for , the first 3 digits of an Australian phone number, using my colossal and highly celestial galaxy brain.A weird pile of what I could only describe as extremely uppercase letters came up. It looked like this:RQST QF HK1 HNDSYD/03EN|FQTV QF HK1|CTCM QF HK1 614[phone number]|CKIN QF HN1 DO NOT SEAT ROW [row number] PLS SEAT LAST ROW OF [row letter] WINDOW
So, there’s a lot going on here. There is indeed a phone number in here. But what the heck is all this  stuff?I realised this was like… Qantas staff talking to eachother  Tony Abbott, but not  him?In what is surely the subtweeting of the century, it has a section saying HITOMI CALLED RQSTING FASTTRACK FOR MR. ABBOTT. Hitomi must be requesting a “fasttrack” (I thought that was only a thing in movies???) from another Qantas employee.This is messed up for many reasonsWhat is even going on here? Why do Qantas flight staff talk to eachother via this passenger information field? Why do they send these messages, and your passport number  you when you log in to their website? I’ll never know because I suddenly got distracted withI realised the allcaps muesli I saw must be some airline code for something. Furious and intense googling led me to several ancient forbidden PDFs that explained some of the codes.Apparently, they’re called “SSR codes” (Special Service Request). There are codes for things like “Vegetarian lacto-ovo meal” (), “Vegetarian oriental meal” (), and even “Vegetarian vegan meal” (). Because I was curious about these codes, here’s some for you to be curious about too (tag urself, I’m ):RFTV    Reason for Travel
UMNR    Unaccompanied minor
PDCO    Carbon Offset (chargeable)
WEAP    Weapon
DEPA    Deportee—accompanied by an escort
ESAN    Passenger with Emotional Support Animal in Cabin
The phone number I found looked like this: CTCM QF HK1 [phone number]. Googling “SSR CTCM” led me to the developer guide for some kind of airline association, which I assume I am basically a member of now.Is the phone number actually his?I thought maybe the phone number belonged to the travel agency, but I checked and it has to be the passenger’s real phone number. That would be, if my calculations are correct,,,, *steeples fingers* Tony Abbott’s phone number.I’d now found Tony Abbott’s:Weird Qantas staff comments.My friend who messaged me had .Tony Abbott’s passport is probably a Diplomatic passport, which is used to “represent the Australian Government overseas in an official capacity”.By this point I’d had enough defending my country, and had recently noticed some new thoughts in my brain, which were:i gotta get someone, somehow, to reset tony abbott’s passport numbercan you even reset passport numbersis it possible that i’ve done a crimeIn this act, I, your well-meaning but ultimately incompetent protagonist, attempt to do the following things:⬜ figure out whether i have done a crime⬜ notify someone (tony abbott?) that this happened⬜ get permission to publish this here blog post⬜ tell qantas about the security issue so they can fix itSpoilers: This takes almost six months.Let’s skip the boring bitsI contacted a  of people about this. If my calculations are correct, I called at least 30 phone numbers, to say nothing of The Emails. If you laid all the people I contacted end to end along the equator, they would die, and you would be arrested. Eventually I started keeping track of who I talked to in a note I now refer to as “the hashtag struggle”.I’m gonna skip a considerable volume of tedious and ultimately unsatisfying telephony, because it’s been a long day of scrolling already, and you need to save your strength.Alright strap yourself in and enjoy as I am drop-kicked through the goal posts of life.Part 1: is it possible that i’ve done a crimeI didn’t  anything I did sounded like a crime, but I knew that sometimes when the other person is rich or famous, things can suddenly  crimes. Like, was there going to be some Monarch Law or something? Was Queen Elizabeth II gonna be mad about this?My usual defence against being arrested for hacking is making sure the person being hacked is okay with it. You heard me, it’s the power of ✨consent✨. But this time I could uh only get it in retrospect, which is a bit yikes.So I was wondering like… was logging in with someone else’s booking reference a crime? Was  someone else’s passport number a crime? What if they were, say, the former Prime Minister? Would I get in trouble for publishing a blog post about it? I mean you’re reading the blog post right now so obviouslUpdate: I have been arrested.Just straight up Reading The LawIt turned out I could just google these things, and before I knew it I was reading “the legislation”. It’s the rules of the law, just written down.Look, reading pages of HTML? No worries. Especially if it’s to defend my country. But whoever wrote the legislation was just making up words.Eventually, I was able to divine the following wisdoms from the Times New Roman tea leaves:Defamation is where you get in trouble for publishing something that makes someone look bad.But, it’s fine for me to blog about it, since it’s not defamation if you can prove it’s Having Tony Abbott’s passport number isn’t a crimeBut using it to commit identity fraud would beThere are laws about what it’s okay to do on a computerThe things it’s okay to do are: If u EVER even LOOK at a computer the wrong way, the FBI will instantly slam dunk you in a legal fashion dependent on the legislation in your areaI am possibly the furthest thing you can be from a lawyer. So, I’m sure I don’t need to tell you not to take this as legal advice. But, if you  the kind of person who takes legal advice from mango blog posts, who am I to stand in your way? Not a lawyer, that’s who. Don’t do it.You know what, maybe I needed help. From an adult. Someone whose 3-year old kid has been buying iPad apps for months because their parents can’t figure out how to turn it off.“Yeah, maybe I should get some of that free government legal advice”, I thought to myself, legally. That seemed like a pretty common thing, so I thought it should be easy to do. I took a big sip of water and googled “free legal advice”.trying to ask a lawyer if i gone and done a crimeBefore I went and told everyone about my HTML frolicking, I spent a week calling legal aid numbers, lawyers, and otherwise trying to figure out if I’d done a crime.During this time, I didn’t tell  what I’d done. I asked if any laws would be broken if “someone” had “logged into a website with someone’s publicly-posted password and found the personal information of a former politician”. Do you see how that’s not even a lie? I’m starting to see how lawyers do it.First I call the state government’s Legal Aid number.
They tell me they don’t  here, and I should call another Legal Aid place named something slightly different.The second place tells me they don’t  either, and I should call the First Place and “hopefully you get someone more senior”.I call the First Place again, and they say “oh you’ve been given the run around!”. You see where this is going.Let’s skip a lot of phone calls. Take my hand as I whisk you towards the slightly-more-recent past. Based on advice I got from two independent lawyers that was definitely not legal advice: I haven’t done a crime.Helllllll yeah. But I mean it’s a little late because I forgot to mention that by this point I had already emailed explicit details of my activities to the Australian Government.☑️ figure out whether i have done a crime⬜ notify someone (tony abbott?) that this happened⬜ get permission to publish this here blog post⬜ tell qantas about the security issue so they can fix itPart 2: trying to report the problem to someone, anyone, pleaseI had Tony Abbott’s passport number, phone number, and weird Qantas messages about him. I was the only one who  I had these.Anyone who saw that Instagram post could also have them. I felt like I had to like,  someone about this. Someone with like, responsibilities. Someone with an email signature.wait but do u see the irony in this, u have his phone number right there so u could just-Yes I see it thank u for pointing this out, wise, astute, and ultimately self-imposed heading. I  I could just call the number any time and hear a “G’day” I’d  be able to forget. I knew I had a rare opportunity to call someone and have them ask “how did you get this number!?”.But you can’t just  that.You can’t just call someone’s phone number that you got by rummaging around in the HTML ball pit. Tony Abbott didn’t  me to have his phone number, because he didn’t give it to me. Maybe if it was urgent, or I had no other option, sure. But I was pretty sure I should do this the Nice way, and show that I come in peace.I wanted to show that I come in peace because there’s also this pretty yikes thing that happens where you email someone being all like “henlo ur website let me log in with username  and password , maybe u wanna change that??? could just be me but let me kno what u think xoxo alex” and then they reply being like “oh so you’re a HACKER and a CRIMINAL and you’ve HACKED ME AND MY FAMILY TOO and this is a RANSOM and ur from the DARK WEB i know what that is i’ve seen several episodes of mr robot WELL watch out kiddO bc me and my lawyers are bulk-installing tens of thousands of copies of McAfee® Gamer Security as we speak, so i’d like 2 see u try”I googled “tony abbott contact”, but there’s only his official website. There’s no phone number on it, only a “contact me” form.Yeah right, have you  the incredible volume of #content people want to say at politicians? No way anyone’s reading that form.I later decided to try anyway, using the same Inspect Element ritual from earlier. Looking at the network requests the page makes, I divined that the “Contact me” form just straight up does not work. When you click “submit”, you get an error, and nothing gets sent.Well rip I guess. I eventually realised the people to talk to were probably the government.In the beginning, humans developed the concept of language by banging rocks together and saying “oof, oog, and so on”. Then something went horribly wrong, and now people unironically begin every sentence with “in regards to”. Our story begins here.The government has like fifty thousand million different departments, and they all know which acronyms to call each other, but you don’t. If you EVER call it DMP&C instead of DPM&C you are gonna be express email forwarded into a nightmare realm the likes of which cannot be expressed in  number of spreadsheet cells, in spite of all the good people they’ve lost trying.I didn’t even know where to begin with this. Desperately, I called Tony Abbott’s former political party, who were all likeSkip skip skip a few more calls like this.Maybe I knew someone who knew someone right, the true government channels were the friends we made along the way.I asked hacker friends who seemed like they might know government security people. “Where do I report a security issue with like…. a person, not a website?”They told me to call… 1300 CYBER1?I don’t really have a good explanation for this so I’m just gonna post the screenshots.You  I smashed that call button on . Did they just make it  then realise you need one more digit for a phone number? Incredible.Calling “Yes yes hello, ring ring, is this 1300 cyber one”? They  to say yes if you ask that. They’re legally obligated.The person who picked up gave me an email address for ASD (the Australian flavour of America’s NSA), and told me to email them the details.Emailing the government my crimesFeeling like the digital equivalent of three kids in a trenchcoat, I broke out my best Government Email dialect and emailed ASD, asking for them to call me if they were the right place to tell about this.Fooled by my flawless disguise, they replied  (in a relative sense) asking for more details.I  could provide them with more information, so I did, because I love to cooperate with the Australian government.I also asked whether they could give me permission to publish this blog post, and they were all like “Seen 2:35pm”. Eventually, after another big day of getting left on read by the government, they replied, being all like “thanks kiddO, we’re doing like, an  and stuff, so we’ll take it from here”.Overall, ASD were really nice to me about it and happy that I’d helped. They encouraged me to report this kind of thing to them if it happened again, but I’m not really in the business of uhhhhhhhh whatever the heck this is.By the way, at this point in the story (chronologically) I had  idea if what I was emailing the government was actually the confession to a crime, since I hadn’t talked to a lawyer yet. This is widely regarded as a bad move. I do not recommend anyone else use “but I’m being so helpful and earnest!!!” as a legal defence. But also I’m not a lawyer, so idk, maybe it works?Wholesomely emailing the governmentAt one point in what was surely an unforgettable email chain, the person I was emailing added a P.S. containing…. the answer to the puzzle hidden on this website. The one you’re reading this blog on right now. Hello.
I guess they must have found this website (hi asd) by stalking the email address I was sending from. This is unprecedented and everything, but:The puzzle says to tweet the answer at me, not email meThe prize for doing the puzzle is me tweeting this gif of a shakas to youSo I guess I emailed the shakas gif to the government??? Yeah, I guess I did.I asked them if they could give me permission to write this blog post, or who to ask, and they were like “uhhhhhhhhhhh” and gave me two government media email addresses to try. Listen I don’t wanna be an “ummm they didn’t reply to my emAiLs” kinda person buT they simply left me no choice.Still, defending the Commonwealth was in ASD’s hands now, and that’s a win for me at this point.☑️ figure out whether i have done a crime☑️ notify someone (The Government) that this happened⬜ get permission to publish this here blog post⬜ tell qantas about the security issue so they can fix itPart 3: Telling Qantas the bad newsHey remember like fifteen minutes ago when this post was about webpages?I’m guessing Qantas didn’t  to send the customer their passport number, phone number, and staff comments about them, so I wanted to let them know their website was doing that. Maybe the website was well meaning, but ultimately caused more harm than good, like how that time the bike path railings on the Golden Gate Bridge accidentally turned it into the world’s largest harmonica.But why does the website even send you all that stuff in the first place? I don’t know, but to speculate wildly: Maybe the website just sends you  the data it knows about you, and then only shows you your name, flight times, etc, while leaving the passport number etc. still in the page.If that were true, then Qantas would want to unblend the digital smoothie they’ve sent you, if you will. They’d want to change it so that they only send you your name and flight times and stuff (which are a key ingredient of the smoothie to be sure), not the whole identity fraud smoothie.I wanted to tell them the smoothie thing, but how do I contact them?The first place to check is usually , maybe that’ll w-Okay fine maybe I should just email  surely that’s it? I could only find a phone number to report security problems to, and I wasn’t sure if it was like…. airport security?So I just… called the number and was like “heyyyy uhhhh I’d like to report a cyber security issue?”, and the person was like “yyyyya just email ” and i was like “ok sorrY”.Time to email Qantas I guessI emailed Qantas, being like “beep boop here is how the computer problem works”.(Have you been wondering about the little dots in this post? Click this one for the rest of the email .)A few days later, I got this reply.And then I never heard from this person againAirlines were going through kinda a  at the time, so I guess that’s what happened?After filling up my “get left on read” combo meter, I desperately resorted to calling Qantas’ secret media hotline number.They said the issue was being fixed by Amadeus, the company who makes their booking software, rather than with Qantas itself. I’m not sure if that means other Amadeus customers were also affected, or if it was just the way Qantas was using their software, or what.It’s common to give companies 90 days to fix the bug, before you publicly disclose it. It’s a tradeoff between giving them enough time to fix it, and people being hacked because of the bug as long as it’s out there.But, well, this was kinda a special case. Qantas was going through some #struggles, so it was taking longer. Lots of their staff were stood down, and the world was just generally more cooked. At the same time, hardly anybody was flying at the time, due to see above re: #struggles. So, I gave Qantas as much time as they needed.The world is a completely different place, and Qantas replies to me, saying they fixed the bug.
It  take five months, which is why it took so long for you and I to be having this weird textual interaction right now.I don’t have a valid Booking Reference, so I can’t actually check what’s changed. I asked a friend to check (with an expired Booking Reference), and they said they didn’t see a mention of “documentNumber” anymore, which sounds like the passport number is no longer there. But That’s Not Science, so I don’t know for sure.I originally found the bug in March, which was about 60 years ago. BUT we got there baybee, Qantas emailed me saying the bug had been fixed on August 21. They later told me they actually fixed the bug in July, but the person I was talking to didn’t know about it until August.Qantas also said this when I asked them to review this post:Thanks again for letting us have the opportunity to review and again for refraining from posting until the fix was in place for vulnerability.Our standard advice to customers is not to post pictures of the boarding pass, or to at least obscure the key personal information if they do, because of the detail it contains.We appreciate you bringing it to our attention in such a responsible way, so we could fix the issue, which we did a few months ago now.I also asked Qantas what they did to fix the bug, and they said:Unfortunately we’re not able to provide the details of fix as it is part of the protection of personal information.☑️ figure out whether i have done a crime☑️ notify someone (The Government) that this happened⬜ get permission to publish this here blog post☑️ tell qantas about the security issue so they can fix itPart 4: Finding Tony AbbottLike 2003’s , this section was an emotional rollercoaster.The government was presumably helping Tony Abbott reset his passport number, and making sure his current one wasn’t being used for any of that yucky identity fraud.But, much like Shannon Noll’s 2004 , what  me? I really wanted to write a blog post about it, you know? So I could warn people about the non-obvious risk of sharing their boarding passes, and also make dumb and inaccessible references to the early 2000s.The government people I talked to couldn’t give me permission to write this post, so rather than willingly wandering deeper into the procedurally generated labyrinth of government department email addresses (it’s dark in there), I tried to find Tony Abbott or his staff directly.Calling everybody in Australia one by oneI called Tony Abbott’s former political party again, and asked them how to contact him, or his office, or  I’m really having a moment rn. They said they weren’t associated with him anymore, and suggested I call , like I was the Queen or something.In case you don’t know it, Parliament House is sorta like the White House, I think? The Prime Minister lives there and has a nice little garden out the back with a macadamia tree that never runs out, and everyone works in different colourful sections like “Making it so Everyone Gets a Fair Shake of the Sauce Bottle R&D” and “Mateship” and they all wear matching uniforms with lil kangaroo and emu hats, and they all do a little dance every hour on the hour to celebrate another accident-free day in the Prime Minister’s chocolate factory.calling parliament house i guessNot really sure what to expect, I called up and was all like “yeah bloody g’day, day for it ay, hot enough for ya?”. Once the formalities were out of the way, I skipped my usual explanation of why I was calling and just asked point-blank if they had Tony Abbott’s contact details.The person on the phone was casually like “Oh, no, but I can put you through to the , who can give you the contact details of former members”. I was like “…..okay?????”. Was I supposed to know who that was? Isn’t a Serjeant like an army thing?But no, the Serjeant-at-arms was just a nice lady who told me “he’s in a temporary office right now, and so doesn’t have a phone number. I can give you an email address or a P.O. box?”. I was like “ok th-thank you your majesty”.It felt a bit weird just…. emailing the former PM being like “boy do i have bad news for ”, but I figured he probably wouldn’t read it anyway. If it was  easy to get this email address, everyone had it, and so nobody was likely to be reading the inbox.Spoilers: It didn’t work.Finding Tony Abbott’s staffI roll out of bed and stare bleary-eyed into the morning sun, my ultimate nemesis, as Day 40 of not having found Tony Abbott’s staff begins.Retinas burning, in a moment of determination/desperation/hubris, I went and asked even  people that  know how to contact Tony Abbott’s staff.I asked a journalist friend, who had the kind of ruthlessly efficient ideas that come from, like, being a professional journalist. They suggested I find Tony Abbott’s former staff from when he was PM, and contact their offices and see if they have his contact details.It was a strange sounding plan to me, which I thought meant it would  work.Apparently Prime Ministers themselves have “ministers” (not prime), and those are their staff. That’s who I was looking for.Okay but, the problem was that most of these people are retired now, and the glory days of 2013 are over. Each time I hover over one of their names, I see “so-and-so is a former politician and….” and discard their Wikipedia page like a LeSnak wrapper into the wind.Eventually though, I saw  minister.That’s the  Prime Minister of Australia (at the time of writing, that is, for all I know we’re three Prime-Ministers deep into 2020 by the time you read this), you know he’s  gonna be easier to find.Let’s call the Prime Minister’s office I guess?Easy google of the number, absolutely no emotional journey resulting in my growth as a person this time.When I call, I hear what sounds like two women laughing in the background? One of them answers the phone, slightly out of breath, and says “Hello, Prime Minister’s office?”. I’m like “….hello? Am I interrupting something???”.I clumsily explain that I know this is Scott Morrison’s office, but I actually was wondering if they had Tony Abbott’s contact details, because it’s for “a time-sensitive media enquiry”, and I j-
She interrupts to explain “so Tony Abbott isn’t Prime Minister anymore, this is Scott Morrison’s office” and I’m like “yA I  please I am desperate for these contact details”.She says “We wouldn’t have that information but I’ll just check for you” and then pauses for like, a long time? Like 15 seconds? I can only wonder what was happening on the other end. Then she says “Oh actually I can give you Tony Abbott’s personal assistant’s number? Is that good?”.Ummmm YES thanks that’s what I’ve been looking for this whole time? Anyway brb i gotta go be uh a journalist or something.Calling Tony Abbott’s personal assistant’s personal assistantI fumble with my phone, furiously trying to dial the number.I ask if I’m speaking to Tony Abbott’s personal assistant. The person on the other end says no, but he  one of Tony Abbott’s staff. It has been a long several months of calling people. The cold ice is starting to thaw. One day, with enough therapy, I may be able to gather the emotional resources necessary to call another government phone number.I explain the security issue I want to report, and midway through he interrupts with “sorry….  are you and what’s the organisation you’re calling from?” and I’m like “uhhhh I mean my name is Alex and uhh I’m not calling from any organisation I’m just like a person?? I just found this thing and…”.The person is mercifully forgiving, and says that he’ll have to call me back. I stress once again that I’m calling to help them, happy to wait to publish until they feel comfortable, and definitely do not warrant the bulk-installation of antivirus products.Calling Tony Abbott’s personal assistantAn hour later, I get a call from a number I don’t recognise.He explains that the guy I talked to earlier was  assistant, and he’s Tony Abbott’s PA. Folks, we made it. It’s as easy as that.He says he knows what I’m talking about. He’s got . He’s already in the process of getting Tony Abbott a new passport number. This is the stuff. It’s all coming together.I ask if I can publish a blog post about it, and we agree I’ll send a draft for him to review.“These things do interest him - he’s quite keen to talk to you”I was like exCUSE me? Tony Abbott, Leader of the 69th Ministry of Australia, wants to call me on the ? I suppose I owe this service to my country?This story was already completely cooked so sure, whatever. I’d already declared emotional bankruptcy, so nothing was coming as a surprise at this point.I asked what he wanted to talk about. “Just to pick your brain on these things”. We scheduled a call for 3:30 on Monday.And then Tony Abbott just… calls me on the phone?Mostly, he wanted to check whether his understanding of how I’d found his passport number was correct (it was). He also wanted to ask me how to learn about “the IT”.He asked some intelligent questions, like “how much information is in a boarding pass, and what do people like me need to know to be safe?”, and “why can you get a passport number from a boarding pass, but not from a bus ticket?”.The answer is that boarding passes have your password printed on them, and bus tickets don’t. You can use that password to log in to a  (widely regarded as a bad move), and at that point all bets are off, websites can just do whatever they want.He was vulnerable, too, about how computers are harder for him to understand.“It’s a funny old world, today I tried to log in to a [Microsoft] Teams meeting (Teams is one of those apps), and the fire brigade uses a Teams meeting. Anyway I got fairly bamboozled, and I can now log in to a Teams meeting in a way I couldn’t before.It’s, I suppose, a terrible confession of how people my age feel about this stuff.”Then the Earth stopped spinning on its axis.For an instant, time stood still.“You could drop me in the bush and I’d feel perfectly confident navigating my way out, looking at the sun and direction of rivers and figuring out where to go, but this! Hah!”This was possibly the most pure and powerful Australian energy a human can possess, and explains how we elected our strongest as our leader. The raw energy did in fact travel through the phone speaker and directly into my brain, killing me instantly.When I’d collected myself from various corners of the room, he asked if there was a book about the basics of IT, since he wanted to learn about it. That was kinda humanising, since it made me realise that even famous people are just people too.Anyway I hadn’t heard of a book that was any good, so I told a story about my mum instead.A story about my mum insteadI said there probably was a book out there about “the basics of IT”, but it wouldn’t help much. I didn’t learn from a book. 13 year old TikTok influencers don’t learn from a book. They just .My mum always said when I was growing up that:There were “too many buttons”She was afraid to press the buttons, because she didn’t know what they didI can understand that, since grown ups don’t have the sheer dumb hubris of a child, and that’s what makes them afraid of the buttons.Like, when a toddler uses a spoon for the first time, they don’t know what a spoon is, where they are, or who the current Prime Minister is. But they see the spoon, and they see the cereal, and their dumb baby brain is just like “yeA” and they have a red hot go. And like, they get it wrong the first few times, but it doesn’t matter, because they don’t know to be afraid of getting it wrong. So eventually, they get it right.Okay so I didn’t tell the spoon thing to Tony Abbott, but I did tell him what I always told my mum, which was: “Mum you just gotta press  the buttons, to find out what they do”.He was like “Oh, you just learn by trial and error”. Exactly! Now that I think about it, it’s a bit scary. We are dumb babies learning to use a spoon for the first time, except if you do it wrong some clown writes a blog post about you. Anyway good luck out there to all you big babies.Asking to publish this blog postWhen I asked Tony Abbott for permission to publish the post you are reading right now while neglecting your responsibilities, he said “well look Alex, I don’t have a problem with it, you’ve alerted me to something I probably should have known about, so if you wanna do that, go for it”.At the end of the call, he said “If there’s ever anything you think I need to know, give us a shout”.Look you gotta hand it to him. That’s exactly the right way to respond when someone tells you about a security problem. Back at the beginning, I was kinda worried that he might misunderstand, and think I was trying to hack him or something, and that I’d be instantly slam dunked into jail. But nope, he was fine with it. And now you, a sweet and honourable blog post browser, get to learn the dangers of posting your boarding pass by the realest of real-world examples.During the call, I was completely in shock from the lost in the bush thing killing me instantly, and so on. But afterwards, when I looked at the quotes, I realised he just wanted to understand what had happened to him, and more about how technology works. That’s the same kind of curiosity  had, that started this whole surrealist three-act drama. That… wasn’t really what I was expecting from Tony Abbott, but it’s what I found.The point of this story isn’t to say “wow Tony Abbott got hacked, what a dummy”. The point is that if someone famous can unknowingly post their boarding pass, anyone can.Anyway that’s why I vote right wing now baybeeeee.☑️ figure out whether i have done a crime☑️ notify someone (The Government) that this happened☑️ get permission to publish this here blog post☑️ tell qantas about the security issue so they can fix itWait no what the heck did I just readYour boarding pass for a flight can sometimes be used to get your passport number. Don’t post your boarding pass or baggage receipt online, keep it as secret as your passport.The Booking Reference on the boarding pass can be used to log in to the airline’s “Manage Booking” page, which sometimes contains the passport number, depending on the airline. I saw that Tony Abbott had posted a photo of his boarding pass on Instagram, and used it to get his passport details, phone number, and internal messages between Qantas flight staff about his flight booking.One day, my friend who was also in “the group chat” said “I was thinking…. why didn’t  hack Tony Abbott? And I realised I guess it’s because you have more hubris”.I was deeply complimented by this, but that’s not the point. The point is that you, too, can have hubris.You know how they say to commit a crime (which once again I insist did not happen in my case) you need means, motive, and opportunity? Means is the ability to use right click > Inspect Element, motive is hubris, and opportunity is the dumb luck of having my friend message me the Instagram post.I know, I’ve been saying “hubris” a lot. I mean “the willingness to risk breaking the rules”. Now hold up, don’t go outside and do crimes (unless it’s really funny). I’m not talking about breaking the , I’m talking about rules we just follow without realising, like social rules and conventions.Here’s a simple example. You’re at a sufficiently fancy restaurant, like I dunno, with white tablecloths or something? The waiter asks if you’d like “still or sparkling water?”If you say “still”, it costs Eleven Dollars. If you say “sparkling”, it costs Eleven Dollars and tastes all gross and fizzy. But if you say “tap water, please”, you just get tap water, what you wanted in the first place?When I first saw someone do this I was like “you can  that? I just thought you had to pay Eleven Dollars extra at fancy restaurants!”.It’s not written down anywhere that you can ask for tap water. But when I found out you  do that, and like, nothing bad happens, I could suddenly do it too. Miss me with that Eleven Dollars fizzy water.Basically, until you’ve broken the rules, the idea that the rules can be broken might just not occur to you. That’s how it felt for me, at least.In conclusion, to be a hacker u ask for tap water.Why is it bad for someone else to have your passport number?Hey crime gang, welcome back to Identity Fraud tips and tricks with Alex.A passport is government-issued ID. It’s how you prove you’re you. The fact that you have your passport and I don’t is how you prevent  from convincing the government that I’m you and doing crimes in your name.Just having the information on the passport is not quite as powerful as a photo of the full physical passport, with your photo and everything.With your passport number, someone could:Book an international flight as you.Apply for anything that requires proof of identity documentation with the government, e.g. Working with children checkActivate a SIM card (and so get an internet connection that’s traceable to you, not them, hiding them from the government)Create a fake physical passport from a template, with the correct passport number (which they then use to cross a border, open a bank account, or anything)who knows what else, not me, bc i have never done a crimeAm I a big bozo, a big honking goose, if I post my boarding pass on Instagram?Nah, it’s an easy mistake to make. How are you supposed to know not to? It’s not obvious that your boarding pass is secret, like a password. I think it’s on the airline to inform you on the risks you’re taking when you use their stuff. now that you’ve read this blog post, I regret to inform you that you  in fact be an entire sack of geese if you go and post your boarding pass now.When did all of this happen?March 22 - @hontonyabbott posts a picture of a boarding pass and baggage receipt. I log in to the website and get the passport number, phone number, and internal Qantas comments.March 24 - I contact the Australian Signals Directorate (ASD) and let them know what happened.March 27 - ASD tells me their investigation is complete, I send them a shakas gif, and they thank me for being a good citizen.March 29 - I learn from lawyers that I have not done a crime 💯March 30 - I contact Qantas and tell them about the vulnerability.May 1 - Tony Abbott calls me, we chat about being dropped in the middle of the bush.July 17 - Paper Mario: The Origami King is released for Nintendo Switch.August 21 - Qantas emails me saying the security problem has been fixed.September 13 - Various friends finish reviewing this post <3September 15 - Tony Abbott and Qantas review this post.Today - You read this post instead of letting it read you, nice job you.Let me answer that question,,, with a question.Maybe try drinking some water you big goose. Honk honk, I’m so dehydrated lol. That’s you.I wrote this because I can’t go back to the Catholic church ever since they excommunicated me in 1633 for insisting the Earth revolves around the sun.]]></content:encoded></item><item><title>Cell Towers Can Double as Cheap Radar Systems for Ports and Harbors (2014)</title><link>https://spectrum.ieee.org/cell-tower-signals-can-improve-port-security</link><author>transpute</author><category>hn</category><pubDate>Sun, 29 Jun 2025 21:48:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[How do you see ships without a pricey radar system? The question has troubled seaports around the world as they work to improve security. Without radar installations, it can be hard for port employees to detect small ships like those employed by pirates or by the terrorists who attacked the  in 2000. A team of researchers in Germany can now offer security teams a new option, though: putting existing cellular towers to work as quick and dirty radar systems.Developed at the Fraunhofer Institute for Communications, Information Processing and Ergonomics, the new security system employs a technology known as Passive Coherent Location (PCL), which harnesses the radio signals sent out by cell towers to pinpoint the location of ships entering a harbor. (PCL) works in much the same way as radar—sending signals that bounce off of objects and reading the signals that return to determine the objects’ locations.Radar uses strong, directed waves that make it easy to find objects. In contrast, PCL uses the much weaker signals that are being bounced off of objects by cell towers. These bounced waves help a PCL system build a dynamic map of a port and traffic moving through it by looking at where cell signals come into contact with objects in the water. While this technique takes advantage of waves that are already being produced by cell towers and doesn’t require the installation of a new radar system, it also means the signals are more difficult to accurately interpret. To make PCL useful, the Fraunhofer team had to write new algorithms that distinguish the echoes created by objects from the mélange of signal noise.In other words, filtering out the strong signals emanating directly from cell towers lets the PCL system concentrate on finding the weaker signals that represent boats in the water. Improvements to the sensitivity of the new system have even allowed it to track craft as they move across the water. In tests of the PCL system, researchers were able to identify small speedboats from as far away as 4 kilometers. And all the equipment for operating a PCL system can fit in a trailer, making it feasible to install in remote locations and on a budget.]]></content:encoded></item><item><title>China Dominates 44% of Visible Fishing Activity Worldwide</title><link>https://oceana.org/press-releases/china-dominates-44-of-visible-fishing-activity-worldwide/</link><author>scubakid</author><category>hn</category><pubDate>Sun, 29 Jun 2025 21:43:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Today, on the International Day for the Fight Against Illegal, Unreported, and Unregulated Fishing,Oceana released an analysis of China’s global fishing* activity worldwide between 2022 and 2024. The analysis shows China’s global fishing footprint, in which 57,000 of their industrial fishing vessels dominated 44% of the world’s visible fishing activity during this period.  “To protect our oceans and fisheries, we must know who is fishing and where,” said Dr. Max Valentine, illegal fishing and transparency campaign director and senior scientist at Oceana. “It is critical that we have eyes on the seas, paying close attention to the world’s largest fishing fleets, especially from China, which have been linked to illegal, unreported, and unregulated fishing and human rights abuses at sea. The sheer scale of China’s distant-water fleet has a profound impact on marine ecosystems worldwide. Transparency at sea is essential, not just to track distant-water fleets, but to hold bad actors accountable, protect vulnerable communities, and safeguard the sustainability of our ocean for future generations.” Some key takeaways from Oceana’s analysis of China’s apparent fishing activity over a three-year period, from Jan. 1, 2022, to Dec. 31, 2024: 57,000 fishing vessels, primarily trawlers, flagged to China appeared to fish for more than 110 million hours, China’s fishing vessels appeared to conduct 44% of the global fishing activity during this period, Chinese vessels accounted for 30% of all fishing activity on the high seas, appearing to fish for more than 8.3 million hours,  China’s fishing vessels were most active in South Korea (11.8 million hours), Taiwan (4.4 million hours), Japan (1.5 million hours), Kiribati (almost 425,000 hours), and Papua New Guinea (over 415,000 hours),   China appeared to fish in more than 90 countries’ waters for more than 22 million hours. Increased transparency in global fisheries is critical. Oceana calls on governments to require vessel monitoring for both their fishing fleets and vessels they authorize to fish in their waters.   The analysis used data from Global Fishing Watch** (GFW) — an independent nonprofit founded by Oceana in partnership with Google and SkyTruth. Notably, the analysis reflects only a partial view of China’s fishing activities during this time, as it includes only those vessels flagged to China  transmitting automatic identification system (AIS) data, making them “visible” to public tracking systems.  Illegal, unreported, and unregulated (IUU) fishing is a low-risk, high-reward activity, especially on the high seas where a fragmented legal framework and lack of effective enforcement allow it to thrive. IUU fishing can include fishing without authorization, ignoring catch limits, operating in closed or protected areas, targeting protected wildlife, and fishing with prohibited gear. These illicit activities can destroy important ocean habitat, severely deplete fish populations, and threaten global food security. These actions not only contribute to overfishing, but also give illegal fishers an unfair advantage over those who play by the rules.    Oceana released the results of a nationwide poll in 2024, which found that American voters overwhelmingly support transparency and traceability in the seafood supply chain. Included among the key findings, 90% of voters agree that imported seafood should be held to the same standards as U.S. caught seafood. Additionally, 91% of voters agree that seafood caught using human trafficking and slave labor should NOT be bought or sold in the U.S. Eighty-five percent of voters agree that all seafood should be traceable from the fishing boat to the dinner plate, and 88% say consumers should be reassured that the seafood they purchase was legally caught. Oceana’s poll, conducted by the nonpartisan polling company Ipsos using the probability-based KnowledgePanel®, surveyed 1,053 registered U.S. voters from June 28 to 30, 2024.       Read more about Oceana’s campaign here. *Any and all references to “fishing” should be understood in the context of Global Fishing Watch’s (GFW) fishing detection algorithm, which is a best effort to determine “apparent fishing effort” based on vessel speed and direction data from the automatic identification system (AIS) collected via satellites and terrestrial receivers. As AIS data varies in completeness, accuracy, and quality, and the fishing detection algorithm is a statistical estimate of apparent fishing activity, it is possible that some fishing effort is not identified and, conversely, that some fishing effort identified is not fishing. For these reasons, GFW qualifies all designations of vessel fishing effort, including synonyms of the term “fishing effort,” such as “fishing” or “fishing activity,” as “apparent” rather than certain. Any/all GFW information about “apparent fishing effort” should be considered an estimate and must be relied upon solely at your own risk. GFW is taking steps to make sure fishing effort designations are as accurate as possible. All references to EEZ boundaries and sovereignty are based solely off the Marine Regions “World EEZ v12” definitions.**Global Fishing Watch, a provider of open data for use in this analysis, is an international nonprofit organization dedicated to advancing ocean governance through increased transparency of human activity at sea. The views and opinions expressed in this press release and fact sheet are those of the authors, which are not connected with or sponsored, endorsed or granted official status by Global Fishing Watch. By creating and publicly sharing map visualizations, data and analysis tools, Global Fishing Watch aims to enable scientific research and transform the way our ocean is managed. Global Fishing Watch’s public data was used in the production of this fact sheet. ]]></content:encoded></item><item><title>Anticheat Update Tracking</title><link>https://not-matthias.github.io/posts/anticheat-update-tracking/</link><author>not-matthias</author><category>hn</category><pubDate>Sun, 29 Jun 2025 21:06:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Error handling in Rust</title><link>https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html</link><author>emschwartz</author><category>hn</category><pubDate>Sun, 29 Jun 2025 20:28:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The current standard for error handling, when writing a crate, is to define
one error enum per module, or one for the whole crate
that covers all error cases that the module or crate
can possibly produce, and each public function that returns a  will use
said error enum.This means, that a function will return an error enum, containing error variants that
the function cannot even produce. If you match on this error enum, you will
have to manually distinguish which of those variants are not applicable in
your current scope, based on the documentation of the function (and who reads that anyway? /s).The problem with the status quoWhat makes Rust so great, is the ability to express requirements via the type
system in a way that makes it very hard for you to violate them, and yet, we
collectively decided to create these huge error-enums. I completely understand
where this is coming from. Defining an extra error enum for every function
and all the conversions between them is extremely tedious. And so everyone and
their mother is building big error types. Well, not Everyone. A small handful of
indomitable nerds still holds out against the standard.An error is a singular bit of information, might be completely independent
of other errors a function can return, and should probably be represented
by a struct rather than an enum variant. A function returns one of
a set of those if it goes wrong, but it doesn't define the errors
themselves. The first Rust crate I saw that followed this philosophy, was
terrors (Go ahead, check it out).
I still think it's beautiful. It's also a little inconvenient.
You have to write  a lot and some functions
have a lot of possible error points, some of which being
the contents of other function's error sets. And yet, you have to spell
them out all over again. Still, I really like this crate ... from a distance.Speaking of error sets, there is a
crate with this name, that I
prefer to use nowadays. Instead of doing Olympia level type acrobatics (like
terrors) it uses macros. It allows you to define error enums for different
functions in a very concise way and automatically generates the trait
implementations for conversions between those. Want a taste?It allows us to create error sets from variants and from unions with other error sets.
The  operator will work if the error set you use it on is a sub-set of the function's
error set, and it will find out whether that's the case, even if you don't use the
union operator, i.e. this works:This is still a bit too verbose for my tastes if you use many actual struct errors,
e.g. because you want some fields on them to carry additional information, or because
you want to annotate them with error messages. However, I need them seldomly enough,
so that I'll happily pay the extra keystrokes to define a wrapper enum for them
(like the  enum in the first example) for now.There are more libraries out there that explore this paradigm in different ways,
e.g. SmartErr. And I once saw a crate
that offered an attribute macro that you could slap on a function, and then
it would parse the functions body and generate an error enum and insert it into
the functions return type,
based on the errors that occured in the function's body. Sadly I didn't find
it again despite searching for it for an hour. If anyone has a link,
please tell me.]]></content:encoded></item><item><title>Ask HN: What Are You Working On? (June 2025)</title><link>https://news.ycombinator.com/item?id=44416093</link><author>david927</author><category>hn</category><pubDate>Sun, 29 Jun 2025 20:21:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[What are you working on?  Any new ideas which you're thinking about?]]></content:encoded></item><item><title>YouTube No Translation</title><link>https://addons.mozilla.org/en-GB/firefox/addon/youtube-no-translation/</link><author>doener</author><category>hn</category><pubDate>Sun, 29 Jun 2025 20:10:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is an open source Add-on preventing automatic translation on YouTube. It ensures that video titles and description remain in their original language and that the audio track defaults to the original version, providing an authentic viewing experience. ✨ : : Keep titles in their original language : Always use the original audio track (also works on shorts) : Prevent description translations : Choose your preferred subtitle language. If unavailable, subtitles are automatically disabled (auto-generated ones are always ignored)If this Add-On has been useful to you, you can support its development on KO-FI: https://ko-fi.com/yougo 🙏Want to report an issue or ask for a feature : https://github.com/YouG-o/YouTube_No_Translation/issues]]></content:encoded></item><item><title>Many ransomware strains will abort if they detect a Russian keyboard installed (2021)</title><link>https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/</link><author>air7</author><category>hn</category><pubDate>Sun, 29 Jun 2025 18:29:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In a  discussion last week on ransomware attacks, KrebsOnSecurity noted that virtually all ransomware strains have a built-in failsafe designed to cover the backsides of the malware purveyors: They simply will not install on a  computer that already has one of many types of virtual keyboards installed — such as Russian or Ukrainian. So many readers had questions in response to the tweet that I thought it was worth a blog post exploring this one weird cyber defense trick.The Commonwealth of Independent States (CIS) more or less matches the exclusion list on an awful lot of malware coming out of Eastern Europe.The Twitter thread came up in a discussion on the ransomware attack against Colonial Pipeline, which earlier this month shut down 5,500 miles of fuel pipe for nearly a week, causing fuel station supply shortages throughout the country and driving up prices. The FBI said the attack was the work of , a new-ish ransomware-as-a-service offering that says it targets only large corporations.DarkSide and other Russian-language affiliate moneymaking programs have long barred their criminal associates from installing malicious software on computers in a host of Eastern European countries, including Ukraine and Russia. This prohibition dates back to the earliest days of organized cybercrime, and it is intended to minimize scrutiny and interference from local authorities.In Russia, for example, authorities there generally will not initiate a cybercrime investigation against one of their own unless a company or individual within the country’s borders files an official complaint as a victim. Ensuring that no affiliates can produce victims in their own countries is the easiest way for these criminals to stay off the radar of domestic law enforcement agencies.Possibly feeling the heat from being referenced in President Biden’s Executive Order on cybersecurity this past week, the DarkSide group sought to distance itself from their attack against Colonial Pipeline. In a message posted to its victim shaming blog, DarkSide tried to say it was “apolitical” and that it didn’t wish to participate in geopolitics.“Our goal is to make money, and not creating problems for society,” the DarkSide criminals wrote last week. “From today we introduce moderation and check each company that our partners want to encrypt to avoid social consequences in the future.”But here’s the thing: Digital extortion gangs like DarkSide take great care to make their entire platforms geopolitical, because their malware is engineered to work only in certain parts of the world.DarkSide, like a great many other malware strains, has a hard-coded do-not-install list of countries which are the principal members of the Commonwealth of Independent States (CIS) — former Soviet satellites that mostly have favorable relations with the Kremlin. The full exclusion list in DarkSide (published by ) is below:Simply put, countless malware strains will check for the presence of one of these languages on the system, and if they’re detected the malware will exit and fail to install.Will installing one of these languages keep your Windows computer safe from all malware? Absolutely not. There is plenty of malware that doesn’t care where in the world you are. And there is no substitute for adopting a defense-in-depth posture, and avoiding risky behaviors online.But is there really a downside to taking this simple, free, prophylactic approach? None that I can see, other than perhaps a sinking feeling of capitulation. The worst that could happen is that you accidentally toggle the language settings and all your menu options are in Russian.If this happens (and the first time it does the experience may be a bit jarring) hit the Windows key and the space bar at the same time; if you have more than one language installed you will see the ability to quickly toggle from one to the other. The little box that pops up when one hits that keyboard combo looks like this:Cybercriminals are notoriously responsive to defenses which cut into their profitability, so why wouldn’t the bad guys just change things up and start ignoring the language check? Well, they certainly can and maybe even will do that (a recent version of DarkSide analyzed by Mandiant did  perform the system language check).But doing so increases the risk to their personal safety and fortunes by some non-trivial amount, said , chief research officer at New York City-based cyber investigations firm Unit221B.Nixon said because of Russia’s unique legal culture, criminal hackers in that country employ these checks to ensure they are only attacking victims outside of the country.“This is for their legal protection,” Nixon said. “Installing a Cyrillic keyboard, or changing a specific registry entry to say ‘RU’, and so forth, might be enough to convince malware that you are Russian and off limits. This can technically be used as a ‘vaccine’ against Russian malware.”Nixon said if enough people do this in large numbers, it may in the short term protect some people, but more importantly in the long term it forces Russian hackers to make a choice: Risk losing legal protections, or risk losing income.“Essentially, Russian hackers will end up facing the same difficulty that defenders in the West must face — the fact that it is very difficult to tell the difference between a domestic machine and a foreign machine masquerading as a domestic one,” she said.KrebsOnSecurity asked Nixon’s colleague at Unit221B — founder  — what he thought about the efficacy of another anti-malware approach suggested by Twitter followers who chimed in on last week’s discussion: Adding entries to the Windows registry that specify the system is running as a virtual machine (VM). In a bid to stymie analysis by antivirus and security firms, some malware authors have traditionally configured their malware to quit installing if it detects it is running in a virtual environment.But James said this prohibition is no longer quite so common, particularly since so many organizations have transitioned to virtual environments for everyday use.“Being a virtual machine doesn’t stop malware like it used to,” James said. “In fact, a lot of the ransomware we’re seeing now is running on VMs.”But James says he loves the idea of everyone adding a language from the CIS country list so much he’s produced his own clickable two-line Windows batch script that adds a Russian language reference in the specific Windows registry keys that are checked by malware. The script effectively allows one’s Windows PC to look like it has a Russian keyboard installed without actually downloading the added script libraries from Microsoft.To install a different keyboard language on a Windows 10 computer the old fashioned way, hit the Windows key and X at the same time, then select Settings, and then select “Time and Language.” Select Language, and then scroll down and you should see an option to install another character set. Pick one, and the language should be installed the next time you reboot. Again, if for some reason you need to toggle between languages, Windows+Spacebar is your friend.]]></content:encoded></item><item><title>Tools I love: mise(-en-place)</title><link>https://blog.vbang.dk/2025/06/29/tools-i-love-mise/</link><author>micvbang</author><category>hn</category><pubDate>Sun, 29 Jun 2025 17:56:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Once in a while you get introduced to a tool that instantly changes the way you work. For me, mise is one of those tools.mise is the logical conclusion to a lot of the meta-tooling that exists around language-specific version and package managers like asdf, nvm, uv, pyenv etc. It makes it exceptionally easy to install, use, and manage software. It also allows you to manage environment variables and declare tasks (run commands).The first step in getting an intuitive understanding of what mise can help you with is to use it to install a tool. Pick your favorite and try it out; it supports !jj
command_not_found_handler:5: not found: jj

mise use jj
mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0

jj version
jj 0.30.0

 ..

jj version
command_not_found_handler:5: not found: jj

eaxmples_mise

jj version
jj 0.30.0
As the above shows, with mise we’re just one command away from installing and trying out a new tool, e.g. .In the above we that mise printed mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0. This tells us that mise has created (or updated) the mise configuration . 
We also see that if we cd out of , the  command is no longer available. If we cd back into , it becomes available again; unless you explicitly install tools globally, mise will only make the tools available which are mentioned in a  file on the path from your current directory to the root of your file system. That of course means that we could potentially meet multiple  files when going back up to the root of the file system. Mise handles this by concatting the configurations and overwriting conflicting configurations, letting the file furthest down the tree win.This is a clever design as it allows us to configure different versions of the same tool to be available in different directories. Let’s have a look at what the  file looks like:If we want a specific version of  to be installed in a specific directory, we just update the toml file to say e.g. .Let’s see what it looks like to use mise to manage Python versions for two projects with different requirements:tree

├── project_new
│	└── mise.toml
└── project_old
    └── mise.toml

project_new/mise.toml
tools]
python project_old/mise.toml
tools]
python project_new
python 
Python 3.11.13

 ../project_old
python 
Python 3.8.20
When we cd into one of the directories listed above, mise automatically makes the version of the tool configured in  available to us. If it isn’t already installed, mise will install it for us. The implication of this is that you can commit a  to your repository, and anyone that has mise installed will automatically get and use the expected dev tools when they enter the project directory. And when it’s time to upgrade a dev tool, you can just update the version number in  and everyone will start using the new version!The fact that mise makes tools available to you according to the  file in your current working directory has further implications: it’s not just developer machines that can benefit from using mise; CI/CD pipelines can benefit greatly as well! When you use mise in your pipelines, you avoid the problem of having out of sync versions between developer and build machines. You get to have a single place where you can configure the version of your dev tools everywhere!As I mentioned in the beginning, besides managing dev tools, mise also allows you to declare and run so-called tasks. Think of a task as an advanced invocation of a bash script. Even if we use tasks as just plain bash scripts (they can do a lot more), it can be a major advantage to declare common operations such as building, testing, linting etc. as mise tasks, since all developers get access to them and will run their commands in exactly the same way every time. If you’re diligent in your naming, you can even make the experience of building or testing across projects identical.The following are examples of some very simple Python-related tasks declared in :Adding this to  will make the commands  and  available. Again, if you check this in to your repo, the commands will be available to all developers and pipelines. And reusing these names in your rust project means that you can use the same commands to tell cargo to install your crates or run your tests.Once you’ve declared your tasks you should of course also use them in your CI/CD pipeline. Doing this makes you less dependent on the particular yaml syntax and arbitrary requirements of your provider, and makes it easier to move to another one if you need to. It also ensures that there’s a standard way to build and test your code, helping to further reduce the amount of “it works on my machine”.There’s a lot of depth to what you can use mise to help you automate. It’s a lovely tool and I hope I’ve spiked your interest enough to give it a try!Although this is a very obvious problem, I want to make it explicit: a major concern of all software dependency management is control of your supply chain; how easy is it for somebody to insert malicious code into a binary you will run hugely impacts the integrity of your systems and data. Depending on your industry, it might not be feasible to use mise as it’s pretty opaque where your dependencies will be downloaded from.]]></content:encoded></item><item><title>Loss of key US satellite data could send hurricane forecasting back &apos;decades&apos;</title><link>https://www.theguardian.com/us-news/2025/jun/28/noaa-cuts-hurricane-forecasting-climate</link><author>trauco</author><category>hn</category><pubDate>Sun, 29 Jun 2025 17:39:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A critical US atmospheric data collection program will be halted by Monday, giving weather forecasters just days to prepare, according to a public notice sent this week. Scientists that the Guardian spoke with say the change could set hurricane forecasting back “decades”, just as this year’s season ramps up.In a National Oceanic and Atmospheric Administration (Noaa) message sent on Wednesday to its scientists, the agency said that “due to recent service changes” the Defense Meteorological Satellite Program (DMSP) will “discontinue ingest, processing and distribution of all DMSP data no later than June 30, 2025”.Due to their unique characteristics and ability to map the entire world twice a day with extremely high resolution, the three DMSP satellites are a primary source of information for scientists to monitor Arctic sea ice and hurricane development. The DMSP partners with Noaa to make weather data collected from the satellites publicly available.The reasons for the changes, and which agency was driving them, were not immediately clear. Noaa said they would not affect the quality of forecasting.However, the Guardian spoke with several scientists inside and outside of the US government whose work depends on the DMSP, and all said there are no other US programs that can form an adequate replacement for its data.“We’re a bit blind now,” said Allison Wing, a hurricane researcher at Florida State University. Wing said the DMSP satellites are the only ones that let scientists see inside the clouds of developing hurricanes, giving them a critical edge in forecasting that now may be jeopardized.“Before these types of satellites were present, there would often be situations where you’d wake up in the morning and have a big surprise about what the hurricane looked like,” said Wing. “Given increases in hurricane intensity and increasing prevalence towards rapid intensification in recent years, it’s not a good time to have less information.”The satellites also formed a unique source of data for tracking changes to the Arctic and Antarctic, and had been tracking changes to polar sea ice continuously for more than 40 years.“These are some of the regions that are changing the fastest around the planet,” said Carlos Moffat, an oceanographer at the University of Delaware who had been working on a research project in Antarctica that depended on DMSP data. “This new announcement about the sea ice data really amounts to blinding ourselves and preventing us from observing these critical systems.”Researchers say the satellites themselves are operating normally and do not appear to have suffered any errors that would physically prevent the data from continuing to be collected and distributed, so the abrupt data halt might have been an intentional decision.“It’s pretty shocking,” Moffat said. “It’s hard to imagine what would be the logic of removing access now and in such a sudden manner that it’s just impossible to plan for. I certainly don’t know of any other previous cases where we’re taking away data that is being collected, and we’re just removing it from public access.”The loss of DMSP comes as Noaa’s weather and climate monitoring services have become critically understaffed this year as Donald Trump’s so-called “department of government efficiency” (Doge) initiative has instilleddraconiancuts to federal environmental programs.A current Noaa scientist who wishes to remain anonymous for fear of retaliation said that the action to halt the DMSP, when taken in context with other recent moves by the Trump administration, amounted to “a systematic destruction of science”.The researcher also confirmed that federal hurricane forecasters were left unprepared for the sudden change with only a few days of notice.“It’s an instant loss of roughly half of our capabilities,” said the scientist. “You can’t expect us to make accurate forecasts and warnings when you take the useful tools away. It frankly is an embarrassment for the government to pursue a course with less data and just pretend everything will be OK.”“This is a huge hit to our forecasting capabilities this season and beyond, especially our ability to predict rapid intensification or estimate the strength of storms in the absence of hurricane hunters,” said Michael Lowry, a meteorologist who has worked at Noaa’s National Hurricane Center and with the Federal Emergency Management Agency. “The permanent discontinuation of data from these satellites is senseless, reckless and puts at risk the lives of tens of millions of Americans living in hurricane alley.”The DMSP dates back to 1963, when the Department of Defense determined a need for high-resolution cloud forecasts to help them plan spy missions. The program, which had been the longest-running weather satellite initiative in the federal government, has since evolved into a critical source of information not just on the inner workings of hurricanes, but also on polar sea ice, wildfires, solar flares and the aurora.In recent years, the DMSP had struggled to maintain consistent funding and priority within the Department of Defense as it transitioned away from its cold war mission. The only other nation with similar satellite capability is Japan, and messages posted earlier in June indicate that scientists had already been considering a switch to the Japanese data in case of a DMSP outage – though that transition will take time.Neither Noaa nor the Department of Defense specified exactly which service changes may have prompted such a critical program to be so abruptly halted.In a statement to the Guardian, Noaa’s communications director, Kim Doster, said: “The DMSP is a single dataset in a robust suite of hurricane forecasting and modeling tools in the National Weather Service portfolio. This routine process of data rotation and replacement would go unnoticed in past administrations, but the media is insistent on criticizing the great work that Noaa and its dedicated scientists perform every day.“Noaa’s data sources are fully capable of providing a complete suite of cutting-edge data and models that ensure the gold-standard weather forecasting the American people deserve.”One Noaa source the Guardian spoke to said the loss of DMSP’s high-resolution data could not be replaced by any other existing Noaa tool.A statement from an official at US space force, which is part of the Department of Defense, said: “The National Oceanic and Atmospheric Administration (Noaa) operates the Defense Meteorological Satellite Program (DMSP) for the DoD on behalf of the US Space Force, who has satellite control authority.”The official went on to say that Noaa receives the data from the US navy’s Fleet Numerical Meteorology and Oceanography Center (FNMOC) and added: “While the Space Force does provide DMSP data and processing software to DoD users, to include the US Navy, questions about the reasons for FNMOC’s changes to DMSP data processing should be directed to the Navy.“Even as FNMOC is making a change on their end, the posture on sharing DMSP data has not changed. Noaa has been making this DMSP data publicly available, and many non-DoD entities use this data that is originally processed by FNMOC.“DMSP satellites and instruments are still functional. The data provided to FNMOC is just one way the DoD uses DMSP data. DoD users (including the Navy) will continue to receive and operationally use DMSP data sent to weather satellite direct readout terminals across the DoD.”The Guardian is approaching the US navy for comment.]]></content:encoded></item><item><title>Personal care products disrupt the human oxidation field</title><link>https://www.science.org/doi/10.1126/sciadv.ads7908</link><author>XzetaU8</author><category>hn</category><pubDate>Sun, 29 Jun 2025 17:20:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>We accidentally solved robotics by watching 1M hours of YouTube</title><link>https://ksagar.bearblog.dev/vjepa/</link><author>alexcos</author><category>hn</category><pubDate>Sun, 29 Jun 2025 16:08:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The $25k car is going extinct?</title><link>https://media.hubspot.com/why-the-25000-car-is-going-extinct</link><author>pseudolus</author><category>hn</category><pubDate>Sun, 29 Jun 2025 15:59:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Seabaugh says dealers have long preferred to stock pricier vehicles and vehicles with higher trims. During the supply shortages of Covid, however, they took it to another level, as both dealers and manufacturers wanted to focus on vehicles with the highest margins as they tried to overcome lower inventories.But, Drury notes, the proliferation of expensive models and higher trims can’t all be pinned on dealers and automakers. He says  have become accustomed to features like complex infotainment systems, cameras, and heated seats and want to trade up for increasingly luxurious settings when they buy new cars.In 2021, Ford couldn’t keep up with the demand for fancy versions of the Explorer SUV, releasing new higher-trim versions in the middle of the year. For 2025, Honda eliminated its lowest trim of the Odyssey minivan.“When people keep buying, that's the chicken and the egg, right?” says Drury. “Do I blame the automaker? No. Do I blame the dealer? I can't blame them either. Do I blame the customer? No, because they want these things.”]]></content:encoded></item><item><title>Event – Fast, In-Process Event Dispatcher</title><link>https://github.com/kelindar/event</link><author>kelindar</author><category>hn</category><pubDate>Sun, 29 Jun 2025 15:19:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Medley Interlisp Project: Reviving a Historical Software System [pdf]</title><link>https://interlisp.org/documentation/young-ccece2025.pdf</link><author>pamoroso</author><category>hn</category><pubDate>Sun, 29 Jun 2025 14:45:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I made my VM think it has a CPU fan</title><link>https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html</link><author>todsacerdoti</author><category>hn</category><pubDate>Sun, 29 Jun 2025 13:55:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bloom Filters by Example</title><link>https://llimllib.github.io/bloomfilter-tutorial/</link><author>ibobev</author><category>hn</category><pubDate>Sun, 29 Jun 2025 11:56:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Octelium – FOSS Alternative to Teleport, Cloudflare, Tailscale, Ngrok</title><link>https://github.com/octelium/octelium</link><author>geoctl</author><category>dev</category><category>hn</category><pubDate>Sun, 29 Jun 2025 11:24:17 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I have been working on Octelium for quite a few years now but it was open sourced only by late May 2025. Octelium, as described more in detail in the repo's README, is simply an open source, self-hosted, unified platform for zero trust resource access that is primarily meant to be a modern alternative to corporate VPNs and remote access tools. It can operate as a remote access/corporate VPN (i.e. alternative to Twingate, Tailscale, OpenVPN Access Server, etc...), a ZTNA/BeyondCorp platform (i.e. alterntive to Cloudflare Access, Teleport, Google BeyondCorp, etc...), and it can also operate as an API/AI gateway, an infrastructure for MCP and A2A architectures and meshes, an ngrok alternative, a homelab infrastructure or even as a more advanced Kubernetes ingress. It's basically designed to operate like a unified Kubernetes-like scalable architecture for zero trust secure/remote access that's suitable for different human-to-workload and workload-to-workload environments. You can read more in detail the full set of main features and links about how it works in the repo's README or directly in the docs https://octelium.com/docs]]></content:encoded></item></channel></rss>