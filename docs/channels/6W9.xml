<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>US goverment seeks to rehire recently fired nuclear workers</title><link>https://www.bbc.com/news/articles/c4g3nrx1dq5o</link><author>niuzeta</author><category>hn</category><pubDate>Sun, 16 Feb 2025 07:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The US government is trying to bring back nuclear safety employees it fired on Thursday, but is struggling to let them know they should return to work, NBC News has reported.The National Nuclear Security Administration workers were among hundreds of employees in the energy department who received termination letters.An email obtained by NBC said the letters for some NNSA employees "are being rescinded, but we do not have a good way to get in touch with those personnel". The terminations are part of massive effort by President Donald Trump to slash the ranks of the federal workforce, a project he began on his first day in office, less than a month ago.Last week, nearly 10,000 federal workers were let go, according to multiple US outlets. That figure was in addition to the estimated 75,000 workers who have accepted an offer from the White House to leave voluntarily in the autumn. The nuclear security officials who were laid off on Thursday helped oversee the nation's stockpile of nuclear weapons. That included staff who are stationed at facilities where the weapons are built, according to CNN. Attempting to reach the workers, the email, which was sent to current employees, said: "Please work with your supervisors to send this information (once you get it) to people's personal contact emails."Trump is working to slash spending across the board, abroad and at home, and going so far as to call for eliminating the education department. He is getting help from the world's richest man, Elon Musk, who, through an effort called Department of Government Efficiency (Doge), has sent workers to comb through data at federal agencies and helped implement the "buyout" offer.Last week, the Trump administration ordered agencies to fire nearly all probationary employees, those who had generally been in their positions for less than a year and not yet earned job protection. That included the NNSA staff members.Altogether, the move could potentially affect hundreds of thousands of people. Several of the Trump administration's efforts to shrink the government's size and spending have been met with legal challenges. More than 60 lawsuits have been filed against the Trump administration since the president was inaugurated on 20 January.]]></content:encoded></item><item><title>How Medical Research Cuts Would Hit Colleges and Hospitals in Every State</title><link>https://www.nytimes.com/interactive/2025/02/13/upshot/nih-trump-funding-cuts.html</link><author>erickhill</author><category>hn</category><pubDate>Sun, 16 Feb 2025 02:07:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A proposal by the Trump administration to reduce the size of grants for institutions conducting medical research would have far-reaching effects, and not just for elite universities and the coastal states where many are located.Also at risk could be grants from the National Institutes of Health to numerous hospitals that conduct clinical research on major diseases, and to state universities across the country. North Carolina, Missouri and Pennsylvania could face disproportionate losses, because of the concentration of medical research in those states.Based on spending in the 2024 fiscal year.In the 2024 fiscal year, the N.I.H. spent at least $32 billion on nearly 60,000 grants, including medical research in areas like cancer, genetics and infectious disease. Of that, $23 billion went to ‚Äúdirect‚Äù research costs, such as microscopes and researchers‚Äô salaries, according to an Upshot analysis of N.I.H. grant data.The other $9 billion went to the institutions‚Äô overhead, or ‚Äúindirect costs,‚Äù which can include laboratory upkeep, utility bills, administrative staff and access to hazardous materials disposal, all of which research institutions say is essential to making research possible.The N.I.H. proposal, which has been put on hold by a federal court, aims to reduce funding for those indirect costs to a set 15 percent rate that the administration says would save about $4 billion a year. The Upshot analysis estimates that a 15 percent rate would have reduced funding for the grants that received N.I.H. support in 2024 by at least $5 billion. The White House said the savings would be reinvested in more research, but the rate cuts would open up sizable budget holes in most projects at research institutions.It is not clear whether those organizations can fill the gaps with other funding sources or by shifting how they apply for grants. Instead, many officials at universities and hospitals have said that they may have to pull back on medical or scientific research.‚ÄúIt‚Äôs not an overstatement to say that a slash this drastic in total research funding slows research,‚Äù said Heather Pierce, senior director for science policy at the Association of American Medical Colleges, which has sued along with other education and hospital associations to block the policy. And slower scientific progress, she said, would affect anyone who depends on the development of new treatments, medical interventions and diagnostic tools.We estimate that virtually all universities and hospitals would see fewer funds on similar projects in the future. The 10 institutions that receive the most money from N.I.H. stand to lose more than $100 million per year on average.To understand how the change would work, let‚Äôs look at one grant for about $600,000 sent last year to the University of Alabama at Birmingham to study whether exercise can improve memory for people with epilepsy.The calculation above, which we have repeated for every grant paid last year, is a bit simplified. In reality, the researchers would lose even more money than we‚Äôve shown, because of the way indirect funding is calculated (see our methodology at the bottom of this article).Our analysis also makes some other conservative assumptions given the policy‚Äôs uncertainty. We assume, for instance, that the new 15 percent rate is a flat rate that all grantees would receive, and not a maximum rate (a distinction left unclear in the N.I.H. guidance). We also assume that the change applies not just to institutions of higher education, but also to all kinds of grantees, including hospitals.In a statement, the White House indicated it would reserve any savings for additional research grants. ‚ÄúContrary to the hysteria, redirecting billions of allocated N.I.H. spending away from administrative bloat means there will be more money and resources available for legitimate scientific research, not less,‚Äù said Kush Desai, a White House spokesman.The N.I.H. announcement, however, coincides with the Trump administration‚Äôs moves to cut spending across the government, and with the N.I.H.‚Äôs withholding of funding for grants ‚Äî their direct and indirect costs alike ‚Äî in apparent conflictwith separate court orders.The N.I.H. guidance document includes a number of conflicting statements and statistics the Upshot could not reconcile. The N.I.H. also declined to answer questions about the policy and about its public-facing data tracking grant spending.The N.I.H. since 1950 has provided these overhead funds in a formulaic way, and since 1965, the government has used a rate individually calculated for each institution. Federal officials review cost summaries, floor plans and other information to determine that rate. That number can be higher for institutions in more expensive parts of the country, or for those that use more energy-intensive equipment. The proposal from the Trump administration would set aside those differences in standardizing the rate at 15 percent for every grantee.The lists below estimate what would have happened to the 10 universities and hospitals that received the most N.I.H. grant money in the 2024 fiscal year, if the formula change had been in effect then.Largest N.I.H. grant recipients among colleges, universities and medical schoolsUniversity of California, San FranciscoUniversity of PennsylvaniaColumbia University Health SciencesSource: National Institutes of HealthBased on spending in the 2024 fiscal year.Largest N.I.H. grant recipients among hospitalsMassachusetts General HospitalVanderbilt University Medical CenterBrigham and Women‚Äôs HospitalBoston Children‚Äôs HospitalUniversity of Texas MD Anderson Cancer CenterChildren‚Äôs Hospital of PhiladelphiaDana-Farber Cancer InstituteCincinnati Childrens Hospital Medical CenterBeth Israel Deaconess Medical CenterCedars-Sinai Medical CenterSource: National Institutes of HealthBased on spending in the 2024 fiscal year, which extends from Oct. 1 to Sept. 30.If courts allow the change to move forward, some of its consequences are hard to predict.Advocates for the policy change note that these organizations receive numerous other federal subsidies. Most universities and research hospitals are nonprofits that pay no federal taxes, for example. The N.I.H. announcement also noted that these same institutions often accept grants from charitable foundations that offer much lower overhead rates than the federal government, a signal that universities and hospitals willingly pursue research opportunities with less supplemental funding.Because the indirect payments are based on broad formulas and not specific line items, critics say institutions may be diverting these federal dollars into unaccountable funds to pay for programs that taxpayers can‚Äôt see, such as the kinds of diversity, equity and inclusion programs targeted by the Trump administration.‚ÄúThat‚Äôs how you get things like the ability of administrators to use larger overhead pools of money to build out D.E.I. bureaucracies, or to fund Ph.D. programs in the humanities,‚Äù said Jay Greene, a senior research fellow in the Center for Education Policy at the Heritage Foundation, a conservative research group. Mr. Greene was the coauthor of a 2022 article urging the N.I.H. to cut or eliminate indirect grant funding. But he did not have specific examples to cite of research funds being spent in this way.Researchers say the indirect funds have a branding problem, but are a necessary component of research.‚ÄúThe term ‚Äòindirect costs‚Äô or the alternative term ‚Äòoverhead‚Äô sounds dangerously close to ‚Äòslush fund‚Äô to some people,‚Äù said Jeremy Berg, who was the director of the National Institute of General Medical Sciences at the N.I.H. from 2003 to 2011. ‚ÄúThere are real costs somebody has to pay for, and heating and cooling university laboratory buildings is a real cost.‚ÄùSome grant recipients already receive low overhead payments, but a large majority of them currently receive more than 15 percent, meaning they will need to make budgetary changes to absorb the loss. Among the 2024 grants that we analyzed, institutions that received more than $1 million in N.I.H. support got an average of 40 cents of indirect funding for every dollar of direct funding.Distribution of overhead funding at N.I.H.-funded institutions in 2024As a share of direct fundingSource: National Institutes of HealthCalculated for 613 institutions that received at least $1 million in funding in fiscal year 2024. Federally negotiated rates are higher than these.Universities and hospitals may adjust their overall budgets to keep supporting medical research by cutting back on other things they do. Some might be able to raise money from donors to fill the shortfalls, though most universities are already raising as much philanthropic money as they can.But many research institutions have said they would adjust by simply doing less medical research, because they would not be able to afford to do as much with less government help.Universities and hospitals might also shift the kinds of research they do, avoiding areas that require more lab space, regulatory compliance or high-tech equipment, and focusing on types of research that will require them to provide less overhead funding themselves. That may mean disproportionate reductions in complex areas of research like genetics.Those effects may be spread unevenly across the research landscape, as some organizations find a way to adjust, while others abandon medical research altogether.We‚Äôve compiled a list of institutions that received at least $1 million in N.I.H. funding in the 2024 fiscal year, along with our estimates of how much less they would have gotten under the new policy. Most of these institutions are universities or hospitals, but there are also some private companies and nonprofit research groups. Our numbers tend to be underestimates of the cuts.To estimate changes in funding, we relied on data from RePORT, the N.I.H.‚Äôs online registry of grants and projects. We limited our analysis to grants listed within the 50 U.S. states, the District of Columbia or Puerto Rico. We also limited it to grants where the amount of indirect funding was known and where the combined indirect and direct funding was within five percent of the listed total funding. These filters resulted in removing many grants to private organizations such as domestic for-profits.We calculated how much indirect funding each grant would have received under the new guidance by multiplying the listed direct funding amount by 15 percent. We then compared that number to the listed indirect funding amount for each great to estimate the impact of the policy.There are two reasons our calculations are most likely conservative estimates of true reductions in funding. First, only a portion of the direct funding for each grant is considered to be ‚Äúeligible‚Äù for the purposes of calculating indirect funding. For example, laboratory equipment and graduate student tuition reimbursements are deducted from the direct costs before applying the negotiated overhead rate, whereas our calculations assumed 100 percent of the listed direct costs would be eligible. We performed a more accurate version of our calculations for the 10 universities and 10 hospitals receiving the most N.I.H. funds by inferring their eligible direct costs from their reported negotiated rates. When we did this, we saw an additional increase in losses of about 20 percent.Second, we applied a 15 percent rate to all grants in the database, including those with an initial indirect rate  15 percent. An analysis by James Murphy helped inform this approach. According to our analysis, then, some grants would actually receive more money under the new guidance. If the new rate operated more like a cap ‚Äî and grants with rates currently below 15 percent did not change ‚Äî the overall reductions in funding would be larger, as the reductions would no longer be offset by some small number of funding increases.]]></content:encoded></item><item><title>The Sims Game Design Documents (1997)</title><link>https://donhopkins.com/home/TheSimsDesignDocuments/</link><author>krykp</author><category>hn</category><pubDate>Sun, 16 Feb 2025 01:06:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Blunderchess.net ‚Äì blunder for your opponent every five moves</title><link>https://blunderchess.net/</link><author>eviledamame</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Feb 2025 00:22:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jellyfin: The Free Software Media System</title><link>https://jellyfin.org/</link><author>doener</author><category>hn</category><pubDate>Sat, 15 Feb 2025 22:39:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Jellyfin is Free Software, licensed under the GNU GPL. You can use it, study it, modify it, build it, and distribute it for free, as long as your changes are licensed the same way.The project relies entirely on contributions from volunteers. Want to help out? There‚Äôs lots of ways to do so, and you don‚Äôt even have to code! See our contribution guide for more details.The Jellyfin server and official clients are free to download, now and always. There are no costs, hidden or otherwise, to use Jellyfin, either for yourself, for your friends, or for your company. All our incidental costs are paid through donations from users like you.Jellyfin has no tracking, phone-home, or central servers collecting your data. We believe in keeping our software open and transparent. We‚Äôre also not in the media business, so the only media you see is your own.]]></content:encoded></item><item><title>Watt The Fox?</title><link>https://h.43z.one/blog/2025-02-12/</link><author>h43z</author><category>hn</category><pubDate>Sat, 15 Feb 2025 21:32:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[It's not nothing‚Äîabout 1.5 Watt more.So, with a heavy heart, I decided to disable email notifications‚Äîeven though I really wanted to keep them‚Äîbut eliminating the white noise was my priority.I thought I had ‚Äúfixed‚Äù the problem. Of course, I blamed Microsoft, right?But the red sound indicator on my i3 status bar kept lighting up occasionally.
    And it turned out other websites were also triggering the white noise.For instance, as soon as I clicked anywhere on x.com, the noise started. Similarly, whenever I listened to a translation on translate.google.com, there was the noise.So now I was really curious. What is going on here.I started to look up how you can play audio with HTML/JavaScript.
    There seem to be two ways: Either with the  tag or the WebAudio API.As Outlook plays sound dynamically, I knew it must use the WebAudio API.
    And to do anything with audio, you first have to create an .const audioCtx = new AudioContext();And already here I realized the problem. Just creating this AudioContext makes my speakers play white noise.The MDN article is pretty clear about it.AudioContext.suspend()
Suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process.

AudioContext.resume()
Resumes the progression of time in an audio context that has previouslyYet, most websites never bother suspending the AudioContext and create one without the immediate need for playing sound.Chrome stops the battery/CPU waste automatically afte some time. Firefox not. It just keeps playing the whitenoise.I understand that the websites are to blame here.But still, Cmon Firefox, protect me from this resource theft?!Oh and btw I suspect this also wastes my bluetooth headphones battery if they are connected?! Once I do a click on x.com the sending of white noise starts.To address this total mess, I created an extension that automatically suspends the AudioContext while also
tries to resume it if the websites wants to play sound.It's not perfect as resuming takes a little bit of time and it
  may not always resume, as there are multiple paths to starting audio. But it's good enough for me.Some Relevant Bugzilla reports

More fun stuff at h.43z.one.
Unshadow ban me at ùïè]]></content:encoded></item><item><title>The European Vat Is Not a Discriminatory Tax Against US Exports</title><link>https://taxfoundation.org/blog/trump-reciprocal-tariffs-eu-vat-discriminatory/</link><author>dzogchen</author><category>hn</category><pubDate>Sat, 15 Feb 2025 21:20:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The Trump administration has once again floated the idea of ‚Äúreciprocal‚Äù tariffs on foreign countries. While it is unclear what formula the administration will use to determine what is ‚Äúreciprocal,‚Äù the intention of responding to foreign charges‚Äîreal and perceived‚Äîis clear enough.In the past, the administration has made general assertions about different  and nontariff barriers that American exporters face that should be rectified by ‚Äúreciprocal‚Äù US tariffs. Trump commonly mentions that the EU charges a 10 percent import  on US vehicles while the US only levies a 2.5 percent tariff on European cars coming into the US. Though one can certainly find examples of higher trade barriers abroad, the overall tariff gap between the US and its trading partners is relatively minor‚Äîand any increase in US tariffs will ultimately be paid by US businesses and consumers.  However, when discussing trade with the EU specifically, White House deputy chief of staff, Stephen Miller, added a new policy grievance to the mix: value-added taxes (VAT).  ‚ÄúDid you know when you ship a car from the US to Europe, if they let it in at all because they have many nontariff barriers, between the VAT and duties, that car is taxed at 30%? The German car‚Äîor a European car sent the America is taxed at 2.5%‚Äîor basically 0.‚ÄùHis statement assumes that a VAT discriminates against American car exports like a tariff, and conversely, that the VAT rebate provided to European car producers exporting to the US constitutes a subsidy and the car then simply faces a tariff and no VAT. (It is worth noting that both a domestic automobile and a European car sold in the US would face US state .)  may seem like a compelling political argument to justify across-the-board tariffs on the EU, it instead reflects a complete misunderstanding of what a VAT is and how it works. Worse, it misplaces the blame for a lack of US competitiveness on the European VAT instead of reevaluating the flaws of both the US federal and state tax systems.  What is VAT and how does it work for exported goods?VATs are border-adjusted, meaning they rebate tax on exports and impose tax on imports. Despite the appearance of subsidizing exports and punishing imports, however, a border-adjusted VAT is trade neutral. A border adjusted tax leads to currency appreciation for the imposing country, which would make it cheaper to import goods, more expensive to export goods, and thus would cancel out the apparent benefits of the tax on imports and the rebate on exports.If there is a complaint to be made about tax policy and implications for US competitiveness in Europe, it is about uncompetitive state sales tax structures in the US system that yield what is known as ‚Äú.‚ÄùWhat is US sales tax and how does it work for exported goods? Unlike most countries, the United States does not impose a broad-based  at the national (federal) level, and state-level consumption taxes are designed as general sales taxes rather than value-added taxes. Whereas a VAT is imposed on the incremental increase in value of a good or service at each stage of production, a sales tax is imposed on the total transaction price of any taxed good or service.If a sales tax is imposed exclusively on final consumption, then VATs and sales taxes are economically identical. However, when the sales tax is applied to some intermediate transactions (‚Äúbusiness inputs‚Äù), it results in tax pyramiding, where the tax is embedded in the price multiple times over.Consider the following example of a 5 percent VAT and two versions of a 5 percent sales tax‚Äîone which only applies to final consumption, and one which applies to certain intermediate transactions as well.VATs and Ideal Sales Taxes are Economically IdenticalA 5% VAT compared to a 5% ideal sales tax and a 5% sales tax with business input taxationNote that, while a VAT is imposed at every stage of the process, the net effect is to apply the rate one time to the final sales price. The tax is collected in increments (on the ‚Äúvalue added‚Äù at each stage), but unlike with a pyramiding sales tax, it does not double tax inputs. The VAT and ideal sales tax share an identical  and, if imposed at the same rates, yield identical collections.US sales taxes are typically destination-based, meaning that the tax is owed where the product is received or consumed. If a European resident orders from a US retailer, they do not pay US sales tax, just like a US consumer can obtain a VAT rebate on purchases of European products. Neither is a subsidy. These are simply consumption taxes falling on the consumer.In practice, however, US sales taxes diverge sharply from the ideal. More than 40 percent of US sales tax revenue comes from intermediate transactions, which impose costs on US producers. This design flaw is not present in VATs, which do not double-tax intermediate transactions. Consequently, the sales tax imposes a penalty on domestic production that a VAT (or a better designed sales tax) would not. European VATs aren‚Äôt subsidizing anything‚ÄîUS states are just shooting themselves in the foot.Crucially, this is true in domestic as well as international sales. If a state‚Äôs sales tax only applied to final consumption, it would never put in-state businesses at a disadvantage against rivals in other states, because consumers elsewhere are subject to their own state‚Äôs sales tax. A Maryland resident pays 6 percent sales tax on whatever she orders (that‚Äôs subject to Maryland‚Äôs sales tax), regardless of whether she buys from a retailer in Maryland, or Delaware (with no sales tax), or Louisiana (with an average rate north of 10 percent). But when Maryland taxes business inputs, that imposes a cost on Maryland businesses that could be mitigated if businesses operated in lower-tax states or in states which include fewer inputs in their tax base.The disadvantages created by the sales tax, therefore, aren‚Äôt unique to goods exported abroad. They aren‚Äôt the consequence of trade policy, but of poor tax policy. Europe‚Äôs VATs are not tariffs and are not subsidizing European exports. Instead, US states‚Äô poorly-designed sales taxes are harming their own businesses‚Äô competitiveness‚Äîwhether they‚Äôre selling down the street, across state lines, or around the world.What competitiveness issues remain with the US federal tax system?Just like state sales tax systems can create a competitive disadvantage for producers, certain elements of the federal income tax system harm incentives to invest domestically. Despite progress made by the 2017 Tax Cuts and Jobs Act, the US maintains long  schedules for structures investment, now requires amortization for research and development expenses, and is phasing out  for machinery and equipment investment. The absence of full, immediate deductions for investment increases the cost of capital, and thus discourages investment and wage growth.Rather than focus on raising tariffs, which increase the cost of operating in the United States and reduce total output and productivity, fiscal policy reforms to improve the structure of the federal income tax system can better boost competitiveness of the US manufacturing sector.Countries have many reasons why they apply different tariff rates to different products. In the case of the United States, some tariffs date back to the 1930s Smoot-Hawley tariff schedule, while other US trade barriers take on non-tariff forms. The Trump administration appears to be moving in a ‚Äúreciprocal‚Äù policy direction despite the significant negative economic consequences for American consumers of across-the-board tariffs on goods coming into the US. However, the EU‚Äôs VAT system should not be used as a justification for retaliatory tariffs. Stay informed on the tax policies impacting you.Subscribe to get insights from our trusted experts delivered straight to your inbox.Subscribe]]></content:encoded></item><item><title>NASA has a list of 10 rules for software development</title><link>https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm</link><author>vyrotek</author><category>hn</category><pubDate>Sat, 15 Feb 2025 20:24:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[NASA has a list of 10 rules for software developmentThose rules were written from the point of view of people writing
embedded software for extremely expensive spacecraft, where tolerating
a lot of programming pain is a good tradeoff for not losing a mission.
I do not know why someone in that situation does not use the SPARK
subset of Ada, which subset was explicitly designed for verification,
and is simply a better starting point for embedded programming than C.
I am criticising them from the point of view of people writing
programming language processors (compilers, interpreters, editors)
and application software.
We are supposed to teach critical thinking.  This is an example.
How have Gerard J. Holzmann's and my different contexts affected
our judgement?
Can you blindly follow his advice without considering 
context?
Can you blindly follow  advice without considering
your context?
Would these rules necessarily apply to a different/better
programming language?  What if function pointers
were tamed?  What if the language provided opaque abstract
data types as Ada does?
1. Restrict all code to very simple control flow constructs ‚Äî
do not use  statements,
 or  constructs,
and direct or indirect .Note that  and 
are how C does exception handling, so this rule bans any use
of exception handling.

It is true that banning recursion and jumps and loops without
explicit bounds means that you  your program is
going to terminate.  It is also true that recursive functions
can be proven to terminate about as often as loops can, with
reasonably well-understood methods.  What's more important here is
that ‚Äúsure to terminate‚Äù does not imply
‚Äúsure to terminate in my lifetime‚Äù:
    int const N = 1000000000;
    for (x0 = 0; x0 != N; x0++)
    for (x1 = 0; x1 != N; x1++)
    for (x2 = 0; x2 != N; x2++)
    for (x3 = 0; x3 != N; x3++)
    for (x4 = 0; x4 != N; x4++)
    for (x5 = 0; x5 != N; x5++)
    for (x6 = 0; x6 != N; x6++)
    for (x7 = 0; x7 != N; x7++)
    for (x8 = 0; x8 != N; x8++)
    for (x9 = 0; x9 != N; x9++)
        -- do something --;
This does a bounded number of iterations.  The bound is N.
In this case, that's 10.  If each iteration of the loop body
takes 1 nsec, that's 10 seconds, or about 7.9√ó10
years.  What is the  difference between ‚Äúwill stop
in 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
years‚Äù and ‚Äúwill never stop‚Äù?

Worse still, taking a problem that is  expressed
using recursion and contorting it into something that manipulates an
explicit stack, while possible, turns clear maintainable code into
buggy spaghetti.  (I've done it, several times.  There's an example
on this web site.  It is  a good idea.)

2. All loops must have a fixed upper-bound.  It must be trivially
possible for a checking tool to prove statically that a preset
upper-bound on the number of iterations of a loop cannot be exceeded.
If the loop-bound cannot be proven statically, the rule is considered
violated.This is an old idea.  As the example above shows, it is not enough
by itself to be of any practical use.  You have to try to make the
bounds reasonably , and you have to regard hitting an
artificial bound as a run-time error.

By the way, note that putting depth bounds on recursive procedures
makes them every bit as safe as loops with fixed bounds.

3. Do not use dynamic memory allocation after initialization.This is also a very old idea.  Some languages designed for embedded
work don't even  dynamic memory allocation.  The big
thing, of course, is that embedded applications have a fixed amount of
memory to work with, are never going to get any more, and should not
crash because they couldn't handle another record.

Note that the rationale actually supports a much stronger rule:
don't even  dynamic memory allocation.  You can of
course manage your own storage pool:
    typedef struct Foo_Record *foo;
    struct Foo_Record {
	foo next;
	...
    };
    #define MAX_FOOS ...
    static struct Foo_Record foo_zone[MAX_FOOS];
    foo foo_free_list = 0;

    void init_foo_free_list() {
	for (int i = MAX_FOOS - 1; i >= 0; i--) {
	    foo_zone[i].next = foo_free_list;
	    foo_free_list = &foo_zone[i];
	}
    }

    foo malloc_foo() {
	foo r = foo_free_list;
	if (r == 0) report_error();
	foo_free_list = r->next;
	return r;
    }

    void free_foo(foo x) {
	x->next = foo_free_list;
	foo_free_list = x;
    }
This  satisfies the rule, but it
violates the  of the rule.  Simulating malloc()
and free() this way is  than using the real
thing, because the memory in foo_zone is permanently tied up
for Foo_Records, even if we don't need any of those at the
moment but do desperately need the memory for something else.

What you really need to do is to use a memory allocator
with known behaviour, and to prove that the amount of memory
in use at any given time (data bytes + headers) is bounded
by a known value.

Note also that SPlint can verify at compile time that
the errors NASA speak of do not occur.

One of the reasons given for the ban is that the performance
of malloc() and free() is unpredictable.  Are these the only
functions we use with unpredictable performance?  Is there
anything about malloc() and free() which makes them
 unpredictable?  The existence of
hard-real-time garbage collectors suggests not.

The rationale for this rule says that

Note that the only way
to dynamically claim memory in the absence of memory allocation from the
heap is to use stack memory.  In the absence of recursion (Rule 1), an
upper bound on the use of stack memory can derived statically, thus
making it possible to prove that an application will always live within
its pre-allocated memory means.
Unfortunately, the sunny optimism shown here is unjustified.  Given
the ISO C standard (any version, C89, C99, or C11) it is 
to determine an upper bound on the use of stack memory.  There is not even
any standard way to determine how much memory a compiler will use for the
stack frame of a given function.  (There could have been.  There just isn't.)
There isn't even any requirement that two invocations of the same function
with the same arguments will use the same amount of memory.
Such a bound can only be calculated for a  version of a
specific compiler with specific options.  Here's a trivial example:
void f() {
    char a[100000];
}
How much memory will that take on the stack?  Compiled for debugging,
it might take a full stack frame (however big that is) plus traceback
information plus a million bytes for a[].  Compiled with optimisation,
the compiler might notice that a[] isn't used, and might even compile
calls to f() inline so that they generate no code and take no space.
That's an extreme example, but not really unfair.  If you want bounds
you can rely on, you had better  what your compiler does,
and recheck every time anything about the compiler changes.

4.  No function should be longer than what can be printed on
a single sheet of paper in a standard reference format with one line per
statement and one line per declaration.  Typically, this means no more
than about 60 lines of code per function.Since programmers these days typically read their code on-screen,
not on paper, it's not clear why the size of a sheet of paper is
relevant any longer.

The rule is arguably stated about the wrong thing.  The thing that
needs to be bounded is not the size of a function, but the size of a
chunk that a programmer needs to read and comprehend.

There are also question marks about how to interpret this if you
are using a sensible language (like Algol 60, Simula 67, Algol 68,
Pascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,
F#, Clean, Haskell, or Fortran) that allows nested procedures.
Suppose you have a folding editor that presents a procedure to
you like this:
function Text_To_Floating(S: string, E: integer): Double;
   ÔøΩ variables ÔøΩ
   ÔøΩ procedure Mul(Carry: integer) ÔøΩ
   ÔøΩ function Evaluate: Double ÔøΩ

   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;
   for N := 1 to S.length do begin
       C := S[N];
       if C = '.' then begin
          Point := -1
       end else
       if C = '_' then begin
          Base := Round(Evaluate);
          Max, Min, Power := 0, 1, 0
       end else
       if Char ‚â† ' ' then begin
          Q := ord(C) - ord('0');
          if Q > 9 then Q := ord(C) - ord('A') + 10
          Power := Point + Point
          Mul(Q)
       end
    end;
    Power := Power + Exp;
    Value := Evaluate;
    if Sign < 0 then Value := -Value;
end;
which would be much bigger if the declarations
were expanded out instead of being hidden behind ÔøΩfoldsÔøΩ.
Which size do we count?  The folded size or the unfolded size?
I was using a folding editor called Apprentice on the Classic Mac
back in the 1980s.  It was written by Peter McInerny and was lightning
fast.

5.  The  of the code should average to a minimum of
two assertions per function.Assertions are wonderful documentation and the very best debugging tool
I know of.  I have never seen any real code that had too many assertions.

The example here is one of the ugliest pieces of code I've seen in a while.
if (!c_assert(p >= 0) == true) {
    return ERROR;
}
It should, of course, just be
if (!c_assert(p >= 0)) {
    return ERROR;
}
Better still, it should be something like
#ifdef NDEBUG
#define check(e, c) (void)0
#else
#define check(e, c) if (!(c)) return bugout(c), (e)
#ifdef NDEBUG_LOG
#define bugout(c) (void)0
#else
#define bugout(c) \
    fprintf(stderr, "%s:%d: assertion '%s' failed.\n", \
    __FILE__, __LINE__, #s)
#endif
#endif
Ahem.  The more interesting part is the required density.
I just checked an open source project from a large telecoms
company, and 23 out of 704 files (not functions) contained
at least one assertion.  I just checked my own Smalltalk
system and one SLOC out of every 43 was an assertion, but
the average Smalltalk ‚Äúfunction‚Äù is only a few
lines.  If the biggest function allowed is 60 lines, then
let's suppose the average function is about 36 lines, so
this rule requires 1 assertion per 18 lines.
Assertions are good, but what they are especially good
for is expressing the requirements on data that come
from outside the function.  I suggest then that
Every argument whose validity is not guaranteed by
its typed should have an assertion to check it.
Every datum that is obtained from an external
source (file, data base, message) whose validity is
not guaranteed by its type should have an assertion
to check it.
The NASA 10 rules are written for embedded systems, where
reading stuff from sensors is fairly common.

6.  Data objects must be declared at the smallest possible level of
scope.This is excellent advice, but why limit it to data objects?
Oh yeah, the rules were written for crippled languages where you
 declare functions in the right place.

People using Ada, Pascal (Delphi), JavaScript, or functional
languages should also declare types and functions as locally as
possible.

7.  The return value of non-void functions must be checked by each
calling function, and the validity of parameters must be checked inside
each function.This again is mainly about C, or any other language that indicates
failure by returning special values.  ‚ÄúStandard libraries
famously violate this rule‚Äù?  No, the  library does.

You have to be reasonable about this: it simply isn't practical
to check  aspect of validity for 
argument.  Take the C function
void *bsearch(
    void const *key  /* what we are looking for */,
    void const *base /* points to an array of things like that */,
    size_t      n    /* how many elements base has */,
    size_t      size /* the common size of key and base's elements */
    int (*      cmp)(void const *, void const *)
);
This does a binary search in an array.  We must have key‚â†0,
base‚â†0, size‚â†0, cmp‚â†0, cmp(key,key)=0, and for all
1<i<n,
cmp((char*)base+size*(i-1), (char*)base+size*i) <= 0
Checking the validity in full would mean checking
that [key..key+size) is a range of readable addresses,
[base..base+size*n) is a range of readable addresses,
and doing n calls to cmp.  But the whole point of binary
search is to do O(log(n)) calls to cmp.

The fundamental rules here are
Don't let run-time errors go un-noticed, and
any check is safer than no check.
8. The use of the preprocessor must be limited to the inclusion of
header files and simple macro definitions.  Token pasting, variable
argument lists (ellipses), and recursive macro calls are not allowed.Recursive macro calls don't really work in C, so no quarrel there.
Variable argument lists were introduced into macros in
C99 so that you could write code like
#define err_printf(level, ...) \
    if (debug_level >= level) fprintf(stderr, __VA_ARGS__)
...
    err_printf(HIGH, "About to frob %d\n", control_index);
This is a  thing; conditional tracing like this is a
powerful debugging aid.  It should be , not banned.

The rule goes on to ban macros that expand into things that are
not complete syntactic units.  This would, for example, prohibit
simulating try-catch blocks with macros.  (Fair enough, an earlier rule
banned exception handling anyway.)  Consider this code fragment, from
an actual program.
    row_flag = border;     
    if (row_flag) printf("\\hline");
    for_each_element_child(e0, i, j, e1)
        printf(row_flag ? "\\\\\n" : "\n");
        row_flag = true;  
        col_flag = false;
        for_each_element_child(e1, k, l, e2)
            if (col_flag) printf(" & ");
            col_flag = true;
            walk_paragraph("", e2, "");
        end_each_element_child
    end_each_element_child
    if (border) printf("\\\\\\hline");
    printf("\n\\end{tabular}\n");
It's part of a program converting slides written in something like HTML
into another notation for formatting.  The 
‚Ä¶  loops walk over a tree.  Using
these macros means that the programmer has no need to know and no reason to
care how the tree is represented and how the loop actually works.
You can easily see that  must have at
least one unmatched { and  must have at least one
unmatched }.  That's the kind of macro that's banned by requiring
complete syntactic units.  Yet the readability and maintainability of
the code is  improved by these macros.

One thing the rule covers, but does not at the beginning stress, is
‚Äúno  macro processing‚Äù.  That is,
no #if.  The argument against it is, I'm afraid, questionable.  If there
are 10 conditions, there are 2 combinations to test,
whether they are expressed as compile-time conditionals or run-time
conditionals.

In particular, the rule against conditional macro processing
would prevent you defining your own assertion macros.
It is not obvious that that's a good idea.

9.  The use of pointers should be restricted.  Specifically, no more
than one level of dereferencing is allowed.  Pointer dereference
operations may not be hidden in macro definitions or inside typedef
declarations.  Function pointers are not permitted.Let's look at the last point first.

double integral(double (*f)(double), double lower, double upper, int n) {
    // Compute the integral of f from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += f((lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += f(lower + h*i);
    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);
}
This kind of code has been important in numerical calculations since
the very earliest days.  Pascal could do it.  Algol 60 could do it.
In the 1950s, Fortran could do it.  And NASA would ban it, because in
C,  is a function pointer.

Now it's important to write functions like this once and only once.
For example, the code has at least one error.  The comment says n+1
points, but the function is actually evaluated at 2n+1 points.  If we
need to bound the number of calls to f in order to meet a deadline,
having that number off by a factor of two will not help.
It's nice to have just one place to fix.
Perhaps I should not have copied that code from a well-known source (:-).
Certainly I should not have more than one copy!

What can we do if we're not allowed to use function pointers?
Suppose there are four functions foo, bar, ugh, and zoo that we need
to integrate.  Now we can write
enum Fun {FOO, BAR, UGH, ZOO};

double call(enum Fun which, double what) {
    switch (which) {
        case FOO: return foo(what);
        case BAR: return bar(what);
        case UGH: return ugh(what);
        case ZOO: return zoo(what);
    }
}

double integral(enum Fun which, double lower, double upper, int n) {
    // Compute the integral of a function from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += call(which, (lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += call(which, lower + h*i);
    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);
}
Has obeying NASA's rule made the code more reliable?  No, it has made
the code  to understand,  maintainable, and
 that it wasn't before.  Here's a call
illustrating the mistake:
x = integral(4, 0.0, 1.0, 10);I have checked this with two C compilers and a static checker at their
highest settings, and they are completely silent about this.

So there are legitimate uses for function pointers, and simulating
them makes programs , not better.

Now  in Fortran,
Algol 60, or Pascal.  Those languages had procedure 
but not procedure . You could pass a subprogram name as
a parameter, and such a parameter could be passed on, but you could not
store them in variables.  You could have a  of C which
allowed function pointer parameters, but made all function pointer
variables read-only.  That would give you a statically checkable subset
of C that allowed integral().

The other use of function pointers is simulating object-orientation.
Imagine for example
struct Channel {
    void (*send)(struct Channel *, Message const *);
    bool (*recv)(struct Channel *, Message *);
    ...
};
inline void send(struct Channel *c, Message const *m) {
    c->send(c, m);
}
inline bool recv(struct Channel *c, Message *m) {
    return c->recv(c, m);
}
This lets us use a common interface for sending and receiving
messages on different kinds of channels.  This approach has been
used extensively in operating systems (at least as far back as
the Burroughs MCP in the 1960s) to decouple the code that uses
a device from the actual device driver.     I would expect any
program that controls more than one hardware device to do something
like this.  It's one of our key tools for controlling complexity.
Again, we can simulate this, but it makes adding a new kind of
channel harder than it should be, and the code is 
when we do it, not better.

The rule against more than one level of dereferencing is also
an assault on good programming.  One of the key ideas that was
developed in the 1960s is the idea of ;
the idea that it should be possible for one module to define a
data type and operations on it and another module to use instances
of that data type and its operations without having to know
anything about what the data type is.
One of the things I detest about Java is that it spits in the
face of the people who worked out that idea.  Yes, Java (now) has
generic type parameters, and that's good, but you cannot use a
 type without knowing what that type is.

Suppose I have a module that offers operations
And suppose that I have two interfaces in mind.  One of them
uses integers as tokens.
// stasher.h, version 1.
typedef int token;
extern token stash(item);
extern item  recall(token);
extern void  delete(token);
Another uses pointers as tokens.
// stasher.h, version 2.
typedef struct Hidden *token;
extern  token stash(item);
extern  item  recall(token);
extern  void  delete(token);
void snoo(token *ans, item x, item y) {
    if (better(x, y)) {
	*ans = stash(x);
    } else {
	*ans = stash(y);
    }
}
By the NASA rule, the function snoo() would not be accepted or rejected on
its own merits.  With stasher.h, version 1, it would be accepted.
With stasher.h, version 2, it would be rejected.

One reason to prefer version 2 to version 1 is that version 2 gets
more use out of type checking.  There are ever so many ways to get an
int in C.  Ask yourself if it ever makes sense to do
token t1 = stash(x);
token t2 = stash(y);
delete(t1*t2);
I really do not like the idea of banning abstract data types.

10.  All code must be compiled, from the first day of development,
with all compiler warnings enabled at the compiler‚Äôs
most pedantic setting.  All code must compile with these setting without
any warnings.  All code must be checked daily with at least one, but
preferably more than one, state-of-the-art static source code analyzer
and should pass the analyses with zero warnings.This one is good advice.  Rule 9 is really about making your code
worse in order to get more benefit from limited static checkers.  (Since
C has no standard way to construct new functions at run time, the set of
functions that a particular function pointer  point to can
be determined by a fixed-point data flow analysis, at least for most
programs.)  So is rule 1.  



]]></content:encoded></item><item><title>Perplexity Deep Research</title><link>https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research</link><author>vinni2</author><category>hn</category><pubDate>Sat, 15 Feb 2025 20:07:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My Life in Weeks</title><link>https://weeks.ginatrapani.org/</link><author>bookofjoe</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:34:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New SF public health chief was part of McKinsey opioid-marketing operation</title><link>https://sfstandard.com/2025/02/14/san-francisco-department-public-health-daniel-tsai-opioids-mckinsey/</link><author>iancmceachern</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:32:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Dr. David Juurlink, an expert on tramadol, called the drug a ‚Äúminor player in the opioid crisis, but a player nevertheless.‚ÄùHe added, ‚ÄúTo the extent that McKinsey helped advertise it as a notionally safer opioid, I think they did a disservice in doing so. The main reason I say tramadol is a minor player is because it wasn‚Äôt prescribed like candy, like OxyContin was.‚ÄùMcKinsey‚Äôs method of targeting high-volume prescribers was part of its playbook to juice opioid sales, despite mounting evidence that the drugs could be highly addictive. In other emails uncovered in the McKinsey documents, employees wrote excitedly about finding doctors who were willing to write opioid prescriptions. In one instance in 2015, a McKinsey partner wrote, ‚ÄúThe challenge which we need to start working on is to identify the sweet spot of docs so we can do targeting. ‚Ä¶ Fun be[g]ins on Monday!‚ÄùSan Francisco‚Äôs fentanyl crisis is part of a broader trend of opioid overdoses that traces back to the 1990s, when prescription opioids became popular among doctors for chronic pain management. Companies such as Purdue Pharma, which manufactured OxyContin, brought in consulting firms like McKinsey to help with sales strategies. After a Department of Justice probe and settlement, McKinsey acknowledged that it knew the dangers of OxyContin but continued working with Purdue Pharma ‚Äî even after several of the drugmaker‚Äôs executives pled guilty in 2007 to misrepresenting addiction risks.¬†McKinsey, along with a slew of drug companies and pharmacies, agreed to pay billions in settlement funds over their roles in fueling opioid addiction. California received roughly $60 million from the 2021 McKinsey settlement. San Francisco, under the 2023 settlement of an opioid-related lawsuit, was expected to receive about $230 million from Walgreens.In both instances, the funds were slated to be used for opioid recovery efforts.In 2019, McKinsey said it would no longer work on opioid-related businesses.¬†Last year, McKinsey formally apologized for its Purdue Pharma work, saying it was ‚Äúdeeply sorry‚Äù for its role in selling OxyContin. ‚ÄúThis terrible public health crisis and our past work for opioid manufacturers will always be a source of profound regret for our firm,‚Äù the company said in a statement.At the San Francisco Department of Public Health, Tsai replaced Dr. Grant Colfax, who took the reins in 2019 and led the city through the pandemic before stepping down in January. The role paid $546,133 in 2024, one of the highest city salaries.¬†¬†On Monday, the San Francisco Health Commission unanimously nominated Tsai as director of Public Heath. Dr. Laurie Green, president of the commission, said the governing body conducted a ‚Äúmulti-hour‚Äù interview.]]></content:encoded></item><item><title>Schemesh: Fusion between Unix shell and Lisp REPL</title><link>https://github.com/cosmos72/schemesh</link><author>cosmos0072</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:00:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Multiple Russian Threat Actors Targeting Microsoft Device Code Authentication</title><link>https://www.volexity.com/blog/2025/02/13/multiple-russian-threat-actors-targeting-microsoft-device-code-authentication/</link><author>ChrisArchitect</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:59:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Multiple Russian Threat Actors Targeting Microsoft Device Code Authenticationby Charlie Gardner, Steven Adair, Tom LancasterVolexity has observed multiple Russian threat actors conducting social-engineering and spear-phishing campaigns targeting organizations with the ultimate goal of compromising Microsoft 365 accounts via Device Code Authentication phishing.Device Code Authentication phishing follows an atypical workflow to that expected by users, meaning users may not recognize it as phishing.Recent campaigns observed have been politically themed, particularly around the new administration in the United States and the changes this might mean for nations around the world.Starting in mid-January 2025, Volexity identified several social-engineering and spear-phishing campaigns by Russian threat actors aimed at compromising Microsoft 365 (M365) accounts. These attack campaigns were highly targeted and carried out in a variety of ways. The majority of these attacks originated via spear-phishing emails with different themes. In one case, the eventual breach began with highly tailored outreach via Signal.Through its investigations, Volexity discovered that Russian threat actors were impersonating a variety of individuals in order to socially engineer targets, including impersonating individuals from the following:United States Department of StateUkrainian Ministry of DefenceEuropean Union ParliamentProminent research institutionsCommunications carried a variety of different themes and messages, but they all ultimately resulted in the attacker inviting the targeted user to one of the following:Microsoft Teams Meeting / Video ConferenceAccess to applications and data as an external M365 userJoin a chatroom on a secure chat applicationWhen these attacks were successful and the attackers gained access to accounts, the post-exploitation phase often had unique characteristics in each case:The way the attackers accessed material from compromised organizations (scripts versus native applications)The infrastructure used to access stolen accountsDespite the differences, Volexity found the attacks had one thing in common: they were all Device Code Authentication attacks. While this attack method is not new, it is one that is definitely lesser known and not commonly leveraged by nation-state actors. Details on the social-engineering and spear-phishing campaigns, along with how Device Code Authentication attacks work, will be covered further in this blog post. What Volexity has observed is that this method has been more effective at successfully compromising accounts than most other targeted spear-phishing campaigns.Volexity assesses with high confidence that the series of attacks described in this blog post are from Russia-based threat actors. At this time, Volexity is tracking this activity under three different threat actors and assesses with medium confidence that at least one of them is  (overlapping with DarkHalo, APT29, Midnight Blizzard, CozyDuke). Volexity is tracking the remaining activity under  and . It is possible that all the activity described in this blog post is a single threat actor, but despite the similar targeting, timing, and attack method, other observed components of the operations are different enough to be tracked separately, for now.From Secure Chat to Insecure AuthenticationThe discovery of this threat activity started toward the end of January 2025, when Volexity uncovered a highly targeted attack that had successfully compromised the M365 account of one of its customers. This breach was discovered after Volexity identified suspicious sign-in activity to the account, which was followed by a rapid download of files from the user's OneDrive. All authentication and download events came from virtual private server (VPS) and Tor IP addresses, which is not the most subtle way to access an account. Volexity noted this activity was likely scripted, as the User-Agent string for later access and file downloads was the Python User-Agent string .Volexity then performed a detailed investigation into this incident, in an effort to identify how the account was compromised. A review of login activity showed the legitimate user had logged in and approved a multi-factor authentication (MFA) request. However, subsequent access was not from the legitimate user's IP address. This caused Volexity to initially suspect a phishing attack involving an adversary-in-the-middle (AiTM) framework. As a result, Volexity reviewed emails to the user leading up the time of the authentication event. This review identified a suspicious email just moments before the login activity from an email address purporting to be from someone with the name of a high-ranking official from the Ukrainian Ministry of Defence. The email was structured to look like a meeting invite for a chatroom on the messaging application, Element. Element is another encrypted messaging application that offers the ability for users to self-host a server with functionality that includes group video chats. The ‚Äúinvitation‚Äù email sent is shown below .Microsoft describes the purpose of this workflow as allowing '"users to sign in to input-constrained devices such as a smart TV, IoT device, or a printer.‚Äù However, in this case, it means if an attacker can convince a user to enter a specific code into this dialogue (and log in), they are granted long-term access to the user‚Äôs account.After working with its customer more closely, Volexity learned that the victim had been contacted on Signal by an individual purporting to be from the Ukrainian Ministry of Defence. This individual then requested the victim move off Signal to another secure chat application called Element. The attacker then had the victim join an Element server they controlled under the domain . This allowed the attacker to further communicate with the victim in real time and inform them they needed to click a link from an email to join a secure chat room. This is where the email Volexity had discovered came into play. The message was a ploy to fool the user into thinking they were being invited into a secure chat, when in reality they were giving the attacker access to their account. The generated Device Codes are only valid for 15 minutes once they are created. As a result, the real-time communication with the victim, and having them expect the "invitation", served to ensure the phish would succeed through timely coordination.The diagram below sVolexity tracks the threat actor behind this campaign as . Through research conducted on the custom domain used by UTA0304 to operate its own Element server, Volexity was able to pivot and discover additional infrastructure it believes is likely operated by the group. The table below represents the list of infrastructure that Volexity has tied to this threat actor.chromeelevationservice[.]comSpoofing the United States Department of StateIn early February 2025, Volexity observed multiple spear-phishing campaigns targeting users with fake Microsoft invitations purporting to be from the United States (US) Department of State. These emails were themed as invitations to join the US Department of State‚Äôs Microsoft tenant as an external user, or as invitations to a Microsoft Teams chat named ‚ÄúMeasuring Influence Operations".Similar to the campaign conducted by UTA0304, these fake US Department of State emails were targeting users with a Device Code OAuth phishing workflow. Each email was aimed at convincing the user to accept the invitation and enter a unique code provided in the phishing email. The link in the invitations would direct users to the Microsoft Device Code authentication page. If the user entered the code provided in the phishing email, the authentication page would subsequently authorize the threat actor to access to the user‚Äôs account. However, it is worth noting that this campaign was sent out of the blue, with no precursor or build up to the emails, so users would not be expecting these messages. Even if they were to fall for the campaign, they would have to have done it within 15 minutes of receiving the email. This dramatically decreased the likelihood that this attack would be successful.After reviewing various parts of the attack, Volexity assesses with medium confidence that the Russian threat actor CozyLarch (aka APT29 or Midnight Blizzard) was behind these US Department of State themed spear-phishing campaigns. Additional details on each campaign are described in the sections that follow.Campaign 1: M365 Tenant External User InvitationCozyLarch sent invitations to several users, inviting them to access applications within the M365 tenant for the US Department of State. The invitation email was designed to look like a real invitation that would be sent from Microsoft, as shown below.The redirect link takes the user to the Microsoft Device Code OAuth workflow, and it is the same URL that UTA0304 directly embedded in their phishing campaign. However, unlike UTA0304, CozyLarch opted to use the redirect URL rather than the final login URL, perhaps because it may look even more recognizable to a discerning user, given that it is hosted on the main Microsoft domain. If the user entered the code provided from the email and continued through the authentication process, the attacker was granted access to the user‚Äôs M365 account.Campaign 2: M365 Teams Chat InvitationCozyLarch launched a second campaign, in which they targeted users with a fake invitation to join a Microsoft Teams chat named ‚ÄúMeasuring Influence Operations‚Äù. The email made it appear as though there were already 37 other members in the chat. ¬†A screenshot of one of the observed spear-phishing messages is shown below.The ‚ÄúSign in to Microsoft Teams‚Äù button in the email body is a hyperlink that leads to the same https://www.microsoft.com/devicelogin URL observed in the other campaign. The attack flow and end goal are the same, with only a small difference in the theme of the emails.The emails are designed to appear as though they come from Microsoft. The messages used mixed encoding in the ‚Äúfriendly‚Äù name that make the address difficult to discern. An example of the full ‚Äúfrom‚Äù header used in one phishing email is given below:\"Mic\udb40\udc30\udb40\udc30\udb40\udc30\u200br\udb40\udc30\udb40\udc30o\udb40\udc30\udb40\udc30soft Invitations on behal f of US Dep\udb40\udc30\udb40\udc30\udb40\udc30artme\udb40\udc30\udb40\udc30\udb40\udc30nt of St\udb40\udc30\udb40\udc30\udb40\udc30 ate \uff1cinvites\uff20mic\udb40\udc30\udb40\udc30\udb40\udc30\u200br\udb40\udc30\udb40\udc30o\udb40\udc30\udb40\udc30soft.co m\uff1e\u180e\u3000\u180e\u3000\u180e \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e \u180e\u3000\u180e \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e \u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e¬† \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e \u061c Cc:\" <[email¬†protected]>The attacker attempted to make it appear as if the emails were from , and also set the  header as . However, the true address could be seen at the end of the  field; all messages were sent via Google Gmail accounts. Volexity observed the following Gmail accounts as the actual senders of the messages observed:kaylassammers@gmail[.]comkendisggibson@gmail[.]comleslytthomson@gmail[.]comThese addresses are believed to be controlled by CozyLarch and can be used to reliably detect phishing emails that may have been sent.Using Wireless Proxy Networks for Email DistributionVolexity also noted that the sending IP address associated with each spear-phishing email was recorded in the headers. Looking at the  header in the messages, it became apparent that the attacker was using Proxy IP addresses based in the US to send messages. Volexity observed nearly a dozen IP addresses belonging to mobile networks in the US (AT&T and Verizon Wireless).European Parliament and Donald TrumpStarting in late January through to the publication of this blog (February 13, 2025), Volexity has observed another campaign by a Russian threat actor it tracks as  targeting numerous organizations. UTA0307 created a fake email under the identity of a member of the European Parliament who is on the Committee on Foreign Affairs. The threat actor reached out to numerous individuals with personalized emails requesting a Microsoft Teams meeting to discuss Donald Trump and his impact on relations between the US and the European Union. Volexity also observed a smaller set of campaigns centered on discussing China's foreign policy and China-European Union relations.The email subject lines used in these various campaigns are listed below:Discussion on Eastern Europe and the CaucasusDiscussion about Donald Trump's new termDiscussion about Trump & US relations with EuropeCollaboration on China and East Asia ResearchThe image below shows an example spear phish that was sent by UTA0307.None of the initial emails contained any malicious content or links at the onset. The threat actor was leveraging a tactic that has become commonplace for numerous nation-state actors, where they wait until a conversation has started prior to sending anything malicious. This serves the purpose of knowing they have an engaged target, and that the target's guard is potentially down. In the specific cases of Device Code Authentication phishing, it is especially important to have a responsive target, as the threat actor has only 15 minutes to convince the target to enter the code that has been generated.A Different Device Code OAuth Phishing TechniqueVolexity actually discovered the operations of UTA0307 following a successful compromise. Similar to the initial discovery of UTA0304, Volexity worked backwards from detecting a breach to identifying the above spear-phishing emails. In this case, the victim had engaged from the initial email and had several messages back and forth with UTA0307 regarding a meeting being set up. They agreed to join a Microsoft Teams meeting, and a fake invitation email was sent. However, this time the link in the email did not go to Microsoft. The target received an email with the subject "", and the body of the email, shown below, was designed to look like a real invitation.The ‚ÄúJoin the meeting now‚Äù hyperlink, however, linked to a website controlled by UTA0307 (). This page in turn was set up to automatically generate a new Microsoft Device Code each time it was visited. The website was designed to appear as an official Microsoft interstitial page before the user can join a Microsoft Teams meeting. The message that appears on the landing page (shown below) claims that the victim needs to pass a security check by copying a code and entering it on a subsequent pageWhen the user clicks the ‚ÄúNext‚Äù button, a new tab is opened with the real Microsoft Device Code Authentication interface that requests an authentication code. If the victim enters the code supplied by the phishing page, they grant UTA0307 access to their M365 account. Interestingly, in the background of the initial phishing page, Volexity noted that the website would continuously poll the domain . It appears this domain was set up to monitor successful Device Code Authentication and, if detected, would redirect the user to a real Microsoft Teams meeting URL in an effort to make the activity appear legitimate.The threat actor never joined this Microsoft Teams meeting. However, UTA0307 did add authorization for an authentication application under their control to enable multi-factor authentication when logging into the compromised account. Volexity assesses with medium confidence that this was a requirement of logging into the account, even with the stolen authentication token.One benefit of this attack workflow versus other previously observed DeviceID phishing workflows is that, when a DeviceID code is generated, it is only valid for 15 minutes. Having an interstitial page that automatically generates new codes means UTA0307 does not have to worry about their phishing content expiring.UTA0307 Post-compromise Activities, Targeting and AttributionVolexity observed UTA0307 exfiltrating documents from a compromised M365 account that would be of interest to a Russian threat actor. This was determined based on identification of FileDownloaded operations observed in M365 audit log data. Given this information about the threat actor‚Äôs objectives, their targeting, and their use of a highly similar technique to that used in recent days and weeks by CozyLarch and UTA0304, Volexity assesses with medium confidence that UTA0307 is also a Russian threat actor.However, the exact implementation of the DeviceID OAuth phishing technique used in this activity differs slightly from those previously documented by Volexity, which provides some evidence that this activity may have been conducted by a separate threat actor. For example, while the previously observed phishing campaigns saw the attacker use the client ID for Microsoft Office when handling Device Code Authentication, this activity instead used the client ID for Microsoft Teams, as shown below (note that Microsoft uses  and  interchangeably in their logs when referring to the ID for an application):"appDisplayName": "Microsoft Teams","appId": "1fec8e78-bce4-4aaf-ab1b-5451cc387264",Another difference between this and the UTA0304 campaign is that in this case, all subsequent access to the compromised account occurred via Mullad VPN exit nodes (versus the other observed VPS and Tor IP addresses). Based on these two factors, Volexity has chosen to track this activity under the UTA0307 alias, rather than CozyLarch or UTA0304.Detecting Device Code AuthenticationVolexity identified a way to reliably detect this attack through monitoring and analysis of Microsoft Entra ID sign-in logs. When a user enters a device code and subsequently authenticates, it results in a login to the application associated with the generated code. This can be a common application like Microsoft Office that is frequently accessed by users and would not be a reliable indicator. However, the good news is that Device Code Authentications result in the  field being set with the value .The line below is what will appear in the JSON data in the Entra ID sign-in logs when a Device Code Authentication occurs:‚ÄúauthenticationProtocol": "deviceCode",Volexity further noted that as authenticated sessions refresh and are kept alive, subsequent sign-ins that initially occurred via a  often do not have anything set for  but they contain the following entry: ‚ÄúoriginalTransferMethod": "deviceCodeFlow",These values can be searched and filtered on in the Entra Admin center by adding filters for "Authentications Protocol" and "Original Transfer Method". The latter can be filtered in both  and  sign-ins. The frequency and legitimacy of these values occurring in the sign-in logs for a particular organization may vary, as this is a legitimate Microsoft feature. An organization can evaluate their risk and usage of these workflows, and potentially use this information as a proactive detection mechanism.If an organization has the ability to monitor URLs that are being accessed by users or sent in email, there are additional detection opportunities to discover Device Code Authentication attacks. The following official URLs can be monitored for as related to Microsoft Device Code Authentication:https://login.microsoftonline.com/common/oauth2/deviceauthhttps://www.microsoft.com/deviceloginhttps://aka.ms/deviceloginOrganizations can monitor for access to these URLs or for their presence in various communication methods, such as email. Attackers can find other means to redirect users to these URLs, but one of the main advantages of using the list above in phishing attacks is that the URL displayed is hosted on a legitimate Microsoft domain.Preventing Device Code AuthenticationVolexity believes the most effective way to prevent this potential attack vector is through conditional access policies on an organization's M365 tenant. It is possible for organizations to create a conditional access policy that disallows device code authentication altogether. It is fairly trivial to set up, and Microsoft provides online guidance on exactly how to do this. Based on Volexity's own testing, blocking the "Device code flow" from "Authentications flows" prevents this attack from working.¬† The image below shows what a conditional access policy would look like once it's set up and in place to block this authentication flow.Prior to implementing such a policy, organizations should evaluate the use of Device Code Authentication in their environment. This feature is used legitimately, and blocking it could have a negative impact. Volexity's review of its own customers identified several instances of legitimate access to resources via these means. However, at the majority of Volexity's customers, there was either no recent Device Code Authentication activity or there was only activity tied to the attacks described in this blog post.Volexity continues to track multiple spear-phishing campaigns targeting Device Code Authentication. This blog post serves to cover a few of the larger and unique campaigns observed. Volexity has observed other similar spear-phishing campaigns in recent weeks targeting Device Code Authentication that it believes are the work of Russian threat actors. Further, it should be noted that it is possible this is the work of a single threat actor running multiple, different campaigns. However, at this time, Volexity believes this activity is sufficiently different enough to warrant tracking this activity under two different unknown threat actors and one it believes is likely CozyLarch.While Device Code Authentication attacks are not new, they appear to have been rarely leveraged by nation-state threat actors . Volexity's visibility into targeted attacks indicates this particular method has been far more effective than the combined effort of years of other social-engineering and spear-phishing attacks conducted by the same (or similar) threat actors. It appears that these Russian threat actors have made a concerted effort to launch several campaigns against organizations with a goal of simultaneously abusing this method before the targets catch on and implement countermeasures.The detection mechanisms and countermeasures to these attacks have been available for years. However, Volexity believes they are seldom implemented and that most organizations are not even aware of this authentication flow, let alone the means to detect its misuse. These attacks serve as a reminder that threat actors will constantly look for ways to abuse legitimate features, and organizations must continually evaluate and implement methods to detect and prevent such attacks.These attacks also serve as a good opportunity to engage with users and remind them to be on the lookout for anything out of the ordinary when it comes to accessing resources when they are asked for login credentials or authorization grants. This phishing workflow has proven useful for an attacker, as many traditional sources of evidence and detection, both for a user and network defenders, are not present. For example:There is no ‚Äúmalicious‚Äù link or attachment. The only link is to the provider‚Äôs infrastructure (in this case, Microsoft). This means users cannot easily identify the link as being suspicious, and automated solutions detecting malicious emails will likely fail to do so for the same reason.Users are generally less aware of attacks that leverage legitimate services, and may be even less aware when it comes to those that involve entering a device code rather than their username or password.After successful authentication, the logs will show the authenticating application as a legitimate or benign application, reducing signal that can be keyed off of in sign-in logs by detection teams.These are items that organizations should look to further train users on and implement technical countermeasures against where possible.Volexity GitHub.If you believe you have been targeted by a similar attack and want to share details with Volexity for informational purposes, additional investigation, or incident response, please contact us.]]></content:encoded></item><item><title>PAROL6: 3D-printed desktop robotic arm</title><link>https://source-robotics.github.io/PAROL-docs/</link><author>bo0tzz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:26:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Basketball has evolved into a game of calculated decision-making</title><link>https://nabraj.com/blog/basketball-solved-sport/</link><author>nabaraz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:21:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Basketball has evolved from a game of unpredictability into a game of calculated decision-making with the use of data and analytics. From a game of points, assists, and rebounds, it has progressed into using thousands of data points to optimize every element of the game.All decisions are made based on numbers not intuition. Long-range shooting and layups are preferred over mid-range shooting. Players are no longer do-it-alls; they are now given specialized roles.In the last decade, long-range shooting has gone from a secondary option to a primary choice for building offense. Recently, teams have realized three-pointers have higher point value despite their lower scoring percentage. This has led to a revolution in structuring an offense around taking long-range shots. The Golden State Warriors, led by Stephen Curry, probably jump-started this trend with 34 three-pointer attempts per game in the 2018-19 season, twice as much from five years ago. Celtics, this season, have averaged almost 50 three-pointers attempt this season (2024-25 season).In the past, the team built its roster around a big name like Shaq. Most of the offense were from the center. This has now changed, with the primary strategy being to stretch the opposition and take long-range shots.The 3-and-D model refers to a player, usually a wing player, who is just above average at three-pointers and plays competent defense.  Forget about positions; just get a guy who can do some 3s and Ds.Danny Green is probably the father of this model, with his 40% career three-point field goal percentage and he also made into all-defensive team.In  recent years, every team has had at least one 3-and-D model player on the roster.Gone are the days of an all-around player. There is no longer a need for a player who does everything. Look at players like Kobe Bryant and Lebron James (early career); they not only scored but guarded defense, caught rebounds and played the role of playmakers.Now, it‚Äôs all about creating lineups with specialized players. A team typically consists of a three-point shooter, a defensive specialist, a playmaker, and rebounders. They all have specific roles assigned to them.A catch-all word for statistics, technology has played a pivotal role in shaping this game. In addition to data collection, biomechanics and motion cameras track every player‚Äôs movement. NBA even brought SportVU from football; it follows the ball and supposedly captures images 25 times per second. Coaches can now use this to analyze the speed, position, form, and motion of each player on the court. In the end, it‚Äôs all about optimizing every ball possession. Basketball might have lost its flair; every move is now predictable and measured. What is the future of basketball, is anyone‚Äôs guess? Maybe a rule change is around the corner?]]></content:encoded></item><item><title>Jill ‚Äì a functional programming language for the Nand2Tetris platform</title><link>https://github.com/mpatajac/jillc</link><author>mailgolub</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:04:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alzheimer&apos;s biomarkers now visible up to a decade ahead of symptoms</title><link>https://newatlas.com/brain/alzheimers-dementia/alzheimers-biomarkers-visible-decade-before-symptoms/</link><author>01-_-</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:02:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Researchers at the University of Pittsburgh have devised a biomarker test that can spot small amounts of clumping tau protein in the brain and cerebrospinal fluid, which lead to Alzheimer's disease.Catching these clumps early while still in minute quantities can enable effective intervention. This test can help detect the tangled proteins years in advance of them appearing prominently in brain scans ‚Äì as much as up to a decade.That's heartening because Alzheimer's disease not only has devastating impact on patients' lives long term, but is also currently incurable. It begins to show up as forgetfulness, and progresses to confusion and disorientation, delusions, hallucinations, and trouble sleeping. As the condition worsens, patients may experience difficulty eating, moving around, incontinence, loss of speech, and significant memory loss.‚ÄúEarly detection is key to more successful therapies for Alzheimer‚Äôs disease since trials show that patients with little-to-no quantifiable insoluble tau tangles are more likely to benefit from new treatments than those with a significant degree of tau brain deposits," explained Thomas Karikari, senior author of the paper published in  this week.Here's a quick bit of context on what's happening in and around the brain. Humans have some 86 billion nerve cells, and they're connected by what are called synapses. These synapses are supported by 'rail tracks' that enable the flow of essential nutrients and information, and they're called microtubules.Tau proteins (abbreviated from tubulin associated unit) stabilize these microtubules, and help keep the brain healthy. These proteins can malfunction, clump together and create tangles ‚Äì preventing the microtubules from functioning properly.Here's where this new biomarker test comes in. The researchers have found an important section of the tau protein that causes it to form harmful tangles in the brain. This section is made up of 111 building blocks (amino acids). Within this section, they discovered two specific spots that, when modified, can tell doctors if tau proteins are starting to clump together. This is important because if doctors can detect these changes early enough, they might be able to treat the problem before it gets worse.The two specific spots they found (called p-tau-262 and p-tau-356) work like early warning signals, letting doctors know that tau proteins are beginning to malfunction. This could help identify Alzheimer's disease sooner, when treatments might be more effective.]]></content:encoded></item><item><title>More Solar and Battery Storage Added to TX Grid Than Other Power Src Last Year</title><link>https://insideclimatenews.org/news/10022025/solar-battery-storage-texas-grid/</link><author>indigodaddy</author><category>hn</category><pubDate>Sat, 15 Feb 2025 16:37:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As the market for renewables in Texas continues to strengthen and innovate, the power makeup of the state‚Äôs electric grid is slated to keep shifting toward adding more renewables. Last year, solar and battery storage installation led capacity growth within Texas‚Äô electric grid, according to research from the Federal Reserve Bank of Dallas published in January.¬†Texas added nearly 1,500 megawatts of battery storage to the grid‚Äôs summer rated capacities in 2023. That figure almost tripled to 4,374 megawatts added in 2024, according to the report.¬†Capacity from solar power added to the grid enjoyed a similar trajectory. In 2023, 4,570 megawatts were added to the grid, while in 2024, nearly 9,700 megawatts were added.¬†Given that Texas has its own isolated energy grid, the Electricity Reliability Council of Texas (ERCOT) is responsible for managing the majority of energy for its residents.¬†Please take a look at the new openings in our newsroom.See jobsHistorically, ERCOT, which manages 90 percent of the state‚Äôs power load, has primarily relied on natural gas. But other energy sources, like wind and solar, have played a critical role in offsetting high demand.¬†ERCOT added 3,400 megawatts from natural gas power plants in 2024. That‚Äôs after more than 1,000 megawatts of natural gas power were inactive within the grid from 2021 to 2023, according to Dallas Fed data.¬†The capacity growth from solar and battery storage allowed the grid to manage another hot Texas summer in 2024, reported Garret Golding, an assistant vice president for energy program at the Dallas Fed.¬†Battery storage is relatively new to ERCOT. One of the first battery power storage plants to be connected to ERCOT was in 2021, southeast of Dallas in Kaufman County.¬†The 50-megawatt plant run by Enel, one of the largest renewable energy owners and operators in the country, was the first of Enel‚Äôs 14 battery projects it has since developed across the state.¬†The forecasted power demand in Texas from population growth and heavy load users like data centers, cryptocurrency mining and artificial intelligence, alongside the competitive market for batteries, is dictating the rising use of storage within ERCOT, said Randald Bartlett, a senior director of operations and management for battery energy storage systems at Enel North America.Texas‚Äô permitting processes and ability to develop has made it easier to add and build new capacity in comparison to other states with more laborious entryways, Bartlett said.¬†Before, there wasn‚Äôt really adequate criteria and evidence to forecast what batteries would contribute to the grid, ERCOT CEO Pablo Vegas said during a board of directors meeting on Tuesday.¬†Now, the grid operator added battery contribution to its report forecasting future capacity, demand and reserves.¬†Battery storage in the ERCOT grid has nearly doubled every year since 2021, Vegas said. At the end of 2024, there were nearly 10,000 megawatts from batteries within ERCOT.Vegas said the capacity from batteries made a significant difference in ERCOT during bridge hours, or the winter morning hours when the sun has yet to rise and in the evenings after the sun sets.¬†‚ÄúBatteries made a meaningful contribution to what those shoulder periods look like and how much scarcity we get into during these peak events,‚Äù Vegas said when analyzing the grid‚Äôs performance during recent winter storms.¬†In the spring of 2024, Texas‚Äô installation of utility scale solar outpaced California‚Äôs, and jumped from 1,900 megawatts in 2019 to over 20,000 megawatts in 2024. As a result, solar met nearly 50 percent of the state‚Äôs peak power demand on some days.¬†The state‚Äôs quick deployment of utility scale solar didn‚Äôt happen overnight. It started in 2005, when the legislature instructed the Public Utility Commission of Texas to create competitive renewable energy zones, where the state planned transmission lines to connect cities to renewable energy sources in west Texas.Initially, it was intended to capture wind power but was able to quickly include solar because of the existing infrastructure, said Dustin Mulvaney, an environmental studies professor at San Jose State University and an author of Planning to Build Faster: A Solar Energy Case Study, published in October by the Roosevelt Institute.¬†That forward-looking plan is often held up as a model renewable energy advocates and developers say the Federal Energy Regulatory Commission could implement across other regional transmission organizations, by requiring proactive planning and by creating rules of how to pay for transmission systems.¬†As the state‚Äôs grid continues to experience a rapid shift in the type of generation available to serve demand, the state‚Äôs grid operator is looking to build a higher voltage transmission system, upgrading from 345-kilovolt lines to 765-kilovolt lines.¬†In 2024, nearly 78 gigawatts of transmission-connected wind, solar and battery energy storage capacity was installed to the grid. And more than 102 gigawatts of transmission-connected renewable capacity is expected to be installed by the end of 2025, according to a December ERCOT report.It‚Äôs that growth of both demand and renewables connected to the grid that‚Äôs led ERCOT to ask the Public Utility Commission to consider upgrading the state‚Äôs transmission system rather than expanding its existing one.¬†The 765-kV lines would provide significant economic and reliability benefits to the ERCOT system, wrote Kristi Hobbs, ERCOT‚Äôs vice president of system planning and weatherization, in the grid operator‚Äôs submission of its regional transmission plans to the commission in late January.Regardless of which transmission plan is chosen, Hobbs wrote, the explosive growth projected throughout the next six years and beyond will require major public investment.¬†Perhaps you noticed: This story, like all the news we publish, is free to read. That‚Äôs because Inside Climate News is a 501c3 nonprofit organization. We do not charge a subscription fee, lock our news behind a paywall, or clutter our website with ads. We make our news on climate and the environment freely available to you and anyone who wants it.That‚Äôs not all. We also share our news for free with scores of other media organizations around the country. Many of them can‚Äôt afford to do environmental journalism of their own. We‚Äôve built bureaus from coast to coast to report local stories, collaborate with local newsrooms and co-publish articles so that this vital work is shared as widely as possible.Two of us launched ICN in 2007. Six years later we earned a Pulitzer Prize for National Reporting, and now we run the oldest and largest dedicated climate newsroom in the nation. We tell the story in all its complexity. We hold polluters accountable. We expose environmental injustice. We debunk misinformation. We scrutinize solutions and inspire action.Donations from readers like you fund every aspect of what we do. If you don‚Äôt already, will you support our ongoing work, our reporting on the biggest crisis facing our planet, and help us reach even more readers in more places? Please take a moment to make a tax-deductible donation. Every one of them makes a difference.Reporter, Texas RenewablesArcelia Martin is an award-winning journalist at Inside Climate News. She covers renewable energy in Texas from her base in Dallas. Before joining ICN in 2025, Arcelia was a staff writer at The Dallas Morning News and at The Tennessean. Originally from San Diego, California, she‚Äôs a graduate of Gonzaga University and Columbia University Graduate School of Journalism.]]></content:encoded></item><item><title>Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5</title><link>https://github.com/b4rtaz/distributed-llama/discussions/162</link><author>b4rtazz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 16:11:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Carbon capture more costly than switching to renewables, researchers find</title><link>https://techxplore.com/news/2025-02-carbon-capture-renewables.html</link><author>Brajeshwar</author><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dust from car brakes more harmful than exhaust, study finds</title><link>https://e360.yale.edu/digest/brake-pads-lung-damage-study</link><author>Brajeshwar</author><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Diablo hackers uncovered a speedrun scandal</title><link>https://arstechnica.com/gaming/2025/02/the-diablo-hackers-that-debunked-a-record-speedrun/</link><author>pitwin</author><category>hn</category><pubDate>Sat, 15 Feb 2025 14:00:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[But simply splitting a run into segments doesn't explain away all of the problems the TAS team found. Getting Naj's Puzzler on dungeon level 9, for instance, still requires outside modification of a save file, which is specifically prohibited by longstanding Speed Demos Archive rules that "manually editing/adding/removing game files is generally not allowed." Groobo's apparent splicing of multiple game versions and differently seeded save files also seems to go against SDA rules, which say that "there obviously needs to be continuity between segments in terms of inventory, experience points or whatever is applicable for the individual game."After being presented with the TAS team's evidence, SDA wrote that "it has been determined that Groobo's run very likely does not stem from only legitimate techniques, and as such, has itself been banished barring new developments." But Groobo's record is still listed as the "Fastest completion of an RPG videogame" by Guinness World Records, which has not offered a substantive response to the team's findings (Guinness has not responded to a request for comment from Ars Technica).
      A recent  speedrun on a confirmed legitimate dungeon seed.

          This might seem like a pretty petty issue to spend weeks of time and attention debunking. But at a recent presentation attended by Ars, Cecil said he was motivated to pursue it because "it did harm. Groobo's alleged cheating in 2009 completely stopped interest in speedrunning this category [of ]. No one tried, no one could."Because of Groobo's previously unknown modifications to make an impossible-to-beat run, "this big running community just stopped trying to run this game in that category," Cecil said. "For more than a decade, this had a chilling impact on that community." With Groobo's run out of the way, though, new runners are setting new records on confirmed legitimate RNG seeds, and with the aid of TAS tools.In the end, Cecil said he hopes the evidence regarding Groobo's run will make people look more carefully at other record submissions. "Groobo had created a number of well-respected ... speedruns," he said. "[People thought] there wasn't any good reason to doubt him. In other words, there was bias in familiarity. This was a familiar character. Why would they cheat?"]]></content:encoded></item><item><title>Show HN: I Built a Reddit-style Bluesky client ‚Äì still rough, but open to ideas</title><link>https://threadsky.app/</link><author>lakshikag</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 13:19:17 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Kreuzberg ‚Äì Modern async Python library for document text extraction</title><link>https://github.com/Goldziher/kreuzberg</link><author>nhirschfeld</author><category>hn</category><pubDate>Sat, 15 Feb 2025 10:07:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I'm excited to showcase Kreuzberg!Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.Key technical features:
- Built with modern Python best practices (async/await, type hints, functional-first)
- Optimized async I/O with anyio for multi-loop compatibility
- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)
- Efficient batch processing with concurrent extractions
- Clean error handling with context-rich exceptionsI built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.Key advantages over alternatives:
- True async support with optimized I/O
- Minimal dependencies (much smaller than alternatives)
- Perfect for serverless and async web apps
- Local processing without API calls
- Built for modern Python codebases with rigorous typing and testingThe library is MIT licensed and open to contributions.]]></content:encoded></item><item><title>Jane Street&apos;s Figgie card game</title><link>https://www.figgie.com/</link><author>eamag</author><category>hn</category><pubDate>Sat, 15 Feb 2025 09:59:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Jane Street's fast-paced
              Figgie game simulates exciting elements of markets and trading. At
              Jane Street, Figgie is a game we teach and also one we really
              enjoy playing.
            Read our FAQs
              for more. If you have a question that isn‚Äôt answered there, we‚Äôd
              like to hear
              what‚Äôs missing and what
              would be helpful to know, and we‚Äôll do our best to update FAQs
              along the way.
            ]]></content:encoded></item><item><title>The 20 year old PSP can now connect to WPA2 WiFi Networks</title><link>https://wololo.net/2025/02/14/the-20-year-old-psp-can-now-connect-to-wpa2-wifi-networks/</link><author>zdw</author><category>hn</category><pubDate>Sat, 15 Feb 2025 03:31:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Screenshot source: Zekiu_ on youtubeAcid_Snake and the ARK Development team have released a significant update to the ARK custom Firmware for the Sony PSP. Custom Firmware now allows the Playstation Portable to connect to WPA2 encrypted Wifi networks. This is thanks to the recently released  plugin, created by developer  and published on the PSP Homebrew discord.Playstation Portable gets WPA2 Wifi accessThe PSP has been out of official support from Sony for years, but lots of enthusiasts keep maintaining this great handheld through homebrew and custom Firmware updates. As technology evolves around us, older devices such as the PlayStation Portable can lose some of their features.For example, as WPA2 has become the defacto encryption standard for home wifi networks (WPA3‚Äôs adoption rate remains low), older devices such as the PSP, that do not support these new* encryption standards become technically unable to access the internet.Wifi access was a very strong feature of the PSP when it was released, and, although it‚Äôs probably less important nowadays, losing that feature because newer networks aren‚Äôt compatible is a bummer.WPA2 support has been a request by many enthusiasts for years on PSP discussion channels, and it seems that the wpa2psp plugin by developer Moment now brings this to life. According to Acid_Snake, the developer was kind enough to provide the source code of the plugin, which allowed the ARK team to embed it into the ARK Custom Firmware for PSP.This reddit thread by Nebula_NL covers a lot of details on how to install and use the plugin. But the bottom line is: install the latest release of the ARK CFW on your PSP, and take it from there. (Note that you can also manually install the plugin if you‚Äôre using another CFW than ARK)This is of course the first iteration of this plugin, and it comes with limitations, specifically:2.4 GHz Only
WPA2 support works with 2.4 GHz WiFi.If your router uses a single SSID for both 2.4 GHz and 5 GHz, you may need to separate them and connect your PSP to the 2.4 GHz network.WPA2 AES Only
Requires WPA2 with AES (AES-CCMP) encryption.TKIP is not supported and will not work.WEP/WPA Compatibility
While WPA2 is active, WEP and WPA encryption will not work.To use WEP or WPA again, disable WPA2, and they will function normally.WPA2/WPA3 Mixed Mode
If your router is set to WPA2/WPA3 mixed mode, your PSP may struggle to obtain an IP address.Try manually setting the IP address instead of using DHCP in [AUTO] mode.Download and install ARK-4 + enable WPA2 Support for the PSP* WPA2 was certified in 2004‚Ä¶ It‚Äôs ‚Äúnew‚Äù from the PSP‚Äôs perspective which launched the same year and didn‚Äôt ‚Äúneed‚Äù to support it at the time. WPA3 launched in 2018 but its adoption is taking time]]></content:encoded></item><item><title>Show HN: VimLM ‚Äì A Local, Offline Coding Assistant for Vim</title><link>https://github.com/JosefAlbers/VimLM</link><author>JosefAlbers</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:34:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[VimLM is a local, offline coding assistant for Vim. It‚Äôs like Copilot but runs entirely on your machine‚Äîno APIs, no tracking, no cloud.- Deep Context: Understands your codebase (current file, selections, references).  
- Conversational: Iterate with follow-ups like "Add error handling".  
- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  
- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.Perfect for privacy-conscious devs or air-gapped environments.Try it:  
```
pip install vimlm
vimlm
```]]></content:encoded></item><item><title>A decade later, a decade lost (2024)</title><link>https://meyerweb.com/eric/thoughts/2024/06/07/a-decade-later-a-decade-lost/</link><author>ZeWaka</author><category>hn</category><pubDate>Fri, 14 Feb 2025 23:10:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I woke up this morning about an hour ahead of my alarm, the sky already light, birds calling.¬† After a few minutes, a brief patter of rain swept across the roof and moved on.I just lay there, not really thinking.¬† Feeling.¬† Remembering.Almost sixteen years to the minute before I awoke, my second daughter was born.¬† Almost ten years to the same minute before, she‚Äôd turned six years old, already semi-unconscious, and died not quite twelve hours later.So she won‚Äôt be taking her first solo car drive today.¬† She won‚Äôt be celebrating with dinner at her favorite restaurant in the whole world.¬† She won‚Äôt kiss her niece good night or affectionately rag on her siblings.Or maybe she wouldn‚Äôt have done any of those things anyway, after a decade of growth and changes and paths taken.¬† What would she really be like, at sixteen?We will never know.¬† We can‚Äôt even guess.¬† All of that, everything she might have been, is lost.This afternoon, we‚Äôll visit Rebecca‚Äôs grave, and then go to hear her name read in remembrance at one of her very happiest places, Anshe Chesed Fairmount Temple, for the last time.¬† At the end of the month, the temple will close as part of a merger.¬† Another loss.A decade ago, I said that I felt the weight of all the years she would never have, and that they might crush me.¬† Over time, I have come to realize all the things she never saw or did adds to that weight.¬† Even though it seems like it should be the same weight.¬† Somehow, it isn‚Äôt.I was talking about all of this with a therapist a few days ago, about the time and the losses and their accumulated weight.¬† I said, ‚ÄúI don‚Äôt know how to be okay when I failed my child in the most fundamental way possible.‚Äù‚ÄúYou didn‚Äôt fail her,‚Äù they said gently.‚ÄúI know that,‚Äù I replied. ‚ÄúBut I don‚Äôt feel it.‚ÄùA decade, it turns out, does not change that.¬† I‚Äôm not sure now that any stretch of time ever could.]]></content:encoded></item><item><title>We were wrong about GPUs</title><link>https://fly.io/blog/wrong-about-gpu/</link><author>mxstbr</author><category>hn</category><pubDate>Fri, 14 Feb 2025 22:36:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We‚Äôre building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren‚Äôt going anywhere, but: GPUs aren‚Äôt going anywhere.A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It‚Äôs a Fly Machine that can do fast CUDA.Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn‚Äôt fit the moment. It‚Äôs a bet that doesn‚Äôt feel like it‚Äôs paying off.If you‚Äôre using Fly GPU Machines, don‚Äôt freak out; we‚Äôre not getting rid of them. But if you‚Äôre waiting for us to do something bigger with them, a v2 of the product, you‚Äôll probably be waiting awhile.GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel‚Äôs Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers(not even bidirectional: in common configurations, GPUs talk to each other)with arbitrary, end-user controlled computation, all operating outside our normal security boundary.We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren‚Äôt mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there‚Äôs a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.Security wasn‚Äôt directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we‚Äôd have been on Nvidia‚Äôs driver happy-path.Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.Instead, we burned months trying (and ultimately failing) to get Nvidia‚Äôs host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.I‚Äôm not sure any of this really mattered in the end. There‚Äôs a segment of the market we weren‚Äôt ever really able to explore because Nvidia‚Äôs driver support kept us from thin-slicing GPUs. We‚Äôd have been able to put together a really cheap offering for developers if we hadn‚Äôt run up against that, and developers love ‚Äúcheap‚Äù, but I can‚Äôt prove that those customers are real.On the other hand, we‚Äôre committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer‚Äôs OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.The biggest problem: developers don‚Äôt want GPUs. They don‚Äôt even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don‚Äôt care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can‚Äôt just give them a GPU.For those developers, who probably make up most of the market, it doesn‚Äôt seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of ‚Äútokens per second‚Äù aren‚Äôt counting milliseconds.(you should all feel sympathy for us)This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they‚Äôll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn‚Äôt seem to matter yet, so the market doesn‚Äôt care.Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.We think there‚Äôs probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it‚Äôs not baked; we can‚Äôt use it. And I‚Äôm not sure how many of those customers there are, or whether we‚Äôd get the density of customers per server that we need.That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they‚Äôre the one part we have in our inventory people seem to get a lot of use out of. We‚Äôre happy with them. But they‚Äôre just another kind of compute that some apps need; they‚Äôre not a driver of our core business. They‚Äôre not the GPU bet paying off.Really, all of this is just a long way of saying that for most software developers, ‚ÄúAI-enabling‚Äù their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.A very useful way to look at a startup is that it‚Äôs a race to learn stuff. So, what‚Äôs our report card?First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.But Cursor happened, and, as they say, how are you going to keep ‚Äòem down on the farm once they‚Äôve seen Karl Hungus? It seems much clearer where things are heading.GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn‚Äôt a winning strategy. I‚Äôd rather we‚Äôd flopped the nut straight, but I think going in on this hand was the right call.A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren‚Äôt recoverable. But the hardware parts that aren‚Äôt generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I‚Äôm even more comfortable making bets backed by tradable assets with durable value.In the end, I don‚Äôt think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I‚Äôm very happy about is that we didn‚Äôt compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we‚Äôre scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.We started this company building a Javascript runtime for edge computing. We learned that our customers didn‚Äôt want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That‚Äôs usually how we figure out the right answers:  by being wrong about a lot of stuff.]]></content:encoded></item><item><title>The hardest working font in Manhattan</title><link>https://aresluna.org/the-hardest-working-font-in-manhattan/</link><author>robinhouston</author><category>hn</category><pubDate>Fri, 14 Feb 2025 21:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
		In 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I <3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama‚Äôs first campaign. 
	
		But there was one font I didn‚Äôt even notice, even though it was everywhere around me.
			
		Last year in New York, I walked over 100 miles and took thousands of photos of one and one font only.
			
		The font‚Äôs name is Gorton.
			
		It‚Äôs hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for my book about the history of typing. 
			
		Many keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird m√©lange of ‚Äúmechanical‚Äù and ‚Äúchildish.‚Äù
			 
		The more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. & and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it ‚Äì a beginning of a ligature that never came.
			
		The strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual ‚Äì perhaps even na√Øve ‚Äì symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.
				
		Looking at just a few keyboards, it was also obvious that it wasn‚Äôt just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) 
			
		My first thought was: What a mess. Is this how ‚Äúgrotesque‚Äù fonts got their name?
			
		Then, the second thought: I kind of like it.
	The most distinctive letterforms of Gorton		
		But what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham‚Äôs.
			
		A few places for keyboard nerds referred to the font as ‚ÄúGorton,‚Äù but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.
					
		I originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones ‚Äì those from the early 1960s laboratory computers, or the 1980s microcomputers ‚Äì¬†the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible ‚Äì the legend could not wear off any more than its key ‚Äì and I imagined required some specialized keyboard-making machinery that came with the ‚Äúkeyboard font‚Äù already there.
	
		An example of a ‚Äúdouble-shot‚Äù key from above and from below
				
		But then, I started seeing Gorton in other places.
						
		Hours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.
						
		One day, I saw what felt like Gorton on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist‚Äôs office. In an alley. 
							
		These had one thing in common. All of the letters were carved into the respective base material ‚Äì metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.
						
		At one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.
								
		Some searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.
									
		This immediately explained both the metaphorical and literal rough edges of Gorton.
						
		A lot of typography has roots in calligraphy ‚Äì someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most ‚Äúmechanical‚Äù fonts have surprising humanistic touches if you inspect them close enough.
				
		But not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts ‚Äúmonoline‚Äù). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.
		
		Monoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. 
			
		It seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century‚Äôs middle decades. But casual research through materials preserved by some of George Gorton Machine Company‚Äôs fans ‚Äì including the grandson of the founder ‚Äì revealed something even more interesting. Gorton the font was a lot older than I expected. 
			
		I found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.
				
		To put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston‚Äôs London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.
	
		Imagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine‚Äôs then-current logo!
						
		I started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote ‚ÄúI used Gorton to typeset this,‚Äù because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adorning the screens of early computers.
						
		The difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.
	
		Gorton wasn‚Äôt just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.
						
		It was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE‚Ä¶ and it went  and then the Moon, as key legends on Apollo‚Äôs onboard computer.
								
		But why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?
						
		Some of it might be the power of the default. Popular engraving equipment comes with a built-in font that‚Äôs so standard it reuses the router‚Äôs name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.
	
		Gorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering ‚Äì every letter done by hand ‚Äì then a modern font you could simply  and one designed with ‚Äúa minimum of sharp corners for rapid tracing with a smooth stroke,‚Äù must have felt like a breath of fresh air.
								
		But why engraving to begin with? Because the affordable and casual printing options we enjoy today ‚Äì the office laser printers and home inkjets, the FedEx Kinko‚Äôs, the various cheap labelers ‚Äì weren‚Äôt there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.
				
		On top of that, Gorton‚Äôs longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.
	Various items from the 20th century typeset in Gorton					
		I felt good about all my findings: What a strange story of a strange routing font! 
	
		But it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.
	We‚Äôre used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren‚Äôt as easy back when fonts were made out of metal. Life‚Äôs not as much fun outside of the glamor of a TTF file, and a routing font couldn‚Äôt immediately become a regular font ‚Äì so seeing Gorton being additive and not subtractive was an unexpected discovery.
								
		It turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.
								
		A company called Keuffel & Esser Co. grabbed Gorton‚Äôs machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away ‚Äì still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving ‚Äì¬†instead, you‚Äôd mount a special pen and draw letters by tracing them.
								
		Another company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.
								
		Some of the traditional draftspeople pooh-poohed these inventions ‚Äì one handbook wrote ‚Äú[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.‚Äù
				
		But the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway‚Äôs, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.
				
		Then, both EC Comics and All-Star Comics used Leroy in the 1940s and 1950s, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.
				
		Elsewhere silkscreening ‚Äì a pretty cool technique of applying paint on surfaces through a wet mesh of fabric ‚Äì took Gorton and Leroy in a different direction, by allowing paint on metal.
				
		There was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.
	
		I also counted at least three different systems of ‚ÄúGorton movable type‚Äù ‚Äì some where you could assemble physical letters, and some where you could impress them into soft materials using steel types ‚Äì and I imagine there were probably more.
				
		Letraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too ‚Äì and so did a few competitors.
						
		In the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998‚Ä¶ then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via ANSI Y14.2M, and so did institutions like the US National Park Service.
					
		Gorton went on and on and on. The early Hershey vector fonts, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren‚Äôt even necessary ‚Äì¬†any letter could now be drawn by a series of straight lines.
						
		And even in the first universe Gorton inhabited things weren‚Äôt standing still. 
	
		As the engraving industry learned what‚Äôs popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called ‚ÄúThe Whereabouts of 230 Engraving Machines‚Äù listed Gorton customers ranging from biscuit makers to fire engine constructors. Othercatalogsproudly listed applications like book covers, billiard balls, organ keys, and toothbrushes, as well as ‚Äútools making more tools‚Äù ‚Äì using Gorton engravers to create legends for other machines.
			
		After you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:
	
		The original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with Nixie tubes and complex interfaces labeled‚Ä¶ in Gorton itself. 
	
		But the technology matured quickly and soon more and more early manual ‚Äútracer-guided‚Äù pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by computerized ones, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.
					
		You no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8¬¥¬¥ to 3¬¥¬¥ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) 
	
		Now, fonts came as digits or formulas built into computer memory, or ‚Äì for a moment in time ‚Äì¬†as separate cartridges you‚Äôd insert in eager slots. (And yes, before you ask:¬†there were other routing monoline fonts, too. But I really don‚Äôt care about any of them.)
						
		It was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.
						
		And automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I‚Äôve seen ‚Äì a weird combination of a machine pretending to be another machine pretending to be a human hand.)
					
		If this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.
				
		I don‚Äôt think the ‚Äì by now 80 years and counting ‚Äì fractal explosion of Gorton made its original creators rich.
				
		Copy protection in the world of typography is complicated. The font‚Äôs name can be trademarked and other companies legally prevented from using it, and you can‚Äôt just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it ‚Äì and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.
				
		This is why Keuffel & Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn‚Äôt name it Gorton. 
								
		But of course, Gorton was barely named ‚ÄúGorton‚Äù to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called ‚Äústandard copies.‚Äù) Then, as other fonts were added, it was retroactively named Gorton Normal ‚Äì the name of the company and the most generic word possible.
						
		Leroy lettering sets started with one font, so similarly to Gorton the font started to be known as ‚ÄúLeroy,‚Äù then ‚ÄúSeries C,‚Äù then ‚ÄúGothic.‚Äù New Hermes called it simply ‚ÄúBlock,‚Äù Letraset went with ‚ÄúEngineering Standard,‚Äù and Rotring ‚Äì another producer of little computerized plotters¬†‚Äì with ‚ÄúUniversal.‚Äù I‚Äôve also seen ‚ÄúA style,‚Äù ‚ÄúPlain Gothic,‚Äù and, mysteriously, ‚ÄúStandpoint.‚Äù 
								
		I don‚Äôt think this was meant to be disrespectful. ‚ÄúStandard,‚Äú ‚ÄúUniversal,‚Äù ‚ÄúA style‚Äù might not have had the connotations of ‚Äúgeneric‚Äù we associate with them today, but rather meaning ‚Äúthe only one you need,‚Äù ‚Äúapproved of by millions,‚Äù or ‚Äúthe ultimate.‚Äù
								
		But there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as ‚ÄúLinetica.‚Äù
	A few rare examples of Gorton Extended in use						
		Each of these reappearances made small changes to the shapes of some letters. Leroy‚Äôs ampersand was a departure from Gorton‚Äôs. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation ‚Äì¬†commas, quotes, question marks ‚Äì¬†was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton‚Äôs pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition‚Ä¶ and at times both. The extended version ‚Äì not that popular to begin with ‚Äì was often skipped.)
	Classic Gorton vs. Gorton Modified						
		The only ‚Äúofficial‚Äù update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. 
	
		It is also that last Gorton that mattered.
	A collection of movies and TV shows featuring Gorton							
		My every walk in Chicago or San Francisco was counting down ‚Äútime to Gorton‚Äù ‚Äì sometimes mere minutes before I saw a placard or an intercom with the familiar font.
								
		This might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise ‚Äì a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn‚Äôt exist, I saw it surprisingly often.
										
		I even spotted Gorton a few times in Spain, or the U.K., and didn‚Äôt make too much of it, not thinking about the likelihood of machines from George Gorton‚Äôs company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.
	Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R						
		It was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:
								
		Some letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:
								
		And a visit to a naval museum cemented it all:
	
		It was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn‚Äôt stretch all the way across, and the distinctive shape of S was slightly different here.
								
		Fortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn‚Äôt actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor & Hobson. 
								
		In 1894, TT&H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do ‚Äì instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.
	
		I don‚Äôt know how this first proto-Gorton was designed ‚Äì unfortunately, Taylor, Taylor & Hobson‚Äôs history seems sparse and despite personal travels to U.K. archives, I haven‚Äôt found anything interesting ‚Äì but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.
	From a 1895 ‚ÄúFree-hand lettering‚Äù book by Frank T. Daniels					
		This was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font ‚Äì the first time technical writing was able to be replicated consistently via the aid of the machine.
				
		No wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor & Hobson licensed their stuff to a fledgling American company named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&H font must have been slightly adjusted on arrival. 
	A Taylor-Hobson pantograph in use in 1942			
		This adds to the accomplishments of Gorton ‚Äì the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in one of the UK‚Äôs national parks), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.
				
		If the Taylor, Taylor & Hobson (or, Taylor-Hobson, as their engravers were known) ‚Äúbranch‚Äù of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&H didn‚Äôt properly name the font either, eventually calling it ‚ÄúA style‚Äù for regular and ‚ÄúC style‚Äù for condensed variants. Google searches for ‚Äútaylor hobson font‚Äù are a lot more sparse than those for Gorton.)  
	GortonGorton CondensedThe Gorton quasisuperfamily
		In the end, I‚Äôm sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)
	A comparison of all major editions of Gorton					
		And so, throughout the 20th century, Gorton has lived two parallel lives ‚Äì¬†one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. 
								
		I am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (Chances are, you will see Gorton later today!) I‚Äôm curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I‚Äôm interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton‚Äôs niche.
	
		The history of this strange font spans over a century and I‚Äôve seen it in so many countries by now, used in so many situations. But it‚Äôs impossible for me to say Gorton is the most hard-working font in the world.
								
		To this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be ‚Äúthe default‚Äù ‚Äì from Times New Roman through Calibri to Roboto ‚Äì immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5√ó7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.
								
		But there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.
							
		In 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.
								
		Gorton is everywhere in Manhattan. It‚Äôs there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it‚Äôs there on so, so many intercoms.
						
		I wouldn‚Äôt be surprised if there weren‚Äôt a single block without any Gorton in a whole of Manhattan.
	A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn‚Äôt have access to the interiors of most buildings.	
		The omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font‚Äôs already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:
								
		You can see all sorts of kerning mistakes:
										
		You will notice the many, many routing imperfections ‚Äì an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:
	
		There are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:
												
		You can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:
	
    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, carved into acrylic, of unknown provenance and even more nameless than I thought possible for an already name-free font.
  
		But New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:
	
	
			All my Gorton walks in Manhattan in 2024
		

		This was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.
	
		In a city that never sleeps, Gorton wasn‚Äôt allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil‚Äôs work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.
	
		And once in a while, I‚Äôd find Gorton that would wink at me with a story ‚Äì¬†followed by that aching in the heart as I realized I‚Äôd never know what the story was.
				
		You‚Äôre not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.
								
		So why do I love Gorton so much? 
								
		The Occam‚Äôs Razor seems sharp on this one. Perhaps I like it because I‚Äôm a boy and Gorton is often attached to heavy machinery. 
					
		But there must be more to it. Perhaps it‚Äôs all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it‚Äôs been made by machines for machines ‚Äì¬†but in its use, it‚Äôs also the font that allows you to see so many human mistakes and imperfections.
					
		Gorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don‚Äôt apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. 
	
		Stretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don‚Äôt stretch the tip of the router itself.
								
		There are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibility.
		And on top of that, the simplicity of the letterforms themselves feels compatible with the typical na√Øvet√© of Gorton‚Äôs typesetting. 
	Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow
    Sure, there are really bad renditions that are inexcusable. 

		But most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don‚Äôt feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don‚Äôt feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used ‚Äì haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)
				
		The transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn‚Äôt meant to be admired, celebrated, treasured. It‚Äôs meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton ‚Äì especially those without instructions on how letters are positioned against each other ‚Äì might not even classify as a font!)
						
		And I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. 	
		With Gorton, carving into metal or plastic means good-looking fixes are impossible:
	
		And unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.
	
		Perhaps above everything else, Gorton is all about . 
  
    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  
		mid-century intercom and filled in with special paste or wax, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. 
				
		(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts ‚Äì you just have to look very, very closely.)
	
		That‚Äôs the last, and perhaps the best thing to fall in love with. 
	
		You won‚Äôt be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton‚Äôs non-beauty can be:
						
		This has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. 
						
		I feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don‚Äôt know the full extent of Gorton‚Äôs use. I don‚Äôt know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor & Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.
						
		Also, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?
						
		But mostly, I can‚Äôt shake the feeling that we all missed a window. That this essay can‚Äôt be just a celebration, but also needs to be the beginnings of a eulogy.
						
		Walking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.
								
		Gorton itself is obsolete. All the keyboards that use Gorton Modified you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don‚Äôt change all that much. But few of the new placards use Gorton, and none of the new intercoms do. 
	
		Taylor, Taylor & Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny & Trecker, which became Cross & Trecker, was acquired by Giddings & Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&H and George Gorton‚Äôs company are today selling Gorton in any capacity.
								
		It will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (I eventually found quite a few) won‚Äôt help. They are perhaps all missing a point, anyway.
	
		But there‚Äôs a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.
	
		But fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.
								
		Manhattan‚Äôs tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton‚Äôs no stranger to water, snow, rust, or dirt.
			
		This is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing ‚Äì they‚Äôre a .
								
		You find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.		
	]]></content:encoded></item><item><title>Show HN: Transform your codebase into a single Markdown doc for feeding into AI</title><link>https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html</link><author>tesserato</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:23:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.
The output for the current repository can be found here.Comprehensive Codebase Documentation: Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format. Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types). Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.Simple Command-Line Interface:  Offers an intuitive command-line interface with straightforward options for customization.If you have Go installed, run go install github.com/tesserato/CodeWeaver@latestto install the latest version of CodeWeaver or go install github.com/tesserato/CodeWeaver@vX.Y.Z to install a specific version.Alternatively, download the appropriate pre built executable from the releases page.If necessary, make the  executable by using the  command:The root directory to scan and document.The name of the output Markdown file.-ignore "<regex patterns>"Comma-separated list of regular expression patterns for paths to exclude.-included-paths-file <filename>File to save the list of paths that were included in the documentation.-excluded-paths-file <filename>File to save the list of paths that were excluded from the documentation.Display this help message and exit.Generate documentation for the current directory:This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).Specify a different input directory and output file:./codeweaver -dir=my_project -output=project_docs.md
This command will process the  directory and save the documentation to .Ignore specific file types and directories:./codeweaver -ignore="\.log,temp,build" -output=detailed_docs.md
This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.Save lists of included and excluded paths:./codeweaver -ignore="node_modules" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md
This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.CodeWeaver is released under the MIT License. See the  file for complete license details.]]></content:encoded></item><item><title>Show HN: A New Way to Learn Languages</title><link>https://www.langturbo.com/</link><author>sebnun</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: SQL Noir ‚Äì Learn SQL by solving crimes</title><link>https://www.sqlnoir.com/</link><author>chrisBHappy</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 21:49:16 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Sri Lanka scrambles to restore power after monkey causes islandwide outage</title><link>https://www.reuters.com/world/asia-pacific/sri-lanka-scrambles-restore-power-after-monkey-causes-islandwide-outage-2025-02-13/</link><author>abe94</author><category>hn</category><pubDate>Thu, 13 Feb 2025 13:47:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I made my own OS from scratch because I was bored</title><link>https://jotalea.com.ar/misc/jotaleaos/</link><author>Jotalea</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 20:55:12 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: yknotify ‚Äì Notify when YubiKey needs touch on macOS</title><link>https://github.com/noperator/yknotify</link><author>noperator</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 20:24:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: A no-build fullstack SSR TypeScript web framework</title><link>https://jsr.io/@fullsoak/fullsoak</link><author>thesephi</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 19:54:52 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Game Bub ‚Äì open-source FPGA retro emulation handheld</title><link>https://eli.lipsitz.net/posts/introducing-gamebub/</link><author>elipsitz</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 17:11:25 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I‚Äôm excited to announce the project I‚Äôve been working on for the last year and a half: , an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the Game Link Cable in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.Check out the instructions, code, and design files on GitHub. Note that building a Game Bub unit is fairly complex. If you might be interested in buying a complete Game Bub kit, please fill out this form to help me gauge interest.I had a lot of fun implementing a Game Boy at the hardware level, and I started thinking about how far I could take the project. I was using a Pynq-Z2 development board, which was definitely the right way to get started, but it came with a lot of limitations.I had to use an external monitor for audio/video, and an external gamepad for input, but a real Game Boy, of course, is a portable handheld. I also wanted to add Game Boy Advance support, but the memory architecture of the Pynq-Z2 had access latency that was just barely acceptable for the Game Boy, and would have been completely unacceptable for the Game Boy Advance. I also wanted to make something less ‚Äúhacky‚Äù: a real device that I could play and give to people, not just a bare PCB.Furthermore, while there are open-source FPGA retrogaming projects (e.g. MiSTer), there doesn‚Äôt appear to be anything open-source that supports physical Game Boy and Game Boy Advance cartridges, let alone an open-source handheld device.Thus, I somewhat naively set out to design what would become by far my most complex electrical engineering and hardware design project to date.I set out some goals for the project:Build a standalone, rechargable battery-powered FPGA handheldMinimize cost and complexity by using off-the-shelf components wherever possibleCapable of playing Game Boy, Game Boy Color, and Game Boy Advance gamesCapable of using physical cartridges, or emulating cartridges (reading ROM files off of a microSD card)Easy to use: graphical menu and in-game overlayIntegrated display and speakers, with headphone supportIntegrated peripherals (rumble, real-time clock, accelerometer) for emulated cartridgesHDMI video output support for playing on a big screenDecent looking design with good ergonomicsExpansion opportunities in the future: support for more systems, Wi-Fi, etc.And finally, since I was building this project for fun and learning, I wanted to be able to fully understand every single component of the system. I wanted to use my own emulator cores (e.g. not just port them from MiSTer), do my own board design, and write my own drivers to interface with peripherals.A brief rant about FPGA retrogaming#There‚Äôs a lot of misleading marketing and hype out there around FPGA retrogaming. Some claim that FPGA retrogaming devices are not emulators (because they supposedly ‚Äúact like [the system] at the gate level‚Äù), that they achieve ‚Äúperfect accuracy‚Äù, or that they‚Äôre superior to software emulators.In my opinion, this is blatantly wrong and actively harmful. FPGA retrogaming devices are emulators: they pretend to be something they‚Äôre not. And they‚Äôre only as accurate as they‚Äôre programmed to be, since they‚Äôre recreations. An FPGA can make certain aspects of accuracy easier to achieve, but it doesn‚Äôt guarantee it.Software emulators can be extremely accurate. Furthermore, perfect accuracy (if it‚Äôs even possible) is by no means a requirement to play an entire system‚Äôs library of games. Some people claim that FPGA emulators are the only way to ‚Äúpreserve‚Äù a system, but I‚Äôd argue that software emulators are a significantly more accessible (no special hardware needed!) way to further this goal.I believe that FPGA emulators have only one real advantage over software emulators: they can more easily interface with original hardware, such as physical cartridges or other consoles via link cables.I did this project not because I think that FPGA emulators are inherently better than software emulators, but because I think they‚Äôre interesting and fun to build.I began work on the project by doing some initial research and sketching out a high level design.My previous FPGA emulator project used a Xilinx Zynq chip, which integrates FPGA fabric (‚ÄúPL‚Äù) with a dual-core ARM processor running Linux (‚ÄúPS‚Äù). I implemented the entire emulator on the FPGA, and used the Linux system to configure the FPGA, render the UI, and load ROM files from the filesystem.I decided to keep this same division of responsibilities: using the FPGA to do the core emulation, with a separate processor to do support tasks. However, to make the overall design easier to reason about, I decided to to use an FPGA-only chip (without any hard processor cores), and an external microcontroller (MCU) to do the tasks that the ARM cores did before.The FPGA would consume input, directly interface to the game cartridges (through level shifters to support both the 3.3 volt GBA and 5 volt Game Boy), and output audio and video to the speakers and display. The MCU would handle the UI, read ROM files from the microSD card, initialize peripherals (display, DAC, IMU), handle power sequencing, and load the FPGA configuration.I wanted to have Wi-Fi and Bluetooth support: Wi-Fi for software updates, and the possibility of emulating the Game Boy Advance Wireless Adapter, and Bluetooth to support wireless game controllers (when connected to an external display). To reduce complexity (and avoid the need for careful RF design), I looked only for complete Wi-Fi/Bluetooth modules with integrated antennas.An early block diagram I sketched outI also drew out rough sketches of what the final device might look like: placement of buttons, screen, speakers, ports, cartridge slot, and battery. I settled on a vertical Game Boy Color-esque design (as opposed to a horizontal Game Boy Advance-style design), because I felt that this would maximize the space in the back of the device for full-size Game Boy Color cartridges and a battery.After sketching out the goals and high level design, I started component selection: picking out each non-trivial component of the system, evaluating features and requirements (e.g. how they communicate, power consumption and voltages needed).Since I intended to have this manufactured and assembled at JLCPCB, I strongly preferred parts that were available in their part library. One technique I even used for narrowing down part choices was finding the relevant category in their part search, and sorting by their stock count.I initially planned to use an RP2040 microcontroller, with a separate ESP32-WROOM module to support Wi-Fi and Bluetooth.The ESP32 supports both Bluetooth Classic and LE, which is essential for supporting a wide range of controllers, and the RP2040 has USB host support, to support wired controllers.During the schematic design process, I ended up simplifying the RP2040 + ESP32 combination to just a single ESP32-S3 module for a few reasons:I started running out of GPIOs on the RP2040, and I was dedicating 4 of them (2 for UART, 1 for reset, 1 for booting in firmware download mode) to communication with the ESP32. Plus, the ESP32-S3 has more GPIOs overall.I wanted to write the MCU firmware in Rust, and the ESP32-S3 had support for the Rust standard library (via ESP-IDF and esp-idf-hal). This seemed like it would be easier to get the software up and running.Fewer components means easier routing and assemblyThe ESP32-S3 has an SDIO module (for interfacing with the microSD card), and FAT filesystem support (via ESP-IDF). It would be possible to do this with the RP2040 PIO, but having a proper peripheral and driver for this makes it a lot easier.The ESP32-S3 is more powerful than the RP2040, and would probably be able to render a smoother UI.However, the ESP32-S3 has one main disadvantage compared to the original ESP32: it doesn‚Äôt have Bluetooth Classic support, only LE. This would greatly limit the range of supported wireless controllers, but I believed the compromise was worth it. I also decided to scrap USB host support, because supporting USB-C dual role (switchable device or host) would have added a lot of additional complexity.If the RP2350 microcontroller (the successor to the RP2040) had been available when I started this project, I may very well have chosen it, since it has even more power, PIO blocks, memory, and GPIO pins. I might have paired it with an RM2 radio module for Wi-Fi and Bluetooth.I wanted a display that would support integer scaling for the Game Boy Advance, which has a 240x160 pixel screen. I was also looking for a screen roughly on the order of 3.0-3.5 inches wide (diagonal), to be comfortable to hold in the hand.I found the ER-TFT035IPS-6 LCD module from EastRising, with a 3.5 inch display, and a 320x480 pixel resolution. This allows for a 2x integer scale for the Game Boy Advance (and a 2x scale plus centering for the 160x144 Game Boy display). This checked off almost all of the boxes: integer scaling, a good size, available at a reasonable price, pretty good documentation (for the ILI9488 LCD controller).ER-TFT035IPS-6 LCD moduleThe main issue, which actually ended up being fairly annoying, is that it‚Äôs a 320x480 display, not 480x320. Meaning, it‚Äôs oriented in portrait mode, not landscape. I rotated the device 90 degrees to fit in a landscape orientation, but this created two issues:In landscape orientation, the bottom of the display (containing the LCD driver chip and the flex cable) faces to the left or the right, which means that larger bazels are required on the left and right of the display to center the ‚Äúactive area‚Äù of the LCD within the handheld.In landscape orientation, the display refreshes from left to right, not top to bottom.The problem with refreshing from left to right is that the Game Boy and Game Boy Advance (and almost every other system) refresh from top to bottom. This means that the display can‚Äôt be refreshed perfectly in sync with the game (zero buffering), and single buffering leads to unsightly diagonal tearing. Instead, I had to use triple buffering, where the game is writing to one framebuffer, the LCD driver is reading from another buffer, and there‚Äôs one spare swap buffer. This increases the amount of memory used ‚Äì and because it needed to be accessed by both the game and LCD driver simultaneously (dual port), it needed to be stored in internal block RAM in the FPGA, a scarce resource.So, even though the Game Boy emulator uses <10% of the total logic resources of the FPGA, and the Game Boy Advance uses around 30%, I had to use a large (more expensive, and power hungry) FPGA so that I had enough block RAM.I also stuck a standard size HDMI port into the design, connected directly to the FPGA. HDMI has a few additional, non-video signals that need level shifting from 5V to 3.3V (I opted for discrete transistors), and it requires the source (me!) to supply a small amount of power.I had never previously designed anything that used a lithium ion battery, so I had a fair amount of learning to do. Adafruit was a helpful resource. I needed a way to charge the battery from USB power, and a way to measure how charged it is.Lithium ion batteries can be dangerous if misused. Safely charging a battery is non-trivial, and requires a feedback loop and adjustable voltage sources. A dedicated IC seemed like the best way to do this. A lot of hobbyists use the ultra-cheap TP4056 1A battery charger, but I‚Äôd read about a lot of issues it has around safely charging the battery while using it. I decided instead to opt for the TI BQ2407x series of battery charger ICs. They seem to be widely used in commercial products, came with a comprehensive datasheet, and had a few critical features: programmable input and charge current limits, safety timers, and ‚Äúpower path management‚Äù for safely charging the battery while the device is on.Typical discharge curve for a 3.7V lipo battery (source: Adafruit)There are a few ways to measure the charge level of the battery, which generally relies on the fact that a lithium ion battery‚Äôs voltage depends on its charge level. A fully charged battery is about 4.2 volts, a battery with between 80% and 20% charge is about 3.7 volts, and below that a drained battery falls off pretty quickly to under 3.0 volts. If all you want is a coarse estimate of the battery level, you can use an ADC to read the voltage and estimate whether the battery is fully charged or nearly discharged. However, since the voltage curve is nearly flat between 20% and 80% charge (and is also dependent on the load), this can‚Äôt give the fine-grained battery percentage that we‚Äôre used to on phones and laptops. Instead, I opted for a discrete fuel gauge IC, the MAX17048. It‚Äôs simple to integrate and inexpensive.I decided to use a push button for the main power switch, because I needed to be able to do a graceful shutdown, where the microcontroller could save state (e.g. the current save file for an emulated cartridge) before it actually powered off.I briefly considered using an ultra-low power, always on microcontroller to act as a custom PMIC to provide power switch functionality (and perhaps avoid the need for a separate real-time clock IC, and even a battery gauge). While this would have been flexible and really cool, I figured it wasn‚Äôt worth the additional complexity.The main system power ranges from about 3.4 V when the battery is discharged, to 4.2 V when the battery is fully charged, up to 5.0 V when the device is plugged in with USB.The ESP32-S3 module required 3.3 V, and most of the other ICs in the system did too. The main exception is the FPGA, which requires a 1.0 V core power rail, a 1.8 V ‚Äúauxiliary‚Äù power rail, and a 3.3 V power rail for I/O. Moreover, according to the Xilinx Artix-7 datasheet (DS181), these power rails need to be powered on in a particular sequence: for my use, this means 1.0 V, then 1.8 V, then 3.3 V. Additionally, I needed a 5.0 V supply to interface with Game Boy / Game Boy Color cartridges.There are multi-rail power regulators available, and a lot of FPGA development boards use them. However, they all seemed to be expensive and difficult to purchase in low quantities. Instead, I opted for separate power regulators for each rail. I used buck converters instead of linear regulators to maximize power efficiency.I used the TLV62585 converter for the 3.3 V, 1.8 V, and 1.0 V rails. This is a simple, performant buck converter with a ‚Äúpower good‚Äù output, which is useful for power sequencing: you can connect the  output of one regulator to the  pin of the next regulator, to power on the rails in the desired order.For the 5.0 V rail, I used the TPS61022 boost converter. This converter is way overkill for the 5.0 V rail (which might use 75mA ), but it was readily available, and conveniently compatible with the same 1¬µH inductor as the buck converters.According to the FPGA datasheet, the XC7A100T consumes more than 100mW of static power. That is, it consumes that as long as it‚Äôs connected to power, even if it‚Äôs doing absolutely nothing. I figured I might want to support a low power sleep mode, so I decided to split the FPGA into a separate power domain with an explicit power enable signal from the MCU. I also used an AP2191W load switch for the FPGA‚Äôs 3.3 V rail to be able to keep the 1.0 V ‚Üí 1.8 V ‚Üí 3.3 V sequencing.I wanted the device to have both speakers and a 3.5mm headphone jack. Ultimately, the FPGA generates an I2S digital audio signal, and I needed a DAC to convert it to an analog audio signal, and then an amplifier to drive the speakers (or headphones). I wanted digital volume control (to support volume buttons, rather than a volume knob or slider), and I needed some way to switch the audio output between speakers and the headphones, depending on whether or not headphones are plugged in. With no real audio experience, this seemed like a daunting task.While searching for multiple separate components, I stumbled upon the TLV320DAC3101. It combines a stereo DAC with a speaker amplifier and a headphone driver. Additionally, it supports digital volume control, and headphone detection. I think this chip is a good example of how thoughtful component selection can simplify the overall design. Looking through the datasheet, it required a 1.8 V core voltage (unlike essentially every other component other than the FPGA) and a fair amount of configuration registers to set over I2C, but it had all of the features I needed.I was originally planning to have just a single (mono) speaker, but I figured if I had a stereo DAC, I might as well put two in there. I chose the CES-20134-088PMB, an enclosed microspeaker with a JST-SH connector. Having an enclosed speaker simplified audio design, because as it turns out, you can‚Äôt just stick a speaker to a board and expect it to sound okay (Same Sky, the manufacturer of that speaker, has a blog post explaining some of the nuances).I prefer the feeling of clicky, tactile buttons (such as those found in the GBA SP, Nintendo DS (original), Nintendo 3DS, Switch) compared to ‚Äúmushy‚Äù membrane buttons (such as those found in the Game Boy Color, original GBA, and Nintendo DS Lite). I learned that the tactile switches used in the GBA SP are a widely available off-the-shelf part from Alps Alpine. I used similar, but smaller buttons for the Start/Select/Home buttons, and a right-angle button from the same manufacturer for side volume and power buttons.Although I only had plans to support Game Boy and Game Boy Advance (requiring a D-pad, A and B buttons, L and R shoulder buttons, and Start/Select), I opted to add two more ‚ÄúX‚Äù and ‚ÄúY‚Äù face buttons to leave the possibility open of supporting more systems in the future.The L and R buttons posed an additional challenge ‚Äì I found numerous right-angle tactile buttons (to be soldered onto the back, facing towards the top). However, none of them seemed to have the actuator (the part of the button you make contact with) far enough away from the PCB to be easily pressed. At first, I thought about making a separate shoulder button board to move them at the correct distance, but then I started looking at what existing devices do for inspiration. The Game Boy Advance SP actually uses a more complex mechanism for the shoulder buttons: rather than a simple actuator like the face buttons, there‚Äôs a hinge with a torsion spring that hits the actuator at an angle. This is actually part of what makes the shoulder buttons pleasant to press: you don‚Äôt need to hit them from exactly the right direction, because they pivot. I ended up just going with a standard right-angle tactile button, opting to solve the problem with the mechanism in the enclosure.GBA SP shoulder button mechanismOne of my main goals was to allow ROM files to be loaded from a microSD card, rather than only being able to be played from a physical cartridge. To do this, I‚Äôd need dedicated RAM for the FPGA to hold the game. Game Boy Advance games, typically, are a maximum of 32 MB. They don‚Äôt make SRAMs that large (and if they did, they‚Äôd be very expensive). Instead, I needed to use DRAM.Asynchronous SRAM is very simple: supply a read address to the address pins, and some amount of nanoseconds later, the data you‚Äôre reading appears on the data pins. DRAM is more complex: the simplest kind is ‚Äúsingle data rate synchronous DRAM‚Äù (SDR SDRAM, or just SDRAM, distinguishing it from the significantly more complex DDR SDRAM). However, even SDRAM is non-trivial to use. DRAM is organized into banks, rows, and columns, and accessing DRAM requires sending commands to ‚Äúactivate‚Äù (open) a row before reading out ‚Äúcolumns‚Äù, and then ‚Äúprecharging‚Äù (closing) a row. Handling all of this requires a DRAM controller (see this simple description of the state machine required). This isn‚Äôt terribly complex, but I was signing myself up for more work.Alternatively, I could have chosen a PSRAM chip (essentially DRAM with an integrated controller to make it have a more SRAM-like interface). However, I couldn‚Äôt find a PSRAM part that I was happy with (cost, availability, interface), and so I ended up going with the inexpensive W9825G6KH 32MB 16-bit SDRAM.I also decided to stick a 512 KiB SRAM chip in the design in case I ended up needing some more simple memory later, like for emulating the SRAM used for Game Boy cartridge save files. Despite being 1/64 the capacity, this chip was about 3x the cost of the SDRAM. This ended up being a wise decision, since a lot of my internal FPGA block ram was eaten up by the triple buffer for the display (see above).Cartridge and Link Ports#The cartridge slot and link ports are no-name parts from AliExpress, easily available for cheap. These seem to mostly be GBA SP compatible, and are often used as repair parts.The Game Boy Advance can play both Game Boy [Color] and Game Boy Advance games. These run at different voltages and use different protocols, so the device needed some way of determining which type of cartridge is inserted.GBA cartridge (top) vs GB cartridge (bottom)The cartridges are physically different at the bottom: GBA cartridges (the top cartridge in the image) have a notch on either side. The GBA has a  that senses the absence of a notch on an inserted cartridge and switches the device into Game Boy Color mode.I measured the size and position of this notch, and searched Digi-Key and Mouser for switches that met these constraints. In the end, I was only able to find a single switch that would work.Miscellaneous peripherals#I used the surprisingly cheap LSM6DS3TR-C IMU from ST. This tiny IMU has a 3-axis accelerometer and gyroscope, more than sufficient for emulating the few GB/GBA cartridges that have motion controls.For keeping track of time even when the device was off, I used the PCF8563T real-time clock chip. I chose this because it was 1) I2C (no additional pins required), 2) cheap, and 3) readily available from JLCPCB. Interestingly, all of the real-time clock chips I found count in seconds/minutes/hours/days/months/years. This makes sense for a really simple device with minimal computational power. However, it‚Äôs annoying for my purposes, since all I really want is a timestamp I can pass to some other datetime library, and converting between the calendar time and a unix timestamp is non-trivial due to how the chips incompletely handle leap years.I picked up a few cheap coin vibration motors to use for vibration support (for the rare cartridge that had a built-in vibration motor).I also used a TCA9535 I2C I/O expander to connect the face buttons to the MCU. I ran out of pins, and while I  have used the FPGA as a sort of I/O expander, I figured I‚Äôd make it simpler for myself (and allow the buttons to be used even if the FPGA was powered off) by letting the MCU read them itself.For this project, as with my previous ones, I used KiCad to create my schematic and do PCB layout. I really can‚Äôt recommend KiCad enough: it‚Äôs a great program, intuitive to use, and it‚Äôs free and open source.This was a very ambitious project for my level of electrical engineering experience, and creating the schematic took a couple of weeks. I spent a lot of time designing the circuit for each component, because I was afraid I‚Äôd do something wrong and end up with a stack of useless boards without the skills needed to debug them. A lot of the component selection actually happened in parallel with schematic design, as I found new requirements or problems and had to change components.I gained a lot of experience reading component datasheets. It‚Äôs a really valuable skill, both for component selection and for creating designs that use the components. Nearly every datasheet has a ‚Äútypical application‚Äù section, where the manufacturer shows how the component would fit into a circuit. At minimum, this has power supply information (e.g. these voltages to these pins with these decoupling capacitors). For more complex components like the DAC, it also has information about power sequencing, different ways the device could be connected to the rest of the system, a register list, that sort of thing. Some components also included PCB layout recommendations. This information was all really helpful, and gave me a good deal of confidence that my board would work as long as I read through the datasheet and followed the manufacturer‚Äôs recommendations.Then I got to the FPGA. Nearly every component has a single datasheet. Some of them have an additional application note or two. Particularly complex chips (like the ESP32-S3 microcontroller) have a separate datasheet, reference manual, and hardware design guide. The Xilinx Series 7 FPGAs have . Overviews, packaging and pinout, configuration guides, BGA design rules, power specifications, clocking resources, I/O specifications, PCB layout guides, design checklists‚Ä¶ even a 4MB Excel spreadsheet for estimating power consumption! And believe me, Xilinx didn‚Äôt just write documentation for fun: there‚Äôs so much documentation because the chip  this much documentation.Designing with the FPGA was overwhelming, and  beyond my experience level. At several points I genuinely considered dropping the project altogether. Fortunately, I persevered, and gradually internalized a lot of the information. I also read through the schematics of any open-source Artix-7 development board I could get my hands on. Seeing what other people were doing gave me more confidence that I was doing the right thing.Eventually, after I placed all of the components, connected them, ensured all of the nets were labeled, and ran KiCad‚Äôs electrical rules checker (ERC) to find obvious mistakes, I moved on to layout.I did PCB layout at the same time as some of the initial enclosure CAD. The mechanics of how everything fit together influenced the placement of the display connector, cartridge slot, buttons, speakers, and connectors. After I came up with a plausible enclosure design, I placed some of the first key components onto the PCB and locked them into place while I did the rest of the routing.Rough enclosure design to help with board layoutI first focused on components that would be hardest to route. Primarily, the FPGA: the package I was using (CSG324) is a BGA, 18x18 with 0.8mm pitch between pins. ‚ÄúFanning out‚Äù all of the I/O signals requires careful routing, and at 0.8mm pitch, it‚Äôs difficult to do this routing with cheap PCB manufacturing techniques. I ended up being able to do this routing with a 6-layer PCB (three signal, two ground, one power), with 0.1mm track width and spacing, and 0.4/0.25mm vias. Fortunately, this is all within the realm of JLCPCB‚Äôs capabilities.BGA fanout with thin traces and small viasAs I routed signals out from the FPGA to other parts, I assigned those signals to the FPGA pins. Similarly, with the MCU, I assigned signals to pins in a way that made routing easier. Certain signals had restrictions (e.g. on the FPGA, the main 50 MHz clock signal can only go into certain pins, or the configuration bitstream can only go to certain pins, or certain pins are differential pairs for HDMI output), but overall, I had a lot of flexibility with pin assignment.KiCad has a feature where it automatically backs up your project as you work on it. I changed the settings to save every 5 minutes and not delete old backups, which allowed me to generate this timelapse of my layout process:Revision 1 board layout timelapseOnce I finished placing and routing all of the components, I ran the design rules checker (DRC) and fixed issues. I hesitated for a while before sending the PCB for manufacturing. I re-read the schematics, reviewed the layout, and eventually felt confident enough that I was done. I submitted the order to JLCPCB, and after a few questions by their engineers about component placement, they started manufacturing it.Board testing and bring-up#After two weeks or so, I received the assembled boards in the mail:An assembled board and an unassembled boardFirst, I probed the power rail test points with a multimeter to check for shorts. Then, I plugged the boards in for the first time, and pressed the power button. To my delight, the green LED turned on, indicating that the power button circuit, power path, and 3.3V regulator worked. The microcontroller USB enumerated, and I could see that it logged some errors (since I hadn‚Äôt flashed anything to it yet).I intended to write the MCU firmware in Rust, but I did initial board testing and bring-up with MicroPython. This would let me interactively type in Python and write basic scripts to communicate with the peripherals on the board and make sure I had connected everything correctly. I didn‚Äôt have to worry about writing efficient or well-organized code, and could just focus on functionality.I flashed the MicroPython firmware image, and wrote a couple lines of Python to blink the LED. I powered on the FPGA power domain, and checked that the , , and  rails had the correct voltage.Next, I wrote a simple bitstream for the FPGA that read the state of the buttons and produced a pattern on the shared signals between the FPGA and the MCU. I wrote simple Python code to configure the FPGA, loaded up the bitstream, and polled the signals from the FPGA. Pressing buttons changed the state, and confirmed that the FPGA was properly powered, and configurable from the MCU.After I confirmed the FPGA worked, I started writing a simple display driver to initialize the LCD and push some pixels from the MCU over SPI. The initialization sequence uses a number of LCD-specific parameters (voltages, gamma correction, etc.), that I learned from the LCD manufacturer‚Äôs example code.(Slowly) pushing pixels to the LCDThe LCD module‚Äôs controller, an ILI9488, has a few quirks: despite claiming that it supports 16-bit colors over SPI, it actually only supports 18-bit colors. This unfortunately meant that the MCU‚Äôs LCD driver would be more inefficient than I expected, since it has to expand 16-bit colors to 18-bit before sending them over the bus. This didn‚Äôt end up being a huge issue, however, because the FPGA is the one driving the display most of the time.Another quirk (hardware bug?) is that the ILI9488 doesn‚Äôt stop driving its SPI output line, even when its chip-select signal is inactive. This means that the chip will interfere with any other communication on the bus‚Ä¶ including the FPGA, which sits on the same bus. I never actually needed to read any data back from the LCD (and even if I did, it supports three-wire SPI), so I just cut the trace between the LCD‚Äôs SDO line and the SPI bus.Debugging the LCD test codeTrouble with power domains#I started trying to communicate with the I2C peripherals (I/O expander, RTC, etc.), and found that nothing was responding. A bit of probing with a logic analyzer revealed that the SCL/SDA lines were being held low, and that powering on the FPGA power domain let the lines be pulled high and communication to happen.I deduced that this was due to the DAC, which had its IOVDD powered by , which likely caused its protection diodes to pull the IO lines (SCL and SDA) low:The problematic portion of the schematicI tested out this theory by cutting the PCB traces connecting the DAC‚Äôs IOVDD and  with a knife. After this, I2C worked even with the FPGA power disabled. Then, I tested a possible fix by adding a wire to power the DAC‚Äôs IOVDD from the  rail. I confirmed that I could still talk to the other I2C devices, and once enabling FPGA power, that I could talk to the DAC too.While bringing up the LCD, I saw that the FPGA was also pulling down the shared SPI bus lines while it was unpowered. Not enough to prevent communication with the LCD, but it still wasn‚Äôt great. Between this and the DAC issue, I learned an important EE lesson: be careful when connecting components in different power domains together. A tristate buffer, such as the 74LVC1G125, could have helped here to isolate the buses.Once I2C was working, I wrote some basic driver code for the fuel gauge, real-time clock, IMU, and I/O expander, just to check that they all worked correctly. I also checked that the MCU could read from and write to the attached microSD card.Audio and video output from the FPGA#Next, I updated my testing FPGA bitstream to output a test pattern over the LCD parallel interface (‚ÄúDPI‚Äù), and a test tone to the DAC over the I2S interface. Then, I began poking on the MCU side to configure the LCD controller and DAC appropriately.With some amount of trial and error, I convinced the LCD to accept input from the FPGA. Most of the trial and error revolved around the rotation of the LCD module. Soon after, I configured the DAC properly, and it played the test tone from the FPGA over the speakers and the headphones.WIP video output from the FPGAAt this point, much of the board was working, so I soldered on the rest of the components (cartridge slot, cartridge switch, link port, shoulder buttons).With the cartridge slot in place, I had everything I needed to port over the Game Boy emulator from my last project. I did a quick-and-dirty port of the emulator, with some hacking around to connect the core to the audio, video, and the physical cartridge. I was able to play the first Game Boy game on the device far sooner than I was expecting:Pokemon Silver running from cartridgeI spent the next month or so implementing things on the FPGA. I started on the SPI receiver implementation, so that the MCU and FPGA could communicate.It was relatively straightforward to write the initial version, which 4x oversampled the SPI signals from the main system clock. For the Game Boy, that was ~8 MHz, for a maximum SPI speed of 2 MHz. The MicroPython ESP32-S3 SPI implementation supported only single SPI, so that allowed for a maximum transfer speed of 256 KB/s. This was sufficient to do most of my initial testing, but I later wrote an improved SPI receiver to run with an internal 200 MHz clock (from a PLL that turned on and off with the chip-select signal to save power), communicating with the rest of the system via a pair of FIFOs. This added a lot of complexity and edge cases, but it greatly improved performance, allowing the bus to run at 40 MHz.I wrote the SPI interface to the FPGA with memory-like semantics: each SPI transfer starts with a command byte, encoding whether it‚Äôs a read or write transfer, the size of each word in the transfer (8, 16, or 32 bits), and whether the ‚Äútarget address‚Äù should autoincrement as the transfer progresses. Then, a 32-bit address, followed by reading or writing the data. Each thing that the MCU might want to access (control registers, blocks of memory) are mapped into the 32-bit address space.As with my previous FPGA project, I wrote almost all of the FPGA code in Chisel, a Scala-based HDL. The remaining bits were the top-level Verilog. Chisel made it really simple to parametrize, compose, and test the various modules that I wrote.Once I had the SPI receiver working, I wrote controllers for the on-board SRAM and SDRAM. The SRAM was relatively simple (although I still got it slightly wrong at first). The SDRAM was a bit tricky, and even as I write this I‚Äôm not quite satisfied with its performance, and intend to rewrite it in the future.I exposed the SRAM and SDRAM interfaces to the MCU via SPI, which allowed me to read and write to these pieces of memory from the MCU. I used this a lot for testing: writing patterns into memory and reading them back to ensure that read and write both worked.Side note: SDRAM has to be continuously refreshed, otherwise the stored data decays over time. It depends on the chip, but typically each row has to be read and written back (or auto-refreshed, which does the same thing) at least once every 64 milliseconds to avoid losing state. What I found interesting, however, is that the data can actually persist for quite a bit longer. I discovered that when I was reconfiguring the FPGA between tests, most of the test data that I had previously written would still stick around even without being refreshed. In the first few seconds some bits would start flipping, and over the course of a few minutes, most of what was written was completely unintelligible.With the SDRAM controller and SPI receiver written, I was then able to implement the ‚Äúemulated cartridge‚Äù part of the Game Boy emulator, where the MCU reads a ROM file off of the microSD card and sends it to the FPGA to be stored in SDRAM. Then, the FPGA ‚Äúemulates‚Äù a cartridge (rather than interfacing with a real physical cartridge). After a few stupid mistakes, I was able to run test ROMs and homebrew. As an added bonus, since I was using my own SDRAM controller directly, I didn‚Äôt have any of the performance issues I‚Äôd faced before when accessing the ROM stored in memory.Writing the microcontroller firmware in Rust#By this point I had tested, in some form or another, all of the different components of the system. I‚Äôm really surprised that everything worked in my first board revision ‚Äì even the rework I did early on wasn‚Äôt actually required for functionality.I decided now was a good time to start building an interactive GUI. Up until this point, I had just been running commands in the MicroPython REPL. However, I didn‚Äôt want to build a whole UI in Python just to throw it away later, so I also started working on the ‚Äúproduction‚Äù Rust firmware.In the last few years, a lot of progress has been made towards making Rust on the ESP32 chips work well, even on the chips that use the Xtensa ISA. I followed the Rust on ESP Book and quickly had an environment set up. I opted for the ‚ÄúRust with the Standard Library‚Äù approach, so that I could benefit from ESP-IDF, especially the built-in support for USB and SD cards with the FAT filesystem.I started porting over the drivers I had written in Python. I found embedded Rust to be a bit verbose in some cases, but overall pleasant to use and worth the (little) trouble.I starting writing my own minimal GUI framework for basic menus. I poked around with the  library, but soon found that the typical patterns I was expecting to use weren‚Äôt a great fit for Rust. I also started planning out different screens and realized that I probably actually wanted to use a more comprehensive UI framework.Ultimately, I settled on Slint, a Rust-native declarative GUI framework with excellent support for embedded devices. Slint has a custom DSL to describe the UI and composable components. After a bit of practice I found myself to be really productive with it. I enjoyed using Slint, and I‚Äôd use it again in the future. The authors are responsive on GitHub, and the project has steadily improved over the year or so that I‚Äôve been using it.There were a few rough edges for my use case, however:The built-in GUI elements and examples were all heavily oriented around mouse or touchscreen navigation. Game Bub only has buttons for navigation, however, so I had to make my own widgets (buttons, lists) that worked with key navigation. This involved a few hacks, because Slint‚Äôs focus handling was a little bit simplistic.The built-in GUI styles looked (in my opinion) bad on a low DPI screen. Text was excessively anti-aliased and hard to read at small sizes. This was also fixed by building my own widgets.Slint doesn‚Äôt have a great story around supporting different ‚Äúscreens‚Äù ‚Äì I had to build some of my own infrastructure to be able to support navigation between the main menu, games, rom select, settings, etc.The GUI is rendered on the MCU, and then the rendered framebuffer is sent over to the FPGA. Slint supports partial rendering, where only the parts of the screen that have changed are updated, which improved performance. The FPGA maintains a copy of the framebuffer and ultimately is responsible for driving the display. This has a few advantages over driving the display directly from the MCU:Sending a framebuffer at 40 MHz QSPI to the FPGA is 16x faster than sending it to the LCD controller at 10 MHz (the fastest speed supported by the ILI9488)The UI is rendered at 240x160 to improve performance and maintain the GBA aesthetic, but the LCD controller doesn‚Äôt have a scaler, so the MCU would have to send 4x the pixels. The FPGA can easily scale the UI framebuffer itself.The FPGA can composite the emulator output with a semi-transparent ‚Äúoverlay‚Äù to support an in-game menu, volume / brightness bars, battery notifications, etc.An external display (e.g. monitor or TV) can be driven by the FPGA via HDMII spent some time making a variety of firmware improvements, mostly polish and quality-of-life. I added a settings screen to set the date and time, whether to use Game Boy (DMG) or Game Boy Color (CGB) mode when playing Game Boy games, etc.Then I improved the ROM select file browser, and added a battery level indicator.I also got sick of having to take the microSD card out of the device and connect it to my computer through a series of adapters (microSD to SD to USB-A to USB-C), so I implemented a basic utility to expose the microSD card as a USB Mass Storage Device, using TinyUSB and the ESP32-S3‚Äôs USB-OTG capabilities.It was a little bit more difficult than I expected, because USB Mass Storage requires the device to provide raw block access. This means that the filesystem has to be unmounted by the device, otherwise the device and host could conflict and corrupt the filesystem. The ESP32-S3 also only supports USB Full Speed, for a practical maximum transfer speed of ~600KB/sec. It‚Äôs really useful for transferring save files or updating the FPGA bitstreams, but less useful for transferring a large number of ROM files.Later, I implemented MBC7 support in the Game Boy emulator for Kirby Tilt ‚Äôn Tumble, using the on-board accelerometer.After I implemented a decent amount of software functionality, I decided to finish the enclosure design. The bare board just wasn‚Äôt cutting it anymore, and the taped LCD module/loose speakers/rubber-banded battery contraption was fragile.Game Bub looking rough without an enclosureI came into this project without any CAD or 3D printing experience. I looked at a few different CAD software packages, and I ultimately settled on FreeCAD, primarily because it was free and open source. I learned how to use the software with some video tutorials. FreeCAD, unfortunately, was a little bit rough around the edges and I ended up running into some annoying issues. Nevertheless, I powered through and finished the design.FreeCAD view of the enclosure and some buttonsI found parametric modeling, where the geometry of the model is defined by constraints and dimensions, to be intuitive. However overall, I found 3D CAD to be very time consuming. I think a large part of this is my inexperience, but thinking in three dimensions is a lot more difficult than, say, a 2D PCB layout. Creating a full assembly was even more difficult: I had to visualize how the front and rear pieces would fit together, where the screws would go, and how the buttons, screen, speaker, cartridge slot, battery, and ports would all fit in. This project definitely pushed the boundaries of my (previously non-existent) product design skills.After finishing the design, I printed out the technical drawing at a 1:1 scale and physically placed the board and other components down as a final check. Then, I sent it to JLCPCB for manufacturing. I opted for SLA resin printing, for high precision and a smooth finish.Enclosure technical drawingAfter a couple weeks, I got the finished enclosure and custom buttons back.Front and rear half, outsideI put the buttons, speakers, and screen into the enclosure, screwed on the PCB, and put the whole thing together.Assembling the front sideGame Bub, fully assembled and functionalI wasn‚Äôt sure how dimensionally accurate the 3D printing would be, so I added a lot of extra clearance around the buttons and ports. As it turned out, the printing was very precise, so the buttons rattled around a little in the oversized button holes.It‚Äôs a little bit chunky (smaller than an original Game Boy, though!) and the ergonomics aren‚Äôt ideal, but I was really happy to finally have an enclosure. It actually started (sort of) looking like a real product, and I wasn‚Äôt constantly worried about breaking it anymore.Game Boy Advance support#I won‚Äôt go into all of the details of how I wrote the emulator here (this article is already long enough!). If you‚Äôre interested, my previous article about my Game Boy FPGA emulator goes into detail about the general process of writing an emulator, and for a high-level introduction to the Game Boy Advance (from a technical perspective), I recommend Rodrigo Copetti‚Äôs article. In general, I tried to implement the emulator the way it might actually have been implemented in the original hardware: each cycle of the FPGA corresponds to one actual hardware cycle (no cheating!).As with the Game Boy, I did nearly all of my development with a simulator backed by Verilator and SDL. By the end of the development process, the simulator was running at about 8% of the real-time speed (on an M3 MacBook Air with excellent single-core performance), which was a bit painful.The Game Boy Advance CPU, the ARM7TDMI, is significantly more complicated than the Game Boy‚Äôs SM83 (a Z80 / 8080-ish hybrid). However, in some ways, it was easier to understand and implement: the ARM7TDMI is much closer to a simple modern processor architecture, and it‚Äôs extensively documented by ARM. For example, the ARM7TDMI Technical Reference Manual has block diagrams and detailed cycle-by-cycle instruction timing descriptions.I had a lot of fun implementing the CPU. The architecture has a three-stage pipeline (fetch, decode, execute) ‚Äì a division that feels natural when you implement it in hardware. The ARM7TDMI has two instruction sets: the standard 32-bit ARM instruction set, and the compressed 16-bit THUMB instruction set. I implemented the CPU the way it works in hardware, where the only difference between ARM and THUMB is the decode stage.As I was implementing the CPU, I wrote test cases for each instruction. Each test checks the functionality of the instruction: processor state, register values after, as well as the cycle-by-cycle behavior and interaction with the memory bus. This was helpful for catching regressions as I implemented more and more control logic. It was also really satisfying to be able to implement individual instructions, then write the tests, and check that everything worked.Chisel made it easy to write out the CPU control logic. The CPU control logic is a state machine that generates microarchitectural control signals (e.g. bus A should hold the value from the first read register, bus B should hold an immediate value, the memory unit should start fetching the computed address, etc.). Chisel allowed me to collect common functionality into functions (e.g.  to set up the signals to dispatch the next decoded instruction, or  to signal that the pipeline should be flushed and a new instruction should be fetched from the current program counter).I found it helpful to draw out timing diagrams with WaveDrom when working through instructions, especially to deal with the pipelined memory bus.My timing diagram of the ARM7TDMI branch instructionsBy mid-May (about a month later), I finished the CPU implementation (with occasional bug fixes after) and moved onto the rest of the system.PPU, MMIO, and everything else#Over the next month and a half, I implemented the majority of the rest of the Game Boy Advance. The CPU interacts with the rest of the system via memory-mapped IO (MMIO) registers. Unlike the Game Boy CPU, which can only access memory a single byte at a time, the ARM7TDMI can make 8-bit, 16-bit, and 32-bit accesses. This complicates MMIO, and the different hardware registers and memory regions in the GBA respond to different access widths in different ways.I started with the Picture Processing Unit (PPU), which produces the video output. The author of NanoBoyAdvance, fleroviux, had helpfully documented the PPU VRAM access patterns, which gave a lot of insight into how the PPU might work internally. Tonc was also immensely helpful for implementing the PPU and testing individual pieces of functionality.(Sort of) running a Tonc PPU demoThe PPU took a few weeks, and then I moved onto DMA, followed by hardware timers, and audio. Of course, as I‚Äôd try new tests, demos, and games, I‚Äôd uncover bugs and fix them.Kirby  in Dream LandGame Boy and Game Boy Advance cartridges use the same 32-pin connector. However, they work very differently. The Game Boy cartridge bus is asynchronous: the game outputs the 16-bit address (64 KiB address space) on one set of pins and lowers the  pin. Some time later, the 8-bit read data from the ROM stabilizes on a separate set of pins.For the GBA, Nintendo extended the bus data width to 16-bit and the address space to 25-bit (32 MiB). However, they kept roughly the same set of pins, accomplishing this by multiplexing the 24 data/address pins: the console outputs the address (in increments of the data word size of 16-bits, for a 24-bit physical address), then lowers the  signal to ‚Äúlatch‚Äù the address in the cartridge. Then, each time the console pulses the  pin, the cartridge increments its latched address and outputs the next data over the same pins. This allows for a continuous read of sequential data without having to send a new address for each access. The GBA also allows games to configure cartridge access timings to support different ROM chips.I had to do a lot of my own research here. Software emulators don‚Äôt need to care about the precise timing of the cartridge bus, so there wasn‚Äôt much documentation. To figure out the exact cycle-accurate timing, I used a Saleae logic analyzer and connected it to the cartridge bus. I wrote a test program for the GBA to do different types of accesses (reads, writes, sequential, non-sequential, DMA) with different timing configurations.Cartridge bus analysis setupAfter coming up with numerous scenarios (especially around the interaction between DMA and the CPU, and starting and stopping burst accesses), I came up with a consistent model for how cartridge accesses worked. I created some timing diagrams to help:Timing diagram of a non-sequential access followed by a sequential accessFinally, I started implementing the cartridge controller state machine based on my observations, paired with an emulated cartridge implementation. With the emulated cartridge, I was able to properly run real games in the simulator.I quickly implemented physical cartridge support, to be able to finally run it on the actual FPGA. I connected the signals, built a new bitstream, and‚Ä¶ it didn‚Äôt work at all. The Game Boy Advance boot screen ran, but it didn‚Äôt get any further than that. I implemented the emulated cartridge on the FPGA (reading ROM files from the SD card), and it worked! Which was great, but physical cartridges still didn‚Äôt.I used the logic analyzer to observe how my emulator was interacting with the cartridge compared to how an actual GBA, and found numerous issues.One of the first things I noticed was short glitches on the  line. I knew these had to be glitches (rather than incorrect logic), because they were 8 nanoseconds long, much shorter than the ~59.6ns clock period. Since the cartridge latches the address on a falling edge of , glitches cause it to latch an address when it shouldn‚Äôt, screwing up reads.Glitches on the cartridge busHere, I learned an important lesson in digital design: output signals should come directly from flip-flops, with no logic in between.After each flip-flop outputs a new value (on the rising edge of the clock), the signals propagate through the chip. As they propagate, taking different paths of different lengths throughout the chip, the output from each lookup table (LUT) is unstable. These values only stabilize near the end of the clock cycle (assuming the design met timing closure), and then each flip-flop stores the stable value at the next rising edge. If you output a signal from logic, this instability is visible from outside of the chip, manifesting as glitches in the output signal. If you instead output the signal from a flip-flop, it‚Äôll change only on each clock edge, remaining stable in the middle.And of course, I had written the cartridge controller without thinking about this, and  of the output signals were generated from logic. I rewrote the controller to output everything from flip-flops, which had a series of cascading changes since all of the signals now had to be computed one clock cycle earlier than I expected.There were other issues too ‚Äì part of the problem was that my emulated cartridge model was too permissive, and didn‚Äôt catch some fairly obvious incorrect behavior. After a few days of intensive debugging with the logic analyzer, I got to the point where I could play games from physical cartridges.Metroid: Zero Mission running from the cartridgeCartridge prefetch buffer#The ARM7TDMI has a single shared instruction and data memory bus. As a result, a long series of sequential memory accesses is rare. Even a linear piece of code without branches that includes ‚Äúload‚Äù or ‚Äústore‚Äù instructions would produce a series of non-sequential memory accesses, as the CPU fetches an instruction from one location, loads a register from a different location, and then goes back to fetching the next instruction.This poses a real performance issue on the GBA, because every non-sequential access from the cartridge incurs a multi-cycle penalty. Nintendo attempted to mitigate this somewhat with the ‚Äúprefetch buffer‚Äù (read this post by endrift, the author of mGBA, for more details) which attempts to keep a cartridge read burst active between CPU accesses. Without emulating the prefetch buffer, some games lag (I noticed this the most in Mario Kart Super Circuit, and some rooms of Metroid: Zero Mission).The prefetch buffer, while simple in theory, is not well documented and has a lot of corner cases and weird interactions. Emulator developers often start by taking a shortcut: making all cartridge accesses take a single cycle when the prefetch buffer is enabled. This wouldn‚Äôt work for me, since I actually had to interface with the physical cartridge.So, I set out to do some more research to figure out exactly how the prefetch buffer worked. After making some educated guesses and tests, I came up with a reasonable model of how it might work.Notes about the prefetch state machineActually implementing it took a lot of work, and I kept stumbling upon more and more corner cases. Eventually I got to the point where all games appeared to run at full speed, and most importantly, didn‚Äôt randomly crash. My implementation isn‚Äôt perfect: there are still a few mGBA test suite timing tests I don‚Äôt pass, but it‚Äôs certainly sufficient to play games.: standard duplex SPI, used for communicating with accessories: custom multi-drop UART-like protocol, used to link up to four GBAs together for multiplayer games: the Nintendo N64 and GameCube controller protocol, used to connect to a GameCube: duplex UART with flow control, : controlling the four pins individually as GPIO, The timing of these isn‚Äôt well documented, so I did my own research.A  mode transfer with no attached consolesI did a lot of testing with examples from the gba-link-connection library, intended for homebrew GBA games, but helpful for testing the different transfer modes in a controlled environment.Multiplayer Mario Kart with Game Bub and a GBAGame Bub linked to a GameCube playing Animal CrossingDuring the emulator development, I had used various test ROMs (mentioned before) to test basic functionality in isolation. As my emulator became mature enough to run commercial games, however, I started to shift some of my focus to accuracy-focused test ROMs.These test ROMs (such as the mGBA test suite) generally test really specific hardware quirks and timing. For example, they might test what happens when you run an instruction that ARM calls ‚Äúunpredictable‚Äù, or the exact number of cycles it takes to service an interrupt in specific scenarios, or the value of the ‚Äúcarry‚Äù flag after performing a multiplication. These are the kinds of things that don‚Äôt actually matter for playing games, but present a fun challenge and a way to ‚Äúscore‚Äù your emulator against others. This also highlights the collaborative nature of the emulation development community: people sharing their research and helping each other out.I won‚Äôt talk about all of the tests here (for my emulator‚Äôs test results, see this page). But I do want to mention the . This is an official test cartridge from Nintendo, likely used as part of a factory test or RMA procedure. Apparently, Nintendo has  used it to test their emulators (e.g. their GBA emulator on the Nintendo Switch). This test has generally been considered to be difficult to pass (it tests some specific hardware quirks), but it‚Äôs easier now that the tests have been thoroughly reverse engineered and documented. Still, passing it is a nice milestone:Passing the AGB Aging CartridgeSecond hardware revision#Towards the end of 2024, approximately one year after I originally designed Game Bub, I decided to make a second hardware revision. Over the past year, I had been keeping track of all of the things I would want to change in a future revision. Since the first version of Game Bub miraculously worked without any major issues, this list was primarily minor issues and ergonomics changes.I fixed the minor I2C power issues, removed the reference designators from the PCB silkscreen (they looked messy with the dense board, and I didn‚Äôt use them for anything anyway), and changed around some test points. I improved the rumble circuit to be more responsive, and switched to a PCB-mounted vibration motor.The first version of Game Bub was fairly thick, measuring 12.9mm at the top and 21.9mm on the bottom. The thickness of the rear enclosure was dictated by the thickness of Game Boy cartridges, but I made several changes to the front. I moved the  (8.5mm!) link port to the back, and removed the HDMI port (more on that later). I changed the headphone jack (5.0mm tall ‚Äì no wonder they started getting removed from phones) to a mid-mount one that sunk into the PCB and reduced the overall height.I also switched from an  module (3.1mm depth) to an  (2.4mm depth). I should have done this from the beginning, I just didn‚Äôt even know the ESP32-S3-MINI existed. This had the side effect of giving me 3 more GPIOs, which allowed me to put the FPGA and LCD on separate SPI busses, avoiding the minor issue of an unpowered FPGA interfering with LCD communication, and allowed for faster boot because the LCD could be configured at the same time as the FPGA.I switched the speakers, from the fully-enclosed CES-20134-088PMB to the CMS-160903-18S-X8. I made this change primarily for ease of assembly. The first speaker had a wire connector that plugged into the board, and I found it difficult to connect during assembly without having the wire interfere with buttons. The new speaker is smaller and has a spring contact connector, so it just presses against the PCB as the device is assembled. This required some speaker enclosure design ‚Äì an unenclosed speaker in free air sounds quiet and tinny.I reworked the layout of the face buttons and D-pad to match the spacing of the Nintendo DSi. This allowed me to use the silicone membranes from the DSi for an improved button feel and reduced rattling. I was also hoping to use the plastic buttons from the DSi (which were higher quality compared to my 3D printed buttons), but even with the new thinner design, the buttons weren‚Äôt quite tall enough to be easily pressed.I created another timelapse of my modifications to produce the second version of the PCB:Revision 2 board layout timelapseFor the second revision of the enclosure, I switched to Fusion 360 for the CAD work. While I would have preferred to keep using FreeCAD, I found that it was making it harder for me to be productive. Fusion 360 has a free version for hobbyists (with some limitations that have gradually increased over time), and overall I‚Äôve found it very pleasant to use.Fusion 360 view of the second enclosure, fully assembledUnlike with the first revision, I waited until I had a final design for both the enclosure and the PCB before getting anything manufactured. This let me go back and forth, making small modifications to each of them as needed.I wanted to make the end result look more polished and professional, so I contracted a factory to produce custom LCD cover glass, made out of 0.7mm thick tempered glass with a black silkscreen. It was relatively expensive for a low quantity order, but I‚Äôm really happy with how it turned out.Custom LCD cover glass with adhesive backingManufacturing and assembly#I got the PCBs manufactured and assembled, this time with black solder mask to look .Assembled PCB, revision 2I had two enclosures made. The first was black PA-12 Nylon, printed with MJF. Nylon is strong and durable, and the MJF 3D printing technology produces a slightly grainy surface that‚Äôs really pleasant to hold in your hand.Closeup of the nylon grainy textureThe second one was made of transparent resin (SLA, like before). This lets me show off the PCB that I worked so hard on, and evokes the transparent electronics trend from the 90s.Assembly was a lot easier this time around: the silicone membranes held the face buttons in place, the speakers had a spring contact instead of wires, and the shoulder button assembly was better. In the first revision, I had excessively large tolerances because I wasn‚Äôt sure how precise the 3D printing would be. In the second version, I was able to shrink these.The final product looked and felt a lot better, too. The edges were more rounded, and the device was thinner and easier to hold. The buttons felt  better to press and didn‚Äôt rattle around, and the cover glass over the LCD added polish.First revision (left), second revision (center and right)I previously mentioned that I removed the full-size HDMI port from the first revision. I had first planned to change it to a mini-HDMI or micro-HDMI port to reduce the size, but I was worried about durability.What I  wanted to do was output video through the USB-C port, avoiding the need for any HDMI port at all. Unfortunately, I had already concluded earlier that I wouldn‚Äôt be able to output DisplayPort video signals from the FPGA, which meant that I couldn‚Äôt use the standard USB-C DisplayPort alternate mode.However, an idea struck me towards the end of 2024: I didn‚Äôt actually  to use the DisplayPort alt-mode. The USB-C connector, in addition to the USB 2.0 D+/D- pins, has four differential pairs (for USB superspeed). Conveniently, HDMI  uses four differential pairs. The USB specification allows for vendor-specific alt-modes, so I could just implement my own, outputting the HDMI signal directly from the FPGA over the additional pins. Then I could build a custom dock that takes those pins and connects them to the data lines of an HDMI port.According to the USB specification, alternate modes must be negotiated by both sides first, using the USB-C Power Delivery (USB-PD) protocol, to prevent them from interfering with devices that aren‚Äôt expecting them. I don‚Äôt actually have a USB-PD controller in Game Bub (too much added complexity), so I took a shortcut: have a microcontroller in the dock communicate with the Game Bub over regular USB and perform a handshake before enabling HDMI output from the FPGA. Once Game Bub detects that it‚Äôs been disconnected from the dock, it can just switch back to using the internal display.I realized that the dock also presents another opportunity for controller support. I originally wanted to build wireless controller support into the handheld, but the ESP32-S3 only supports Bluetooth Low Energy, and the majority of controllers use Bluetooth Classic. Fortunately, the Raspberry Pi Pico W (with an RP2040 MCU) supports both types of Bluetooth, so I just decided to use that as the microcontroller on the dock. Game controllers connect to the dock over Bluetooth, and the Pico sends the controller inputs to the device. I wired up the  and  USB-C pins as a direct connection between the FPGA and the dock for low latency input.The RP2040 acts as the USB host, and Game Bub only needs to be a device. I also added a USB hub chip and some additional USB ports on the back of the dock to allow for wired controller support too. Just like with wireless controllers, the dock handles the direct controller communication, and just passes inputs back to the main Game Bub unit.Since the dock is so simple (comparatively), it only took about a day to design and lay out.I had also hoped to use the dock to solve another problem around HDMI output: HDMI sinks (monitors, TVs) pull the HDMI data lines up to 3.3 volts, and can actually backfeed power to the HDMI source. For Game Bub, this meant that a powered-off unit would turn itself on when connected over HDMI. I used a HDMI buffer chip in the dock to try to alleviate this problem, but the chip I used wasn‚Äôt actually properly suited to this use-case and interfered with video output, so I had to carefully rework the board to bypass the chip. I‚Äôll have to fix it in a later revision.Bypassing the HDMI buffer chipAfter the rework, HDMI output worked! The rest of the features are still a work in progress.Game Bub PCB on the dock, connected to an external monitorCongratulations on reading this far! This writeup ended up being incredibly long, even with a lot of details left out.I‚Äôm proud of what I accomplished over the last year and a half: I met all of my goals to produce a polished handheld FPGA retrogaming device. I pushed my electrical engineering and product design skills to the limit, and learned a lot in the process. Professional product and hardware designers deserve  respect.I deliberately designed this project with lots of possible extension opportunities to keep me occupied for a long time. I worked hard to get to the point where I‚Äôm comfortable sharing Game Bub with the world, but I still have a long list of TODOs for the future.In the near term, I‚Äôm going to work on finishing the dock, implementing wireless controller support (and maybe wired). I plan to use the Bluepad32 library to do so.I also want to improve the accuracy of my Game Boy Advance emulator: my goal here is to someday pass the entire mGBA test suite. I hope that I can contribute back to the wonderful  community with my emulator, and I plan to write-up some of my research around the GBA cartridge interface and link port.I have a long list of mostly minor changes to make to the MCU firmware: improving UI render performance, bits of polish like low battery notifications, eliminating display glitching when reloading the FPGA, and that sort of thing. I also plan to add more utilities, like a cartridge dumper and save backup/restore feature.Some day, I want to emulate the Game Boy Advance Wireless Adapter over Wi-Fi, e.g. with ESP-NOW. This won‚Äôt be compatible with the original wireless adapter, unfortunately, since that uses raw 2.4 GHz modulation rather than Wi-Fi.I designed Game Bub with extremely low production volumes in mind, using off-the-shelf commodity parts to keep the overall cost down. However, there are a few things I would have liked to be able to do, but are only possible with much higher volumes:A better LCD module (likely custom): native landscape mode to avoid the need for triple-buffering. Ideally a 720x480 resolution display, to allow for 3x GBA scaling and filter effects.High-quality injection molded case and buttons: 3D printing is great for low volume production, but an injection molded case would be great. It would be more precise (allowing for tighter tolerances), stronger, and allow for significantly more color options.Custom battery pack: or at least customizing the length of the connector wire. The current solution is hacky and doesn‚Äôt make the best use of internal space, due to limited off-the-shelf battery options.Smaller BGA parts for SRAM and SDRAM to free up board space (and move internal signals to 1.8 volts): this is actually something that would be possible in smaller volumes too, if I were willing to send parts from Mouser or DigiKey to JLCPCB for assembly.]]></content:encoded></item><item><title>Show HN: Letting LLMs Run a Debugger</title><link>https://github.com/mohsen1/llm-debugger-vscode-extension</link><author>mohsen1</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 09:54:14 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I just built an experimental VSCode extension called LLM Debugger. It‚Äôs a proof-of-concept that lets a large language model take charge of debugging. Instead of only looking at the static code, the LLM also gets to see the live runtime state‚Äîactual variable values, function calls, branch decisions, and more. The idea is to give it enough context to help diagnose issues faster and even generate synthetic data from running programs.* Active Debugging: It integrates with Node.js debug sessions to gather runtime info (like variable states and stack traces).* Automated Breakpoints: It automatically sets and manages breakpoints based on both code analysis and LLM suggestions.* LLM Guidance: With live debugging context, the LLM can suggest actions like stepping through code or adjusting breakpoints in real time.I built this out of curiosity to see if combining static code with runtime data could help LLMs solve bugs more effectively. It‚Äôs rough around the edges and definitely not production-readyI‚Äôm not planning on maintaining it further. But I thought it was a fun experiment and wanted to share it with you all.Check out the attached video demo to see it in action. Would love to hear your thoughts and any feedback you might have!]]></content:encoded></item><item><title>TL;DW: Too Long; Didn&apos;t Watch Distill YouTube Videos to the Relevant Information</title><link>https://tldw.tube/</link><author>pkaeding</author><category>hn</category><pubDate>Wed, 12 Feb 2025 02:15:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Mapping the Unix Magic Poster ‚Äì An Interactive Annotation Project</title><link>https://drio.github.io/unixmagic/</link><author>drio</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 22:22:37 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I built this as a static site that lets us annotate the Unix Magic poster by placing markers on references and adding descriptions to explain them. I've added a few so far, but there's much more to document.What I love about this approach is that contributions happen not just on the site itself but also through PRs, where we can discuss and refine the details of each reference. Feel free to send a PR!Would love feedback, suggestions, and PRs from the community!]]></content:encoded></item><item><title>Show HN: I made a tiny book using a pen-plotter and AI</title><link>https://muffinman.io/blog/the-tiny-book-of-great-joys/</link><author>stankot</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 17:52:31 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[If you are interested in how I over-engineered the process of making a tiny book for my wife, using AI, a pen plotter, a 3D printer, and a lot of time, you are in the right place. The book is titled  , and here is how it turned out:My wife is delighted with it, so it was worth all the effort.This post will take you through the process. It will be a long one, but please stick around - I promise there will be a lot of pretty pictures.Here is the outline of the post:I had this idea for a while after seeing something similar somewhere on the internet.. Since then, I always wanted to make one for my wife - a physically small book with a bunch of small drawings of our memories together, inside jokes, and little things she likes.I wanted the illustrations to be hand-drawn, and I had a plan to ask my friend to do them. But I knew he would refuse any kind of payment, so I felt bad adding more work to his plate. So I shelved the idea, but every now and then, it would pop up in my head.Fast forward a few years - we got a kid, and our routine completely changed. We are enjoying it a lot, but it can be very exhausting, and every day seems identical to the last. That's why I decided I needed to do something for her to break the routine. The book idea seemed perfect - personal and handcrafted - so I gave it a try.To be able to do everything myself, I went to create digital drawings and then draw them on paper using my trusty pen plotter.With the idea in place, I moved on to creating the drawings - which turned out to be a challenge of its own.For pen plotting, one needs vector files, so I started drawing in Figma. Unfortunately, I quickly realized that my drawing skills would not get me the result I had envisioned. Determined to do it this time, I decided to try using AI to generate images.I got myself a Midjourney subscription and started playing with it. It took a lot of failed attempts to figure out how to get drawings that were simple and had a strong hand-drawn feel to them. Even then, I ended up editing every one of them, but more on that later.One of the first images I was satisfied with (it didn't end up in the book, though):It took a lot of time, but it was fun. Failed attempts were often quirky and funny, and I was learning how to use the tool. And it made me feel like a secret agent, doing it next to my wife, who had no idea what I was up to.I may be wrong, but I think Midjourney wasn't built for the kind of illustrations I had in mind. I was after simple, hand-drawn illustrations that felt personal. Luckily, I found a style reference () that worked well for my case. I used it to generate almost all of the drawings that ended up in the book. For those who haven't used Midjourney - you can use images as style references to influence the style of images you want to generate.Most of my images were generated using that  code and a  between 150 and 400 (it can go from 0 to 1000).As for the prompts, these are the key terms I combined with the description and the style reference:isolated on white backgroundIt took me a lot of tries - between 10 and 30 attempts for each image you see in the book.Once I solved the image generation part, I had to figure out how to turn them into vector files for plotting. The first thing I tried was something similar to halftone. As you can see below, in this process, the images completely lost the hand-drawn feel.Then I remembered this plot of Marble Machine X I did a while ago, for which I used AutoTrace to convert the original image to a vector file. The great thing about AutoTrace is that it supports "centerline tracing". And this time, I learned that Inkscape has a great AutoTrace plugin, which made it even easier to convert.What makes centerline tracing differentMost of the tools that convert raster to vector images do it by outlining shapes. This is not suitable for plotting, as each line in the original image becomes a sausage-like shape. Centerline tracing, on the other hand, tries to draw a single line following the middle path through shapes. Don't worry if it sounds confusing; the example below should make things clearer.Here is the image of Link from  generated by Midjourney:After applying a common vectorization technique, we get this. As you can see, each line in the original drawing is now outlined, creating this messy-looking image.But if we use centerline tracing, it suddenly looks a lot more like a drawing. It is not perfect, but don't worry - we are going to clean it up in the next step.In the points where lines touch or cross, AutoTrace is not sure which line to follow and creates these funky-looking joints. Here is an exaggerated example to show you what I'm talking about. Input is the raster image at the top and the vectorized result is at the bottom:But I found out that if I roughly separate these lines, I get a much better result.Let's now apply this technique to the image of Link we've seen above. After separating lines (and some cleaning up) this is the image I ended up with. It is rough, but it is only used as an input for the tracing process, so it doesn't really matter. This was manual and somewhat tedious process, but I enjoyed it overall. It was a sort of meditation for me.And finally, when we trace this image, we get a really nice and clean vector file perfect for plotting. Here is another example. We start with the image I generated using Midjourney:After editing, removing details and separating lines, we get this one:And the traced vector result:You'll notice that in both examples I did  . I did that for pretty much all of the images, to fix things I wasn't able to polish using prompts. I also removed a lot of details to make sure images are crisp and readable at the small size.All of this took a lot of experimentation, but it gave me a pretty solid workflow which I used to generate all of the images. The complete flow looks like this:Generate images using Midjourney.Upscale them two times, because upscaled images were easier to edit and tracing was more precise.Clean up, redraw and separate lines by hand using Gimp.Use Inkscape plugin to run AutoTrace centerline tracing.It took me a while to generate all the images, and the fact that I was trying to keep it a secret from my wife didn't help. I think I did it over the span of two weeks, mostly in the evening after she would go to bed. never stood a chance!Before we continue I just want to show you two funky images of Link that really made me laugh:With the drawings ready, I turned to the next crucial part - the text. I first wanted to write everything by hand, photograph it and then vectorize it in the same way I did with the images. But it was a hassle - I had to do a lot of editing for text to look as my handwriting.Evil Mad Scientist, the maker of my pen plotter, has a fantastic tool called Hershey Text. It contains a bunch of single-line fonts ideal for plotting. I chose the EMS Elfin font as it looked playful and hand-drawn. I used it to write all of the text in the book and I think it turned out great.The tricky part with bookbinding is that pages are not printed in order, but in a way that when you fold the sheets in half, you get the right order. I used Figma to design the layout, with a great care to make sure pages are in order after double-sided plotting.Here is the layout laid out on A4 sized paper. Sorry for blurring the text, but a lot of it is very personal and I want to keep it for our eyes only.Plotting is the part that went the smoothest, but not without hiccups. I usually use Pigma Micron blackliner markers. They use archival quality ink and they are literally indestructible. But this time, even the thinnest one I had was too thick for the book this small.Here you can see the first  using markers of 0.2mm and 0.1mm thickness respectively. Lines got a bit smudged and looked much thicker than I expected. This was also the moment I realized I need to remove  from the images to make them readable at this size.I needed to find a thinner pen.Technical pen to the rescueBlackliner markers were made as a more practical replacement for technical pens. But from what I've read, an old-school technical pen was the only thing capable of achieving super-fine lines I wanted. I went online and ordered Rotring Isograph 0.2mm. As soon as it arrived I sneaked out to my study and did another test plot using it. Oh boy, was I happy when I saw the result:Lines were thin and crisp and at this point I was convinced the project will be a success!All of the first plots were done on 120gsm printer paper. It is somewhat thick paper and drawings looked fantastic. Unfortunately, when I bound the pages together, the drawings and letters would get transferred on the opposite pages. I could probably get away with it, considering the whole hand made feel of the book. But I wanted it to be perfect.A friend advised me to leave ink to dry for a few hours. I left each side to dry for 24 hours, but it smudged again. Next time I tried putting the plot (before cutting the pages) between two sheets of papers and pressing it with heavy books. I did that for more than 24 hours, but still after cutting and bounding the pages, they got smudged again. At this point I was becoming somewhat desperate. As the last resort I ordered different, 100gsm paper and to my relief it worked! Crisis averted!In the final version you can still see tiny traces on a few pages, but these are barely visible and don't really bother me.After plotting and cutting I was left with a stack of somewhat delicate pages. Now, it was finally time to turn them into a book.As you can imagine, I had zero bookbinding experience. There are a lot of resources online, but two of them were crucial for my project as they were on how to bind tiny books:After reading and watching these and a few generic articles on bookbinding, I gathered enough info to try doing it myself. I thought I was super clever because I 3D printed sides and spine of the book. I designed sewing holes in the spine so I can connect the pages directly to it without using glue. It was a decent idea, but it left a gap between two  . Still, I went with it for the first try.I laid everything down on the canvas that the book would be wrapped in and started assembling it. But I made a crucial mistake - I used super glue. It dries quickly, it is stiff, and doesn't glue 3D printed plastic well and it dissolved the paper I used. Long story short, I made a mess. But I didn't stress too much, I just proclaimed that version is a prototype and used it as a learning experience.I ordered proper bookbinding glue (PVA). While I was waiting for it, I focused on properly sewing the pages together.The first time I sewed the pages together, I poked the holes by hand and they were somewhat uneven. Again, it was nothing major, but I didn't like it. So I designed and 3D printed a simple tool to help me drill the holes evenly.The tool has two parts, and the pages fit snugly between them. Both top and bottom parts have holes, so I was able to put the needle through and poke perfectly even holes in the pages. I'm very proud of this silly contraption.Here you can see all of the eight sheets with sewing holes.Fun fact, I designed all 3D parts using JavaScript and Replicad library. Here is a link if you want to play with the model in your browser.But I ditched the 3D printed spine and used the technique called , which works great when you have only two signatures. It made signatures way more tight than when I connected them separately to the 3D printed spine.When the glue arrived, I plotted everything again and took it from the top. I swapped 3D printed sides for cardboard. Using proper glue was a game changer. I had enough time to apply it before it hardened, and when it dried it stayed flexible. And when it got onto my fingers, it was easy to remove. Everything was much cleaner, and I finally managed to put it all together.Unfortunately, I was rushing to finish the book, so I didn't take any photos of the process. But here are a few I do have:If you are an experienced bookbinder and reading this, I'm sorry for the bookbinding crimes I probably committed. I promise I won't use super glue again.It looked great! It was not perfect (more on that below), but I was super happy with how it turned out. It had a distinct handcrafted feel to it, the images turned out fantastic, and I think I really managed to bring out a personal touch with it.On the day I finished the book and gave it to my wife, we were both exhausted (our kid was teething, and we had a very rough night), so I thought she would appreciate a little pick-me-up.When I gave it to her, the first thing she asked was, "Will I cry?". She was brave, but it definitely got her all mushy and made her day. After reading, she carefully put it on the shelf, out of the reach of the little one.Then I asked her if she ever suspected I was preparing a surprise for her, and she said that she had no idea. But she also said that she thought it was weird that I would often plot something and not brag about it to her afterwards. It was true, I love showing her my work, but luckily she didn't give it too much thought, and I was able to finish my secret project.One thing I would like to fixLike I mentioned, the book isn't perfect. The sides are a bit too large, so the pages seem too deep inside when the book is closed. For the same reason, the end pages turned out to be a bit short, which gives it a weird, uneven look. It is purely aesthetic, but I think it is the only thing keeping it from being perfect.Lesson learned if I ever end up doing something similar.It took way longer than it should have‚Äîit took me a month and a half to finish it. It took so long because I did it in secrecy, which meant working late in the evenings when my wife and kid were asleep. A bunch of little failures... ehm, I mean  also prolonged the project. And finally, I had to order multiple things, so I was blocked a few times while I was waiting for four different deliveries.But the final assembly took me around two and a half hours from start to finish - plotting, cutting, sewing, and bookbinding. Mostly because I had already practiced all of them and defined the exact process.It was so much fun. I love projects that span across multiple disciplines. This one touched AI, drawing, plotting, modeling, 3D printing, sewing, and bookbinding. I encountered a lot of little hiccups, but I also learned about all of them. Some of the errors I made could have been avoided if I had been more patient. But I hope you'll cut me some slack - I was super excited and eager to see how it would turn out, and I had limited time windows when I could do it in secrecy. Still, I need to take it as a lesson - being patient will help me save time when doing projects like this one.The highlight for me was that I could do it without an illustrator. Love it or hate it, AI ended up being a fantastic tool that filled the gap in my skill set, which was crucial for making the book.I hope you enjoyed this write-up as much as I enjoyed making the book and writing the post. And I do hope I inspired you to try making something of your own. If I did, please reach out on GitHub, I would love to see it.]]></content:encoded></item><item><title>Show HN: Global 3D topography explorer</title><link>https://topography.jessekv.com/</link><author>jessekv</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 15:54:45 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
                Click catchements or regions on the map to render them in 3D.
                Learn more]]></content:encoded></item><item><title>Show HN: A unique generated maze to share with your valentine</title><link>https://love.berk.es/</link><author>berkes</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 14:35:55 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[ is an art project. It is so called generative art: I write the software, and the software creates art autonomously. 
            A nice introduction into generative art is this talk by Tim Holman.
          
          In a gallery in London, I came across gorgeaus screenprints by Ricky Byrne. I loved their use of color and hand-produced feel. Attention for color, layout, tension.
          So I started experimenting with maze generation algorithms in Rust, with Nannou.
          In the process, I decided to make it a web app, for valentine, so everyone can create their own maze. And ported the Rust code to TypeScript in a tiny web app. The Coding Train has a great tutorial set on maze generation. I used the same common recursive backtracking algorithm, because the aesthetics are what I was looking for. 
          The names you provide are used to generate unique randomnes. 
          This is used in a maze generation algorithm to create a maze. 
          I deliberately chose to animate the maze generation, to show the process. It is a slow process, but I think it is interesting to watch it carve out your maze. 
          Saving, copying and sharing the maze is disabled until the maze is generated. Once the maze is generated, you can save, copy and share the maze. 
          Sharing, copying won't work on all browsers and is affected by some browser plugins or settings. The best result is on Chrome on Android. 
          , the only parameter you can provide is the names. The maze is generated based on these names, and the algorithm is fixed. This is by design. Only the two names determine the artwork. 
            However, you can change the source code, see below.
          
            The code is available on GitHub. Feel free to fork, change and improve it, or just have fun with it. Nannou for the original maze generationVite for the build and web stuff]]></content:encoded></item><item><title>Show HN: HTML visualization of a PDF file&apos;s internal structure</title><link>https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md</link><author>desgeeko</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 13:52:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi,
I've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... 
I think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.
This is a pet project and I would be happy to receive some feedback!
Regards]]></content:encoded></item><item><title>Show HN: Infinite horizontal arrays of text editors</title><link>https://zeminary.com/arrays/app.html</link><author>tsydenzhap</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 01:25:05 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Searchable library of free audiobooks</title><link>https://booksearch.party/</link><author>libridev</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 21:52:33 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
                If you're seeking free audiobooks and an efficient book finder, booksearch.party is an excellent resource. This platform aggregates a vast collection of audiobooks for free from various sources, including LibriVox, Project Gutenberg, and Lit2Go, into a user-friendly, searchable database.LibriVox offers a wide range of public domain audiobooks, read by volunteers worldwide. Their catalog includes thousands of titles across various genres, all available for free.Project Gutenberg provides over 60,000 free eBooks, many of which have been converted into audiobooks. This extensive collection includes classic literature and historical texts, all accessible without cost.Lit2Go is a free online collection of public domain fiction, poetry, and nonfiction audiobooks. It offers a user-friendly interface where users can browse texts and listen to individual chapters.By compiling these resources, booksearch.party serves as a comprehensive book finder, allowing users to easily search and access a wide array of free audiobooks. Whether you're interested in classic literature, historical documents, or educational materials, this platform simplifies the process of discovering and enjoying audiobooks at no cost.]]></content:encoded></item><item><title>Show HN: My first side project, streamlined book clubs on Slack</title><link>https://booktalk.club/</link><author>Papamanolis</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 13:28:10 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Get personalized suggestions and find theperfect reads for your team]]></content:encoded></item></channel></rss>