<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Microsoft suspended the email account of an ICC prosecutor at The Hague</title><link>https://www.nytimes.com/2025/06/20/technology/us-tech-europe-microsoft-trump-icc.html</link><author>blinding-streak</author><category>hn</category><pubDate>Sat, 21 Jun 2025 12:06:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scaling our observability platform by embracing wide events and replacing OTel</title><link>https://clickhouse.com/blog/scaling-observability-beyond-100pb-wide-events-replacing-otel</link><author>valyala</author><category>hn</category><pubDate>Sat, 21 Jun 2025 09:23:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Our internal system grew from 19 PiB to 100 PB of uncompressed logs and from ~40 trillion to 500 trillion rows. We absorbed a 20Ã— surge in event volume using under 10% of the CPU previously needed. The required parsing and marshalling of events in OpenTelemetry proved a bottleneck and didnâ€™t scale - our custom pipeline addressed this. ClickHouse-native observability UI for seamless exploration, correlation, and root-cause analysis with Lucene-like syntax.About a year ago, we shared the story of LogHouse - our internal logging platform built to monitor ClickHouse Cloud. At the time, it managed what felt like a massive 19 PiB of data. More than just solving our observability challenges, LogHouse also saved us millions by replacing an increasingly unsustainable Datadog bill. The response to that post was overwhelming. It was clear our experience resonated with others facing similar struggles with traditional observability vendors and underscored just how critical effective data management is at scale.A year later, LogHouse has grown beyond anything we anticipated and is now storing over 100 petabytes of uncompressed data across nearly 500 trillion rows. That kind of scale forced a series of architectural changes, new tools, and hard-earned lessons that we felt were worth sharing - not least that OpenTelemetry (OTel) isnâ€™t always the panacea of Observability (though we still love it), and that sometimes custom pipelines are essential.In our case, this shift enabled us to handle a 20x increase in event volume using less than 10% of the CPU for our most critical data source - a transformation with massive implications for cost and efficiency.Other parts of our stack have also changed, not least due to the ClickHouse acquisition of HyperDX. Not only did this give us a first-party ClickHouse-native UI, but it also led to the creation of ClickStack - an opinionated, end-to-end observability stack built around ClickHouse. With HyperDX, weâ€™ve started transitioning away from our Grafana-based custom UI, moving toward a more integrated experience for exploration, correlation, and root cause analysis.As more teams adopt ClickHouse for observability and realize just how much they can store and query affordably, we hope these insights prove as useful as our first post. If youâ€™re curious about this journey, when and where OTel is appropriate, and how we scaled a log pipeline to 100PBâ€¦read on.Interested in seeing how ClickHouse works on your data? Get started with ClickHouse Cloud in minutes and receive $300 in free credits.Over the past year, our approach to observability has undergone a significant transformation. We've continued to leverage OpenTelemetry to gather general-purpose logs, but as our systems have scaled, we began to reach its limits. While OTel remains a valuable part of our toolkit, it couldn't fully deliver the performance and precision we needed for our most demanding workloads. This prompted us to develop purpose-built tools tailored to our critical systems and rethink where generic solutions truly fit. Along the way, we've broadened the range of data we collect and revamped how we present insights to engineers.When we last wrote about LogHouse, we were proud to handle 19 PiB of uncompressed data across 37 trillion rows. Today, those numbers feel like a distant memory. LogHouse now stores over 100 petabytes of uncompressed data, representing nearly 500 trillion rows.
Here's a quick look at the breakdown:These numbers also tell a story. In our original post, 100% of our telemetry flowed through OpenTelemetry, with every log line collected via the same general-purpose pipeline. But as the scale and complexity of our data grew, so did the need for specialization.
While our total volume has grown more than 5x, the breakdown reveals a deliberate shift in strategy: today, the vast majority of our data comes from â€œSysExâ€, a new purpose-built exporter we developed to handle high-throughput, high-fidelity system logs from ClickHouse itself. This shift marks a turning point in how we think about observability pipelines - and brings us to our first key topic.We hope the following helps comprehend the scale at which LogHouse operates.Initially, we used OpenTelemetry (OTel) for all log collection. It was a great starting point and an established industry standard which allowed us to quickly establish a baseline where every pod in our Kubernetes environment shipped logs to ClickHouse. However, as we scaled, we identified two key reasons to build a specialized tool for shipping our core ClickHouse server telemetry.First, while OTel capably captured the ClickHouse text log via stdout, this represents only a narrow slice of the telemetry ClickHouse exposes. Any ClickHouse expert knows that the real gold lies in its  - a rich, structured collection of logs, metrics, and operational insights that go far beyond whatâ€™s printed to standard output. These tables capture everything from query execution details to disk I/O and background task states, and unlike ephemeral logs, they can be retained indefinitely within a cluster. For both real-time debugging and historical analysis, this data is invaluable. We wanted all of it in LogHouse.Second, the inefficiency of the OTel pipeline for this specific task became obvious as we scaled.The data journey involved:A customer's ClickHouse instance writes logs as JSON to stdout.The kubelet persists these logs in An OTel collector collects these logs from the disk, parsing and marshalling the JSON into an in-memory representation.The collector transforms these into the OTel log format - again an in-memory representation.Finally, they are inserted back into another ClickHouse instance (LogHouse) over the native format (requiring another transformation within the ClickHouse Go client).Note: The architecture described here is simplified. In reality, our OTel pipeline is more involved. Logs were first collected at the edge in JSON, converted into the OTel format, and sent over OTLP to a set of gateway instances. These gateways (also OTel collectors) performed additional processing before finally converting the data into ClickHouseâ€™s native format for ingestion. Each step introduced overhead, latency, and further complexity.At our scale, this pipeline introduced two critical problems: inefficiency and data loss. First, we were burning substantial compute on repeated data transformations. Native ClickHouse types were being flattened into JSON, mapped into the OTel log format, and then re-ingested - only to be reinterpreted by ClickHouse on the other end. This not only wasted CPU cycles but also degraded the fidelity of the data.
Even more importantly, we were hitting hard resource limits on the collectors themselves. Deployed as agents on each Kubernetes node, they were subject to strict CPU and memory constraints via standard Kubernetes limits. As traffic spiked, many collectors ran so hot they began dropping log lines outright - unable to keep up with the volume emitted by ClickHouse. We were losing data at the edge before it ever had a chance to reach LogHouse.
We found ourselves at a crossroads: either dramatically scale up the resource footprint of our OTel agents (and gateways) or rethink the entire ingestion model. We chose the latter.Note: To put the cost in perspective - handling 20 million rows per second through the OpenTelemetry pipeline without dropping events would require an estimated 8,000 CPU cores across agents and collectors. Thatâ€™s an enormous footprint dedicated solely to log collection, making it clear that the general-purpose approach was unsustainable at our scale.Our solution was to develop the , or . This is a specialized tool designed to transfer data from one ClickHouse instance to another as efficiently as possible. We wanted to go directly from the system tables in a customer's pod to the tables in LogHouse, preserving native ClickHouse types and eliminating all intermediate conversions. This has the fantastic side benefit that any query our engineers use to troubleshoot a live instance can be trivially adapted to query historical data across our entire fleet in LogHouse, as the table schemas are identical, with the addition of some enrichment columns (such as the Pod Name, ClickHouse version, etc).Firstly we should emphasize that SysEx performs a literal byte-for-byte copy of data from the source to the destination. This preserves full fidelity, eliminates unnecessary CPU overhead, and avoids the pitfalls of repeated marshalling.The architecture is simple and powerful. We run a pool of SysEx scrapers connecting to our customer's ClickHouse instances. A hash ring assigns each customer pod to a specific scraper replica to distribute the load. These scrapers then run SELECT queries against the source pod's system tables and stream the data directly into LogHouse, without any deserialization. The scrapers simply coordinate and forward bytes between the source and destination.
Scraping system tables requires careful handling to ensure no data is missed due to buffer flushes. Fortunately, nearly all system table data is inherently time-series in nature. SysEx leverages this by querying within a sliding time window, deliberately trailing real time by a small buffer - typically five minutes. This delay allows for any internal buffers to flush, ensuring that when a scraper queries a node, all relevant rows for that time window are present and complete. This strategy has proven reliable and meets our internal SLAs for timely and complete event delivery to LogHouse.SysEx is written in Go, like most of our infrastructure components for ClickHouse Cloud. Naturally, this raises a question for anyone familiar with the Go ClickHouse client: how do we avoid the built-in marshalling and unmarshalling of data when reading from and writing to ClickHouse? By default, the client converts data into Go-native types, which would defeat the purpose of a byte-for-byte copy. To solve this, we contributed improvements to the Go client that allow us to bypass internal marshalling entirely, enabling SysEx to stream data in its native format directly from the source cluster to LogHouse - without decoding, re-encoding, or allocating intermediary data structures.This approach is broadly equivalent to a simple bash command:curl -s -u  | curl -s -X POST --data-binary @- An actual go implementation for the curious can be found here.Most importantly, SysEx doesnâ€™t require the heavy buffering that OTel does, thanks to its pull-based model. Because scrapers query data at a steady, controlled rate, we donâ€™t risk dropping logs when LogHouse is temporarily unavailable or when the source experiences a spike in telemetry. Instead, SysEx naturally handles backfill by scraping historical windows, ensuring reliable delivery without overloading the system or requiring complex retry buffers.One of the key challenges with the SysEx approach is that it assumes the source and target schemas match. But in reality, as any ClickHouse user knows, system table schemas change frequently. Engineers continuously add new metrics and columns to support emerging features and accelerate issue diagnosis, which means the schema is a moving target.To handle this, we generate schemas dynamically. When SysEx encounters a system table, it inspects and hashes its schema to determine if a matching table already exists in LogHouse. If it does, the data is inserted there. If not, a new schema version is created for this system table e.g. .At query time, we use ClickHouseâ€™s Merge table engine to unify all schema iterations into a single logical view. This allows us to query across multiple versions of a system table seamlessly. The engine automatically resolves schema differences by selecting only the columns that are compatible across tables, or by restricting the query to tables that contain the requested columns. This gives us forward compatibility as schemas evolve, without sacrificing query simplicity or requiring manual schema management.As we continued to scale and refine our observability capabilities, one of our primary focuses was capturing in-memory system tables, such as . Unlike the time-series data weâ€™ve been capturing, these tables provide a snapshot of the serverâ€™s state at a specific point in time. To handle this, we implemented a periodic snapshot process, capturing these in-memory tables and storing them in LogHouse.This approach not only allows us to capture the state of the cluster at any given moment, but also provides time-travel through critical details like table schemas and cluster settings. With this additional data, we are able to enhance our diagnostic capabilities by performing cluster-wide or ClickHouse Cloud-wide analyses. This we can join against service settings or query characteristics like used_functions to pinpoint anomalies, making it easier to identify the root causes of issues as they arise. By correlating queries with particular schemas, we further improved our ability to proactively identify and resolve performance or reliability problems for our customers.One of the many powerful capabilities we've unlocked with SysEx is the ability to take the same Advanced Dashboard queries  that customers use to monitor their individual ClickHouse instances and run them across our entire fleet of customer instances simultaneously.For release analysis, we can now execute proven diagnostic queries before and after deployments to immediately identify behavioral changes across our entire fleet. This has been rolled into our comprehensive release analysis process. Queries that analyze query performance patterns, resource utilization trends, and error rates complete in real time, allowing us to quickly spot regressions or validate improvements at fleet scale.Secondly, our support dashboards can now embed the same deep diagnostic queries that customers rely on, but with enriched context from our centralized telemetry. When investigating customer issues, support engineers can run familiar Advanced Dashboard queries while simultaneously correlating with network logs, Kubernetes events, data and control plane events - all within the same interface.The efficiency gains from this SysEx are staggering. Consider these stats from LogHouse: Use over 800 CPU cores to ship 2 million logs per second.LogHouse Scrapers (SysEx): Use just 70 CPU cores to ship 37 million logs per second.This specialized approach has allowed us to handle a 20x increase in event volume with less than 10 percent of the CPU footprint for our most important data source. Most importantly, it means we no longer drop events at the edge. To achieve this same level of reliability with our previous OTel-based pipeline, we would have needed over 8,000 CPU cores. SysEx delivers it with a fraction of the resources, maintaining full fidelity and consistent delivery.If youâ€™ve read this far, you might be wondering: when is OpenTelemetry still the right choice, and is it still useful?
We firmly believe that it is. While our architecture has evolved to meet challenges at extreme scale, such as parsing and processing over 20 million log lines per second, OpenTelemetry remains a critical part of our stack. It offers a standardized, vendor-neutral format and provides an excellent onboarding experience for new users - and is hence the default choice for ClickStack. Unlike SysEx, which is tightly integrated with ClickHouse internals, OpenTelemetry decouples producers from consumers, which is a major architectural advantage, especially for users who want flexibility across observability platforms.It is also well suited for scenarios where SysEx cannot operate. SysEx is pull-based and relies on querying live system tables, which means the service must be healthy and responsive. If a service is crash-looping or down, SysEx is unable to scrape data because the necessary system tables are unavailable. OpenTelemetry, by contrast, operates in a passive fashion. It captures logs emitted to  and , even when the service is in a failed state. This allows us to collect logs during incidents and perform root cause analysis even if the service never became fully healthy.
For this reason, we continue to run OpenTelemetry across all ClickHouse services. The key difference is in what we collect. Previously, we ingested everything, including trace-level logs. Now, we collect only info-level and above. This significantly reduces the data volume and allows our OTel collectors and gateways to operate with far fewer resources. The result is a smaller, more focused pipeline that still accounts for the 2 million log lines per second referenced earlier.Collecting all this data is just the beginning. Making it usable and accessible is what really matters. In the first iteration of LogHouse, we built a highly customized observability experience on top of Grafana. It served us well, but as our internal data sources grew and diversified, particularly with the introduction of SysEx and wide-column telemetry, it became clear we needed something more deeply integrated with ClickHouse.This challenge was not unique to us. Many teams building observability solutions on ClickHouse have encountered the same issue. Getting data into ClickHouse was straightforward, but building a UI that fully unlocked its value required significant engineering effort. For smaller teams or companies without dedicated frontend resources, ClickHouse-powered observability was often out of reach.HyperDX changed that. It provided a first-party, ClickHouse-native UI that supports log and trace exploration, correlation, and analysis at scale. Its workflows are designed with ClickHouse in mind, optimizing queries and minimizing latency. When we evaluated HyperDX prior to the acquisition, it was already clear that it addressed many of the pain points we and others had experienced. The ability to query using Lucene syntax dramatically simplifies data exploration and is often sufficient. Importantly, it still allows us to query in SQL - something which we still find essential for more complex event analysis - see â€œSQL for more complex analysisâ€.A key reason HyperDX was such a compelling fit was the schema-agnostic approach introduced in v2.0. It doesn't require log tables to conform to a single, rigid structure. This flexibility is critical for a system like LogHouse, which ingests data from numerous sources:It seamlessly handles the standardized, yet evolving, data format from our  pipeline.More importantly, it works out-of-the-box with the highly specialized, wide-column tables produced by  and our other custom exporters. It does this with no prior knowledge of the SysEx schemas, or complex  specializations. It simply inspects the schema behind-the-scenes and adapts to work with them.This means our engineering teams can add new data sources with unique, optimal schemas to LogHouse without ever needing to worry about breaking or reconfiguring the user interface. By combining HyperDX's powerful UI and session replay capabilities with LogHouse's massive data repository, we have created a unified and adaptable observability experience for our engineers.It is worth emphasizing that Grafana still has its place in our observability stack. Our internal Grafana-based application has some distinct advantages, particularly in how it handles routing and query scoping. Users are required to specify the namespace (effectively a customer service) they intend to query. Behind the scenes, the application knows exactly where data for each service resides and can route queries directly to the appropriate ClickHouse instance within LogHouse. This minimizes unnecessary query execution across unrelated services and helps keep resource usage efficient.This is especially important in our environment, where we operate LogHouse databases across many regions. As our previous blog post described, efficiently querying across these distributed systems is critical for performance and reliability. Weâ€™re currently exploring how we might push this routing logic to ClickHouse itself, allowing HyperDX to benefit from the same optimization..so stay tuned.In addition to its routing capabilities, Grafana remains the home for many of our long-standing dashboards and alerts, particularly those built on Prometheus metrics. These remain valuable, and migrating them is not currently a priority. For example, kube_state_metrics has almost become a de facto standard for cluster health monitoring. These high-level metrics are well suited for alerting, even if they are not ideal for deep investigation. For now, they continue to serve their purpose effectively.For now, the two tools serve complementary purposes and coexist effectively within our observability stack.Store everything, aggregate nothingThe development of SysEx has brought more than just technical gains. It has driven a cultural shift in how we think about observability. By unlocking access to system tables that were previously unavailable, where only standard output logs had been captured, we have embraced a model centered on wide events and high cardinality data.Some refer to this as Observability 2.0. We simply call it LogHouse combined with ClickStack.This approach replaces the traditional three-pillar model with something more powerful: a centralized warehouse that can store high-cardinality telemetry from many sources. Each row contains rich context - query identifiers, pod names, version metadata, network details - without needing to pre-aggregate or discard dimensions to fit within the limits of a metric store.As engineers, we have adapted to this new model, leaving behind outdated concerns about cardinality explosions. Instead of summarizing at ingest time, we store everything as is and push aggregation to query time. This approach allows for in-depth inspection and flexible exploration without sacrificing fidelity.One pattern we have found particularly impactful is logging wide events that include timeseries attributes in place of traditional metrics. For example, here is a log line from SysEx that tracks data pushed from a source ClickHouse instance to the LogHouse cluster:At this point, you may be asking: how is this different from a traditional metrics store like Prometheus?The key difference is that we store . We do not pre-aggregate fields like ; instead, we capture and retain each value and store it together.In contrast, a system like Prometheus typically stores either a gauge per series or, more commonly, pre-aggregates values into histograms to support efficient querying. This design introduces significant limitations. For example, storing time series for all label combinations in Prometheus would lead to a cardinality explosion. In our environment, with tens of thousands of unique pod names, each label combination would require its own timeseries just to preserve query-time flexibility. Pre-aggregating with histograms helps control resource usage but comes at the cost of fidelity. It makes certain questions impossible to answer, such as:"Which exact insert is represented by this spike in insertDuration - down to the specific instance, table, and time window?"With our approach, we avoid these trade-offs entirely. We log each event as a wide row that captures all relevant dimensions and metrics in full. This shifts aggregation and summarization to query time while preserving the ability to drill down into individual events when necessary.This model isnâ€™t entirely new. Systems like Elasticsearch have long encouraged the ingestion of wide events and flexible document structures. The difference is that ClickHouse makes this approach operationally viable at scale. Its columnar design allows us to store high-cardinality, high-volume event data efficiently - without the runaway storage costs or query latency that traditionally limited these kinds of approaches to storing events.The power of this approach is in how we can use that single event to draw many different conclusions by visualising its various characteristics, and we can always jump back to the raw logs from any given point on a chart.First, we can focus on a particular service and see its inserts line by line in series. This is the raw view upon the data.We can visualize the insert lag for all tables for this individual instance triviallyâ€¦We may go a layer up and visualise the insert lag for all servers in a region, which have lag > desired.And, because Observability is Just another Data Problem, we get to borrow all of the tooling in the data science space for our observability data, so we can visualise our logs in any tool of our choice for which ClickHouse either integrates directly or via a client library. For example, Plotly in a Jupyter notebook; plotly.express  px
 pandas  pd
 clickhouse_connect

client = clickhouse_connect.get_client(
â€¦
)
query = 

df = client.query_df(query)


df[] = pd.to_datetime(df[], unit=)
df[] = pd.to_datetime(df[], unit=)

fig = px.timeline(df, x_start=, x_end=, y=)

fig.update_traces(width=)
fig.update_layout(bargap=)

fig.show()
The plot shows scrape time versus wall time, allowing us to inspect each event for duplication. With Plotly, I could size the width of the rectangles as the exact start/end times. The annotations highlight a window where duplicate scrapes occurred, confirming the presence of overlapping data in that range.This plot illustrates the varying insert duration for some tables collected by the LogHouse Scraper.While I tend to prefer Plotly, we recognize that others may favor more modern visualization libraries. Thanks to ClickHouse's broad integration support, our SREs can choose the best tools for their workflows. Whether itâ€™s Hex, Bokeh, Evidence, or any other platform that supports SQL-driven analysis, they are free to work with the approach that suits them best.Here, we saw five views of the same event - demonstrating the flexibility we have to choose how we render at query time, using different charting tools, always with the ability to drill down into the raw line-by-line events.HyperDX offers a robust event search interface utilizing Lucene syntax, ideal for quick lookups and filtering. However, to answer more complex observability questions, a more expressive query language is needed. With ClickHouse as the engine behind LogHouse, we can always drop into full SQLSQL allows us to express joins, time-based operations, and transformations that would be difficult or impossible to perform in typical log query tools. One example is identifying pod termination times by correlating Kubernetes event streams. The query below uses ASOF JOIN to align Killing and Created events for the same container, calculating the time between termination and restart:
    KE 
    (
         loghouse.kube_events
         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )
    ),
    CE 
    (
         loghouse.kube_events
         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )
    )

    Name,
    KE.FirstTimestamp  killTime,
    CE.FirstTimestamp  createTime,
    createTime  killTime  delta,
    formatReadableTimeDelta(createTime  killTime)  readableDelta
 KE
ASOF  CE  (CE.Name  KE.Name)  (CE.FirstTimestamp  KE.FirstTimestamp)
 createTime  delta 
LIMIT â”Œâ”€Nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€killTimeâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€createTimeâ”€â”¬â”€deltaâ”€â”¬â”€readableDeltaâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ c-emerald-tu-48-server-p0jw87g-0 â”‚ 2025-03-10 19:01:39 â”‚ 2025-03-10 20:15:59 â”‚  4460 â”‚ 1 hour, 14 minutes and 20 seconds â”‚
â”‚ c-azure-wb-13-server-648r93g-0   â”‚ 2025-03-10 11:30:23 â”‚ 2025-03-10 12:28:50 â”‚  3507 â”‚ 58 minutes and 27 seconds         â”‚
â”‚ c-azure-wb-13-server-3mjrr1g-0   â”‚ 2025-03-10 11:30:23 â”‚ 2025-03-10 12:28:47 â”‚  3504 â”‚ 58 minutes and 24 seconds         â”‚
â”‚ c-azure-wb-13-server-v31soea-0   â”‚ 2025-03-10 11:30:23 â”‚ 2025-03-10 12:28:46 â”‚  3503 â”‚ 58 minutes and 23 seconds         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4 rows in set. Elapsed: 0.099 sec. Processed 17.78 million rows, 581.49 MB (180.05 million rows/s., 5.89 GB/s.)
Peak memory usage: 272.88 MiB.
Sure, we could write a component to track this as a metric, but the power of ClickHouse is that we donâ€™t need to do so. Itâ€™s sufficient to store a warehouse of wide events and derive the metric we need at query time from them. So, when a colleague asks, â€˜whatâ€™s the p95 replacement time for Pods after termination is requestedâ€™, we can just find a relevant set of events instead of responding, 'let me ship a new metric ', and getting back to them with an answer after the next release goes out.Sold on the immense value of having deep, structured telemetry in a high-performance analytics engine, we've been busy adding more data sinks to LogHouse, mainly at the request of our engineering and support team, who love using LogHouse and want all critical data to live in the warehouse. This year, we've embraced a cultural shift towards high-cardinality, wide-event-based observability as shown above.Some of our new data sources, which adhere to this wide event philosophy, include: Our open-source tool for monitoring Kubernetes networking, giving us deep insights into cluster traffic.  uses Linux's conntrack system to capture L3/L4 connection data with byte/packet counts. This provides three key capabilities: forensics (time-series connection records with per-minute bandwidth), attribution (mapping connections to specific workloads and pods), and metering (cost tracking for expensive data transfer like cross-region egress). The system processes millions of connection observations per minute, helping us identify costly cross-regional downloads, track cross-AZ traffic patterns, and correlate network usage with actual costs. You can find the project at https://github.com/ClickHouse/kubenetmon.Kubernetes Event Exporter: We forked the popular exporter and added a native ClickHouse sink, allowing us to analyze Kubernetes API events at scale. You can find our fork here. This is hugely useful for understanding why things changed in K8s over time. Weâ€™re not stopping there, however! Weâ€™re already working on a plan to ingest not just the events, but the entire k8s object model into LogHouse, with snapshots at every change. This would allow us to model the full state of all clusters at any moment in time over the past six months, and step through all of the changes. Instead of just knowing "Pod X was terminated at 15," weâ€™ll see the full cluster state before and after, understand dependencies, resource constraints, and the cascading effects of changes. We collect all operational data from our Control Plane department, who had not yet onboarded into LogHouse.Real User Monitoring (RUM): In a project that is still a work in progress, we collect frontend performance metrics from our users' browsers, which are pushed via a public gateway into our OTel pipeline. We ingest HTTP-level traffic data from our Istio service mesh, capturing request/response patterns, latencies, and routing decisions. Combined with ClickHouse's system.query_log and kubenetmon's network flows, this creates a powerful tri-dimensional correlation capability. When network usage spikes occur, our support team can trace the complete story: which specific SQL queries were executing, what HTTP requests triggered them, and the exact packet flow patterns. This cross-layer visibility transforms debugging from guesswork into precise root cause analysis - if we see unusual egress traffic, we can immediately identify whether it's from expensive cross-region queries, backup operations, or unexpected replication, making troubleshooting incredibly efficient for the support team.Itâ€™s been an incredible year of growth for LogHouse. By moving beyond a one-size-fits-all approach and embracing specialized, highly efficient tooling, weâ€™ve scaled our observability platform to remarkable new heights while significantly enhancing our cost performance. Integrating HyperDX is a key part of that evolution, providing a flexible and powerful user experience on top of our petabyte-scale data warehouse. We're excited to see what the next year brings as we continue to build on this strong foundation.While SysEx is designed to be efficient and resource-conscious, customers occasionally notice our scrape queries in their logs and metrics. These queries are tightly constrained with strict memory limits, but when they error (as they sometimes do) it can create concern. Although the actual resource impact is minimal, we recognize that even lightweight queries can create noise or confusion in sensitive environments.To address this, weâ€™re exploring what we call  - the next evolution of SysEx. The goal is to eliminate all in-cluster query execution by entirely decoupling scraping from the live system. One promising direction involves leveraging , where ClickHouse already writes its service logs. In this model, a pool of SysEx workers would mount these disk-based log tables directly, bypassing the need to query the running ClickHouse instance. This design would deliver all the benefits of our current system - native format, high fidelity, minimal transformation - while removing even the perception of operational impact.OpenTelemetry remains a critical component of our platform, particularly for early-stage data capture before service tables are available. This is especially useful during crash loops, where structured logs may be unavailable. However, if our zero-impact scraping approach proves successful, it could reduce our reliance on OTel even further by providing a high-fidelity, low-disruption path for log ingestion throughout the lifecycle of a cluster.This effort is still in progress, and weâ€™ll share more once weâ€™ve validated the approach in production.The JSON type has been available in ClickHouse for some time and recently reached GA in version 25.3. It offers a flexible and efficient way to store semi-structured data, dynamically creating columns with appropriate types as new fields appear. It even supports fields with multiple types and gracefully handles schema explosion.Despite these advantages, weâ€™re still evaluating how well JSON fits common observability access patterns at scale. For example, querying a string across an entire JSON blob can effectively involve scanning thousands of columns. There are workarounds - such as also storing a raw string version of the JSON alongside the structured data - but weâ€™re still developing best practices in this area.Culturally, we have also come to recognize the practical limits of the Map type, which has served us well. Most of our log and resource attributes are small and stable enough that the Map continues to be the right fit. We have found that single-level JSON logs are often all you need, and for exceptions, tools like HyperDX automatically translate map access into JSONExtract functions. While we plan to adopt JSON more broadly, this is still a work in progress. Expect us to share more in a future update.Over the past year, LogHouse has evolved from an ambitious logging system into a foundational observability platform powering everything from performance analysis to real-time debugging across ClickHouse Cloud. What began as a cost-saving measure has become a catalyst for both cultural and technical transformation, shifting us toward high-fidelity, wide-event telemetry at massive scale. By combining specialized tools like SysEx with general-purpose frameworks like OpenTelemetry, and layering on flexible interfaces like HyperDX, we have built a system that not only keeps up with our growth but also unlocks entirely new workflows. The journey is far from over, but the lessons from scaling to 100PB and 500 trillion rows continue to shape how we think about observability as a core data problem we are solving at warehouse scale.]]></content:encoded></item><item><title>Show HN: We moved from AWS to Hetzner, saved 90%, kept ISO 27001 with Ansible</title><link>https://medium.com/@accounts_73078/goodbye-aws-how-we-kept-iso-27001-slashed-costs-by-90-914ccb4b89fc</link><author>sksjvsla</author><category>dev</category><category>hn</category><pubDate>Sat, 21 Jun 2025 09:02:29 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[The European CTOâ€™s Dilemma: Keeping Compliance outside AWSEarlier this year, I faced a dilemma many tech leaders know well. Our entire infrastructure was built on AWS. We loved their powerful, ISO 27001-certified services. Yet, two critical issues kept me up at night:The Compliance Black Hole: It was clear that American cloud providers couldnâ€™t fully shield us from US government jurisdiction. Under the CLOUD Act and FISA, our European customer data was potentially exposed, regardless of the serverâ€™s physical location. This undermined our GDPR promises.The $2,000/Month Question: While not a fortune for every company, our $24,000 annual bill felt disproportionate to our actual needs. I asked myself: how often does a well-maintained Linux server actually crash? Isnâ€™t RDS just a managed Postgres instance with scripts I could write myself? That $2,000 a month could buy a phenomenal amount of resilient, dedicated hardware in Europe.This wasnâ€™t just about cost or compliance; it was a strategic risk. Was tying our companyâ€™s future to a single US-based provider a responsible choice?We are a Danish workforce management company doing employee scheduling. Beyond our ISO 27001 certificate, we have a few legal requirements on our operation as well as we perform overtime compensation salary adjustments and are source of truth for time-and-attendance data. Maintaining the tech side of this, is just like maintaining a bank software: Things must be accounted for, always add up and never be lost.Born and raised in AWS, many aspects of our legal requirement was architected as AWS native workflows and migrating that to independent alternatives always had to go along with legal requirements.Letâ€™s be honest: leaving AWS feels like walking away from a fortress of convenience. You lose the â€œmagicâ€ of deeply integrated services like Lambda, one-click RDS deployments, and the rich ecosystem of built-in compliance tooling that makes ISO 27001 audits smoother.Giving this up is the primary source of fear and inaction for most teams. It means trading the comfort of managed services for a higher degree of control and responsibility.By migrating to European providers like Hetzner and OVHcloud, the gains werenâ€™t just theoretical. They were immediate and strategic. Hosting on European-owned infrastructure gave us undeniable proof of data residency â€” a game-changer for GDPR audits and ISO 27001 recertification. We could tell our customers exactly where their data was, with no ambiguity. Our cloud costs dropped by . This wasnâ€™t a typo. By replacing expensive managed services with our own automated, self-hosted solutions, our budget became predictable and transparent. The biggest surprise was how losing AWSâ€™s pre-built tools forced us to get better. We built a powerful infrastructure-as-code setup using Ansible that gave us even tighter security controls and auditability than before.The Blueprint: Key Lessons for Your Own MigrationThis migration taught us invaluable lessons that can serve as a blueprint for others. Hereâ€™s the core of our strategy:Ansible as Your Compliance Engine: Forget simple compliance checks. With properly structured Ansible playbooks, you can tie every line of your server configuration directly to a specific ISO 27001 Annex A control. Your infrastructure code becomes a self-documenting audit trail.Monitoring That Rivals AWS: You donâ€™t need CloudWatch to have enterprise-grade monitoring. A combination of Prometheus, Grafana, and Loki allowed us to replicate â€” and in some ways exceed â€” the visibility we had on AWS, ensuring faster incident response.Security-by-Design Becomes Reality: When there isnâ€™t a pre-made security solution to click on, you build it into the foundation. This â€œsecurity-by-designâ€ approach, automated with Ansible, makes your ISMS (Information Security Management System) incredibly robust and easy for developers to follow.This wasnâ€™t just a technical project; it was a business transformation.We minimized our compliance risk regarding US surveillance laws.We used our European hosting as a sales tool, strengthening brand trust.We returned 90% of our cloud spend to the businessIf this story resonates with you, youâ€™re likely asking: â€œCould we actually do this? What would it cost? What are the hidden risks?â€Our journey created a repeatable playbook for migrating from AWS to a sovereign, cost-effective European cloud while maintaining ISO 27001 certification. I offer  for CTOs and founders facing this exact challenge.In a one-hour session, we can map out:A high-level cost analysis of your current AWS setup vs. a European alternative.The key compliance and ISO 27001 risks in your specific situation.A realistic timeline and the first 3 steps of a potential migration plan.Interested in exploring this for your company?Connect with me on LinkedIn and mention this article, or for a faster response, book a preliminary chat directly on my Calendly.]]></content:encoded></item><item><title>Cosmoe: BeOS Class Library on Top of Wayland</title><link>https://cosmoe.org/index.html</link><author>Bogdanp</author><category>hn</category><pubDate>Sat, 21 Jun 2025 09:00:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Cosmoe needs 2 main improvements to be a reliable, full-featured UI library:While I've made incredible strides with getting the BeOS class libraries to talk to Wayland, much work still remains to weed out crashes and incorrect behavior.  Wayland is powerful, but not friendly.Cosmoe implements about 95% of the BeOS API currently.  Notable feature that are not yet implemented include "offscreen" BBitmaps for accelerated drawing, and BFilePanel which implements Open and Save dialog boxes.  Some file-related classes like BVolume are only partially implemented.  Also, for security reasons, Wayland forbids certain Window-related actions such as window positioning and centering, so that functionality can never exist.For more details on this items and other in-progress aspects of Cosmoe, please see the TODO file in the Cosmoe repo.
				]]></content:encoded></item><item><title>&apos;Gwada negative&apos;: French scientists find new blood type in woman</title><link>https://www.lemonde.fr/en/science/article/2025/06/21/gwada-negative-french-scientists-find-new-blood-type-in-woman_6742577_10.html</link><author>spidersouris</author><category>hn</category><pubDate>Sat, 21 Jun 2025 07:38:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A French woman from the Caribbean island of Guadeloupe has been identified as the only known carrier of a new blood type, dubbed "Gwada negative," France's blood supply agency has announced. "The EFS has just discovered the 48 blood group system in the world!" the agency said in a statement on the social network LinkedIn. "This discovery was officially recognised in early June in Milan by the International Society of Blood Transfusion (ISBT)."The announcement was made 15 years after researchers received a blood sample from a patient who was undergoing routine tests ahead of surgery, the French Blood Establishment (EFS) said on Friday. The scientific association had until now recognized 47 blood group systems. The discovery was first reported by radio France Inter.Thierry Peyrard, a medical biologist at the EFS involved in the discovery, told AFP that a "very unusual" antibody was first found in the patient in 2011. However, resources at the time did not allow for further research, he added. Scientists were finally able to unravel the mystery in 2019 thanks to "high-throughput DNA sequencing", which highlighted a genetic mutation, Peyrard said.The patient, who was 54 at the time and lived in Paris, was undergoing routine tests before surgery when the unknown antibody was detected, Peyrard said. This woman "is undoubtedly the only known case in the world," said the expert. "She is the only person in the world who is compatible with herself," he said. Peyrard said the woman inherited the blood type from her father and mother, who each had the mutated gene.The name "Gwada negative", which refers to the patient's origins and "sounds good in all languages," has been popular with the experts, said Peyrard. He and colleagues are now hoping to find other people with the same blood group. "Discovering new blood groups means offering patients with rare blood types a better level of care," the EFS said.The ABO blood group system was first discovered in the early 1900s. Thanks to DNA sequencing the discovery of new blood groups has accelerated in recent years.]]></content:encoded></item><item><title>Delta Chat is a decentralized and secure messenger app</title><link>https://delta.chat/en/</link><author>Bluestein</author><category>hn</category><pubDate>Sat, 21 Jun 2025 06:29:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ðŸ’¬ Reliable instant messaging with multi-profile and multi-device supportAvailable on mobile and desktop.]]></content:encoded></item><item><title>Sega mistakenly reveals sales numbers of popular games</title><link>https://www.gematsu.com/2025/06/sega-mistakenly-reveals-sales-numbers-for-like-a-dragon-infinite-wealth-persona-3-reload-shin-megami-tensei-v-and-more</link><author>kelt</author><category>hn</category><pubDate>Sat, 21 Jun 2025 06:23:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Signal â€“ An Ethical Replacement for WhatsApp</title><link>https://greenstarsproject.org/2025/06/15/signal-an-ethical-replacement-for-whatsapp/</link><author>miles</author><category>hn</category><pubDate>Sat, 21 Jun 2025 05:21:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[You may not need any incentive to do this, but Iâ€™m going to argue that moving from WhatsApp to Signal is an important action, right now. There are other alternatives to WhatsApp, of course, but several have received criticism over various issues so Iâ€™m going to focus on Signal here. Please comment below if youâ€™d like to propose ethical alternatives to WhatsApp. Â In March I argued that we should choose Bluesky over X/Twitter, based on the obvious reason of boycotting companies connected to Musk. Bluesky is structured as a benefit corporation (like Patagonia) and based on an open network, two features that create strong incentives for the company to not upset its users.This creates an alignment between Bluesky and its users â€“ Bluesky is disincentivized to make a bad user experience. This also acts as a kind of poison pill against a billionaire takeover, or at least a deterrent against this.Moving from X/Twitter to Bluesky does require some sacrifice for people who worked hard to build up a following on the former. But weâ€™ve reached a point where support of Musk is unconscionable, whatever the sacrifice involved. The move from WhatsApp to Signal, however, is quite straightforward and completely painless. But in this case, some may be wondering, why should I bother moving at all?Ethical issues with WhatsAppMy biggest issues with using WhatsApp are related to its parent corporation, Meta (formerly Facebook) and its founder/CEO Mark Zuckerberg. But first, I want to mention a few issues specific to WhatsApp.In early 2021, WhatsApp updated its terms of service, requiring users to opt into sharing their data with Facebook â€“ including network details and location (even if you havenâ€™t turned on location sharing). This 180Â° reversal on an earlier promise made by Zuckerberg that â€œWhatsApp is going to operate completely autonomouslyâ€ was one of several events over the last 5 years that accelerated an exodus from WhatsApp to Signal. The large amount of metadata that WhatsApp collects is also shared with law enforcement agencies.WhatsApp shares metadata, unencrypted records that can reveal a lot about a userâ€™s activity, with law enforcement agencies such as the Department of Justice. Some rivals, such as Signal, intentionally gather much less metadata to avoid incursions on its usersâ€™ privacy, and thus share far less with law enforcement. â€“ ProPublicaI have to admit that I donâ€™t give a huge amount of time fretting over these things during normal times. But we no longer live in normal times â€“ people are now guilty by suspicion (or by association, based on their WhatsApp contacts) and convicted without due process.Over the years, Zuckerberg and Meta have done more flip flopping than a White Lotus guest (walking around in flip-flops, you see). Facebook has since been fined â‚¬110 million by EU antitrust regulators and $5 billion by the US Federal Trade Commission for deceiving regulators and users. Following this greater level of scrutiny, Zuckerberg has shown himself to be a person of little scruples, capitulating to whoever will do him favors.Meta â€“ a company so bad that it had to change its name to try to clean its reputation. The Facebookâ€“Cambridge Analytica scandal involved the collection of user data without consent to create psychographic profiles that were used in Donald Trumpâ€™s 2016 election campaign and suppressing Black voters in Trinidad and Tobago. WhatsApp, in particular, played a role in the Brazilâ€™s 2018 election.The vast majority of false information shared on WhatsApp in Brazil during the presidential election favoured the far-right winner, Jair Bolsonaro. The analysis sheds light on the spread of misinformation on the Facebook-owned app, with fears it could be poisoning political debate in one of the largest democracies in the world. â€“ The Guardian.Companies supporting Jair Bolsonaro are buying a service called â€œmass blasts,â€ using the candidateâ€™s list of WhatsApp users or buying lists from agencies specializing in digital strategy. â€“ Folha de S.Paulo.In 2018, Zuckerberg testified before US Congress, admitting: â€œIt was my mistake, and Iâ€™m sorry.â€Aww! Youâ€™re totally forgiven, Zuck! Youâ€™ve learned your lesson, right?But wait! Last year, as you probably know, Zuckerberg did the cowardly act of endorsing Trump by implication. I happened to catch this interview with the Zuck live on Bloomberg and was totally gagged (Hey Gen Z readers!). Take a look and marvel at Zuck sticking to his guns and maintaining his neutrality:Post-election, Zuck flew down to Florida for dinner with Trump (Iâ€™m imagining scenes from the hunting episode of ) and then got rid of DEI and moderation and all that weak stuff. It didnâ€™t take long for Zuck to switch from humility (It was my mistake, and Iâ€™m sorry) to going on the Joe Rogan show to say this:â€œMasculine energy is good, and obviously, society has plenty of that, but I think corporate culture was really trying to get away from it,â€ Zuckerberg continued. â€œI think having a culture that celebrates the aggression a bit more has its own merits that are really positive.â€ â€“ Mashable.Â Signal provides a very similar user experience to WhatsApp, so switching over is pretty seamless. This is not too surprising, considering that the current CEO of Signal LLC., Brian Action, was one of the cofounders of WhatsApp. WhatsApp was acquired by Facebook in 2014, and Acton left the company three years later due to differences around the use of customer data and targeted advertising.A year later, Acton contributed $50 million to launch a non-profit, the Signal Foundation and a subsidiary, Signal Messenger, LLC.On 21 February 2018, Moxie Marlinspike and WhatsApp co-founder Brian Acton announced the formation of the Signal Technology Foundation, a 501(c)(3) nonprofit organization whose mission is â€œto support, accelerate, and broaden Signalâ€™s mission of making private communication accessible and ubiquitousâ€. â€“ WikipediaWhen the Electronic Frontier Foundation ranked messaging apps for privacy and transparency, Signal was one of the few that received a perfect score. The nonprofit also has an updated guide to using Signal, navigating its settings, etc.The protection of data and personal privacy is important. The nonprofit Signal Foundation is led by Meredith Whittaker, a former director of the AI Now Institute at NYU, which examined the social impacts of AI and concentration of power in tech. She is a strong proponent of protecting privacy as a human right and an opponent of surveillance capitalism. Signalâ€™s predecessor, Open Whisper Systems received funding from journalism nonprofit Freedom of the Press Foundation.Signal has been recommended to Democratic Party staffers and officially approved by the US Senate in 2017. Just not for texting top secret plans, obviously!Action plan for migrating from WhatsAppItâ€™s easy to see how we all get drawn into using specific apps like WhatsApp â€“ the user base reaches a critical size and it becomes the app to find everyone on. At the same time, the company becomes more lucrative (and in many cases is taken over by a bigger fish) and then ethical issues creep in. We learn to suppress our concerns because the app is just too convenient and, to quote The Cranberries, Everybody Else Is Doing It, So Why Canâ€™t We? (R.I.P., Dolores).The exit plan from WhatsApp is quite simple. Start by installing Signal and setting it up â€“ it takes only a couple of minutes. Then, resume any WhatsApp conversations on Signal if that person is already a Signal user. If they are not, then switch to regular text messaging and gently suggest to that person to switch over to Signal. [Shout out to CeCe for reminding me to install Signal!] Group chats are a good way to get people to switch over as nobody will want to be the person who canâ€™t be bothered making the switch.The interference with elections is not OK.Iâ€™ll leave the last word to future president, AOC:Meta as in â€˜we are a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil societyâ€¦ for profit!'â€ â€“ Alexandria Ocasio-Cortez]]></content:encoded></item><item><title>Samsung embeds IronSource spyware app on phones across WANA</title><link>https://smex.org/open-letter-to-samsung-end-forced-israeli-app-installations-in-the-wana-region/</link><author>the-anarchist</author><category>hn</category><pubDate>Sat, 21 Jun 2025 03:06:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In recent months, we have received numerous reports from users across West Asia and North Africa (WANA) expressing alarm over a little-known but deeply intrusive bloatware applicationâ€”AppCloudâ€”pre-installed on Samsungâ€™s A and M series smartphones. Without usersâ€™ knowledge or consent, this bloatware collects sensitive personal data, cannot be removed without compromising device security, and offers no clear information about its privacy practices.AppCloud, developed by the controversial Israeli-founded company ironSource (now owned by the American company Unity), is embedded into devices sold in countries where such affiliations carry legal implications. Despite the serious privacy and security risks, Samsung has offered no transparency on how AppCloud functions, what data it collects, or why users cannot opt out.This open letter, addressed to Samsung, calls for immediate transparency, accountability, and dialogue. Users deserve to know what is installed on their devices and how their data is being used, especially amid Israelâ€™s espionage campaigns in the region.Â We are writing to urgently request that Samsung be transparent regarding the pre-installation of AppCloud on its A and M series smartphones, particularly in West Asia and North Africa (WANA). We ask that Samsung provide information about AppCloudâ€™s privacy practices, opt-out and removal options, and that Samsung reconsider future pre-installations in light of privacy rights. We also request a meeting with Samsung teams to discuss these concerns further.Â According to our analysis, this intrusive software is , deeply integrated into the devicesâ€™ operating system, making it nearly impossible for regular users to uninstall it without root access, which voids warranties and poses security risks. Even disabling the bloatware is not effective as it can reappear after system updates.Â The privacy policy is there is no accessible and transparent privacy policy for this bloatware and users are in the dark about what data is collected and how it is used. There is also no straightforward opt-out mechanism. The bloatware collects sensitive user data, including biometric information, IP addresses, device fingerprints.Â The installation of AppCloud is done from the user, which violates GDPR provisions in the EU and relevant data protection laws in the WANA region states.Â AppCloud is developed by ironSource, an Israel-founded company (now acquired by American company Unity), raising additional legal and ethical concerns in countries where Israeli companies are barred from operating, such as Lebanon. ironSource is notorious for its questionable practices regarding user consent and data privacy.Â Samsungâ€™s terms of service mention third party applications but do not specifically address AppCloud or ironSource, despite the significant data access and control granted to this bloatware app.Â The forced installation of AppCloud undermines the privacy and security rights of users in the MENA region and beyond. The lack of transparency and control over personal data is particularly alarming given Samsungâ€™s significant market share in the region.In light of these concerns, we respectfully request that Samsung:Disclose the full privacy policy and data handling practices of AppCloud, making this information easily accessible to all users.Offer a straightforward and effective method for users to opt out of AppCloud and remove it from their devices without compromising device functionality or warranty.Provide a clear explanation for the decision to pre-install AppCloud on all A and M series devices in the WANA region.Reconsider the continued pre-installation of AppCloud on future devices, in line with the right to privacy as established by Article 12 of the Universal Declaration of Human Rights.We also request a meeting with the relevant Samsung teams to discuss these issues in detail and to better understand the companyâ€™s approach to user privacy and data protection in the WANA region.We look forward to your prompt response and to working together to ensure the privacy and security of all Samsung users.]]></content:encoded></item><item><title>Tiny Undervalued Hardware Companions (2024)</title><link>https://vermaden.wordpress.com/2024/03/21/tiny-undervalued-hardware-companions/</link><author>zdw</author><category>hn</category><pubDate>Sat, 21 Jun 2025 02:19:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After playing/working with computers for more then 25 years I started to appreciate small but handy valuable stuff â€“ like adapters or handlers or â€¦ yeah â€“ all kind of stuff. With many of them I did not even knew they existed until I find out about them â€“ mostly accidentally or after long searching for some problem solution. Today I will share them with You â€“ so maybe they will end up handy also for You.â€¦ and while they make my life easier â€“ they are mostly very cheap too.The  is below.RJ45 Angle Cable AdaptersSATA to USB-C or USB-A AdaptersAngle USB-C and USB-A AdaptersTiny USB WiFi or Bluetooth DongleUSB-C <=> Micro USB AdapterUSB-C <=> Laptops/Routers/5.5mmx2.5mm AdaptersCreative BT-W2 USB-A Bluetooth AdapterExternal Microphone for SONY HeadphonesDual USB-C and USB-A Pendrive (SanDisk)Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card SlotC13/C14 Power Adapters with Additional C1/C2 or C5/C6 SocketsHDMI 3in1 Switch with Remote ControlThe whole article can â€˜feelâ€™ like a sponsored entry for the https://aliexpress.com portal â€“ but it is not â€“ its just the most cheap place I was able to find these gems. Feel free to share even cheaper one if You have one.I mostly use laptops to do various tasks and cables sticking out on the sides perpendicularly does not help. Not many laptops today have the RJ45 LAN socket â€“ but if they do â€“ they are mostly on the side of the laptop.Thanks to such angle RJ45 adapters it is no longer a problem.You can find them for about  â€“ for example â€“ on https://aliexpress.com page â€“ with the  keywords in their search.RJ45 Angle Cable AdaptersThe mentioned earlier  are quite bulky â€“ but as an alternative its possible to get a short 40cm cable with smaller plug.Not sure if its noticeable on the picture below â€“ but I also cut the top â€˜coverâ€™ with knife of the plug â€“ so its easier to detach.There are of course all four angles to choose from.One may also use the end of that 40cm cable-adapter as a â€˜stopperâ€™ to not fall inside the desk hole as shown on the image below.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.Often I found myself in a situation that the currently available LAN cable was too short to reach and it needed a lot of work to plot another â€“ longer one.With these simple â€˜joinâ€™ adapters it is no longer a problem. You would not use them in a serious Data Center with 10+ GE speeds â€“ but for home 1.0-2.5 GE speeds its more then enough.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.SATA to USB-C or USB-A AdaptersMultiple times I needed to clone some old disk to new SSD â€“ just to make an old system faster.I usually boot from some USB drive with FreeBSD and while new SSD is attached with these adapters â€“ I then execute  command to clone the old HDD disk to new SSD drive â€¦ and then just swap them out.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.Angle USB-C and USB-A AdaptersAs we already talked about RJ45 angle adapters â€¦ there are also USB-C and USB-A angle adapters.The do the same good job with cables to not stick out on a side of a laptop.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.In the progressing and always changing world yesterday the USB-A was king and tomorrow the USB-C will be.There are multiple cases in which you will need these â€“ from simple USB headphones to USB pendrives and other stuff.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.Tiny USB WiFi or Bluetooth DongleMultiple times I have found myself in a situation where it was very convenient to just add some WiFi or Bluetooth chip over USB port and do the job instead of trying to achieve the same without such chips.While I usually omit Bluetooth I can not say the same about WiFi â€¦ and as FreeBSD lacks a little in that department â€“ using a very tiny chip such as Realtek RTL8188CUS often does the job done.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  or  keywords in their search.USB-C <=> Micro USB AdapterIn the past â€“ in the USB Micro times â€“ I remember using an adapter to be able to charge â€“ then new and uncommon â€“ USB-C devices.Fast forward several years and now the situation is the other way around (as expected). The USB-C is the standard and USB Micro devices are less and less common â€¦ but there are still here. To not have to keep separate dedicated USB Micro cables I use a small USB-C to USB Micro adapters.Such adapter takes USB-C as input power and is able to charge USB Micro devices.You can find them for about  â€“ for example â€“ on https://aliexpress.com â€“ with the  keywords in their search.USB-C <=> Laptops/Routers/5.5mmx2.5mm AdaptersWhen it comes to delivering power to my (and not only) laptops â€“ the new standard seems to be the USB-C connector with â€˜requirementâ€™ of 45W or more (it depends).Not that long ago I discovered that even laptops as old as 13 years â€“  â€“ can be powered the same â€“ but with simple and very cheap adapter cables â€“ such as these below. From the left there is  typical router socket â€“ then more modern  (and many more) â€“ then oldschool models from 2011 year â€“  such as  models.All they need is a USB-C power input.You need to only meet two requirements â€“ the USB charger that will make enough power for example 20V at 3.25A for 65W that would power  or 20V at 6.75A for 135W that would power . While the official power supply for ThinkPad W520 is 170W â€“ its perfectly fine to use the 135W power adapter from  to power  laptop.This makes organizing cables (and chargers) a lot easier â€“ for example â€“ I would not be able to fit 3 â€˜dedicatedâ€™ ThinkPad chargers in that white cable organizer behind laptops â€“ but I will fir there two powerful 65W and 85W USB-C chargers perfectly fine.You can find these power adapters for about  â€“ for example â€“ on https://aliexpress.com â€“ with the USB-C ADAPTER LAPTOP ROUTER keywords in their search.Creative BT-W2 USB-A Bluetooth AdapterWhen I have to cope with Bluetooth technology â€“ its â€˜tolerableâ€™ on Android devices such as phones/tablets and mostly nowhere else. After bad audio (just not working) Bluetooth possibilities on FreeBSD I decided to try the hardware solution instead. The audio related Bluetooth on FreeBSD have failed me too many times â€“ to the point called  â€“ that also means I do not want to waste any more time trying to figure the way using FreeBSD Bluetooth stack devices anymore â€“ at least for audio related devices.Not so long ago I got the  headphones. I am/was a big fan of the  cable headphones (Jack or Mini Jack based). They have so much BASS and â€˜powerâ€™ that I could not ask for more â€¦ and their cost is very low â€“ like  or less. The only â€˜downsideâ€™ of the  headphones is that they are audio only â€“ they do not have any microphone at all â€“ they are dedicated for music only â€“ and that is OK â€“ they do GREAT in that role.I have tried some Bluetooth based headphones in the past â€“ and they were SHIT to say the least. Not enough â€˜powerâ€™ â€“ not enough BASS etc. After reading multiple reviews I decided to give  headphones a chance â€¦ and I was not disappointed. Its the first time after  cable headphones that ANY Bluetooth based headphone delivered. I was (and I still am) really satisfied with them.This is where the USB powered  comes handy. Its also relatively cheap as the cost of used unit is less then  â€“ at least that is the price I payed for mine in Poland. The  allows to connect Bluetooth audio devices everywhere â€“ even on OpenBSD â€“ on the system that cut off Bluetooth stack entirely â€“ and it works well on FreeBSD too. The â€˜downsideâ€™ of the  headphones is that they do have microphone â€“ but only in Bluetooth node â€“ they have Mini Jack connector â€“ but for audio only â€¦This is also only downside of the  solution â€“ it transmits only audio â€“ but w/o microphone. Its more then OK for listening music â€“ but if You have to do live conferencing/meetings on FreeBSD as I do â€“ its a dead end.I have tried to find a solution to this problem â€“ to the point that I wanted to abandon  headphones entirely and find some Mini Jack (or Jack) based BASS oriented headphones that will also have a working microphone.On my journey I have found a solution that I did not expected at all â€“ and that was the solution that solved all my problems â€“ and allowed me to enjoy the  headphones â€“ but more about that in the next â€˜subsectionâ€™.External Microphone for SONY HeadphonesYou already know the downsides of the  headphones that were giving me headaches. Now its time to address them.After many hours of searching the Internet I have found a very â€˜usableâ€™ Mini Jack cable. A cable that came with microphone and a one that perfectly integrated with  headphones â€¦ and FreeBSD as well.Its available to buy for  on  (and possible other locations) and its called . Thanks to the knowledge that  headphones have Mini Jack port with microphone part â€“ the  cable even comes with volume controls and even come with physical kill switch for microphone.After You attach this  to the  headphones it looks (and works) like a natural solution.The only â€˜downsideâ€™ is generally the downside of the  headphones â€“ that You CAN NOT disable their silencing while you speak â€“ so using them in â€˜passiveâ€™ mode with  is preferred to meet all needs.After reading comments to this article I learned that this â€˜silencingâ€™ is called  and it can be disabled in the SONY Android app or by holding  on the  until the headphones say â€œSpeak to Chat disabled.â€ Thank You for that.I got used to the fact that I just put my headphones on the desk â€¦ but I wanted something more useful â€“ after some searching it was obvious to me that I needed just some headphones handle that I could attach somewhere.After another several hours of browsing I have found a â€˜partâ€™ that would fit perfectly â€“ a  part from https://aliexpress.com that I could find with the  keywords in their search.Here is how it works on my desk.â€¦ and its 360 degrees adjustable as well.Dual USB-C and USB-A Pendrive (SanDisk)With all my â€˜badâ€™ experiences with PTP connections for Android based devices (and other places) I really liked the .Its really handy for many transfers â€¦ and its more fast then slow as well.When You need to connect several USB-A devices the USB ports count often come short fast â€“ this is where this tiny USB-A hub comes handy.With its dirt cheap  price (at https://aliexpress.com with  keywords) its a â€˜stealâ€™ â€¦ and it is a 3 port hub â€“ there is another USB-A port at the end of it â€“ the one that is not visible.Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card Slotâ€¦ as we are talking various USB-A or USB-C solutions I could not mention this quad port adapter with MicroSD card slot.I do not even remember how many times I have used it to copy/backup contents of my phone(s) and/or tablet(s).Nowadays I believe I use the Dual USB-C / USB-A Pendrive more â€¦ but not always.For  on its not a bad solution to have.Batteries â€¦ I mean SD card â€“ not included ðŸ™‚I have often found that the angle with which the power cord sticks out of a PC is definitely not ideal â€“ this is where angle power adapters come handy.Here is how it looks (being used) on my PC.C13/C14 Power Adapters with Additional C1/C2 or C5/C6 SocketsAfter You have spent some time to lay down the C13/C14 power cables just to power your PC its really annoying to do the same for another set of C1/C2 or C5/C6 cables/sockets â€¦ but not anymore.Now with single cable adapter You are able to power more then one computer â€“ depending on the needs with additional connectors.HDMI 3in1 Switch with Remote ControlI happen to have a 2010 FullHD 50 Inch TV that has ONLY ONE port of HDMI kind â€¦ and it was pretty annoying to say the least â€¦ up to the time I added a HDMI switch/hub to it.The HDMI switch along with its remote below.For the record â€“ I have used the UGreen 3in1 HDMI Switch with 4K @ 30Hz Capability and Remote and I was able to get one for .To not have a mess in the cables its useful to have them organized in some way.I use multiple solutions for that.Lets start with simple organizers.â€¦ and a larger/taller one for more capacity/possibilities.I also use some IKEA containers â€¦â€¦ and smaller boxes in which I keep the tiny things.I do not even remember after what product these boxes are â€¦ and that does not even matter I think.While there are many software settings or solutions to prevent screen from locking up â€“ there is one bulletproof solution what just always works â€“ a hardware USB mouse jigger.I use a very simple one with 3 modes â€“ but its more then enough for me needs.Last but not least â€“ the car FM transmitter.My daily â€˜realâ€™ driver (I mean on the real road outside) is the  car. I really love it for the simplicity and calm that it provides during the ride â€“ but on the audio side it only has an old FM/AM radio and a CD slot â€¦ and not MP3 support in that one.This is where the FM transmitter such as mine  comes really handy.It supports two modes. One is being a Bluetooth slave of your phone â€“ it just plays on the car speakers anything you are currently playing on your phone â€“ it also has microphone builtin â€“ so You can also use it as a â€˜loudâ€™ phone talking device.I use it in a more simple mode â€“ I just attach a tiny  pendrive to it â€“ and play a random song of it.Besides these features it also has additional USB-A port available to attach a cable to it and charge some device.I was able to get one a new one for about .The mentioned devices above are probably not the only ones that make my life easier â€“ but definitely the most crucial ones.Feel free to share your â€˜helperâ€™ hardware in the comments.]]></content:encoded></item><item><title>Learn you Galois fields for great good (2023)</title><link>https://xorvoid.com/galois_fields_for_great_good_00.html</link><author>signa11</author><category>hn</category><pubDate>Sat, 21 Jun 2025 00:21:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This is the introduction to a series on Abstract Algebra. In particular, our focus will be on Galois Fields (also known as Finite Fields) and their applications in Computer Science. This is a project I've been excited about for many years now, but have been too busy to dedicate the adequate effort to meet my perfectionism standards (yay perfectionism!).Many moons back I was self-learning Galois Fields for some erasure coding theory applications. I was quite disappointed with the lack of accessible resources for computer scientists.
Many resources assumed either:Its beyond your skill level so let's oversimplify ("it's hard, don't worry about it"), orYou had prior Pure Math studies in Abstract Algebra ("it's easy, just use jargon jargon jargon")Unfortunately, Abstract Algebra is not standard subject matter in most computer science curriculums. Often computer science mathematics start and end
with Discrete Math. If you're lucky, maybe you've also been exposed to Linear Algebra.So, ultimately, I ended up self-learning Abstract Algebra from a pure math textbook. But for the great majority of computer scientists, there has to be a better way.
This series intends to fill this gap. This is the gentle step-by-step approach with applications implemented with actual code. It's the intro I wanted when I was starting out.Abstract algebra is a beautiful subject. It's the idea that the numbers you're familiar with don't matter. The numbers are just
arbitrary labels. What matters is the relationships they have with other numbers when you add or multiply them. If the numbers
don't matter, then we can swap those labels for different labels and all the normal math rules will still work.For example, we could create an algebra that allows us to add or multiply colors:And this is what makes the subject abstract and confusing. How can you just say that numbers don't matter? It doesn't make sense.And even so, why would we want to study this? Why would a computer scientist care?Well, we use computer algorithms to manipulate data. We encode/decode it, we encrypt/decrypt it, we detect corruption, etc.
Wouldn't it be great if we could use normal math to do those things? Wouldn't it be great if would could add or multiply
an 8-bit byte by an 8-bit byte and get another 8-bit byte? And if we could do that, could we also do Linear Algebra over Data? Yes, yes, and more yes.
This is why studying Abstract Algebra is worthwhile.(Hint: Neither 263 nor 9282 are answers, they are not 8-bit numbers)You can also make quirky blog posts that make your friends think you've gone crazy, like milk and cookies ðŸ¥› ðŸª ðŸ˜ŠThe applications and algorithms are staggering. You interact with implementations of abstract algebra everyday: CRC,
AES Encryption, Elliptic-Curve Cryptography, Reed-Solomon, Advanced Erasure Codes, Data Hashing/Fingerprinting, Zero-Knowledge Proofs, etc.Having a solid-background in Galois Fields and Abstract Algebra is a prerequisite for understanding these applications.Approach: Step-by-step, Active Learning, and Literate ProgrammingIn this series, we will start from the very basics of theory and build up step-by-step to interesting applications such as Reed-Solomon,
AES, etc. As such, the material will be  cumulative. Many exercises will be included to aid understanding. Each section will build gradually,
but will assume mastery of the previous section. Active learning is . We will expect that
readers are putting in adequate effort to grok the abstract concepts.We won't assume too much mathematical background beyond high-school level algebra. However, in some applications (for example: Reed-Solomon),
familiarity with Linear Algebra will be required. We won't explain Linear Algebra since great resources already
exist here. You are encouraged to supplement as needed.We will be including code in a Literate Programming style where appropriate.  For example, to aid understanding, we will build some interactive
command-line tools that allow you to play around with various theoretical concepts in practice. All code will be the
Rust Programming Language, but advanced features will be intentionally avoided so that the code will be readable
by most experienced computer programmers.The main goal of this series is understandability and education. As such, the implementations will not be optimal. We will forgo nearly all
optimizations you'd see in a production quality implementation: lookup-tables, vectorized instructions, clever representations, computer
architecture optimizations, etc. It's possible that later posts in the series will discuss optimizations, but this is not a primary goal. At the end,
we hope these implementations will serve as good reference implementations. This can have it's own merit since highly optimized algorithms are
often difficult to read and understand.For active learning, I strongly encourage you to do your own implementations and to play around with the command-line tools while reading. If you'd
like to open-source your implementations, I'm more than happy to link them here:Original: xorvoid (Rust): herePlanning is Essential, Plans are WorthlessHere's the rough plan. We will see how it actually goes:Other possible advanced subjects:Extended Euclidean AlgorithmBit-matrix RepresentationsFast Multiplication with FFTsVectorization Implementation TechniquesThe first few sections are theory. There's not much coding in these sections, but they are very important for success later in the series.
Don't skip them.]]></content:encoded></item><item><title>Plastic bag bans and fees reduce harmful bag litter on shorelines</title><link>https://www.science.org/doi/10.1126/science.adp9274</link><author>miles</author><category>hn</category><pubDate>Fri, 20 Jun 2025 23:46:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AbsenceBench: Language models can&apos;t tell what&apos;s missing</title><link>https://arxiv.org/abs/2506.11440</link><author>JnBrymn</author><category>hn</category><pubDate>Fri, 20 Jun 2025 22:26:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD&apos;s Freshly-Baked MI350: An Interview with the Chief Architect</title><link>https://chipsandcheese.com/p/amds-freshly-baked-mi350-an-interview</link><author>pella</author><category>hn</category><pubDate>Fri, 20 Jun 2025 21:20:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Transcript below has been edited for conciseness and readability.And so we did that and delivered the fastest supercomputer in the world along with Lawrence Livermore, with El Capitan. But so as a consideration there, we wanted to have more compute units for XCD so that we could get 224 total within MI300A. On 350, where it's designed specifically as an accelerator only, a discrete accelerator, we had more flexibility there. And so we decided that having a power of two number of active compute units per die - so 36 physical, like you said, but we enable 32. Four of them, one per shader engine, are used for harvesting and we yield those out in order to give us good high-volume manufacturing through TSMC-N3, which is a leading edge technology. So we have some of the spare ones that allow us to end up with 32 actually enabled.And that's a nice power of two, and it's easy to tile tensors if you have a power of two. So most of the tensors that you're working with, or many of them, would be matrices that are based on a power of two. And so it allows you to tile them into the number of compute units easily, and reduces the total tail effect that you may have. Because if you have a non-power of two number of compute units, then some amount of the tensor may not map directly nicely, and so you may have some amount of work that you have to do at the end on just a subset of the compute unit. So we find that there's some optimization there by having a power of two.Alan: Yeah, so what we did, as you mentioned, so in MI350, the I/O dies, there's only two of them. And then each of them host four of the accelerator chiplets versus in MI300, we had four of the I/O dies, with each of them hosting two of the accelerator chiplets. So that's what you're talking about.So what we did was, we wanted to increase the bandwidth from global, from HBM, which, MI300 was designed for HBM3 and MI350 was specially designed for HBM3E. So we wanted to go from 5.2 or 5.6 gigabit per second up to a full 8 gigabit per second. But we also wanted to do that at the lowest possible power, because delivering the bytes from HBM into the compute cores at the lowest energy per bit gives us more power at a fixed GPU power level, gives us more power into the compute at that same time. So on bandwidth-bound kernels that have a compute element, by reducing the amount of power that we spend in data transport, we can put more power into the compute and deliver a higher performance for those kernels.So what we did by combining those two chips together into one was we were able to widen up the buses within those chips; so we deliver more bytes per clock, and therefore we can run them at a lower frequency and also a lower voltage, which gives us the V-squared scaling of voltage for the amount of power that it takes to deliver those bits. So that's why we did that.So when we do our total power and thermal architecture of these chips, we consider from the motherboard all the way up to the daughterboards, which are the UBB (Universal Baseboard), the OAM (OCP Accelerator Module) modules in this case, and then up through the stack of CoWoS (Chip on Wafer on Substrate), the I/O dies, which are in this intermediate layer, and then the compute that's above those. So we look at the total thermal density of that whole stack, and the amount of thermal transport or thermal resistance that we have within that stack, and the thermal interface materials that we need in order to build on top of that for heat removal, right?And so we offer two different classes of thermal solutions for MI350 series. One of them air-cooled, like you mentioned. The other one is a direct-attach liquid cool. So the cold plate would then, in the liquid cool plate, liquid-cooled case would directly attach to the thermal interface material on top of the chips. So we do thermal modeling of that entire stack, and work directly with all of our technology partners to make sure that the power densities that we build into the chips can be handled by that entire thermal stack up.If you would like to support the channel, hit like, hit subscribe. And if you like interviews like this, tell us in the comments below. Also, there will be a transcript on the Chips and Cheese website. If you want to directly monetarily support Chips and Cheese, there's Patreon, as well as Stripe through Substack, and PayPal. So, thank you so much for that interview, Alan.]]></content:encoded></item><item><title>Wiki Radio: The thrilling sound of random Wikipedia</title><link>https://www.monkeon.co.uk/wikiradio/</link><author>if-curious</author><category>hn</category><pubDate>Fri, 20 Jun 2025 21:15:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
    The thrilling sound of random Wikimedia
  
    Inspired by WikiTok, I thought I'd make something to discover
    sounds uploaded to Wikimedia. From political speeches and bird noises to genuine bangers,
    it's mostly wholesome, though I cant guarantee it won't play you something horrible once in a while.
    If you want shorter sounds, try it in Revolution 9 Mode.

      ]]></content:encoded></item><item><title>BYD begins testing solid-state EV batteries in the Seal</title><link>https://electrek.co/2025/06/20/byd-tests-solid-state-batteries-seal-ev-with-1000-miles-range/</link><author>toomuchtodo</author><category>hn</category><pubDate>Fri, 20 Jun 2025 20:42:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[BYD has now begun testing solid-state EV batteries in its Tesla Model 3-rivalling Seal. Initial tests suggest that the total driving range could reach nearly 1,200 miles (1,875 km).BYD begins testing solid-state EV batteries in the SealIt has been over a decade since BYD first began researching and developing the promising new EV battery technology.Last year, the company reached a milestone by testing its first solid-state battery cells with capacities of 20 Ah and 60 Ah. We knew BYD was planning to launch its first vehicles powered by the new batteries in 2027 after Sun Huajun, the CTO of BYDâ€™s battery business, confirmed the timeline earlier this year.At the 2025 China All-Solid-State Battery Innovation and Development Summit, Sun stated that BYD has officially installed solid-state batteries in its popular Seal EV and is now testing them on roads.Once testing is finalized, which is expected to occur in 2027, BYD plans to begin installing solid-state batteries in its production vehicles.Between 2027 and 2029, production will be limited during the first two years. However, in 2030, BYD plans to begin mass production. BYD has previously said that by the end of the decade, it expects â€œliquid and solid to be the same price.â€ In other words, solid-state batteries will be about the same cost as current liquid lithium-ion batteries.The Seal, BYDâ€™s Tesla Model 3-rivalling electric sedan, is expected to be the first EV available with solid-state batteries, starting in 2027. Other models will begin to hit the market in 2028 and the following years.BYDâ€™s solid-state batteries have an energy density of 400 Wh/kg, or nearly twice that of current lithium-ion batteries.According to local reports, BYDâ€™s solid-state EV batteries set a record by gaining 1,500 km (932 miles) range in just 12 minutes of charging. The test charged the battery to just 80%, meaning total EV range could reach upwards of 1,875 km (1,165 miles). Keep in mind, that is CLTC range. On the EPA scale, it would be closer to 1,300 km (808 miles), which is still way more than enough.BYDâ€™s Seal currently starts at just 175,800 yuan in China, or about $25,000. When it initially hits the market in 2027 with solid-state batteries, the Seal will likely be priced higher.BYD is already dominating the global EV market. It just surpassed Tesla in Europe and the UK in monthly registrations for the first time, and this could be just the start.With several new batteries and plenty of other EV technologies, including ultra-fast chargers, smart driving features, and advanced new platforms, BYD is laying the groundwork for more growth over the next few years.Not only that, BYD is already known for its low-cost cars like the Seagull (Dolphin Surf in Europe), priced under $10,000 in China. The new tech is expected to unlock longer driving range, faster charging, and lower costs.BYD will compete with CATL, Mercedes-Benz, Volkswagen, Stellantis, Nissan, and several others that are also aiming to launch their first EVs with solid-state batteries around 2027 or 2028. Nissanâ€™s director of product planning in Europe, Christop Ambland, confirmed the companyâ€™s timeline this week withÂ , saying, â€œWe will be ready for SSB (solid-state batteries) in 2028.â€FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>Show HN: Inspect and extract files from MSI installers directly in your browser</title><link>https://pymsi.readthedocs.io/en/latest/msi_viewer.html</link><author>rmast</author><category>dev</category><category>hn</category><pubDate>Fri, 20 Jun 2025 20:04:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Harper â€“ an open-source alternative to Grammarly</title><link>https://writewithharper.com/</link><author>ReadCarlBarks</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:51:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The JAWS shark is public domain</title><link>https://ironicsans.ghost.io/how-the-jaws-shark-became-public-domain/</link><author>MBCook</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:28:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As weâ€™re all celebrating the 50th anniversary of the movie , hereâ€™s something I bet you didnâ€™t know: Due to a fluke of publishing and copyright law, the  shark is public domain.Itâ€™s not the  of the shark thatâ€™s public domain â€“ or someone would surely be making a low-budget horror prequel about how he became the Amity Island Killer. But Iâ€™m talking about the famous shark painting from the movie poster:Yep. That painting, the same one that appeared on the cover of the paperback edition of the novel, is public domain.This is kind of a wild story.When the book first came out, it didnâ€™t have this cover art. An old article about the bookâ€™s origin explains that the author, Peter Benchley, actually had his own idea for the cover. He thought it should show â€œa peaceful unsuspecting town through the bleached jaws of a shark.â€ He pitched his idea to Doubleday, who was publishing the hardcover version of the book.So Doubleday senior editor Tom Congdon worked with art director Alex Gotfryd and had an artist produce this mock-up:Congdon didnâ€™t like it. He said, â€œthe sharkâ€™s bones look too liplike and pendulous.â€ A preview of the cover was shown at a book sales managerâ€™s conference, and there was â€œconsiderable resistanceâ€ from the salesmen who said it resembled a vagina with teeth.The book was supposed to come out in January, 1974, but publication was delayed until February to rework the cover.Congdon asked, â€œCan we have just a fish on the cover?â€But Gotfryd said no. â€œThe coverâ€™s not big enough. It will look like a sardine.â€So they settled on no image at all. The first printed cover was just black:Bantam had purchased the paperback rights to the book, but when Bantam president Oscar Dystal saw the empty cover, he didnâ€™t like it. He said, â€œWithout an image, no one would know what  was. It could have been a book about dentistry.â€ It needed a shark.So Gotfryd contacted an artist named Paul Bacon who made a rough sketch of the sharkâ€™s head, and at Gotfrydâ€™s suggestion added a swimmer for scale. The next day, Bacon came back with the finished artwork that became the new hardcover dust jacket:Congdon later said, â€œWe realized that the new version looked like a penis with teeth. But was that bad?â€Has any other design project ever gone from â€œwe canâ€™t use that because it looks like a vagina with teethâ€ to â€œthat looks like a penis with teeth but letâ€™s go with it?â€A year later, when Bantam was preparing to publish the paperback edition, they hired artist Roger Kastel to make an updated version of the cover. He went to the Museum of Natural History to study sharks, and he had a model pose across a couple of stools for reference of what someone looks like swimming. He used those elements in creating the now-famous illustration:There are slightly different accounts of how Universal acquired permission to use the illustration on the movie poster. Kastelâ€™s official website says that â€œUniversal Studios, so impressed by the work, purchased the right to use this image as the poster for the movie.â€But a 2012 article in  says that Universal actually got the rights from Bantam for free:Impressed by the cover, Universal purloined Kastelâ€™s work for the movie poster â€” Bantam books chief Oscar Dystel gave it to the filmmakers for free, losing out on millions of dollars â€” and it quickly became iconic.Whether Universal paid for the rights or not, Kastel became bitter about all the places it was showing up.In 2015, he told the New York Post, â€œWhat really bothered me was that they used the image for merchandising. You see that poster on everything.â€ And in 2020, Kastel told writer Michael P. Coleman, â€œWe were floored by how they merchandised that image, from t-shirts to cartoons.â€Itâ€™s unclear when exactly Kastel realized that there was something fishy about the paintingâ€™s copyright situation. I can only speculate that at some point he wondered if he was entitled to some of the money from all the licensing, and discovered that the copyright to the image had never been properly established.See, when he made the painting in 1975, copyright was still ruled by a 1909 law that said you had to include a copyright notice upon publication of a work, and that notice had to include your name. When the book was published, it carried no such notice for the artwork. It only had a copyright notice for the text. That meant that the painting became public domain as soon as it was published.In a bit of timing bad luck, a new copyright law enacted just a year after the book came out eliminated the notice requirement.So in early 2013, almost 40 years after  was published, Kastel filed a copyright application for the  illustration. But the copyright office denied it.He filed an appeal. They denied that, too.In 2014, he submitted a final appeal to the Copyright Office Review Board, and the decision from the board is available to read online. The main points are:The image was published without a proper copyright notice, so it became public domain under the law at the time.The fact that the book said â€œCopyright Â© 1974 Peter Benchleyâ€ isnâ€™t good enough to let the public know that the cover is copyrighted because thatâ€™s not the name of the cover artâ€™s copyright owner.This situation is not like magazines and anthologies, which are collective works with multiple contributors that can be covered by a single copyright notice. One illustration on the cover of a novel doesnâ€™t make it a collective work.If Benchley had licensed the artwork from Kastel or had some other legal relationship, that might make a difference. But he didnâ€™t, so the works are unrelated and require separate notices.So the review board unambiguously rejected the claim:The Board has concluded that the copyright notice in the Book, which includes only Peter Benchleyâ€™s name, does not meet the statutory notice requirement under the 1909 Act and, as such, the Work was forfeited to the public domain upon its publication.So whether Universal paid anything for permission to use the artwork becomes a moot point because it turns out that they didnâ€™t need permission. And anyone could have sold merchandise with the image without paying any licensing fees to Universal, Bantam, or Kastel.Kastel died in 2023 and, while he didnâ€™t see any  royalties from merchandise that featured his artwork, he did get more work as a result of it, including illustrating the inspired poster for .One thing that Kastel definitely did own is the original artwork used for the  book cover and poster. Unfortunately, the painting went missing in 1976 and nobody knows where it is.One story goes that the painting went on a national tour to promote the book, making stops to appear in various book store windows, and then disappeared somewhere in Hollywood. Or it may have been last seen hanging in an exhibit at the Society of Illustrators in New York.Kastelâ€™s son Matthew thinks it was last seen at the New York Historical Society. At least thatâ€™s what he said in an article a few years back for Daily Art Magazine but I suspect he was actually thinking of the Society of Illustrators. Either way, he wrote:But the question remains. What happened to my fatherâ€™s work?Where is one to start on a mystery over 45 years old with no clues? Outside of putting my fatherâ€™sÂ Â illustration on the back of milk cartons under the captionÂ , I have no clue.Was the painting simply lost, misplaced, or thrown away like an old movie prop by Universal out of lack of care or ignorance? Or was it stolen somewhere in Universalâ€™s care by an admirer and/or enterprising thief?My recourse is limited. By writing this article, I am taking a longshot approach that someone out there reading this may know about its whereabouts or fate and step forward. If you happen to know where the original  painting is, here is where you can reach out to Matthew Kastel.So thatâ€™s the story of how a pop culture icon became freely available for anyone to use. Itâ€™s good that creative works eventually become public domain, but I also believe an artist should be able to enjoy the fruits of his work in his lifetime if he wants to. So while Iâ€™m usually in favor of the commons, I feel like this time... itâ€™s personal.What a story. I was originally inspired by a reddit post where someone noticed that Wikimedia lists the original  dust jacket as public domain (the â€œpenis with teethâ€ illustration). I thought that was interesting and dug deeper. As I went down that rabbit hole I learned about the situation with Kastelâ€™s better-known paperback illustration.If you liked this article, you can show your support by becoming a paid subscriber of this newsletter, a free subscriber if you arenâ€™t already, or by giving a one-time tip, or just leaving a comment or reply to tell me you liked it.And hereâ€™s a personal fun fact for people who got this far: My wedding was at the Society of Illustrators. We got married surrounded by incredible paintings by artists like J. C. Leyendecker and Norman Rockwell. As far as I recall, the  poster art was not among them.Thanks as always for reading. See you next time!P.S. Have you played Gisnep lately?]]></content:encoded></item><item><title>EU Eyes Ditching Microsoft Azure for France&apos;s OVHcloud</title><link>https://www.euractiv.com/section/tech/news/scoop-commission-eyes-ditching-microsoft-azure-for-frances-ovhcloud-over-digital-sovereignty-fears/</link><author>doener</author><category>hn</category><pubDate>Fri, 20 Jun 2025 19:20:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The European Commission is in advanced business negotiations with OVHcloud, the France-based major European cloud service provider, to transition its cloud services away from Microsoft, according to three senior sources with internal knowledge of the matter who spoke to Euractiv on condition of anonymity.
The infrastructure shift is being driven by a push for European digital sovereignty in the cloud market, following concerns raised by a US executive order that led to the shutdown of Microsoft services for an employee of a European-based institution.

The goal of the move would be to ensure that European institutions have greater control over their digital infrastructure and data â€“ an idea that has been championed by the EuroStack initiative. It is also a blow for US tech behemoth Microsoft, which has been striving to reassure its European customers in the past weeks.

Once the European Commission "gets its house into order," it is expected to set a precedent for national public administrations to direct public procurement funds towards homegrown cloud providers, one source said. The Commission sees itself as a trend setter, they added, aligning with its broader strategy to enhance the EU's digital autonomy and reduce reliance on non-European tech giants.

We understand the Commission has been in discussions with OVHcloud for several weeks. However, an unknown number of other European cloud providers, including Germany's IONOS, France's Scaleway and Italy's Aruba, are also being considered as potential alternatives.

A unique aspect of this situation is that, for the first time, the two key digital departments of the Commission â€“ DG CNECT, which drafts and enforces digital policies, and DG DIGIT, the IT department â€“ are under the oversight of a single Commissioner (Henna Virkkunen) with a tech sovereignty portfolio.

This consolidation has made it easier to harmonise the political and technical priorities of the European executive, our sources told us.

"Discussions are indeed underway, both with the Commission and with other public and private institutions and organisations that are evaluating projects to migrate to a sovereign cloud," an OVHcloud spokesperson told Euractiv when we sought comment.

The Commission is "constantly scanning the market" and already "has a contract with OVHcloud" a Commission spokesperson said in response to Euractiv's request for comment. It did not confirm whether the Commission will actually switch away from Microsoft Azure.

In January, Euractiv revealedÂ that the Commission was concerned about its reliance on Microsoft, quoting internal documents.

Additionally, the EU institutions watchdog, the European Data Protection Supervisor found last year that the EU's executive is in breach of data protection rules that apply to EU institutions over its use of Microsoft Azure cloud for some of its data.
]]></content:encoded></item><item><title>Malicious AI swarms can threaten democracy</title><link>https://osf.io/preprints/osf/qm9yk_v2</link><author>anigbrowl</author><category>hn</category><pubDate>Fri, 20 Jun 2025 18:51:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>YouTube&apos;s new anti-adblock measures</title><link>https://iter.ca/post/yt-adblock/</link><author>smitop</author><category>hn</category><pubDate>Fri, 20 Jun 2025 17:01:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Over the past few months, YouTube has been trying another round of anti-adblock measures. Currently the anti-adblock stuff is being A/B tested, and one of my accounts is in the experimental group. I wrote a filter that partially avoids one of the anti-adblock measures, fake buffering, on uBlock Origin (and Brave browser, since it uses the same filter rules). (Itâ€™s already in the default filter lists, you donâ€™t need to manually add the filter.)One thing that people have ran into is â€œfake bufferingâ€, where videos will take a while to load due to a lot of buffering, but only at the very start of the video (thereâ€™s no mid-video fake buffering). As Iâ€™ll explain, the fake buffering is 80% of the length of the ads you wouldâ€™ve seen, so even with fake buffering youâ€™re still saving time using an adblocker.InnerTube is YouTubeâ€™s first party internal API that the web client and mobile apps use to interact with videos and get details about them. There are some legacy API endpoints that donâ€™t go through InnerTube, but none of those are relevant today. InnerTube endpoints look like https://www.youtube.com/youtubei/. One of those endpoints is , which the web client calls when you click on a video to get data about the URL and GVS stream URLs.GVS (Google Video Services) is a service that serves video streams for YouTube, Google Drive, and Google Photos. To stream a video from GVS, you need to get a GVS  URL from InnerTube (or the Drive/Photos internal API). GVS URLs are signed and have an expiry time (usually 6 hours), so you canâ€™t construct them on your own, you need to get one from InnerTube. One weird thing about GVS is that it isnâ€™t only hosted from Googleâ€™s data center: ISPs can put Google Global Cache servers in their infrastructure so that they can serve YouTube videos without needing to send traffic outside the ISPâ€™s network (only public/unlisted YouTube videos are served by GGC; private YT videos and Drive/Photos videos are always served from Google data centers). GVS URLs look like https://rr1---sn-gvbxgn-tt1e6.googlevideo.com/videoplayback?expire=1750321185&... (but with a lot more query parameters).Originally the web client streamed video by just using some query parameters to the GVS URL to specify what range of video it wanted, and GVS responded with the video contents for that range. But for complicated reasons, YouTube decided to improve on this with SABR (Server ABR (Adaptive Bit Rate)), which is YouTubeâ€™s proprietary binary protocol for streaming video data, which is better at avoiding buffering than todayâ€™s open formats (e.g. MPEG-DASH). One thing that SABR supports is the server sending a backoff to the client, instructing the client to wait some amount of time before trying again instead of sending video/audio data.The source of fake bufferingWhatâ€™s happening is that InnerTube is providing GVS streams that will give a backoff of 80% of the ad duration for ads for the first  request (for the content video, not the ads), so for example if the ad is 15 seconds youâ€™ll get 12 seconds of backoff when blocking ads. If you have an unskippable 6 second ad AND a 15 second unskippable ad together the backoff will be 16.8 seconds. To be clear this isnâ€™t server-side ad insertion; the ad and content streams are still separate (YouTube  doing a server-side ad insertion experiment, but thatâ€™s separate from fake buffering). The â€œExperiencing interruptionsâ€ dialogs are likely triggered by long backoffs from GVS.This backoff  happen if youâ€™re in the A/B test, regardless of if YouTube thinks youâ€™re using an adblocker. You just donâ€™t notice it if youâ€™re not blocking ads, because the web client starts loading the content video while the ad plays, so the only difference for non-ad-blocking users is that the content video doesnâ€™t start buffering until the ad is 80% over.Iâ€™ve seen claims online that YouTube is â€œDAMAGING Computers By SPIKING CPU Usage If You Use Ad Blockâ€. This is completely false; YouTube doesnâ€™t use CPU usage waiting for the backoff to expire (and even if they used a spinloop to implement the wait, maxing a single core for <30 seconds wonâ€™t damage any CPU).How can you avoid getting backoffed until the unskippable ad is over? Donâ€™t get served an ad in the first place. If you set the playbackContext.contentPlaybackContext.isInlinePlaybackNoAd property in player requests to true, InnerTube wonâ€™t serve you any ads and thus wonâ€™t include any backoff in the GVS streams.We can write a filter rule that makes it so whenever the web client stringifies JSON bound for a server request, we add "isInlinePlaybackNoAd":true to the stringified JSON.How did I know to set that property? Itâ€™s referenced in the frontend JavaScript, so I could have spent a bunch of time reading all that. But thereâ€™s an easier way - while the web client interacts with InnerTube using JSON, that JSON API is actually generated from a Protocol Buffers definition, and thereâ€™s a way you can extract most of the underlying protobuf definition. I used req2proto, which is a tool to extract protobuf definitions from Googleâ€™s internal APIs to get the full definitions used in the  call, and used that to find the  property.This method only works for warm navigation, where youâ€™ve already loaded the YouTube single page app and are clicking around within it. When you navigate directly to a watch page, the YouTube backend embeds a player response directly into the page as . Since the player request is made on the backend, we canâ€™t set  on it.  One way to fix this for cold loads is to just remove that initial data to force YouTube to make a player request we can control (but see below before using these):This approach has some problems though, so you might not want to use it:It completely breaks livestreams, and probably some other things I havenâ€™t testedIt causes the video player to briefly flashIt slows the page loading timeBypassing the locker scriptSo that filter kinda worked, but sometimes uBlock Origin wasnâ€™t hooking . I investigated further and it turns out YouTube is running an A/B test where sometimes they add this to the frontend HTML as the very first thing in the  tag:This locks a few global objects by using  to set them as non-writable, which prevents later code from overwriting them with a Proxy that alters their behaviour. So uBlock Origin can only proxy JSON.stringify if it can run before this locker script does. On Firefox this is easily resolvable - you can use a HTML filter to filter out the script tag from the source HTML before the page even starts being parsed. But that relies on extension APIs that Chromium doesnâ€™t support.The â€œfixâ€ for the locker script so far is to hook  instead of .  is another function that handles the request body before it gets fetched. It would be nice if there was a way to actually defuse the locker script, instead of working around it. The version of the filter that hooks  instead is more complicated because uBOâ€™s scriptlets donâ€™t let you replace text on a key of an object, so the filter in uBOâ€™s filter list injects this JS:Thanks to the uAssets maintainers for helping with that.If you have any questions for me you can DM me on Discord as .]]></content:encoded></item><item><title>Tuxracer.js play Tux Racer in the browser</title><link>https://github.com/ebbejan/tux-racer-js</link><author>retro_guy</author><category>hn</category><pubDate>Fri, 20 Jun 2025 16:54:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alpha Centauri</title><link>https://www.filfre.net/2025/06/alpha-centauri/</link><author>doppp</author><category>hn</category><pubDate>Fri, 20 Jun 2025 16:50:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In the spring of 1996, Brian Reynolds and Jeff Briggs took a long, hard look around them and decided that theyâ€™d rather be somewhere else.At that time, the two men were working for MicroProse Software, for whom they had just completedÂ , with Reynolds in the role of primary designer and programmer and Briggs in that of co-designer, producer, and soundtrack composer. They had brought the project in for well under $1 million, all that their bosses were willing to shell out for what they considered to be a game with only limited commercial potential. And yet the early sales were very strong indeed, proof that the pent-up demand for a modestly modernized successor to Sid Meierâ€™s masterstroke that Reynolds and Briggs had identified had been very, very real. Which is not to say that they were being given much credit for having proved their managers wrong.MicroProseâ€™s executives were really Spectrum Holobyteâ€™s executives, ever since the latter company had acquired the former in December of 1993, in a deal lubricated by oodles of heedless venture capital and unsustainable levels of debt. Everything about the transaction seemed off-kilter; while MicroProse had a long and rich history and product portfolio, Spectrum Holobyte was known for the  series of ultra-realistic combat flight simulators, for the first version ofÂ  to run on Western personal computers, and for not a whole lot else. Seeing the writing on the wall, â€œWild Billâ€ Stealey, the partner in crime with whom Sid Meier had founded MicroProse back in 1982, walked out the door soon after the shark swallowed the whale. The conjoined company went on to lose a staggering $57.8 million in two years, despite such well-received, well-remembered, and reasonably if not extraordinarily popular games as , , andÂ . By the spring of 1996, the two-headed beast, which was still publishing games under both the Spectrum Holobyte and MicroProse banners, was teetering on the brink of insolvency, with, in the words of its CEO Stephen M. Race, a â€œnegative tangible net worth.â€ It would require a last-minute injection of foreign investment capital that June to save it from being de-listed from the NASDAQ stock exchange.The unexpectedly strong sales of  â€” the game would eventually sell 3 million copies, enough to make it MicroProseâ€™s best seller ever by a factor of three â€” were a rare smudge of black in this sea of red ink. Yet Reynolds and Briggs had no confidence in their managersâ€™ ability to build on their success. They thought it was high time to get off the sinking ship, time to get away from a company that was no longer much fun to work at. They wanted to start their own little studio, to make the games they wanted to make their way.But that, of course, was easier said than done. They had a proven track record inside the industry, but neither Brian Reynolds nor Jeff Briggs was a household name, even among hardcore gamers. Most of the latter still believed that  was the work of Sid Meier â€” an easy mistake to make, given how prominently Meierâ€™s name was emblazoned on the box. Reynolds and Briggs needed investors, plus a publisher who would be willing to take a chance on them. Thankfully, the solution to their dilemma was quite literally staring them in the face every time they looked at that  box: they asked Sid Meier to abandon ship with them. After agonizing for a while about the prospect of leaving the company he had co-founded in the formative days of the American games industry, Meier agreed, largely for the same reason that Reynolds and Briggs had made their proposal to him in the first place: it just wasnâ€™t any fun to be here anymore.So, a delicate process of disentanglement began. Keenly aware of the legal peril in which their plans placed them, the three partners did everything in their power to make their departure as amicable and non-dramatic as possible. For instance, they staggered their resignations so as not to present an overly united front: Briggs left in May of 1996, Reynolds in June, and Meier in July. Even after officially resigning, Meier agreed to continue at MicroProse for some months more as a part-time consultant, long enough to see through his computerized version of the ultra-popular  collectible-card game. He didnâ€™t even complain when, in an ironic reversal of the usual practice of putting Sid Meierâ€™s name on things that he didnâ€™t actually design, his old bosses made it clear that they intended to scrub him from the credits of this game, which he had spent the better part of two years of his life working on. In return for all of this and for a firm promise to stay in his own lane once he was gone, he was allowed to take with him all of the code he had written during the past decade and a half at MicroProse. â€œThey didnâ€™t want to be making detailed strategy titles any more than we wanted to be making  flight simulators,â€ writes Meier in his memoir. On the face of it, this was a strange attitude for his former employer to have, given that  was selling so much better than any of its other games. But Brian Reynolds, Jeff Briggs, and Sid Meier were certainly not inclined to look the gift horse in the mouth.They decided to call their new company Firaxis Games, a name that had its origin in a piece of music that Briggs had been tinkering with, which he had dubbed â€œFiery Axis.â€ Jason Coleman, a MicroProse programmer who had coded on , quit his job there as well and joined them. Sid Meierâ€™s current girlfriend and future second wife Susan Brookins became their office manager.The first office she was given to manage was a cramped space at the back of Absolute Quality, a game-testing service located in Hunt Valley, Maryland, just a stoneâ€™s throw away from MicroProseâ€™s offices. Their landlords/flatmates were, if nothing else, a daily reminder of the need to test, test, test when making games. Brian Reynolds (who writes of himself here in the third person):CEO Jeff Briggs worked the phones to rustle up some funding and did all the hard work of actually putting a new company together. Sid Meier and Brian Reynolds worked to scrape together some playable prototype code, and Jason Coleman wrote the first lines of JACKAL, the engine which these days pretty much holds everything together. Office-manager Susan Brookins found us some office furniture and bought crates of Coke, Sprite, and Dr. Pepper to stash in a mini-fridge Brian had saved from his college days. We remembered that at some indeterminate point in the past we were considered world-class game designers, but our day-to-day lives werenâ€™t providing us with a lot of positive reinforcement on that point. So, for the first nine months of our existence as a company, we clunked over railroad tracks in the morning, played Spy Hunter in the upstairs kitchen, and declared â€œwork at homeâ€ days when Absolute Quality had competitors in the office.Once the necessary financing was secured, the little gang of five moved into a proper office of their own and hired more of their former colleagues, many of whom had been laid off in a round of brutal cost-cutting that had taken place at MicroProse the same summer as the departure of the core trio. These folks bootstrapped Firaxisâ€™s programming and art departments. Thanks to the cachet of the Sid Meier name/brand, the studio was already being seen as a potential force to be reckoned with. Publishers flew out to them instead of the other way around to pitch their services. In the end, Firaxis elected to sign on with Electronic Arts, the biggest publisher of them all.The three founding fathers had come into the venture with a tacit understanding about the division of labor. Brian Reynolds would helm a sprawlingly ambitious but fundamentally iterative 4X strategy game, a â€œspiritual successorâ€ to  and . This was the project that had gotten Electronic Artsâ€™s juices flowing; its box would, it went without saying, feature Sid Meierâ€™s name prominently, no matter how much or how little Meier ultimately had to do with it. Meanwhile Meier himself would have free rein to pursue the quirkier, more esoteric ideas that he had been indulging in ever since finishing . And Briggs would be the utility player, making sure the business side ran smoothly, writing the music, and pitching in wherever help was needed on either partnerâ€™s project.Sid Meier has a well-earned reputation for working rapidly and efficiently. Itâ€™s therefore no surprise that he was the first Firaxis designer to finish a game, and by a wide margin at that. Called simply  â€” or ratherÂ  â€” it was based upon the battle that took place in that Pennsylvania city during the American Civil War. More expansively, it was an attempt to make a wargame that would be appealing to grognards but accessible enough to attract newcomers, by virtue of being real-time rather than turn-based, of being audiovisually attractive, and of offering a whole raft of difficulty levels and tutorials to ease the player into the experience. Upon its release in October of 1997,  magazine called it â€œa landmark, a real-time-strategy game whose unique treatment of its subject matter points to a [new] direction for the whole genre.â€ For my own part, being neither a dedicated grognard nor someone who shares the fascination of so many Americans for the Civil War, I will defer to the contemporary journal of record. Iâ€™m sure that  does what it does very well, as almost all Sid Meier games do. On the broader question of whether it brought new faces into the grognard fold, the verdict is more mixed. Meier writes today that â€œit was a success,â€ but it was definitely not a hit on the scale of SSIâ€™s , the last wargame to break out of its ghetto in a big way.To the hungry eyes of Electronic Arts,  was just the appetizer anyway. The main dish would be .The idea forÂ  had been batted around intermittently as a possible â€œsequel toÂ â€ ever since Sid Meier had made one of the two possible victory conditions of that game the dispatching of a spaceship to that distant star, an achievement what was taken as a proof that the nation so doing had reached the absolute pinnacle of terrestrial achievement. In the wake of the original s release and success, Meier had gone so far as to prototype some approaches to what happens after humanity becomes a star-faring species, only to abandon them for other things. Now, though, the old idea was newly appealing to the principals at Firaxis, for commercial as much as creative reasons. They had left the rights to the  franchise behind them at MicroProse, meaning that a FiraxisÂ  was, at least for the time being, not in the cards. But if they made a game calledÂ  that used many of the same rules, systems, and gameplay philosophies, and that sported the name of Sid Meier on the boxâ€¦ well, people would get the message pretty clearly, wouldnâ€™t they? This would be a sequel to  in all but its lack of a Roman numeral.When he actually started to try to make it happen, however, Brian Reynolds learned pretty quickly why Sid Meier had abandoned the idea. What seemed like a no-brainer in the abstract proved beset with complications when you really engaged. The central drama of  was the competition and conflict  civilizations â€” which is also, not coincidentally, the central drama of human history itself. But where would the drama come from for a  group of enlightened emissaries from an earthly Utopia settling an alien planet? Whom would they compete against? Just exploring and settling and building werenâ€™t enough, Reynolds thought. There needed to be a source of tension. There needed to be an Other.So, Brian Reynolds started to read â€” not history this time, as he had when working on , but science fiction. The eventual manual for  would list seven authors that Reynolds found particularly inspiring, but it seems safe to say that his lodestar was Frank Herbert, the first writer on the list. This meant not only the inevitable , but also â€” and perhaps even more importantly â€” a more obscure Herbert novel calledÂ  that wasnâ€™t even still in print at the time. One of its authorâ€™s more polarizing creations,Â  is an elliptical, intensely philosophical and even spiritual novel about the attempt of a group of humans to colonize a planet that begins to manifest a form of sentience of its own, and proves more than capable of expressing its displeasure at their presence on its surface. This same conceit would become the central plot hook of .Yes, I just used the word â€œplot.â€ And make no mistake about its significance. Of the threads that have remained unbroken throughout Sid Meierâ€™s long career in game design, one of the most prominent is this mild-mannered manâ€™s deep-seated antipathy toward any sort of set-piece, pre-scripted storytelling in games. Such a thing is, he has always said, a betrayal of computer gamesâ€™ defining attribute as a form of media, their interactivity. For it prevents the player from playing her way, having her own fun, writing her own personal story using the sandbox the designer has provided. Firaxis had never been intended as exclusively â€œSid Meierâ€™s company,â€ but it had been envisioned as a studio that would create, broadly speaking, his type of games. For Reynolds to suggest injecting strong narrative elements into the studioâ€™s very first 4X title was akin to Deng Xiaoping suggesting to his politburo that what post-Cultural Revolution China could really use was a shot of capitalism.And yet Meier and the others around Reynolds let him get away with it, just as those around Deng did. They did so because he had proven himself with  andÂ , because they trusted him, and because  was at the end of the day his project. They hadnâ€™t gone to the trouble of founding Firaxis in order to second-guess one another.Thus Reynolds found himself writing far more snippets of static text for his strategy game than he had ever expected to. He crafted a series of textual â€œinterludesâ€ â€” theyâ€™re described by that word in the game â€” in which the planetâ€™s slowly dawning consciousness and its rising anger at the primates swarming over its once-pristine surface are depicted in ways that mere mechanics could not entirely capture. They appear when the player reaches certain milestones, being yet one more attempt in the annals of gaming history to negotiate the tricky terrain that lies between emergent and fixed narrative.An early interlude, delivering some of the first hints that the planet on which youâ€™ve landed may be more than it seems.Walking alone through the corridors of Morgan Industries, you skim the security reports on recent attacks by the horrific native â€œmind worms.â€ Giant swarms, or â€œboils,â€ of these mottled 10cm nightmares have wriggled out of the fungal beds of late, and now threaten to overwhelm base perimeters in several sectors. Victims are paralyzed with psi-induced terror, and then experience an unimaginably excruciating death as the worms burrow into the brain to implant their ravenous larvae.Only the most disciplined security squads can overcome their fear long enough to trigger the flame guns which can keep the worms at bay. Clearly you will have to tend carefully to the morale of the troops.Furthermore, since terror and surprise increase human casualties dramatically in these encounters, it will be important to strike first when mind-worm boils are detected. You consider ordering some Former detachments to construct sensors near vulnerable bases to aid in such detection efforts. became a darker game as it became more story-oriented, separating itself in the process from the sanguine tale of limitless human progress that is . Reynolds subvertedÂ s original backstory about the perfect society that had finally advanced so far as to colonize the stars. In his new version, progress on Earth has not proved all it was cracked up to be. In fact, the planet his interstellar colonists left behind them was on its last legs, wracked by wars and environmental devastation. Itâ€™s strongly implied if not directly stated that earthly humanity is in all likelihood extinct by the time the colonists wake up from cryogenic sleep and look down upon the virgin new world that the game calls simply â€œPlanet.â€Although the plot was destined to culminate in a reckoning with the consciousness of Planet itself, Brian Reynolds sensed that the game needed other, more grounded and immediate forms of conflict to give it urgency right from the beginning. He created these with another piece of backstory, one as contrived as could possibly be, but not ineffective in its context for all that. As told at length in a novella that Firaxis began publishing in installments on the gameâ€™s website more than six months before its release, mishaps and malevolence aboard the colony ship, which bore the sadly ironic name of , led the colonists to split into seven feuding factions, each of whom inflexibly adhere to their own ideology about the best way to organize human society. The factions each made their way down to the surface of Planet separately, to become s equivalent ofÂ s nations. The player chooses one of them to guide.So, in addition to the unusually strong plot, we have a heaping dose of political philosophy added to the mix;  is an unapologetically heady game. Brian Reynolds had attended graduate school as a philosophy major in a previous life, and he drew from that background liberally. The factionsâ€™ viewpoints are fleshed out largely through a series of epigrams that appear as you research new technologies, that are attributed to whichever of the seven leaders would likely approve most of that development, with an occasional quote from Aristotle or Nietzsche dropped in for good measure.Fossil fuels in the last century reached their extreme prices because of their inherent utility: they pack a great deal of potential energy into an extremely efficient package. If we can but sidestep the 100 million year production process, we can corner this market once again.â€” CEO Nwabudike Morgan,
Strategy SessionGaiaâ€™s Stepdaughters, staunch environmentalists who believe that humanity must learn to live in harmony with nature to avoid repeating the mistakes that led to the ruination of Earth.Morgan Industries, hardcore capitalists whose only complaint about Ayn Rand is that she didnâ€™t go far enough.The University of Planet, STEM specialists who are convinced that scientific and technological progress alone would correct all that ails society if people would just let it run unfettered and go where it takes them.The Lordâ€™s Believers, a fundamentalist sect who are convinced that God will deliver humanity to paradise if we all just pray really hard and abide by a set of stringent, arbitrary dictates.The Spartan Federation, who train their children from birth to be hardened, self-sacrificing warriors like the Spartans of old.The Peacekeepers, the closest thing to pragmatists in this rogueâ€™s gallery of ideologues; they value human rights, democracy, dialog, and consensus-building, and can sometimes seem just as wishy-washy and ineffectual in the face of militant extremism as the earthly United Nations that spawned them.Unlike the nations that appear in  andÂ , each of the factions inÂ  has a very significant set of systemic advantages and disadvantages that to a large extent force even a human player to guide them in a certain direction. For example, the Human Hive is excellent at building heavy infrastructure and pumping out babies, but poor at research, and can never become a democracy; the University of Planet is crazily great at research, but its populace has little patience for extended wars and is vulnerable to espionage. Trying to play a faction against type is, if not completely impossible for the advanced player, not an exercise for the faint of heart.There is a lot of food for thought in the backstory of a ruined Earth and the foreground story of an angry Planet, as there is in the factions themselves and their ideologies, and trust me when I say that plenty of people have eaten their fill. Even today, more than a quarter-century after s release, YouTube is full of introspective think-pieces purporting to tell us What It All Means.Indeed, if anything, the gameâ€™s themes and atmosphere resonate more strongly today than they did when it first came out in February of 1999, at which time the American economy was booming, our world was as peaceful and open as it has ever been, and the fantasy that liberal democracy had won the day and we had reached the end of history could be easily maintained by the optimistic and the complacent. Alas, today  feels far more believable than and its  about the inevitability of perpetual progress. These days,Â s depiction of bickering, bitterly entrenched factions warring over the very nature of truth, progressing not at all spiritually or morally even as their technology runs wild in a hundred different perilous directions, strikes many as the more accurate picture of the nature of our species. People play  to engage with modern life; they playÂ  to escape from it.The original  was ahead of the curve on global warming, prompting accusations of â€œpolitical correctnessâ€ from some gamers. Paying heed to the environment is even more important in , since failing to do so can only aggravate Planetâ€™s innate hostility. The â€œEco-Damageâ€ statistic is key.That said, we must also acknowledge that  is disarmingly good at mirroring the beliefs of its players back at them. Many people like to read a strong environmentalist message in the game, and itâ€™s not hard to see why. Your struggles with the hostile Planet, which is doing everything it can to protect itself against the alien parasites on its surface, is an extreme interpretation of the Gaia hypothesis about Earth, even as s Â â€œtranscendenceâ€ victory â€” the equivalent ofÂ s tech victory that got us here in the first place â€” sees humanity overcoming its estrangement from its surroundings to literally become one with Planet.For what itâ€™s worth, though, in his â€œDesignerâ€™s Notesâ€ at the back of the  manual, the one message that Brian Reynolds explicitly states that he wishes for the game to convey is a very different one: that we ought to be getting on with the space race. â€œAre we content to stew in our collective juices, to turn inward as our planet runs inexorably out of resources?â€ he asks. â€œThe stars are waiting for us. We have only to decide that itâ€™s worth the effort to go there.â€ Personally, although I have nothing against space exploration in the abstract, I must say that I find the idea of space colonization as the solution to the problem of a beleaguered Planet Earth shallow if not actively dangerous. Even in the best-case scenario, many, many generations will pass before a significant number of humans will be able to call another celestial object their permanent home. In the meantime, there is in fact nothing â€œinexorableâ€ about polluting our own planet and bleeding it dry; we have the means to stop doing so. To steal a phrase from Reynolds, we have only to decide that itâ€™s worth the effort.But enough with the ideology and the politics, you might be saying â€” how does  play as a game? Interestingly, Brian Reynolds himself is somewhat ambivalent on this subject. He recalls that he set aside a week just to play  after he pronounced that game done, so thrilled was he at the way it had come out. Yet he says that he could barely stand to look at  after it was finished. He was very proud of the world-building, the atmosphere, the fiction. But he didnâ€™t feel like he had quite gotten the gameplay mechanics sorted so that they fully supported the fiction. And I can kind of see what he means.To state the obvious: the gameplay of  is deeply indebted toÂ . Like, really, really indebted. So indebted that, when you first start to play it, you might be tempted to see it as little more than a cosmetic reskin. The cities of  are now â€œbasesâ€; the â€œgoody-hutâ€ villages are now supply pods dropped by theÂ  in its last hours of life; barbarian tribes are native â€œmind wormsâ€; settler engineers are terraformers; money is â€œenergy creditsâ€; Wonders of the World are Secret Projects; etc., etc. It is true that, as you continue to play, some aspects will begin to separate themselves from their inspiration. For example, and perhaps most notably, the mind worms prove to be more than just the early-game annoyance that s barbarians are; instead they steadily grow in power and quantity as Planet is angered more and more by your presence. Still, the apple never does roll all that far from the tree.Very early in a game of , when only a tiny part of the map has been revealed. Of all the contrivances in the fiction, this idea that you could have looked down on Planet from outer space and still have no clue about the geography of the place might be the most absurd.WhereÂ  does innovate in terms of its mechanics, its innovations are iterative rather than transformative. An isometric view replaces s top-down view; this allows you to see the topography of the landscape, which is now important, as the solar farms you use to generate energy function best at higher elevations. The most welcome improvement might be the implementation of territorial borders for each faction, drawn automatically around each cluster of bases. To penetrate the borders of another faction with your own units is considered a hostile act. This eliminates the weirdness that dogged the first two iterations of , which essentially saw your empire as a linked network of city-states rather than a contiguous territorial holding. No longer do the computer players walk in and plop down a cityâ€¦ err, base right in the middle of five of your own; no longer do the infantry units of your alleged allies decide to entrench themselves on the choicest tile of your best base. Unsurprisingly given the increased verisimilitude they yielded, national borders would show up in every iteration of the main series after .Other additions are of more dubious value. Brian Reynolds names as one of his biggest regrets his dogged determination to let you design your own units out of the raw materials â€” chassis, propulsion systems, weapons, armor, and so on â€” provided by your current state of progression up the tech tree, in the same way that galaxy-spanning 4X games like  allowed. It proved a time-consuming nightmare to implement in this uni-planetary context. And, as Reynolds admits, itâ€™s doubtful how much it really adds to the game. All that time and effort could likely have been better spent elsewhere.When I look at it in a more holistic sense, it strikes me thatÂ  got itself caught up in what had perchance become a self-defeating cycle for grand-strategy games by the end of the 1990s. Earlier games had had their scope and complexity strictly limited by the restrictions of the relatively primitive hardware on which they ran. Far from being a problem, these limits often served to keep the game manageable for the player. One thinks of 1990â€™s , another Sid Meier classic, which only had memory enough for 35 trains and 35 stations; as a result, the growth of your railroad empire was stopped just before it started to become too unwieldy to micro-manage. Even the originalÂ  was arguably more a beneficiary than a victim of similar constraints. By the time Brian Reynolds made , however, strategy games could become a whole lot bigger and more complex, even as less progress had been made on finding ways to hide some of their complexity from the player who didnâ€™t want to see it and to give her ways of automating the more routine tasks of empire management. Grand-strategy games became ever huger, more intricate machines, whose every valve and dial still had to be manipulated by hand. Some players love this sort of thing, and more power to them. But for a lot of them â€” a group that includes me â€” it becomes much, much too much.To its credit,Â  is aware of this problem, and does what it can to address it. If you start a new game at one of the two lowest of the six difficulty levels, it assumes you are probably new to the game as a whole, and takes you through a little tutorial when you access each screen for the first time. More thoroughgoingly, it gives you a suite of automation tools that at least nod in the direction of letting you set the high-level direction for your faction while your underlings sweat the details. You can decide whether each of your citiesâ€¦ err, bases should focus on â€œexploring,â€ â€œbuilding,â€ â€œdiscovering,â€ or â€œconqueringâ€ and leave the rest to its â€œgovernorâ€; you can tell your terraforming units to just, well, terraform in whatever way they think best; you can even tell a unit just to go out and â€œexploreâ€ the blank spaces on your map.Is the cure worse than the disease?Sadly, though, these tools are more limited than they might first appear. The tutorials do a decent job of telling you what the different stuff on each screen is and does, but do almost nothing to explain the concepts that underlie them; that is to say, they tell you how to twiddle a variety of knobs, but donâ€™t tell you  you might want to twiddle them. Meanwhile the automation functions are undermined by being abjectly stupid more often than not. Your governor will happily continue researching string theory while his rioting citizens are burning the place down around his ears. You can try to fine-tune his instructions, but there comes a point when you realize that itâ€™s easier just to do everything yourself. The same applies to most of the automated unit functions. The supreme booby prize has to go to the aforementioned â€œexploreâ€ function. As far as I can determine, it just causes your unit to move in a random direction every turn, which tends to result in it chasing its tail like a dog that sat down in peanut butter rather than charging boldly into the unknown.This, then, is the contradiction at the heart ofÂ , which is the same one that bothers me in . A game that purports to be about Big Ideas demands that you spend most of your time engaged in the most fiddly sort of busywork. I hasten to state once again that this is not automatically a bad thing; again, some people enjoy that sort of micro-management very much. For my own part, I can get into it a bit at the outset, but once I have a dozen bases all demanding constant attention and 50 or 60 units pursuing their various objectives all over the map, I start to lose heart. For me, this problem is the bane of the 4X genre. Iâ€™m not enough of an expert on the field to know whether anyone has really come close to solving it; I look forward to finding out as we continue our journey through gaming history. As of this writing, though, my 4X gold standards remain  andÂ , because their core systems are simple enough that the late game never becomes completely overwhelming.Speaking ofÂ : alongside the questionable idea of custom-built units,  also lifts from that game the indubitably welcome one of a â€œdiplomatic victory,â€ which eliminates the late-game tedium of having to hunt down every single enemy base and unit for a conquest victory that you know is going to be yours. If you can persuade or intimidate enough of the other factions to vote for you in the â€œPlanetary Councilâ€ â€” or if you can amass such a large population of your own that you can swamp the vote â€” you can make an inevitability a reality by means of an election. Likewise, you can also win an â€œeconomicâ€ victory by becoming crazy rich. These are smart additions that work as advertised. They may only nibble at the edges of the central problem I mentioned above, but, hey, credit where itâ€™s due.Aesthetically,Â  is a marked improvement overÂ , which, trapped in the Windows 3.1 visual paradigm as it was, could feel a bit like â€œplayingâ€ a really advanced Excel spreadsheet. But  also exhibits a cold â€” not to say sterile â€” personality, with none of the goofy humor that has always been one of s most underrated qualities, serving to nip any pretentiousness in the bud by reminding us that the designers too know how silly a game that can pit Abraham Lincoln against Mahatma Gandhi in a nuclear-armed standoff ultimately is. Thereâ€™s nothing like that understanding on display in  â€” much less the campy troupe of live-action community-theater advisors who showed up to chew the scenery in . The look and feel of  is more William Gibson than Mel Brooks.While the aesthetics of  represent a departure from what came before, weâ€™re back to the same old same old when it comes to the actual interface, just with more  packed into the menus and sub-menus. Iâ€™m sure that Brian Reynolds did what he could, but it will nevertheless come off as a convoluted mess to the uninitiated modern player. Itâ€™s heavily dependent on modes, a big no-no in GUI design since the days when the Apple Macintosh was a brand new product. If youâ€™re anything like me, youâ€™ll accidentally move a unit about ten times in any given evening of play because you thought you were in â€œviewâ€ mode when you were actually in â€œmoveâ€ mode. And no, there is no undo function, a feature for which Iâ€™d happily trade the ability to design my own units.The exit dialog is one of the few exceptions to  as a humor-free zone. â€œPlease donâ€™t go,â€ says a passable imitation of HAL from . â€œThe drones need you.â€ Note that this is a game in which you click â€œOKâ€ to cancel. Somewhere out there a human-factors interface consultant is shuddering in horror.As so often happens in reviews like these, I find now that Iâ€™ve highlighted the negative here more than I really intended to.  is by no means a bad game; on the contrary, for some players it is a genuinely great one. It is, however, a sharply bifurcated game, whose fiction and gameplay are rather at odds with one another. The former is thoughtful and bold, even disturbing in a way that  never dared to be. The latter is pretty much what you would expect from a game that was promoted as â€œ in space,â€ and, indeed, that was crafted by the same man who gave us . A quick survey of YouTube reveals the two halves of the whole all too plainly. Alongside those earnest think-pieces about What It All Means, there are plenty of videos that offer tips on the minutiae of its systems and show off the hostâ€™s skill at beating it at superhuman difficulty levels, untroubled by any of its deeper themes or messages.As youâ€™ve probably gathered from the tone of this article,  leaves me with mixed feelings. Iâ€™m already getting annoyed by the micro-management by the time I get into the mid-game, even as I miss a certain magic sauce that is part and parcel of . Thereâ€™s something almost mythical or allegorical about going from inventing the wheel to sending a colony ship on its way out to the stars. Going from Biogentics to the â€œThreshold of Transcendenceâ€ in  is less relatable. And while the story and the additional philosophical textures that  brings to the table are thought-provoking, they can only be fully appreciated once. After that, youâ€™re mostly just clicking past the interludes and epigrams to get on to building the next thing you need for your extraterrestrial empire.In fact, it seems to me that  at the gameplay level favors the competitive player more than the experiential one; being firmly in the experiential camp myself, this may explain why it doesnâ€™t completely agree with me. Itâ€™s a more fiercely zero-sum affair than . Those players most interested in the development side of things canâ€™t ensure a long period of peaceful growth by choosing to play against only one or two rivals. All seven factions are always in this game, and they seem to me far more prone to conflict than those of , what with the collection of mutually antithetical ideologies that are such inseparable parts of their identities. Suffice to say that the other faction leaders are exactly the self-righteous jerks that rigid ideological extremists tend to be in real life. This does not lend itself to peace and harmony on Planet even before the mind worms start to rise up en masse. Even when playing as the Peacekeepers, I found myself spending a lot more time fighting wars in  than I ever did inÂ , where I was generally able to set up a peaceful, trustworthy democracy, forge strong diplomatic and trading links with my neighbors, and ride my strong economy and happy and prosperous citizenry to the stars. Playing , by contrast, is more like being one of seven piranhas in a fishbowl than a valued member of a community of nations. If you can find one reliable ally, youâ€™re doing pretty darn well on the diplomatic front. Intervals of peace tend to be the disruption in the status quo of war rather than the other way around.The other factions spend an inordinate amount of time trying to extort money out of you.There was always an understanding at Firaxis that, for all thatÂ  was the best card they had to play at that point in time from a commercial standpoint, its sales probably werenâ€™t destined to rival those of . For the  franchise has always attracted a fair number of people from outside the core gaming demographics, even if it is doubtful how many of them really buckle down to play it.Nonetheless,  did about as well as one could possibly expect after its release in February of 1999. (Electronic Arts would surely have preferred to have the game a few months earlier, to hit the Christmas buying season, but one of the reasons Firaxis had been founded had been to avoid such compromises.) Sales of up to 1 million units have been claimed for it by some of the principals involved. Even if that figure is a little inflated, as I suspect it may be, the game likely sold well into the high hundreds of thousands.By 1999, an expansion pack for a successful game likeÂ  was almost obligatory. And indeed, itâ€™s hard to get around the feeling thatÂ Alpha Centauri: Alien Crossfire, which shipped in October of that year, was created more out of obligation than passion. Neither the navel-gazers nor the zero-summers among the original gameâ€™s fan base seem all that hugely fond of it. Patched together by a committee of no fewer than eight designers, with the name of Brian Reynolds the very last one listed, it adds no fewer than seven new factions, which only serve to muddy the narrative and gameplay waters without adding much of positive interest to the equation; the two alien factions that appear out of nowhere seem particularly out of place. If you ask me,  is best played in its original form â€” certainly when you first start out with it, and possibly forever.Be that as it may, the end of the second millennium saw Firaxis now firmly established as a studio and a brand, both of which would prove very enduring. The company remains with us to this day, still one of the leading lights in the field of 4X strategy, the custodian of the beloved â€¦Yes,Â . For their next big trick, Firaxis was about to get the chance to make a game under the name that they thought theyâ€™d left behind forever when they said farewell to MicroProse.Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like. The bookÂ Sid Meierâ€™s Memoir!: A Life in Computer Games by Sid Meier with Jennifer Lee Noonan.Â  of August 1996, January 1998, September 1998, April 1999, and January 2000;  of July 1997;  241. Also theÂ  manual, one of the last examples of such a luxuriously rambling 250-page tome that the games industry would produce.]]></content:encoded></item><item><title>Show HN: Nxtscape â€“ an open-source agentic browser</title><link>https://github.com/nxtscape/nxtscape</link><author>felarof</author><category>hn</category><pubDate>Fri, 20 Jun 2025 16:35:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hi HN - we're Nithin and Nikhil, twin brothers and founders of nxtscape.ai (YC S24). We're building Nxtscape ("next-scape") - an open-source, agentic browser for the AI era.-- Why bother building a new browser?
For the first time since Netscape was released in 1994, it feels like we can reimagine browsers from scratch for the age of AI agents. The web browser of tomorrow might not look like what we have today.We saw how tools like Cursor gave developers a 10x productivity boost, yet the browserâ€”where everyone else spends their entire workdayâ€”hasn't fundamentally changed.And honestly, we feel like we're constantly fighting the browser we use every day. It's not one big thing, but a series of small, constant frustrations. I'll have 70+ tabs open from three different projects and completely lose my train of thought. And simple stuff like reordering tide pods from amazon or filling out forms shouldn't need our full attention anymore. AI can handle all of this, and that's exactly what we're building.-- What makes us different
We know others are exploring this space (Perplexity, Dia), but we want to build something open-source and community-driven. We're not a search or ads company, so we can focus on being privacy-first â€“ Ollama integration, BYOK (Bring Your Own Keys), ad-blocker.Btw we love what Brave started and stood for, but they've now spread themselves too thin across crypto, search, etc. We are laser-focused on one thing: making browsers work for YOU with AI. And unlike Arc (which we loved too but got abandoned), we're 100% open source. Fork us if you don't like our direction.-- Our journey hacking a new browser
To build this, we had to fork Chromium. Honestly, it feels like the only viable path todayâ€”we've seen others like Brave (started with electron) and Microsoft Edge learn this the hard way.We also started with why not just build an extension. But realized we needed more control. Similar to the reason why Cursor forked VSCode. For example, Chrome has this thing called the Accessibility Tree - basically a cleaner, semantic version of the DOM that screen readers use. Perfect for AI agents to understand pages, but you can't use it through extension APIs.That said, working with the 15M-line C++ chromium codebase has been an adventure. We've both worked on infra at Google and Meta, but Chromium is a different beast. Tools like Cursor's indexing completely break at this scale, so we've had to get really good with grep and vim. And the build times are brutalâ€”even with our maxed-out M4 Max MacBook, a full build takes about 3 hours.Full disclosure: we are still very early, but we have a working prototype on GitHub. It includes an early version of a "local Manus" style agent that can automate simple web tasks, plus an AI sidebar for questions, and other productivity features (grouping tabs, saving/resuming sessions, etc.).Looking forward to any and all comments!]]></content:encoded></item><item><title>How to Design Programs 2nd Ed (2024)</title><link>https://htdp.org/</link><author>AbuAssar</author><category>hn</category><pubDate>Fri, 20 Jun 2025 15:51:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Visualizing environmental costs of war in Hayao Miyazaki&apos;s NausicaÃ¤</title><link>https://jgeekstudies.org/2025/06/20/wilted-lands-and-wounded-worlds-visualizing-environmental-costs-of-war-in-hayao-miyazakis-nausicaa-of-the-valley-of-the-wind/</link><author>zdw</author><category>hn</category><pubDate>Fri, 20 Jun 2025 15:23:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Upland High School, Upland, CA, USA.Email: audrey.a.aguirre (at) gmail (dot) comPast studies on NausicaÃ¤ of the Valley of the Wind (é¢¨ã®è°·ã®ãƒŠã‚¦ã‚·ã‚«; Topcraft, 1984) have primarily focused on its ecological themes and anti-war messages through analysis of the narrative as a whole or NausicaÃ¤â€™s character. These studies address the ethical and environmental consequences of war shown through the dystopian nature of the filmâ€™s setting and its religious symbolism. However, I have seen almost no research on how visual storytelling contributes to these messages.This paper addresses how the visual representation of the environmental consequences of war in NausicaÃ¤ of the Valley of the Wind can impact our views of those issues in our world. The paper will show that the visuals in the film are not simply aesthetic decisions, but a crucial narrative device to convey the effects of war on both people and nature.Therefore, this paper explores how Miyazaki uses elements of mise-en-scÃ¨ne such as color, lighting, body language and other visual storytelling elements to communicate and add emphasis to the anti-war messaging of the film; especially those that display both the ecological and human consequences of war. I argue that the filmâ€™s use of visuals not only supports the anti-war themes of the film as a whole but also adds a stronger emotional and moral weight to the story by reflecting real-life war technologies in its visuals. This allowed audiences to reflect on real-life issues regarding the environmental and human consequences of warfare. In this way, the film created a bridge between fantasy and reality, urging its viewers to strive for a more peaceful and environmentally conscious world.Animated films, like many other art forms, can convey so much emotion and be filled to the brim with meaningful messages and ideas. Especially with animation, the director is able to display a myriad of stunning visuals that can be impossible to recreate in live action film. Throughout history we as humans have used storytelling and art to reflect on and understand the world and the current issues we face. With the state of our world being as it is, with conflict at every corner, with global warming and other environmental issues being more pressing matters than ever before, it is clear that the environmental messages in  are of the utmost importance to our society today. This is why it is important to ask, how can the depiction of the environmental impact of war in this film help us change our view of and approach to these issues in real life?Hayao Miyazakiâ€™s 1984 film NausicaÃ¤ of the Valley of the Wind follows the pacifistic and kindhearted princess of the Valley of the Wind as she navigates the apocalyptic landscape of the film, searching for a way to undo the damage caused by the wars of the past and prevent further damage in the present. Miyazakiâ€™s filmography as a whole contains many important messages regarding the way in which humans interact with and affect the natural world. His films, including , focus on the relationships between humans and animals, exploring the imbalance between the human and natural worlds. With  being the only film directed by Miyazaki that depicts a more modern style of warfare, this film could help to open peopleâ€™s eyes to the damage war puts not just on the humans involved but on the environment which we all share.NausicaÃ¤ of the Valley of the Wind was originally a manga of the same name that Hayao Miyazaki wrote for Animage, with it being released from February 1982 to March 1994. It saw great success amongst Japanese readers through its run. This prompted the mangaâ€™s adaptation into an animated film which was directed by Miyazaki and released to Japanese audiences in 1984. Due to the extreme inconsistencies between the original film and Showmen Inc.â€™s English dub of â€“renamed â€“ Miyazaki considered never releasing his films to foreign audiences again. This dub had changed the names of most characters, cut nearly 30 minutes off of the film and completely altered its message. Later, a deal was made between Studio Ghibli and Walt Disney Studios Home Entertainment, and Miayzaki allowed them to dub his films under the condition that they make no cuts and keep the original meaning intact (Cinematheque, 2016). Because of this, Disneyâ€™s English dub will be the main focus of this paper (Walt Disney Studios Home Entertainment, 1985).For the purpose of this article, when referring to the natural world it would more specifically be described as the surrounding elements that are essential for the well-being of both human and non-human life which includes green spaces, wildlife habitats, biodiversity and clean air, water and soil. Additionally, there are several terms which one who has not watched the film would not be able to understand, and these terms are as follows.The ohmu (Fig. 1) are enormous, powerful and intelligent pill bug-like animals which are feared by the people in the film due to the fact that they could be considered the kings of the toxic jungle.The Toxic Jungle is a vast forest where the air, water, soil and plants are poisonous to all but the giant arthropods which inhabit it. The toxic jungle only became so poisonous after the Giant Warriors were used in the last days of the Ceramic War. The Giant Warriors (Fig. 2) are giant, biomechanical lifeforms which are treated, and act, more as weapons rather than as independent beings.The Ceramic War is an apocalyptic war that occurred 1000 years before the events of the film in what was called the Ceramic Period, which destroyed civilization, caused an ecocide and created the vast Toxic Jungle during the Seven Days of Fire. The Seven Days of Fire was a seven-day period at the end of the Ceramic War in which the Giant Warriors were deployed.When analyzing Miyazakiâ€™s films, including NausicaÃ¤ of the Valley of the Wind, often the messages regarding warfareâ€™s effects on the natural world focus more on the direct impacts rather than the indirect ones. Additionally, many researchers look at the film from a spiritual lens with DeWeese-Boyd (2009) believing NausicaÃ¤ serves as a Christ-like figure, while both Morgan (2015) and Nunes (2021) believe that NausicaÃ¤ serves as an example of how to restore balance between humans and nature. Despite that, these authors see her role in restoring balance differently. Nunes believes that NausicaÃ¤ gave herself to nature, sacrificing her free will in order to heal the earth without personal bias. On the other hand, Morgan believes that she serves as an example of how the fragmentation between the mind, body, spirit and nature can be restored through respect and care for the natural world.In addition to this, there are some researchers who believe that  can be used as a teaching tool, with Kleese (2024) believing that, since the film is not directly associated with any singular real-life issue but with many different events and issues, it can be used in classrooms to help children understand the importance of nature and finding a democratic solution to both environmental and political issues in a more digestible way.It is no wonder the previous research on NausicaÃ¤ has such a spiritual focus, as Miyazakiâ€™s filmography as a whole often focuses on the relationship and imbalance between humans and the natural world in a very spiritual manner. However,  not only explores the direct impacts of war and other human activities, but also their indirect ramifications. Thus, this paper will hopefully fill a gap in the existing literature by analyzing the filmâ€™s messages regarding warfareâ€™s indirect effects on the environment and how this exercise can give us insight to our real-world problems.The importance of the environmentClimate change and other environmental crises such as soil and water contamination are all issues which have been recognized as important by the public eye. From everyday people to those in important positions, most can agree that the health of the natural world is important to the health and survival of life on earth. As UN Secretary-General AntÃ³nio Guterres said at the biodiversity COP in Montreal 2022, â€œWithout nature, we have nothingâ€ (Abbasi et al., 2023). Access to clean water is undeniably fundamental to life on Earth, be it human or not. Even with this being common knowledge, pollution has damaged water quality to a point where it is causing a rise in waterborne diseases and damaging the health of both freshwater and saltwater ecosystems (Abbasi et al., 2023). Additionally, the rising temperatures, extreme weather events, air pollution and the heightened spread of infectious diseases are just a few examples of major health issues exacerbated by climate change (Abbasi et al., 2023). As the human population grows, we see that demands on Earthâ€™s ecosystems are becoming more unsustainable; with the way that we currently treat our environment, long-term human and non-human security are clearly at stake (dos Santos, 2024).The ever-worsening health of our natural world impacts us in many ways. Land degradation and other environmental issues which may cause an area to be less habitable can lead to the disruption of social and economic systems. Shortages of land, shelter, food and water exacerbate poverty and the poor living conditions in many areas of the world (Anonymous, 2004), this in turn leads to mass migration and conflict over usable land and resources necessary for life (Abbasi et al., 2023). While the degradation of land can in itself cause wars to erupt between peoples disputing over the usable land, war can and has caused additional land degradation. In Afghanistan, for example, forests have been leveled and its land and farmlands polluted from the years of use of fuel, chemicals and mines during wartime (Bonds, 2015).Certain practices of war have more devastating impacts on the longevity of our natural world. One poignant example of how destructive war can be to our planet can be seen in the burning of Kuwaiti oil wells that took place during the The Gulf War as a part of their scorched earth tactics. This resulted both in the soil becoming contaminated with excessive amounts of hydrocarbons and heavy metals and in the release of massive amounts of particulate matter and other pollutants into the atmosphere (Aldawsari, 2024).Everything from the way a shot is framed to the smallest detail captured within the shot can have an effect on the viewers of a film. Displaying intense emotional imagery has been proven to have significant psychological effects on viewers. For instance, the Kuleshov effect is a famous example of how film influences viewersâ€™ emotional perception; it has shown that point-of-view editing practices influence viewersâ€™ emotional interpretation of neutral facial expressions in a face-scene-face sequence (Cao et al., 2024). With this in mind it is evident that the things that films show their viewers can and do impact how they view the world.At the same, time films utilize the elements of mise-en-scÃ¨ne to convey messages to their audience. Mise-en-scÃ¨ne is a French term meaning what is put into a scene or frame and consists of all the visual information in front of the camera (Caprio, 2021). Understanding these elements may help understand what the director of a film wants to convey with any given scene and understand how the visuals impact the audience.All forms of filmography are capable of affecting their audiences with the previously mentioned methods. However, animation is often able to employ visuals and different types of shots which can often be extremely difficult or expensive to replicate in live action film. Because of this and because animated films in the West are often avoidant of more serious topics, seeing as they are viewed as being only for small children, that Japanese animation, or anime, can be an immensely powerful and impactful type of filmmaking. Several studies have shown that anime can influence its audience and evoke positive changes in them (e.g., Yusof et al., 2024). With the unique perspective that anime provides, which is more provocative, tragic and contains far more complicated storylines than the ones seen in American popular cinema, anime has proven itself to be a tool for understanding the complex humanâ€“environment relationship and environmental problems (Mumcu & YÄ±lmaz, 2018).Within the anime community Hayao Miyazaki has carved out an image for himself as a masterful director who is skilled in creating both enchanting fantasies and incredibly thought-provoking films (Mumcu & YÄ±lmaz, 2018). He is quite well renowned for his beautiful landscapes, heavy ecological themes and overall beautiful and touching storytelling. Several of his films such as , NausicaÃ¤ of the Valley of the Wind and even  feature themes of ecological imbalance as the main plot points of the films. Even those which do not have these themes quite as ingrained in the plot still have some commentary on the matter or feature more lighthearted takes on natureâ€™s relationship with humanity. This can be seen in the stories of  and . Additionally, many of his films also approach themes of war and conflict, such as , NausicaÃ¤ of the Valley of the Wind, , ,  and . This leaves only six of his fifteen films, all of which have seen great success, with no themes of interest to this paper. For this reason, it is evident that Miyazaki is a perfect choice for the subject matter of this paper.With only six of the fifteen films Miyazaki has directed or written having no themes of interest to this paper, one may wonder what it is that makes  more suitable than any of his other films. Well, though many of his films touch on environmental issues with Miyazaki even going as far as saying â€œIâ€™ve come to a point where I just canâ€™t make a movie without addressing the problem of humanity as part of an ecosystemâ€ in an interview with Asia Pulse, May 16, 1997, not many of his films include war in the main storyline. Only five do, as mentioned above. There are only two films, and , that cover both environmental issues and war. The major factor which puts  over  is the setting; is set in Japanâ€™s late Muromachi Period, which was characterized by rapid industrialization and frequent conflicts. On the other hand, is set in the post-apocalyptic future which bears much resemblance to our world, with some current technologies such as guns, grenades and tanks being included, as well as fictitious technologies which closely mirror real-life technologies (e.g., how the Giant Warriors function similarly to nuclear bombs).As previously stated, the Kuleshov effect demonstrates that intense visuals can alter oneâ€™s interpretation of the world and this in itself proves that analysis via the elements of mise-en-scÃ¨ne is a viable method for breaking down  with the purpose of determining how the visual depiction of the environmental impact of war can affect our view of and approach to these issues in real life. However, this isnâ€™t the only reason mise-en-scÃ¨ne analysis was used within this study, since it has been used for teaching aspiring filmmakers how films communicate messages with visuals; as my question focuses on visual depictions in film, mise-en-scÃ¨ne analysis was perfectly suited for my project. The elements of mise-en-scÃ¨ne are as follows: settings and props, costume, hair and makeup, facial expressions and body language, lighting and color and lastly positioning. Each of the 38 scenes of the film were analyzed to see how these elements are utilized and what effect it can have on the audience.Throughout the film the elements of mise-en-scÃ¨ne can be seen in use in many ways. Despite the subtle differences in the amount each element is used in the different parts of the story, the overall message urges viewers to rethink their stance on warfare and its technologies, not just for the impact it directly has on humans, but also for the sake of the natural world.One example of how body language, facial expressions and color are used to push the filmâ€™s message regarding the use of war technologies can be perfectly seen in the opening credits, which appear in scene two directly after the narrator introduces the world reading aloud the words that can be seen in figure one that state â€œOne thousand years have passed since the collapse of industrialized civilization. The Toxic Jungle now spreads, threatening the survival of the last of the human race.â€ Right after this is read as the credits roll a tapestry is panned over, shown in Figure 3.The tapestry shows how the creation of the Giant Warriors led to the creation of the Toxic Jungle and the fall of humanity. Within this tapestry, while in the process of building the warriors they appear confident in their body language and facial expressions, they are clearly calm and all is well. However, the colors of the warrior are bright and clash with that of the people which are more muted browns rather than the bright blues reds and yellows of the warrior. This itself already sets up for the destructive and overpowering nature of the Giant Warriors before they have even been finished and this is only confirmed when they are soon after in the tapestry shown wreaking havoc on the very same people who created them. In contrast to their poised expression and way of holding themselves previously in the tapestry, here they appear to be in great distress and panic. While this on its own is a striking visual representation of how dangerous technologies can be, even to their own creators, these visuals are given new meaning when we see the Giant Warriors in actual use towards the end of the film.The audience was shown the similarities between the Giant Warriors and nuclear bombs in several instances towards the end of the film; in particular, there is one which really drives home the parallels between them (Figs. 4 and 5). In this scene, the Giant Warrior is being used by the main antagonist, its form being completely fictional and bearing no resemblance to real-life technologies, with the warrior shooting a beam of light from its mouth. The explosion that this caused very closely resembles the mushroom shape of nuclear bombs. Other aspects of this scene that help to draw similarities between this fictional tool of destruction and the very real nuclear weapons we have, such as the fact that before it dies the warrior only sets off two explosions. This could be a reflection of the fact that these technologies have only seen practical use twice, once in Hiroshima and once in Nagasaki. While these parallels alone drive a case for the filmâ€™s anti-war messaging, other elements regarding the Giant Warriors in other scenes help to push this narrative as well.Every scene which has the Giant Warrior in it, from the beginning to the end of the film, is decisively negative. Around the middle of the film there is a scene where the warrior is in a sort of incubation. The film explains that the warrior needs time to develop before it is able to be used as a war machine and walk on its own. The warrior is depicted within this scene with its colors being both muddy and bloody, with the lighting highlighting not only its strange shape but also how slimy it appears. This all helps to show how grotesque it is, even when it hasnâ€™t started to be destructive. With the clear parallels it has to nuclear weapons, it becomes obvious from nearly every mention and appearance of the Giant Warrior that if it is so grotesque and dangerous, then nuclear weapons must be just as horrifying. This specific scene very well pushes the idea that not only is the use of nuclear weapons immoral but so too is the development of them.Aside from nuclear weapons, there are other very real weapons displayed in the film that we continue to use to this day. Such weapons include machine guns, shotguns, hand grenades, flash grenades and even larger things like gunships or tanks. While nearly all of these are represented in a very realistic way, the gunships are undoubtedly designed with significant creative liberty as the bodies of these aircrafts do not resemble any real aircraft. However, this does not mean these do not give a good representation of the flaws of such technologies. For examples, see Figures 6 and 7.All of the previously mentioned technologies of war are portrayed within the film in a distinctly negative way. However, this does not mean that the film means to say there is absolutely no acceptable use of such things; there are also times when these are shown to be neutral or even positive. In truth there is a strong possibility that this film means to say that these technologies themselves are not evil, but the way that we as humans interact with them can make them that way. NausicaÃ¤ herself uses her gun towards the beginning of the film to remove part of an ohmu shell and she uses flash grenades to stun an ohmu, saving Lord Yupa. In the former instance, the use of the gun is shown in a completely neutral context, with it not even being used on any living thing and with the entire scene remaining quite peaceful; even the choices of lighting and colors being brighter than other parts of the Toxic Jungle adding to the serenity of the scene. This scene shows that such things can be used without causing any harm whatsoever. In the other instance the grenades are clearly used to stun the ohmu in order to protect and reduce harm for both parties.In the instances in which these technologies are used in a harmful way, the negativity of it is conveyed both in the expression of the characters witnessing it or by the colors shown in the scene. With the scene of the Giant Warrior being used towards the end of the film, the characters on both sides are clearly in shock and awe of just how destructive this technology can be. Additionally, the use of color in that scene helps in pushing just how the use of these technologies causes far more harm than good. The warrior itself is melting into this dark bloody red sludge; this nasty red is contrasted with its sharp green eyes that appear completely devoid of any sort of soul. Its visage is utterly grotesque and it remains that way till it falls apart.Other technologies that are more often used in warfare today are also critiqued. Towards the end of the film conflicts arise in the Valley with the people finally fighting back against the Tolmakian forces that have been occupying their land. Prior to this, the valley is shown to be a very peaceful and beautiful place with plenty of lush greenery and farmlands; the downfall of this serene environment begins when spores are found in the forest surrounding the valley. See Figures 8 and 9 for the before and after, respectively.Initially, the people mean only to use their tools to burn the spores, using fire in moderation in order to solve their problem. However, things quickly get out of hand and they realize that they have no choice but to burn the entire forest down if they donâ€™t want the Toxic Jungle to spread into the valley. This is the first time in the film that the valley is shown with colors like black, brown and others that are associated with decay, being more prevalent than greens and other more natural or lively colors. Throughout the entire film the use of fire is heavily frowned upon with Ohbaba warning the Tolmekians of why they should not even attempt to burn down the toxic jungle and, towards the end of the film, several characters going on about why they prefer the ways of the water and the wind over the way of fire since â€œToo much fire gives birth to nothing. Fire can reduce a forest to ashes in a day, while it takes the water and the wind 100 years to grow oneâ€. With this film Miyazaki is urging us to stop relying on fire to solve our problems, both in a literal and figurative sense. This is shown not just through the speech of the characters but also in the way that the scenery changes and the way the characters react to the use of excess fire.Human & environmental impactIn the film the human and environmental impact of human activities such as war are explored in many scenes. One example of this can be seen in one scene towards the end of the film where NausicaÃ¤ and another character, Asbel, fly into the city of Pejite together on NausicaÃ¤â€™s glider (Figs. 10, 11). It had previously been occupied by the Tolmekian forces but as they fly over it, the city is desolate and run down. The film reveals that the people of Pejite baited some of the ohmu into the city and let them wreak havoc in order to drive out the Tolmekians occupying the city. The film tries to show the audience how immoral this decision was not just from the sorrowful and ashamed expressions on the faces of the Pejite refugees and Asbel, the prince of Pejite. It also shows the impact these actions have had on the fauna, as the scenery is full of deceased animals of the Toxic Jungle. Additionally, while it is apparent that the city was once livable, now the characters need to use their masks to even be able to breathe within its premises. This shows that the damage done here was severe as some buildings were broken down, lives were lost and the land has been made uninhabitable for the foreseeable future.The Pejite use the same strategy again later in the film. In this second instance, they lure the ohmu into the Valley of the Wind in order to keep the Giant Warrior out of the hands of the Tolmekians. There were several scenes which depict this event since it is the major conflict of the film; however, since the previous scene examined focused more on the environmental impact of this act, here the focus is on the human impact. In the scene Mito, who serves sort of as the assistant to Princess NausicaÃ¤, returns to the Valley of the Wind and informs both the people of the valley and the occupying forces of the Tolmekians of the ohmu horde heading towards them. The audience is shown in this scene how much panic and distress the news brings both to the innocent citizens of the occupied territory and to the occupying forces, who were the only real target of this attack. The expressive use of facial expression and body language in this scene helps drive home just how impactful this is, even to the people who were never meant to be a target of this attack. Another scene that comes soon after reinforces this idea, when the insects begin their attack on the valley. This scene is utter chaos, the people attempt to seek shelter and the Tolmekian soldiers who were previously standing without shelter, emboldened by their control of the Giant Warrior, now scramble seeking shelter and clearly fearing for their lives. The expressiveness of this scene comes not only from the facial expressions and body language but also from the shaky and strange positioning of the camera which gives the scene a more panicked feel. In addition, the colors were muddled and the lighting was quite dim, with sudden flashes of bright blaring light that fed into the chaotic nature of the scene. This all works to show just how impactful efforts of war can be to humans, their environment and the other creatures who share it.Certain aspects of the method I chose when designing this project limited the results of my research. For instance, because I focused only on the visuals, my data does not take into account the role that the script, voice acting (especially the original) or soundtrack played in delivering certain messages to the audience. Additionally, while one can analyze, interpret, and hypothesize the main message(s) in the film and in each scene, it is impossible to extrapolate for all audiences. During the research process for this study, many additional questions have come up, including the following. Firstly, what effects do the audio elements of  have on the audience? Secondly, can  even be compared to Western religions given the fact that it was made in Japan? Lastly, how is the film interpreted by younger viewers versus older viewers?Through its stunning visual story-telling, NausicaÃ¤ of the Valley of the Wind invites its audience to reconsider the toll war takes not just on people, but also the environment. With color, body language, facial expressions, and the overall environment of scenes, Miyazaki warns against the dangers of harmful technology and the moral implications of its use. From the violent and gruesome images of the Giant Warrior and destroyed landscape as a result of the use of war technology, to the depleted and unsafe landscapes and fearful gazes of terrified civilians robbed of their homes and any sense of normalcy, the film seeks to depict the enormous destructiveness that results from war. Additionally, Miyazakiâ€™s illustrations are much more than fantasy, nodding toward the real world where parallel destruction occurs via nuclear weapons, environmental crises and modern warfare. The overarching message of the filmâ€™s visuals serves not only to paint a narrative, but to urge us to meet conflict with empathy, to protect our natural environment, and understand that it takes more courage and wisdom to find peace than to wage war. Although this data is subjective, seeing as it was based on my interpretation of the filmâ€™s visuals, it still suggests that visual storytelling has the potential to convey the ecologically damaging effects of war. Additionally, this discussion makes a contribution to the academic conversations on film, war and environmental sustainability all at the same time. Offering a visual, scene-based approach to analyzing storytelling that depicts war and environmental crisis in anime. Despite all this, further research is needed on large audience reception of these visual messages, cultural responses to these messages, or the influence auditory elements of film can have in delivering these messages.Abbasi, K.; Ali, P.; Barbour, V.; et al. (2023) Time to treat the climate & nature crisis as one indivisible global health emergency. The Indian Journal of Medical Research 158(4): 330â€“333. (2024) War practices and experiences: analyzing their effects on the environment in the Gulf Cooperation Council (GCC) region. The American Journal of Management and Economics Innovations 6(8): 64â€“88. (2004) Warâ€™s environmental impact. Alternatives Journal 30(4): 26. (2015) Legitimating the environmental injustices of war: toxic exposures and media silence in Iraq and Afghanistan. Environmental Politics 25(3): 395â€“413.Cao, Z.; Jin, S.; Yang, C.; et al. (2024) Reexamining the Kuleshov Effect: behavioral and neural evidence from authentic film experiments. PLOS ONE 19(8): e0308295. (2009) Shojo savior: Princess NausicaÃ¤, ecological pacifism, and the green gospel. Journal of Religion and Popular Culture 21(2): 1â€“16. (2021) Climate change, air pollution, and human health in the Kruger to Canyons Biosphere Region, South Africa, and Amazonas, Brazil: a narrative review. Atmosphere 15(5): 562. (2024) Democracy and kinship in NausicaÃ¤ of the Valley of the Wind. Climate Literacy in Education 2(1): 67â€“73. (2019) The Art of NausicaÃ¤ of the Valley of the Wind. VIZ Media LLC, San Francisco. (2015) Creatures in crisis: apocalyptic environmental visions in Miyazakiâ€™s NausicaÃ¤ of the Valley of the Wind and Princess Mononoke. Resilience: A Journal of the Environmental Humanities 2(3): 172â€“183. (2018) Anime landscapes as a tool for analyzing the humanâ€“environment relationship: Hayao Miyazaki films. Arts 7(2): 16. (2021) The toxic heroine in NausicaÃ¤ of the Valley of the Wind. In: Ferstl, P. (Ed.) Dialogues between Media, Vol. 5. De Gruyter, Berlin. Pp. 83â€“94.Walt Disney Studios Home Entertainment [Translator]. (1985) NausicaÃ¤ of the Valley of the Wind. Directed by Hayao Miyazaki. Studio Ghibli. English dub.Yusof, N.A.; Hussin, S.A.; Hashim, M.A.; Amin, A. (2024) Exploring the impact of anime on Muslim teenagersâ€™ moral behaviour. International Journal of Academic Research in Business and Social Sciences 14(7): 861â€“873.ChatGPT (GPT-4, OpenAI) was used to improve the writing style of this article. The author reviewed, edited, and revised the ChatGPT-generated texts to her own liking and takes ultimate responsibility for the content of this publication. Special thanks to my AP Research teacher and classmates for their feedback and support during the development of this paper. is a student of Upland High School. She has been fascinated with Studio Ghibliâ€™s Films since the first time she saw one and couldnâ€™t get enough of the studioâ€™s beautiful animation and wonderful storytelling. For the longest time her two favorite films had been and . She canâ€™t be sure what the future has in store for her but she hopes she can see many more meaningful films such as the ones sheâ€™ve loved from Studio Ghibli.]]></content:encoded></item><item><title>Phoenix.new â€“ Remote AI Runtime for Phoenix</title><link>https://fly.io/blog/phoenix-new-the-remote-ai-runtime/</link><author>wut42</author><category>hn</category><pubDate>Fri, 20 Jun 2025 14:57:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Iâ€™m Chris McCord, the creator of Elixirâ€™s Phoenix framework. For the past several months, Iâ€™ve been working on a skunkworks project at Fly.io, and itâ€™s time to show it off.I wanted LLM agents to work just as well with Elixir as they do with Python and JavaScript. Last December, in order to figure out what that was going to take, I started a little weekend project to find out how difficult it would be to build a coding agent in Elixir.A few weeks later, I had it spitting out working Phoenix applications and driving a full in-browser IDE. I knew this wasnâ€™t going to stay a weekend project.If you follow me on Twitter, youâ€™ve probably seen me teasing this work as it picked up steam. Weâ€™re at a point where weâ€™re pretty serious about this thing, and so itâ€™s time to make a formal introduction.World, meet Phoenix.new, a batteries-included fully-online coding agent tailored to Elixir and Phoenix. I think itâ€™s going to be the fastest way to build collaborative, real-time applications.First, even though it runs entirely in your browser, Phoenix.new gives both you and your agent a root shell, in an ephemeral virtual machine (a Fly Machine) that gives our agent loop free rein to install things and run programs  â€” without any risk of messing up your local machine. You donâ€™t think about any of this; you just open up the VSCode interface, push the shell button, and there you are, on the isolated machine you share with the Phoenix.new agent.Second, itâ€™s an agent system I built specifically for Phoenix. Phoenix is about real-time collaborative applications, and Phoenix.new knows what that means. To that end, Phoenix.new includes, in both its UI and its agent tools, a full browser. The Phoenix.new agent uses that browser â€œheadlesslyâ€ to check its own front-end changes and interact with the app. Because itâ€™s a full browser, instead of trying to iterate on screenshots, the agent sees real page content and JavaScript state â€“ with or without a human present.Agents build software the way you did when you first got started, the way you still do today when you prototype things. They donâ€™t carefully design Docker container layers and they donâ€™t really do release cycles. An agent wants to pop a shell and get its fingernails dirty.A fully isolated virtual machine means Phoenix.newâ€™s fingernails can get  If it wants to add a package to , it can do that and then run  or  and check the output. Sure. Every agent can do that. But if it wants to add an APT package to the base operating system, it can do that too, and make sure it worked. It owns the whole environment.This offloads a huge amount of tedious, repetitive work.At his AI Startup School talk last week, Andrej Karpathy related his experience of building a restaurant menu visualizer, which takes camera pictures of text menus and transforms all the menu items into pictures. The code, which he vibe-coded with an LLM agent, was the easy part; he had it working in an afternoon. But getting the app online took him a whole week.With Phoenix.new, Iâ€™m taking dead aim at this problem. The apps we produce live in the cloud from the minute they launch. They have private, shareable URLs (we detect anything the agent generates with a bound port and give it a preview URL underneath , with integrated port-forwarding), they integrate with Github, and they inherit all the infrastructure guardrails of Fly.io: hardware virtualization, WireGuard, and isolated networks.Githubâ€™s  CLI is installed by default. So the agent knows how to clone any repo, or browse issues, and you can even authorize it for internal repositories to get it working with your teamâ€™s existing projects and dependencies.Full control of the environment also closes the loop between the agent and deployment. When Phoenix.new boots an app, it watches the logs, and tests the application. When an action triggers an error, Phoenix.new notices and gets to work.Phoenix.new can interact with web applications the way users do: with a real browser.The Phoenix.new environment includes a headless Chrome browser that our agent knows how to drive. Prompt it to add a front-end feature to your application, and it wonâ€™t just sketch the code out and make sure it compiles and lints. Itâ€™ll pull the app up itself and poke at the UI, simultaneously looking at the page content, JavaScript state, and server-side logs.Phoenix is all about â€œliveâ€ real-time interactivity, and gives us seamless live reload. The user interface for Phoenix.new itself includes a live preview of the app being worked on, so you can kick back and watch it build front-end features incrementally. Any other  tabs you have open also update as it goes. Itâ€™s wild.Phoenix.new can already build real, full-stack applications with WebSockets, Phoenixâ€™s Presence features, and real databases. Iâ€™m seeing it succeed at business and collaborative applications right now.But thereâ€™s no fixed bound on the tasks you can reasonably ask it to accomplish. If you can do it with a shell and a browser, I want Phoenix.new to do it too. And it can do these tasks with or without you present.For example: set a  and tell the agent about it. The agent knows enough to go explore it with , and itâ€™ll propose apps based on the schemas it finds. It can model Ecto schemas off the database. And if MySQL is your thing, the agent will just  a MySQL client and go to town.Frontier model LLMs have vast world knowledge. They generalize extremely well. At ElixirConfEU, I did a demo vibe-coding Tetris on stage. Phoenix.new nailed it, first try, first prompt. Itâ€™s not like thereâ€™s gobs of Phoenix LiveView Tetris examples floating around the Internet! But lots of people have published Tetris code, and lots of people have written LiveView stuff, and 2025 LLMs can connect those dots.At this point you might be wondering â€“ can I just ask it to build a Rails app? Or an Expo React Native app? Or Svelte? Or Go?Our system prompt is tuned for Phoenix today, but all languages you care about are already installed. Weâ€™re still figuring out where to take this, but adding new languages and frameworks definitely ranks highly in my plans.Agents can do real work, today, with or without a human present. Buckle up: the future of development, at least in the common case, probably looks less like cracking open a shell and finding a file to edit, and more like popping into a CI environment with agents working away around the clock.Local development isnâ€™t going away. But thereâ€™s going to be a shift in where the majority of our iterations take place. Iâ€™m already using Phoenix.new to triage  Github issues and pick problems to solve. I close my laptop, grab a cup of coffee, and wait for a PR to arrive â€” Phoenix.new knows how PRs work, too. Weâ€™re already here, and this space is just getting started.This isnâ€™t where I thought Iâ€™d end up when I started poking around. The Phoenix and LiveView journey was much the same. Something special was there and the projects took on a life of their own. Iâ€™m excited to share this work now, and see where it might take us. I canâ€™t wait to see what folks build.]]></content:encoded></item><item><title>Congestion pricing in Manhattan is a predictable success</title><link>https://www.economist.com/united-states/2025/06/19/congestion-pricing-in-manhattan-is-a-predictable-success</link><author>edward</author><category>hn</category><pubDate>Fri, 20 Jun 2025 14:26:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[, a speech therapist in New York City, was dreading the introduction of congestion pricing. To see her patients in Queens and Manhattan she sometimes drives across the East River a couple of times a day. The idea of paying a $9 toll each day infuriated her. Yet since the policy was actually implemented, she has changed her mind. A journey which used to take an hour or more can now be as quick as 15 minutes. â€œWell, this is very nice,â€ she admits thinking. Ms Ryan is not alone. Polls show more New Yorkers now support the toll than oppose it. A few months ago, it saw staunch opposition.]]></content:encoded></item><item><title>Meta announces Oakley smart glasses</title><link>https://www.theverge.com/news/690133/meta-oakley-hstn-ai-glasses-price-date</link><author>jmsflknr</author><category>hn</category><pubDate>Fri, 20 Jun 2025 13:17:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Meta is announcing its next pair of smart glasses with Oakley. The limited-edition Oakley Meta HSTN (pronounced â€œhow-stuhnâ€) model costs $499 and is available for preorder starting July 11th. Other Oakley models with Metaâ€™s tech will be available starting at $399 later this summer. Like the existing Meta Ray-Ban glasses, the Oakley model features a front-facing camera, along with open-ear speakers and microphones that are built into the frame. After they are paired with a phone, the glasses can be used to listen to music or podcasts, conduct phone calls, or chat with Meta AI. By utilizing the onboard camera and microphones, Meta AI can also answer questions about what someone is seeing and even translate languages. Given the Oakley design, Meta is positioning these new glasses as being geared towards athletes. They have an IPX4 water resistance rating and offer double the battery life of the Meta Ray-Bans, providing 8 hours of use, along with a charging case that can power them for up to 48 hours. The built-in camera now shoots in 3K video, up from 1080p for the Meta Ray-Bans. The new lineup comes in five Oakley frame and lens combos, all of which are compatible with prescriptions for an extra cost. The frame colors are warm grey, black, brown smoke, and clear, with several lens options available, including transitions. The limited-edition $499 model, available for order starting July 11th, features gold accents and gold Oakley PRIZM lenses. The glasses will be on sale in the US, Canada, the UK, Ireland, France, Italy, Spain, Austria, Belgium, Australia, Germany, Sweden, Norway, Finland, and Denmark.Meta recently signed a multi-year deal with EssilorLuxottica, the parent company behind Ray-Ban, Oakley, and other eyewear brands. The Meta Ray-Bans have sold over two million pairs to date, and EssilorLuxottica recently disclosed that it plans to sell 10 million smart glasses with Meta annually by 2026. â€œThis is our first step into the performance category,â€ Alex Himel, Metaâ€™s head of wearables, tells me. â€œThereâ€™s more to come.â€]]></content:encoded></item><item><title>Klong: A Simple Array Language</title><link>https://t3x.org/klong/</link><author>tosh</author><category>hn</category><pubDate>Fri, 20 Jun 2025 12:44:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Klong is an array language, like K, but without the ambiguity. If
you know K or APL, you may be disappointed by Klong. If you don't
know any array languages, it might explode your brain. Use at your
own risk!
A Klong program is a set of functions that use various pre-defined
operators to manipulate lists (vectors) and (multi-dimensional)
arrays. Here is a program that checks whether a number  is prime
(for ):
Note that Klong is a mathematical notation rather than a programming
language. If you try to use it like your favorite functional/procedural/OO
programming language, you will only get frustrated. Here's an
explanation of the above program.
The Reference Manual (klong-ref.txt) provides a complete and detailed
semi-formal description of the Klong language. It is probably the
best starting point for exploring Klong.
The Quick Reference (klong-qref.txt) summarizes the syntax and
semantics of the language. It will probably only make sense if you
already know K or APL.
Then there is a Really Short Introduction to Klong (klong-intro.txt),
which you might want to read if you have never used an array language
before.
Finally, if you already know K, here is a (probably incomplete)
summary of differences between Klong and K: klong-vs-k.txt.
Compiling and Installing KlongKlong is written in pure ANSI C (C99), so it should compile on any
system providing a C compiler. Just run make and make test. It also
compiles natively on Plan 9!
To install Klong, just copy the  binary to  or some
similar place and point the  environment variable to the
 directory of the Klong source tree.
Files ending in  are Klong programs, you can load them with
 or  (with or without the suffix).
In case my Klong interpreter is not fast enough for you, Brian
Gurraci has created a vectorized version of Klong named KlongPy.
You can find it on GitHub: https://github.com/briangu/klongpy]]></content:encoded></item><item><title>Show HN: SnapQL â€“ Desktop app to query Postgres with AI</title><link>https://github.com/NickTikhonov/snap-ql</link><author>nicktikhonov</author><category>dev</category><category>hn</category><pubDate>Fri, 20 Jun 2025 11:08:18 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[SnapQL is an open-source desktop app (built with Electron) that lets you query your Postgres database using natural language. Itâ€™s schema-aware, so you donâ€™t need to copy-paste your schema or write complex SQL by hand.Everything runs locally â€” your OpenAI API key, your data, and your queries â€” so it's secure and private. Just connect your DB, describe what you want, and SnapQL writes and runs the SQL for you.]]></content:encoded></item><item><title>Oklo, the Earth&apos;s Two-billion-year-old only Known Natural Nuclear Reactor (2018)</title><link>https://www.iaea.org/newscenter/news/meet-oklo-the-earths-two-billion-year-old-only-known-natural-nuclear-reactor</link><author>keepamovin</author><category>hn</category><pubDate>Fri, 20 Jun 2025 09:52:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Physicist Francis Perrin sat at a nuclearfuel-processing plant down in the south of France, thinking to himself: â€œThis cannot be possible.â€ It was 1972. On the one hand, there was a dark piece of radioactive natural uranium ore, extracted from a mine in Africa. On the other, accepted scientific data about the constant ratio of radioactive uranium in ore.Examination of this high-grade ore from a mine in Gabon was found to contain a lower proportion of uranium-235 (U-235) â€” the fissile sort. Only a tiny bit less, but enough to make the researchers sit back and scratch their heads.The physicistsâ€™ first, logical response to such an unusual ratio of U-235 was that this was not natural uranium. All natural uranium today contains 0.720% of U-235. If you were to extract it from the Earthâ€™s crust, or from rocks from the moon or in meteorites, thatâ€™s what you would find. But that bit of rock from Oklo contained only 0.717%.What did this mean? At first, all the physicists could think of was that the uranium ore had gone through artificial fission, i.e.Â that some of the U-235 isotopes had been forced to split in a nuclear chain reaction. This could explain why the ratio was lower than normal.But after complementary analyses, Perrin and his peers confirmed that the uranium ore was completely natural. Even more bedazzling, they discovered a footprint of fission products in the ore. The conclusion: the uranium ore was natural and had gone through fission. There was only one possible explanation â€” the rock was evidence of natural fission that occurred over two billion years ago.â€œAfter more studies, including on-site examinations, they discovered that the uranium ore had gone through fission on its own,â€ said Ludovic FerriÃ¨re, curator of the rock collection at Viennaâ€™s Natural History Museum, where a part of the curious rock will be presented to the public in 2019. â€œThere was no other explanation.â€For such a phenomenon to have happened naturally, these uranium deposits in western Equatorial Africa must have had to contain a critical mass of U-235 to start the reaction. Back in those days, they did.Â A second contributing factor was that, for a nuclear chain reaction to happen and be maintained, there needed to be a moderator. In this case: water. Without water to slow the neutrons down, controlled fission would not have been possible. The atoms would simply not have split.â€œLike in a man-made light-water nuclear reactor, the fission reactions, without anything to slow down the neutrons, to moderate them, simply stop,â€ said Peter Woods, team leader in charge of uranium production at the IAEA. â€œThe water acted in Oklo as a moderator, absorbing the neutrons, controlling the chain reaction.â€The specific geological context in what today is Gabon also helped. The chemical concentrations of total uranium (including U-235) were high enough, and the individual deposits thick and large enough. And, lastly, Oklo managed to survive the passing of time. Experts suspect there may have been other such natural reactors in the world, but these must have been destroyed by geological processes, eroded away or subducted â€” Â or simply not yet found.Â â€œThatâ€™s what makes it so fascinating: that Â the circumstances of time, geology, water came together for this to happen at all,â€ Woods said. â€œAnd that it was preserved until today. The detective story has been successfully solved.â€A rock sample in the IAEAâ€™s Â home city]]></content:encoded></item><item><title>I will do anything to end homelessness except build more homes (2018)</title><link>https://www.mcsweeneys.net/articles/i-will-do-anything-to-end-homelessness-except-build-more-homes</link><author>2color</author><category>hn</category><pubDate>Fri, 20 Jun 2025 08:07:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Homelessness in America has reached crisis levels, and I am determined to do everything in my power to fix the problem as long as it doesnâ€™t involve changing zoning laws or my ability to drive alone to work or, well, changing anything, really. Iâ€™m more than happy to give a hungry man a sandwich once a year and then brag to my friends about it as long as he doesnâ€™t sit down anywhere in my line of sight to eat it. Same goes for hungry women because Iâ€™m also a feminist.This is so important because everyone should have a bed to sleep in at night, and also, nothing destroys property values faster than a desperate person on a sidewalk asking for change. Iâ€™m not saying I donâ€™t care about human suffering; I just care much, much more about my immediate self-interest because Iâ€™m the kind of person who contributes to society by starting companies that leverage technology to build smart tea kettles that brew themselves while you sleep at night. Iâ€™m a fucking innovator.Iâ€™m innovating for win-win-whatever solutions where I win, my community wins, and we do whatever to get rid of homelessness. Fixing the problem means lots of things: letters to the editor of my local newspaper, bombastic statements to the press that will make the fruit of my loins cringe for generations, and especially writing vaguely discriminatory, definitely ugly posts on social media about the crisis as it unfolds in my community. Also, I call the police a lot.Ending homelessness doesnâ€™t mean building more homes because this town is full of homes already, especially mine, which is a single-family mini-mansion on an acre lot that I inherited from my parents and/or managed to purchase with the kind of job and bank terms and economic equality that donâ€™t exist anymore for anyone and only ever really existed for well-educated white Americans. Either that or itâ€™s a magnificent luxury condo with expansive views that I donâ€™t want marred by more luxury condos orâ€”god forbidâ€”affordable housing.Every room in my Instagram-worthy abode is either filled with clutter or rented out nightly to hipsters from another gentrified, monotone city also suffering from a homelessness crisisâ€”this is a national epidemic, after all. Iâ€™m a good person, a generous person, and what made me the person I am is having to work hard for everything my parents gave me, and everything I will, in turn, give to my children.Listen, I know that the unholy concentration of wealth in America is a big, big problem, but so is having to constantly say no to people asking for change as I whizz into Whole Foods in my Tesla or Prius (depending on how my startup investments pan out). Whatâ€™s the point of having all this money if I have to feel bad about it? Also, has anyone actually verified that the homeless people claiming to be veterans arenâ€™t just pulling some elaborate fraud? Iâ€™ve never actually met a veteran and I forget for, like, decades at a time that the military even exists because the bubble of privilege where I reside is literally impregnable, but Iâ€™m suspicious nonetheless.I know we need more housing, but I was here first, and Iâ€™m not giving up even one blade of grass on my water-guzzling, pesticide-leaching lawn or a single burner on my twelve-burner Viking range that I never actually use to house another human soul. Tough luck, homeless people. You and your allies can call me names, but I wonâ€™t hear you over the lushness of my climate-inappropriate rose bushes and the stucco walls Iâ€™m paying some desperate immigrant under the table to build for me on the cheap before I low-key call  and have them deported.Look, if you give people homes, the next thing you know, theyâ€™re going to start to get their lives together and then get jobs and start organizing. Then theyâ€™ll expand Medicare to everyone and build a fucking light rail line instead of a goddamn border wall, and no one will drive anymore, and cars will die out, and the air will get clean, and can you imagine the problems weâ€™ll have then?No. Stop it with the new housing; Iâ€™d rather have a homeless crisis.]]></content:encoded></item><item><title>Learn Makefiles</title><link>https://makefiletutorial.com/</link><author>dsego</author><category>hn</category><pubDate>Fri, 20 Jun 2025 08:05:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I built this guide because I could never quite wrap my head around Makefiles. They seemed awash with hidden rules and esoteric symbols, and asking simple questions didnâ€™t yield simple answers. To solve this, I sat down for several weekends and read everything I could about Makefiles. I've condensed the most critical knowledge into this guide. Each topic has a brief description and a self contained example that you can run yourself.If you mostly understand Make, consider checking out the Makefile Cookbook, which has a template for medium sized projects with ample comments about what each part of the Makefile is doing.Good luck, and I hope you are able to slay the confusing world of Makefiles!Makefiles are used to help decide which parts of a large program need to be recompiled. In the vast majority of cases, C or C++ files are compiled. Other languages typically have their own tools that serve a similar purpose as Make. Make can also be used beyond compilation too, when you need a series of instructions to run depending on what files have changed. This tutorial will focus on the C/C++ compilation use case.Here's an example dependency graph that you might build with Make. If any file's dependencies changes, then the file will get recompiled:What alternatives are there to Make?Interpreted languages like Python, Ruby, and raw Javascript don't require an analogue to Makefiles. The goal of Makefiles is to compile whatever files need to be compiled, based on what files have changed. But when files in interpreted languages change, nothing needs to get recompiled. When the program runs, the most recent version of the file is used.The versions and types of MakeThere are a variety of implementations of Make, but most of this guide will work on whatever version you're using. However, it's specifically written for GNU Make, which is the standard implementation on Linux and MacOS. All the examples work for Make versions 3 and 4, which are nearly equivalent other than some esoteric differences.To run these examples, you'll need a terminal and "make" installed. For each example, put the contents in a file called , and in that directory run the command . Let's start with the simplest of Makefiles:Note: Makefiles  be indented using TABs and not spaces or  will fail.Here is the output of running the above example:
echo "Hello, World"
Hello, WorldThat's it! If you're a bit confused, here's a video that goes through these steps, along with describing the basic structure of Makefiles.A Makefile consists of a set of . A rule generally looks like this:
	command
	command
	commandThe  are file names, separated by spaces. Typically, there is only one per rule.The  are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces.The  are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called Let's start with a hello world example:
	echo 
	echo There's already a lot to take in here. Let's break it down:We have one  called This target has two This target has no We'll then run . As long as the  file does not exist, the commands will run. If  does exist, no commands will run.It's important to realize that I'm talking about  as both a  and a . That's because the two are directly tied together. Typically, when a target is run (aka when the commands of a target are run), the commands will create a file with the same name as the target. In this case, the  does not create the .Let's create a more typical Makefile - one that compiles a single C file. But before we do, make a file called  that has the following contents:Then create the Makefile (called , as always):This time, try simply running . Since there's no target supplied as an argument to the  command, the first target is run. In this case, there's only one target (). The first time you run this,  will be created. The second time, you'll see make: 'blah' is up to date. That's because the  file already exists. But there's a problem: if we modify  and then run , nothing gets recompiled.We solve this by adding a prerequisite:
	cc blah.c -o blahWhen we run  again, the following set of steps happens:The first target is selected, because the first target is the default targetThis has a prerequisite of Make decides if it should run the  target. It will only run if  doesn't exist, or  is This last step is critical, and is the . What it's attempting to do is decide if the prerequisites of  have changed since  was last compiled. That is, if  is modified, running  should recompile the file. And conversely, if  has not changed, then it should not be recompiled.To make this happen, it uses the filesystem timestamps as a proxy to determine if something has changed. This is a reasonable heuristic, because file timestamps typically will only change if the files are
modified. But it's important to realize that this isn't always the case. You could, for example, modify a file, and then change the modified timestamp of that file to something old. If you did, Make would incorrectly guess that the file hadn't changed and thus could be ignored.Whew, what a mouthful. Make sure that you understand this. It's the crux of Makefiles, and might take you a few minutes to properly understand. Play around with the above examples or watch the video above if things are still confusing.The following Makefile ultimately runs all three targets. When you run  in the terminal, it will build a program called  in a series of steps:Make selects the target , because the first target is the default target requires , so make searches for the  target requires , so make searches for the  target has no dependencies, so the  command is runThe  command is then run, because all of the  dependencies are finishedThe top  command is run, because all the  dependencies are finishedThat's it:  is a compiled c program
	cc blah.o -o blah 
	cc -c blah.c -o blah.o 
	echo  > blah.c If you delete , all three targets will be rerun. If you edit it (and thus change the timestamp to newer than ), the first two targets will run. If you run  (and thus change the timestamp to newer than ), then only the first target will run. If you change nothing, none of the targets will run. Try it out!This next example doesn't do anything new, but is nontheless a good additional example. It will always run both targets, because  depends on , which is never created.
	echo 
	touch some_file


	echo  is often used as a target that removes the output of other targets, but it is not a special word in Make. You can run  and  on this to create and delete .Note that  is doing two new things here:It's a target that is not first (the default), and not a prerequisite. That means it'll never run unless you explicitly call It's not intended to be a filename. If you happen to have a file named , this target won't run, which is not what we want. See  later in this tutorial on how to fix this
	touch some_file


	rm -f some_fileVariables can only be strings. You'll typically want to use , but  also works. See Variables Pt 2.Here's an example of using variables:files := file1 file2

	echo 
	touch some_file


	touch file1

	touch file2


	rm -f file1 file2 some_fileSingle or double quotes have no meaning to Make. They are simply characters that are assigned to the variable. Quotes  useful to shell/bash, though, and you need them in commands like . In this example, the two commands behave the same:a := one two
b := 'one two' 
	printf '$a'
	printf $bReference variables using either  or x := dude


	echo 
	echo ${x}

	
	echo $x Making multiple targets and you want all of them to run? Make an  target.
Since this is the first rule listed, it will run by default if  is called without specifying a target.
	touch one

	touch two

	touch three


	rm -f one two three
When there are multiple targets for a rule, the commands will be run for each target.  is an automatic variable that contains the target name.

f1.o f2.o:
	echo Both  and  are called wildcards in Make, but they mean entirely different things.  searches your filesystem for matching filenames. I suggest that you always wrap it in the  function, because otherwise you may fall into a common pitfall described below.
	ls -la   may be used in the target, prerequisites, or in the  function.Danger:  may not be directly used in a variable definitionsDanger: When  matches no files, it is left as it is (unless run in the  function)thing_wrong := *.o 
thing_right :=  is really useful, but is somewhat confusing because of the variety of situations it can be used in.When used in "matching" mode, it matches one or more characters in a string. This match is called the stem.When used in "replacing" mode, it takes the stem that was matched and replaces that in a string. is most often used in rule definitions and in some specific functions.See these sections on examples of it being used:
	echo 
	echo 
	echo 
	echo 

	touch hey


	touch one


	touch two


	rm -f hey one two
Make loves c compilation. And every time it expresses its love, things get confusing. Perhaps the most confusing part of Make is the magic/automatic rules that are made. Make calls these "implicit" rules. I don't personally agree with this design decision, and I don't recommend using them, but they're often used and are thus useful to know. Here's a list of implicit rules:Compiling a C program:  is made automatically from  with a command of the form $(CC) -c $(CPPFLAGS) $(CFLAGS) $^ -o $@Compiling a C++ program:  is made automatically from  or  with a command of the form $(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $^ -o $@Linking a single object file:  is made automatically from  by running the command $(CC) $(LDFLAGS) $^ $(LOADLIBES) $(LDLIBS) -o $@The important variables used by implicit rules are:: Program for compiling C programs; default : Program for compiling C++ programs; default : Extra flags to give to the C compiler: Extra flags to give to the C++ compiler: Extra flags to give to the C preprocessor: Extra flags to give to compilers when they are supposed to invoke the linkerLet's see how we can now build a C program without ever explicitly telling Make how to do the compilation:CC = gcc 
CFLAGS = -g 
	echo  > blah.c


	rm -f blah*Static pattern rules are another way to write less in a Makefile. Here's their syntax:
   commandsThe essence is that the given  is matched by the  (via a  wildcard). Whatever was matched is called the . The stem is then substituted into the , to generate the target's prereqs.A typical use case is to compile  files into  files. Here's the :objects = foo.o bar.o all.o
 -o all

 -c foo.c -o foo.o

 -c bar.c -o bar.o

 -c all.c -o all.o


	echo  > all.c


	touch 
	rm -f *.c *.o allHere's the more , using a static pattern rule:objects = foo.o bar.o all.o
 -o all

: %.o: %.c
	 -c  -o 
	echo  > all.c


	touch 
	rm -f *.c *.o allStatic Pattern Rules and FilterWhile I introduce the filter function later on, it's common to use in static pattern rules, so I'll mention that here. The  function can be used in Static pattern rules to match the correct files. In this example, I made up the  and  extensions.obj_files = foo.result bar.o lose.o
src_files = foo.raw bar.c lose.c

: %.o: %.c
	echo : %.result: %.raw
	echo  

%.c %.raw:
	touch 
	rm -f Pattern rules are often used but quite confusing. You can look at them as two ways:A way to define your own implicit rulesA simpler form of static pattern rulesLet's start with an example first:
%.o : %.c
		 -c  -o Pattern rules contain a '%' in the target. This '%' matches any nonempty string, and the other characters match themselves. â€˜%â€™ in a prerequisite of a pattern rule stands for the same stem that was matched by the â€˜%â€™ in the target.Double-Colon Rules are rarely used, but allow multiple rules to be defined for the same target. If these were single colons, a warning would be printed and only the second set of commands would run.
	echo 
	echo Add an  before a command to stop it from being printedYou can also run make with  to add an  before each line  
	@echo 
	echo Each command is run in a new shell (or at least the effect is as such)
	cd ..
	
	echo `pwd`

	
	cd ..;echo `pwd`

	
	cd ..; \
	echo `pwd`
The default shell is . You can change this by changing the variable SHELL:SHELL=/bin/bash


	echo If you want a string to have a dollar sign, you can use . This is how to use a shell variable in  or .Note the differences between Makefile variables and Shell variables in this next example.make_var = I am a make variable

	sh_var='I am a shell variable'; echo $$sh_var

	
	echo Error handling with , , and Add  when running make to continue running even in the face of errors. Helpful if you want to see all the errors of Make at once.Add a  before a command to suppress the errorAdd  to make to have this happen for every command.Interrupting or killing makeNote only: If you  make, it will delete the newer targets it just made.To recursively call a makefile, use the special  instead of  because it will pass the make flags for you and won't itself be affected by them.new_contents = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	cd subdir && 
	rm -rf subdir
Export, environments, and recursive makeWhen Make starts, it automatically creates Make variables out of all the environment variables that are set when it's executed.
	echo $$shell_env_var

	
	echo The  directive takes a variable and sets it the environment for all shell commands in all the recipes:shell_env_var=Shell env var, created inside of Make
 shell_env_var

	echo 
	echo $$shell_env_varAs such, when you run the  command inside of make, you can use the  directive to make it accessible to sub-make commands. In this example,  is exported such that the makefile in subdir can use it.new_contents = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	@echo 
	@cd subdir && cat makefile
	@echo 
	cd subdir && 
cooly =  cooly

	rm -rf subdirYou need to export variables to have them run in the shell as well.  one=this will only work locally
 two=we can run subcommands with this


	@echo 
	@echo $$one
	@echo 
	@echo $$two exports all variables for you.
new_contents = 

cooly = 
	mkdir -p subdir
	printf  | sed -e 's/^ //' > subdir/makefile
	@echo 
	@cd subdir && cat makefile
	@echo 
	cd subdir && 
	rm -rf subdirThere's a nice list of options that can be run from make. Check out , , . You can have multiple targets to make, i.e.  runs the  goal, then , and then .There are two flavors of variables:  recursive (use ) - only looks for the variables when the command is , not when it's .  simply expanded (use ) - like normal imperative programming -- only those defined so far get expanded
one = one ${later_variable}

two := two ${later_variable}

later_variable = later


	echo 
	echo Simply expanded (using ) allows you to append to a variable. Recursive definitions will give an infinite loop error.  one = hello

one := ${one} there


	echo  only sets variables if they have not yet been setone = hello
one ?= will not be set
two ?= will be set


	echo 
	echo Spaces at the end of a line are not stripped, but those at the start are. To make a variable with a single space, use with_spaces = hello   
after = there

nullstring =
space = 
	echo 
	echo startendAn undefined variable is actually an empty string!foo := start
foo += more


	echo You can override variables that come from the command line by using .
Here we ran make with  option_one = did_override

option_two = not_override

	echo 
	echo The define directive is not a function, though it may look that way. I've seen it used so infrequently that I won't go into details, but it's mainly used for defining canned recipes and also pairs well with the eval function./ simply creates a variable that is set to a list of commands. Note here that it's a bit different than having a semi-colon between commands, because each is run in a separate shell, as expected.one =  blah=; echo $$blah

 two
 blah=
echo $$blah

	@echo 
	@
	@echo 
	@Target-specific variablesVariables can be set for specific targets
	echo one is defined: 
	echo one is nothing: Pattern-specific variablesYou can set variables for specific target 
	echo one is defined: 
	echo one is nothing: foo = ok

 (, ok)
	echo 
	echo Check if a variable is emptynullstring =
foo =  (,)
	echo  (,)
	echo Check if a variable is definedifdef does not expand variable references; it just sees if something is defined at allbar =
foo =  foo
	echo  bar
	echo This example shows you how to test make flags with  and . Run this example with  to see it print out the echo statement. (,)
	echo  are mainly just for text processing. Call functions with  or . Make has a decent amount of builtin functions.bar := ${subst not,, }

	@echo If you want to replace spaces or commas, use variablescomma := ,
empty:=
space := 
foo := a b c
bar := 
	@echo Do NOT include spaces in the arguments after the first. That will be seen as part of the string.comma := ,
empty:=
space := 
foo := a b c
bar := 
	@echo $(patsubst pattern,replacement,text) does the following:"Finds whitespace-separated words in text that match pattern and replaces them with replacement. Here pattern may contain a â€˜%â€™ which acts as a wildcard, matching any number of any characters within a word. If replacement also contains a â€˜%â€™, the â€˜%â€™ is replaced by the text that matched the â€˜%â€™ in pattern. Only the first â€˜%â€™ in the pattern and replacement is treated this way; any subsequent â€˜%â€™ is unchanged." (GNU docs)The substitution reference $(text:pattern=replacement) is a shorthand for this.There's another shorthand that replaces only suffixes: $(text:suffix=replacement). No  wildcard is used here.Note: don't add extra spaces for this shorthand. It will be seen as a search or replacement term.foo := a.o b.o l.a c.o
one := 
two := $(foo:%.o=%.c)

three := $(foo:.o=.c)


	echo 
	echo 
	echo The foreach function looks like this: . It converts one list of words (separated by spaces) to another.  is set to each word in list, and  is expanded for each word.This appends an exclamation after each word:foo := who are you

bar := 
	@echo  checks if the first argument is nonempty. If so, runs the second argument, otherwise runs the third.foo := 
empty :=
bar := 
	@echo 
	@echo Make supports creating basic functions. You "define" the function just by creating a variable, but use the parameters , , etc. You then call the function with the special  builtin function. The syntax is $(call variable,param,param).  is the variable, while , , etc. are the params.sweet_new_fn = Variable Name: $(0) First: $(1) Second: $(2) Empty Variable: $(3)


	@echo shell - This calls the shell, but it replaces newlines with spaces!The  function is used to select certain elements from a list that match a specific pattern. For example, this will select all elements in  that end with .obj_files = foo.result bar.o lose.o
filtered_files = 
	@echo Filter can also be used in more complex ways:Filtering multiple patterns: You can filter multiple patterns at once. For example, $(filter %.c %.h, $(files)) will select all  and  files from the files list.: If you want to select all elements that do not match a pattern, you can use . For example, $(filter-out %.h, $(files)) will select all files that are not  files.: You can nest filter functions to apply multiple filters. For example, $(filter %.o, $(filter-out test%, $(objects))) will select all object files that end with  but don't start with .The include directive tells make to read one or more other makefiles. It's a line in the makefile that looks like this:This is particularly useful when you use compiler flags like  that create Makefiles based on the source. For example, if some c files includes a header, that header will be added to a Makefile that's written by gcc. I talk about this more in the Makefile CookbookUse vpath to specify where some set of prerequisites exist. The format is vpath <pattern> <directories, space/colon separated> can have a , which matches any zero or more characters.
You can also do this globallyish with the variable VPATH %.h ../headers ../other-directory


	touch some_binary


	mkdir ../headers


	touch ../headers/blah.h


	rm -rf ../headers
	rm -f some_binary
The backslash ("\") character gives us the ability to use multiple lines when the commands are too long
	echo This line is too long, so \
		it is broken up into multiple linesAdding  to a target will prevent Make from confusing the phony target with a file name. In this example, if the file  is created, make clean will still be run. Technically, I should have used it in every example with  or , but I wanted to keep the examples clean. Additionally, "phony" targets typically have names that are rarely file names, and in practice many people skip this.
	touch some_file
	touch clean


	rm -f some_file
	rm -f cleanThe make tool will stop running a rule (and will propogate back to prerequisites) if a command returns a nonzero exit status. will delete the target of a rule if the rule fails in this manner. This will happen for all targets, not just the one it is before like PHONY. It's a good idea to always use this, even though make does not for historical reasons.  
	touch one
	false


	touch two
	falseLet's go through a really juicy Make example that works well for medium sized projects.The neat thing about this makefile is it automatically determines dependencies for you. All you have to do is put your C/C++ files in the  folder.
TARGET_EXEC := final_program

BUILD_DIR := ./build
SRC_DIRS := ./src


SRCS := 
OBJS := $(SRCS:%=/%.o)


DEPS := $(OBJS:.o=.d)


INC_DIRS := 
INC_FLAGS := 
CPPFLAGS :=  -MMD -MP

/:  -o /%.c.o: %.c
	mkdir -p  -c  -o /%.cpp.o: %.cpp
	mkdir -p  -c  -o 
	rm -r ]]></content:encoded></item><item><title>Break Up Big Tech: Civil Society Declaration â€“ People vs. Big Tech</title><link>https://peoplevsbig.tech/break-up-big-tech-civil-society-declaration/</link><author>janandonly</author><category>hn</category><pubDate>Fri, 20 Jun 2025 08:02:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We, people and civil society organisations from Europe and around the world, call on the European Commission to act now to break up the powerful Big Tech monopolies that have a stranglehold over our digital world. Big Tech isnâ€™t just dominating markets â€“ itâ€™s dominating European democracy.Europe needs a thriving and diverse digital economy that serves the needs of European citizens, not billionaire tech CEOs. President von der Leyen has affirmed that in the EU, â€œwe donâ€™t have bros or oligarchs making the rules.â€ The Commission must now stand up to the tech oligarchy by strongly enforcing the EUâ€™s digital rules and competition law.Right now, the Commission has a once-in-a-generation opportunity to dismantle Googleâ€™s advertising monopoly, which is destroying the news media, ripping off consumers, and was ruled illegal in a landmark US judgement.Big Techâ€™s monopoly power threatens democracyWe cannot address Big Techâ€™s harms without first confronting its power. A handful of tech giants have concentrated control of our core digital infrastructure â€“ including search engines, social media, app stores, and cloud. The companiesâ€™ unchecked power over their digital empires enables them to abuse peopleâ€™s rights, exploit businesses, and crush competitors.Spanish Prime Minister Pedro Sanchez has warned that tech billionaires want â€œâ€. When a small number of billionaires and tech giants control the internet, they wield their power â€“ and their vast profits â€“ to influence political discourse and interfere with democratic laws. This year, tech CEOs and the Trump administration have  to try to thwart the EUâ€™s landmark digital laws that hold Big Tech to account.Break up Big Tech monopoliesTeresa Ribera, the EUâ€™s competition chief,  that break-ups can prevent Big Tech from grabbing too much market power. These corporations treat billion-euro fines as the cost of doing business, while behavioural remedies are ineffective and often flouted by the companies. Forcing these giants to sell off parts of their businesses will curb conflicts of interest, level the digital playing field, and make the companies easier to hold accountable for their growing societal harms.Breaking up tech monopolies is a step towards a freer, fairer internet. Europe can and must resist threats from Big Tech and the Trump administration, and stand firm in upholding EU law against Big Tech. Break up Google. Break up Big Tech.Foundation The London StoryXnet, Institute for Democratic Digitalisation â€“ SpainCorporate Europe Observatory (CEO)Enforce (Irish Council for Civil Liberties)Save Social - Networks for democracyCanadian Anti-Monopoly Project (CAMP)Hope and Courage Collective, IrelandGerman NGO Forum on Environment & DevelopmentAnother Europe Is PossibleEpicenter.works - for digital rightsEuropean Federation of Journalists (EFJ)Deutscher Journalisten-Verband (DJV, German Journalists Association)Global Project Against Hate and Extremism]]></content:encoded></item><item><title>Hurl: Run and test HTTP requests with plain text</title><link>https://github.com/Orange-OpenSource/hurl</link><author>flykespice</author><category>hn</category><pubDate>Fri, 20 Jun 2025 03:55:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Tool to Automatically Create Organized Commits for PRs</title><link>https://github.com/edverma/git-smart-squash</link><author>edverma2</author><category>dev</category><category>hn</category><pubDate>Fri, 20 Jun 2025 03:22:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I've found it helps PR reviewers when they can look through a set of commits with clear messages and logically organized changes. Typically reviewers prefer a larger quantity of smaller changes versus a smaller quantity of larger changes. Sometimes it gets really messy to break up a change into sufficiently small PRs, so thoughtful commits are a great way of further subdividing changes in PRs. It can be pretty time consuming to do this though, so this tool automates the process with the help of AI.The tool sends the diff of your git branch against a base branch to an LLM provider. The LLM provider responds with a set of suggested commits with sensible commit messages, change groupings, and descriptions. When you explicitly accept the proposed changes, the tool re-writes the commit history on your branch to match the LLM's suggestion. Then you can force push your branch to your remote to make it match.The default AI provider is your locally running Ollama server. Cloud providers can be explicitly configured via CLI argument or in a config file, but keeping local models as the default helps to protect against unintentional data sharing. The tool always creates a backup branch in case you need to easily revert in case of changing your mind or an error in commit re-writing. Note that re-writing commit history to a remote branch requires a force push, which is something your team/org will need to be ok with. As long as you are working on a feature branch this is usually fine, but it's always worth checking if you are not sure.]]></content:encoded></item><item><title>Show HN: Ts-SSH â€“ SSH over Tailscale without running the daemon</title><link>https://github.com/derekg/ts-ssh</link><author>i8code</author><category>dev</category><category>hn</category><pubDate>Fri, 20 Jun 2025 03:03:05 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[ts-ssh solves a specific problem: accessing machines on your Tailnet from
  environments where you can't install the full Tailscale daemon (like CI/CD runners or
   restricted systems).  It uses Tailscale's tsnet library to establish userspace connectivity, then provides
  a standard SSH experience. Works with existing workflows since it supports normal SSH
   features like ProxyCommand, key auth, and terminal handling.

  Some features that proved useful:
  â€¢ Parallel command execution across multiple hosts
  â€¢ Built-in tmux session management for multi-host work
  â€¢ SCP-style file transfers
  â€¢ Works on Linux/macOS/Windows (AMD64 and ARM64)

  The codebase is interesting from a development perspective - it was written almost
  entirely using AI tools (mainly Claude Code, with some OpenAI and Jules). Not as an
  experiment, but because it actually worked well for this kind of systems programming.
   Happy to discuss the workflow if anyone's curious about that aspect.

  Source and binaries are on GitHub. Would appreciate feedback from anyone dealing with
   similar connectivity challenges.]]></content:encoded></item><item><title>Asterinas: A new Linux-compatible kernel project</title><link>https://lwn.net/SubscriberLink/1022920/ad60263cd13c8a13/</link><author>howtofly</author><category>hn</category><pubDate>Fri, 20 Jun 2025 01:56:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!
This article was contributed by Ronja KoistinenAsterinas is a new
Linux-ABI-compatible kernel project written in Rust, based on what the
authors call a "framekernel architecture".  The project overlaps somewhat
with the goals of the Rust for Linux
project, but approaches the problem space from a different direction by
trying to get the best from both monolithic and microkernel designs.


Traditionally, monolithic kernels lump everything into one kernel-mode
address space, whereas microkernels only implement a minimal trusted
computing base (TCB) in kernel space and rely on user-mode services for
much of the operating system's functionality.  This separation implies the
use of interprocess communication (IPC) between the microkernel and those
services. This IPC often has a performance impact, which is a big part of
why microkernels have remained relatively unpopular.


The core of Asterinas's "framekernel" design is the encapsulation of all
code that needs Rust's  features inside a library, enabling
the rest of the kernel (the services) to be developed using safe
abstractions.  Those services remain within the kernel's address space, but
only have access to the resources that the core library gives to them.
This design is meant to improve the safety of the system while retaining
the simple and performant shared-memory architecture of monolithic
kernels. The Asterinas book
on the project's website provides a nice 
architectural mission statement and overview.



The aptness of the "framekernel" nomenclature can perhaps be debated.  The
frame part refers to the development framework wrapping the unsafe
parts behind a memory-safe API.  The concept of the TCB is, of
course, not exclusive to microkernel architectures but, because there are
strong incentives to strictly scrutinize and, in some contexts, even formally
verify the TCB of a system, keeping the TCB as small as possible is a
central aspect of microkernel designs.



An update on the project is available on the Asterinas blog in the
JuneÂ 4 post titled "Kernel
Memory Safety: Mission Accomplished".  The post explains the team's
motivations and the need for the industry to address memory-safety
problems; it provides some illustrations that explain how the framekernel
is different from monolithic kernels and microkernels. It also takes a
moment to emphasize that the benefits of Rust don't stop with memory
safety; there are improvements to soundness as well.
Perhaps most importantly, the post highlights the upcoming Asterinas
presentation at the 2025
USENIX Annual Technical Conference.

In their paper, the authors compare Asterinas to some prior Rust-based
operating-system work, exploring the benefits of the language's
memory-safety features and explain how Asterinas differs from that previous
work.  Specifically, the paper contrasts Asterinas with 
RedLeaf, an operating system written in Rust and presented at the 14th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)
in 2020.  Asterinas uses hardware isolation to permit running user-space
programs written in any programming language, aims to be general-purpose,
and provides a Linux-compatible ABI, while RedLeaf is a microkernel that is
designed  to use the hardware's isolation features, and the
project focuses on different things.

Another project of interest is Tock, an
embedded system that targets SoCs with limited hardware protection
functionality. Like Asterinas, Tock also divides the kernel into a
trusted core allowed to use  and untrusted "capsules" that
are not.  As mentioned, Asterinas does rely on hardware protection and
isn't intended for strictly embedded use, which differentiates it from
Tock.



It bears mentioning that the Rust for Linux project, which is introducing
Rust code into the upstream Linux kernel, has similar goals as
Asterinas. It also aims to encapsulate kernel interfaces with safe
abstractions in such a way that drivers can be written in Rust without any
need for .


Work toward formal verification
One goal of shrinking the TCB of an operating system is to make it feasible
to have it formally verified.  In February 2025, the Asterinas blog
featured a
post detailing plans to do just that.  The best known formally verified
kernel is seL4, an L4-family
microkernel.


Asterinas aims to use the framekernel approach to achieve a system that has
a small, formally verified TCB akin to a lean microkernel, but also a
simple shared-memory architecture with Linux ABI compatibility, all at the
same time.  This is a radical departure from any previously formally
verified kernel; the blog post describes those kernels as deliberately
small and limited compared to "full-fledged, UNIX-style OSes".



The Asterinas project is collaborating with a security-auditing company
called CertiK to use Verus to formally verify the
kernel.  There is an extensive 
report available from CertiK on how Asterinas was audited and the
issues that were found.



The Asterinas kernel is only one result of the project. The other two are
OSTD, described as "a Rust
OS framework that facilitates the development of and innovation in OS
kernels written in Rust", and OSDK, a
Cargo addon to assist with the development, building, and testing of
kernels based on OSTD.



There are four stated goals for OSTD as a separate crate. One is to lower
the entry bar for operating-system innovation and to lay the groundwork for
newcomers to operating-system development. The second is to enhance memory
safety for operating systems written in Rust; other projects can benefit
from its encapsulation and abstraction of low-level operations. The third is
to promote code reuse across Rust-based operating-system projects. The
fourth is to boost productivity by enabling testing of new code in user
mode, allowing developers to iterate without having to reboot.



It is worth emphasizing that the kernels that can be written with OSTD do
not have to be Linux-compatible or, in any way, Unix-like. The APIs
provided are more generic than that; they are memory-safe abstractions for
functionality like x86 hardware management, booting, virtual memory, SMP,
tasks, users, and timers.  Like most Rust crates, OSTD is documented on
docs.rs.



Asterinas reports Intel, among others, as a sponsor of the project.
Intel's interest is likely related to its Trust
Domain Extensions (TDX) feature, which provides hardware modes and
features to facilitate isolation of virtual machines, and memory
encryption.  The Asterinas book has a brief section
on TDX, and the OSDK supports it.



The OSTD, or at least the parts that Asterinas ends up using, seems to
essentially be the restricted TCB that allows . For an
illustrative example, we could take a look at the  kernel
component's source
code and see that the buffer code uses DMA, locking, allocation, and
virtual-memory code from the OSTD through memory-safe APIs.



Asterinas was first released under the Mozilla Public License in early
2024; it has undergone rapid development over the past year.  GitHub lists 45
individual committers, but the majority of the commits are from a
handful of PhD students from the Southern University of Science and
Technology, Peking University, and Fudan University, as well as a Chinese
company called Ant Group, which
is a sponsor of Asterinas.


At the time of writing, Asterinas supports two architectures, x86 and RISC-V.
In the January blog post linked above, it was reported that Asterinas
supported 180 Linux system calls, but the number has since grown to 206
on x86.  As of version 6.7, Linux has 368 system calls in total, so there is
some way to go yet.



Overall, Asterinas is in early development. There have been no releases,
release announcements, changelogs, or much of anything other than Git tags
and a short installation guide in the documentation.  The Dependents
tab of the OSTD crate on crates.io shows that no unrelated, published
crate yet uses OSTD.



It does not seem like Asterinas is able to run any applications yet.  Issue #1868
in Asterinas's repository outlines preliminary plans toward a first
distribution.  The initial focus on a custom initramfs and some rudimentary
user-space applications, followed by being able to run
Docker. There are initial plans to bootstrap a distribution based on
Nix. Notably (but unsurprisingly), this issue mentions that Asterinas
doesn't support loading Linux kernel modules, nor does it ever
plan to.



The Roadmap
section of the Asterinas book says that the near-term goals are to expand
the support for CPU architectures and hardware, as well as to focus on
real-world usability in the cloud by providing a host OS for virtual
machines.  Apparently, the support for Linux virtio devices is already
there, so a major hurdle has already been cleared.  In particular, the
Chinese cloud market, in the form of Aliyun (also known as Alibaba Cloud)
is a
focus.  The primary plans involve creating a container host OS with a
tight, formally verified TCB and support for some trusted-computing
features in Intel hardware, for the Chinese cloud service.



While both Rust for Linux and Asterinas have similar goals (providing a
safer kernel by relying on Rust's memory safety), their scopes and
approaches are different.  Rust for Linux focuses on safe abstractions
strictly for new device drivers to be written in safe Rust, but this leaves
the rest of the kernel untouched.
Asterinas, on the other hand, aims to build a whole new kernel from the ground
up, restricting the -permitting core to the absolute minimum,
which can then be formally verified.  Asterinas also focuses on
containers and cloud computing, at least for now, while Rust for Linux looks to
benefit the whole of the Linux ecosystem.



Despite the stated cloud focus, there is more going on, for example building
support for X11
and Xfce.
Also, the OSTD could, of course, prove interesting for OS development
enthusiasts irrespective of the Asterinas project, but so far it remains unknown
and untested by a wider audience.


Asterinas is certainly a refreshingly innovative take on principles for
operating-system development, leaning on the safety and soundness
foundations provided by the Rust language and compiler. So far it is at an
early exploratory stage driven by enthusiastic Chinese researchers and
doesn't see any serious practical use, but it is worth keeping an eye
on. It will be interesting to see the reception it will get from the
Rust for Linux team and the Linux community at large.]]></content:encoded></item><item><title>FedFlix â€” Public Domain Stock Footage Library</title><link>https://public.resource.org/ntis.gov/index.html</link><author>bookofjoe</author><category>hn</category><pubDate>Fri, 20 Jun 2025 01:07:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
							Public.Resource.Org, Inc. 
							
							1005 Gravenstein Hwy North 
							
							Sebastopol CA 95472 
						
							Associate Director 
							
							Product and Program Management 
							
							NTIS 
						]]></content:encoded></item><item><title>Open source can&apos;t coordinate?</title><link>https://matklad.github.io/2025/05/20/open-source-cant-coordinate.html</link><author>LorenDB</author><category>hn</category><pubDate>Fri, 20 Jun 2025 01:06:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Infinite Mac OS X</title><link>https://blog.persistent.info/2025/03/infinite-mac-os-x.html</link><author>kristianp</author><category>hn</category><pubDate>Fri, 20 Jun 2025 00:16:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ Infinite Mac can now run early Mac OS X, with 10.1 and 10.3 being the best supported versions. Itâ€™s not particularly snappy, but as someone who lived through that period, I can tell you that it wasnâ€™t much better on real hardware. Infinite HD has also been rebuilt to have some notable indie software from that era.Iâ€™ve been tracking DingusPPC progress since my initial port and making the occasional contribution myself, with the hope of using it to run Mac OS X in Infinite Mac. While it has continued to improve, I reached a plateau last summer; my attempts would result in either kernel panics or graphical corruption. I tried to reduce the problem a bit via a deterministic execution mode, but it wasnâ€™t really clear where to go next. I decided to take a break from this emulator and explore alternate paths of getting Mac OS X to run.PearPC was the obvious choice â€“ it was created with the express purpose of emulating Mac OS X on x86 Windows and Linux machines in the early 2000s. By all accounts, it did this successfully for a few years, until interest waned after the Intel switch (sadly one of the authors passed away around then). I had earlier dismissed it as a â€œdeadâ€ codebase, but I decided that the satisfaction of getting something working compensated for dealing with legacy C++ (complete with its own string class, sprintf implementation, and GIF decoder). An encouraging discovery was that kanjitalk755 (the de-facto Basilisk II and SheepShaver maintainer) had somewhat recently set up an experimental branch of PearPC that built and ran on modern macOS. I was able to replicate their work without too much trouble, and with that existence proof I started on my sixth port of an emulator to WebAssembly/Emscripten and the Infinite Mac runtime.In some ways PearPC not being actively developed made things easier â€“Â I didnâ€™t have to worry about merging in changes from upstream, or agonize over how to structure my modifications to make them easier to contribute back. It was also helpful that PearPC was already a multi-platform codebase and thus had the right layers of abstraction to make adding another target pretty easy. As a bonus, it didnâ€™t make pervasive use of threads or other harder-to-port concepts. Over the course of a few days, I was able to get it to build, output video, load disk images, and get mouse and keyboard input hooked up. It was pretty satisfying to have Mac OS X 10.2 running in a browser more reliably than it previously had.Performance is still not as good as DingusPPCâ€™s â€“ the biggest bottleneck is the lack of any kind of caching in the MMU, so all loads and stores are expensive since they involve complex address computations. DingusPPC has a much more mature tiered cache that appears to be quite effective. More generally, while PearPC may be more stable than DingusPPC at running 10.2-10.4, itâ€™s a much less principled codebase (I came across many mystery commits) and it â€œcheatsâ€ in many ways (it has a custom firmware and video driver, and only the subset of PowerPC instructions that are needed for Mac OS X are implemented). Iâ€™m still holding out hope for DingusPPC to be the fast, stable, and correct choice for the long term.I implemented the â€œunified decoding tableâ€ approach in PearPCâ€™s interpreter one opcode family at a time. When I got to the floating point operations, I assumed it was going to be another mechanical change. I was instead surprised to see that behavior regressed â€“ I got some rendering glitches in the Dock, and the Finder windows would not open at all. After some debugging, I noticed that the dispatching for opcode groups 59 and 63 didnâ€™t just do a basic lookup on the relevant instruction bits. It first checked the  bit of the Machine State Register (MSR), and if it was not set it would throw a â€œfloating point unavailableâ€ exception.I initially thought this was the emulator being pedantic â€“ all PowerPC chips used in Macs had an FPU, so this should never happen. However, setting a breakpoint showed that the exception was being hit pretty frequently during Mac OS X startup. The xnu kernel sources of that time period are available, and though Iâ€™m not familiar with the details, there are places where the FP bit iscleared and a handler for the resulting exception is registered. I assume this is an optimization to avoid having to save/restore FPU registers during context switches (if theyâ€™re not being used). The upshot was that once I implemented the equivalent  check in my optimized dispatch code, the rendering problems went away.This reminded me of the rendering glitches that I had encountered when trying to run Mac OS X under DingusPPC. Even when booting from the 10.2 install CD (which does not kernel panic) I would end up with missing text and other issues:Checking the DingusPPC sources showed that it never checked the  bit, and always allowed floating point instructions to go through. I did a quick hack to check it and raise an exception if needed, and the glitches went away!The proper implementation was a bit more complicated, and I ended up revising it a bit to avoid a performance hit (and another contributor did another pass). But at the end of it all, DingusPPC became a lot more stable, which was a nice side effect. Better yet, it can run 10.1 reliably, which PearPC cannot. I ended up using a combination of both emulators to run a broader subset of early Mac OS X (unfortunately 10.0 is still unstable, and the Public Beta kernel panics immediately, but Iâ€™m holding out hope for the future).Part of the appeal of Infinite Mac is that the emulated machines also have an â€œInfinite HDâ€ mounted with a lot of era-appropriate software to try. With Mac OS X running, it was time to build an alternate version that went beyond the 80s and 90s classic Mac apps I had collected. I had my favorites, but I also put out a call for suggestions and got plenty of ideas.For actually building the disk image, I extended the automated approach that I first launched the site with. Disk images were even more popular in the early days of Mac OS X than they are today, so I added a way to import .dmgs as additional folders in the generated image. However, I quickly discovered that despite having the same extension, there are many variants, and the  that ships with modern macOS cannot always mount images generated more than 20 years ago. In the end I ended up with a Rube Goldberg approach that first extracts the raw partition via dmg2img and then recreates a â€œmodernâ€ disk image that can be mounted and copied from.As for getting the actual software, the usual sites like Macintosh Garden do have some from that era, but itâ€™s not a priority for them. Early to mid 2000s Mac OS X software appears to be a bit of a blind spot â€“Â itâ€™s too new to be truly â€œretroâ€, but too old to still be available from the original vendor (thoughthereareexceptions). I ended up using the Wayback Machine a lot. As a bonus, I also installed the companion â€œDeveloperâ€ CDs for each Mac OS X version, so tools like Project Builder and Interface Builder are also accessible.The only limitation that I ran into is that my disk build process is centered around HFS, but HFS+ was the default of that time period, and it introduced more advanced capabilities like longer file names containing arbitrary Unicode characters. Files from disk images that rely HFS+ features do not translate losslessly, but luckily this was not an issue for most software. To actually mount multiple drives (up to 3, between the boot disk, Infinite HD, and Saved HD), I endedupborrowing a clever solution from a DingusPPC fork: a multi-partition disk image is created on the fly from an arbitrary number of partition images that are specified at startup.To make the addition of Mac OS X to Infinite Mac complete, I also wanted to have an Aqua mode for the siteâ€™s controls, joining the classic, Platinum, and NeXT appearances. That prompted the question: which Aqua?Though the more subdued versions from 10.3 and 10.4 are my favorites, I decided to go with the 10.0/10.1 one since it has the biggest nostalgia factor. I wanted to use the exact same image assets as the OS, and since they make heavy use of semi-transparency, regular screenshots were not going to be good enough. I used resource_dasm and pxm2tga to extract the original assets from Extras.rsrc and create my own version of Aqua:If the recent rumors of a big UI revamp do come true, itâ€™ll be nice to have this reference point of its ancestor.The ability to mount multiple images means that you can also have a Mac OS 9 partition and start the Classic compatibility environment (this only works under 10.1 â€“ PearPC never supported Classic). You can thus emulate classic Mac apps inside an emulated Mac OS X inside a WebAssembly virtual machine:There was a recent storm in a teacup about a Calculator behavior change. Using these Mac OS X images, itâ€™s possible to verify that versions through 10.3 didnâ€™t have the â€œrepeatedly press equalsâ€ behavior, but 10.4 did.Though Iâ€™ve moved away from custom domain names, I thought macosx.app would make a nice additiontomycollection. Unfortunately itâ€™s taken, though in a rather weird way. I even contacted the YouTuber whose video it redirects to, and he said he was not the one that registered it. It expires in a couple of months, so maybe Iâ€™ll be able to grab it.â€œWhen Alexander saw the breadth of his domain, he wept for there were no more worlds to conquer.â€
â€” Some FrenchmanMac OS X support catches Infinite Mac up to the modern day, unless I happen to get access to some time travel mechanics. There are of course two more CPU transitions to go through and numerous small changes, but Tiger is fundamentally recognizable to any current-day macOS user.Except that in the retrocomputing world, itâ€™s always possible to go deeper or more obscure. A/UX is not something that Iâ€™m very familiar with, but it was a contemporary of classic Mac OS and would be interesting to compare to NeXTStep. Shoebill runs it, and the codebase looks approachable enough to port. Then thereâ€™s Lisa, the Pippin (DingusPPC has some nascent support), and further afield the Newton (via Einstein?). Weâ€™ll see what moves me next.When I first began exploring ways of running Mac OS X, I mentioned that QEMU seemed too daunting to port to WebAssembly given my limited time. Furthermore, the performance of the qemu.js experiment from a few years ago made it seem like even if it did run, it would be much too slow to be usable. However, I recently became aware of qemu-wasm via this FOSDEM presentation. The performance of its Linux guest demos is encouraging: I ran an impromptu bennmark of computing an MD5 checksum of 100 MB of data and it completed it in 8 seconds (vs. 13 for DingusPPC and 18 for PearPC). Thereâ€™s still a big gap between that and a graphical guest like Mac OS X, but itâ€™s nice to have this existence proof.]]></content:encoded></item><item><title>Giant, all-seeing telescope is set to revolutionize astronomy</title><link>https://www.science.org/content/article/giant-all-seeing-telescope-set-revolutionize-astronomy</link><author>gammarator</author><category>hn</category><pubDate>Thu, 19 Jun 2025 23:17:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I wrote a new BitTorrent tracker in Elixir</title><link>https://github.com/Dahrkael/ExTracker</link><author>dahrkael</author><category>dev</category><category>hn</category><pubDate>Thu, 19 Jun 2025 22:49:49 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I'm currently in a journey to learn and improve my Elixir and Go skills (my daily job uses C++) and looking through my backlog for projects to take on I decided Elixir is the perfect language to write a highly-parallel BitTorrent tracker.
So I have spent my free time these last 3 months writing one! Now I think it has enough features to present it to the world (and a docker image to give it a quick try).I know some people see trackers as relics of the past now that DHT and PEX are common but I think they still serve a purpose in today's Internet (purely talking about public trackers). That said there is not a lot going on in terms of new developments since everyone just throws opentracker in a vps a calls it a day (honorable exceptions: aquatic and torrust).I plan to continue development for the foreseeable future and add some (optional) esoteric features along the way so if anyone currently operates a tracker please give a try and enjoy the lack of crashes.note: only swarm_printout.ex has been vibe coded, the rest has all been written by hand.]]></content:encoded></item><item><title>Literate programming tool for any language</title><link>https://github.com/zyedidia/Literate</link><author>LorenDB</author><category>hn</category><pubDate>Thu, 19 Jun 2025 22:18:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Estrogen: A Trip Report</title><link>https://smoothbrains.net/posts/2025-06-15-estrogen.html</link><author>sebg</author><category>hn</category><pubDate>Thu, 19 Jun 2025 20:15:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I have gender dysphoria. I find labels overly reifying; I feel reluctant to call myself , per se: when prompted to state my gender identity or preferred pronouns, I fold my hands into the  and state that I . Mostly people seem to vibe it, but sometimes it feels a little like weasel words. Other times, when Iâ€™m in a sillier mood, Iâ€™ll tell people Iâ€™m  â€“ if only because it sounds like something Iâ€™d put in my station wagon. Of course, my faithful Subaru Outback was made before 2008, which means it wants the green, long-life genderfluidâ€¦I experience an ongoing brain-body mapprediction error â€“ my brain seems to expect a differently shaped body to the one I wound up with. I have been acutely aware of this since before I hit puberty. Out of shame and embarassment, I suppressed this, but I also made a promise to myself that if I hadnâ€™t come out by the time I turned thirty then I was allowed to get as weird as I needed to.During the COVID-19 pandemic I went through a phase of using self-administered ketamine therapy to refactor a long list of maladaptive behavioural patterns, and eventually this particular issue became impossible to ignore. I had avoided reifying it for long enough, and this wasnâ€™t working for me â€“ I had to try something different. One evening in July 2021, I sat down with a close friend. I am going to put a large amount of ketamine up my nose, I said. Your job is to start asking me questions about my sexuality.Not long after, I had jumped through the relevant bureaucratic hoops,  and subsequently found myself cycling home from the pharmacy with a paper bag filled with repurposed menopause medication â€“ a starter pack of  estradiol patches, to be applied twice a week.While the  effects of estrogen are well-documented, back when I came out I had difficulty finding detailed phenomenological reports of the  effects of estrogen. I did wind up reading a large number of anecdotal reports on Reddit, and found that in aggregate, people tend to report positive subjective effects. One could propose a number of non-exclusive hypotheses as to why â€“ Iâ€™ll attempt to review these later in this post.Did it make sense for me to try this? It was time to find out for myself. I unboxed the patches and placed one on my stomach.Estrogen receptors are located throughout the body. Of these, there are two main types â€“ ERÎ± and ERÎ². These have similar binding affinities for estradiol, but are expressed in different proportions in different bodily tissues, and can have different effects on gene regulation.Estradiol is a steroid hormone that influences the serotonergic, dopaminergic, and glutamatergic systems. Estradiol exerts its effects through classical mechanisms by binding to nuclear estrogen receptors Î±, and Î², or through nonclassical mechanisms through binding to membrane bound estrogen receptors Î±, Î², and GPER.The effects are so wide-ranging that any review I can write will no doubt oversimplify things. That said, Iâ€™d like to highlight two findings relevant to neurotransmitter levels: synthesis is upregulated by ERÎ± and downregulated by ERÎ² via tyrosine hydroxylase transcription. Potentially, they work in tandem to maintain homeostatic levels, but ERÎ± has greater influence at higher estradiol levels.These neurotransmitters are, of course, stereotypically associated with  and .Evidence from neuroimaging findings to link estrogen and the
serotonergic system in humans are still relatively sparse. Animal
data support ovariectomy to decrease 5-HT1 binding, 5-HT2A binding and expression, and 5-HT transporter binding sites and expression. These findings have
been shown to be reversible with estrogen replacement therapy.This process involves several steps: First, researchers remove the ovaries from female rats, and then divide them into two groups â€“ one receiving estrogen treatment and the other serving as a control. Next, finely sliced brain samples are taken from both groups and exposed to a radioactive ligand, which binds to the receptor of interest. Finally, these radioactive samples are used to create images on radiosensitive film, which is then developed and analysed.This is not the kind of procedure we generally perform on humans. Additionally, these preclinical rat model studies concern ovarectomized female rats and are intended to inform treatment programmes for postmenopausal human women â€“ so their relevance to humans starting from an androgenic baseline is possibly somewhat limited. Still, thereâ€™s a coupleof these studies Iâ€™d like to highlight, which found estrogen caused:That said, there  exist a coupleof studies assessing the influence of estrogen on 5-HT2A receptor binding in humans, using a radioactive ligand and positron emission tomography. In both studies, five postmenopausal women were assessed both before and after hormone replacement therapy, and both found estrogen increased 5-HT2A receptor binding in prefrontal regions. The resolution is pretty low, but see for yourself:Alright, so what are these  and  receptors responsible for? These are glutamate and serotonin receptors, respectively â€“ and these are also the specific receptors that are  by ketamine and  by most serotonergic psychedelics. If recreational drugs targeting these receptors can engender euphoric subjective effects â€“ what might estrogen be capable of?The subjective perceptual and psychological effects of estrogen are wide-ranging and subtle. Iâ€™ll start by discussing the more mundane sensory changes I experienced before moving on to those which might be more nebulous or ineffable.At the time of writing, Iâ€™ve been on and off estrogen for a period of nearly three years. My initial dosage was one of the estradiol patches, but I doubled this after a short while. I have also tried estradiol valerate pills, twice or three times daily â€“ though this turned out to be too low, and I wound up switching back to patches. I have found using patches to result in the most striking and noticeable subjective effects. I have not yet tried injected estrogen, though I anticipate doing so before long.Additionally, at one point I tried taking a progesterone suppository. This made me feel quite stupid the following day, so I did not try this again.Iâ€™ve long been in the bad habit of rolling out of bed and grabbing a Monster Zero straight from the fridge first thing in the morning, and I tend to follow this up with Diet Coke throughout the day. This means that Iâ€™m fairly attuned to the taste of artificial sweeteners, so naturally the change in taste perception was the first thing I noticed â€“ within a day or two of first putting the patches on, I found that  things tasted ; and  things tasted both  and more  â€“ and the cinnamon taste in my standard reference Diet Coke really .This was rather exciting; I was not expecting to find that the primary tastes were not in fact primal, but in fact could shift around inside a lower-dimensional latent space. This got me theorising â€“ as I wrote elsewhere:Perhaps taste could be built out of something like  vibrations, tuned by evolution towards consonance or dissonance in order to generate an attractive or aversive response in the organism?It took me a little while before I noticed any change to my sense of smell, but this was more a factor of encountering the relevant stimulus. It was boys. Boys smelt different.Much earlier in life, Iâ€™d had to convince myself I was gay by using the fact that boys smelt . This was very much no longer the case, and I began to notice wide variation in the way boys smelt, which sometimes was really quite unpleasant â€“ , even.I have somatic sensory issues. Skin sensations have always been overwhelming â€“ my mother will confirm that I would scream if she attempted to dress me in wool, and in adulthood I avoid buying clothing with sleeves. By default, my skin feels like a bag of white noise â€“ and when things get bad it can feel like my whole body is covered with randomised pinprick sensations, like minuscule topological defects in the somatic field.This has interfered with my ability to experience intimacy; simply lying in bed with somebody could be a stressful time for me. Estrogen ramps all of this way down in intensity â€“ itâ€™s a tremendous relief.Perhaps my cleaning habits can provide an objective measure. Because things like sweat on my skin or leftover food in my mouth constitute intolerable sensory distractions, Iâ€™d tend to shower up to four times a day and brush my teeth about as often â€“ since starting estrogen, Iâ€™m much less neurotic about both of these things.A less turbulent nervous system also seems to be less disruptive for sleep. Beforehand, I took it for granted that I would often wake up throughout the night in a state of discomfort, whereas while I am on estrogen I reliably wake up in the morning feeling well-rested.This oneâ€™s quite subtle â€“ it was the kind of thing that was more noticeable when I experimented with deliberately spiking my hormones. Iâ€™ll do my best to explain. Itâ€™s as if I took the entire volumetric representation of the space around me and increased the degree to which every point within that could influence the location of every other point, recursively. This allows everything to elastically settle into a more harmonious equilibrium. This effect is basically identical to what a small dose of psychedelics can do, specifically a tryptamine like psilocybin or DMT.Itâ€™s hard to say what the utility of this might be. The balance between entropy and harmony is an important one â€“ too much entropy and itâ€™s hard to tell signal from noise, and too much harmony and you might miss important details. I did feel that with a more parsimonious model of the space around me, I got better at driving â€“ though my friends would say I got more  at driving. Competence might be orthogonal to confidence, but I maintain that parallel parking is much easier now.I ride my bicycle every morning â€“ this is my primary meditative practice. I am also surrounded by steep hills, so I noticed within a couple of days that I could not activate my quads and hamstrings as hard as I was used to. This happened much faster than could possibly be accounted for by muscular atrophy, so I surmised that this must be a neuromuscular phenomenon. Later on I switched back to my own hormones for a short period, and once again the change was quite rapid. There was nothing quite like the rush of  uphill once again.Being less strong honestly sucked pretty bad, and this required some psychological adjustment.  The flipside of this was that I found estrogen to be a  muscle relaxant, and ultimately this made the effect a net positive.Around the time I transitioned was also the period when I was exploring some quite extreme ketamine-assisted myofascial release techniques in order to shake off a lifetimeâ€™s worth of accumulated tension from things like bad ergonomics and social anxiety. Iâ€™d say estrogen has been partially instrumental in getting me from a place where Iâ€™m constantly attacking myself with a foam roller and massage gun just to feel comfortable in my own body â€“ to one where massage is more of a light maintenance task, like a bird preening its feathers.I have spent a big chunk of my life navigating chronic emotional disaffection â€“ high school sucked, and later I had an acute week-long dissociative episode when I was twenty-one which Iâ€™m not sure I ever quite came back from. Suffice it to say I lived an emotionally stagnant existence for most of my twenties â€“ so when the hormones opened things up, I got quite attached to my new feelings.Funny things were  â€“ I recall a moment about a week after I started the hormone treatment, when I laughed at something I saw on YouTube â€“ the surge of joy was like an electric cauteriser through my breastbone. Music  now. I can lean in to the sense of affection I feel towards my friends. I cry more frequently; but this is clearly critical for releasing tension that would otherwise remain below the surface.Thereâ€™s another side to all this. I have had to navigate a number of situations where I now found myself unable to dissociate from some issue in my life that had been bothering me â€“ I  to do something. Often this felt destructive; in retrospect thereâ€™s things I could have handled with far more grace and care, but instead I chose to drive a bulldozer through them.For better or worse, this is what the hormones can do. Itâ€™s a bit of an epistemic nightmare â€“ do I take action to deal with the thing thatâ€™s bugging me, or would it be better to skip my hormones for a day or two and see if I consider things differently? I can only recommend entering into this groundless game of instrumental hormone manipulation if one is comfortable taking responsibility for epistemic frame-shifting.Iâ€™d engaged with a number of deliberate psychological interventions in the lead-up to coming out, with the general aim of managing my social self-awareness. I knew I needed the confidence. If I was to socially transition, Iâ€™d need to not get too overwhelmed or hung up on what other people thought of me.Hereâ€™s how I usually explain it to people: You have , which corresponds to everything currently in your sensorium. Then you have , which is a subset of that â€“ like the beam of a spotlight â€“ and most importantly, you have  over it, you can choose where to point it and how wide or narrow you would like it to be.Sometimes we might feel that our attention is involuntarily yanked around by invisible aversive forces that are seemingly beyond our control â€“ for instance, I might find it challenging to make eye contact with people at a party, and spend the whole time with my attention collapsed and pointed at the floor.I think what Alexander Technique does is teach mindfulness of this class of phenomena and how to  attention out of them. Prior to transition, I deliberately experimented with this form of attentional modulation â€“ primarily in the kind of social setting that I would normally find overwhelming, but also just while riding my bicycle outside. I think this kind of practice has a lot of potential for helping undo the archetypal trauma-induced behavioural patterns displayed by socially anxious autistic people. Personally I found it to be remarkably effective, and after some months of this I felt that my anxiety disorder was mostly in remission.So it was a humungous letdown when I found that all of this got  on estrogen. I cringed my way through social events, and returned to staring at the floor â€“ but I didnâ€™t let this stop me. I thought of it like Goku training in the one hundred times Earth gravity chamber. I just learned it all again from scratch.If someone feels that theyâ€™d rather have a feminine body, estrogen is going to satisfy this desire â€“ . This is obvious. Iâ€™m more interested in finer-grained,  rather than  sensory phenomena. What are the other reasons that estrogen might ? Iâ€™d like to propose a number of theories â€“ Iâ€™ll try to order them from least speculative to most speculative.There was no point where I didnâ€™t feel somehow removed from the world around me â€“ this disconcerting sensation was present from my earliest memories. As a child I just didnâ€™t really see the point of practically anything I was doing, or that anyone else was doing; it held no real emotional resonance or meaning for me. Whatever interests I chose to pursue felt more like an obligatory way of filling time, not something that had any value or importance in its own right.I always felt the lack of spontaneity characteristic of depersonalization disorder, and whenever I chose to say anything, it felt rehearsed and acted out as if I had to engage my every word and action manually. Most of the time I would choose to say nothing at all. My feelings seemed to be kept at a distance, happening as something separate from an interior â€œmeâ€ who didnâ€™t truly experience these emotions and seemingly couldnâ€™t be touched by them. I was painfully conscious of all of these things.She continues with a visual description which I found particularly fascinating:In sufferers of depersonalization, symptoms can become more prominent in the form of sudden attacks â€“ and it gets worse the more you keep thinking about it. Later that night, I step outside to get some air, and the thought enters my mind that the trees, cars, and houses on our street could just be particularly elaborate Lego pieces. The clouds in the night sky could easily pass for a simple rendering in Blender. Isnâ€™t at least half of what we see practically a hallucination thatâ€™s filled in by our brain without us even noticing? If all these things were just renderings, it seems like it would be easy to take advantage of that.I can almost envision everything on our street coming apart piece by piece like an exploded technical diagram. The asphalt, the curb, the patches of grass, all of them could just lift into the air and drift apart, nothing but thin surfaces, almost like abstractions or mere representations. If I were to take a shovel and start digging a hole in the road, it would just be an indentation in that surface, pushing it to extend a bit in one direction or another â€“ but underneath it, nothing. The houses along the street are just outgrowths of the surface, a sort of puckering in it, like a ball on a rubber sheet to demonstrate how gravity is the curvature of spacetime.When I read this, I could not help but think two things:Wow, this sounds just like my life.Wow, .I related very strongly to both her description of feeling distanced from life â€“ as well as her description of the visual field being â€“ as she has written elsewhere â€“ nothing but some strange infinitesimally thin surface stretched over infinite hollowness. Both of these effects increase when I experiment with ketamine, and I also notice that both of these effects reverse when I use estrogen. In particular, the way in which estrogen alters attentional modulation also seems responsible for an increase in amodal perception, which in turn makes the visual field feel less  â€“ though I donâ€™t necessarily regard this as  or . It just is.Iâ€™d also previously read Scott Alexanderâ€™s blog post, Why Are Transgender People Immune To Optical Illusions, in which he speculates that if ketamine is an  which causes depersonalisation â€“ and if estrogen upregulates NDMA receptor expression â€“ then itâ€™s possible that changes to the NMDA receptor network could be whatâ€™s responsible for the relevant changes in phenomenology.Right now I donâ€™t have much to add beyond: wow, I think this checks out. The question remains â€“ does estrogen correct some kind of underlying NMDA receptor expression deficit, which ultimately leads to the psychological problems correlated with gender dysphoria â€“ and, how does this relate to gender dysphoria itself?Much of my personal research simply consists of reading a large number of Reddit comments. As such, I sorely wish for there to exist an equivalent paper to the DMT phenomenology one, but which scrapes transgendersupportsubreddits for subjective reports instead. However, until such time as one exists, the reader may just have to take my word for it when I claim a particular effect of estrogen is â€œcommonly reportedâ€.Yes! I was at the art museum yesterday and I became utterly infatuated with a shade of blue Iâ€™ve never seen before. Sat and stared at it for like 15 minutes.Then again, I may just be happier now.Referring back to the DMT phenomenology paper,  or  colours were reported in 25.2% of experiences. Personally, I didnâ€™t experience any shift in colour perception, but I did find the other visual perception changes I experienced to be distinctively  in nature â€“ as I mentioned earlier, particularly reminiscent of a tryptamine like psilocybin or DMT. This is especially noticeable when I deliberately spike my levels with an extra patch, and on some days I suspect I even notice a slight amount of increased symmetrical texture repetition.Estrogen is known to upregulate 5-HT2A receptor expression, which is of course the same serotonin receptor which is agonised by most serotonergic psychedelics. It seems quite reasonable to me to assume that this is whatâ€™s responsible for the various reported sensory enhancements in addition to the changes in mood.I now have an additional question. In addition to correcting some kind of NMDA receptor expression deficit inherent to the gender dysphoric neurotype, does estrogen also correct a 5-HT2A receptor expression deficit â€“ or does tripping on estrogen ?As mentioned above, I found estrogen to be an incredibly powerful muscle relaxant. Using the Bayesian brain model to understand this, it seems as if my nervous system holds  for how tense every muscle in my body should be in response to a given situation â€“ and these priors are  under the influence of estrogen.I have to credit estrogen with helping fix a number of long-standing neck and upper back problems which Iâ€™ve been dealing with for most of my life. Itâ€™s sufficiently powerful that I am skeptical that I would have been able to fix these issues while I was on testosterone. Notably, these issues donâ€™t return when I stop taking estrogen.The effects feel more foundational than this, however; estrogen feels like it reshapes my body map itself, smoothing out knots â€“ like an elastic membrane being tightened, or a soap bubble reaching equilibrium. Iâ€™ve seen it â€œcommonly reportedâ€ that estrogen makes people feel , and I suspect that this is what people might tend to mean by that.Could this be related to the serotonergic activity? Might the estrogen be unwinding a lifetime of accumulated neuromuscular trauma through a form of low-dose psychedelic therapy? I suspect this effect is also responsible for my changes in mood â€“ do emotions resonate more freely through a more parsimonious bodymind?Every circuit has its own natural density/dimensionality itâ€™s designed for, and my intuition is that organs closer to the brain are designed to have higher dimensionality. In some sense this makes them more capable of general processing, but also more prone to the particular deficits expressed in autism, with the brain as the apex of this hierarchy.Over time, civilization has thrown humanity increasingly high-dimensional challenges, leading to evolution progressively â€˜dialing the dimensionality knob upâ€™ on our nervous systems. Perhaps we can view dysfunctional autists as those who overshot the human nervous systemâ€™s current â€˜Goldilocks zoneâ€™ for dimensionality and have nervous systems dominated by static/turbulence as a result. There may be different â€˜flavorsâ€™ of autism, depending on which brain regions and tissues have elevated dimensionality.When I read this some years ago, I had something of an Iâ€™m in this picture and I donâ€™t like it moment. I donâ€™t know that his theory is necessarily true, but I certainly felt that my own sensorium was dominated by static/turbulence.As mentioned above, I found that estrogen toned down my ongoing somatic sensitivities to more manageable levels â€“ and thereâ€™s a handful of trans women Iâ€™ve spoken to who agreed with me that it turned the static down.My guess is something like joint issues â†’ poor proprioception â†’ all sensory experience is noisy and confusing â†’ the brain, which is embodied and spends most of its time trying to process sensory experience, learns a different reasoning style â†’ different reasoning style is less context-dependent (producing symptoms of autism) â†’ different reasoning style when trying to interpret bodily correlates of gender (eg sex hormones) â†’ transgender.Personally, I donâ€™t have any joint issues, and I think that his theory of dyphoria could be simpler than this. Perhaps autistic sensory sensitivities mean that the brain is constanly dealing with having to reject overly noisy sensory input, leading to a stressed out, overly tense, disembodied nervous system â€“ and this is what ultimately manifests as dysphoria? However, this would only explain , and not .It has previously been argued that autism-spectrum conditions can be understood as resulting from a predictive-processing mechanism in which an inflexibly high weight is given to sensory-prediction errors that results in overfitting their predictive models to the world. Deficits in executive functioning, theory of mind, and central coherence are all argued to flow naturally from this core underlying mechanism.The diametric model of autism and psychosis suggests a simple extension of this hypothesis. If people on the autism spectrum give an inflexibly high weight to sensory input, could it be that people with a predisposition to psychosis (i.e., people high in ) give an inflexibly  weight to sensory input?Andersen carefully describes the terms  and  as he uses them in the paper, emphasizing that these categories should be viewed as flexible and not defined by dysfunction:In this article I refer to this axis as the autism-schizotypy continuum. For convenience, I refer to people on either end of this continuum as being an â€œautistic typeâ€ or a â€œschizotypeâ€, although it should be understood that there are no clear-cut â€œtypesâ€ and that these differences are continuous rather than categorical.According to these models, everyone falls somewhere on the autismâ€“schizotypy continuum, and neither autistic-like traits nor positive schizotypy represent dysfunction. Instead, each side of the continuum is accompanied by its own set of cognitive-perceptual strengths and weaknesses. People high in autistic-like traits are detail-oriented, have a focused attentional style that allows them to ignore distractors, have some advantages in sensory-discrimination abilities, and have highly developed systemizing skills, allowing them to learn and use complicated rules-based systems.People high in positive schizotypy tend to be imaginative and creative and have a more diffuse attentional style (compared with the average person) that allows them to switch their attention more easily. There is also some evidence that people high in positive schizotypy tend to direct their attention toward highly abstract, â€œbig-pictureâ€ concerns rather than focusing on details.Andersen proposes that in the case of schizotypy, lower sensitivity to prediction errors permits sensory input to flow further up the predictive processing hierarchy, which is what results in the observed behavioural traits:In autism, inflexibly high precision weighting of sensory input means that prediction matching tends to take place at relatively low levels of the processing hierarchy. Inflexibly low precision weighting of sensory input with positive schizotypy would have the opposite effect. Because the schizotype is, on average, handling fewer sensory-prediction errors than the autistic type (because they pay attention only to the large errors and ignore the smaller ones), prediction errors will tend to propagate farther up the processing hierarchy, affecting values, goals, and beliefs at higher levels of abstraction.At this stage, I had to ask myself if the hormone Iâ€™d been taking which seemed to reduce my symptoms of autism was doing so by reducing an inherent oversensitivity to prediction errors? If this was the case, might it also be pushing me further towards the other end of the autism-schizotypy continuum? What might that look like? The paper has this to say about schizotypal patterns of belief:Although the autistic type may rely more on culturally inherited high-level belief systems, the schizotypeâ€™s proclivity for tinkering with high-level priors may lead to the construction of relatively idiosyncratic high-level belief systems. In our own culture, this could manifest as having odd or (seemingly) unlikely beliefs about high-level causes. This may include beliefs in the paranormal, idiosyncratic religious beliefs (e.g., being â€œspiritual but not religiousâ€), or believing conspiracy theories, all of which are associated with positive schizotypy.Iâ€™ll outline some of the psychological changes Iâ€™ve noticed in myself since starting estrogen. The term â€œschizoâ€ is used very informally in todayâ€™s internet vernacular, making it difficult to discuss these concepts in a sensible manner â€“ but if the reader is comfortable playing armchair psychologist, perhaps they can judge for themselves whether the following makes me more â€œschizoâ€:Increased  of other peopleâ€™s internal states, resulting in a mixture of higher empathy and higher social anxiety. Iâ€™m somewhat more neurotic about potential threats.Decreased  and , for instance with tedious matters like finances.Armchair diagnoses aside, I do wish to assert that these psychological changes are quite similar to the kind of psychological changes I tend to experience while on a mild dose of psychedelics. So far as the pharmacology goes, there is an argument to be made that psychedelics induce a temporary state of psychosis via 5-HT2A agonism. From Pivotal mental states (Brouwer and Carhart-Harris, 2021):The psychotomimetic (psychosis-mimicking) effects of classic 5-HT2A receptor agonist psychedelics have been well documented. Importantly, psychedelics are felt to be useful models of  psychotic states that may be more likely to display psychedelic-like phenomena, such as changes in perception, cognition and ego functioning. Conversely, established psychotic disorders such as schizophrenia are more likely to feature characteristics of  cognition such as fixed delusions. Selective 5-HT2A receptor antagonism attenuates the main characteristic subjective effects of LSD, psilocybin and ayahuasca and the intensity of psychedelic states is reliably predicted by 5-HT2A receptor occupancy.Itâ€™s important to note that the authors are specifically discussing  rather than , and I couldnâ€™t find any evidence that  involves 5-HT2A receptor signalling. That said, given the two are related, and given that estrogen upregulates 5-HT2A receptor expression, could estrogen be responsible for increased positive schizotypy via a similar mechanism to psychedelics?Iâ€™d like to review what Iâ€™ve claimed so far:First of all, I should note that I donâ€™t expect these claims about estrogen phenomenology to generalise from trans women to cis women, and Iâ€™d also be cautious about generalizing neuroendocrinological findings from postmenopausal women to people starting from an androgenic baseline.  All this aside, I think this should be mostly sufficient to explain why estrogen might make somebody , especially if they are predisposed to depersonalisation, disembodiment, or autistic sensory sensitivities. However, I donâ€™t think weâ€™re that much closer to understanding whether hormone replacement therapy is actually correcting some kind of .The simplest explanation which fits the data (including nonbrain intersex conditions) is that sexual differentiation is a fragile rube goldberg machine, prone to random breakage. I speculate that humans have intersex brains so often because of evolution pulling out all stops for large brains and breaking things as a side effect.While I donâ€™t think people should have to convince the medical system of the validity of their internal experience in order to justify a hormone prescription â€“ other people , and I donâ€™t think this is likely to change anytime soon. So I think this research is  â€“ not least because this is an issue that directly affects an unusually productive and talented segment of society, many of whom I consider my friends.Hereâ€™s how Iâ€™d like to thread the needle. Gender dysphoria occupies an unusual epistemic status within a society not known for taking phenomenology seriously, because â€“ at least in liberal spaces â€“ peopleâ€™s self reports are generally never questioned.Iâ€™m not complaining â€“ I donâ€™t think this is a bad thing, even though I can be picky with my metaepistemics sometimes. What I would like to see is further development into phenomenological models of gender dysphoria. Existing models are already quite comprehensive, covering phenomena from high-level  to low-level  â€“ but I think they could utilise  detail, as it may provide essential clues to whatâ€™s going on.Could it be the case that gender dysphoria is a morphic resonance phenomenon â€“ and estrogen helps access the cosmic feminine unconscious by loading a different configuration file from the akashic records?  After all, if estrogen does make me more schizotypalâ€¦ ?]]></content:encoded></item><item><title>Compiling LLMs into a MegaKernel: A path to low-latency inference</title><link>https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17</link><author>matt_d</author><category>hn</category><pubDate>Thu, 19 Jun 2025 19:20:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single also known as a . In this design, the system launches just  GPU kernel to execute the entire model â€” from layer-by-layer computation to inter-GPU communication â€” without interruption. This approach offers several key performance advantages:Eliminates kernel launch overhead, even in multi-GPU settings,by avoiding repeated kernel invocations;Enables software pipelining, allowing the kernel to begin loading data for the next layer while computing the current one;Overlaps computation and communication, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency.Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks â€” such as PyTorch, Triton, and TVM â€” do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: NCCL or NVSHMEM for communication, FlashInfer or FlashAttention for efficient attention, and CUDA or Triton for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel.Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed â€” a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers.A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers.Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from  â€” as achieved by optimized systems like vLLM and SGLang â€” to , approaching the theoretical lower bound of  (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth).Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems increase with the number of GPUs, making it particularly effective for multi-GPU deployments.The rest of this blog dives deeper into how MPK works: introduces the , which transforms an LLMâ€™s computation graph into an optimized task graph; covers the , which executes this task graph within a megakernel to achieve high throughput and low latency.The computation performed by a large language model (LLM) is typically represented as a , where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this kernel-per-operator execution model often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity â€” across entire kernels â€” rather than the actual data units.Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication.To address this issue, MPK introduces a compiler that automatically transforms the LLMâ€™s computation graph into a fine-grained . This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers.Each  (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM).Each  (shown as a circle) represents a synchronization point between tasks.Each task has an outgoing edge to a , which is activated once all associated tasks complete.Each tasks also has an incoming edge from a , indicating the task can start execution as soon as the event is activated.Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input â€” enabling partial execution and overlap.In addition to generating an optimized task graph, MPK also automatically generates high-performance CUDA implementations for each task using the Mirage kernel superoptimizer. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see this post.)MPK includes an on-GPU runtime system that executes the task graph entirely within a single GPU megakernel, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference.To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles:  and . The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead.Each  operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop:Fetch the next task from its queue.Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers).Notify the triggering event upon task completion.This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations.Scheduling decisions are handled by MPKâ€™s , each of which runs on a . Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously:Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed).Launches the set of tasks that depend on the activated event.This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs.Figure 3 illustrates MPKâ€™s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a schedulerâ€™s event queue. The scheduler then launches any downstream tasks that depend on this event.This design allows for fine-grained software pipelining and overlap between computation and communication. For example:Matmul tasks can execute in parallel with attention tasks from different layers.Allreduce communication can begin as soon as partial matmul results are available.Because all scheduling and task transitions occur within a single kernel context, the overhead between tasks is extremely low â€” typically just  â€” enabling efficient execution of multi-layer, multi-GPU LLM workloads.Our vision for MPK is to make megakernel compilation both easy to use and highly performant. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code â€” mainly to specify the megakernelâ€™s inputs and outputs. Weâ€™re excited about this direction, and thereâ€™s still much more to explore. Some of the key areas weâ€™re actively working on include:Support for modern GPU architectures. One of our next milestones is extending MPK to support next-generation architectures such as . A major challenge lies in integrating warp specialization â€” a key optimization for newer GPUs â€” with MPKâ€™s megakernel execution model.Handling workload dynamism. MPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as  models. Weâ€™re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels.Advanced scheduling and task assignment: MPK unlocks a new level of  at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in advanced scheduling policies â€” such as priority-aware or throughput-optimized strategies â€” for use cases like latency-SLO-driven serving or hybrid batching.We believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and weâ€™re eager to collaborate with the community to push this vision forward.To learn more about MPK and explore our code and documentation, please visit our project website: https://github.com/mirage-project/mirage.We welcome feedback, contributions, and collaborations from the community!]]></content:encoded></item><item><title>Juneteenth in Photos</title><link>https://texashighways.com/travel-news/the-history-of-juneteenth-in-photos/</link><author>ohjeez</author><category>hn</category><pubDate>Thu, 19 Jun 2025 17:41:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>In praise of â€œnormalâ€ engineers</title><link>https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/</link><author>zdw</author><category>hn</category><pubDate>Thu, 19 Jun 2025 17:36:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This article was originally commissioned by Luca Rossi (paywalled) for refactoring.fm, on February 11th, 2025. Luca edited a version of it that emphasized the importance of building â€œ10x engineering teamsâ€ . It was later picked up by IEEE Spectrum (!!!), who scrapped most of the teams content and published a different, shorter piece on March 13th.This is my personal edit. It is not exactly identical to either of the versions that have been publicly released to date. It contains a lot of the source material for the talk I gave last week at #LDX3 in London, â€œIn Praise of â€˜Normalâ€™ Engineersâ€ (slides), and a couple weeks ago at CraftConf.Â In Praise of â€œNormalâ€ EngineersMost of us have encountered a few engineers who seem practically magician-like, a class apart from the rest of us in their ability to reason about complex mental models, leap to non-obvious yet elegant solutions, or emit waves of high quality code at unreal velocity.I have run into any number of these incredible beings over the course of my career. I think this is what explains the curious durability of the â€œ10x engineerâ€ meme. It may be based on flimsy, shoddy research, and the claims people have made to defend it have often beenÂ risible (e.g. â€œ10x engineers have dark backgrounds, are rarely seen doing UI work, are poor mentors and interviewersâ€), or blatantly double down on stereotypes (â€œwe look for young dudes in hoodies that remind us of Mark Zuckerbergâ€). But damn if it doesnâ€™t resonate with experience. It just feels true.The problem is not the idea that there are engineers who are 10x as productive as other engineers. I donâ€™t have a problem with this statement; in fact, that much seems self-evidently true. The problems I do have are twofold.Measuring productivity is fraught and imperfectFirst: how are you measuring productivity? I have a problem with the implication that there is One True Metric of productivity that you can standardize and sort people by. Consider, for a moment, the sheer combinatorial magnitude of skills and experiences at play:Also: people and their skills and abilities are not static. At one point, I was a pretty good DBRE (I even co-wrote the book on it). Maybe I was even a 10x DB engineer then, but certainly not now. I havenâ€™t debugged a query plan in years.â€œ10x engineerâ€ makes it sound like 10x productivity is an immutable characteristic of a person. But someone who is a 10x engineer in a particular skill set is still going to have infinitely more areas where they are normal or average (or less). I know a lot of world class engineers, but Iâ€™ve never met anyone who is 10x better than everyone else across the board, in every situation.Engineers donâ€™t own software, teams own softwareSecond, and even more importantly: So what? It doesnâ€™t matter. Individual engineers donâ€™t own software, teams own software. The smallest unit of software ownership and delivery is the engineering team. It doesnâ€™t matter how fast an individual engineer can write software, what matters is how fast the team can collectively write, test, review, ship, maintain, refactor, extend, architect, and revise the software that they own.Everyone uses the same software delivery pipeline. If it takes the slowest engineer at your company five hours to ship a single line of code, itâ€™s going to take the fastest engineer at your company five hours to ship a single line of code. The time spent writing code is typically dwarfed by the time spent on every other part of the software development lifecycle.If you have services or software components that are owned by a single engineer, that person is a single point of failure.Iâ€™m not saying this should never happen. Itâ€™s quite normal at startups to have individuals owning software, because the biggest existential risk that you face is not moving fast enough, not finding product market fit, and going out of business. But as you start to grow up as a company, as users start to demand more from you, and you start planning for the survival of the company to extend years into the futureâ€¦ownership needs to get handed over to a team. Individual engineers get sick, go on vacation, and leave the company, and the business has got to be resilient to that.If teams own software, then the key job of any engineering leader is to craft high-performing engineering teams. If you must 10x something, 10x this. Build 10x engineering teams.The best engineering orgs are the ones where normal engineers can do great workWhen people talk about world-class engineering orgs, they often have in mind teams that are top-heavy with staff and principal engineers, or recruiting heavily from the ranks of ex-FAANG employees or top universities.But I would argue that a truly great engineering org is one where you donâ€™t HAVE to be one of the â€œbestâ€ or most pedigreed engineers in the world to get shit done and have a lot of impact on the business.I think itâ€™s actually the other way around. A truly great engineering organization is one where perfectly normal, workaday software engineers, with decent software engineering skills and an ordinary amount of expertise, can consistently move fast, ship code, respond to users, understand the systems theyâ€™ve built, and move the business forward a little bit more, day by day, week by week.Any asshole can build an org where the most experienced, brilliant engineers in the world can build product and make progress. That is not hard. And putting all the spotlight on individual ability has a way of letting your leaders off the hook for doing their jobs. It is a HUGE competitive advantage if you can build sociotechnical systems where less experienced engineers can convert their effort and energy into product and business momentum.A truly great engineering org also happens to be one that mints world-class software engineers. But weâ€™re getting ahead of ourselves, here.Letâ€™s talk about â€œnormalâ€ for a momentA lot of technical people got really attached to our identities as smart kids. The software industry tends to reflect and reinforce this preoccupation at every turn, from Netflixâ€™s â€œwe look for the top 10% of global talentâ€ to Amazonâ€™s talk about â€œbar-raisingâ€ or Coinbaseâ€™s recent claim to â€œhire the top .1%â€. (Seriously, guys? Ok, well, Honeycomb is going to hire only the top !)In this essay, I would like to challenge us to set that baggage to the side and think about ourselves as .It can be humbling to think of ourselves as normal people, but most of us are in fact pretty normal people (albeit with many years of highly specialized practice and experience), and there is . Even those of us who are certified geniuses on certain criteria are likely quite normal in other ways â€” kinesthetic, emotional, spatial, musical, linguistic, etc.Software engineering both selects for and develops certain types of intelligence, particularly around abstract reasoning, but  is born a great software engineer. Great engineers are made, not born. I just donâ€™t think thereâ€™s a lot more we can get out of thinking of ourselves as a special class of people, compared to the value we can derive from thinking of ourselves collectively as relatively normal people who have practiced a fairly niche craft for a very long time.Build sociotechnical systems with â€œnormal peopleâ€ in mindWhen it comes to hiring talent and building teams, yes, absolutely, we should focus on identifying the ways people are exceptional and talented and strong. But when it comes to building sociotechnical systems for software delivery, we should focus on all the ways people are .Normal people have cognitive biases â€” confirmation bias, recency bias, hindsight bias. We work hard, we care, and we do our best; but we also forget things, get impatient, and zone out. Our eyes are inexorably drawn to the color red (unless we are colorblind). We develop habits and ways of doing things, and resist changing them. When we see the same text block repeatedly, we stop reading it.We are embodied beings who can get overwhelmed and fatigued. If an alert wakes us up at 3 am, we are much more likely to make mistakes while responding to that alert than if we tried to do the same thing at 3pm. Our emotional state can affect the quality of our work. Our relationships impact our ability to get shit done.When your systems are designed to be used by normal engineers, all that excess brilliance they have can get poured into the product itself, instead of wasting it on navigating the system itself.How do you turn normal engineers into 10x engineering teams?None of this should be terribly surprising; itâ€™s all well known wisdom. In order to build the kind of sociotechnical systems for software delivery that enable normal engineers to move fast, learn continuously, and deliver great results as a team, you should:Shrink the interval between when you write the code and when the code goes live.Make it as short as possible; the shorter the better. Iâ€™ve written and given talks about this many, many times. The shorter the interval, the lower the cognitive carrying costs. The faster you can iterate, the better. The more of your brain can go into the product instead of the process of building it.One of the most powerful things you can do is have a short, fast enough deploy cycle that you can ship one commit per deploy. Iâ€™ve referred to this as the â€œsoftware engineering death spiralâ€ â€¦ when the deploy cycle takes so long that you end up batching together a bunch of engineersâ€™ diffs in every build. The slower it gets, the more you batch up, and the harder it becomes to figure out what happened or roll back. The longer it takes, the more people you need, the higher the coordination costs, and the more slowly everyone moves.Deploy time is the feedback loop at the heart of the development process. It is almost impossible to overstate the centrality of keeping this short and tight.Make it easy and fast to roll back or recover from mistakes.Developers should be able to deploy their own code, figure out if itâ€™s working as intended or not, and if not, roll forward or back swiftly and easily. No muss, no fuss, no thinking involved.Make it easy to do the right thing and hard to do the wrong thing. Wrap designers and design thinking into all the touch points your engineers have with production systems. Use your platform engineering team to think about how to empower people to swiftly make changes and self-serve, but also remember that a lot of times people will be engaging with production late at night or when theyâ€™re very stressed, tired, andÂ possibly freaking out. Build guard rails. The fastest way to ship a single line of code should also be the easiest way to ship a single line of code.Invest in instrumentation and observability.Youâ€™ll never know â€” not really â€” what the code you wrote does just by reading it. The only way to be sure is by instrumenting your code and watching real users run it in production. Good, friendly sociotechnical systems invest  in tools for sense-making.Being able to visualize your work is what makes engineering abstractions accessible to actual engineers. You shouldnâ€™t have to be a world-class engineer just to debug your own damn code.Devote engineering cycles to internal tooling and enablement.If fast, safe deploys, with guard rails, instrumentation, and highly parallelized test suites are â€œeverybodyâ€™s jobâ€, they will end up nobodyâ€™s job. Engineering productivity isnâ€™t something you can outsource. Managing the interfaces between your software vendors and your own teams is both a science and an art. Making it look easy and intuitive is really hard. It needs an owner.Build an inclusive culture.Growth is the norm, growth is the baseline. People do their best work when they feel a sense of belonging. An inclusive culture is one where everyone feels safe to ask questions, explore, and make mistakes; where everyone is held to the same high standard, and given the support and encouragement they need to achieve their goals.Diverse teams are resilient teams.Yeah, a team of super-senior engineers who all share a similar background can move incredibly fast, but a monoculture is fragile. Someone gets sick, someone gets pregnant, you start to grow and you need to integrate people from other backgrounds and the whole team can get derailed â€” fast.When your teams are used to operating with a mix of genders, racial backgrounds, identities, age ranges, family statuses, geographical locations, skill sets, etc â€” when this is just table stakes, standard operating procedure â€” youâ€™re better equipped to roll with it when life happens.Assemble engineering teams from a range of levels.The best engineering teams arenâ€™t top-heavy with staff engineers and principal engineers. The best engineering teams are ones where nobody is running on autopilot, banging out a login page for the 300th time; everyone is working on something that challenges them and pushes their boundaries. Everyone is learning, everyone is teaching, everyone is pushing their own boundaries and growing. All the time.By the way â€” all of that work you put into making your systems resilient, well-designed, and humane is the same work you would need to do to help onboard new engineers, develop junior talent, or let engineers move between teams.It gets used and reused. Over and over and over again.The only meaningful measure of productivity is impact to the businessThe only thing that actually matters when it comes to engineering productivity is whether or not you are moving the business materially forward.Which meansâ€¦we canâ€™t do this in a vacuum. The most important question is whether or not we are working on the right thing, which is a problem engineering canâ€™t answer without help from product, design, and the rest of the business.Software engineering isnâ€™t about writing lots of lines of code, itâ€™s about solving business problems using technology.Senior and intermediate engineers are actually the workhorses of the industry. They move the business forward, step by step, day by day. They get to put their heads down and crank instead of constantly looking around the org and solving coordination problems. If you have to be a staff+ engineer to move the product forward, something is seriously wrong.Great engineering orgs mint world-class engineersA great engineering org is one where you donâ€™t HAVE to be one of the best engineers in the world to have a lot of impact. But â€” rather ironically â€” great engineering orgs mint world class engineers like nobodyâ€™s business.The best engineering orgs are not the ones with the smartest, most experienced people in the world, theyâ€™re the ones where normal software engineers can consistently make progress, deliver value to users, and move the business forward, day after day.Places where engineers can get shit done and have a lot of impact are a magnet for top performers. Nothing makes engineers happier than building things, solving problems, making progress.If youâ€™re lucky enough to have world-class engineers in your org, good for you! Your role as a leader is to leverage their brilliance for the good of your customers and your other engineers, without coming to depend on their brilliance. After all, these people donâ€™t belong to you. They may walk out the door at any moment, and that has to be okay.These people can be phenomenal assets, assuming they can be team players and keep their egos in check. Which is probably why so many tech companies seem to obsess over identifying and hiring them, especially in Silicon Valley.But companies categorically overindex on finding these people after theyâ€™ve already been minted, which ends up reinforcing and replicating all the prejudices and inequities of the world at large. Talent may be evenly distributed across populations, but opportunity is not.Donâ€™t hire the â€œbestâ€ people. Hire the right people.We (by which I mean the entire human race) place too much emphasis on individual agency and characteristics, and not enough on the systems that shape us and inform our behaviors.I feel like a whole slew of issues (candidates self-selecting out of the interview process, diversity of applicants, etc) would be improved simply by shifting the focus on engineering hiring and interviewing away from this inordinate emphasis on hiring the BEST PEOPLE and realigning around the more reasonable and accurate RIGHT PEOPLE. Itâ€™s a competitive advantage to build an environment where people can be hired for their unique strengths, not their lack of weaknesses; where the emphasis is on composing teams rather than hiring the BEST people; where inclusivity is a given both for ethical reasons andÂ because it raises the bar for performance for everyone. Inclusive culture is what actual meritocracy depends on.This is the kind of place that engineering talent (and good humans) are drawn to like a moth to a flame. . It feels  to move the business forward. It feels  to sharpen your skills and improve your craft. Itâ€™s the kind of place that people go when they want to become world class engineers. And itâ€™s the kind of place where world class engineers want to stick around, to train up the next generation.]]></content:encoded></item><item><title>Show HN: EnrichMCP â€“ A Python ORM for Agents</title><link>https://github.com/featureform/enrichmcp</link><author>bloppe</author><category>dev</category><category>hn</category><pubDate>Thu, 19 Jun 2025 17:32:21 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I've been working with the Featureform team on their new open-source project, [EnrichMCP][1], a Python ORM framework that helps AI agents understand and interact with your data in a structured, semantic way.EnrichMCP is built on top of [MCP][2] and acts like an ORM, but for agents instead of humans. You define your data model using SQLAlchemy, APIs, or custom logic, and EnrichMCP turns it into a type-safe, introspectable interface that agents can discover, traverse, and invoke.It auto-generates tools from your models, validates all I/O with Pydantic, handles relationships, and supports schema discovery. Agents can go from user â†’ orders â†’ product naturally, just like a developer navigating an ORM.We use this internally to let agents query production systems, call APIs, apply business logic, and even integrate ML models. It works out of the box with SQLAlchemy and is easy to extend to any data source.]]></content:encoded></item><item><title>Homegrown Closures for Uxn</title><link>https://krzysckh.org/b/Homegrown-closures-for-uxn.html</link><author>todsacerdoti</author><category>hn</category><pubDate>Thu, 19 Jun 2025 17:29:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For a week or so now, I've been writing niÃ«nor, a "lispy
environment for uxn". I did not want it to become a full-blown lisp
but just another way of writing uxntal. Uxntal is a
bit too dense for my liking, and I prefer lisp/scheme s-expression
syntax. NiÃ«nor is just a compiler and a macroexpander that takes in
scheme-like code and spits out uxn roms.This article describes my homegrown method of creating lexically
scoped closures in this environment.When ignoring lexical scope lambdas are really simple to implement if
we can already compile named functions.Here is some simplified code with commentary from the compiler that
does exactly that (I've skipped some boring parts). We simply give the
anonymous function a name, skip the compilation for now (add to
epilogue), and  the name (that will get resolved later
by the compiler) in its place.Or, lambdas with extra steps.Closures are anonymous functions that capture variables from the
environment. Consider this classic example:Here, the lambda captures the variable , to later add
it to . If we would try to simply name the lambda and
compile it somewhere else, we wouldn't be able to resolve the symbol
, as it wouldn't be visible in global scope.We could simply  closures. NiÃ«nor was meant
to be quite low-level and closures are, well, quite high-level-ish in my
opinion. That is a valid method, but not really a . I
keep closures close to my heart, so I really wanted to have them in my
lispy uxn.I decided to create objects that tied the required environment to
functions. Well, kind of... Because there are no types on runtime, I
can't really detect if I'm trying to call a closure, a function, or
something else.This, for example, is perfectly valid way to restart a program:This means that we  to generate executable code at
runtime to later jump to - yikes!I thought of copying the entire function to somewhere else and
replacing the unbound variables with environment-specified values during
runtime, but that wouldn't work as absolute jumps would always go back
to the  function, not our copied version. We could
(probably) find all internal jumps and calculate new positions at
runtime, but that's expensive (slow) and hard.The solution i came up with (while showering) is the following:At compile time, when we know what values from the environment the
function will need, add them as parametersThis makes it so all the variables will always be bound, and we can
generate code for this like for a normal lambda. Of course, we can't
just return this to the user, because they will expect a single-argument
function with environment attached.Generate wrapper at runtimeAt runtime, we generate a  - a "function" we'll return
to the user, that adds the environment to the normal function call.For example, if we called the upper-mentioned 
with 32 at runtime, this would happen:Then if  was to be called with , the
program counter would firstly land on the freshly generated closure
code,  would be pushed and 
would be called with  and  as
 and .This is a small example of a gui program that draws hearts in random
spots. returns a closure, we'll take a closer look
at what the generated code for it looks like. This is a decompilation
spat out by  with some extra commentary on
top.So, the code that gets generated at runtime in this case would
be:This is the entirety of the  that we'll return to the
user.I've implemented  &  to
manually manage the memory left in the RAM after the ROM. They are also
used to allocate closures - this means that when the user is done with a
closure, they can simply  it to return unused memory to
the pool.Because memory used by f1  got freed, f3 got allocated
in the same spot .This has not been battle-tested yet and is (of course) purely
experimental. Thank you for reading. If I've sparked your interest, you
can download niÃ«nor and follow its development here.]]></content:encoded></item><item><title>How OpenElections uses LLMs</title><link>https://thescoop.org/archives/2025/06/09/how-openelections-uses-llms/index.html</link><author>m-hodges</author><category>hn</category><pubDate>Thu, 19 Jun 2025 16:11:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In the 12-plus years that weâ€™ve been turning official precinct election results into data at OpenElections, the single biggest problem has been converting pictures of results into CSV files. Many of the precinct results files we get are image PDFs, and for those there are essentially two options: data entry or Optical Character Recognition. The former has some advantages, but not many. While most people are not great at manual repetitive tasks, you can improve with lots of practice, to the point where the results are very accurate. In the past we did pay for data entry services, and while we developed working relationships with two individuals in particular, the results almost always contained some mistakes and the cost could run into the hundreds of dollars pretty quickly. For a volunteer project, it just didnâ€™t make sense.We also used commercial OCR software, most often Able2Extract, which did pretty well, but had a harder time with PDFs that had markings or were otherwise difficult to parse. Thankfully, most election results PDFs are in one of a small handful of formats, which makes things a bit less complicated, but commercial OCR has too many restrictions.For parsing image PDFs into CSV files, Googleâ€™s Gemini is my model of choice, for two main reasons. First, the results are usually very, very accurate (with a few caveats Iâ€™ll detail below), and second, Geminiâ€™s large context window means itâ€™s possible to work with PDF files that can be multiple MBs in size. Here are some examples using image PDFs from Texas counties of how OpenElections uses Gemini for its work.The Limestone County file containing its 2024 general election results isnâ€™t too bad for an image PDF:It has clear black text on a white background without markings. But two big issues make it hard for most OCR software to deal with: the two-column layout, with results from races on the left and the right; and those annoying dots between the end of candidate values and the vote totals. Itâ€™s like a delimited layout within a fixed-width layout. If you use OCR software, generally you have to draw the boxes around areas of PDFs like this in order to make the extraction results usable. This PDF isnâ€™t too large at 42 pages, but thatâ€™s still a fair bit of manual labor to get the results, and even then there would be some cleanup required.This is where good LLMs should be able to make a difference, because what you want is high-quality OCR results  the ability to provide some domain or business logic to the process without having to do it all yourself. You can see from this Google Gemini session that I didnâ€™t have to provide much in the way of instructions after giving an example of the CSV output and some basic office standardization, just â€œThe results are split into two columns on each page; parse the left column first and then the right column.â€How did Gemini do? Pretty well, almost perfectly. The numbers are accurate, according to some spot checks of candidate totals from the Texas Secretary of State website. It did make some formatting mistakes; removing a blank column in some of the Registered Voters and Ballots Cast rows, for example. But thatâ€™s a quick fix, and the finished result is exactly what we need. Itâ€™s easy to be impressed, but itâ€™s also just 42 pages and had a simple format.The PDF with results from Live Oak County comes in a common format that features a green background. But Live Oakâ€™s image PDF is a black and white scan with different variations of shading, plus we donâ€™t want the four columns containing percentages. For commercial OCR software, this would be a real problem thanks to the layout alone. Indeed, for electronic PDFs that are produced using the same software, weâ€™ve got a Python script that converts the PDF to text and parses it into a CSV file. But this one is different:The prompt to convert this 90-page image PDF is like the first one: an example tailored to the first set of results and the unusual placement of the registered voters and ballots cast figures. Gemini repeated the earlier mistake of removing a blank column from the Registered Voters and Ballots Cast rows, but otherwise was spot on in its accuracy. Hereâ€™s the fixed CSV result.One of the areas where LLMs, even Gemini, can struggle with is sustained processes. Converting a few or a few dozen pages is usually pretty simple work for high-performing models, but what about hundreds of pages? Cameron Countyâ€™s PDF, all 11.7 MB of it, offers a good challenge, and not just owing to its size:Notice how the â€œPrecinct 16â€ is slightly obscured by an actual punch-hole in this document, and the same is true at the bottom of the image with â€œOvervotesâ€ and â€œUndervotesâ€. Both of those issues could trip up commercial OCR engines. Providing an example of the output, as in the Limestone example, should help fill those literal holes, along with further instructions to ignore the  column entirely. The first attempt at parsing the 653-page PDF eventually â€œworkedâ€ in that it produced a CSV file. But I had to urge Gemini to â€œcontinueâ€ multiple times, and it appeared to need more attention starting about halfway through. Most important, the vote figures in the CSV file were close, but not always correct. Back to the drawing board.The process that generated an accurate CSV file involved splitting the single PDF into multiple parts of about 100 pages each and feeding them one at a time to Gemini. That did mean copying and pasting the output, and one drawback of providing a lot of information in one session was that some of the offices didnâ€™t get quoted properly in the CSV file (to be fair, this probably wouldnâ€™t matter if I were using Geminiâ€™s structured output feature). That meant a little bit of clean-up work, but again, the end result is an accurate precinct results file in about an hour. From a 653-page image PDF, with no data entry.Could other models do similar work? Probably so, especially for smaller PDFs. But there are couple of other things that make Gemini the first choice for this: its AI Studio UI allows me to turn the temperature down to 0 (less creativity) and, for models where the â€œthinking modeâ€ is optional, the ability to disable it if the task at hand is pretty straight-forward. In the six weeks since we started working on Texas precinct results, weâ€™ve been able to convert them for more than half of the stateâ€™s 254 counties, including many image PDFs like the ones on display here. That pace simply wouldnâ€™t be possible with data entry or traditional OCR software.Speed isnâ€™t the most important factor here, though: accuracy is, and using LLMs still means a system of checks to ensure that the results are what the originals say they are. One step in that is taken care of by a suite of tests that run every time a new or changed CSV gets pushed to one of our data repositories. Those tests look for some formatting issues, duplicate records and basic math inconsistencies. A second step - for now manual - is verifying that multiple totals derived from the precinct CSV match the numbers in an official cumulative report like this one from Live Oak County. A better version of that could also involve using LLMs to produce both cumulative and precinct-level data, but that would raise the possibility that a model makes similar mistakes in different documents. If you have ideas, head over to our GitHub organization and get involved.]]></content:encoded></item><item><title>Curved-Crease Sculpture</title><link>https://erikdemaine.org/curved/</link><author>wonger_</author><category>hn</category><pubDate>Thu, 19 Jun 2025 14:13:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Simons Center for Geometry and Physics Art Gallery, 2012
Smithsonian American Art Museum permanent collection
     Renwick Gallery exhibit in 2012
]]></content:encoded></item><item><title>Show HN: A DOS-like hobby OS written in Rust and x86 assembly</title><link>https://github.com/krustowski/rou2exOS</link><author>krustowski</author><category>dev</category><category>hn</category><pubDate>Thu, 19 Jun 2025 13:38:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[To try it out, simply build the project yourself from source, or use attached bootable ISO image of the system (in Releases on Github) and run it in QEMU.]]></content:encoded></item><item><title>End of 10: Upgrade your old Windows 10 computer to Linux</title><link>https://endof10.org/</link><author>doener</author><category>hn</category><pubDate>Thu, 19 Jun 2025 13:14:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Support for Windows 10 ends on October 14, 2025.Microsoft wants you to buy a new computer.But what if you could make your current one fast and secure again?If you bought your computer after 2010, there's most likely no
reason to throw it out. By just installing an up-to-date Linux
operating system you can keep using it for years to come.Installing an operating system may sound difficult, but you don't have to
do it alone. With any luck, there are people in your area
ready to help!No New Hardware, No Licensing CostsA new laptop costs a lot of money, but several Linux operating systems
are available for free. Software updates are also free, forever. You can
of course show your support with donations!Windows comes with lots of ads and spyware. This slows down your computer,
lets companies spy on you, and increases your energy bills.Production of a computer accounts for 75+% of carbon emissions over its lifecycle.
Keeping a functioning device longer is a hugely effective way to reduce emissions.
With a Linux operating system you can use your device longer.Community & Professional SupportThere are local repair cafes and independent, professional services and
computer shops available for providing you help. You can find support in online
forums, too.Linux grants you the four freedoms of software. You are free to use, study, share, and
improve the program, for as long as you wish. You are in control of your device.These organizations have joined us in support of the campaign.Then find your closest repair cafe or independent computer shop
and enjoy your brand-new, old computer!]]></content:encoded></item><item><title>What would a Kubernetes 2.0 look like</title><link>https://matduggan.com/what-would-a-kubernetes-2-0-look-like/</link><author>Bogdanp</author><category>hn</category><pubDate>Thu, 19 Jun 2025 12:00:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Around 2012-2013 I started to hear a  in the sysadmin community about a technology called "Borg". It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a "Borglet" inside of clusters with "cells" but the basics started to leak. There was a concept of "services" and a concept of "jobs", where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for 'helmsman' that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to "maybe this is a real product?" On July 21st 2015 we got the v1.0 release as well as the creation of the CNCF. In the ten years since that initial commit, Kubernetes has become a large part of my professional life. I use it at home, at work, on side projectsâ€”anywhere it makes sense. It's a tool with a steep learning curve, but it's also a massive force multiplier. We no longer "manage infrastructure" at the server level; everything is declarative, scalable, recoverable and (if youâ€™re lucky) self-healing.But the journey hasn't been without problems. Some common trends have emerged, where mistakes or misconfiguration arise from where Kubernetes isn't opinionated enough. Even ten years on, we're still seeing a lot of churn inside of ecosystem and people stepping on well-documented landmines. So, knowing what we know now, what could we do differently to make this great tool even more applicable to more people and problems? Let's start with the positive stuff. Why are we still talking about this platform now? Containers as a tool for software development make perfect sense. Ditch the confusion of individual laptop configuration and have one standard, disposable concept that works across the entire stack. While tools like Docker Compose allowed for some deployments of containers, they were clunky and still required you as the admin to manage a lot of the steps. I set up a Compose stack with a deployment script that would remove the instance from the load balancer, pull the new containers, make sure they started and then re-added it to the LB, as did lots of folks. K8s allowed for this concept to scale out, meaning it was possible to take a container from your laptop and deploy an identical container across thousands of servers. This flexibility allowed organizations to revisit their entire design strategy, dropping monoliths and adopting more flexible (and often more complicated) micro-service designs. If you think of the history of Operations as a sort of "naming timeline from pets to cattle", we started with what I affectionately call the "Simpsons" era. Servers were bare metal boxes set up by teams, they often had one-off names that became slang inside of teams and everything was a snowflake. The longer a server ran, the more cruft it picked up until it became a scary operation to even reboot them, much less attempt to rebuild them. I call it the "Simpsons" era because among the jobs I was working at the time, naming them after Simpsons characters was surprisingly common. Nothing fixed itself, everything was a manual operation. Then we transition into the "01 Era". Tools like Puppet and Ansible have become common place, servers are more disposable and you start to see things like bastion hosts and other access control systems become the norm. Servers aren't all facing the internet, they're behind a load balancer and we've dropped the cute names for stuff like "app01" or "vpn02". Organizations designed it so they could lose some of their servers some of the time. However failures still weren't self-healing, someone still had to SSH in to see what broke, write up a fix in the tooling and then deploy it across the entire fleet. OS upgrades were still complicated affairs. We're now in the "UUID Era". Servers exist to run containers, they are entirely disposable concepts. Nobody cares about how long a particular version of the OS is supported for, you just bake a new AMI and replace the entire machine. K8s wasn't the only technology enabling this, but it was the one that accelerated it. Now the idea of a bastion server with SSH keys that I go to the underlying server to fix problems is seen as more of a "break-glass" solution. Almost all solutions are "destroy that Node, let k8s reorganize things as needed, make a new Node". A lot of the Linux skills that were critical to my career are largely nice to have now, not need to have. You can be happy or sad about that, I certainly switch between the two emotions on a regular basis, but it's just the truth. The k8s jobs system isn't perfect, but it's so much better than the "snowflake cron01 box" that was an extremely common sight at jobs for years. Running on a cron schedule or running from a message queue, it was now possible to reliably put jobs into a queue, have them get run, have them restart if they didn't work and then move on with your life. Not only does this free up humans from a time-consuming and boring task, but it's also simply a more efficient use of resources. You are still spinning up a pod for every item in the queue, but your teams have a lot of flexibility inside of the "pod" concept for what they need to run and how they want to run it. This has really been a quality of life improvement for a lot of people, myself included, who just need to be able to easily background tasks and not think about them again. Service Discoverability and Load BalancingHard-coded IP addresses that lived inside of applications as the template for where requests should be routed has been a curse following me around for years. If you were lucky, these dependencies weren't based on IP address but were actually DNS entries and you could change the thing behind the DNS entry without coordinating a deployment of a million applications. K8s allowed for simple DNS names to call other services. It removed an entire category of errors and hassle and simplified the entire thing down. With the Service API you had a stable, long lived IP and hostname that you could just point things towards and not think about any of the underlying concepts. You even have concepts like ExternalName that allow you to treat external services like they're in the cluster. What would I put in a Kubernetes 2.0?YAML was appealing because it wasn't JSON or XML, which is like saying your new car is great because it's neither a horse nor a unicycle. It demos nicer for k8s, looks nicer sitting in a repo and has the  of being a simple file format. In reality. YAML is just too much for what we're trying to do with k8s and it's not a safe enough format. Indentation is error-prone, the files don't scale great (you really don't want a super long YAML file), debugging can be annoying. YAML has  subtle behaviors outlined in its spec.I still remember not believing what I was seeing the first time I saw the Norway Problem. For those lucky enough to not deal with it, the Norway Problem in YAML is when 'NO' gets interpreted as false. Imagine explaining to your Norwegian colleagues that their entire country evaluates to false in your configuration files. Add in accidental numbers from lack of quotes, the list goes on and on. There are much better posts on why YAML is crazy than I'm capable of writing: https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hellHCL is already the format for Terraform, so at least we'd only have to hate one configuration language instead of two. It's strongly typed with explicit types. There's already good validation mechanisms. It is specifically designed to do the job that we are asking YAML to do and it's not much harder to read. It has built-in functions people are already using that would allow us to remove some of the third-party tooling from the YAML workflow. I would wager 30% of Kubernetes clusters today are  being managed with HCL via Terraform. We don't need the Terraform part to get a lot of the benefits of a superior configuration language. The only downsides are that HCL is slightly more verbose than YAML, and its Mozilla Public License 2.0 (MPL-2.0) would require careful legal review for integration into an Apache 2.0 project like Kubernetes. However, for the quality-of-life improvements it offers, these are hurdles worth clearing.Let's take a simple YAML file. # YAML doesn't enforce types
replicas: "3"  # String instead of integer
resources:
  limits:
    memory: 512  # Missing unit suffix
  requests:
    cpu: 0.5m    # Typo in CPU unit (should be 500m)Even in the most basic example, there are footguns everywhere. HCL and the type system would catch all of these problems. replicas = 3  # Explicitly an integer

resources {
  limits {
    memory = "512Mi"  # String for memory values
  }
  requests {
    cpu = 0.5  # Number for CPU values
  }
}Take a YAML file like this that you probably have 6000 in your k8s repo. Now look at HCL without needing external tooling. # Need external tools or templating for dynamic values
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Can't easily generate or transform values
  DATABASE_URL: "postgres://user:password@db:5432/mydb"
  API_KEY: "static-key-value"
  TIMESTAMP: "2023-06-18T00:00:00Z"  # Hard-coded timestampresource "kubernetes_config_map" "app_config" {
  metadata {
    name = "app-config"
  }
  
  data = {
    DATABASE_URL = "postgres://${var.db_user}:${var.db_password}@${var.db_host}:${var.db_port}/${var.db_name}"
    API_KEY      = var.api_key != "" ? var.api_key : random_string.api_key.result
    TIMESTAMP    = timestamp()
  }
}

resource "random_string" "api_key" {
  length  = 32
  special = false
}Here's all the pros you get with this move. : Preventing type-related errors before deployment: Reducing duplication and improving maintainabilityFunctions and Expressions: Enabling dynamic configuration generation: Supporting environment-specific configurations: Simplifying repetitive configurations: Improving documentation and readability: Making errors easier to identify and fix: Enabling reuse of configuration components: Preventing invalid configurations: Supporting complex data manipulationsI know, I'm the 10,000 person to write this. Etcd has done a fine job, but it's a little crazy that it is the only tool for the job. For smaller clusters or smaller hardware configuration, it's a large use of resources in a cluster type where you will never hit the node count where it pays off. It's also a strange relationship between k8s and etcd now, where k8s is basically the only etcd customer left. What I'm suggesting is taking the work of kine and making it official. It makes sense for the long-term health of the project to have the ability to plug in more backends, adding this abstraction means it (should) be easier to swap in new/different backends in the future and it also allows for more specific tuning depending on the hardware I'm putting out there. What I suspect this would end up looking like is much like this: https://github.com/canonical/k8s-dqlite. Distributed SQlite in-memory with Raft consensus and almost zero upgrade work required that would allow cluster operators to have more flexibility with the persistence layer of their k8s installations. If you have a conventional server setup in a datacenter and etcd resource usage is not a problem, great! But this allows for lower-end k8s to be a nicer experience and (hopefully) reduces dependence on the etcd project. Beyond Helm: A Native Package ManagerHelm is a perfect example of a temporary hack that has grown to be a permanent dependency. I'm grateful to the maintainers of Helm for all of their hard work, growing what was originally a hackathon project into the de-facto way to install software into k8s clusters. It has done as good a job as something could in fulfilling that role without having a deeper integration into k8s. All that said, Helm is a nightmare to use. The Go templates are tricky to debug, often containing complex logic that results in really confusing error scenarios. The error messages you get from those scenarios are often gibberish. Helm isn't a very good package system because it fails at some of the basic tasks you need a package system to do, which are transitive dependencies and resolving conflicts between dependencies. Tell me what this conditional logic is trying to do:# A real-world example of complex conditional logic in Helm
{{- if or (and .Values.rbac.create .Values.serviceAccount.create) (and .Values.rbac.create (not .Values.serviceAccount.create) .Values.serviceAccount.name) }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ template "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
{{- end }}Or if I provide multiple values files to my chart, which one wins:helm install myapp ./mychart -f values-dev.yaml -f values-override.yaml --set service.type=NodePortOk, what if I want to manage my application and all the application dependencies with a Helm chart. This makes sense, I have an application that itself has dependencies on other stuff so I want to put them all together. So I define my sub-charts or umbrella charts inside of my Chart.yaml. dependencies:
- name: nginx
  version: "1.2.3"
  repository: "<https://example.com/charts>"
- name: memcached
  version: "1.2.3"
  repository: "<https://another.example.com/charts>"
But assuming I have multiple applications, it's entirely possible that I have 2 services both with a dependency on nginx or whatever like this:Helm doesn't handle this situation gracefully because template names are global with their templates loaded alphabetically. Basically you need to:Don't declare a dependency on the same chart more than once (hard to do for a lot of microservices)If you do have the same chart declared multiple times, has to use the exact same versionThe list of issues goes on and on. Cross-Namespace installation stinksChart verification process is a pain and nobody uses itLet's just go to the front page of artifacthub:I'll grab elasticsearch cause that seems important. Seems  for the Official Elastic helm chart. Certainly  will be right, it's an absolute critical dependency for the entire industry. Nope. Also how is the maintainer of the chart "Kubernetes" and it's  not marked as a . Like Christ how much more verified does it get.No metadata in chart searching. You can only search by name and description, not by features, capabilities, or other metadata.Helm doesn't strictly enforce semantic versioning# Chart.yaml with non-semantic version
apiVersion: v2
name: myapp
version: "v1.2-alpha" If you uninstall and reinstall a chart with CRDs, it might delete resources created by those CRDs. This one has screwed me  and is crazy unsafe. I could keep writing for another 5000 words and still wouldn't have outlined all the problems. There isn't a way to make Helm good enough for the task of "package manager for all the critical infrastructure on the planet". What would a k8s package system look like?Let's call our hypothetical package system KubePkg, because if there's one thing the Kubernetes ecosystem needs, it's another abbreviated name with a 'K' in it. We would try to copy as much of the existing work inside the Linux ecosystem while taking advantage of the CRD power of k8s. My idea looks something like this:The packages are bundles like a Linux package:There's a definition file that accounts for as many of the real scenarios that you actually encounter when installing a thing. apiVersion: kubepkg.io/v1
kind: Package
metadata:
  name: postgresql
  version: 14.5.2
spec:
  maintainer:
    name: "PostgreSQL Team"
    email: "[emailÂ protected]"
  description: "PostgreSQL database server"
  website: "https://postgresql.org"
  license: "PostgreSQL"
  
  # Dependencies with semantic versioning
  dependencies:
    - name: storage-provisioner
      versionConstraint: ">=1.0.0"
    - name: metrics-collector
      versionConstraint: "^2.0.0"
      optional: true
  
  # Security context and requirements
  security:
    requiredCapabilities: ["CHOWN", "SETGID", "SETUID"]
    securityContextConstraints:
      runAsUser: 999
      fsGroup: 999
    networkPolicies:
      - ports:
        - port: 5432
          protocol: TCP
    
  # Resources to be created (embedded or referenced)
  resources:
    - apiVersion: v1
      kind: Service
      metadata:
        name: postgresql
      spec:
        ports:
        - port: 5432
    - apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: postgresql
      spec:
        # StatefulSet definition
  
  # Configuration schema using JSON Schema
  configurationSchema:
    type: object
    properties:
      replicas:
        type: integer
        minimum: 1
        default: 1
      persistence:
        type: object
        properties:
          size:
            type: string
            pattern: "^[0-9]+[GMK]i$"
            default: "10Gi"
  
  # Lifecycle hooks with proper sequencing
  hooks:
    preInstall:
      - name: database-prerequisites
        job:
          spec:
            template:
              spec:
                containers:
                - name: init
                  image: postgres:14.5
    postInstall:
      - name: database-init
        job:
          spec:
            # Job definition
    preUpgrade:
      - name: backup
        job:
          spec:
            # Backup job definition
    postUpgrade:
      - name: verify
        job:
          spec:
            # Verification job definition
    preRemove:
      - name: final-backup
        job:
          spec:
            # Final backup job definition
  
  # State management for stateful applications
  stateManagement:
    backupStrategy:
      type: "snapshot"  # or "dump"
      schedule: "0 2 * * *"  # Daily at 2 AM
      retention:
        count: 7
    recoveryStrategy:
      type: "pointInTime"
      verificationJob:
        spec:
          # Job to verify recovery success
    dataLocations:
      - path: "/var/lib/postgresql/data"
        volumeMount: "data"
    upgradeStrategies:
      - fromVersion: "*"
        toVersion: "*"
        strategy: "backup-restore"
      - fromVersion: "14.*.*"
        toVersion: "14.*.*"
        strategy: "in-place"There's a real signing process that would be required and allow you more control over the process. apiVersion: kubepkg.io/v1
kind: Repository
metadata:
  name: official-repo
spec:
  url: "https://repo.kubepkg.io/official"
  type: "OCI"  # or "HTTP"
  
  # Verification settings
  verification:
    publicKeys:
      - name: "KubePkg Official"
        keyData: |
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvF4+...
          -----END PUBLIC KEY-----
    trustPolicy:
      type: "AllowList"  # or "KeyRing"
      allowedSigners:
        - "KubePkg Official"
        - "Trusted Partner"
    verificationLevel: "Strict"  # or "Warn", "None"Like how great would it be to have something where I could automatically update packages without needing to do anything on my side. apiVersion: kubepkg.io/v1
kind: Installation
metadata:
  name: postgresql-main
  namespace: database
spec:
  packageRef:
    name: postgresql
    version: "14.5.2"
  
  # Configuration values (validated against schema)
  configuration:
    replicas: 3
    persistence:
      size: "100Gi"
    resources:
      limits:
        memory: "4Gi"
        cpu: "2"
  
  # Update policy
  updatePolicy:
    automatic: false
    allowedVersions: "14.x.x"
    schedule: "0 2 * * 0"  # Weekly on Sunday at 2am
    approvalRequired: true
  
  # State management reference
  stateRef:
    name: postgresql-main-state
    
  # Service account to use
  serviceAccountName: postgresql-installerWhat k8s needs is a system that meets the following requirements:: Everything is a Kubernetes resource with proper status and eventsFirst-Class State Management: Built-in support for stateful applications: Robust signing, verification, and security scanningDeclarative Configuration: No templates, just structured configuration with schemas: Comprehensive lifecycle hooks and upgrade strategies: Linux-like dependency management with semantic versioning: Complete history of changes with who, what, and when, not what Helm currently provides. : Support for organizational policies and compliance. Simplified User Experience: Familiar Linux-like package management commands. It seems wild that we're trying to go a different direction from the package systems that have worked for decades. Try to imagine, across the entire globe, how much time and energy has been invested in trying to solve any one of the following three problems. I need this pod in this cluster to talk to that pod in that cluster. There is a problem happening somewhere in the NAT traversal process and I need to solve itI have run out of IP addresses with my cluster because I didn't account for how many you use. Remember: A company starting with a /20 subnet (4,096 addresses), deploys 40 nodes with 30 pods each, and suddenly realizes they're approaching their IP limit. Not that many nodes!I am not suggesting the entire internet switches over to IPv6 and right now k8s happily supports IPv6-only if you want and a dualstack approach. But I'm saying now is the time to flip the default and just go IPv6. You eliminate a huge collection of problems all at once. Flatter, less complicated network topology inside of the cluster. The distinction between multiple clusters becomes a thing organizations can choose to ignore if they want if they want to get public IPs.Easier to understand exactly the flow of traffic inside of your stack. It has nothing to do with driving IPv6 adoption across the entire globe and just an acknowledgement that we no longer live in a world where you have to accept the weird limitations of IPv4 in a universe where you may need 10,000 IPs suddenly with very little warning. The benefits for organizations with public IPv6 addresses is pretty obvious, but there's enough value there for cloud providers and users that even the corporate overlords might get behind it. AWS never needs to try and scrounge up more private IPv4 space inside of a VPC. That's gotta be worth something. The common rebuttal to these ideas is, "Kubernetes is an open platform, so the community can build these solutions." While true, this argument misses a crucial point: defaults are the most powerful force in technology. The "happy path" defined by the core project dictates how 90% of users will interact with it. If the system defaults to expecting signed packages and provides a robust, native way to manage them, that is what the ecosystem will adopt.This is an ambitious list, I know. But if we're going to dream, let's dream big. After all, we're the industry that thought naming a technology 'Kubernetes' would catch on, and somehow it did!We see this all the time in other areas like mobile developer and web development, where platforms assess their situation and make  jumps forward. Not all of these are necessarily projects that the maintainers or companies  take on but I think they're all ideas that  should at least revisit and think "is it worth doing now that we're this nontrivial percentage of all datacenter operations on the planet"? ]]></content:encoded></item><item><title>Guess I&apos;m a rationalist now</title><link>https://scottaaronson.blog/?p=8908</link><author>nsoonhui</author><category>hn</category><pubDate>Thu, 19 Jun 2025 10:22:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A week ago I attended LessOnline, a rationalist blogging conference featuring many people Iâ€™ve known for yearsâ€”Scott Alexander, Eliezer Yudkowsky, Zvi Mowshowitz, Sarah Constantin, Carl Feynmanâ€”as well as people Iâ€™ve known only online and was delighted to meet in person, like Joe Carlsmith and Jacob Falkovich and Daniel Reeves.  The conference was at Lighthaven, a bewildering maze of passageways, meeting-rooms, sleeping quarters, gardens, and vines off Telegraph Avenue in Berkeley, which has recently emerged as the nerd Shangri-La, or Galtâ€™s Gulch, or Shire, or whatever.  I did two events at this yearâ€™s LessOnline: a conversation with Nate Soares about the Orthogonality Thesis, and an ask-me-anything session about quantum computing and theoretical computer science (no new ground there for regular consumers of my content).What Iâ€™ll remember most from LessOnline is not the sessions, mine or othersâ€™, but the unending conversation among hundreds of people all over the grounds, which took place in parallel with the sessions and before and after them, from morning till night (and through the night, apparently, though Iâ€™ve gotten too old for that).  It felt like a single conversational archipelago, the largest in which Iâ€™ve ever taken part, and the conferenceâ€™s real point.  (Attendees were exhorted, in the opening session, to skip as many sessions as possible in favor of intense small-group conversationsâ€”not only because it was better but also because the session rooms were too small.)Within the conversational blob, just making my way from one building to another could take hours.  My mean free path was approximately five feet, before someone would notice my nametag and stop me with a question.  Here was my favorite opener:â€œYouâ€™re Scott Aaronson?!  The quantum physicist whoâ€™s always getting into arguments on the Internet, and whoâ€™s essentially always right, but who sustains an unreasonable amount of psychic damage in the process?â€â€œYes,â€ I replied, not bothering to correct the â€œphysicistâ€ part.One night, I walked up to Scott Alexander, who sitting on the ground, with his large bald head and a blanket he was using as a robe, resembled a monk.  â€œAre you enjoying yourself?â€ he asked.I replied, â€œyou know, after all these years of being coy about it, I think Iâ€™m finally ready to become a Rationalist.  Is there, like, an initiation ritual or something?â€Scott said, â€œOh, you were already initiated a decade ago; you just didnâ€™t realize it at the time.â€  Then he corrected himself: â€œtwo decades ago.â€The first thing I did, after coming out as a Rationalist, was to get into a heated argument with Other Scott A., Joe  Carlsmith, and other fellow-Rationalists about the ideas I set out twelve years ago in my Ghost in the Quantum Turing Machine essay.  Briefly, my argument was that the irreversibility and ephemerality of biological life, which contrasts with the copyability, rewindability, etc. of programs running on digital computers, and which can ultimately be traced back to microscopic details of the universeâ€™s initial state, subject to the No-Cloning Theorem of quantum mechanics, which then get chaotically amplified during brain activity â€¦ might be a clue to a deeper layer of the world, one that we understand about as well as the ancient Greeks understood Newtonian physics, but which is the layer where mysteries like free will and consciousness will ultimately need to be addressed.I got into this argument partly because it came up, but partly also because this seemed like the biggest conflict between my beliefs and the consensus of my fellow Rationalists.  Maybe part of me wanted to demonstrate that my intellectual independence remained intactâ€”sort of like a newspaper that gets bought out by a tycoon, and then immediately runs an investigation into the tycoonâ€™s corruption, as well as his diaper fetish, just to prove it can.The funny thing, though, is that all my beliefs are the same as they were before.  Iâ€™m still a computer scientist, an academic, a straight-ticket Democratic voter, a liberal Zionist, a Jew, etc. (all identities, incidentally, well-enough represented at LessOnline that I donâ€™t even think I was the unique attendee in the intersection of them all).Given how much I resonate with what the Rationalists are trying to do, why did it take me so long to identify as one?Firstly, while 15 years ago I shared the Rationalistsâ€™ interests, sensibility, and outlook, and their stances on most issues, I also found them bizarrely, inexplicably obsessed with the question of whether AI would soon become superhumanly powerful and change the basic conditions of life on earth, and with how to make the AI transition go well.  Why , as opposed to all the other sci-fi scenarios one could worry about, not to mention all the nearer-term risks to humanity?Suffice it to say that empirical developments have since caused me to withdraw my objection.  Sometimes weird people are weird merely because they see the future sooner than others.  Indeed, it seems to me that the biggest thing the Rationalists got wrong about AI was to  how soon the revolution would happen, and to overestimate how many new ideas would be needed for it (mostly, as we now know, it just took lots more compute and training data).  Now that I, too, spend some of my time working on AI alignment, I was able to use LessOnline in part for research meetings with colleagues.A second reason I didnâ€™t identify with the Rationalists was cultural: they were, and are, centrally a bunch of twentysomethings who â€œworkâ€ at an ever-changing list of Berkeley- and San-Francisco-based â€œorgsâ€ of their own invention, and who live in group houses where they explore their exotic sexualities, gender identities, and fetishes, sometimes with the aid of psychedelics.  I, by contrast, am a straight, monogamous, middle-aged tenured professor, married to another such professor and raising two kids who go to normal schools.  Hanging out with the Rationalists always makes me feel older and younger at the same time.So what changed?  For one thing, with the march of time, a significant fraction of Rationalists now have marriages, children, or bothâ€”indeed, a highlight of LessOnline was the many adorable toddlers running around the Lighthaven campus.  Rationalists are successfully reproducing!  Some because of explicit pronatalist ideology, or because they were persuaded by Bryan Caplanâ€™s arguments in .  But others simply because of the same impulses that led their ancestors to do the same for eons.  And perhaps because, like the Mormons or Amish or Orthodox Jews, but unlike typical secular urbanites, the Rationalists in something.  For all their fears around AI, they donâ€™t  doomy, but buzz with ideas about how to build a better world for the next generation.At a LessOnline parenting session, hosted by Julia Wise, I was surrounded by parents who worry about the same things I do: how do we raise our kids to be independent and agentic yet socialized and reasonably well-behaved, technologically savvy yet not droolingly addicted to iPad games?  What schooling options will let them accelerate in math, save them from the crushing monotony that we experienced?  How much of our own lives should we sacrifice on the altar of our kidsâ€™ â€œenrichment,â€ versus trusting Judith Rich Harris that such efforts quickly hit a point of diminishing returns?A third reason I didnâ€™t identify with the Rationalists was, frankly, that they gave off some (not all) of the vibes of a cult, with Eliezer as guru.  Eliezer writes in parables and koans.  He teaches that the fate of life on earth hangs in the balance, that the select few who understand the stakes have the terrible burden of steering the future.  Taking what Rationalists call the â€œoutside view,â€ how good is the track record for this sort of thing?OK, but what did I actually see at Lighthaven?  I saw something that seemed to resemble a cult only insofar as the Beatniks, the Bloomsbury Group, the early Royal Society, or any other community that believed in something did.  When Eliezer himselfâ€”the bearded, cap-wearing Moses who led the nerds from bondage to their Promised Land in Berkeleyâ€”showed up, he was argued with like anyone else.  Eliezer has in any case largely passed his staff to a new generation: Nate Soares and Zvi Mowshowitz have found new and, in various ways, better ways of talking about AI risk; Scott Alexander has for the last decade written the blog thatâ€™s the communityâ€™s intellectual center; figures from Kelsey Piper to Jacob Falkovich to Aella have taken Rationalism in new directions, from mainstream political engagement to the â€¦ err â€¦ statistical analysis of orgies.Iâ€™ll say this, though, on the naysayersâ€™ side: itâ€™s  hard to make dancing to AI-generated pop songs about Bayesâ€™ theorem and Tarskiâ€™s definition of truth not feel cringe, as I can now attest from experience.The cult thing brings me to the deepest reason I hesitated for so long to identify as a Rationalist: namely, I was scared that if I did, people whose approval I craved (including my academic colleagues, but also just randos on the Internet) would sneer at me.  For years, I searched of some way of explaining this communityâ€™s appeal so reasonable that it would silence the sneers.It took years of psychological struggle, and (frankly) solidifying my own place in the world, to follow the true path, which of course is not to give a shit what some haters think of my life choices.  Consider: five years ago, it felt obvious to me that the entire Rationalist community might be about to implode, under existential threat from Cade Metzâ€™s  article, as well as RationalWiki and SneerClub and all the others laughing at the Rationalists and accusing them of every evil.  Yet last week at LessOnline, I saw a community thatâ€™s never been thriving more, with a beautiful real-world campus, excellent writers on every topic who felt like this was the place to be, and even a crop of kids.  How many of the sneerers are living such fulfilled lives?  To judge from their own angry, depressed self-disclosures, probably not many.But are the sneerers right that, even if the Rationalists are enjoying their own lives, theyâ€™re making other peopleâ€™s lives miserable?  Are they closet far-right monarchists, like Curtis Yarvin?  I liked how  put it in its recent, long and (to my mind) devastating profile of Yarvin:The most generous engagement with Yarvinâ€™s ideas has come from bloggers associated with the rationalist movement, which prides itself on weighing evidence for even seemingly far-fetched claims. Their formidable patience, however, has also worn thin. â€œHe never addressed me as an equal, only as a brainwashed person,â€ Scott Aaronson, an eminent computer scientist, said of their conversations. â€œHe seemed to think that if he just gave me one more reading assignment about happy slaves singing or one more monologue about F.D.R., Iâ€™d finally see the light.â€The closest to right-wing politics that I witnessed at LessOnline was a session, with Kelsey Piper and current and former congressional staffers, about the prospects for moderate Democrats to articulate a moderate, pro-abundance agenda that would resonate with the public and finally defeat MAGA.But surely the Rationalists are incels, bitter that they canâ€™t get laid?  Again, the closest I saw was a session where Jacob Falkovich helped a standing-room-only crowd of mostly male nerds confront their fears around dating and understand women better, with Rationalist women eagerly volunteering to answer questions about their perspective.  Gross, right?  (Also, for those already in relationships, Eliezerâ€™s primary consort and former couples therapist Gretta Duleba did a session on relationship conflict.)So, yes, when it comes to the Rationalists, Iâ€™m going to believe my own lying eyes over the charges of the sneerers.  The sneerers can even say about me, in their favorite formulation, that Iâ€™ve â€œgone mask off,â€ confirmed the horrible things theyâ€™ve always suspected.  Yes, the mask is offâ€”and beneath the mask is the same person I always was, who has an inordinate fondness for the Busy Beaver function and the complexity class BQP/qpoly, and who uses too many filler words and moves his hands too much, and who strongly supports the Enlightenment, and who once feared that his best shot at happiness in life would be to earn womenâ€™s pity rather than their contempt.  Incorrectly, as Iâ€™m glad to report.  From my nebbishy nadir to the present, a central thing thatâ€™s changed is that, from my family to my academic colleagues to the Rationalist community to my blog readers, I finally found some people who want what I have to sell.My replies to comments on this post might be light, as Iâ€™ll be accompanying my daughter on a school trip to the Galapagos Islands!A few weeks ago, I was â€œambushedâ€ into leading a session on philosophy and theoretical computer science at UT Austin.  (I.e., asked to show up for the session, but thought Iâ€™d just be a participant rather than the main event.)  The session was then recorded and placed on YouTubeâ€”and surprisingly, given the circumstances, some people seemed to like it!Friend-of-the-blog Alon Rosen has asked me to announce a call for nominations for a new theoretical computer science prize, in memory of my former professor (and fellow TCS blogger) Luca Trevisan, who was lost to the world too soon.And one more: Mahdi Cheraghchi has asked me to announce the STOCâ€™2025 online poster session, registration deadline June 12; see here for more.  Incidentally, Iâ€™ll be at STOC in Prague to give a plenary on quantum algorithms; I look forward to meeting any readers who are there!]]></content:encoded></item><item><title>Show HN: Claude Code Usage Monitor â€“ real-time tracker to dodge usage cut-offs</title><link>https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor</link><author>Maciej-roboblog</author><category>dev</category><category>hn</category><pubDate>Thu, 19 Jun 2025 09:46:43 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I kept slamming into Claude Code limits mid-session and couldnâ€™t find a quick way to see how close I was getting, so I hacked together a tiny local tracker.Streams your prompt + completion usage in real timePredicts whether youâ€™ll hit the cap before the session endsRuns 100 % locally (no auth, no server)Presets for Pro, Max Ã— 5, Max Ã— 20 â€” tweak a JSON if your planâ€™s differentItâ€™s already spared me a few â€œwhy did my run just stop?â€ moments, but itâ€™s still rough around the edges. Feedback, bug reports, and PRs welcome!]]></content:encoded></item><item><title>Base44 sells to Wix for $80M cash</title><link>https://techcrunch.com/2025/06/18/6-month-old-solo-owned-vibe-coder-base44-sells-to-wix-for-80m-cash/</link><author>myth_drannon</author><category>hn</category><pubDate>Thu, 19 Jun 2025 09:31:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Thereâ€™s a lot of talk in the startup world about how AI makes individuals so productive that it could give rise to a generation of â€œsolo unicornsâ€ â€” one-person companies worth over $1 billion.While an actual solo unicorn remains a mythical creature, Israeli developer Maor Shlomo provided compelling evidence Wednesday that the concept might not be impossible.Â Shlomo sold his 6-month-old, bootstrapped vibe-coding startup Base44 to Wix for $80 million, Wix announced Wednesday. And the deal was cash, Wix confirmed to TechCrunch.Â Admittedly, this wasnâ€™t a billion dollars or close to it. And Shlomo wasnâ€™t truly solo â€” he had eight employees, Wix confirmed. They will collectively receive $25 million of the $80 million as a â€œretentionâ€ bonus. Wix declined to give details on that part of the deal, like how long they have to stay in their jobs to get full payouts.In its six months as a stand-alone company, Base44 reportedly grew to 250,000 users, hitting 10,000 users within its first three weeks. According to Shlomoâ€™s posts on X and LinkedIn, the company was profitable, generating $189,000 in profit in May even after covering high LLM token costs, which he also documented publicly.Base44 spread mostly through word of mouth as Shlomo, a 31-year-old programmer, shared his building journey on LinkedIn and Twitter. The project began as a side venture, he told Israeli tech news site CTech.Â Â â€œBase44 is a moonshot experiment â€” helping everyone, technical or not, build software without coding at all,â€ he explained on LinkedIn when he launched it to the public.Itâ€™s one of the newer crop of vibe-coding products designed for non-programmers. Users enter text prompts, and the platform builds complete applications, with database, storage, authentication, analytics, and integration. It also supports email, texting, and maps, with a roadmap for more enterprise-grade security support.Base44 isnâ€™t unique in this area. Other vibe coders like Adaptive Computer handle similar infrastructure work. But Base44â€™s fast rise was astounding all the same.Shlomo was already known in the Israeli startup community through his previous startup, the Insight Partners-backed data analytics company Explorium. His brother is also a co-founder of an AI security startup, Token Security, which just raised $20 million led by Notable Capital (formerly GGV Capital) and a bunch of Israeli tech angels.He quickly gained partnership agreements  for Base44 with big Israeli tech companies like eToro and Similarweb.After posting about his decision to use Anthropicâ€™s Claude LLM through AWS instead of models by OpenAI â€” mostly for cost-per-performance reasons â€” Amazon invited Base44 to demo at a Tel Aviv AWS event last month, which Shlomo documented.â€œCrazy f***ing journey so far,â€ Shlomo posted on LinkedIn when announcing the news of the acquisition. Despite the growth and the profits â€” or really because of it â€” he sold his still-bootstrapped company because â€œthe scale and volume we need is not something we can organically grow intoÂ â€¦ If we were able to get so far organically, bootstrapped, Iâ€™m excited to see our new pace now that we have all the resources in place,â€ he wrote.For its part, Wix picked up a proven, fast-growing, local vibe-coding platform for a relative song because of its youth. OpenAI paid $3 billion for Windsurf, which was founded in 2021.Â Wix, of course, offers no-code website building that look professionally designed. Adding a profitable LLM vibe-coding product to its offerings is a logical move.Shlomo could not be immediately reached for additional comment.]]></content:encoded></item><item><title>From LLM to AI Agent: What&apos;s the Real Journey Behind AI System Development?</title><link>https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent</link><author>codelink</author><category>hn</category><pubDate>Thu, 19 Jun 2025 09:29:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[AI agents are a hot topic, but not every AI system needs to be one.While agents promise autonomy and decision-making power, simpler & more cost-saving solutions better serve many real-world use cases. The key lies in choosing the right architecture for the problem at hand.In this post, we'll explore recent developments in Large Language Models (LLMs) and discuss key concepts of AI systems.We've worked with LLMs across projects of varying complexity, from zero-shot prompting to chain-of-thought reasoning, from RAG-based architectures to sophisticated workflows and autonomous agents.This is an emerging field with evolving terminology. The boundaries between different concepts are still being defined, and classifications remain fluid. As the field progresses, new frameworks and practices emerge to build more reliable AI systems.To demonstrate these different systems, we'll walk through a familiar use case â€“ a resume-screening application â€“ to reveal the unexpected leaps in capability (and complexity) at each level.A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It excels at tasks involving this stored knowledge: summarizing novels, writing essays about global warming, explaining special relativity to a 5-year-old, or composing haikus.However, without additional capabilities, an LLM cannot provide real-time information like the current temperature in NYC. This distinguishes pure LLMs from chat applications like ChatGPT, which enhance their core LLM with real-time search and additional tools.That said, not all enhancements require external context. There are several prompting techniques, including in-context learning and few-shot learning that help LLMs tackle specific problems without the need of context retrieval.To check if a resume is a good fit for a job description, an LLM with one-shot prompting and in-context learning can be utilized to classify it as Passed or Failed.RAG (Retrieval Augmented Generation)Retrieval methods enhance LLMs by providing relevant context, making them more current, precise, and practical. You can grant LLMs access to internal data for processing and manipulation. This context allows the LLM to extract information, create summaries, and generate responses. RAG can also incorporate real-time information through the latest data retrieval.The resume screening application can be improved by retrieving internal company data, such as engineering playbooks, policies, and past resumes, to enrich the context and make better classification decisions.Retrieval typically employs tools like vectorization, vector databases, and semantic search.LLMs can automate business processes by following well-defined paths. They're most effective for consistent, well-structured tasks.Tool use enables workflow automation. By connecting to APIs, whether for calculators, calendars, email services, or search engines, LLMs can leverage reliable external utilities instead of relying on their internal, non-deterministic capabilities.An AI workflow can connect to the hiring portal to fetch resumes and job descriptions â†’ Evaluate qualifications based on experience, education, and skills â†’ Send appropriate email responses (rejection or interview invitation).For this resume scanning workflow, the LLM requires access to the database, email API, and calendar API. It follows predefined steps to automate the process programmatically.AI Agents are systems that reason and make decisions independently. They break down tasks into steps, use external tools as needed, evaluate results, and determine the following actions: whether to store results, request human input, or proceed to the next step.This represents another layer of abstraction above tool use & AI workflow, automating both planning and decision-making.While AI workflows require explicit user triggers (like button clicks) and follow programmatically defined paths, AI Agents can initiate workflows independently and determine their sequence and combination dynamically.An AI Agent can manage the entire recruitment process, including parsing CVs, coordinating availability via chat or email, scheduling interviews, and handling schedule changes.This comprehensive task requires the LLM to access databases, email and calendar APIs, plus chat and notification systems.1. Not every system requires an AI agentStart with simple, composable patterns and add complexity as needed. For some systems, retrieval alone suffices. In our resume screening example, a straightforward workflow works well when the criteria and actions are clear. Consider an Agent approach only when greater autonomy is needed to reduce human intervention.2. Focus on reliability over capabilityThe non-deterministic nature of LLMs makes building dependable systems challenging. While creating proofs of concept is quick, scaling to production often reveals complications. Begin with a sandbox environment, implement consistent testing methods, and establish guardrails for reliability.]]></content:encoded></item><item><title>SpaceX Starship 36 Anomaly</title><link>https://twitter.com/NASASpaceflight/status/1935548909805601020</link><author>Ankaios</author><category>hn</category><pubDate>Thu, 19 Jun 2025 04:49:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Elliptic Curves as Art</title><link>https://elliptic-curves.art/</link><author>nill0</author><category>hn</category><pubDate>Thu, 19 Jun 2025 04:02:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mathematicians hunting prime numbers discover infinite new pattern</title><link>https://www.scientificamerican.com/article/mathematicians-hunting-prime-numbers-discover-infinite-new-pattern-for/</link><author>georgecmu</author><category>hn</category><pubDate>Thu, 19 Jun 2025 03:28:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For centuries, prime numbers have captured the imaginations of mathematicians, who continue to search for new patterns that help identify them and the way theyâ€™re distributed among other numbers. Primes are whole numbers that are greater than 1 and are divisible by only 1 and themselves. The three smallest prime numbers are 2, 3 and 5. It's easy to find out if small numbers are primeâ€”one simply needs to check what numbers can factor them. When mathematicians consider large numbers, however, the task of discerning which ones are prime quickly mushrooms in difficulty. Although it might be practical to check if, say, the numbers 10 or 1,000 have more than two factors, that strategy is unfavorable or even untenable for checking if gigantic numbers are prime or composite. For instance, the largest known prime number, which is 2 âˆ’ 1, is 41,024,320 digits long. At first, that number may seem mind-bogglingly large. Given that there are infinitely many positive integers of all different sizes, however, this number is minuscule compared with even larger primes.Furthermore, mathematicians want to do more than just tediously attempt to factor numbers one by one to determine if any given integer is prime. â€œWeâ€™re interested in the prime numbers because there are infinitely many of them, but itâ€™s very difficult to identify any patterns in them,â€ says Ken Ono, a mathematician at the University of Virginia. Still, one main goal is to determine how prime numbers are distributed within larger sets of numbers.Recently, Ono and two of his colleaguesâ€”William Craig, a mathematician at the U.S. Naval Academy, and Jan-Willem van Ittersum, a mathematician at the University of Cologne in Germanyâ€”identified a whole new approach for finding prime numbers. â€œWe have described infinitely many new kinds of criteria for exactly determining the set of prime numbers, all of which are very different from â€˜If you canâ€™t factor it, it must be prime,â€™â€ Ono says. He and his colleaguesâ€™ paper, published in the Proceedings of the National Academy of Sciences USA was runner-up for a physical science prize that recognizes scientific excellence and originality. In some sense, the finding offers an infinite number of new definitions for what it means for numbers to be prime, Ono notes.On supporting science journalismIf you're enjoying this article, consider supporting our award-winning journalism bysubscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.At the heart of the teamâ€™s strategy is a notion called integer partitions. â€œThe theory of partitions is very old,â€ Ono says. It dates back to the 18th-century Swiss mathematician Leonhard Euler, and it has continued to be expanded and refined by mathematicians over time. â€œPartitions, at first glance, seem to be the stuff of childâ€™s play,â€ Ono says. â€œHow many ways can you add up numbers to get other numbers?â€ For instance, the number 5 has seven partitions: 4 + 1, 3 + 2, 3 + 1 + 1, 2 + 2 + 1, 2 + 1 + 1 + 1 and 1 + 1 + 1 + 1 + 1.Yet the concept turns out to be powerful as a hidden key that unlocks new ways of detecting primes. â€œIt is remarkable that such a classical combinatorial objectâ€”the partition functionâ€”can be used to detect primes in this novel way,â€ says Kathrin Bringmann, a mathematician at the University of Cologne. (Bringmann has worked with Ono and Craig before, and sheâ€™s currently van Ittersumâ€™s postdoctoral adviser, but she wasnâ€™t involved with this research.) Ono notes that the idea for this approach originated in a question posed by one of his former students, Robert Schneider, whoâ€™s now a mathematician at Michigan Technological University.Ono, Craig and van Ittersum proved that prime numbers are the solutions of an infinite number of a particular type of polynomial equation in partition functions. Named Diophantine equations after third-century mathematician Diophantus of Alexandria (and studied long before him), these expressions can have integer solutions or rational ones (meaning they can be written as a fraction). In other words, the finding shows that â€œinteger partitions detect the primes in infinitely many natural ways,â€ the researchers wrote in their paper.George Andrews, a mathematician at Pennsylvania State University, who edited the  paper but wasnâ€™t involved with the research, describes the finding as â€œsomething that's brand newâ€ and â€œnot something that was anticipated,â€ making it difficult to predict â€œwhere it will lead.â€The discovery goes beyond probing the distribution of prime numbers. â€œWeâ€™re actually nailing all the prime numbers on the nose,â€ Ono says. In this method, you can plug an integer that is 2 or larger into particular equations, and if they are true, then the integer is prime. One such equation is (3âˆ’ 13 + 18âˆ’ 8)() + (12 âˆ’ 120 + 212)() âˆ’ 960() = 0, where (), () and () are well-studied partition functions. â€œMore generally,â€ for a particular type of partition function, â€œwe prove that there are infinitely many such prime detecting equations with constant coefficients,â€ the researchers wrote in their paper. Put more simply, â€œitâ€™s almost like our work gives you infinitely many new definitions for prime,â€ Ono says. â€œThatâ€™s kind of mind-blowing.â€The teamâ€™s findings could lead to many new discoveries, Bringmann notes. â€œBeyond its intrinsic mathematical interest, this work may inspire further investigations into the surprising algebraic or analytic properties hidden in combinatorial functions,â€ she says. In combinatoricsâ€”the mathematics of countingâ€”combinatorial functions are used to describe the number of ways that items in sets can be chosen or arranged. â€œMore broadly, it shows the richness of connections in mathematics,â€ she adds. â€œThese kinds of results often stimulate fresh thinking across subfields.â€Bringmann suggests some potential ways that mathematicians could build on the research. For instance, they could explore what other types of mathematical structures could be found using partition functions or look for ways that the main result could be expanded to study different types of numbers. â€œAre there generalizations of the main result to other sequences, such as composite numbers or values of arithmetic functions?â€ she asks.â€œKen Ono is, in my opinion, one of the most exciting mathematicians around today,â€ Andrews says. "This isnâ€™t the first time that he has seen into a classic problem and brought really new things to light.â€There remains a glut of open questions about prime numbers, many of which are long-standing. Two examples are the twin prime conjecture and Goldbachâ€™s conjecture. The twin prime conjecture states that there are infinitely many twin primesâ€”prime numbers that are separated by a value of two. The numbers 5 and 7 are twin primes, as are 11 and 13. Goldbachâ€™s conjecture states that â€œevery even number bigger than 2 is a sum of two primes in at least one way,â€ Ono says. But no one has proven this conjecture to be true.â€œProblems like that have befuddled mathematicians and number theorists for generations, almost throughout the entire history of number theory,â€ Ono says. Although his teamâ€™s recent finding doesnâ€™t solve those problems, he says, itâ€™s a profound example of how mathematicians are pushing boundaries to better understand the mysterious nature of prime numbers.]]></content:encoded></item><item><title>The Zed Debugger Is Here</title><link>https://zed.dev/blog/debugger</link><author>SupremumLimit</author><category>hn</category><pubDate>Thu, 19 Jun 2025 02:42:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Over 2,000 developers asked, and we delivered.Debugging in Zed is now a realityâ€”and it's a big leap toward Zed 1.0.We set out to build a debugger with three primary focuses:Fast: Spend less time context switching and more time debuggingFamiliar: In line with Zed's design language and supports everything expected from a typical debugger flowConfigurable: You're able to customize the UI, keybindings, debug configurations and moreOut of the box, Zed supports debugging popular languages including Rust, C/C++, JavaScript, Go, and Python.
With our extension system, Zed can support any debug adapter that implements the Debug Adapter Protocol (DAP).To simplify the setup process, we've introduced locators, a system that translates build configurations into debug configurations. Meaning that you can write a build task once in  and reference it from  â€” or, even better, rely on Zed's automatic configuration.Zed automatically runs locators on built-in or language server-generated runnables, so in many cases you won't even need to write a debug configuration to get up and running.We currently support locators for Cargo, Python, JavaScript, and Go, with more coming in the future.
For more information on configuring a debug session, see our documentation.Once in a debug session, Zed makes it easy to inspect your program's state, such as threads, variables, breakpoints, the call stack, and more.Setting some breakpoints and running the test in a debug session.The debugger panel is fully customizable too, just drag and rearrange tabs in whatever order you want; you can even move the debug panel around so it fits your workflow.Zed also supports keyboard-driven debugging for users that prefer to keep their hands on the keyboard.
You can step through code, toggle breakpoints, and navigate a debug session without ever touching the mouse.Navigating through the Debugger surfaces using only the keyboard.Special thanks to Remco Smits for driving a lot of the heavy lifting on this projectâ€”your contributions have been critical to getting us here.Zed's debugger supports debugging a variety of languages through the Debug Adapter Protocol.
But simply implementing the protocol wasn't enoughâ€”we needed an architecture that could scale to collaborative debugging, support extensions, and efficiently cache and manage responses from debug adapters.To achieve this, we built a two-layer architecture: a data layer that communicates directly with the debug adapters, and a UI layer that fetches data from the data layer to render the interface.This separation means the UI layer only requests what it needs, allowing the data layer to lazily fetch information and avoid unnecessary requests.
It also makes the data layer solely responsible for maintaining session state, caching responses, and invalidating stale data.
This architecture will make implementing collaborative debugging significantly easier, since the same UI code can be reused across multiplayer sessionsâ€”and we only send essential data across the wire, preserving bandwidth.Supporting every debug adapter out of the box wasn't feasibleâ€”there are over 70 DAP implementations, each with its own quirks.
To solve this, we extended Zed's extension API to support debugger integration.Adding DAP support via an extension involves defining a custom schema that integrates with our JSON server, implementing logic for downloading and launching the adapter, processing debug configuration to add sane default values, and integrating with locators for automatic configuration.
This design follows our approach to LSP extensions, giving extension authors full control to bring their own debug adapters to Zed with minimal friction.We also wanted inline variable values to work out of the box.
Surprisingly, the inline values request is a part of the Language Server Protocol (LSP) instead of the DAP.
Using the inline values approach would limit Zed to only showing inline values for DAPs which integrate with LSPs, which isn't many.
A naive workaround might be to use regular expressions to match variable names between the source code and debugger values, but that quickly breaks down when dealing with scopes, and comments.
Instead, we turned to Tree-sitter. After all Zed is built by the creators of Tree-sitter!Through Tree-sitter queries, we can accurately identify variables within the current execution scope, and easily support any language through  files without relying on an LSP server to be tightly integrated with a debug adapter.
At launch, inline values are supported for Python, Rust, and Go.
More languages will be supported in the coming weeks.When we set out to build the debugger, we wanted to make it seamless to use, out of the way, and in line with Zed's high standard of quality.
Now that we've built a strong foundation that is compatible with any debug adapter, we're ready to explore and implement advanced features such as:New views: While we support all the fundamental views, we're planning on adding more advanced views such as a watch list, memory view, disassembly view, and a stack trace viewAutomatic configuration: We're going to add support for more languages and build systems]]></content:encoded></item><item><title>TI to invest $60B to manufacture foundational semiconductors in the U.S.</title><link>https://www.ti.com/about-ti/newsroom/news-releases/2025/texas-instruments-plans-to-invest-more-than--60-billion-to-manufacture-billions-of-foundational-semiconductors-in-the-us.html</link><author>TMWNN</author><category>hn</category><pubDate>Thu, 19 Jun 2025 01:50:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Unleashing whatâ€™s next in American innovationToday, TI is the largest foundational semiconductor manufacturer in the U.S., producing analog and embedded processing chips that are critical for smartphones, vehicles, data centers, satellites and nearly every other electronic device. In order to meet the steadily growing demand for these essential chips, TI is building on its legacy of technology leadership and expanding its U.S. manufacturing presence to help its customers pioneer the next wave of technological breakthroughs.Igniting intelligence with Appleâ€œTexas Instruments' American-made chips help bring Apple products to life, and together, weâ€™ll continue to create opportunity, drive innovation, and invest in the future of advanced manufacturing across the U.S.,â€ said Appleâ€™s CEO Tim Cook.Fueling the future with FordFord and TI are working to strengthen American manufacturing, combining Fordâ€™s automotive expertise with TIâ€™s semiconductor technology to help drive innovation and secure a robust, domestic supply chain for the future of mobility. â€œAt Ford, 80% of the vehicles we sell in the U.S. are assembled in the U.S., and we are proud to stand with technology leaders like TI that continue to invest in manufacturing in the U.S.,â€ said Jim Farley, President and CEO of Ford Motor Company.Connecting patient care with MedtronicMedtronic and TI are partnering to improve lives when it matters most. â€œAt Medtronic, our life-saving medical technologies rely on semiconductors to deliver precision, performance, and innovation at scale,â€ said Geoff Martha, Medtronic chairman and CEO. â€œTexas Instruments has been a vital partner â€“ especially during the global chip shortages â€“ helping us maintain supply continuity and accelerate the development of breakthrough therapies. Weâ€™re proud to leverage TIâ€™s U.S.-manufactured semiconductors as we work to transform healthcare and improve outcomes for patients around the world.â€NVIDIA is partnering with TI to unleash the next generation of artificial intelligence architectures. â€œNVIDIA and TI share the goal to revitalize U.S. manufacturing by building more of the infrastructure for AI factories here in the U.S.,â€ said Jensen Huang, founder and CEO of NVIDIA. â€œWe look forward to continuing our collaboration with TI by developing products for advanced AI infrastructure.â€Securing high-speed satellite internet with SpaceXSpaceX is increasingly leveraging TIâ€™s high-speed process technology to connect its Starlink satellite internet service with TIâ€™s latest 300mm SiGe technology manufactured in Sherman, Texas. â€œOur fundamental mission is to revolutionize global connectivity and eliminate the digital divide. Core to this mission is constantly pushing the boundaries of what is possible,â€ said Gwynne Shotwell, president and COO of SpaceX. â€œSpaceX is manufacturing tens of thousands of Starlink kits a day â€“ all right here in the U.S. â€“ and we are making huge investments in PCB manufacturing and silicon packaging to expand even further. TIâ€™s U.S.-made semiconductors are crucial for securing a U.S. supply chain for our products, and their advanced silicon manufacturing capabilities provide the performance and reliability needed to help us meet the growing demand for high-speed internet all around the world.â€]]></content:encoded></item><item><title>Andrej Karpathy: Software in the era of AI [video]</title><link>https://www.youtube.com/watch?v=LCEmiRjPEtQ</link><author>sandslash</author><category>hn</category><pubDate>Thu, 19 Jun 2025 00:33:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MCP Specification â€“ version 2025-06-18 changes</title><link>https://modelcontextprotocol.io/specification/2025-06-18/changelog</link><author>owebmaster</author><category>hn</category><pubDate>Wed, 18 Jun 2025 23:59:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This document lists changes made to the Model Context Protocol (MCP) specification since
the previous revision, 2025-03-26.Add  field to , providing for completion requests to include
previously-resolved variables (PR #598).Add  field for human-friendly display names, so that  can be used as a programmatic
identifier (PR #663)For a complete list of all changes that have been made since the last protocol revision,
see GitHub.]]></content:encoded></item><item><title>Show HN: Unregistry â€“ â€œdocker pushâ€ directly to servers without a registry</title><link>https://github.com/psviderski/unregistry</link><author>psviderski</author><category>dev</category><category>hn</category><pubDate>Wed, 18 Jun 2025 23:17:10 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I got tired of the push-to-registry/pull-from-registry dance every time I needed to deploy a Docker image.In certain cases, using a full-fledged external (or even local) registry is annoying overhead. And if you think about it, there's already a form of registry present on any of your Docker-enabled hosts â€” the Docker's own image storage.So I built Unregistry [1] that exposes Docker's (containerd) image storage through a standard registry API. It adds a `docker pussh` command that pushes images directly to remote Docker daemons over SSH. It transfers only the missing layers, making it fast and efficient.  docker pussh myapp:latest user@server

Under the hood, it starts a temporary unregistry container on the remote host, pushes to it through an SSH tunnel, and cleans up when done.I've built it as a byproduct while working on Uncloud [2], a tool for deploying containers across a network of Docker hosts, and figured it'd be useful as a standalone project.Would love to hear your thoughts and use cases!]]></content:encoded></item><item><title>New US visa rules will force foreign students to unlock social media profiles</title><link>https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening</link><author>sva_</author><category>hn</category><pubDate>Wed, 18 Jun 2025 23:11:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Foreign students will be required to unlock their social media profiles to allow US diplomats to review their online activity before receiving educational and exchange visas, the state department has announced. Those who fail to do so will be suspected of hiding that activity from US officials.The new guidance, unveiled by the state department on Wednesday, directs US diplomats to conduct an online presence review to look for â€œany indications of hostility toward the citizens, culture, government, institutions, or founding principles of the United Statesâ€.A cable separately obtained by Politico also instructs diplomats to flag any â€œadvocacy for, aid or support for foreign terrorists and other threats to US national securityâ€ and â€œsupport for unlawful antisemitic harassment or violenceâ€.The screening for â€œantisemiticâ€ activity matches similar guidance given at US Citizenship and Immigration Services under the Department of Homeland Security and has been criticised as an effort to crack down on opposition to the conduct of Israelâ€™s war in Gaza.The new state department checks are directed at students and other applicants for visas in the F, M and J categories, which refer to academic and vocational education, as well as cultural exchanges.â€œIt is an expectation from American citizens that their government will make every effort to make our country safer, and that is exactly what the Trump administration is doing every single day,â€ said a senior state department official, adding that Marco Rubio was â€œhelping to make America and its universities safer while bringing the state Department into the 21st centuryâ€.The Trump administration paused the issuance of new education visas late last month as it mulled new social media vetting strategies. The US had also targeted Chinese students for special scrutiny amid a tense negotiation over tariffs and the supply of rare-earth metals and minerals to the United States.The state department directive allowed diplomatic posts to resume the scheduling of interviews for educational and exchange visas, but added that consular officers would conduct a â€œcomprehensive and thorough vettingâ€ of all applicants applying for F, M and J visas.â€œTo facilitate this vetting, all applicants for F, M and J non-immigrant visas will be asked to adjust the privacy settings on all their social media profiles to â€˜publicâ€™â€, the official said. â€œThe enhanced social media vetting will ensure we are properly screening every single person attempting to visit our country.â€]]></content:encoded></item><item><title>Fang, the CLI Starter Kit</title><link>https://github.com/charmbracelet/fang</link><author>bewuethr</author><category>hn</category><pubDate>Wed, 18 Jun 2025 22:40:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Missing 11th of the Month (2015)</title><link>https://drhagen.com/blog/the-missing-11th-of-the-month/</link><author>xk3</author><category>hn</category><pubDate>Wed, 18 Jun 2025 21:45:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[On November 28th, 2012, Randall Munroe published an xkcdÂ comicÂ that wasÂ a calendar in whichÂ the size of each date wasÂ proportional toÂ how often each date is referenced by its ordinal nameÂ (e.g. "October 14th") in the Google Ngrams database since 2000. Most of the largeÂ days are pretty much what you would expect:Â July 4th, December 25th, the 1st of every month, the last day of most months, and of course a September 11th that shovesÂ its neighborsÂ into the margins. There are not many days that seem to be smaller than the typical size. February 29th is a tiny speck, for instance. But ifÂ you stare at the comicÂ long enough, you may get the impression that the 11th of mostÂ months is unusually small. The titleÂ text of the comic concurs, reading "In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why." After digging into the raw data, I believe I have figured out why.First I confirmedÂ that the  is actually interesting. There are 31 days and one of them  to be smallest. Maybe the  isn't an outlier; it's just on the smaller end and our eyesÂ are picking up on a pattern that doesn't exist. To confirm this is real,Â I compared actual numbers, not text size. The Ngrams database returns the total number times a phrase is mentioned in a given year normalized by the total number of books published that year. The database only goes up to the year 2008, so it is presumably unchanged from when Randall queried it in 2012.IÂ retrieved the count for each day for the year (,  etc.) and took the median over the months for each day (median of , , etc.) forÂ each year. This summarizesÂ how often the  and the other 30 days of the month appear in a given year. Using the median prevents outlier days like  from dragging up the average for itsÂ corresponding ordinalÂ (the ). Only if a ordinalÂ is unusual for at least 6Â of the 12 months will its median appear unusual.I took theÂ median for each ordinal over the years 2000-2008. The graph below is a histogram of the 31 medians. The  of the month stands out far above them allÂ and the  just barely distinguishes itself from the remainder. Being the first day and the middle day of the month, these two make sense.Â However, the  stands out as the lowest by a significantÂ margin (p-value < 0.05), with no immediate explanation.This deficitÂ has been around for a long time. Below is all the ordinals for every year in the data set, 1800-2008. The data is smoothed over elevenÂ years to flatten out the noise. Even at the beginning, the  is significantly lower than the main group. This mild deficit continues for a few decades and then something weird happens in 1860s; theÂ 11th suddenly diverges from its place just below the pack. The gap between the  and the ordinary ordinals expands rapidly until the  is about half of what one would expect it to be throughout the first half of the twentieth century. The gap shrinks in the second half of the twentieth century, but stillÂ persists at a smaller levelÂ untilÂ the end.Astute graph readers will notice that something else weird is going on. There are four other lines that are much lower than they should be. From highest to lowest, they are the , the , the , and the .Â They wereÂ even lower than the  from 1800 untilÂ the 1890s. However, starting around 1900, their gaps started shrinking even as the  diverged until the gap disappeared completely in the 1930s. There is an interesting story there, but because their effect doesn't persist to the present, I'll continue to focus on the  and leave the othersÂ for a future post.When I began this study, I was hoping to find a hidden taboo of holding events on the 11th or typographicalÂ biasÂ against theÂ shorthand ordinal. Alas, the reason is far is far more mundane: a numeral  looks a lot like a capital  or a lowercase  or a lowercase  in most of the fonts used for printing books. An  also looks like an , apparently. Google'sÂ algorithms made mistakes when reading the  from a page, interpretingÂ the ordinalÂ as some other word.We can find some of these mistakes by directly searching for nonsense phrases like  or  or . There are nine possible combinationsÂ of , , and  that a  could be mistaken for. Â Five of them can actually be found in the database for at least one month: , , , , and . Also found was , , and , in which only one letter was misread. I collectively refer to these errors asÂ .Â Google booksÂ queries a newer database than the one on which Ngrams was built, but bona fide examples of the misreads can still be found. Here is something that Google books thinks says : . And here is one for : . And finally oneÂ for : . There are hordes of these in the database. You can find other ordinals that were misread as well, but the  with its slender and ambiguous s was misread far more often than the others.I added back in every instance of , , etc. to  and did the same to the other months. The graph below shows that the  gets a bigÂ boost by adding backÂ theÂ nonsense phrases. Before the 1860s,Â the difference between the  and the main groupÂ is erased. After the 1860s, about a quarter to a third of the difference is erased.So where didÂ the rest of the missing  go? Well, starting in the 1860s, the Google algorithm starts to make a rather peculiar errorâ€”it misreads  as . Here is oneÂ example from a page full of s: . In some years, the number of incorrect reads actually exceeds the number of correct reads. I added  to  and did the same for all the months. The graph below shows both theÂ Â and its sum with the .Â There wasÂ little impactÂ before the 1860s, but then this error alone accounts for nearly all of the missing .When the  misreads andÂ  misreads are both added back intoÂ the , the gap disappears across the entire timeline and the Â looks like an ordinary day of the year.Â This suggests that the misreading of the  as , , , etc. is sufficient to explain the unusually low incidence of the  as a day of the month.While it makes sense that the  wasÂ misread more than others, why is the misread rate not uniform? What happened in the 1860s that caused the dramatic rise in the error rate? I suspect that it has something to do withÂ a special device invented in the 1860s_â€”_the typewriter. The earliest typewriters did not have a separate key for the numeral .Â Typists were expected to use the lowercase  to represent a . When the algorithm read , it was far moreÂ correct than we have beenÂ giving it credit. There are not that many documents in Google books that are typewritten, butÂ this popular new contraption had a powerful effect on the evolution of fonts. The  and  were identical on the increasingly familiar typewriters, andÂ the fonts even of printed materials began to reflect this expectation. Compare the s and s in this font fromÂ 1850: .Â There is a clear difference between an  with no serifs on the top and the  with a pronounced serif. Compare that to a font from 1920: .Â The characters are identical except for the kerning. Even to this day, most fonts represent both the  and the  as tall characters with two serifs on the bottom and one left-facing serif at the top. The only difference is that the serif on the  is slightly more angled than on the . (In this post, I used a special monospace font to make it easier to tell the difference.)Â The print quality of more recent books (post 1970s) has reduced the rate ofÂ failure, but it still has not gone away entirely, so that the remaining failures were noticeable in the xkcd comic.The largest open question is why  was chosen so often. It seems like such a strange error to make. The word  is a legal wordÂ in mathematical and scientific publications, so that should help its chances of getting picked. In most fonts the top of the  is really thin, and is likely invisible in many texts on which they trained the algorithm. But there is a big different in height between  and , especially in the typewriter era, which is where the errors occur. And the phrase  is nonsense so that should have hurt its chances of being selected. Is it possible thereÂ was an error in one of the modern training texts that had an  labeled as , thereby confusing the algorithm? The only way to know for sure would be to crack open the source code of Google's text-reading algorithm. This is left as an exercise for the reader.The code used for the analysis in this post is available onÂ Github.]]></content:encoded></item><item><title>Bento: A Steam Deck in a Keyboard</title><link>https://github.com/lunchbox-computer/bento</link><author>MichaelThatsIt</author><category>hn</category><pubDate>Wed, 18 Jun 2025 21:21:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Websites are tracking you via browser fingerprinting</title><link>https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html</link><author>gnabgib</author><category>hn</category><pubDate>Wed, 18 Jun 2025 20:55:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Clearing your cookies is not enough to protect your privacy online.Â New research led by Texas A&M University found that websites are covertly using browser fingerprinting â€” a method to uniquely identify a web browser â€” to track people across browser sessions and sites.â€œFingerprinting has always been a concern in the privacy community, but until now, we had no hard proof that it was actually being used to track users,â€ said Dr. Nitesh Saxena, cybersecurity researcher, professor of computer science and engineering and associate director of the Global Cyber Research InstituteÂ at Texas A&M. â€œOur work helps close that gap.â€When you visit a website, your browser shares a surprising amount of information, like your screen resolution, time zone, device model and more. When combined, these details create a â€œfingerprintâ€ thatâ€™s often unique to your browser. Unlike cookies â€” which users can delete or block â€” fingerprinting is much harder to detect or prevent. Most users have no idea itâ€™s happening, and even privacy-focused browsers struggle to fully block it.â€œThink of it as a digital signature you didnâ€™t know you were leaving behind,â€ explained co-author Zengrui Liu, a former doctoral student in Saxenaâ€™s lab. â€œYou may look anonymous, but your device or browser gives you away.â€This research marks a turning point in how computer scientists understand the real-world use of browser fingerprinting by connecting it with the use of ads.â€œWhile prior works have studied browser fingerprinting and its usage on differentÂ websites, ours is the first to correlate browser fingerprints and ad behaviors, essentially establishing the relationship between web tracking and fingerprinting,â€ said co-author Dr. Yinzhi Cao, associate professor of computer science and technical director of the Information Security Institute at Johns Hopkins University.]]></content:encoded></item></channel></rss>