<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Ugandan runner Jacob Kiplimo completes first ever sub-57 minute half marathon</title><link>https://www.cnn.com/2025/02/16/sport/jacob-kiplimo-smashes-half-marathon-record-spt-intl/index.html</link><author>mooreds</author><category>hn</category><pubDate>Mon, 17 Feb 2025 02:29:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
            Jacob Kiplimo shattered the half marathon world record with a blistering run on Sunday.
    
            The 24-year-old Ugandan was competing in the Barcelona half marathon where he became the first person ever to complete the distance in under 57 minutes, per World Athletics, which added that the record would be subject to its usual ratification procedure.
    
            Kiplimo finished the race with a time of 56:42 and smashed the previous world record by 48 seconds, the biggest ever single improvement on the men’s half marathon world record. Prior to Kiplimo’s incredible run, Ethiopian athlete Yomif Kejelcha held the record with a time of 57:30.
    
            “It has been the perfect race,” Kiplimo said after his run, per World Athletics. “Ideal temperature, no wind at all, fantastic circuit - everything went better than expected.
    
            “The pacemaker set the agreed 2:45 pace but I found myself full of energy and decided to inject a brisker rhythm from the third kilometre, but I never imagined to perform under the 57 minute barrier, that’s astonishing.”
    
            Kiplimo ran at an average speed of 22.3 kilometers-per-hour on his way to breaking the record and finished more than two minutes ahead of Kenya’s Geoffrey Kamworor in second. Samwel Mailu, also from Kenya, finished another 56 seconds back in third.
    
            Kiplomo now has his sights set on the marathon, and said he will be resting until the London Marathon in April where he will be making his debut at the distance.
    
            Meanwhile, Kenya’s Joyciline Jepkosgei set a course record in the women’s race, finishing in 1:04:13 to fend off her compatriot Gladys Chepkurui in second and Ethiopia’s Alemtsehay Zerihun in third.
    ]]></content:encoded></item><item><title>Did the Windows 95 setup team forget that MS-DOS can do graphics?</title><link>https://devblogs.microsoft.com/oldnewthing/20250211-00/?p=110862</link><author>zinekeller</author><category>hn</category><pubDate>Mon, 17 Feb 2025 01:45:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[One of the reactions to my discussion of  why Windows 95 setup used three operating systems (and oh there were many) was my explanation that an MS-DOS based setup program would be text-mode. But c’mon, MS-DOS could do graphics! Are you just a bunch of morons?Yes, MS-DOS could do graphics, in the sense that it didn’t actively prevent you from doing graphics. You were still responsible for everything yourself, though. There were no graphics primitives aside from a BIOS call to plot a single pixel. Everything else was on you, and you didn’t want to use the BIOS call to plot pixels anyway because it was slow. If you wanted any modicum of performance, you had to access the frame buffer directly.Okay, so now you have to write a graphics library for drawing things fancier than a single pixel. Fortunately, Windows 95 required a VGA video card at a minimum, so didn’t have to worry about CGA or EGA. Mind you, the VGA adapter required you to deal with planar modes, so that was annoying. Fortunately, you have a team of folks expert in VGA planar modes sitting down the hall working on Windows video drivers who can help you out.But the setup program needs more than just pixels. It also wants dialog boxes, so you’ll have to write a window manager to sit on top of your graphics library so you can show dialog boxes with a standard GUI dialog interface, which includes keyboard support for tabbing between elements and assigning hotkeys to fields.You’ll also have to add support for typing characters in non-alphabetic languages like Japanese. Fortunately, you have a team of folks expert in Japanese input sitting in the Tokyo office working on Windows input methods who can help you out, though the time zone difference between Tokyo and Redmond is going to slow you down.You also want to  take advantage of those fancy new controls that the UI team has been making, so maybe you can walk down the hall and ask them if they could port their controls library to your custom UI framework.The setup program also wants to do simple animations, so you’ll need a scheduler that can trigger events based on the system hardware timer.So now you’re going to write all this code for your setup program, none of which is actually involved in setting up Windows 95, but is just the infrastructure needed to run the setup program at all! There’s a lot of stuff here, and you probably won’t be able to cram all of it into 640KB of memory. So now you need to write a protected mode manager (also known as an MS-DOS extender) so you can take advantage of the larger address space afforded by protected mode.An operating system with exactly one application: Windows 95 Setup.What if I told you that Microsoft already had an operating system that did all the things you are trying to do, and it’s fully debugged, with video drivers, a graphics library, a dialog manager, a scheduler, a protected mode manager, and input methods. And it has a fully staffed support team. And that operating system has withstood years of real-world usage? And Microsoft fully owns the rights to it, so you don’t have to worry about royalties or licensing fees? And it’s a well-known system that even has books written about how to program it, so it’ll be easier to hire new people to join your team, since you don’t have to spend a month teaching them how to code for your new custom Setup UI miniature operating system.Go and grab a copy of the Windows 3.1 runtime.: If you committed to the custom operating system route, you’d have to make sure your miniature operating system could run in a Windows 3.1 MS-DOS session in case somebody wanted to install Windows 95 as an upgrade from Windows 3.1, and in a Windows 95 MS-DOS session in case somebody wants to do a repair install of Windows 95. And then you’d have this weird setup experience where Windows 95 setup is running inside an MS-DOS session.: Windows setup still follows this pattern of installing a miniature operating system to bootstrap the setup program. But today, the miniature operating system is Windows PE, the Windows Preinstallation Environment.]]></content:encoded></item><item><title>All Kindles can now be jailbroken</title><link>https://kindlemodding.org/jailbreaking/WinterBreak/</link><author>lumerina</author><category>hn</category><pubDate>Mon, 17 Feb 2025 01:42:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After all, all devices have their dangers. The discovery of speech introduced communication – and lies.  - Isaac AsimovWinterBreak is a jailbreak which was released on New Year’s Day 2025 by HackerDudeSpecial thanks to Marek, NiLuJe, Katadelos and all the beta-testers during the development of this jailbreak.RIP bricked Kindles during beta-testing  RIP the original deadlinesYour Kindle must be registeredYour Kindle must have a valid, internet-connected WiFi network saved to it that it can connect to during steps 8 to 10 (inclusive) Download the latest WinterBreak release: Turn on airplane mode on your KindlePlug the Kindle into your computer and extract the contents of the `WinterBreak.tar.gz` file to your Kindle For Linux/MacOS users, ENSURE the hidden folder `.active_content_sandbox` has been copied to your Kindle Eject your Kindle from your computer and reboot itOpen the Kindle Store on your KindleWhen prompted, click `yes` to turn off airplane modeClick on the WinterBreak icon when it loads:Wait around 30 seconds, and your Kindle will say something along the lines of "Now you are ready to install the hotfix"Once it does, you can move onto the post-jailbreak stage! Kindle store encountered an unexpected error If an  occurs when you try to log in to the Kindle Store or only the Kindle Store home page is displayed, try the following solution:Before registering your Kindle/logging into your account - plug your Kindle into your PC, move the WinterBreak files to the root of your storage spaceLog in account and enter airplane mode as soon as possibleConnect Kindle to PC and delete the cache directory at the path .active_content_sandbox/store/resource/LocalStorage (Skip this step if the LocalStorage directory does not exist)Open the Kindle Store on your KindleWhen prompted, click  to turn off airplane modeCrystals (Bricked their PW4 testing)mergen3107 (Came up with the “WinterBreak” name)]]></content:encoded></item><item><title>YouTube asks channel owner to verify phone, permanently overwrites personal info</title><link>https://old.reddit.com/r/VirtualYoutubers/comments/1iqmul1/if_you_have_a_moment_i_need_your_help/</link><author>Tijdreiziger</author><category>hn</category><pubDate>Mon, 17 Feb 2025 01:25:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Homemade polarimetric synthetic aperture radar drone</title><link>https://hforsten.com/homemade-polarimetric-synthetic-aperture-radar-drone.html</link><author>picture</author><category>hn</category><pubDate>Mon, 17 Feb 2025 01:22:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Radar drone ready to take off from snow.I have made several homebuilt radars and done some synthetic aperture imaging
testing with them on the ground. I have wanted for a long time to put a radar on
a drone and capture synthetic aperture images from air. When I last looked at
this few years ago, medium sized drones with payload capability were
around 1,000 EUR and up. For example in Low-Cost, High-Resolution, Drone-Borne SAR
Imaging
paper by A. Bekar, M. Antoniou and C. J. Baker, the imaging results look
excellent. They used DJI S900 drone with a list price of about 1,000 EUR. The
price for the whole system is quoted to be £15,000, which is a way too
high for my personal budget even just for the price of the drone. Many other
papers use similar style medium sized drones designed for carrying cameras and
they are usually equipped with RTK-GPS for accurate positioning.One of many cheap Chinese FPV kits.Recently small FPV drone prices have dropped a lot. Small 5 and 7 inch propeller
quadcopters can be bought for about 100 EUR from China (not including battery
and RC controller). Despite their small size they are able to lift about 1 kg or
even heavier payload which is plenty for a small radar.I bought the cheapest Chinese no-name 7-inch FPV kit and a small GPS+compass
module to support autonomous flying with the goal of making a light weight
synthetic aperture radar system that it can carry.A single-channel radar can only measure the distance to a target and is unable
to detect the angle of the target. When multiple receiver channels are arranged
in a line, the signal travels slightly different distances to each receiver
based on the target's angle, causing phase shifts in the received signals. These
phase shifts allows calculating the angle of the target.Angular resolution () of antenna depends on its size
approximately as: , where  is the
wavelength, and  is the diameter of the antenna. For example to have
1 m resolution at 1 km distance requires 0.03° angular resolution with 6 GHz RF
frequency. This would require antenna size to be about 100 meters.Instead of making a single large antenna, it's possible to move a single radar
and take multiple measurements at different positions. If the scene remains
static, this approach yields the same results as having one many channel radar
system with big antenna. With synthetic aperture radar it's possible to attach
a single-channel radar to a drone, fly it while making measurements, creating
a large synthetic aperture that provides exceptional angular resolution.The design goal for the radar is to get the best imaging performance while being
able to fit into the FPV drone and achieving it with minimal budget (<500 EUR).
The budget limitation rules out using any low-loss RF materials and both
electronics and antennas should be implemented with lossy FR4 PCB material.The drone is quite small and this limits the maximum size of the radar. The
width of the frame is about 40 mm and propeller tip-to-tip distance is 50 mm
across the frame. Length is about 170 mm, which is much more than width and
means that ideally the radar is skinny. For example Raspberry Pi is 56 x 85 mm
which is too wide. The small size severely limits on what the radar can include.Block diagrams of FMCW (left) and pulse radar
    (right).There are several possible architectures for the radar. I previously made pulse
radar that is about 64 x 132 mm. That's little too
wide for the drone, but I believe it would be possible to shrink it a little.
Issue with that radar is that the maximum bandwidth is about 100 MHz and it's
limited by the ADC sampling rate. This corresponds to a range resolution of 1.5
m, which isn't quite high enough for a detailed image. It's hard to get much
larger ADC bandwidth on a reasonable budget and fitting high speed ADCs on the
limited space is also an issue. There's also a variation of pulse radar that has
two ramp generators, one for RX and one for TX. This results in low frequency IF
signal like with FMCW radar. This is an architecture that is often used on SAR
radars as it allows for high RF bandwidth without requiring high speed ADC.Since pulse radar with switched antenna can't transmit and receive at the same
time, pulsed radar maximum pulse length is limited by the time it takes for the
pulse to travel to the target and back. With for example 100 m minimum distance,
the maximum pulse length to not miss any part of the pulse is only 670 ns.
Because pulse radar needs to divide measurement time between transmission and
reception it reduces the average transmit power and decreases the
signal-to-noise ratio. Large number of very short pulses also complicates the
image formation. Time taken by the SAR image formation scales with the number of
pulses and very short maximum pulse length requires transmitting very large
number of them for a good SNR image.FMCW radar can transmit and receive at the same time and which improves the
signal-to-noise ratio. The maximum sweep length is only limited by the synthetic
aperture sampling speed requirements but it can be hundreds of µs. Unlike pulse
radar, there is also a minimum sweep length requirement, since reflected signal
is mixed to the transmitted signal it needs to be received while the sweep is
still being transmitted. Long sweep length allows collecting much more reflected
power per one sweep. Pulse radar could also use separate TX and RX antennas so
that this wouldn't be an issue, but that removes its advantages compared to
cheaper to implement FMCW radar. Separate transmit and receive antennas require
more space, but due to large maximum length it should be possible to fit two
small antennas side-by-side under the drone.In general FMCW radar has advantage for short range and slow moving platform
applications. Pulse radar is required when long range (more than few km) is needed.Above is the block diagram of the RF parts of the FMCW radar with dual-polarized
antennas. The sweep is generated by PLL, it's passed through a variable
attenuator and then amplified by the power amplifier. Most of it is passed to
the transmit antenna, polarization switch controls whether vertically or
horizontally polarized antenna is used. Part of the transmitted signal is
coupled to the receiver mixer, where it's mixed together with the received
reflected signal that has been amplified by the LNA. Receiver also has
a polarization switch, together with the transmit switch it allows the radar to
receiver and transmit any of the four combinations of polarizations. The mixer
outputs a low frequency signal that is amplified and then digitized by the ADC.
Some filtering is needed in the receiver to avoid large out of band signals and
ADC aliasing.DAC or DDS based sweep generation would likely be better than PLL. DDS phase
noise is often better and it can change frequencies essentially instantly
compared to PLL, but PLL is chosen because it's cheaper and requires less space. RF frequency is going to be around 6 GHz as this is the maximum frequency where
there are many cheap RF components for consumer applications. The highest output
power cheap power amplifiers at this frequency output around 30 dBm. Low noise
amplifiers for the receiver with 1 - 2 dB noise figure can also be obtained
cheaply.Receiver is direct conversion architecture and the mixer does not have any image
rejection. This causes both frequencies above and below the transmitted signal
to be converted to the same output frequency. This is not ideal as noise below
and above the instantaneous sweep frequency is received increasing the noise
floor by 3 dB. IQ sampling receiver that could reject the other sideband would
need two mixers and ADCs. For only 3 dB increase in the signal to noise
ratio, I didn't think it was worth the cost and PCB space.Polarization switches allow choosing which polarization is used to transmit and
receive. H is horizontal and V is vertical polarization. This allows measuring
four polarizations: HH, HV, VH and VV, where the first letter denotes TX
polarization and the second RX. Some targets reflect some polarizations more
than others and it is used in remote sensing to determine properties of
reflected targets. For example many smooth targets often reflect the same
polarization with shape of the target determining if it reflects more HH or VV
components. Forest and vegetation usually has higher cross-polarized HV and VH
component reflection compared to roads and bare ground due to multiple
reflections inside the vegetation.Although H and V antennas are drawn separately in the block diagram, this
doesn't mean that the system requires four antennas. It's possible to design
antenna with two ports, one which radiates H and the other V polarization. Dual
polarized antenna doesn't necessarily need any more space than single polarized
antenna.It would be possible to receive both H and V at the same time if the radar would
have two receivers. This would have some advantages, it would allow removing the
RX polarization switch which would decrease the losses and only the TX should be
switched allowing more time for each measurement which would also increase SNR.
It would also allow transmitting sweeps faster as there isn't need to multiplex
the receiver polarization switch. However, I didn't consider it being worth the
cost.TX-RX leakage can saturate the receiver on high
    powered FMCW radar if the leakage is too high.More RF power generally improves the signal-to-noise ratio, but since FMCW radar
transmits and receives at the same time, it's important to consider the TX-RX
leakage signal. The receiver must be sensitive enough to be able to detect
the thermal noise floor at -174 dBm/Hz without saturating due to leaked
RF power from the transmitter antenna. Typical maximum input power that
saturates the LNA is around -20 dBm. With +30 dBm transmitted power more than 50
dB isolation is needed between transmitter and receiver to prevent the receiver
saturation. Even more isolation might be required if some other receiver
component, such as ADC, saturates first. The variable attenuator before PA can be used to
decrease the transmit power in case high enough isolation antennas don't fit in
the drone. It also affects the receiver mixer's LO power, but mixers LO input
power range should be large enough for it to not be an issue.The equation for the received power at the receiver input can be written as:$$P_r = \frac{P_t G^2 \lambda^2 \sigma}{(4\pi)^3 r^4}$$where  is the transmitter power,  is the antenna gain,  is
wavelength,  is radar cross section of the target, and  is range to
the target. This is the received power from one pulse. Synthetic aperture is
formed by sending multiple pulses while moving and these can all be coherently
summed together to increase the signal to noise ratio. If image is formed from
 pulses, the received power can be multiplied by  to get the received
power in the whole image.Radar cross section  of the target depends on the resolution of the
radar and radar reflectivity of the ground patch. Target radar cross section
can be written as: , where  is
the resolution in the range direction,  is resolution in cross-range
direction, and  is reflectivity of the ground patch per square meter.
In this case , depending on the radar
parameters, range and imaging geometry. Reflectivity of the ground patch
depends on the material of the ground patch and the angle of illumination.
Reflectivity of ground is generally higher when it's illuminated at 90 degree
angle (in direction of the ground normal vector). This causes the specular
reflection to reflect back to the radar. At smaller angles there is still some
reflection back to the radar, but it decreases as the look angle decreases.
Typical ground reflectivity is around -20 to 0 dBsm (decibels square meter) with
moderate look angle.The minimum detectable power is limited by thermal noise of the receiver. It
can be written as , where  is Boltzmann
constant,
 is receiver temperature,  is the noise bandwidth and  is noise figure
of the receiver. It's a common mistake to confuse the noise bandwidth 
with RF bandwidth, but they are not related to each other. Noise bandwidth is
the minimum bandwidth where receiver can separate noise and signal from each other.
By taking Fourier transform of the input signal, we can discard all the
frequency bins that are beyond the signal we are currently looking at, and
noise at those discarded frequencies won't affect the detection capabilities of
the receiver. FFT resolution resolution is equal to , where  is the
sweep length.Setting the received power  equal to noise power and solving for
 we get noise equivalent sigma zero (NESZ) that is value often used
for comparing synthetic aperture radars.$$
\text{NESZ} = \frac{(4 \pi)^3 r^4 k F T}{n \delta x \delta y G^2 \lambda^2 P_t t_s}
$$The number of pulses in image  could also be written as ,
where  is the measurement time and  is pulse repetition frequency.
Or equivalently , where  is the length of the flown
track, and  is the velocity of the drone during measurement. If the antenna
points at a constant angle during measurement (stripmap image) the number of
pulses in the image depends on the time that ground patch is illuminated by the
antenna beam and can be much smaller than the previous number if the antenna beam is
narrow. Quadcopter can easily fly pointing in arbitrary angle and it's possible to
constantly point the antenna at the target (spotlight imaging) and this
limitation doesn't necessarily apply in this case.Number of pulses in imageNoise equivalent sigma zero vs range with above
    listed parameters.Plotting the NESZ as a function of range gives the above plot. There are some
parameters that can be adjusted, mainly sweep length and the number of sweeps in
image to slightly improve this figure. The requires NESZ for good quality image
depends on the actual reflectivity of the ground, but typically for satellite
based SAR NESZ is around -20 dBsm. With these parameters we should expect to see
to around 1 - 2 km with ok image quality.Pulse repetition frequencyMinimum alias-free pulse repetition frequency with
    6 GHz RF frequency and with different number of time-multiplexed channels.Radar image formation relies on the phase information of the received signal.
If we consider a target that is 90 degrees from the antenna beam center (in the
direction of movement) to avoid phase ambiguity, the maximum phase difference
between two adjacent measurements needs to be less than 180 degrees. If the
movement is larger than this, there can be multiple targets at different azimuth
angles that have the same phase difference between measurements causing them to
overlap in the image. Since both transmitter and receiver antennas move,
a distance of a quarter wavelength between two measurements of the same target
results in a half-wavelength difference in the distance traveled by the signal
and a half-wavelength distance difference corresponds to 180-degree phase
difference. At this spacing, targets at ±90 degrees will have the same 180-degree
phase difference between measurements. If the measurement spacing is increased
further more of the image starts to alias.If the antenna is very directive then it's possible to use larger measurement
spacing. Directive antenna won't radiate to large angles that would alias, and
the more directive the antenna is, the larger the measurement spacing can be.
However, since the drone is space-limited and antenna directivity is related to
its size, it might not be possible to design a very directive antenna causing the
maximum measurement spacing to be around quarter wavelength.The common flying speed for a quadcopter is around 10 m/s, but flying speed can easily
be decreased if needed. With 6 GHz RF frequency, the quarter wavelength is 12.5
mm (0.5 inches) and 10 m/s flying speed means that pulse repetition frequency
needs to be at least 800 Hz. Since we have time multiplexed four different
polarizations, we need to be able to measure all of them in this time.PRF sets the requirement for maximum sweep length. With 4 * 800 Hz = 3.2 kHz
PRF requirement this leaves maximum of 312.5 µs per sweep. However, some time
needs to be reserved for time between sweeps due to limited locking time of the
PLL. The locking time of the PLL is around 20 - 30 µs, leaving 280 µs for the
maximum sweep length.Required ADC sampling frequencyRequired ADC sampling rate vs maximum range and RF
    bandwidth for FMCW radar with 250 µs sweep length.FMCW radar mixes the received signal with a copy of the transmitted sweep,
resulting in a sine wave signal at the mixer output with frequency depending on
the range to the target. If the target is at distance , the IF frequency 
can be calculated as: , where  is the bandwidth of
the RF sweep,  is the speed of light, and  is the sweep length. The
range resolution depends on the RF bandwidth  as .
For example, 150 MHz bandwidth is required for 1 m resolution, and 300 MHz
bandwidth results in 0.5 m resolution.If we have 300 MHz of RF bandwidth (0.5 m range resolution) and  is 280 µs
as calculated earlier, then we can calculate the required ADC sampling speed
given the maximum target range we want to detect. For example, with 2 km
maximum range, the IF signal frequency is 14 MHz. The ADC sampling frequency
needs to be at least double this due to Nyquist sampling requirement, and some
additional margin is needed for the anti-alias filter roll-off. This results in
a minimum ADC sampling frequency of about 35 MHz. I chose to use 50 MHz
sampling frequency.Block diagram of digital parts.The amount of data and strict timing requirements of the sweep generation make
it difficult to handle with microcontroller and FPGA is necessary.
Microcontroller is useful for more complicated tasks such as communication with the
drone flight controller and ground station, configuring the radar, and writing
the data to a filesystem. I decided to use Zynq 7020 FPGA, the same one
I used in my previous pulse radar. It has FPGA fabric 
and a dual-core ARM processor in the same package. This FPGA is nominally 150 EUR
from ordinary distributors, but is available for fraction of that from Chinese
distributors.The drawback of this FPGA is that the microcontroller doesn't have many high
speed connections. For example SD-card and EMMC interfaces are limited to 25
MB/s, which is below the data rate of the ADC. It does have 1 Gbps Ethernet, but
using that would require adding a Raspberry Pi or similar computer, which
isn't possible due to size constraints. For instance, newer Ultrascale+ FPGAs
support SD-cards with 52 MB/s and EMMC at 200 MB/s speeds, but they cost around
500 EUR and are not available from Chinese low cost distributors.Zynq can have external DDR3 DRAM, but due to space limitations, it's not
possible to fit enough DRAM to store the whole measurement in memory. With
limitation of only one DDR3 module, the memory is limited to 1 GB, while size of
the measurement can be several gigabytes.This leaves only the option of implementing fast enough external communication
interface in the programmable logic side of the FPGA. Luckily Dan Gisselquist
(ZipCPU) has made GPL3-licensed SD-card and EMMC
controller that supports faster high-speed
communication modes than the hard IP included with the ARM processor.At the time, sdspi controller hadn't been tested with real hardware at the
speeds I required. To make sure I have something working if I'm unable to get
the sdspi core working I connected the SD-card to the ARM processor's
integrated controller, which is limited to 25 MB/s, and EMMC memory to the
programmable logic side that is used with the sdspi controller. This way I'm
able to at least use the SD-card with the integrated controller if the sdspi
core isn't suitable, but this turned out to be unnecessary and the sdspi
controller worked fine. In future versions, it would be better to also connect
the SD-card to PL side using sdspi core for faster SD-card speeds.I also added FT600 USB3 bridge IC that can be used to connect FPGA to PC. This
is not needed for drone usage, but allows real-time connection to PC for other
applications.FPGA program block diagram.FPGA functionality is quite simple at the block diagram level. The design
primarily consists of a few independent blocks wired with either DMA or AXI bus
to the processor. For radar operation, the radar timer block is important as it
switches internal and external signals during the measurement. It needs to be
implemented in the FPGA fabric to ensure that the timing is clock cycle accurate
to achieve phase-stable radar measurements. AXI bus is memory-mapped on the
processor side, allowing the radar to be controlled by writing values to fixed
memory addresses.A decimating FIR filter after ADC data input can be used to change the sample
rate of the ADC data. It can decimate by 1, 2, or 4. For long-range
measurements, the decimation should be disabled for the maximum IF
bandwidth. But for shorter-range measurements it makes sense to use higher
decimation value to decrease the amount of data that needs to be stored.Labeled PCB 3D model in KiCad. Some 3D models are missing.The PCB has six layers and is designed to be as compact as possible, with
components placed closely together to minimize size. Since one-sided assembly
is cheaper than assembling both sides, the bottom side is empty except for one
SD-card connector that I will solder myself.As with many of my previous radars, the RF part is a relatively small part of
the PCB and the overall design effort. Digital electronics and voltage regulators
take up the majority of the PCB space.The radar is designed to accept input voltage from 12 to 30 V and connect
directly to the drone's battery eliminating the need for an external DC/DC
regulator.Due to space constraints, there wasn't enough space to fit four SMA connectors
and I didn't want to use any miniature RF connectors. The top two connectors
are switchable TX outputs for H and V polarization antenna inputs, while the
bottom third one is RX input. The RX polarization switch is located on an
external PCB and connects to one of the three four-pin JST connectors on the
bottom right of the PCB. Another JST connectors is for flight controller's
serial port, and the third connector is currently unused, but it could be used to
connect for example GPS.There are two USB-C connectors: one for JTAG programming and
debugging of the FPGA, and the other connects to USB3 to FIFO bridge chip that enables
fast data transfer to PC. It isn't needed in drone use, but it's useful for
testing and other applications.PCB dimensions are 113 x 48 mm. Width is just skinny enough to fit on the drone,
while it could have been slightly longer.This component is not sale for individuals.After ordering the PCB I made an order for the components from Mouser. The order
seemed to succeed fine and they accepted my money but I got the above email
afterwards telling me that they can't sell me one of the components. Reason
seems to be that the supplier of the component has forbidden them for selling it
to individuals. This was very frustrating as there was absolutely no warnings on
the website and I had already ordered the PCBs. I was unable to order this
component from anywhere, but I did find older obsolete PE43204 pin compatible IC
from obsolete component reseller. It's specified only for maximum of 4 GHz when
the original was up to 6 GHz but it does seems to have low enough loss at 6 GHz
to not cause too large issues.After asking on
reddit
the reason is probably that the manufacturer of the component wants to know who
they are selling to, to avoid their components ending up in defense
applications. That's fine, but it would have been nice to know that in advance.I did make one mistake: SD-card pins are connected to 1.8 V I/O pins, while they
should be connected to 3.3 V I/O and SD-card didn't work with this lower voltage.
The radar could be used without SD-card by storing the data into EMMC instead
and then reading it through USB, but SD-card would be much easier to use.
I really didn't want to order another PCB to just fix this one mistake and
I managed to fix it by designing a small interposer PCB with level shifter that
I soldered on top of the previous SD-card footprint. I wrote about it in more
detail in a previous post.Aluminium PCB heatsink under the radar PCB.Power amplifier can get quite hot if the transmit duty cycle is high. To keep it
cool I ordered custom aluminium substrate PCB that I bolted under the radar PCB.
Solder mask is removed under the PA and thermal pad is placed between the PCB
and heatsink. This cost only $4 for 5 pieces and works very well.Speedybee F405 V3 flight controller.Flight controller came with the drone kit. The included flight controller is
Speedybee F405
V3.
This is a cheap low-end flight controller with 1 MB of flash. It does the job,
but I would recommend getting a little bit better flight controller with
2 MB of flash, the price difference isn't very large.There are several possible flight controller softwares. The three most used for
FPV drones are: Betaflight,
Inav, and
ArduPilot. The main differences of them are:
Betaflight focuses on fast-response manual flying and doesn't have autonomous
flight support, Inav shares lot of code with Betaflight and it includes some
autonomous flight support, and ArduPilot has the most advanced autonomous flight
capability with lot of features but it's more challenging to configure.I chose to use Ardupilot and found it to be excellent for this purpose. It has
very good IMU and GPS sensor fusion algorithm that is very helpful for
improving the position accuracy. The flight controller can communicate with the
radar through a serial port allowing it to enable and disable the radar during
autonomous mission and provide position information for the radar.GPS with integrated compass. It needs to be
    mounted far away from the battery leads to avoid magnetic fields causing issues
    with the compass.SAR imaging needs very accurate position information for proper image focusing.
Position information should be accurate within to a fraction of wavelength,
this is just few cm (1 - 2 inches) at this frequency. Many commercial SAR
imaging drones use RTK GPS with a second stationary GPS receiver on the ground
it's possible to obtain about 1 cm accurate positioning. The drawback
is that it's much more costly than regular GPS and RTK GPS receivers are
usually much larger than ordinary GPS receivers which makes it very difficult
to fit it in this drone.Good non-RTK GPS might be accurate to about 1 m accuracy. This large
positioning errors cause significant errors in the image if not corrected.
Luckily, it's possible to solve for position error from the radar data which is
called autofocusing. With drawback of needing more processing during the image
formation for autofocusing, it's possible to use regular GPS. Sensor fusion
with inertial measurement unit (IMU) can be used to improve accuracy of the
positioning and obtain position updates faster than the maximum of about 4 Hz that
is possible with only GPS.For autonomous flying the drone's flight computer also needs GPS, IMU and
compass. It would be a waste of space to have a second GPS and IMU for just the
radar and instead I'm relying on the flight computer to output its position
estimate to the radar through a serial interface.The drone is controlled with a radio controller. I use
ExpressLRS radio link, which is very common with
FPV drones. The drone also has a radio link to the ground control software
running on PC. This is needed for programming the autonomous mission parameters,
changing drone settings, it can be used to control the drone, and it displays
telemetry during the flight. Ground station can also be used to send messages to
radar through the flight controller and this allows programming the radar
parameters from the laptop.Typically Ardupilot has required using two radios. One for radio controller and
other for telemetry, but ELRS recently added Mavlink
support that allows using
a single radio for both purposes. This is very convenient in this application.In theory, the wider the antenna beam width is, the better the resolution of
SAR image is. A famous result in SAR imaging is that the best possible
cross-range resolution in strip mode SAR (fixed antenna angle and straight
baseline) is , where  is the length of the antenna. However, in
practice wider antenna beam isn't often better. Wider antenna beam means lower
gain which decreases the signal to noise ratio and limits the maximum range.
Antenna gain is squared in the link budget so halving the antenna gain, means
that number of pulses need to be quadrupled to get the same SNR.The cross-range resolution depends on the length of the baseline where the
target is visible and the wider the antenna beam is the longer this is. With
spotlight imaging, where antenna tracks the target, cross-range resolution is
not limited by the antenna beam width and spotlight imaging is easy with drone.
With drone SAR the maximum possible baseline length is often the limiting
factor for resolution as it's hard to fly very long track when limited by
visual line of sight.The azimuth angle resolution in spotlight imaging mode (or in stripmap mode
where antenna beam always covers the target) can be approximated as: , where  is wavelength and  is the length
of the track. Cross-range resolution can be obtained with: , where  is distance to the target.With drone SAR a big issue is how to fit large enough antennas on the drone.
Since the radar is FMCW, separate transmitter and receiver antennas are needed
further decreasing the space per antenna and low TX-RX leakage requires having
some distance between them.I have previously used self-made horn antennas
with my radars, but they are too big to fit on drone. The whole length of the
horn antenna is 100 mm, and even just the coaxial-to-waveguide transition is 25
mm long. This size makes it impossible to fit on the drone, which has only 50 mm
spacing between propeller tips. My previous horn antenna isn't dual-polarized,
but horn can be made dual-polarized easily by adding two feeds 90 degree apart.Patch antenna can be made much smaller since they are just copper on a PCB. They
can also be made dual-polarized with two feeds 90 degree apart. However,
a simple patch on 1.6 mm thick FR4 PCB has poor bandwidth, FR4 dielectric
inaccuracy can cause frequency shift, and the gain isn't very high. Gain can be
improved by making an array of patches, but with FR4 substrate the losses in
feeding network increase quickly.Stacked aperture coupled patch antenna. Side (left) and top (right) views.Neither horn nor patch antenna seemed suitable. After reading some scientific
papers I found this dual-polarized slot-fed stacked patch
antenna paper. It
consists of patch antenna that is fed by microstrip lines that couple to patch
through H-shaped slots in the ground plane. Two feed lines and slots 90 degree
apart can be used to make it dual-polarized. A second patch is suspended few mm
away from the first patch with air in between them. This structure can achieve
much wider bandwidth than a single patch making it tolerant to frequency shift
caused by inaccuracy of FR4 permittivity. The second patch also slightly
increases the gain.TX and RX patch fed horn antennas.However, more gain would be good to have for improving the signal-to-noise
ratio. Sidelobes at 90 degree angle should also be lower to decrease TX-RX
leakage. To solve this I added a sheet metal horn structure around the antenna,
making it a stacked patch fed horn antenna. The horn increases the height by 10
mm, but even just a sheet metal wrapped around the patch gap increases the gain
and decreases sidelobes without increasing the height. I haven't found
a similar structure in any publications, but it wouldn't surprise me if some
exist, as it does seem straightforward.A pyramidal horn with filled-in corners would likely offer slightly higher gain
and be mechanically stiffer than this four-flap design. However, this was
easier for me to manufacture. I cut the copper sheet by hand with scissors and
soldered it to keep it together.This antenna has everything I wanted. It's dual-polarized, has very wide
bandwidth, good gain, isn't as tall as similar gain horn antenna fed with
coaxial-to-waveguide transition, and it's cheap to manufacture requiring just
two FR4 PCBs, some copper sheet and few bolts and spacers. FR4 is lossy and gain
would likely be around 0.5 - 1.0 dB higher if a proper low loss RF material was
used, but the cost would be about 100x higher in prototype quantities and it
doesn't make sense for me to spend so much more for so little improvement.Between the antennas is a small 0.25 x 0.5 wavelength wall that decreases TX-RX
coupling. I tested few different size walls and this small wall is more
effective than no wall and also more effective than taller walls.Not counting the SMA connectors, the height of the antenna is 18 mm, of which 10
mm is the height of the horn above the suspended patch. Total height including
the SMA connector is 28 mm. Patch substrate dimension is 45 x 45 mm and the horn
aperture is 65 x 65 mm.Backside of the antennas. Each antenna has two SMA
    connectors, one for H and other for V polarization.Backside of the antennas is covered by copper sheet to decrease the backwards
radiation. This is needed because antennas are mounted right on top of the radar
PCB which doesn't have any shielding. Without shielding TX antenna backwards
radiation would increase the TX-RX coupling. Copper sheet is also inserted into
the wall between antennas and the tape keeps it in place. Simulated radiation pattern of the antenna.The simulated -3 dB beamwidth is 50/60 degrees in 0/90 degree angles. H and
V feed slots are rotated 90 degrees and the radiation pattern from the other
port is similar but rotated 90 degrees. Simulated peak gain is 10.0 dB.Sidelobe to 90 degree direction is about -10 dB. It's important for this to be
low to minimize TX-RX leakage.Since the antenna radiation pattern isn't quite symmetrical, mounting the other
antenna at a 90-degree rotation compared to the first one ensures good antenna
pattern matching between HH and VV polarizations. If the TX antenna transmits
H polarization from the first port, RX antenna receives H polarization from the
other port, and other way around for V, causing the two-way antenna pattern to
match between the both cases. However, HV and VH antenna patterns don't match
the HH and VV patterns, since in the cross-polarization case the same port is
used to transmit and receive on both antennas.Measured S-parameters of the antenna.There is a slight difference in the H and V port matching due to their slightly
different coupling slot sizes. Antenna useable bandwidth is from about 4.5 GHz
to 6.2 GHz which is more than enough for this application.Drone mechanical model in Blender.The drone needs some mechanical parts to hold the radar on the drone frame.  The
flight controller is mounted inside the frame, but there isn't enough room
inside it for the radar, so I designed a 3D printed mount that holds the radar
PCB under the drone frame. This mounting position also requires landing legs so
that the drone doesn't land on the radar. I designed it in Blender since I don't
know any mechanical CAD programs. It works just fine for these simple parts.Radar attached under the drone.The radar mount holds the radar PCB on the drone frame. It would be a good idea
to have some sort of weather proof enclosure for it, but I haven't got around it
yet. I added some material over the PCB to protect it in case the landing legs
fail.Radar holder attaches with four screws to the drone. Drone radio controller
antenna is visible at the bottom left. Propellers are collapsible, these make it
much easier to transport the drone as with these it fits in a backpack.Receiver polarization switch PCB.Antenna board is held with two bolts that allow its angle to be changed. Flight
controller serial port is attached to one of the JST connectors. There also
other JST connector from flight controller that is currently unused and just
held with tape in place.Transmitter has polarization switch and two SMA connectors on the PCB, but the
receiver polarization switch is on a separate PCB due to lack of space. I had
a mounting holes for it on the radar holder part, but the SMA cables are stiff
enough that I found it easier to just leave it hanging there. Polarization
switch connects to the radar PCB with another JST connector.Landing legs are 10 cm diameter carbon tubes with 3D printed TPU caps. Smaller
diameter tube would have likely been fine, these are very stiff and something
else would fail before them if they are stressed too much in landing.Radar is powered directly from the drone battery. XT60 splitter is used to
connect both flight controller and radar to the same battery.Drone balanced on a kitchen scale.Weight of the drone without battery is just 752 grams (1.66 lbs). I have two six
cell LiPo batteries, the smaller 1300 mAh capacity battery weights 196 grams and
the bigger 2200 mAh battery weights 322 grams. With the smaller battery the total
weight of the whole system is just 948 grams.Radar measures the distance and phase of each target. To convert these
measurements into a radar image, matched filtering can be used. For each pixel
in the image, generate a reference signal corresponding to what a target at that
position would reflect. Multiply the measured signal with the complex conjugate
of the reference signal for each measurement, then sum these products over
all measurements. When the measured signal closely matches the reference signal,
their product becomes large because the phases align. If it doesn't match,
the result of the multiplication is a complex number with a random phase, and summing
random complex numbers will average out generating a low response.The image formation can be written as:$$I = \sum_{p \in \mathcal{P}} \sum_{n=1}^N S_n(d(p,x_n)) \cdot H(d(p,x_n))^* $$, where  is the set of pixels in the image,  is the number of
radar measurements,  is Fourier transformed measured IF signal ,
 is the distance to location of pixel  from radar position at that
measurement , and  is complex conjugate of the reference function of
what target at that position in the image should look like (Fourier transform of
the radar IF signal from target at that position in the image).This is called backprojection algorithm. It's simple and doesn't make any
approximations or assumptions about flight geometry, but it's very demanding to
calculate. For example with 1 km x 1 km image with 0.3 m resolution and 10,000
radar sweeps, calculating the image needs  backprojections. This means over 100 billion complex exponentials and
square root calculations is needed for one image, and image size and number of
sweeps can be even larger in practice. There are some clever algorithms that can
be used to speed this up, but they often have approximations or only work with
linear flight tracks. One easy improvement that can be made without many
drawbacks, is to use polar coordinates instead of Cartesian coordinates, as that
requires less pixels in the image since angular resolution is constant while
cross-range resolution is better closer to the radar. Polar coordinate image can
then be afterwards interpolated to Cartesian grid.Some years ago this would have been unbelievable amount of compute, but with
modern GPU this can be calculated in under a second. This problem is especially
well suited for GPU implementation since every pixel can be calculated
independently in parallel. Very straightforward CUDA kernel is able to calculate
220 Billion backprojections per second on RTX 3090 Ti GPU. This is very
respectable speed considering that each backprojection requires square root and
complex exponential (which can be calculated with just sin and cos). I'm sure
that someone experienced with CUDA programming could make this even faster as
this doesn't have any optimizations or approximations and is just the direct
algorithm implementation.Positioning accuracy of the GPS and IMU isn't good enough to form a high-quality
image. Ideally, the position should be known to a fraction of the wavelength,
but accuracy of the GPS isn't good enough. To achieve good image quality, an
autofocus algorithm is necessary to focus the radar image using information from
the radar data.The most commonly used autofocus algorithm is phase gradient
autofocus. It's simple
and fast autofocus algorithm that works by taking an unfocused radar image as an
input and solving for a phase vector that when multiplied with the azimuth FFT
of the image gives a focused image. However, it doesn't work well in this case
since the azimuth beam is wide and the radar baseline is long causing the
focusing errors to be spatially dependent.I updated my previous backpropagation autofocus to
use PyTorch and made some improvements. This autofocus algorithm works by
forming the radar image, calculating the gradient of the input velocity, and 
clipping the learning rate to limit the maximum position change to a predefined
value. The input velocity is then updated using a gradient descent optimizer.
I found that using a 3D position doesn't work well, as it often tends to just nudge
each position in random directions. Instead, using velocity and integrating it
to position seems to yield much better results. A small regularization term is
also included to minimize the distance between the optimized and original
positions, favoring smaller updates.Adjusting the learning rate based on the maximum position change makes it easier
to set the optimizer meta parameters. Instead of setting the learning rate
directly, maximum position update is given which is used to set the learning
rate.This is very general autofocus algorithm that makes no assumptions about the
radar system, scene, or the flight track. The obvious disadvantage is that it
requires forming the radar image many times making the already slow image
formation many times slower. Without the fast GPU image formation this would be
too slow to be useful.The autofocus algorithm is available on
Github.Configuring the mission in ArduPilot Mission
    Planner.The mission is programmed beforehand with the ArduPilot Mission Planner. The
drone will automatically fly the programmed waypoints, there are also commands
to set the ROI (region of interest) so that antenna always points towards it,
and the radar measurement is started with digicam configure command in the
mission. It's originally meant for configuring ordinary camera, but
I programmed the radar microcontroller to listen to it. Using an existing
command makes it easy to make the radar work with the existing ArduPilot software.Setting ROI, which is needed for spotlight imaging, needs a patch to ArduPilot
firmware. By default drone's front will always point towards the ROI and there
isn't a way to configure it to point the antenna towards ROI instead. The patch
is available on the ArduPilot Github as a pull request.The scene is a wide open field. There's about 1.5 km distance to the forest at
the antenna pointing direction. The drone flies at 110 m altitude in a straight
line for about 500 m at 5 m/s velocity. The radar was configured to transmit
only VV polarization with 400 µs long sweep, 500 MHz bandwidth, and 1 kHz
pulse repetition frequency.Range compressed raw data.The range compressed (Fourier transformed) captured data doesn't look very
impressive. It doesn't look anything like an image since due to the wide
antenna beam at each sweep many targets at different angles are captured.At the zero distance there is a large response from the TX-RX leakage, then the
next reflection is at 100 m distance from the ground. Even though the antenna
gain at directly below is much smaller than at the beam center, due to the angle
of the reflection and close distance, the reflection from directly below is very
large. At large distances reflections are mostly below the noise floor of
individual sweeps, but during image formation many sweeps are integrated
improving the signal to noise ratio. Some large individual objects are visible
and their distance to the radar changes as the drone moves.Recorded drone position and antenna pointing
    vector from GPS and IMU. Note the unequal axes scale.Ideally the track should have been a straight line but there
is some disturbances due to for example wind. The drone is very light and even
a slight wind can easily affect it. The ROI was set quite far away there is only
few degree of change in the antenna pointing direction during the measurement.SAR image without autofocus.Above is the processed SAR image without autofocus in pseudo-polar coordinates
that the image formation uses internally. It's pseudo-polar because angle axis
is in sine of radians instead of just radians, this is slightly more efficient
than ordinary polar coordinates. Image size is 6k x 20k pixels using 51,200
sweeps.Compared to the raw data it's a night and day. Various geographical features can
be now identified, but polar format makes it hard to compare to the map.SAR image without autofocus.Cartesian coordinate image can be obtained by projecting the pseudo-polar image
to Cartesian grid. This is very fast operation compared forming the image
directly on the Cartesian grid. The image is also aligned so that north points
up using the drone's electronic compass measurements. Left corner is missing
a small patch of data due to the rotation.The resulting image is still quite blurry. Clearly only relying on the GPS and
IMU positioning isn't good enough and autofocus is needed to get a sharply
focused image.After applying 30 iterations of the minimum entropy gradient optimization
autofocus, the image quality is much better. Five iterations would have been
probably enough, but using more iterations does improve the quality slightly.
This does take several minutes since each iteration requires calculating forward
and backwards pass of the backprojection.Due to the low look angle, tall structures such as trees cast long shadows. The
image amplitude isn't normalized, which is why it's brighter closer to the
origin.  The antenna radiation pattern can be also visualized in the image. The
beam center is tilted slightly to the right, and the antenna gain at the left
side of the image is much smaller due to it being farther from the beam center,
causing it to be dimmer.SAR image detail comparison. Without autofocus
    (left) and with minimum entropy optimization autofocus (right).There is quite lot of detail in the resulting radar image when zooming in.
Comparing a 300 x 300 m patch, the autofocused image reveals surface details of
the field that was just blur in the image without autofocus.I also tried using phase gradient autofocus, but it doesn't work well in this
case. The result is very similar to
the image without autofocus.The three lines at the bottom left are power lines. They seem to be only visible
in the image when the radar is looking at them at 90 degree angle, at other angles
the reflectivity is so low that they are invisible.Original and optimized velocity.Comparing the drone velocity before and after optimization the changes aren't
very large. Along the track and range direction velocity components are both
adjusted slightly and height direction velocity component is basically
unchanged.Google maps screenshot of the SAR imaging area.I also made another measurement at other location using all four polarizations.
The radar flies a linear track autonomously as before, but now the radar quickly
switches between each of the four polarization switch states. Sweep length was
reduced to 200 µs, pulse repetition frequency is 715 Hz for each
polarization, and other parameters are kept the same. Number of sweeps in the
image is the same 51,200.Four SAR images with different polarizations.The four polarizations look very similar. The main differences are that
cross-polarization images (HV and VH) have weaker amplitude due to
cross-polarization component in general being smaller than the reflection of the
same polarization.Polarized SAR image with autofocus.Instead of looking at four different images for each polarization, it's common
to use RGB color channels for different polarizations in the same image. In this
colored image it's easier to visualize how each target reflects specific
polarizations. The ground is tinted purple, indicating that it
reflects VV and HH polarization better than the cross-polarized components. The same
can be seen on the buildings and in the light poles along the road. Forest areas
are colored white as they reflect all polarizations about equally. However,
since the effects of the antenna radiation pattern and possibly slightly
different losses for different polarization switch states are not calibrated
some of the observed differences could be attributed to the hardware. Better
accuracy measurements would require calibration.The area with bunch of points around (200, 500) meters is a some kind of garden
of small trees each surrounded with a metal wire mesh.Picture at the ground at (-50, -80) m coordinates
    in the SAR image looking towards negative Y-axis.There was slight amount of snow on the ground during the measurements. The
visible picture is from the top of the SAR image looking down. The small forest
on the left is the small patch of trees in the middle of the image.Drone waypoints for the octagonal flight path. Red
    marker is the region-of-interest where drone points the antenna.The previous measurements synthesized one high-resolution image from a long
baseline. It's also possible to synthesize many images with small baselines from
one long measurement, and these many images can then be turned into a video.For the backprojection algorithm, the flight track doesn't need to be linear and for
this case I programmed the drone to fly octagonal track while pointing the
antenna at the octagon's center.Each frame has 1024 radar sweeps with 512 of them overlapping with the previous
frame.  Since each frame has less sweeps than the previous full images, the
frames are noisier and have worse angular resolution. The video is sped up by
about 10x. All four polarizations are used, and the image colorization is the
same as in the previous polarized SAR image.Frames are autofocused separately and there isn't any alignment of adjacent
frames, which causes the frames slightly wobble or jump around occasionally. Corners
of the octagon are especially challenging for the image formation since both
along- and cross-range positions need to be solved accurately for a good-quality
image. Angular resolution can also vary between frames as the baseline
length between the frames can vary, as only the number of sweeps is the same
between the frames.Natural targets such as the ground and forest look very similar at different frames,
but at several points in the video large reflections can be seen for example at
bridge and power lines when they are oriented at a 90-degree angle to the radar.
The bright spot that looks like it's moving at the bridge is just glint from the
railing. The mismatch between antenna patterns of different polarizations is
also visible as the same target can have slightly different color at the beam
center or at the edge.Radar look angle with 120 m flying height.Without special permits, it's allowed to fly the drone at a maximum altitude of
120 m. Usually, for SAR imaging, the look angle is around 10 to 50 degrees. If the
look angle is close to 90 degrees (i.e., looking straight down at the ground), the
reflected power is high, but the range resolution is poor as the distance to the
radar is almost the same for nearby locations. For low look angles the range
resolution is good, but due to the low grazing angle the reflected power back to
the radar is low. With extremely low look angle the reflected power can be about
10 to 20 dB lower than it would be compared to more usual around 45 degree look
angle reducing the maximum distance the radar can see.Another issue is shadowing caused by tall objects.  For instance, when flying at
120 m height, the grazing angle at 2 km distance is only 3.4 degrees. A 10
m tall tree casts 170 m long shadow at this low angle, making it impossible to
see any reflections from the ground after the tall object. This is clearly
visible in all of the measurements. Especially in the full-polarization
measurement only the tops of the buildings are visible at long distances.Schematic of the radar (click to open).The synthetic aperture radar drone can image at least up to 1.5 km and likely
even farther if flown higher. It weighs under 1 kg including the radar, drone,
and battery. The system can capture HH, HV, VH, and VV polarizations.
A gradient-based minimum entropy autofocus algorithm is capable of producing
good good-quality images with a wide antenna beam using only non-RTK GPS and IMU
sensor information. The total cost of the drone was about 200 EUR, 600 EUR for
two radar PCBs, and about 10 months of my free-time after work. I'm very happy
with the performance of the system considering its low cost.]]></content:encoded></item><item><title>San Francisco homelessness: Park ranger helps one person at a time</title><link>https://sfstandard.com/2025/02/08/golden-gate-park-ranger-homelessness/</link><author>NaOH</author><category>hn</category><pubDate>Mon, 17 Feb 2025 00:15:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Barrows found him in the park a few days later, with the catheter still in place. He’d been using it to inject crystal meth. He walked away from her, then veered into the street, directly into the path of a truck. It skidded to a stop, missing him by inches. “What are you doing, dude?” Barrows yelled. Morrisette continued across the street. “He flashes,” she says of his now-familiar state of agitation. “It’s almost like he’s just blinded by rage or emotions or upset. And you can’t really do much other than watch, because if you try to forcibly intervene, it escalates.” Two days later, Morrisette suffered an overdose from fentanyl. By then he was living in an encampment of RVs and tents on the Lower Great Highway, next to the park. Barrows and one of the RV residents administered Narcan and CPR until an ambulance arrived. The incident shook her deeply. She feared if Morrisette didn’t change his life soon, he would die, either deliberately or accidentally. In early 2024, after a punishing series of storms, Barrows helped Morrisette and another resident of the encampment secure placement in a double room in transitional housing. To get the slot, they decided to lie and say they were same-sex partners, even though they were barely friends. But the arrangement didn’t last long. The man got sick and was hospitalized, and a stranger was moved into the room. Morrisette got into a fight with the guy and was kicked out of the facility. He returned to the Lower Great Highway.To Barrows, the fault was in the placement, not in Morrisette. “Ronnie was always very clear about his needs. He knows he’s a volatile person. He doesn’t want to be in a shared room, especially with a stranger,” she said.Still, his setbacks, and those of Barrows’ other clients, were taking a toll on her. Some days, she admitted, she felt burned out by all the distress she had to witness. “This can be just so exhausting to try to show up fully for so many people every day. And then be a container for all this brokenness.”]]></content:encoded></item><item><title>Does or did COBOL default to 1875-05-20 for corrupt or missing dates?</title><link>https://retrocomputing.stackexchange.com/questions/31288/does-or-did-cobol-default-to-1875-05-20-for-corrupt-or-missing-dates</link><author>SeenNotHeard</author><category>hn</category><pubDate>Sun, 16 Feb 2025 23:56:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Javier Milei backtracks on $4.4B memecoin after &apos;insiders&apos; pocket $87M</title><link>https://www.coindesk.com/business/2025/02/15/javier-milei-backtracks-on-usd4-4b-memecoin-after-insiders-pocket-usd87m</link><author>techlover14159</author><category>hn</category><pubDate>Sun, 16 Feb 2025 23:15:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Argentina's president Javier Milei has backtracked on a tweet promoting a memecoin called Libra, which rose to a $4.4 billion market cap before plunging by more than 95%.In a now-deleted tweet, Milei initially wrote: "This is a private project dedicated to encouraging the growth of the Argentine economy," along with a Solana contract address linked to the Libra token.Libra rose by more than 2,000% in a 40-minute span following the tweet, only to tumble rapidly as a group of early holders began to cash out. X account KobeissiLetter shared a series of BubbleMaps screenshots showing that alleged "insiders" liquidate tokens by adding one-sided liquidity pools on Metora with only Libra, allowing them to remove SOL and stablecoins.Trading volume for Libra hit $1.1 billion after launch, although it appeared that purchases and sales were skewed; there were 74,500 individual buy orders and 28,900 sales - indicating that larger sell orders flattened the flurry of retail activity.Milei later addressed the botched memecoin on X, stating that he "was not aware of the details of the project." "A few hours ago I posted a tweet, as I have so many other times, supporting a supposed private enterprise with which I obviously have no connection whatsoever," Milei wrote. "I was not aware of the details of the project and after having become aware of it I decided not to continue spreading the word (that is why I deleted the tweet)."The sell-off in Libra rippled across the wider memecoin market, with TRUMP losing $500 million from its market cap, according to market data, in a 30-minute period after Libra began to tumble.]]></content:encoded></item><item><title>Uchū – Color palette for internet lovers</title><link>https://uchu.style/</link><author>NetOpWibby</author><category>hn</category><pubDate>Sun, 16 Feb 2025 22:22:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[oklch(68.5% 0.136 303.78)oklch(58.47% 0.181 302.06)oklch(49.39% 0.215 298.31)oklch(46.11% 0.198 298.4)oklch(42.77% 0.181 298.49)oklch(39.46% 0.164 298.29)oklch(36.01% 0.145 298.35)]]></content:encoded></item><item><title>Show HN: Air traffic control radio and chill music for focus</title><link>https://www.chillyatc.com/</link><author>usernameis42</author><category>hn</category><pubDate>Sun, 16 Feb 2025 21:36:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Critics say new Google rules put profits over privacy</title><link>https://www.bbc.com/news/articles/cm21g0052dno</link><author>latexr</author><category>hn</category><pubDate>Sun, 16 Feb 2025 20:45:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Privacy campaigners have called Google's new rules on tracking people online "a blatant disregard for user privacy."Changes which come in on Sunday permit so-called "fingerprinting", which allows online advertisers to collect more data about users including their IP addresses and information about their devices.Google says this data is already widely used by other companies, and it continues to encourage responsible data use.However the company had previously come out strongly against this kind of data collection, saying in a 2019 blog that fingerprinting "subverts user choice and is wrong."But in a post announcing the new rule changes, Google said the way people used the internet - such as devices like smart TVs and consoles - meant it was harder to target ads to users using conventional data collection, which users control with cookie consent.It also says more privacy options provide safety to users.Google told the BBC in a statement: "Privacy-enhancing technologies offer new ways for our partners to succeed on emerging platforms... without compromising on user privacy."But opponents to the change say fingerprinting and IP address collection are a blow to privacy because it is harder for users to control what data is collected about them. "By allowing fingerprinting, Google has given itself - and the advertising industry it dominates - permission to use a form of tracking that people can't do much to stop," said Martin Thomson, distinguished engineer at Mozilla, a rival to Google.Fingerprinting collects information about a person's device and browser and puts it together to create a profile of that person. The information is not explicitly collected in order to advertise to people, but it can be used to target specific ads based on that user's data.For example, a person's screen size or language settings are legitimately needed in order to display a website properly. But when that is combined with their time zone, browser type, battery level - and many other data points - it can create a unique combination of settings which makes it easier to work out who is using a web service.These details along with someone's IP address - the unique identifier used by internet devices - were previously prohibited by Google for ad targeting. Privacy campaigners say that unlike cookies, which are small files stored on a local device, users have little control over whether they send fingerprinting information to advertisers."By explicitly allowing a tracking technique that they previously described as incompatible with user control, Google highlights its ongoing prioritisation of profits over privacy," said Lena Cohen, staff technologist at the Electronic Frontier Foundation. "The same tracking techniques that Google claims are essential for online advertising also expose individuals' sensitive information to data brokers, surveillance companies, and law enforcement," she added."My argument would be that fingerprinting sits in a little bit of a grey area," says Pete Wallace, from advertising technology company GumGum. "Should people feel comfortable staying in a grey area of privacy? I'd say no," he adds.GumGum, which has worked with the BBC on ad campaigns before, relies on something called contextual advertising, which uses other data points to target adverts to online users, such as keywords on the website they are on - rather than their personal data.Mr Wallace says allowing fingerprinting represents a shift in the industry."Fingerprinting feels like it's taking a much more business-centric approach to the use of consumer data rather than a consumer-centric approach," he says."This sort of flip-flopping is, in my opinion, detrimental to that route that the industry seemed to be taking towards this idea of really putting consumer privacy at the forefront."He adds that he hopes ad tech companies conclude "that it isn't the appropriate way to use consumer data," but expects them to look at fingerprinting as an option in order to better target adverts.Advertising is the lifeblood of the internet business model, and allow many websites to be freely available to users without them having to directly pay to access them. But in return, users often have to give up private information about themselves so that advertisers can show them relevant adverts.The UK's data watchdog, the Information Commissioner's Office (ICO), says "fingerprinting is not a fair means of tracking users online because it is likely to reduce people's choice and control over how their information is collected."In a blog post in December, the ICO's Executive Director of Regulatory Risk Stephen Almond wrote: "We think this change is irresponsible."He added that advertisers and businesses which decide to use this technology will have to demonstrate how they are staying within data and privacy laws in the UK. "Based on our understanding of how fingerprinting techniques are currently used for advertising this is a high bar to meet," he wrote.Google said in a statement: "We look forward to further discussions with the ICO about this policy change. "We know that data signals like IP addresses are already commonly used by others in the industry today, and Google has been using IP responsibly to fight fraud for years."A spokesperson added: "We continue to give users choice whether to receive personalised ads, and will work across the industry to encourage responsible data use." ]]></content:encoded></item><item><title>Kindle is removing download and transfer option on Feb 26th</title><link>https://old.reddit.com/r/kindle/comments/1inr9uy/fyi_amazon_is_removing_download_transfer_option/</link><author>andyjohnson0</author><category>hn</category><pubDate>Sun, 16 Feb 2025 18:13:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Caddy – The Ultimate Server with Automatic HTTPS</title><link>https://caddyserver.com/</link><author>huang_chung</author><category>hn</category><pubDate>Sun, 16 Feb 2025 17:56:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
								"Servers running Caddy exhibit nearly ubiquitous HTTPS deployment and use modern TLS configurations. ... We hope to see other popular server software follow Caddy's lead."
							
								—Josh Aas, Richard Barnes, Benton Case, Zakir Durumeric, Peter Eckersley, Alan Flores-López, J. Alex Halderman, Jacob Hoffman-Andrews, James Kasten, Eric Rescorla, Seth Schoen, and Brad Warren. 2019. Let's Encrypt: An Automated Certificate Authority to Encrypt the Entire Web. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19). Association for Computing Machinery, New York, NY, USA, 2473–2487. https://doi.org/10.1145/3319535.3363192
								"No popular server software does [session ticket key rotation], with the exception of Caddy."
							
								—Drew Springall, Zakir Durumeric, and J. Alex Halderman. 2016. Measuring the Security Harm of TLS Crypto Shortcuts. In Proceedings of the 2016 Internet Measurement Conference (IMC '16), Association for Computing Machinery, Santa Monica, California, USA, 33-47. https://doi.org/10.1145/2987443.2987480]]></content:encoded></item><item><title>IPv6 Is Hard</title><link>https://techlog.jenslink.net/posts/ipv6-is-hard/</link><author>miyuru</author><category>hn</category><pubDate>Sun, 16 Feb 2025 17:04:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Yesterday I read this toot (German) over on mastodon
which starts with “IPv6 is hard.”No it’s not. It’s different.I ran across this multiple times: There is an A and an AAAA-record for a FQDN, but the
web server is only reachable via IPv4. You can easily test this with curl$ curl -4  https://github.com -o /dev/null
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  273k      273k      3417k       --:--:-- --:--:-- --:--:-- 3553k
$ curl -6  https://github.com -o /dev/null
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
   --:--:-- --:--:-- --:--:--     
curl: 7 Couldnt connect to server
When using IPv4 273k are “saved” to /dev/null, using IPv6 we get an error message “Couldn’t connect to server” that I’m using GitHub here because it is a well known example for not
offering IPv6 as a time of writing, and it probably will have not IPv6 soon.So in the case of the toot mentioned above there is an AAAA for the FQDN. But the connection doesn’t work. But the end user
doesn’t notice. Because there is a browser feature called Happy Eyeballs (HE).
Basically: A browser tries both protocols and uses the
faster on. In case of non-working IPv6 IPv4 is always faster.HE can have some funny side effects. In a project a connection to a development web server sometimes worked and sometimes
didn’t. The solution was quite simple. The customer used a split VPN tunnel. IPv4 was routed via the VPN tunnel and those
IPv4 addresses were allowed in the web servers access list. IPv6 was routed via the normal Internet connection and those
addresses weren’t allowed.But back to the original problem: In another toot we learn that a support ticket
was opened four weeks ago and that traceroute
was prohibited. But there is always the option to use the  option for traceroute or . This use tcp instead
of UDP (standard traceroute) or ICMP (Windows tracert).When you run traceroute -T for both IPv4 and IPv6 you’ll see that in this case IPv6 ends one hop earlier than IPv4. Or
in this case the last hop loops back to itself.This could mean that there is a missing firewall rule allowing traffic to the server or there is a routing issue on the
firewall.But we can learn much more from this:If browser wouldn’t do happy eyeballs, the support hotline would have been explodedThere is no external monitoring, at least not for IPv6There is no automation or at least no complete automationSo please: If you do IPv6 take it seriously. If you don’t take it seriously,
don’t do IPv6. That leaves to people thinking that
IPv6 is hard and can not be done.Some notes: I’ll cover traceroute and co. in a future blog post and regarding
GitHub check out this
link.]]></content:encoded></item><item><title>Half-Life 2 and Dishonored art lead Viktor Antonov has died</title><link>https://www.eurogamer.net/half-life-2-and-dishonored-art-lead-viktor-antonov-dies-aged-just-52</link><author>Trasmatta</author><category>hn</category><pubDate>Sun, 16 Feb 2025 17:01:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Former Valve writer Marc Laidlaw revealed the news on social media, saying he had no details of Antonov's death, but he had had it confirmed that the "visionary art lead" had died."I didn't want to say much till I felt it was confirmed, but I learned today that Viktor Antonov, our visionary art lead on HL2, has died," Laidlaw said."I don't have details. Just sadness. Brilliant and original. Made everything better."Antonov moved from Bulgaria to Paris when he was just 17 and went on to work or consult on a number of notable games, including Half-Life 2, Counter-Strike: Source, Half-Life 2: Lost Coast, Dishonored. Dishonored: Dunwall City Trials, Wolfenstein: The New Order, Fallout 4, Dishonored 2, Doom, and Prey.]]></content:encoded></item><item><title>United States Power Outage Map</title><link>https://poweroutage.us/</link><author>jonbaer</author><category>hn</category><pubDate>Sun, 16 Feb 2025 16:45:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Electric customerswithout power:Electric customerswithout power:Electric customers 
            Electric customers without power:]]></content:encoded></item><item><title>Vim after Bram: a core maintainer on how they&apos;ve kept it going</title><link>https://thenewstack.io/vim-after-bram-a-core-maintainer-on-how-theyve-kept-it-going/</link><author>MilnerRoute</author><category>hn</category><pubDate>Sun, 16 Feb 2025 15:44:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Its community performed a quietly heroic effort to make sure his project stayed alive.“What you can see is basically that the development did not stop,” Brabandt told his audience in Tokyo.Every day there are fresh pull requests and issues to review, so “It’s still quite active. There’s a lot of activity going on on GitHub.”And in January of 2024, they released Vim 9.1 — and dedicated it to Moolenaar.‘The Development Did Not Stop’Platform consultant Christian Brabandt had been active in the Vim community since 2006, contributing bug reports, fixes and a few new features. He’d worked on things like Vim’s regular expression handling and its support for encryption, as well as helping build its daily Appimage and “moving the home page around.” And then suddenly in August of 2023, “I became one of the main maintainers of Vim.”The news of Moolenaar’s death was “quite shocking for all of us,” even though Vim’s mailing list had gone “pretty quiet” in the weeks before, and “people already started to wonder what happened with Bram? Where is he?”“We had to decide what we were going to do.”Brabandt first acknowledged that they “lost a lot of knowledge” — and not just Moolenaar’s test scripts.Moolenaar started Vim 30 years ago, and he carried in his head “a lot of knowledge on the original Vim of all the features he wanted to have.” But more than that, Moolenaar was also the project’s leader. “He basically determined the strategy — where he wanted the project to go and what he wanted to be included and what he didn’t like.”“We had to restructure and find ways to continue.”And right from the beginning, there was one essential crisis. When it came to Vim’s GitHub account, “Bram was the owner. That means only he could make certain decisions — final decisions like setting up roles and permissions for other maintainers… We needed to have this power to continue working and invite other maintainers to the project.”Fortunately, GitHub actually has a “deceased user” policy, including “pre-designated successors.” But unfortunately, Brabandt told his audience, using that policy “is not as easy as it sounds,” since after the paperwork is filled out, the GitHub account “becomes basically deactivated. Which wasn’t the best thing for us, since Bram’s family was able to access his account, and I didn’t want them to lose this ability.” Instead, Moolenaar’s family changed the permissions so that other maintainers could be invited.Shortly after Moolenaar’s death, “Quite a few pull requests” started accumulating on GitHub, Brabandt said. “So I started going through those and importing them.” And when another long-time contributor and core maintainer, Charles Campbell, decided to retire, “I decided to invite a few more maintainers… mainly people who have been long-time contributors to Vim.”But besides the source code, they also had to manage the project’s other infrastructure, and unfortunately, there were no documented processes, “so I had to find out all of this — how this is managed — basically, the hard way.”And it seems like everything that could go wrong did.The site handling Vim’s vulnerability reports was acquired by an AI-security company that Brabandt says “just wanted to concentrate on AI and only on AI… Open source vulnerability reporting was basically shut down almost immediately.” So the project turned to GitHub Security Advisory.Brabandt learned the basic code of Vim’s home page hasn’t changed in 20 years. It still included PHP 7 code — though support for Php 7 ended in November of 2022.The service hosting their home page was acquired by Open Source China in July of 2023, and soon began serving visitors database errors, while support tickets went unanswered. So in the middle of restructuring the Vim project, the project team had to also find a new host for Vim’s home page — but, “Unfortunately, this also meant that we had to upgrade the home page from PHP 7 to at least PHP 8 support.”The FTP server was still being run by the Dutch Unix User Group. “This was fine in the ’90s and maybe early 2000s,” Brabandt said, but “Nowadays I think people typically just download everything from GitHub or from the home page!” The Dutch Unix User Group was also reluctant to give Brabrandt access, and “It’s fine…” he said, “because we then decided to retire the old FTP server. And if a download needs to be done, it can be done via the Vim home page.”And since retiring FTP access, Brabandt says he hasn’t heard a single complaint.It wasn’t until late 2024 that they realized the help pages still mentioned email addresses that were forwarding to Moolenaar’s old email account. “Just two weeks ago or so, I changed those, so now they have been forwarded to my address,” Brabandt told his audience in November.Vim famously urges its users to contribute to Moolenaar’s favorite charity, the International Child Care Fund Holland, and Brabandt says the Moolenaar family is still maintaining Bram’s Paypal account for those donations (still linked to from Vim.org). After Moolenaar passed a lot of people donated to the ICCF, with another 90,000 euros donated in 2024. Brabandt is also committed to making sure those donations go through as intended — and says he’s not planning to create any Vim sponsorships any time in the near future.There was one change made: Bram Moolenaar’s feature which allowed ICCF donors to vote on future Vim features was shut down. It was hard to figure out which ICCF donations should be linked back to Vim.org users. (“I’m not sure how Bram did it in the past,” Brabandt says, and “the other people from the ICCF weren’t able to tell me!”) But in reality, it turns out that most of the new enhancement requests and issues are already coming from other sources like GitHub and Vim’s own to-do list.So what does the future hold? Vim plans “a bit more potentially controversial changes” for the upcoming release of Vim 9.2, Brabandt told the audience. These include supporting the XDG specification’s base-directory specifications (“The community has been wishing for it at least maybe 10 years.”) and better support for Wayland. There are a few new options and plugins and some inevitable bug fixes.So while changes are being made, this led Brabandt to a quietly momentous statement on the future of Vim. “However, currently I would say Vim is more or less in maintenance mode. I don’t think any of the maintainers can perform full-time work on Vim or bigger features.” As an example, he’s aware the Neovim community has been making big changes like support for parsing library Tree-sitter, but adding that to Vim would take a “tremendous effort… I’m not quite sure we can achieve it, at least not in the near-term.”But Brabandt announced another worthy goal: making sure that the community is healthy. And this means welcoming new contributors and making it easy for them to start contributing code. Brabandt has even imported some automatic code-formatting tools, since before Vim’s source code used an idiosyncratic formatting style that Brabandt called “strange. It’s basically Bram’s style of working, which is okay, but it doesn’t help new users.”A later slide suggested things people could work on include “Tree-Sitter integration?” along with a GTK-version of Vim’s GUI interface and more advanced terminal features. Vim’s spell-checking code, for instance, “hasn’t been touched for a few years.”“If you’re looking for big new features in the future, we do depend on the community to help us with this,” Brabandt said. But he always advises new contributors to “start small” while they’re first getting familiar with the codebase.And for right now, “most of the changes that have been merged are relatively self-contained, small-feature sets, which can be easily tested and don’t have that much impact on other parts of the code.”Testing, Refactoring and Maybe Retiring That Python 2 InterfaceThey’re still using “defensive and safe” C coding — Brabandt says refactoring everything into a modern programming language like Rust just isn’t an option right now. There’s a comprehensive suite of tests that he’s running over all changes — and every day they run the code-analyzing tool Coverity. And going forward, they’ll refactor parts of the code “which are quite long and lengthy and complex and hard-to-understand.” (Does Vim really still need an external interface to Python 2? Since the Python community moved on to Python 3 years ago, Brabandt believes it’s an example of one of the outdated interfaces that could be retired “at some point in the future.”)A big policy goal is making sure to continue Vim’s backward compatibility. And of course, learning from the past, Brabandt put up a slide titled “The new Vim Project — future,” which included a key “policy” bullet point: “Better documentation of (internal) processes.”Brabandt said he came up with these policy principles while going through Moolenaar’s backlog of outstanding pull requests.But another improvement he’d like to see is just a better understanding of Vim’s community — and he’s even considering a user survey. Toward the end of his talk, Brabandt told the audience what he’s learned since Moolenaar’s passing: that maintaining Vim is  — and that it’s a full-time job. “It’s not only about writing code; it’s about managing the community.” And that means  to that community — “Listening to their requests, fixing bugs that come up and making sure that we can keep up and do what the community wants.”“It’s an open-source project — that means the community can contribute and should contribute and also help us steer the project into the future.”And Brabandt said there’s already a clear signal of that healthy community: the Vim conference itself.]]></content:encoded></item><item><title>Flea-Scope: $18 Source Available USB Oscilloscope, Logic Analyzer and More [pdf]</title><link>https://rtestardi.github.io/usbte/flea-scope.pdf</link><author>burgerone</author><category>hn</category><pubDate>Sun, 16 Feb 2025 15:08:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Finding Flow: Escaping digital distractions through deep work and slow living</title><link>https://www.ssp.sh/blog/finding-flow/</link><author>articsputnik</author><category>hn</category><pubDate>Sun, 16 Feb 2025 11:40:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Johann Hari says in Stolen Focus that rats and pigeons can be manipulated as we want. Just give them food whenever they do what you want them to. And shortly after, they will repeat that over and over again.This made me think. In times when Instagram and other apps give us likes, hearts, and views on things we post, how much does big tech influence our behavior?Aren’t they the same as the researcher, feeding us with dopamine to tell us to do what they want? Are they doing the same as the researcher who feeds the rats or pigeons?This question, and recent improvements and tinkering with my flow as I started working for myself, made me ask how we can control the addiction and the influence and find a better way to slow living.Johann continues that people who don’t follow the above dopamine flow are artists and painters.Instead of sharing or selling them immediately and getting rewarded, they draw the next one. The drawing, the way to the end, is the reward, not the likes or selling the art. After hundreds of interviews, Mihaly Csikszentmihalyi said all artists mentioned something like a Flow State.  that they just tried to be there.This is also what I find most rewarding and experienced as an author: writing in the flow state, as I’m writing these words right now when I am merely in control, but the words flow out of my fingers. I guide them, but I do not write or choose the words.This flow and the outcome of what the article looks like after are just magical.I have rediscovered the effects of the environment lately when working for myself when I had to find productivity in my everyday work without a boss telling me what to do. At home, I wasn’t always focused enough. I had all the distractions. Going to the library usually produces my best work.  Focused without distractions. But always going there, sometimes it’s open late, or closed. Another great way to write and be creative is to use coffee shops, specifically moving to the next every 1.5 to 2.5 hours. It’s the sweet spot for me between being creative and getting distracted. And interestingly enough, the same for CGP Grey in his Redundant Office podcast, he said the same.Music, especially non-vocal music on repeat or dedicated Music that keeps you focused, helps you stay in the flow longer. It’s another way of signaling an environment to the brain, like “Now is work time”. I also use a dedicated browser; for example, I use Zen browser for work and Brave for everything else. Zen, the work browser, has no extensions. No password manager to automatically log in and no history to quickly choose the latest website distraction. These little obstacles make it harder to distract, whereas my Brave has all the convenience at one tab away.Another huge benefit of my writing work is that all my files are offline in my Obsidian Vault. No need to go online or log in to a web app to get going. It’s just there to start writing, brainstorming, and getting into the flow. Another way to stay in the flow is to use Markdown files and Vim motions. The Markdown files are simple text files that can be easily accessed and moved around. The key here is that the text’s formatting allows me to move and restructure entire chapters or ideas without friction. Adding the vim motions, I can do everything in the comfort of my keyboard. As I use them daily, they are muscle memory, and I can write at the speed of thought.My second brain, offline notes in my Obsidian vault, allows me to create connections, observe the graph, and see connected or relevant notes that could influence my thinking on the topic I am writing. A note on a book I wrote 10 years ago, with some insight that didn’t touch much back then, but connected to the current thought, with more knowledge, can make all the difference.One crucial ingredient that is mostly overlooked is breaks. Small drinking breaks and walking around as observed with the 1.5-2.5h sweet spot. But much more, long periods off, vacations where you don’t think of work or related. Your brain has time to wander. Good sleep to a certain extent is also a form of break. The more we learn about sleep (see Why We Sleep by Matthew Walker), the more prevalent it is how important it is. I also notice to myself. If I sleep, the chances of being productive in the morning, achieving my goals early, and getting a sense of calm throughout the rest of my day are much higher. As we process primarily during sleep, especially with the REM (Rapid Eye Movement) sleep that we usually get only after sleeping for 7-8 hours, is where the brain is cleansing itself, and making it ready for the next day.I am on vacation writing this article, and I haven’t written for a couple of days. I am enjoying time on vacation, nature, and with family, letting my brain drift and follow its thoughts. These ideas, connections, and words basically flow themselves, as my brain is super relaxed. While being offline and taking a break, the brain makes sense of things in the subconscious.There Are Different Kinds of RestTo unblock, you can unblock or rest in different types of rest:Another way of taking breaks is small . I explored these in my Pathless Path. These are like Sabbaticals. I did many of these early in my life, and they heavily influenced who I am today and how my life unfolded. But more on this in Finding My Pathless Path.Slowing down, Slow LivingSlowing down, and just , is another essential concept. Slowing down, being more present. As Cal Newport says in his book Slow Productivity, his main points are:If we apply this to life, we get a Deeper Life. One that is more fulfilled and present. It is about finding the balance between the important and the urgent.Turning off the phone, without the pressure or the urge to check if someone has messaged you, if you got a like on social media, your brain starts wondering. You start getting ideas and thoughts to ponder. But these don’t start immediately. If you have been distracted, living a fast life, are constantly on the go, and have no time to think about anything, without defined principles to work towards, without clarity, your brain can’t just switch, and be . But practicing it, like meditation or yoga, is undoubtedly worth it.Another book by Cal Newport is Digital Minimalism, which goes in the same direction—trying to use fewer digital mediums, less social media, and less distraction. I thoroughly recommend it, too.The basics: Food, Sleep, and SmartphonesThe basis on top of these, is also a good sleep, nutritious food. And the phone needs to be away on do not disturb mode, or best in another room.Shallow vs. Deep HappinessThese can also be summarized into what I call shallow happiness versus deep happiness. Buying something new, eating fast food, having another coffee, or watching a YouTube video all give us pleasure, but only in the . Writing an article or learning something hard, like coding or vim motions, is initially hard. But the reward is long-term. We get  from being in the flow and perfecting something we have learned and perfected over a long time.Maybe that’s the secret of a Deep Life? Not reading news, journaling, praying, meditating, or following a Stoic practice. Reading books is a similar thing; it’s hard to listen to or read an entire book, but when we do, especially if we learn something new, without distractions, in a format the author spent a year or more to curate and put into its format, is so calming and almost peaceful. In fact,   is known to be the biggest and most common version of flow.There’s much more to dig into, which I might in another article about “Why we are here on earth?”. This article has been cooking over many months and years already. It will go much deeper and more philosophical about why we even do what we do. It will be connected and even strengthen the flow more if we have clear principles, a clear path, and a passion for why we do something.But now, instead, I want to focus on how to get into a flow state or how to unblock us when we feel stressed and not in the mood to do the Deep Work. If we easily get distracted.These are all based on personal experience and books I read. They don’t always work, but one variation usually does the trick by iterating on them.These are more pointers related to this article’s topic, thoughts, or steps I followed, some of which are already on my Second Brain.Walk every day; nature is tiding our thoughts, just like naps and sleep. Walking, or going outside, is suitable for mental, spiritual, and physical health. When I start walking, the ideas flow, my brain calms down, and I slowly begin to think and process anything I am pondering. It’s best when done in . Nature in itself is a remedy for calming down. It has helped all the big thinkers before our time and will help many after ours.Like our inner monkeys or daemons, they don’t want us to go out of bed or get fresh air. Walking frees us from them—the opposite of what social media or smartphones do.Sometimes, bring a paper notebook or camera to capture ideas or thoughts. But other times, go deliberately without.Try to change the medium. If writing on a computer does not work, try pen or paper. Or try talking. As Jonny Miller said:There is no talking block, only writer's block.Interestingly, if we change the medium, we can unblock it:Change environment, physically as well as digitally:
: Go to another room, a coffee shop, or what works best for me, Libraries.: Also, changing your computer environment, e.g., changing from dark to light mode or changing the theme, can give you a sense of change and fresh energy.Timebox yourself, also called Pomodoro Technique. It has the same effect and works in the train with only the battery. The urge to finish before you run out of battery will give you focus and added clarity.Change between Text and Visual: I use a lot of MindMap on paper, but Obsidian Canvas, Draw with ExaliDraw, and others work as well.Try to get into the same habit when working (same place, same music, etc.). Your brain will know it needs to write before you even start.
Check Spaceship You by CGP Grey, showcasing how designated space helped during the pandemic, but also for clarity.How I get into Deep Work — FlowThe above strategies to unblock are also related to Deep Work and how I get into deep work:Journal before bed what 1-2 tasks you must/want to achieve tomorrow.Go to bed early (you usually don’t achieve much at night).Get up before the kids :) (don’t turn on the phone).Later, take a walk in nature.Change environment/room if stuck.Clear some time in your calendar.Get rid of distractions: Turn off notifications, close Slack, and put your phone out of reach. Clear your space and your mind. Put on concentration music, and get rid of any clutter around. Get to work on a specific task, taking periodic short breaks.Deeper: Acknowledge your fears, write from authenticity and a place of .
 is stored in our body (not in our mind) are like a GPS, help us to find truth and navigate aroundAlso, like , add boundaries and constraints like no internet, use another browser for searching the web, and suddenly ideas flow. Unlimited freedom is super-blocking, especially for creativity. Great read on the topic: If You Have Writer’s Block, Maybe You Should Stop Lying.When I don’t have inspiration or I have a block, I do nothing. . And it’s absolutely because of the deeper inspiration that I’m blessed to feel. I feel it’s been cultivated. I’m connected to it, and I know it’s real, and it doesn’t have to greet me every day. I know it’s there.—Turn Off, Shut Down, and Re-Energize for a Happy LifeSomething that is easy to say but hard to do. I enjoy  or a long bicycle ride. It does not matter what you do, but that you do something without distraction. I was listening to books on the walks for a long time, but this always felt like more work. Sure, if you have a great book, listening can be fun too, but I did my best to calm down when going without any decision—just me and nature.From the book “The Well-Lived Life: A 102-Year-Old Doctor’s Six Secrets to Health and Happiness at Every Age”, a comment by Liong on Goodreads says the six secrets may help us live a long, happy, and healthy life:Let go of the past (Forget the past regrets and anger).Live in the present moment. (Live now; don’t stay in your history and worry about your future).Connect with nature. (Read and find out yourself).Eat a healthy diet. (Choose and be aware of what you eat daily).Get regular exercise. (Try exercise every day).Find your purpose. (Having a purpose is vital to living longer).Nature: How It Flourishes CreativityTake inspiration from nature. Nature is so powerful.For example, watching a tree through the four seasons: In summer, it is green with many leaves; in autumn, it loses them all; in winter, it looks dead. But instead, it builds and prepares its strength for the spring. Where the colorful flowers sprout, it’s calming and inspiring.Take gardening; it takes so much patience to grow those three. It puts time into different perspectives. Threes  houses and plants are harvested over many years. If you sell a home, things might have been there long before the house.I like to get inspiration from nature. I think of my creative process as gardening. Things inside us build up , but great things will come out with constant nurturing: Art, a thought, an idea, anything. It doesn’t matter which day. It’s seasonal, similar to creativity; we can’t be creative all year.I hope any of these helped you; more so, it helped me to clarify focus in connection with the ever-growing (primarily digital) distractions to find a better life worth living—A slower and deeper life could be possible.Books and related reads I recommend that gave me lots of inspiration:]]></content:encoded></item><item><title>“A calculator app? Anyone could make that”</title><link>https://chadnauseam.com/coding/random/calculator-app</link><author>pie_flavor</author><category>hn</category><pubDate>Sun, 16 Feb 2025 10:16:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>50 Years of Travel Tips</title><link>https://kk.org/thetechnium/50-years-of-travel-tips/</link><author>marban</author><category>hn</category><pubDate>Sun, 16 Feb 2025 09:31:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I’ve been seriously traveling for more than 50 years, and I’ve learned a lot.I’ve traveled solo, and I’ve led a tour group of 40 friends. I’ve slept in dormitories and I’ve stayed in presidential suites with a butler. I’ve hitchhiked penniless for months, and I’ve flown by private jet. I’ve traveled months with siblings, and with total strangers. I’ve gone by slow boat and I’ve ridden my bicycle across America, twice. I’ve been to the largest gathering of humans on the planet, and trekked into remotest areas on the planet on my own. I’ve paid for luxury tours, and I’ve done my own self-guided tours. I regularly travel for business, and once I went to Hawaii on a door-prize award. I’ve circumnavigated the globe in only 48 hours, and I traveled uninterrupted for 9 months. I’ve gone first class and third class, sometimes on the same trip. So far I’ve visited half the world’s countries, and usually manage to get far from the capital city when I do. Here is what I know about how to travel.There are two modes of travel; retreat or engage. People often travel to escape the routines of work, to recharge, relax, reinvigorate, and replenish themselves— R&R. In this mode you travel to remove yourself from your routines, or to get the pampering and attention you don’t ordinarily get, and ideally to do fun things instead of work things. So you travel to where it is easy. This is called a vacation, or R&R. The other mode is engagement and experience, or E&E. In this mode you travel to discover new things, to have new experiences, to lean into an adventure whose outcome is not certain, to meet otherness. You move to find yourself by encountering pleasures and challenges you don’t encounter at home. This kind of travel is a type of learning, and of the two modes, it is the one I favor in these tips.Organize your travel around passions instead of destinations. An itinerary based on obscure cheeses, or naval history, or dinosaur digs, or jazz joints will lead to far more adventures, and memorable times than a grand tour of famous places. It doesn’t even have to be your passions; it could be a friend’s, family member’s, or even one you’ve read about. The point is to get away from the expected into the unexpected.If you hire a driver, or use a taxi, offer to pay the driver to take you to visit their mother. They will ordinarily jump at the chance. They fulfill their filial duty and you will get easy entry into a local’s home, and a very high chance to taste some home cooking. Mother, driver, and you leave happy. This trick rarely fails.Make no assumptions about whether something will be open. There are other norms at work. If possible check at the last minute, if not, have a plan B.Crash a wedding. You are not a nuisance; you are the celebrity guest! The easiest way to do this is to find the local wedding hall where weddings happen on schedule and approach a wedding party with a request to attend. They will usually feel honored. You can offer the newlyweds a small token gift of cash if you want. You will be obliged to dance. Take photos of them; they will take photos of you. It will make your day and theirs. (I’ve crashed a wedding in most of the countries I have visited.)Don’t balk at the spendy price of admission for a museum or performance. It will be a tiny fraction of your trip’s total cost and you invested too much and have come too far to let those relative minor fees stop you from seeing what you came to see.Google maps will give you very detailed and reliable directions for taking public transit, including where to make transfers in most cities.When visiting a foreign city for the first time, take a street food tour. Depending on the region, the tour will include food carts, food trucks, food courts, or smaller eateries. It will last a few hours, and the cost will include the food. You’ll get some of the best food available, and usually the host will also deliver a great introduction to the culture. Google “street food tour for city X.”The most significant criteria to use when selecting travel companions is: do they complain or not, even when complaints are justified? No complaining! Complaints are for the debriefing afterwards when travel is over.As in any art, constraints breed creativity. Give your travel creative constraints: Try traveling by bicycle, or with only a day bag for luggage, or below the minimum budget, or sleep only on overnight trains. Mix it up. Even vagabonding can become a rut.Renting a car is easier than ever today, even in developing countries, and oftentimes the best bet for getting around if you are headed for many places outside of cities. It is an option worth considering, especially if you are 2 to 3 people traveling. On the other hand, there are still plenty of places where you don’t want to drive because of chaotic roads, lawless attitudes, and unfavorable liabilities. In those places hiring a driver plus car for a multi-day trip is often a surprisingly appealing bargain—especially if you have 2 to 3 people to split the costs. The total could be less than taking trains and taxis, and you get door to door service, and often a built-in guide who knows the local roads and also local festivities and best places to eat. They will be at least 2x the cost of renting a car, but for some kinds of travel 2x as good. If you are a spontaneous traveler, a hired driver is by far the best option allowing you to change your itinerary immediately as mood, weather, or lucky timing dictate. I usually find drivers by searching travel forums for recommendations. I score candidates primarily by their communication skills.If you are fortunate, a fantastic way to share your fortune is to gift a friend the cost of travel with you. You both will have a great time.Go to a cemetery. Look for sacred places. People live authentically there. Don’t just visit the markets, but also go to small workshops, hardware stores and pharmacies –  places with easy access to local practices. See how it’s different and the same all at once.FlightAware is the best free phone app for the status of your flight. It will often tell you about delays hours before the airline will. Tip: use FlightAware to check whether your plane has even arrived at your departure airport.Sketchy travel plans and travel to sketchy places are ok. Take a chance. If things fall apart, your vacation has just turned into an adventure. Perfection is for watches. Trips should be imperfect. There are no stories if nothing goes amiss. Your enjoyment of a trip will be inversely related to the weight of your luggage. Counterintuitively, the longer your trip, the less stuff you should haul. Travelers still happy on a 6-week trip will only have carry-on luggage. That maximizes your flexibility, enabling you to lug luggage up stairs when there is no elevator, or to share a tuk-tuk, to pack and unpack efficiently, and to not lose stuff. Furthermore, when you go light you intentionally reduce what you take in order to increase your experience of living. And the reality of today is that you can almost certainly buy whatever you are missing on the road.Getting an inside tour is the ultimate travel treat. How about a factory tour, a visit to an Amish home, or backstage at an opera? When I travel for business I will sometimes ask for inside access to an uncommon place in lieu of a speaking fee. You are aiming for experiences that simple money can’t buy. Good ones will take planning ahead.It is always colder at night than you think it should be, especially in the tropics. Pack a layer no matter what.Planning your itinerary: You want to see it all and you are likely to never return, so the temptation is to pile it on, maximize your visit. Since we are in X, and Y and Z  are so close, we might as well see Y and Z….. Paradoxically when you are traveling you should minimize the amount of time you spend in transit—once you arrive. The hard-to-accept truth is that it is far better to spend more time in a few places than a little time in a bunch of places. To book a train anywhere in the world outside your home country, your first stop should be The Man in Seat 61, a sprawling website which will conveniently help you book the train you want. In 53 years of travelling with all kinds of people, I’ve seen absolutely no correlation between where you eat and whether you have intestinal problems, so to maximize the enjoyment of local foods, my rule of thumb is to eat wherever healthy-looking locals eat.The list of most coveted cities to visit have one striking thing in common—they are pedestrian centric. They reward walking.Better online hotel sites like Booking.com have map interfaces which allow you to select hotels by their location. Whenever possible I book my hotel near to where it is best to walk, so I can stroll out the door and begin to wander. For a truly memorable trip, go without reservations, just winging it along the way. If you like somewhere, stay a day longer, or if you don’t, split a day earlier. If the train is full, take a bus. That freedom can be liberating.The Google Translate app for your phone is seriously good, and free. It will translate voice, text, or script to and from 250 languages. Use for deciphering menus, signs, talking with clerks, etc. It is often a lifesaver.Large-scale luxury cruises have no appeal to me, yet a small boat cruise is an entirely different species and a valid option worth considering. The advantage of a cruise is that your hotel travels with you, so you unpack only once. It is especially useful for small groups because it eliminates the eternal negotiation of deciding where to eat. (You always eat on the boat.) The advantages of a small boat cruise over a huge boat are several: you disembark very quickly, very often, at smaller more intimate places than large boats can do. And the options for activities are more active than just shopping: such as snorkeling, kayaking, bicycling, hiking, visiting local families and communities. Overall you spend far more time doing things off the boat than on. I define a small boat as 40 passengers or fewer. The per day cost is high, but almost every minute of it is quality time, unlike a series of bus rides. Examples of places I’ve loved a small boat cruise; The Galapagos, Alaska inland passageway, Mekong River, Coast of Turkey, and Kerala, India.The rate you go is not determined by how fast you walk, bike or drive, but by how long your breaks are. Slow down. Take lots of breaks. The most memorable moments—conversations with amazing strangers, an invite inside, a hidden artwork—will usually happen when you are not moving.I generally find “professional” tour guides uninteresting, and too scripted. They are mostly repeating what can be found in guide books. So I rarely hire them. I much prefer to have a friend or local acquaintance show me what interests them in an ad hoc way, with no script. Let friends know you are coming to their area.A few laundry detergent sheets in a tiny ziplock bag weigh nothing and won’t spill and are perfect for emergency laundry washing in the sink or shower. These days it is mandatory that you are connected. You need cell coverage as well as wifi. You’ll want robust mobile coverage for navigation, translation apps, ride shares and a digital wallet for payments. Best option is to use a carrier with “free” international plans (such as T-Mobile or Google Fi) so you need to do nothing. Second best is to get either a sim card or e-sim for your phone for your visiting country. E-sim apps (such Airalo) can be loaded by yourself virtually. Sims and e-sims are also sold at most international airports when you exit. Most are reputable. One tip, turn off your photo and video cloud backup while on the sim to reduce data usage. People in other places are not saints. You might get cheated, swindled, or taken advantage of. Paradoxically, the best way to avoid that is to give strangers your trust and treat them well. Being good to them brings out their good. If you are on your best behavior, they will be on their best behavior. And vice versa. To stay safe, smile. Be humble and minimize your ego. I don’t know why that works everywhere in the world—even places with “bad” reputations—but it does.You can get an inexpensive and authentic meal near a famous tourist spot simply by walking at least five blocks away from the epicenter. Digital wallets on your phone are displacing local currencies in many places. For instance I did not use any cash on my last trips to the UK and China. And in places where it has not completely eliminated cash you can reduce your cash needs by half with mobile payments. Set up your Apple pay, Google Pay or Alipay before you leave. There is no need to exchange money anywhere, especially at airports. Get any cash you need at local ATMs, which are now everywhere. Use a card that does not charge, or reimburses, a foreign fee. If you detect slightly more people moving in one direction over another, follow them. If you keep following this “gradient” of human movement, you will eventually land on something interesting—a market, a parade, a birthday party, an outdoor dance, a festival. Splurge in getting the most recent version of a guidebook to your destination. It is worth the price of a lunch to get the latest, most helpful, reliable information. I supplement the latest guidebook research with recommendations suggested in travel forums online. Guidebooks have depth and breadth, while forums offer speed—results from a week ago. If you are starting out and have seen little of the world, you can double the time you spend traveling by heading to the places it is cheapest to travel. If you stay at the budget end, you can travel twice as long for half price. Check out The Cheapest Destination Blog. In my experience, these off-beat destinations are usually worth visiting.In many parts of the world today motorcycles play the role of cars. That means you can hire a moto-taxi to take you on the back seat, or to summon a moto-taxi with an uber-like app, or to take a motorcycle tour with a guide doing the driving. In areas where motorcycles dominate they will be ten times more efficient than slowly going by car.Even if you never go to McDonalds at home, visit the McDonalds on your travels. Surprisingly, their menus are very localized and reflect different cuisines in a fun and easy way, with unexpected versions of familiar things. Very illuminating!Put inexpensive Apple AirTags into your bags, so you can track them when they are out of your sight. More and more airlines are integrating AirTags into their system to help find wayward bags. The tags work for luggage left in hotel storage, or stashed beneath the bus, or pieces you need to forward. For the best travel experiences you need either a lot of money, or a lot of time. Of the two modes, it is far better to have more time than money. Although it tries, money cannot buy what time delivers. You have enough time to attend the rare festival, to learn some new words, to understand what the real prices are, to wait out the weather, or to get to that place that takes a week in a jeep. Time is the one resource you can give yourself, so take advantage of this if you are young without money.Being beautiful, or well crafted, or cheap is not enough for a souvenir. It should have some meaning from the trip. A good question you may want to ask yourself when buying a souvenir is where will this live when I get home?The best souvenirs from a trip are your memories of the trip so find a way to memorialize them; keep a journal, send updates to a friend, take a sketchbook, post some observations, make a photo book. When asking someone for a restaurant recommendation, don’t ask them where is a good place you should eat; ask them where they eat. Where did they eat the last time they ate out?Here in brief is the method I’ve honed to optimize a two-week vacation: When you arrive in a new country, immediately proceed to the farthest, most remote, most distant place you intend to reach during the trip. If there is a small village, remote spa, a friend’s farm, or a wild place you plan on seeing on the trip, go there immediately. Do not stop near the airport. Do not rest overnight in the arrival city. Do not pause to acclimate. If at all possible proceed by plane, bus, jeep, car directly to the furthest point without interruption. Make it an overnight journey if you have to. Then once you reach your furthest point, unpack, explore, and work your way slowly back to the big city, wherever your international departure airport is.In other words you make a laser-straight rush for the end, and then meander back. Laser out, meander back. This method is somewhat contrary to many people’s first instincts, which are to immediately get acclimated to the culture in the landing city before proceeding to the hinterlands. The thinking is: get a sense of what’s going on, stock up, size up the joint. Then slowly work up to the more challenging, more remote areas. That’s reasonable, but not optimal because most big cities around the world are more similar than different. All big cities these days feel same-same on first arrival. In Laser-Back travel what happens is that you are immediately thrown into Very Different Otherness, the maximum difference that you will get on this trip. You go from your home to extreme differences so fast it is almost like the dissolve effect in a slide show. Bam! Your eyes are wide open. You are on your toes. All ears. And there at the end of the road (but your beginning), your inevitable mistakes are usually cheaper, easier to recover from, and more fun. You take it slower, no matter what country you are in. Then you use the allotted time to head back to the airport city, at whatever pace is your pace. But, when you arrive in the city after a week or so traveling in this strangeness, and maybe without many of the luxuries you are used to, you suddenly see the city the same way the other folks around you do. After eight days in less fancy digs, the bright lights, and smooth shopping streets, and late-night eateries dazzle you, and you embrace the city with warmth and eagerness. It all seems so … civilized and ingenious. It’s brilliant! The hustle and bustle are less annoying and almost welcomed. And the attractions you notice are the small details that natives appreciate. You see the city more like a native and less like a jaded tourist in a look-alike urban mall. You leave having enjoyed both the remote and the adjacent, the old and new, the slow and the fast, the small and the big. We’ve also learned that this intensity works best if we aim for 12 days away from home. That means 10 days for in-country experience, plus a travel day (or two) on each end. We’ve found from doing this many times, with many travelers of all ages and interests, 14 days on the ground is two days too many. There seems to be a natural lull at about 10 days of intense kinetic travel. People start to tune out a bit. So we cut it there and use the other days to come and go and soften the transitions. On the other hand 8 days feels like the momentum is cut short. So 10 days of intensity, and 12 days in a country is what we aim for. Laser-back travel is not foolproof, nor always possible, but on average it tends to work better than the other ways I’ve tried.If you work while you travel, or work remotely, you may enjoy our newsletter Nomadico, which is a weekly one-pager with four brief travel tips. It’s free. (Thanks to early readers, Craig Mod, Derek Sivers, Chris Michel and Will Milne.)]]></content:encoded></item><item><title>US government struggles to rehire nuclear safety staff it laid off days ago</title><link>https://www.bbc.com/news/articles/c4g3nrx1dq5o</link><author>niuzeta</author><category>hn</category><pubDate>Sun, 16 Feb 2025 07:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[US media reported that more than 300 NNSA staff were let go, citing sources with knowledge of the matter.That number was disputed by a spokesperson for the Department of Energy, who told CNN that "less than 50 people" were dismissed from NNSA.The Thursday layoffs included staff stationed at facilities where weapons are built, according to CNN. The Trump administration has since tried to reverse their terminations, according to media outlets, but has reportedly struggled to reach the people that were fired after they were locked out of their federal email accounts.A memo sent to NNSA employees on Friday and obtained by NBC News read: "The termination letters for some NNSA probationary employees are being rescinded, but we do not have a good way to get in touch with those personnel.""Please work with your supervisors to send this information (once you get it) to people's personal contact emails," the memo added.Last week, nearly 10,000 federal workers were let go across several agencies, according to multiple US outlets. That figure was in addition to the estimated 75,000 workers who have accepted an offer from the White House to leave voluntarily in the autumn. Trump is working to slash spending across the board, abroad and at home, and going so far as to call for eliminating the education department. He is getting help from the world's richest man, Elon Musk, who, through an effort called Department of Government Efficiency (Doge), has sent workers to comb through data at federal agencies and helped implement the "buyout" offer.Last week, the Trump administration ordered agencies to fire nearly all probationary employees, those who had generally been in their positions for less than a year and not yet earned job protection. That included the NNSA staff members.Altogether, the move could potentially affect hundreds of thousands of people. Several of the Trump administration's efforts to shrink the government's size and spending have been met with legal challenges. More than 60 lawsuits have been filed against the Trump administration since the president was inaugurated on 20 January.]]></content:encoded></item><item><title>Gixy: Nginx Configuration Static Analyzer</title><link>https://github.com/dvershinin/gixy</link><author>mmsc</author><category>hn</category><pubDate>Sun, 16 Feb 2025 04:06:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How Medical Research Cuts Would Hit Colleges and Hospitals in Every State</title><link>https://www.nytimes.com/interactive/2025/02/13/upshot/nih-trump-funding-cuts.html</link><author>erickhill</author><category>hn</category><pubDate>Sun, 16 Feb 2025 02:07:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A proposal by the Trump administration to reduce the size of grants for institutions conducting medical research would have far-reaching effects, and not just for elite universities and the coastal states where many are located.Also at risk could be grants from the National Institutes of Health to numerous hospitals that conduct clinical research on major diseases, and to state universities across the country. North Carolina, Missouri and Pennsylvania could face disproportionate losses, because of the concentration of medical research in those states.Based on spending in the 2024 fiscal year.In the 2024 fiscal year, the N.I.H. spent at least $32 billion on nearly 60,000 grants, including medical research in areas like cancer, genetics and infectious disease. Of that, $23 billion went to “direct” research costs, such as microscopes and researchers’ salaries, according to an Upshot analysis of N.I.H. grant data.The other $9 billion went to the institutions’ overhead, or “indirect costs,” which can include laboratory upkeep, utility bills, administrative staff and access to hazardous materials disposal, all of which research institutions say is essential to making research possible.The N.I.H. proposal, which has been put on hold by a federal court, aims to reduce funding for those indirect costs to a set 15 percent rate that the administration says would save about $4 billion a year. The Upshot analysis estimates that a 15 percent rate would have reduced funding for the grants that received N.I.H. support in 2024 by at least $5 billion. The White House said the savings would be reinvested in more research, but the rate cuts would open up sizable budget holes in most projects at research institutions.It is not clear whether those organizations can fill the gaps with other funding sources or by shifting how they apply for grants. Instead, many officials at universities and hospitals have said that they may have to pull back on medical or scientific research.“It’s not an overstatement to say that a slash this drastic in total research funding slows research,” said Heather Pierce, senior director for science policy at the Association of American Medical Colleges, which has sued along with other education and hospital associations to block the policy. And slower scientific progress, she said, would affect anyone who depends on the development of new treatments, medical interventions and diagnostic tools.We estimate that virtually all universities and hospitals would see fewer funds on similar projects in the future. The 10 institutions that receive the most money from N.I.H. stand to lose more than $100 million per year on average.To understand how the change would work, let’s look at one grant for about $600,000 sent last year to the University of Alabama at Birmingham to study whether exercise can improve memory for people with epilepsy.The calculation above, which we have repeated for every grant paid last year, is a bit simplified. In reality, the researchers would lose even more money than we’ve shown, because of the way indirect funding is calculated (see our methodology at the bottom of this article).Our analysis also makes some other conservative assumptions given the policy’s uncertainty. We assume, for instance, that the new 15 percent rate is a flat rate that all grantees would receive, and not a maximum rate (a distinction left unclear in the N.I.H. guidance). We also assume that the change applies not just to institutions of higher education, but also to all kinds of grantees, including hospitals.In a statement, the White House indicated it would reserve any savings for additional research grants. “Contrary to the hysteria, redirecting billions of allocated N.I.H. spending away from administrative bloat means there will be more money and resources available for legitimate scientific research, not less,” said Kush Desai, a White House spokesman.The N.I.H. announcement, however, coincides with the Trump administration’s moves to cut spending across the government, and with the N.I.H.’s withholding of funding for grants — their direct and indirect costs alike — in apparent conflictwith separate court orders.The N.I.H. guidance document includes a number of conflicting statements and statistics the Upshot could not reconcile. The N.I.H. also declined to answer questions about the policy and about its public-facing data tracking grant spending.The N.I.H. since 1950 has provided these overhead funds in a formulaic way, and since 1965, the government has used a rate individually calculated for each institution. Federal officials review cost summaries, floor plans and other information to determine that rate. That number can be higher for institutions in more expensive parts of the country, or for those that use more energy-intensive equipment. The proposal from the Trump administration would set aside those differences in standardizing the rate at 15 percent for every grantee.The lists below estimate what would have happened to the 10 universities and hospitals that received the most N.I.H. grant money in the 2024 fiscal year, if the formula change had been in effect then.Largest N.I.H. grant recipients among colleges, universities and medical schoolsUniversity of California, San FranciscoUniversity of PennsylvaniaColumbia University Health SciencesSource: National Institutes of HealthBased on spending in the 2024 fiscal year.Largest N.I.H. grant recipients among hospitalsMassachusetts General HospitalVanderbilt University Medical CenterBrigham and Women’s HospitalBoston Children’s HospitalUniversity of Texas MD Anderson Cancer CenterChildren’s Hospital of PhiladelphiaDana-Farber Cancer InstituteCincinnati Childrens Hospital Medical CenterBeth Israel Deaconess Medical CenterCedars-Sinai Medical CenterSource: National Institutes of HealthBased on spending in the 2024 fiscal year, which extends from Oct. 1 to Sept. 30.If courts allow the change to move forward, some of its consequences are hard to predict.Advocates for the policy change note that these organizations receive numerous other federal subsidies. Most universities and research hospitals are nonprofits that pay no federal taxes, for example. The N.I.H. announcement also noted that these same institutions often accept grants from charitable foundations that offer much lower overhead rates than the federal government, a signal that universities and hospitals willingly pursue research opportunities with less supplemental funding.Because the indirect payments are based on broad formulas and not specific line items, critics say institutions may be diverting these federal dollars into unaccountable funds to pay for programs that taxpayers can’t see, such as the kinds of diversity, equity and inclusion programs targeted by the Trump administration.“That’s how you get things like the ability of administrators to use larger overhead pools of money to build out D.E.I. bureaucracies, or to fund Ph.D. programs in the humanities,” said Jay Greene, a senior research fellow in the Center for Education Policy at the Heritage Foundation, a conservative research group. Mr. Greene was the coauthor of a 2022 article urging the N.I.H. to cut or eliminate indirect grant funding. But he did not have specific examples to cite of research funds being spent in this way.Researchers say the indirect funds have a branding problem, but are a necessary component of research.“The term ‘indirect costs’ or the alternative term ‘overhead’ sounds dangerously close to ‘slush fund’ to some people,” said Jeremy Berg, who was the director of the National Institute of General Medical Sciences at the N.I.H. from 2003 to 2011. “There are real costs somebody has to pay for, and heating and cooling university laboratory buildings is a real cost.”Some grant recipients already receive low overhead payments, but a large majority of them currently receive more than 15 percent, meaning they will need to make budgetary changes to absorb the loss. Among the 2024 grants that we analyzed, institutions that received more than $1 million in N.I.H. support got an average of 40 cents of indirect funding for every dollar of direct funding.Distribution of overhead funding at N.I.H.-funded institutions in 2024As a share of direct fundingSource: National Institutes of HealthCalculated for 613 institutions that received at least $1 million in funding in fiscal year 2024. Federally negotiated rates are higher than these.Universities and hospitals may adjust their overall budgets to keep supporting medical research by cutting back on other things they do. Some might be able to raise money from donors to fill the shortfalls, though most universities are already raising as much philanthropic money as they can.But many research institutions have said they would adjust by simply doing less medical research, because they would not be able to afford to do as much with less government help.Universities and hospitals might also shift the kinds of research they do, avoiding areas that require more lab space, regulatory compliance or high-tech equipment, and focusing on types of research that will require them to provide less overhead funding themselves. That may mean disproportionate reductions in complex areas of research like genetics.Those effects may be spread unevenly across the research landscape, as some organizations find a way to adjust, while others abandon medical research altogether.We’ve compiled a list of institutions that received at least $1 million in N.I.H. funding in the 2024 fiscal year, along with our estimates of how much less they would have gotten under the new policy. Most of these institutions are universities or hospitals, but there are also some private companies and nonprofit research groups. Our numbers tend to be underestimates of the cuts.To estimate changes in funding, we relied on data from RePORT, the N.I.H.’s online registry of grants and projects. We limited our analysis to grants listed within the 50 U.S. states, the District of Columbia or Puerto Rico. We also limited it to grants where the amount of indirect funding was known and where the combined indirect and direct funding was within five percent of the listed total funding. These filters resulted in removing many grants to private organizations such as domestic for-profits.We calculated how much indirect funding each grant would have received under the new guidance by multiplying the listed direct funding amount by 15 percent. We then compared that number to the listed indirect funding amount for each great to estimate the impact of the policy.There are two reasons our calculations are most likely conservative estimates of true reductions in funding. First, only a portion of the direct funding for each grant is considered to be “eligible” for the purposes of calculating indirect funding. For example, laboratory equipment and graduate student tuition reimbursements are deducted from the direct costs before applying the negotiated overhead rate, whereas our calculations assumed 100 percent of the listed direct costs would be eligible. We performed a more accurate version of our calculations for the 10 universities and 10 hospitals receiving the most N.I.H. funds by inferring their eligible direct costs from their reported negotiated rates. When we did this, we saw an additional increase in losses of about 20 percent.Second, we applied a 15 percent rate to all grants in the database, including those with an initial indirect rate  15 percent. An analysis by James Murphy helped inform this approach. According to our analysis, then, some grants would actually receive more money under the new guidance. If the new rate operated more like a cap — and grants with rates currently below 15 percent did not change — the overall reductions in funding would be larger, as the reductions would no longer be offset by some small number of funding increases.]]></content:encoded></item><item><title>The Sims Game Design Documents (1997)</title><link>https://donhopkins.com/home/TheSimsDesignDocuments/</link><author>krykp</author><category>hn</category><pubDate>Sun, 16 Feb 2025 01:06:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Blunderchess.net – blunder for your opponent every five moves</title><link>https://blunderchess.net/</link><author>eviledamame</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Feb 2025 00:22:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Create a QubesOS Gaming HVM with GPU PCI passthrough (2023)</title><link>https://forum.qubes-os.org/t/create-a-gaming-hvm/19000</link><author>transpute</author><category>hn</category><pubDate>Sat, 15 Feb 2025 22:48:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jellyfin: The Free Software Media System</title><link>https://jellyfin.org/</link><author>doener</author><category>hn</category><pubDate>Sat, 15 Feb 2025 22:39:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Jellyfin is Free Software, licensed under the GNU GPL. You can use it, study it, modify it, build it, and distribute it for free, as long as your changes are licensed the same way.The project relies entirely on contributions from volunteers. Want to help out? There’s lots of ways to do so, and you don’t even have to code! See our contribution guide for more details.The Jellyfin server and official clients are free to download, now and always. There are no costs, hidden or otherwise, to use Jellyfin, either for yourself, for your friends, or for your company. All our incidental costs are paid through donations from users like you.Jellyfin has no tracking, phone-home, or central servers collecting your data. We believe in keeping our software open and transparent. We’re also not in the media business, so the only media you see is your own.]]></content:encoded></item><item><title>Watt The Fox?</title><link>https://h.43z.one/blog/2025-02-12/</link><author>h43z</author><category>hn</category><pubDate>Sat, 15 Feb 2025 21:32:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[It's not nothing—about 1.5 Watt more.So, with a heavy heart, I decided to disable email notifications—even though I really wanted to keep them—but eliminating the white noise was my priority.I thought I had “fixed” the problem. Of course, I blamed Microsoft, right?But the red sound indicator on my i3 status bar kept lighting up occasionally.
    And it turned out other websites were also triggering the white noise.For instance, as soon as I clicked anywhere on x.com, the noise started. Similarly, whenever I listened to a translation on translate.google.com, there was the noise.So now I was really curious. What is going on here.I started to look up how you can play audio with HTML/JavaScript.
    There seem to be two ways: Either with the  tag or the WebAudio API.As Outlook plays sound dynamically, I knew it must use the WebAudio API.
    And to do anything with audio, you first have to create an .const audioCtx = new AudioContext();And already here I realized the problem. Just creating this AudioContext makes my speakers play white noise.The MDN article is pretty clear about it.AudioContext.suspend()
Suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process.

AudioContext.resume()
Resumes the progression of time in an audio context that has previouslyYet, most websites never bother suspending the AudioContext and create one without the immediate need for playing sound.Chrome stops the battery/CPU waste automatically afte some time. Firefox not. It just keeps playing the whitenoise.I understand that the websites are to blame here.But still, Cmon Firefox, protect me from this resource theft?!Oh and btw I suspect this also wastes my bluetooth headphones battery if they are connected?! Once I do a click on x.com the sending of white noise starts.To address this total mess, I created an extension that automatically suspends the AudioContext while also
tries to resume it if the websites wants to play sound.It's not perfect as resuming takes a little bit of time and it
  may not always resume, as there are multiple paths to starting audio. But it's good enough for me.Some Relevant Bugzilla reports

More fun stuff at h.43z.one.
Unshadow ban me at 𝕏]]></content:encoded></item><item><title>The European Vat Is Not a Discriminatory Tax Against US Exports</title><link>https://taxfoundation.org/blog/trump-reciprocal-tariffs-eu-vat-discriminatory/</link><author>dzogchen</author><category>hn</category><pubDate>Sat, 15 Feb 2025 21:20:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The Trump administration has once again floated the idea of “reciprocal” tariffs on foreign countries. While it is unclear what formula the administration will use to determine what is “reciprocal,” the intention of responding to foreign charges—real and perceived—is clear enough.In the past, the administration has made general assertions about different  and nontariff barriers that American exporters face that should be rectified by “reciprocal” US tariffs. Trump commonly mentions that the EU charges a 10 percent import  on US vehicles while the US only levies a 2.5 percent tariff on European cars coming into the US. Though one can certainly find examples of higher trade barriers abroad, the overall tariff gap between the US and its trading partners is relatively minor—and any increase in US tariffs will ultimately be paid by US businesses and consumers.  However, when discussing trade with the EU specifically, White House deputy chief of staff, Stephen Miller, added a new policy grievance to the mix: value-added taxes (VAT).  “Did you know when you ship a car from the US to Europe, if they let it in at all because they have many nontariff barriers, between the VAT and duties, that car is taxed at 30%? The German car—or a European car sent the America is taxed at 2.5%—or basically 0.”His statement assumes that a VAT discriminates against American car exports like a tariff, and conversely, that the VAT rebate provided to European car producers exporting to the US constitutes a subsidy and the car then simply faces a tariff and no VAT. (It is worth noting that both a domestic automobile and a European car sold in the US would face US state .)  may seem like a compelling political argument to justify across-the-board tariffs on the EU, it instead reflects a complete misunderstanding of what a VAT is and how it works. Worse, it misplaces the blame for a lack of US competitiveness on the European VAT instead of reevaluating the flaws of both the US federal and state tax systems.  What is VAT and how does it work for exported goods?VATs are border-adjusted, meaning they rebate tax on exports and impose tax on imports. Despite the appearance of subsidizing exports and punishing imports, however, a border-adjusted VAT is trade neutral. A border adjusted tax leads to currency appreciation for the imposing country, which would make it cheaper to import goods, more expensive to export goods, and thus would cancel out the apparent benefits of the tax on imports and the rebate on exports.If there is a complaint to be made about tax policy and implications for US competitiveness in Europe, it is about uncompetitive state sales tax structures in the US system that yield what is known as “.”What is US sales tax and how does it work for exported goods? Unlike most countries, the United States does not impose a broad-based  at the national (federal) level, and state-level consumption taxes are designed as general sales taxes rather than value-added taxes. Whereas a VAT is imposed on the incremental increase in value of a good or service at each stage of production, a sales tax is imposed on the total transaction price of any taxed good or service.If a sales tax is imposed exclusively on final consumption, then VATs and sales taxes are economically identical. However, when the sales tax is applied to some intermediate transactions (“business inputs”), it results in tax pyramiding, where the tax is embedded in the price multiple times over.Consider the following example of a 5 percent VAT and two versions of a 5 percent sales tax—one which only applies to final consumption, and one which applies to certain intermediate transactions as well.VATs and Ideal Sales Taxes are Economically IdenticalA 5% VAT compared to a 5% ideal sales tax and a 5% sales tax with business input taxationNote that, while a VAT is imposed at every stage of the process, the net effect is to apply the rate one time to the final sales price. The tax is collected in increments (on the “value added” at each stage), but unlike with a pyramiding sales tax, it does not double tax inputs. The VAT and ideal sales tax share an identical  and, if imposed at the same rates, yield identical collections.US sales taxes are typically destination-based, meaning that the tax is owed where the product is received or consumed. If a European resident orders from a US retailer, they do not pay US sales tax, just like a US consumer can obtain a VAT rebate on purchases of European products. Neither is a subsidy. These are simply consumption taxes falling on the consumer.In practice, however, US sales taxes diverge sharply from the ideal. More than 40 percent of US sales tax revenue comes from intermediate transactions, which impose costs on US producers. This design flaw is not present in VATs, which do not double-tax intermediate transactions. Consequently, the sales tax imposes a penalty on domestic production that a VAT (or a better designed sales tax) would not. European VATs aren’t subsidizing anything—US states are just shooting themselves in the foot.Crucially, this is true in domestic as well as international sales. If a state’s sales tax only applied to final consumption, it would never put in-state businesses at a disadvantage against rivals in other states, because consumers elsewhere are subject to their own state’s sales tax. A Maryland resident pays 6 percent sales tax on whatever she orders (that’s subject to Maryland’s sales tax), regardless of whether she buys from a retailer in Maryland, or Delaware (with no sales tax), or Louisiana (with an average rate north of 10 percent). But when Maryland taxes business inputs, that imposes a cost on Maryland businesses that could be mitigated if businesses operated in lower-tax states or in states which include fewer inputs in their tax base.The disadvantages created by the sales tax, therefore, aren’t unique to goods exported abroad. They aren’t the consequence of trade policy, but of poor tax policy. Europe’s VATs are not tariffs and are not subsidizing European exports. Instead, US states’ poorly-designed sales taxes are harming their own businesses’ competitiveness—whether they’re selling down the street, across state lines, or around the world.What competitiveness issues remain with the US federal tax system?Just like state sales tax systems can create a competitive disadvantage for producers, certain elements of the federal income tax system harm incentives to invest domestically. Despite progress made by the 2017 Tax Cuts and Jobs Act, the US maintains long  schedules for structures investment, now requires amortization for research and development expenses, and is phasing out  for machinery and equipment investment. The absence of full, immediate deductions for investment increases the cost of capital, and thus discourages investment and wage growth.Rather than focus on raising tariffs, which increase the cost of operating in the United States and reduce total output and productivity, fiscal policy reforms to improve the structure of the federal income tax system can better boost competitiveness of the US manufacturing sector.Countries have many reasons why they apply different tariff rates to different products. In the case of the United States, some tariffs date back to the 1930s Smoot-Hawley tariff schedule, while other US trade barriers take on non-tariff forms. The Trump administration appears to be moving in a “reciprocal” policy direction despite the significant negative economic consequences for American consumers of across-the-board tariffs on goods coming into the US. However, the EU’s VAT system should not be used as a justification for retaliatory tariffs. Stay informed on the tax policies impacting you.Subscribe to get insights from our trusted experts delivered straight to your inbox.Subscribe]]></content:encoded></item><item><title>NASA has a list of 10 rules for software development</title><link>https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm</link><author>vyrotek</author><category>hn</category><pubDate>Sat, 15 Feb 2025 20:24:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[NASA has a list of 10 rules for software developmentThose rules were written from the point of view of people writing
embedded software for extremely expensive spacecraft, where tolerating
a lot of programming pain is a good tradeoff for not losing a mission.
I do not know why someone in that situation does not use the SPARK
subset of Ada, which subset was explicitly designed for verification,
and is simply a better starting point for embedded programming than C.
I am criticising them from the point of view of people writing
programming language processors (compilers, interpreters, editors)
and application software.
We are supposed to teach critical thinking.  This is an example.
How have Gerard J. Holzmann's and my different contexts affected
our judgement?
Can you blindly follow his advice without considering 
context?
Can you blindly follow  advice without considering
your context?
Would these rules necessarily apply to a different/better
programming language?  What if function pointers
were tamed?  What if the language provided opaque abstract
data types as Ada does?
1. Restrict all code to very simple control flow constructs —
do not use  statements,
 or  constructs,
and direct or indirect .Note that  and 
are how C does exception handling, so this rule bans any use
of exception handling.

It is true that banning recursion and jumps and loops without
explicit bounds means that you  your program is
going to terminate.  It is also true that recursive functions
can be proven to terminate about as often as loops can, with
reasonably well-understood methods.  What's more important here is
that “sure to terminate” does not imply
“sure to terminate in my lifetime”:
    int const N = 1000000000;
    for (x0 = 0; x0 != N; x0++)
    for (x1 = 0; x1 != N; x1++)
    for (x2 = 0; x2 != N; x2++)
    for (x3 = 0; x3 != N; x3++)
    for (x4 = 0; x4 != N; x4++)
    for (x5 = 0; x5 != N; x5++)
    for (x6 = 0; x6 != N; x6++)
    for (x7 = 0; x7 != N; x7++)
    for (x8 = 0; x8 != N; x8++)
    for (x9 = 0; x9 != N; x9++)
        -- do something --;
This does a bounded number of iterations.  The bound is N.
In this case, that's 10.  If each iteration of the loop body
takes 1 nsec, that's 10 seconds, or about 7.9×10
years.  What is the  difference between “will stop
in 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
years” and “will never stop”?

Worse still, taking a problem that is  expressed
using recursion and contorting it into something that manipulates an
explicit stack, while possible, turns clear maintainable code into
buggy spaghetti.  (I've done it, several times.  There's an example
on this web site.  It is  a good idea.)

2. All loops must have a fixed upper-bound.  It must be trivially
possible for a checking tool to prove statically that a preset
upper-bound on the number of iterations of a loop cannot be exceeded.
If the loop-bound cannot be proven statically, the rule is considered
violated.This is an old idea.  As the example above shows, it is not enough
by itself to be of any practical use.  You have to try to make the
bounds reasonably , and you have to regard hitting an
artificial bound as a run-time error.

By the way, note that putting depth bounds on recursive procedures
makes them every bit as safe as loops with fixed bounds.

3. Do not use dynamic memory allocation after initialization.This is also a very old idea.  Some languages designed for embedded
work don't even  dynamic memory allocation.  The big
thing, of course, is that embedded applications have a fixed amount of
memory to work with, are never going to get any more, and should not
crash because they couldn't handle another record.

Note that the rationale actually supports a much stronger rule:
don't even  dynamic memory allocation.  You can of
course manage your own storage pool:
    typedef struct Foo_Record *foo;
    struct Foo_Record {
	foo next;
	...
    };
    #define MAX_FOOS ...
    static struct Foo_Record foo_zone[MAX_FOOS];
    foo foo_free_list = 0;

    void init_foo_free_list() {
	for (int i = MAX_FOOS - 1; i >= 0; i--) {
	    foo_zone[i].next = foo_free_list;
	    foo_free_list = &foo_zone[i];
	}
    }

    foo malloc_foo() {
	foo r = foo_free_list;
	if (r == 0) report_error();
	foo_free_list = r->next;
	return r;
    }

    void free_foo(foo x) {
	x->next = foo_free_list;
	foo_free_list = x;
    }
This  satisfies the rule, but it
violates the  of the rule.  Simulating malloc()
and free() this way is  than using the real
thing, because the memory in foo_zone is permanently tied up
for Foo_Records, even if we don't need any of those at the
moment but do desperately need the memory for something else.

What you really need to do is to use a memory allocator
with known behaviour, and to prove that the amount of memory
in use at any given time (data bytes + headers) is bounded
by a known value.

Note also that SPlint can verify at compile time that
the errors NASA speak of do not occur.

One of the reasons given for the ban is that the performance
of malloc() and free() is unpredictable.  Are these the only
functions we use with unpredictable performance?  Is there
anything about malloc() and free() which makes them
 unpredictable?  The existence of
hard-real-time garbage collectors suggests not.

The rationale for this rule says that

Note that the only way
to dynamically claim memory in the absence of memory allocation from the
heap is to use stack memory.  In the absence of recursion (Rule 1), an
upper bound on the use of stack memory can derived statically, thus
making it possible to prove that an application will always live within
its pre-allocated memory means.
Unfortunately, the sunny optimism shown here is unjustified.  Given
the ISO C standard (any version, C89, C99, or C11) it is 
to determine an upper bound on the use of stack memory.  There is not even
any standard way to determine how much memory a compiler will use for the
stack frame of a given function.  (There could have been.  There just isn't.)
There isn't even any requirement that two invocations of the same function
with the same arguments will use the same amount of memory.
Such a bound can only be calculated for a  version of a
specific compiler with specific options.  Here's a trivial example:
void f() {
    char a[100000];
}
How much memory will that take on the stack?  Compiled for debugging,
it might take a full stack frame (however big that is) plus traceback
information plus a million bytes for a[].  Compiled with optimisation,
the compiler might notice that a[] isn't used, and might even compile
calls to f() inline so that they generate no code and take no space.
That's an extreme example, but not really unfair.  If you want bounds
you can rely on, you had better  what your compiler does,
and recheck every time anything about the compiler changes.

4.  No function should be longer than what can be printed on
a single sheet of paper in a standard reference format with one line per
statement and one line per declaration.  Typically, this means no more
than about 60 lines of code per function.Since programmers these days typically read their code on-screen,
not on paper, it's not clear why the size of a sheet of paper is
relevant any longer.

The rule is arguably stated about the wrong thing.  The thing that
needs to be bounded is not the size of a function, but the size of a
chunk that a programmer needs to read and comprehend.

There are also question marks about how to interpret this if you
are using a sensible language (like Algol 60, Simula 67, Algol 68,
Pascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,
F#, Clean, Haskell, or Fortran) that allows nested procedures.
Suppose you have a folding editor that presents a procedure to
you like this:
function Text_To_Floating(S: string, E: integer): Double;
   � variables �
   � procedure Mul(Carry: integer) �
   � function Evaluate: Double �

   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;
   for N := 1 to S.length do begin
       C := S[N];
       if C = '.' then begin
          Point := -1
       end else
       if C = '_' then begin
          Base := Round(Evaluate);
          Max, Min, Power := 0, 1, 0
       end else
       if Char ≠ ' ' then begin
          Q := ord(C) - ord('0');
          if Q > 9 then Q := ord(C) - ord('A') + 10
          Power := Point + Point
          Mul(Q)
       end
    end;
    Power := Power + Exp;
    Value := Evaluate;
    if Sign < 0 then Value := -Value;
end;
which would be much bigger if the declarations
were expanded out instead of being hidden behind �folds�.
Which size do we count?  The folded size or the unfolded size?
I was using a folding editor called Apprentice on the Classic Mac
back in the 1980s.  It was written by Peter McInerny and was lightning
fast.

5.  The  of the code should average to a minimum of
two assertions per function.Assertions are wonderful documentation and the very best debugging tool
I know of.  I have never seen any real code that had too many assertions.

The example here is one of the ugliest pieces of code I've seen in a while.
if (!c_assert(p >= 0) == true) {
    return ERROR;
}
It should, of course, just be
if (!c_assert(p >= 0)) {
    return ERROR;
}
Better still, it should be something like
#ifdef NDEBUG
#define check(e, c) (void)0
#else
#define check(e, c) if (!(c)) return bugout(c), (e)
#ifdef NDEBUG_LOG
#define bugout(c) (void)0
#else
#define bugout(c) \
    fprintf(stderr, "%s:%d: assertion '%s' failed.\n", \
    __FILE__, __LINE__, #s)
#endif
#endif
Ahem.  The more interesting part is the required density.
I just checked an open source project from a large telecoms
company, and 23 out of 704 files (not functions) contained
at least one assertion.  I just checked my own Smalltalk
system and one SLOC out of every 43 was an assertion, but
the average Smalltalk “function” is only a few
lines.  If the biggest function allowed is 60 lines, then
let's suppose the average function is about 36 lines, so
this rule requires 1 assertion per 18 lines.
Assertions are good, but what they are especially good
for is expressing the requirements on data that come
from outside the function.  I suggest then that
Every argument whose validity is not guaranteed by
its typed should have an assertion to check it.
Every datum that is obtained from an external
source (file, data base, message) whose validity is
not guaranteed by its type should have an assertion
to check it.
The NASA 10 rules are written for embedded systems, where
reading stuff from sensors is fairly common.

6.  Data objects must be declared at the smallest possible level of
scope.This is excellent advice, but why limit it to data objects?
Oh yeah, the rules were written for crippled languages where you
 declare functions in the right place.

People using Ada, Pascal (Delphi), JavaScript, or functional
languages should also declare types and functions as locally as
possible.

7.  The return value of non-void functions must be checked by each
calling function, and the validity of parameters must be checked inside
each function.This again is mainly about C, or any other language that indicates
failure by returning special values.  “Standard libraries
famously violate this rule”?  No, the  library does.

You have to be reasonable about this: it simply isn't practical
to check  aspect of validity for 
argument.  Take the C function
void *bsearch(
    void const *key  /* what we are looking for */,
    void const *base /* points to an array of things like that */,
    size_t      n    /* how many elements base has */,
    size_t      size /* the common size of key and base's elements */
    int (*      cmp)(void const *, void const *)
);
This does a binary search in an array.  We must have key≠0,
base≠0, size≠0, cmp≠0, cmp(key,key)=0, and for all
1<i<n,
cmp((char*)base+size*(i-1), (char*)base+size*i) <= 0
Checking the validity in full would mean checking
that [key..key+size) is a range of readable addresses,
[base..base+size*n) is a range of readable addresses,
and doing n calls to cmp.  But the whole point of binary
search is to do O(log(n)) calls to cmp.

The fundamental rules here are
Don't let run-time errors go un-noticed, and
any check is safer than no check.
8. The use of the preprocessor must be limited to the inclusion of
header files and simple macro definitions.  Token pasting, variable
argument lists (ellipses), and recursive macro calls are not allowed.Recursive macro calls don't really work in C, so no quarrel there.
Variable argument lists were introduced into macros in
C99 so that you could write code like
#define err_printf(level, ...) \
    if (debug_level >= level) fprintf(stderr, __VA_ARGS__)
...
    err_printf(HIGH, "About to frob %d\n", control_index);
This is a  thing; conditional tracing like this is a
powerful debugging aid.  It should be , not banned.

The rule goes on to ban macros that expand into things that are
not complete syntactic units.  This would, for example, prohibit
simulating try-catch blocks with macros.  (Fair enough, an earlier rule
banned exception handling anyway.)  Consider this code fragment, from
an actual program.
    row_flag = border;     
    if (row_flag) printf("\\hline");
    for_each_element_child(e0, i, j, e1)
        printf(row_flag ? "\\\\\n" : "\n");
        row_flag = true;  
        col_flag = false;
        for_each_element_child(e1, k, l, e2)
            if (col_flag) printf(" & ");
            col_flag = true;
            walk_paragraph("", e2, "");
        end_each_element_child
    end_each_element_child
    if (border) printf("\\\\\\hline");
    printf("\n\\end{tabular}\n");
It's part of a program converting slides written in something like HTML
into another notation for formatting.  The 
…  loops walk over a tree.  Using
these macros means that the programmer has no need to know and no reason to
care how the tree is represented and how the loop actually works.
You can easily see that  must have at
least one unmatched { and  must have at least one
unmatched }.  That's the kind of macro that's banned by requiring
complete syntactic units.  Yet the readability and maintainability of
the code is  improved by these macros.

One thing the rule covers, but does not at the beginning stress, is
“no  macro processing”.  That is,
no #if.  The argument against it is, I'm afraid, questionable.  If there
are 10 conditions, there are 2 combinations to test,
whether they are expressed as compile-time conditionals or run-time
conditionals.

In particular, the rule against conditional macro processing
would prevent you defining your own assertion macros.
It is not obvious that that's a good idea.

9.  The use of pointers should be restricted.  Specifically, no more
than one level of dereferencing is allowed.  Pointer dereference
operations may not be hidden in macro definitions or inside typedef
declarations.  Function pointers are not permitted.Let's look at the last point first.

double integral(double (*f)(double), double lower, double upper, int n) {
    // Compute the integral of f from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += f((lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += f(lower + h*i);
    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);
}
This kind of code has been important in numerical calculations since
the very earliest days.  Pascal could do it.  Algol 60 could do it.
In the 1950s, Fortran could do it.  And NASA would ban it, because in
C,  is a function pointer.

Now it's important to write functions like this once and only once.
For example, the code has at least one error.  The comment says n+1
points, but the function is actually evaluated at 2n+1 points.  If we
need to bound the number of calls to f in order to meet a deadline,
having that number off by a factor of two will not help.
It's nice to have just one place to fix.
Perhaps I should not have copied that code from a well-known source (:-).
Certainly I should not have more than one copy!

What can we do if we're not allowed to use function pointers?
Suppose there are four functions foo, bar, ugh, and zoo that we need
to integrate.  Now we can write
enum Fun {FOO, BAR, UGH, ZOO};

double call(enum Fun which, double what) {
    switch (which) {
        case FOO: return foo(what);
        case BAR: return bar(what);
        case UGH: return ugh(what);
        case ZOO: return zoo(what);
    }
}

double integral(enum Fun which, double lower, double upper, int n) {
    // Compute the integral of a function from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += call(which, (lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += call(which, lower + h*i);
    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);
}
Has obeying NASA's rule made the code more reliable?  No, it has made
the code  to understand,  maintainable, and
 that it wasn't before.  Here's a call
illustrating the mistake:
x = integral(4, 0.0, 1.0, 10);I have checked this with two C compilers and a static checker at their
highest settings, and they are completely silent about this.

So there are legitimate uses for function pointers, and simulating
them makes programs , not better.

Now  in Fortran,
Algol 60, or Pascal.  Those languages had procedure 
but not procedure . You could pass a subprogram name as
a parameter, and such a parameter could be passed on, but you could not
store them in variables.  You could have a  of C which
allowed function pointer parameters, but made all function pointer
variables read-only.  That would give you a statically checkable subset
of C that allowed integral().

The other use of function pointers is simulating object-orientation.
Imagine for example
struct Channel {
    void (*send)(struct Channel *, Message const *);
    bool (*recv)(struct Channel *, Message *);
    ...
};
inline void send(struct Channel *c, Message const *m) {
    c->send(c, m);
}
inline bool recv(struct Channel *c, Message *m) {
    return c->recv(c, m);
}
This lets us use a common interface for sending and receiving
messages on different kinds of channels.  This approach has been
used extensively in operating systems (at least as far back as
the Burroughs MCP in the 1960s) to decouple the code that uses
a device from the actual device driver.     I would expect any
program that controls more than one hardware device to do something
like this.  It's one of our key tools for controlling complexity.
Again, we can simulate this, but it makes adding a new kind of
channel harder than it should be, and the code is 
when we do it, not better.

The rule against more than one level of dereferencing is also
an assault on good programming.  One of the key ideas that was
developed in the 1960s is the idea of ;
the idea that it should be possible for one module to define a
data type and operations on it and another module to use instances
of that data type and its operations without having to know
anything about what the data type is.
One of the things I detest about Java is that it spits in the
face of the people who worked out that idea.  Yes, Java (now) has
generic type parameters, and that's good, but you cannot use a
 type without knowing what that type is.

Suppose I have a module that offers operations
And suppose that I have two interfaces in mind.  One of them
uses integers as tokens.
// stasher.h, version 1.
typedef int token;
extern token stash(item);
extern item  recall(token);
extern void  delete(token);
Another uses pointers as tokens.
// stasher.h, version 2.
typedef struct Hidden *token;
extern  token stash(item);
extern  item  recall(token);
extern  void  delete(token);
void snoo(token *ans, item x, item y) {
    if (better(x, y)) {
	*ans = stash(x);
    } else {
	*ans = stash(y);
    }
}
By the NASA rule, the function snoo() would not be accepted or rejected on
its own merits.  With stasher.h, version 1, it would be accepted.
With stasher.h, version 2, it would be rejected.

One reason to prefer version 2 to version 1 is that version 2 gets
more use out of type checking.  There are ever so many ways to get an
int in C.  Ask yourself if it ever makes sense to do
token t1 = stash(x);
token t2 = stash(y);
delete(t1*t2);
I really do not like the idea of banning abstract data types.

10.  All code must be compiled, from the first day of development,
with all compiler warnings enabled at the compiler’s
most pedantic setting.  All code must compile with these setting without
any warnings.  All code must be checked daily with at least one, but
preferably more than one, state-of-the-art static source code analyzer
and should pass the analyses with zero warnings.This one is good advice.  Rule 9 is really about making your code
worse in order to get more benefit from limited static checkers.  (Since
C has no standard way to construct new functions at run time, the set of
functions that a particular function pointer  point to can
be determined by a fixed-point data flow analysis, at least for most
programs.)  So is rule 1.  



]]></content:encoded></item><item><title>Perplexity Deep Research</title><link>https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research</link><author>vinni2</author><category>hn</category><pubDate>Sat, 15 Feb 2025 20:07:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My Life in Weeks</title><link>https://weeks.ginatrapani.org/</link><author>bookofjoe</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:34:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New SF public health chief was part of McKinsey opioid-marketing operation</title><link>https://sfstandard.com/2025/02/14/san-francisco-department-public-health-daniel-tsai-opioids-mckinsey/</link><author>iancmceachern</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:32:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Dr. David Juurlink, an expert on tramadol, called the drug a “minor player in the opioid crisis, but a player nevertheless.”He added, “To the extent that McKinsey helped advertise it as a notionally safer opioid, I think they did a disservice in doing so. The main reason I say tramadol is a minor player is because it wasn’t prescribed like candy, like OxyContin was.”McKinsey’s method of targeting high-volume prescribers was part of its playbook to juice opioid sales, despite mounting evidence that the drugs could be highly addictive. In other emails uncovered in the McKinsey documents, employees wrote excitedly about finding doctors who were willing to write opioid prescriptions. In one instance in 2015, a McKinsey partner wrote, “The challenge which we need to start working on is to identify the sweet spot of docs so we can do targeting. … Fun be[g]ins on Monday!”San Francisco’s fentanyl crisis is part of a broader trend of opioid overdoses that traces back to the 1990s, when prescription opioids became popular among doctors for chronic pain management. Companies such as Purdue Pharma, which manufactured OxyContin, brought in consulting firms like McKinsey to help with sales strategies. After a Department of Justice probe and settlement, McKinsey acknowledged that it knew the dangers of OxyContin but continued working with Purdue Pharma — even after several of the drugmaker’s executives pled guilty in 2007 to misrepresenting addiction risks. McKinsey, along with a slew of drug companies and pharmacies, agreed to pay billions in settlement funds over their roles in fueling opioid addiction. California received roughly $60 million from the 2021 McKinsey settlement. San Francisco, under the 2023 settlement of an opioid-related lawsuit, was expected to receive about $230 million from Walgreens.In both instances, the funds were slated to be used for opioid recovery efforts.In 2019, McKinsey said it would no longer work on opioid-related businesses. Last year, McKinsey formally apologized for its Purdue Pharma work, saying it was “deeply sorry” for its role in selling OxyContin. “This terrible public health crisis and our past work for opioid manufacturers will always be a source of profound regret for our firm,” the company said in a statement.At the San Francisco Department of Public Health, Tsai replaced Dr. Grant Colfax, who took the reins in 2019 and led the city through the pandemic before stepping down in January. The role paid $546,133 in 2024, one of the highest city salaries.  On Monday, the San Francisco Health Commission unanimously nominated Tsai as director of Public Heath. Dr. Laurie Green, president of the commission, said the governing body conducted a “multi-hour” interview.]]></content:encoded></item><item><title>Schemesh: Fusion between Unix shell and Lisp REPL</title><link>https://github.com/cosmos72/schemesh</link><author>cosmos0072</author><category>hn</category><pubDate>Sat, 15 Feb 2025 19:00:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Multiple Russian Threat Actors Targeting Microsoft Device Code Authentication</title><link>https://www.volexity.com/blog/2025/02/13/multiple-russian-threat-actors-targeting-microsoft-device-code-authentication/</link><author>ChrisArchitect</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:59:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Multiple Russian Threat Actors Targeting Microsoft Device Code Authenticationby Charlie Gardner, Steven Adair, Tom LancasterVolexity has observed multiple Russian threat actors conducting social-engineering and spear-phishing campaigns targeting organizations with the ultimate goal of compromising Microsoft 365 accounts via Device Code Authentication phishing.Device Code Authentication phishing follows an atypical workflow to that expected by users, meaning users may not recognize it as phishing.Recent campaigns observed have been politically themed, particularly around the new administration in the United States and the changes this might mean for nations around the world.Starting in mid-January 2025, Volexity identified several social-engineering and spear-phishing campaigns by Russian threat actors aimed at compromising Microsoft 365 (M365) accounts. These attack campaigns were highly targeted and carried out in a variety of ways. The majority of these attacks originated via spear-phishing emails with different themes. In one case, the eventual breach began with highly tailored outreach via Signal.Through its investigations, Volexity discovered that Russian threat actors were impersonating a variety of individuals in order to socially engineer targets, including impersonating individuals from the following:United States Department of StateUkrainian Ministry of DefenceEuropean Union ParliamentProminent research institutionsCommunications carried a variety of different themes and messages, but they all ultimately resulted in the attacker inviting the targeted user to one of the following:Microsoft Teams Meeting / Video ConferenceAccess to applications and data as an external M365 userJoin a chatroom on a secure chat applicationWhen these attacks were successful and the attackers gained access to accounts, the post-exploitation phase often had unique characteristics in each case:The way the attackers accessed material from compromised organizations (scripts versus native applications)The infrastructure used to access stolen accountsDespite the differences, Volexity found the attacks had one thing in common: they were all Device Code Authentication attacks. While this attack method is not new, it is one that is definitely lesser known and not commonly leveraged by nation-state actors. Details on the social-engineering and spear-phishing campaigns, along with how Device Code Authentication attacks work, will be covered further in this blog post. What Volexity has observed is that this method has been more effective at successfully compromising accounts than most other targeted spear-phishing campaigns.Volexity assesses with high confidence that the series of attacks described in this blog post are from Russia-based threat actors. At this time, Volexity is tracking this activity under three different threat actors and assesses with medium confidence that at least one of them is  (overlapping with DarkHalo, APT29, Midnight Blizzard, CozyDuke). Volexity is tracking the remaining activity under  and . It is possible that all the activity described in this blog post is a single threat actor, but despite the similar targeting, timing, and attack method, other observed components of the operations are different enough to be tracked separately, for now.From Secure Chat to Insecure AuthenticationThe discovery of this threat activity started toward the end of January 2025, when Volexity uncovered a highly targeted attack that had successfully compromised the M365 account of one of its customers. This breach was discovered after Volexity identified suspicious sign-in activity to the account, which was followed by a rapid download of files from the user's OneDrive. All authentication and download events came from virtual private server (VPS) and Tor IP addresses, which is not the most subtle way to access an account. Volexity noted this activity was likely scripted, as the User-Agent string for later access and file downloads was the Python User-Agent string .Volexity then performed a detailed investigation into this incident, in an effort to identify how the account was compromised. A review of login activity showed the legitimate user had logged in and approved a multi-factor authentication (MFA) request. However, subsequent access was not from the legitimate user's IP address. This caused Volexity to initially suspect a phishing attack involving an adversary-in-the-middle (AiTM) framework. As a result, Volexity reviewed emails to the user leading up the time of the authentication event. This review identified a suspicious email just moments before the login activity from an email address purporting to be from someone with the name of a high-ranking official from the Ukrainian Ministry of Defence. The email was structured to look like a meeting invite for a chatroom on the messaging application, Element. Element is another encrypted messaging application that offers the ability for users to self-host a server with functionality that includes group video chats. The “invitation” email sent is shown below .Microsoft describes the purpose of this workflow as allowing '"users to sign in to input-constrained devices such as a smart TV, IoT device, or a printer.” However, in this case, it means if an attacker can convince a user to enter a specific code into this dialogue (and log in), they are granted long-term access to the user’s account.After working with its customer more closely, Volexity learned that the victim had been contacted on Signal by an individual purporting to be from the Ukrainian Ministry of Defence. This individual then requested the victim move off Signal to another secure chat application called Element. The attacker then had the victim join an Element server they controlled under the domain . This allowed the attacker to further communicate with the victim in real time and inform them they needed to click a link from an email to join a secure chat room. This is where the email Volexity had discovered came into play. The message was a ploy to fool the user into thinking they were being invited into a secure chat, when in reality they were giving the attacker access to their account. The generated Device Codes are only valid for 15 minutes once they are created. As a result, the real-time communication with the victim, and having them expect the "invitation", served to ensure the phish would succeed through timely coordination.The diagram below sVolexity tracks the threat actor behind this campaign as . Through research conducted on the custom domain used by UTA0304 to operate its own Element server, Volexity was able to pivot and discover additional infrastructure it believes is likely operated by the group. The table below represents the list of infrastructure that Volexity has tied to this threat actor.chromeelevationservice[.]comSpoofing the United States Department of StateIn early February 2025, Volexity observed multiple spear-phishing campaigns targeting users with fake Microsoft invitations purporting to be from the United States (US) Department of State. These emails were themed as invitations to join the US Department of State’s Microsoft tenant as an external user, or as invitations to a Microsoft Teams chat named “Measuring Influence Operations".Similar to the campaign conducted by UTA0304, these fake US Department of State emails were targeting users with a Device Code OAuth phishing workflow. Each email was aimed at convincing the user to accept the invitation and enter a unique code provided in the phishing email. The link in the invitations would direct users to the Microsoft Device Code authentication page. If the user entered the code provided in the phishing email, the authentication page would subsequently authorize the threat actor to access to the user’s account. However, it is worth noting that this campaign was sent out of the blue, with no precursor or build up to the emails, so users would not be expecting these messages. Even if they were to fall for the campaign, they would have to have done it within 15 minutes of receiving the email. This dramatically decreased the likelihood that this attack would be successful.After reviewing various parts of the attack, Volexity assesses with medium confidence that the Russian threat actor CozyLarch (aka APT29 or Midnight Blizzard) was behind these US Department of State themed spear-phishing campaigns. Additional details on each campaign are described in the sections that follow.Campaign 1: M365 Tenant External User InvitationCozyLarch sent invitations to several users, inviting them to access applications within the M365 tenant for the US Department of State. The invitation email was designed to look like a real invitation that would be sent from Microsoft, as shown below.The redirect link takes the user to the Microsoft Device Code OAuth workflow, and it is the same URL that UTA0304 directly embedded in their phishing campaign. However, unlike UTA0304, CozyLarch opted to use the redirect URL rather than the final login URL, perhaps because it may look even more recognizable to a discerning user, given that it is hosted on the main Microsoft domain. If the user entered the code provided from the email and continued through the authentication process, the attacker was granted access to the user’s M365 account.Campaign 2: M365 Teams Chat InvitationCozyLarch launched a second campaign, in which they targeted users with a fake invitation to join a Microsoft Teams chat named “Measuring Influence Operations”. The email made it appear as though there were already 37 other members in the chat.  A screenshot of one of the observed spear-phishing messages is shown below.The “Sign in to Microsoft Teams” button in the email body is a hyperlink that leads to the same https://www.microsoft.com/devicelogin URL observed in the other campaign. The attack flow and end goal are the same, with only a small difference in the theme of the emails.The emails are designed to appear as though they come from Microsoft. The messages used mixed encoding in the “friendly” name that make the address difficult to discern. An example of the full “from” header used in one phishing email is given below:\"Mic\udb40\udc30\udb40\udc30\udb40\udc30\u200br\udb40\udc30\udb40\udc30o\udb40\udc30\udb40\udc30soft Invitations on behal f of US Dep\udb40\udc30\udb40\udc30\udb40\udc30artme\udb40\udc30\udb40\udc30\udb40\udc30nt of St\udb40\udc30\udb40\udc30\udb40\udc30 ate \uff1cinvites\uff20mic\udb40\udc30\udb40\udc30\udb40\udc30\u200br\udb40\udc30\udb40\udc30o\udb40\udc30\udb40\udc30soft.co m\uff1e\u180e\u3000\u180e\u3000\u180e \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e \u180e\u3000\u180e \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e \u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e  \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000\u180e\u3000 \u180e\u3000\u180e\u3000\u180e \u061c Cc:\" <[email protected]>The attacker attempted to make it appear as if the emails were from , and also set the  header as . However, the true address could be seen at the end of the  field; all messages were sent via Google Gmail accounts. Volexity observed the following Gmail accounts as the actual senders of the messages observed:kaylassammers@gmail[.]comkendisggibson@gmail[.]comleslytthomson@gmail[.]comThese addresses are believed to be controlled by CozyLarch and can be used to reliably detect phishing emails that may have been sent.Using Wireless Proxy Networks for Email DistributionVolexity also noted that the sending IP address associated with each spear-phishing email was recorded in the headers. Looking at the  header in the messages, it became apparent that the attacker was using Proxy IP addresses based in the US to send messages. Volexity observed nearly a dozen IP addresses belonging to mobile networks in the US (AT&T and Verizon Wireless).European Parliament and Donald TrumpStarting in late January through to the publication of this blog (February 13, 2025), Volexity has observed another campaign by a Russian threat actor it tracks as  targeting numerous organizations. UTA0307 created a fake email under the identity of a member of the European Parliament who is on the Committee on Foreign Affairs. The threat actor reached out to numerous individuals with personalized emails requesting a Microsoft Teams meeting to discuss Donald Trump and his impact on relations between the US and the European Union. Volexity also observed a smaller set of campaigns centered on discussing China's foreign policy and China-European Union relations.The email subject lines used in these various campaigns are listed below:Discussion on Eastern Europe and the CaucasusDiscussion about Donald Trump's new termDiscussion about Trump & US relations with EuropeCollaboration on China and East Asia ResearchThe image below shows an example spear phish that was sent by UTA0307.None of the initial emails contained any malicious content or links at the onset. The threat actor was leveraging a tactic that has become commonplace for numerous nation-state actors, where they wait until a conversation has started prior to sending anything malicious. This serves the purpose of knowing they have an engaged target, and that the target's guard is potentially down. In the specific cases of Device Code Authentication phishing, it is especially important to have a responsive target, as the threat actor has only 15 minutes to convince the target to enter the code that has been generated.A Different Device Code OAuth Phishing TechniqueVolexity actually discovered the operations of UTA0307 following a successful compromise. Similar to the initial discovery of UTA0304, Volexity worked backwards from detecting a breach to identifying the above spear-phishing emails. In this case, the victim had engaged from the initial email and had several messages back and forth with UTA0307 regarding a meeting being set up. They agreed to join a Microsoft Teams meeting, and a fake invitation email was sent. However, this time the link in the email did not go to Microsoft. The target received an email with the subject "", and the body of the email, shown below, was designed to look like a real invitation.The “Join the meeting now” hyperlink, however, linked to a website controlled by UTA0307 (). This page in turn was set up to automatically generate a new Microsoft Device Code each time it was visited. The website was designed to appear as an official Microsoft interstitial page before the user can join a Microsoft Teams meeting. The message that appears on the landing page (shown below) claims that the victim needs to pass a security check by copying a code and entering it on a subsequent pageWhen the user clicks the “Next” button, a new tab is opened with the real Microsoft Device Code Authentication interface that requests an authentication code. If the victim enters the code supplied by the phishing page, they grant UTA0307 access to their M365 account. Interestingly, in the background of the initial phishing page, Volexity noted that the website would continuously poll the domain . It appears this domain was set up to monitor successful Device Code Authentication and, if detected, would redirect the user to a real Microsoft Teams meeting URL in an effort to make the activity appear legitimate.The threat actor never joined this Microsoft Teams meeting. However, UTA0307 did add authorization for an authentication application under their control to enable multi-factor authentication when logging into the compromised account. Volexity assesses with medium confidence that this was a requirement of logging into the account, even with the stolen authentication token.One benefit of this attack workflow versus other previously observed DeviceID phishing workflows is that, when a DeviceID code is generated, it is only valid for 15 minutes. Having an interstitial page that automatically generates new codes means UTA0307 does not have to worry about their phishing content expiring.UTA0307 Post-compromise Activities, Targeting and AttributionVolexity observed UTA0307 exfiltrating documents from a compromised M365 account that would be of interest to a Russian threat actor. This was determined based on identification of FileDownloaded operations observed in M365 audit log data. Given this information about the threat actor’s objectives, their targeting, and their use of a highly similar technique to that used in recent days and weeks by CozyLarch and UTA0304, Volexity assesses with medium confidence that UTA0307 is also a Russian threat actor.However, the exact implementation of the DeviceID OAuth phishing technique used in this activity differs slightly from those previously documented by Volexity, which provides some evidence that this activity may have been conducted by a separate threat actor. For example, while the previously observed phishing campaigns saw the attacker use the client ID for Microsoft Office when handling Device Code Authentication, this activity instead used the client ID for Microsoft Teams, as shown below (note that Microsoft uses  and  interchangeably in their logs when referring to the ID for an application):"appDisplayName": "Microsoft Teams","appId": "1fec8e78-bce4-4aaf-ab1b-5451cc387264",Another difference between this and the UTA0304 campaign is that in this case, all subsequent access to the compromised account occurred via Mullad VPN exit nodes (versus the other observed VPS and Tor IP addresses). Based on these two factors, Volexity has chosen to track this activity under the UTA0307 alias, rather than CozyLarch or UTA0304.Detecting Device Code AuthenticationVolexity identified a way to reliably detect this attack through monitoring and analysis of Microsoft Entra ID sign-in logs. When a user enters a device code and subsequently authenticates, it results in a login to the application associated with the generated code. This can be a common application like Microsoft Office that is frequently accessed by users and would not be a reliable indicator. However, the good news is that Device Code Authentications result in the  field being set with the value .The line below is what will appear in the JSON data in the Entra ID sign-in logs when a Device Code Authentication occurs:“authenticationProtocol": "deviceCode",Volexity further noted that as authenticated sessions refresh and are kept alive, subsequent sign-ins that initially occurred via a  often do not have anything set for  but they contain the following entry: “originalTransferMethod": "deviceCodeFlow",These values can be searched and filtered on in the Entra Admin center by adding filters for "Authentications Protocol" and "Original Transfer Method". The latter can be filtered in both  and  sign-ins. The frequency and legitimacy of these values occurring in the sign-in logs for a particular organization may vary, as this is a legitimate Microsoft feature. An organization can evaluate their risk and usage of these workflows, and potentially use this information as a proactive detection mechanism.If an organization has the ability to monitor URLs that are being accessed by users or sent in email, there are additional detection opportunities to discover Device Code Authentication attacks. The following official URLs can be monitored for as related to Microsoft Device Code Authentication:https://login.microsoftonline.com/common/oauth2/deviceauthhttps://www.microsoft.com/deviceloginhttps://aka.ms/deviceloginOrganizations can monitor for access to these URLs or for their presence in various communication methods, such as email. Attackers can find other means to redirect users to these URLs, but one of the main advantages of using the list above in phishing attacks is that the URL displayed is hosted on a legitimate Microsoft domain.Preventing Device Code AuthenticationVolexity believes the most effective way to prevent this potential attack vector is through conditional access policies on an organization's M365 tenant. It is possible for organizations to create a conditional access policy that disallows device code authentication altogether. It is fairly trivial to set up, and Microsoft provides online guidance on exactly how to do this. Based on Volexity's own testing, blocking the "Device code flow" from "Authentications flows" prevents this attack from working.  The image below shows what a conditional access policy would look like once it's set up and in place to block this authentication flow.Prior to implementing such a policy, organizations should evaluate the use of Device Code Authentication in their environment. This feature is used legitimately, and blocking it could have a negative impact. Volexity's review of its own customers identified several instances of legitimate access to resources via these means. However, at the majority of Volexity's customers, there was either no recent Device Code Authentication activity or there was only activity tied to the attacks described in this blog post.Volexity continues to track multiple spear-phishing campaigns targeting Device Code Authentication. This blog post serves to cover a few of the larger and unique campaigns observed. Volexity has observed other similar spear-phishing campaigns in recent weeks targeting Device Code Authentication that it believes are the work of Russian threat actors. Further, it should be noted that it is possible this is the work of a single threat actor running multiple, different campaigns. However, at this time, Volexity believes this activity is sufficiently different enough to warrant tracking this activity under two different unknown threat actors and one it believes is likely CozyLarch.While Device Code Authentication attacks are not new, they appear to have been rarely leveraged by nation-state threat actors . Volexity's visibility into targeted attacks indicates this particular method has been far more effective than the combined effort of years of other social-engineering and spear-phishing attacks conducted by the same (or similar) threat actors. It appears that these Russian threat actors have made a concerted effort to launch several campaigns against organizations with a goal of simultaneously abusing this method before the targets catch on and implement countermeasures.The detection mechanisms and countermeasures to these attacks have been available for years. However, Volexity believes they are seldom implemented and that most organizations are not even aware of this authentication flow, let alone the means to detect its misuse. These attacks serve as a reminder that threat actors will constantly look for ways to abuse legitimate features, and organizations must continually evaluate and implement methods to detect and prevent such attacks.These attacks also serve as a good opportunity to engage with users and remind them to be on the lookout for anything out of the ordinary when it comes to accessing resources when they are asked for login credentials or authorization grants. This phishing workflow has proven useful for an attacker, as many traditional sources of evidence and detection, both for a user and network defenders, are not present. For example:There is no “malicious” link or attachment. The only link is to the provider’s infrastructure (in this case, Microsoft). This means users cannot easily identify the link as being suspicious, and automated solutions detecting malicious emails will likely fail to do so for the same reason.Users are generally less aware of attacks that leverage legitimate services, and may be even less aware when it comes to those that involve entering a device code rather than their username or password.After successful authentication, the logs will show the authenticating application as a legitimate or benign application, reducing signal that can be keyed off of in sign-in logs by detection teams.These are items that organizations should look to further train users on and implement technical countermeasures against where possible.Volexity GitHub.If you believe you have been targeted by a similar attack and want to share details with Volexity for informational purposes, additional investigation, or incident response, please contact us.]]></content:encoded></item><item><title>PAROL6: 3D-printed desktop robotic arm</title><link>https://source-robotics.github.io/PAROL-docs/</link><author>bo0tzz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:26:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Basketball has evolved into a game of calculated decision-making</title><link>https://nabraj.com/blog/basketball-solved-sport/</link><author>nabaraz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:21:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Basketball has evolved from a game of unpredictability into a game of calculated decision-making with the use of data and analytics. From a game of points, assists, and rebounds, it has progressed into using thousands of data points to optimize every element of the game.All decisions are made based on numbers not intuition. Long-range shooting and layups are preferred over mid-range shooting. Players are no longer do-it-alls; they are now given specialized roles.In the last decade, long-range shooting has gone from a secondary option to a primary choice for building offense. Recently, teams have realized three-pointers have higher point value despite their lower scoring percentage. This has led to a revolution in structuring an offense around taking long-range shots. The Golden State Warriors, led by Stephen Curry, probably jump-started this trend with 34 three-pointer attempts per game in the 2018-19 season, twice as much from five years ago. Celtics, this season, have averaged almost 50 three-pointers attempt this season (2024-25 season).In the past, the team built its roster around a big name like Shaq. Most of the offense were from the center. This has now changed, with the primary strategy being to stretch the opposition and take long-range shots.The 3-and-D model refers to a player, usually a wing player, who is just above average at three-pointers and plays competent defense.  Forget about positions; just get a guy who can do some 3s and Ds.Danny Green is probably the father of this model, with his 40% career three-point field goal percentage and he also made into all-defensive team.In  recent years, every team has had at least one 3-and-D model player on the roster.Gone are the days of an all-around player. There is no longer a need for a player who does everything. Look at players like Kobe Bryant and Lebron James (early career); they not only scored but guarded defense, caught rebounds and played the role of playmakers.Now, it’s all about creating lineups with specialized players. A team typically consists of a three-point shooter, a defensive specialist, a playmaker, and rebounders. They all have specific roles assigned to them.A catch-all word for statistics, technology has played a pivotal role in shaping this game. In addition to data collection, biomechanics and motion cameras track every player’s movement. NBA even brought SportVU from football; it follows the ball and supposedly captures images 25 times per second. Coaches can now use this to analyze the speed, position, form, and motion of each player on the court. In the end, it’s all about optimizing every ball possession. Basketball might have lost its flair; every move is now predictable and measured. What is the future of basketball, is anyone’s guess? Maybe a rule change is around the corner?]]></content:encoded></item><item><title>Jill – a functional programming language for the Nand2Tetris platform</title><link>https://github.com/mpatajac/jillc</link><author>mailgolub</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:04:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alzheimer&apos;s biomarkers now visible up to a decade ahead of symptoms</title><link>https://newatlas.com/brain/alzheimers-dementia/alzheimers-biomarkers-visible-decade-before-symptoms/</link><author>01-_-</author><category>hn</category><pubDate>Sat, 15 Feb 2025 18:02:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Researchers at the University of Pittsburgh have devised a biomarker test that can spot small amounts of clumping tau protein in the brain and cerebrospinal fluid, which lead to Alzheimer's disease.Catching these clumps early while still in minute quantities can enable effective intervention. This test can help detect the tangled proteins years in advance of them appearing prominently in brain scans – as much as up to a decade.That's heartening because Alzheimer's disease not only has devastating impact on patients' lives long term, but is also currently incurable. It begins to show up as forgetfulness, and progresses to confusion and disorientation, delusions, hallucinations, and trouble sleeping. As the condition worsens, patients may experience difficulty eating, moving around, incontinence, loss of speech, and significant memory loss.“Early detection is key to more successful therapies for Alzheimer’s disease since trials show that patients with little-to-no quantifiable insoluble tau tangles are more likely to benefit from new treatments than those with a significant degree of tau brain deposits," explained Thomas Karikari, senior author of the paper published in  this week.Here's a quick bit of context on what's happening in and around the brain. Humans have some 86 billion nerve cells, and they're connected by what are called synapses. These synapses are supported by 'rail tracks' that enable the flow of essential nutrients and information, and they're called microtubules.Tau proteins (abbreviated from tubulin associated unit) stabilize these microtubules, and help keep the brain healthy. These proteins can malfunction, clump together and create tangles – preventing the microtubules from functioning properly.Here's where this new biomarker test comes in. The researchers have found an important section of the tau protein that causes it to form harmful tangles in the brain. This section is made up of 111 building blocks (amino acids). Within this section, they discovered two specific spots that, when modified, can tell doctors if tau proteins are starting to clump together. This is important because if doctors can detect these changes early enough, they might be able to treat the problem before it gets worse.The two specific spots they found (called p-tau-262 and p-tau-356) work like early warning signals, letting doctors know that tau proteins are beginning to malfunction. This could help identify Alzheimer's disease sooner, when treatments might be more effective.]]></content:encoded></item><item><title>More Solar and Battery Storage Added to TX Grid Than Other Power Src Last Year</title><link>https://insideclimatenews.org/news/10022025/solar-battery-storage-texas-grid/</link><author>indigodaddy</author><category>hn</category><pubDate>Sat, 15 Feb 2025 16:37:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As the market for renewables in Texas continues to strengthen and innovate, the power makeup of the state’s electric grid is slated to keep shifting toward adding more renewables. Last year, solar and battery storage installation led capacity growth within Texas’ electric grid, according to research from the Federal Reserve Bank of Dallas published in January. Texas added nearly 1,500 megawatts of battery storage to the grid’s summer rated capacities in 2023. That figure almost tripled to 4,374 megawatts added in 2024, according to the report. Capacity from solar power added to the grid enjoyed a similar trajectory. In 2023, 4,570 megawatts were added to the grid, while in 2024, nearly 9,700 megawatts were added. Given that Texas has its own isolated energy grid, the Electricity Reliability Council of Texas (ERCOT) is responsible for managing the majority of energy for its residents. Please take a look at the new openings in our newsroom.See jobsHistorically, ERCOT, which manages 90 percent of the state’s power load, has primarily relied on natural gas. But other energy sources, like wind and solar, have played a critical role in offsetting high demand. ERCOT added 3,400 megawatts from natural gas power plants in 2024. That’s after more than 1,000 megawatts of natural gas power were inactive within the grid from 2021 to 2023, according to Dallas Fed data. The capacity growth from solar and battery storage allowed the grid to manage another hot Texas summer in 2024, reported Garret Golding, an assistant vice president for energy program at the Dallas Fed. Battery storage is relatively new to ERCOT. One of the first battery power storage plants to be connected to ERCOT was in 2021, southeast of Dallas in Kaufman County. The 50-megawatt plant run by Enel, one of the largest renewable energy owners and operators in the country, was the first of Enel’s 14 battery projects it has since developed across the state. The forecasted power demand in Texas from population growth and heavy load users like data centers, cryptocurrency mining and artificial intelligence, alongside the competitive market for batteries, is dictating the rising use of storage within ERCOT, said Randald Bartlett, a senior director of operations and management for battery energy storage systems at Enel North America.Texas’ permitting processes and ability to develop has made it easier to add and build new capacity in comparison to other states with more laborious entryways, Bartlett said. Before, there wasn’t really adequate criteria and evidence to forecast what batteries would contribute to the grid, ERCOT CEO Pablo Vegas said during a board of directors meeting on Tuesday. Now, the grid operator added battery contribution to its report forecasting future capacity, demand and reserves. Battery storage in the ERCOT grid has nearly doubled every year since 2021, Vegas said. At the end of 2024, there were nearly 10,000 megawatts from batteries within ERCOT.Vegas said the capacity from batteries made a significant difference in ERCOT during bridge hours, or the winter morning hours when the sun has yet to rise and in the evenings after the sun sets. “Batteries made a meaningful contribution to what those shoulder periods look like and how much scarcity we get into during these peak events,” Vegas said when analyzing the grid’s performance during recent winter storms. In the spring of 2024, Texas’ installation of utility scale solar outpaced California’s, and jumped from 1,900 megawatts in 2019 to over 20,000 megawatts in 2024. As a result, solar met nearly 50 percent of the state’s peak power demand on some days. The state’s quick deployment of utility scale solar didn’t happen overnight. It started in 2005, when the legislature instructed the Public Utility Commission of Texas to create competitive renewable energy zones, where the state planned transmission lines to connect cities to renewable energy sources in west Texas.Initially, it was intended to capture wind power but was able to quickly include solar because of the existing infrastructure, said Dustin Mulvaney, an environmental studies professor at San Jose State University and an author of Planning to Build Faster: A Solar Energy Case Study, published in October by the Roosevelt Institute. That forward-looking plan is often held up as a model renewable energy advocates and developers say the Federal Energy Regulatory Commission could implement across other regional transmission organizations, by requiring proactive planning and by creating rules of how to pay for transmission systems. As the state’s grid continues to experience a rapid shift in the type of generation available to serve demand, the state’s grid operator is looking to build a higher voltage transmission system, upgrading from 345-kilovolt lines to 765-kilovolt lines. In 2024, nearly 78 gigawatts of transmission-connected wind, solar and battery energy storage capacity was installed to the grid. And more than 102 gigawatts of transmission-connected renewable capacity is expected to be installed by the end of 2025, according to a December ERCOT report.It’s that growth of both demand and renewables connected to the grid that’s led ERCOT to ask the Public Utility Commission to consider upgrading the state’s transmission system rather than expanding its existing one. The 765-kV lines would provide significant economic and reliability benefits to the ERCOT system, wrote Kristi Hobbs, ERCOT’s vice president of system planning and weatherization, in the grid operator’s submission of its regional transmission plans to the commission in late January.Regardless of which transmission plan is chosen, Hobbs wrote, the explosive growth projected throughout the next six years and beyond will require major public investment. Perhaps you noticed: This story, like all the news we publish, is free to read. That’s because Inside Climate News is a 501c3 nonprofit organization. We do not charge a subscription fee, lock our news behind a paywall, or clutter our website with ads. We make our news on climate and the environment freely available to you and anyone who wants it.That’s not all. We also share our news for free with scores of other media organizations around the country. Many of them can’t afford to do environmental journalism of their own. We’ve built bureaus from coast to coast to report local stories, collaborate with local newsrooms and co-publish articles so that this vital work is shared as widely as possible.Two of us launched ICN in 2007. Six years later we earned a Pulitzer Prize for National Reporting, and now we run the oldest and largest dedicated climate newsroom in the nation. We tell the story in all its complexity. We hold polluters accountable. We expose environmental injustice. We debunk misinformation. We scrutinize solutions and inspire action.Donations from readers like you fund every aspect of what we do. If you don’t already, will you support our ongoing work, our reporting on the biggest crisis facing our planet, and help us reach even more readers in more places? Please take a moment to make a tax-deductible donation. Every one of them makes a difference.Reporter, Texas RenewablesArcelia Martin is an award-winning journalist at Inside Climate News. She covers renewable energy in Texas from her base in Dallas. Before joining ICN in 2025, Arcelia was a staff writer at The Dallas Morning News and at The Tennessean. Originally from San Diego, California, she’s a graduate of Gonzaga University and Columbia University Graduate School of Journalism.]]></content:encoded></item><item><title>Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5</title><link>https://github.com/b4rtaz/distributed-llama/discussions/162</link><author>b4rtazz</author><category>hn</category><pubDate>Sat, 15 Feb 2025 16:11:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Carbon capture more costly than switching to renewables, researchers find</title><link>https://techxplore.com/news/2025-02-carbon-capture-renewables.html</link><author>Brajeshwar</author><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dust from car brakes more harmful than exhaust, study finds</title><link>https://e360.yale.edu/digest/brake-pads-lung-damage-study</link><author>Brajeshwar</author><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Diablo hackers uncovered a speedrun scandal</title><link>https://arstechnica.com/gaming/2025/02/the-diablo-hackers-that-debunked-a-record-speedrun/</link><author>pitwin</author><category>hn</category><pubDate>Sat, 15 Feb 2025 14:00:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[But simply splitting a run into segments doesn't explain away all of the problems the TAS team found. Getting Naj's Puzzler on dungeon level 9, for instance, still requires outside modification of a save file, which is specifically prohibited by longstanding Speed Demos Archive rules that "manually editing/adding/removing game files is generally not allowed." Groobo's apparent splicing of multiple game versions and differently seeded save files also seems to go against SDA rules, which say that "there obviously needs to be continuity between segments in terms of inventory, experience points or whatever is applicable for the individual game."After being presented with the TAS team's evidence, SDA wrote that "it has been determined that Groobo's run very likely does not stem from only legitimate techniques, and as such, has itself been banished barring new developments." But Groobo's record is still listed as the "Fastest completion of an RPG videogame" by Guinness World Records, which has not offered a substantive response to the team's findings (Guinness has not responded to a request for comment from Ars Technica).
      A recent  speedrun on a confirmed legitimate dungeon seed.

          This might seem like a pretty petty issue to spend weeks of time and attention debunking. But at a recent presentation attended by Ars, Cecil said he was motivated to pursue it because "it did harm. Groobo's alleged cheating in 2009 completely stopped interest in speedrunning this category [of ]. No one tried, no one could."Because of Groobo's previously unknown modifications to make an impossible-to-beat run, "this big running community just stopped trying to run this game in that category," Cecil said. "For more than a decade, this had a chilling impact on that community." With Groobo's run out of the way, though, new runners are setting new records on confirmed legitimate RNG seeds, and with the aid of TAS tools.In the end, Cecil said he hopes the evidence regarding Groobo's run will make people look more carefully at other record submissions. "Groobo had created a number of well-respected ... speedruns," he said. "[People thought] there wasn't any good reason to doubt him. In other words, there was bias in familiarity. This was a familiar character. Why would they cheat?"]]></content:encoded></item><item><title>Show HN: I Built a Reddit-style Bluesky client – still rough, but open to ideas</title><link>https://threadsky.app/</link><author>lakshikag</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 13:19:17 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Kreuzberg – Modern async Python library for document text extraction</title><link>https://github.com/Goldziher/kreuzberg</link><author>nhirschfeld</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 10:07:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I'm excited to showcase Kreuzberg!Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.Key technical features:
- Built with modern Python best practices (async/await, type hints, functional-first)
- Optimized async I/O with anyio for multi-loop compatibility
- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)
- Efficient batch processing with concurrent extractions
- Clean error handling with context-rich exceptionsI built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.Key advantages over alternatives:
- True async support with optimized I/O
- Minimal dependencies (much smaller than alternatives)
- Perfect for serverless and async web apps
- Local processing without API calls
- Built for modern Python codebases with rigorous typing and testingThe library is MIT licensed and open to contributions.]]></content:encoded></item><item><title>Jane Street&apos;s Figgie card game</title><link>https://www.figgie.com/</link><author>eamag</author><category>hn</category><pubDate>Sat, 15 Feb 2025 09:59:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Jane Street's fast-paced
              Figgie game simulates exciting elements of markets and trading. At
              Jane Street, Figgie is a game we teach and also one we really
              enjoy playing.
            Read our FAQs
              for more. If you have a question that isn’t answered there, we’d
              like to hear
              what’s missing and what
              would be helpful to know, and we’ll do our best to update FAQs
              along the way.
            ]]></content:encoded></item><item><title>The 20 year old PSP can now connect to WPA2 WiFi Networks</title><link>https://wololo.net/2025/02/14/the-20-year-old-psp-can-now-connect-to-wpa2-wifi-networks/</link><author>zdw</author><category>hn</category><pubDate>Sat, 15 Feb 2025 03:31:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Screenshot source: Zekiu_ on youtubeAcid_Snake and the ARK Development team have released a significant update to the ARK custom Firmware for the Sony PSP. Custom Firmware now allows the Playstation Portable to connect to WPA2 encrypted Wifi networks. This is thanks to the recently released  plugin, created by developer  and published on the PSP Homebrew discord.Playstation Portable gets WPA2 Wifi accessThe PSP has been out of official support from Sony for years, but lots of enthusiasts keep maintaining this great handheld through homebrew and custom Firmware updates. As technology evolves around us, older devices such as the PlayStation Portable can lose some of their features.For example, as WPA2 has become the defacto encryption standard for home wifi networks (WPA3’s adoption rate remains low), older devices such as the PSP, that do not support these new* encryption standards become technically unable to access the internet.Wifi access was a very strong feature of the PSP when it was released, and, although it’s probably less important nowadays, losing that feature because newer networks aren’t compatible is a bummer.WPA2 support has been a request by many enthusiasts for years on PSP discussion channels, and it seems that the wpa2psp plugin by developer Moment now brings this to life. According to Acid_Snake, the developer was kind enough to provide the source code of the plugin, which allowed the ARK team to embed it into the ARK Custom Firmware for PSP.This reddit thread by Nebula_NL covers a lot of details on how to install and use the plugin. But the bottom line is: install the latest release of the ARK CFW on your PSP, and take it from there. (Note that you can also manually install the plugin if you’re using another CFW than ARK)This is of course the first iteration of this plugin, and it comes with limitations, specifically:2.4 GHz Only
WPA2 support works with 2.4 GHz WiFi.If your router uses a single SSID for both 2.4 GHz and 5 GHz, you may need to separate them and connect your PSP to the 2.4 GHz network.WPA2 AES Only
Requires WPA2 with AES (AES-CCMP) encryption.TKIP is not supported and will not work.WEP/WPA Compatibility
While WPA2 is active, WEP and WPA encryption will not work.To use WEP or WPA again, disable WPA2, and they will function normally.WPA2/WPA3 Mixed Mode
If your router is set to WPA2/WPA3 mixed mode, your PSP may struggle to obtain an IP address.Try manually setting the IP address instead of using DHCP in [AUTO] mode.Download and install ARK-4 + enable WPA2 Support for the PSP* WPA2 was certified in 2004… It’s “new” from the PSP’s perspective which launched the same year and didn’t “need” to support it at the time. WPA3 launched in 2018 but its adoption is taking time]]></content:encoded></item><item><title>Show HN: VimLM – A Local, Offline Coding Assistant for Vim</title><link>https://github.com/JosefAlbers/VimLM</link><author>JosefAlbers</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:34:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[VimLM is a local, offline coding assistant for Vim. It’s like Copilot but runs entirely on your machine—no APIs, no tracking, no cloud.- Deep Context: Understands your codebase (current file, selections, references).  
- Conversational: Iterate with follow-ups like "Add error handling".  
- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  
- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.Perfect for privacy-conscious devs or air-gapped environments.Try it:  
```
pip install vimlm
vimlm
```]]></content:encoded></item><item><title>A decade later, a decade lost (2024)</title><link>https://meyerweb.com/eric/thoughts/2024/06/07/a-decade-later-a-decade-lost/</link><author>ZeWaka</author><category>hn</category><pubDate>Fri, 14 Feb 2025 23:10:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I woke up this morning about an hour ahead of my alarm, the sky already light, birds calling.  After a few minutes, a brief patter of rain swept across the roof and moved on.I just lay there, not really thinking.  Feeling.  Remembering.Almost sixteen years to the minute before I awoke, my second daughter was born.  Almost ten years to the same minute before, she’d turned six years old, already semi-unconscious, and died not quite twelve hours later.So she won’t be taking her first solo car drive today.  She won’t be celebrating with dinner at her favorite restaurant in the whole world.  She won’t kiss her niece good night or affectionately rag on her siblings.Or maybe she wouldn’t have done any of those things anyway, after a decade of growth and changes and paths taken.  What would she really be like, at sixteen?We will never know.  We can’t even guess.  All of that, everything she might have been, is lost.This afternoon, we’ll visit Rebecca’s grave, and then go to hear her name read in remembrance at one of her very happiest places, Anshe Chesed Fairmount Temple, for the last time.  At the end of the month, the temple will close as part of a merger.  Another loss.A decade ago, I said that I felt the weight of all the years she would never have, and that they might crush me.  Over time, I have come to realize all the things she never saw or did adds to that weight.  Even though it seems like it should be the same weight.  Somehow, it isn’t.I was talking about all of this with a therapist a few days ago, about the time and the losses and their accumulated weight.  I said, “I don’t know how to be okay when I failed my child in the most fundamental way possible.”“You didn’t fail her,” they said gently.“I know that,” I replied. “But I don’t feel it.”A decade, it turns out, does not change that.  I’m not sure now that any stretch of time ever could.]]></content:encoded></item><item><title>We were wrong about GPUs</title><link>https://fly.io/blog/wrong-about-gpu/</link><author>mxstbr</author><category>hn</category><pubDate>Fri, 14 Feb 2025 22:36:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them. But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel’s Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers(not even bidirectional: in common configurations, GPUs talk to each other)with arbitrary, end-user controlled computation, all operating outside our normal security boundary.We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.(you should all feel sympathy for us)This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.But Cursor happened, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren’t recoverable. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I’m even more comfortable making bets backed by tradable assets with durable value.In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.]]></content:encoded></item><item><title>The hardest working font in Manhattan</title><link>https://aresluna.org/the-hardest-working-font-in-manhattan/</link><author>robinhouston</author><category>hn</category><pubDate>Fri, 14 Feb 2025 21:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
		In 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I <3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama’s first campaign. 
	
		But there was one font I didn’t even notice, even though it was everywhere around me.
			
		Last year in New York, I walked over 100 miles and took thousands of photos of one and one font only.
			
		The font’s name is Gorton.
			
		It’s hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for my book about the history of typing. 
			
		Many keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird mélange of “mechanical” and “childish.”
			 
		The more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. & and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it – a beginning of a ligature that never came.
			
		The strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual – perhaps even naïve – symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.
				
		Looking at just a few keyboards, it was also obvious that it wasn’t just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) 
			
		My first thought was: What a mess. Is this how “grotesque” fonts got their name?
			
		Then, the second thought: I kind of like it.
	The most distinctive letterforms of Gorton		
		But what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham’s.
			
		A few places for keyboard nerds referred to the font as “Gorton,” but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.
					
		I originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones – those from the early 1960s laboratory computers, or the 1980s microcomputers – the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible – the legend could not wear off any more than its key – and I imagined required some specialized keyboard-making machinery that came with the “keyboard font” already there.
	
		An example of a “double-shot” key from above and from below
				
		But then, I started seeing Gorton in other places.
						
		Hours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.
						
		One day, I saw what felt like Gorton on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist’s office. In an alley. 
							
		These had one thing in common. All of the letters were carved into the respective base material – metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.
						
		At one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.
								
		Some searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.
									
		This immediately explained both the metaphorical and literal rough edges of Gorton.
						
		A lot of typography has roots in calligraphy – someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most “mechanical” fonts have surprising humanistic touches if you inspect them close enough.
				
		But not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts “monoline”). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.
		
		Monoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. 
			
		It seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century’s middle decades. But casual research through materials preserved by some of George Gorton Machine Company’s fans – including the grandson of the founder – revealed something even more interesting. Gorton the font was a lot older than I expected. 
			
		I found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.
				
		To put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston’s London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.
	
		Imagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine’s then-current logo!
						
		I started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote “I used Gorton to typeset this,” because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adorning the screens of early computers.
						
		The difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.
	
		Gorton wasn’t just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.
						
		It was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE… and it went  and then the Moon, as key legends on Apollo’s onboard computer.
								
		But why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?
						
		Some of it might be the power of the default. Popular engraving equipment comes with a built-in font that’s so standard it reuses the router’s name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.
	
		Gorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering – every letter done by hand – then a modern font you could simply  and one designed with “a minimum of sharp corners for rapid tracing with a smooth stroke,” must have felt like a breath of fresh air.
								
		But why engraving to begin with? Because the affordable and casual printing options we enjoy today – the office laser printers and home inkjets, the FedEx Kinko’s, the various cheap labelers – weren’t there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.
				
		On top of that, Gorton’s longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.
	Various items from the 20th century typeset in Gorton					
		I felt good about all my findings: What a strange story of a strange routing font! 
	
		But it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.
	We’re used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren’t as easy back when fonts were made out of metal. Life’s not as much fun outside of the glamor of a TTF file, and a routing font couldn’t immediately become a regular font – so seeing Gorton being additive and not subtractive was an unexpected discovery.
								
		It turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.
								
		A company called Keuffel & Esser Co. grabbed Gorton’s machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away – still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving – instead, you’d mount a special pen and draw letters by tracing them.
								
		Another company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.
								
		Some of the traditional draftspeople pooh-poohed these inventions – one handbook wrote “[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.”
				
		But the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway’s, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.
				
		Then, both EC Comics and All-Star Comics used Leroy in the 1940s and 1950s, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.
				
		Elsewhere silkscreening – a pretty cool technique of applying paint on surfaces through a wet mesh of fabric – took Gorton and Leroy in a different direction, by allowing paint on metal.
				
		There was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.
	
		I also counted at least three different systems of “Gorton movable type” – some where you could assemble physical letters, and some where you could impress them into soft materials using steel types – and I imagine there were probably more.
				
		Letraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too – and so did a few competitors.
						
		In the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998… then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via ANSI Y14.2M, and so did institutions like the US National Park Service.
					
		Gorton went on and on and on. The early Hershey vector fonts, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren’t even necessary – any letter could now be drawn by a series of straight lines.
						
		And even in the first universe Gorton inhabited things weren’t standing still. 
	
		As the engraving industry learned what’s popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called “The Whereabouts of 230 Engraving Machines” listed Gorton customers ranging from biscuit makers to fire engine constructors. Othercatalogsproudly listed applications like book covers, billiard balls, organ keys, and toothbrushes, as well as “tools making more tools” – using Gorton engravers to create legends for other machines.
			
		After you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:
	
		The original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with old-fashioned displays and complex interfaces labeled… in Gorton itself. 
	
		But the technology matured quickly and soon more and more early manual “tracer-guided” pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by computerized ones, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.
					
		You no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8´´ to 3´´ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) 
	
		Now, fonts came as digits or formulas built into computer memory, or – for a moment in time – as separate cartridges you’d insert in eager slots. (And yes, before you ask: there were other routing monoline fonts, too. But I really don’t care about any of them.)
						
		It was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.
						
		And automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I’ve seen – a weird combination of a machine pretending to be another machine pretending to be a human hand.)
					
		If this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.
				
		I don’t think the – by now 80 years and counting – fractal explosion of Gorton made its original creators rich.
				
		Copy protection in the world of typography is complicated. The font’s name can be trademarked and other companies legally prevented from using it, and you can’t just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it – and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.
				
		This is why Keuffel & Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn’t name it Gorton. 
								
		But of course, Gorton was barely named “Gorton” to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called “standard copies.”) Then, as other fonts were added, it was retroactively named Gorton Normal – the name of the company and the most generic word possible.
						
		Leroy lettering sets started with one font, so similarly to Gorton the font started to be known as “Leroy,” then “Series C,” then “Gothic.” New Hermes called it simply “Block,” Letraset went with “Engineering Standard,” and Rotring – another producer of little computerized plotters – with “Universal.” I’ve also seen “A style,” “Plain Gothic,” and, mysteriously, “Standpoint.” 
								
		I don’t think this was meant to be disrespectful. “Standard,“ “Universal,” “A style” might not have had the connotations of “generic” we associate with them today, but rather meaning “the only one you need,” “approved of by millions,” or “the ultimate.”
								
		But there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as “Linetica.”
	A few rare examples of Gorton Extended in use						
		Each of these reappearances made small changes to the shapes of some letters. Leroy’s ampersand was a departure from Gorton’s. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation – commas, quotes, question marks – was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton’s pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition… and at times both. The extended version – not that popular to begin with – was often skipped.)
	Classic Gorton vs. Gorton Modified						
		The only “official” update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. 
	
		It is also that last Gorton that mattered.
	A collection of movies and TV shows featuring Gorton							
		My every walk in Chicago or San Francisco was counting down “time to Gorton” – sometimes mere minutes before I saw a placard or an intercom with the familiar font.
								
		This might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise – a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn’t exist, I saw it surprisingly often.
										
		I even spotted Gorton a few times in Spain, or the U.K., and didn’t make too much of it, not thinking about the likelihood of machines from George Gorton’s company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.
	Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R						
		It was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:
								
		Some letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:
								
		And a visit to a naval museum cemented it all:
	
		It was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn’t stretch all the way across, and the distinctive shape of S was slightly different here.
								
		Fortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn’t actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor & Hobson. 
								
		In 1894, TT&H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do – instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.
	
		I don’t know how this first proto-Gorton was designed – unfortunately, Taylor, Taylor & Hobson’s history seems sparse and despite personal travels to U.K. archives, I haven’t found anything interesting – but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.
	From a 1895 “Free-hand lettering” book by Frank T. Daniels					
		This was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font – the first time technical writing was able to be replicated consistently via the aid of the machine.
				
		No wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor & Hobson licensed their stuff to a fledgling American company named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&H font must have been slightly adjusted on arrival. 
	A Taylor-Hobson pantograph in use in 1942			
		This adds to the accomplishments of Gorton – the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in one of the UK’s national parks), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.
				
		If the Taylor, Taylor & Hobson (or, Taylor-Hobson, as their engravers were known) “branch” of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&H didn’t properly name the font either, eventually calling it “A style” for regular and “C style” for condensed variants. Google searches for “taylor hobson font” are a lot more sparse than those for Gorton.)  
	GortonGorton CondensedThe Gorton quasisuperfamily
		In the end, I’m sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)
	A comparison of all major editions of Gorton					
		And so, throughout the 20th century, Gorton has lived two parallel lives – one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. 
								
		I am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (Chances are, you will see Gorton later today!) I’m curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I’m interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton’s niche.
	
		The history of this strange font spans over a century and I’ve seen it in so many countries by now, used in so many situations. But it’s impossible for me to say Gorton is the most hard-working font in the world.
								
		To this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be “the default” – from Times New Roman through Calibri to Roboto – immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5×7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.
								
		But there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.
							
		In 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.
								
		Gorton is everywhere in Manhattan. It’s there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it’s there on so, so many intercoms.
						
		I wouldn’t be surprised if there weren’t a single block without any Gorton in a whole of Manhattan.
	A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn’t have access to the interiors of most buildings.	
		The omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font’s already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:
								
		You can see all sorts of kerning mistakes:
										
		You will notice the many, many routing imperfections – an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:
	
		There are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:
												
		You can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:
	
    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, of unknown provenance and even more nameless than I thought possible for an already name-free font.
  
		But New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:
	
	
			All my Gorton walks in Manhattan in 2024
		

		This was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.
	
		In a city that never sleeps, Gorton wasn’t allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil’s work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.
	
		And once in a while, I’d find Gorton that would wink at me with a story – followed by that aching in the heart as I realized I’d never know what the story was.
				
		You’re not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.
								
		So why do I love Gorton so much? 
								
		The Occam’s Razor seems sharp on this one. Perhaps I like it because I’m a boy and Gorton is often attached to heavy machinery. 
					
		But there must be more to it. Perhaps it’s all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it’s been made by machines for machines – but in its use, it’s also the font that allows you to see so many human mistakes and imperfections.
					
		Gorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don’t apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. 
	
		Stretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don’t stretch the tip of the router itself.
								
		There are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibility.
		And on top of that, the simplicity of the letterforms themselves feels compatible with the typical naïveté of Gorton’s typesetting. 
	Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow
    Sure, there are really bad renditions that are inexcusable. 

		But most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don’t feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don’t feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used – haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)
				
		The transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn’t meant to be admired, celebrated, treasured. It’s meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton – especially those without instructions on how letters are positioned against each other – might not even classify as a font!)
						
		And I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. 	
		With Gorton, carving into metal or plastic means good-looking fixes are impossible:
	
		And unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.
	
		Perhaps above everything else, Gorton is all about . 
  
    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  
		mid-century intercom and filled in with special paste or wax, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. 
				
		(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts – you just have to look very, very closely.)
	
		That’s the last, and perhaps the best thing to fall in love with. 
	
		You won’t be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton’s non-beauty can be:
						
		This has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. 
						
		I feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don’t know the full extent of Gorton’s use. I don’t know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor & Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.
						
		Also, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?
						
		But mostly, I can’t shake the feeling that we all missed a window. That this essay can’t be just a celebration, but also needs to be the beginnings of a eulogy.
						
		Walking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.
								
		Gorton itself is obsolete. All the keyboards that use Gorton Modified you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don’t change all that much. But few of the new placards use Gorton, and none of the new intercoms do. 
	
		Taylor, Taylor & Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny & Trecker, which became Cross & Trecker, was acquired by Giddings & Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&H and George Gorton’s company are today selling Gorton in any capacity.
								
		It will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (I eventually found quite a few) won’t help. They are perhaps all missing a point, anyway.
	
		But there’s a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.
	
		But fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.
								
		Manhattan’s tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton’s no stranger to water, snow, rust, or dirt.
			
		This is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing – they’re a .
								
		You find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.		
	]]></content:encoded></item><item><title>Ask HN: What is the best method for turning a scanned book as a PDF into text?</title><link>https://news.ycombinator.com/item?id=43048698</link><author>resource_waste</author><category>hn</category><pubDate>Fri, 14 Feb 2025 14:32:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I like reading philosophy, particularly from the authors rather than a secondhand account.However I often run into that these come as scanned documents, Discourses on Livy and Politics Among Nations for example.I would greatly benefit from turning these into text. I can snipping tool pages and put them in ChatGPT and it turns out perfect. If I used classic methods, it often screws up words. My final goal is to turn these into audiobooks, (or even just make it easier to copypaste for my personal notes)Given the state of AI, I'm wondering what my options are. I don't mind paying.]]></content:encoded></item><item><title>Show HN: Transform your codebase into a single Markdown doc for feeding into AI</title><link>https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html</link><author>tesserato</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:23:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.
The output for the current repository can be found here.Comprehensive Codebase Documentation: Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format. Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types). Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.Simple Command-Line Interface:  Offers an intuitive command-line interface with straightforward options for customization.If you have Go installed, run go install github.com/tesserato/CodeWeaver@latestto install the latest version of CodeWeaver or go install github.com/tesserato/CodeWeaver@vX.Y.Z to install a specific version.Alternatively, download the appropriate pre built executable from the releases page.If necessary, make the  executable by using the  command:The root directory to scan and document.The name of the output Markdown file.-ignore "<regex patterns>"Comma-separated list of regular expression patterns for paths to exclude.-included-paths-file <filename>File to save the list of paths that were included in the documentation.-excluded-paths-file <filename>File to save the list of paths that were excluded from the documentation.Display this help message and exit.Generate documentation for the current directory:This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).Specify a different input directory and output file:./codeweaver -dir=my_project -output=project_docs.md
This command will process the  directory and save the documentation to .Ignore specific file types and directories:./codeweaver -ignore="\.log,temp,build" -output=detailed_docs.md
This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.Save lists of included and excluded paths:./codeweaver -ignore="node_modules" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md
This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.CodeWeaver is released under the MIT License. See the  file for complete license details.]]></content:encoded></item><item><title>Show HN: A New Way to Learn Languages</title><link>https://www.langturbo.com/</link><author>sebnun</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenVINO AI effects [denoising and transcription] for Audacity</title><link>https://www.audacityteam.org/blog/openvino-ai-effects/</link><author>wazoox</author><category>hn</category><pubDate>Fri, 14 Feb 2025 11:19:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Introducing OpenVINO AI effects for AudacityIntel has built a suite of AI tools for Audacity, useful for spoken word audio and music alike. These AI features run 100% locally on your PC.For spoken word content, the OpenVINO effects contain a noise supression and a transcription plugin.The  does what it says on the tin - it suppresses noise. As such it behaves similar to Audacity’s built-in Noise Removal effect.The  powered by Whisper.cpp can both transcribe and translate words and outputs to a label track. If you want to export these transcriptions, you can do so via File → Export Other → Export Labels.For music, both generation and separation plugins are part of the OpenVINO effects. and  use Stable Diffusion (and Riffusion in particular) to generate new music from a prompt, or based on pre-existing music, respectively. can split a song into either it’s vocal and instrumental parts, or into vocals, drums, bass and a combined “anything else” part. This is ideal for creating covers and playalongs.Download and installationCurrently, only a Windows version is available for download. The project may be compiled on Linux and macOS, though no instructions are available for the latter yet.If you have any questions or comments, feel free to post them to the plugin’s issue tracker.]]></content:encoded></item></channel></rss>